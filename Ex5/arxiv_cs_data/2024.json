[
    {
        "title": "The Cytnx Library for Tensor Networks",
        "authors": [
            "Kai-Hsin Wu",
            "Chang-Teng Lin",
            "Ke Hsu",
            "Hao-Ti Hung",
            "Manuel Schneider",
            "Chia-Min Chung",
            "Ying-Jer Kao",
            "Pochung Chen"
        ],
        "summary": "We introduce a tensor network library designed for classical and quantum physics simulations called Cytnx (pronounced as sci-tens). This library provides almost an identical interface and syntax for both C++ and Python, allowing users to effortlessly switch between two languages. Aiming at a quick learning process for new users of tensor network algorithms, the interfaces resemble the popular Python scientific libraries like NumPy, Scipy, and PyTorch. Not only multiple global Abelian symmetries can be easily defined and implemented, Cytnx also provides a new tool called Network that allows users to store large tensor networks and perform tensor network contractions in an optimal order automatically. With the integration of cuQuantum, tensor calculations can also be executed efficiently on GPUs. We present benchmark results for tensor operations on both devices, CPU and GPU. We also discuss features and higher-level interfaces to be added in the future.",
        "published": "2024-01-03T14:59:50Z",
        "link": "http://arxiv.org/abs/2401.01921v1",
        "categories": [
            "cs.MS",
            "cond-mat.str-el"
        ]
    },
    {
        "title": "Toward a comprehensive simulation framework for hypergraphs: a   Python-base approach",
        "authors": [
            "Quoc Chuong Nguyen",
            "Trung Kien Le"
        ],
        "summary": "Hypergraphs, or generalization of graphs such that edges can contain more than two nodes, have become increasingly prominent in understanding complex network analysis. Unlike graphs, hypergraphs have relatively few supporting platforms, and such dearth presents a barrier to more widespread adaptation of hypergraph computational toolboxes that could enable further research in several areas. Here, we introduce HyperRD, a Python package for hypergraph computation, simulation, and interoperability with other powerful Python packages in graph and hypergraph research. Then, we will introduce two models on hypergraph, the general Schelling's model and the SIR model, and simulate them with HyperRD.",
        "published": "2024-01-08T14:24:54Z",
        "link": "http://arxiv.org/abs/2401.03917v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Efficient N-to-M Checkpointing Algorithm for Finite Element Simulations",
        "authors": [
            "David A. Ham",
            "Vaclav Hapla",
            "Matthew G. Knepley",
            "Lawrence Mitchell",
            "Koki Sagiyama"
        ],
        "summary": "In this work, we introduce a new algorithm for N-to-M checkpointing in finite element simulations. This new algorithm allows efficient saving/loading of functions representing physical quantities associated with the mesh representing the physical domain. Specifically, the algorithm allows for using different numbers of parallel processes for saving and loading, allowing for restarting and post-processing on the process count appropriate to the given phase of the simulation and other conditions. For demonstration, we implemented this algorithm in PETSc, the Portable, Extensible Toolkit for Scientific Computation, and added a convenient high-level interface into Firedrake, a system for solving partial differential equations using finite element methods. We evaluated our new implementation by saving and loading data involving 8.2 billion finite element degrees of freedom using 8,192 parallel processes on ARCHER2, the UK National Supercomputing Service.",
        "published": "2024-01-11T12:20:50Z",
        "link": "http://arxiv.org/abs/2401.05868v2",
        "categories": [
            "cs.DC",
            "cs.MS"
        ]
    },
    {
        "title": "Approximations of the integral of a class of sinusoidal composite   functions",
        "authors": [
            "Alberto Costa"
        ],
        "summary": "Two approximations of the integral of a class of sinusoidal composite functions, for which an explicit form does not exist, are derived. Numerical experiments show that the proposed approximations yield an error that does not depend on the width of the integration interval. Using such approximations, definite integrals can be computed in almost real-time.",
        "published": "2024-01-16T03:13:02Z",
        "link": "http://arxiv.org/abs/2401.08080v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Proceedings 14th International Conference on Automated Deduction in   Geometry",
        "authors": [
            "Pedro Quaresma",
            "Zoltán Kovács"
        ],
        "summary": "ADG is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. The conference is held every two years. The previous editions of ADG were held in Hagenberg in 2021 (online, postponed from 2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996.   The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22, 2023. This edition of ADG had an additional special focus topic, Deduction in Education.   Invited Speakers: Julien Narboux, University of Strasbourg, France \"Formalisation, arithmetization and automatisation of geometry\"; Filip Mari\\'c, University of Belgrade, Serbia, \"Automatization, formalization and visualization of hyperbolic geometry\"; Zlatan Magajna, University of Ljubljana, Slovenia, \"Workshop OK Geometry\"",
        "published": "2024-01-19T14:42:08Z",
        "link": "http://arxiv.org/abs/2401.10725v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CG",
            "cs.MS"
        ]
    },
    {
        "title": "PlasmoData.jl -- A Julia Framework for Modeling and Analyzing Complex   Data as Graphs",
        "authors": [
            "David L Cole",
            "Victor M Zavala"
        ],
        "summary": "Datasets encountered in scientific and engineering applications appear in complex formats (e.g., images, multivariate time series, molecules, video, text strings, networks). Graph theory provides a unifying framework to model such datasets and enables the use of powerful tools that can help analyze, visualize, and extract value from data. In this work, we present PlasmoData.jl, an open-source, Julia framework that uses concepts of graph theory to facilitate the modeling and analysis of complex datasets. The core of our framework is a general data modeling abstraction, which we call a DataGraph. We show how the abstraction and software implementation can be used to represent diverse data objects as graphs and to enable the use of tools from topology, graph theory, and machine learning (e.g., graph neural networks) to conduct a variety of tasks. We illustrate the versatility of the framework by using real datasets: i) an image classification problem using topological data analysis to extract features from the graph model to train machine learning models; ii) a disease outbreak problem where we model multivariate time series as graphs to detect abnormal events; and iii) a technology pathway analysis problem where we highlight how we can use graphs to navigate connectivity. Our discussion also highlights how PlasmoData.jl leverages native Julia capabilities to enable compact syntax, scalable computations, and interfaces with diverse packages.",
        "published": "2024-01-21T05:04:38Z",
        "link": "http://arxiv.org/abs/2401.11404v2",
        "categories": [
            "cs.MS",
            "cs.LG"
        ]
    },
    {
        "title": "Sphractal: Estimating the Fractal Dimension of Surfaces Computed from   Precise Atomic Coordinates via Box-Counting Algorithm",
        "authors": [
            "Jonathan Yik Chang Ting",
            "Andrew Thomas Agars Wood",
            "Amanda Susan Barnard"
        ],
        "summary": "The fractal dimension of a surface allows its degree of roughness to be characterized quantitatively. However, limited effort is attempted to calculate the fractal dimension of surfaces computed from precisely known atomic coordinates from computational biomolecular and nanomaterial studies. This work proposes methods to estimate the fractal dimension of the surface of any 3D object composed of spheres, by representing the surface as either a voxelized point cloud or a mathematically exact surface, and computing its box-counting dimension. Sphractal is published as a Python package that provides these functionalities, and its utility is demonstrated on a set of simulated palladium nanoparticle data.",
        "published": "2024-01-22T07:29:22Z",
        "link": "http://arxiv.org/abs/2401.11737v2",
        "categories": [
            "cs.MS",
            "physics.atom-ph",
            "physics.comp-ph",
            "J.2"
        ]
    },
    {
        "title": "Open Source Prover in the Attic",
        "authors": [
            "Zoltán Kovács",
            "Alexander Vujic"
        ],
        "summary": "The well known JGEX program became open source a few years ago, but seemingly, further development of the program can only be done without the original authors. In our project, we are looking at whether it is possible to continue such a large project as a newcomer without the involvement of the original authors. Is there a way to internationalize, fix bugs, improve the code base, add new features? In other words, to save a relic found in the attic and polish it into a useful everyday tool.",
        "published": "2024-01-22T12:50:29Z",
        "link": "http://arxiv.org/abs/2401.13702v1",
        "categories": [
            "cs.PL",
            "cs.MS",
            "cs.SC",
            "cs.SE"
        ]
    },
    {
        "title": "Scalable Automated Verification for Cyber-Physical Systems in   Isabelle/HOL",
        "authors": [
            "Jonathan Julián Huerta y Munive",
            "Simon Foster",
            "Mario Gleirscher",
            "Georg Struth",
            "Christian Pardillo Laursen",
            "Thomas Hickman"
        ],
        "summary": "We formally introduce IsaVODEs (Isabelle verification with Ordinary Differential Equations), a framework for the verification of cyber-physical systems. We describe the semantic foundations of the framework's formalisation in the Isabelle/HOL proof assistant. A user-friendly language specification based on a robust state model makes our framework flexible and adaptable to various engineering workflows. New additions to the framework increase both its expressivity and proof automation. Specifically, formalisations related to forward diamond correctness specifications, certification of unique solutions to ordinary differential equations (ODEs) as flows, and invariant reasoning for systems of ODEs contribute to the framework's scalability and usability. Various examples and an evaluation validate the effectiveness of our framework.",
        "published": "2024-01-22T15:54:47Z",
        "link": "http://arxiv.org/abs/2401.12061v1",
        "categories": [
            "cs.LO",
            "cs.MS"
        ]
    },
    {
        "title": "LongMemory.jl: Generating, Estimating, and Forecasting Long Memory   Models in Julia",
        "authors": [
            "J. Eduardo Vera-Valdés"
        ],
        "summary": "LongMemory.jl is a package for time series long memory modelling in Julia. The package provides functions to generate long memory, estimate model parameters, and forecast. Generating methods include fractional differencing, stochastic error duration, and cross-sectional aggregation. Estimators include the classic ones used to estimate the Hurst effect, those inspired by log-periodogram regression, and parametric ones. Forecasting is provided for all parametric estimators. Moreover, the package adds plotting capabilities to illustrate long memory dynamics and forecasting. This article presents the theoretical developments for long memory modelling, show examples using the data included with the package, and compares the properties of LongMemory.jl with current alternatives, including benchmarks. For some of the theoretical developments, LongMemory.jl provides the first publicly available implementation in any programming language. A notable feature of this package is that all functions are implemented in the same programming language, taking advantage of the ease of use and speed provided by Julia. Therefore, all code is accessible to the user. Multiple dispatch, a novel feature of the language, is used to speed computations and provide consistent calls to related methods. The package is related to the R packages LongMemoryTS and fracdiff.",
        "published": "2024-01-25T10:55:38Z",
        "link": "http://arxiv.org/abs/2401.14077v1",
        "categories": [
            "cs.MS",
            "stat.CO"
        ]
    },
    {
        "title": "Evaluation of POSIT Arithmetic with Accelerators",
        "authors": [
            "Naohito Nakasato",
            "Yuki Murakami",
            "Fumiya Kono",
            "Maho Nakata"
        ],
        "summary": "We present an evaluation of 32-bit POSIT arithmetic through its implementation as accelerators on FPGAs and GPUs. POSIT, a floating-point number format, adaptively changes the size of its fractional part. We developed hardware designs for FPGAs and software for GPUs to accelerate linear algebra operations using Posit(32,2) arithmetic. Our FPGA- and GPU-based accelerators in Posit(32,2) arithmetic significantly accelerated the Cholesky and LU decomposition algorithms for dense matrices. In terms of numerical accuracy, Posit(32,2) arithmetic is approximately 0.5 - 1.0 digits more accurate than the standard 32-bit format, especially when the norm of the elements of the input matrix is close to 1. Evaluating power consumption, we observed that the power efficiency of the accelerators ranged between 0.043 - 0.076 Gflops/watts for the LU decomposition in Posit(32,2) arithmetic. The power efficiency of the latest GPUs as accelerators of Posit(32,2) arithmetic is better than that of the evaluated FPGA chip.",
        "published": "2024-01-25T11:54:44Z",
        "link": "http://arxiv.org/abs/2401.14117v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.MS"
        ]
    },
    {
        "title": "Mixed-Order Meshes through rp-adaptivity for Surface Fitting to Implicit   Geometries",
        "authors": [
            "Ketan Mittal",
            "Veselin A. Dobrev",
            "Patrick Knupp",
            "Tzanio Kolev",
            "Franck Ledoux",
            "Claire Roche",
            "Vladimir Z. Tomov"
        ],
        "summary": "Computational analysis with the finite element method requires geometrically accurate meshes. It is well known that high-order meshes can accurately capture curved surfaces with fewer degrees of freedom in comparison to low-order meshes. Existing techniques for high-order mesh generation typically output meshes with same polynomial order for all elements. However, high order elements away from curvilinear boundaries or interfaces increase the computational cost of the simulation without increasing geometric accuracy. In prior work, we have presented one such approach for generating body-fitted uniform-order meshes that takes a given mesh and morphs it to align with the surface of interest prescribed as the zero isocontour of a level-set function. We extend this method to generate mixed-order meshes such that curved surfaces of the domain are discretized with high-order elements, while low-order elements are used elsewhere. Numerical experiments demonstrate the robustness of the approach and show that it can be used to generate mixed-order meshes that are much more efficient than high uniform-order meshes. The proposed approach is purely algebraic, and extends to different types of elements (quadrilaterals/triangles/tetrahedron/hexahedra) in two- and three-dimensions.",
        "published": "2024-01-29T18:10:01Z",
        "link": "http://arxiv.org/abs/2401.16369v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "Reproducibility, energy efficiency and performance of pseudorandom   number generators in machine learning: a comparative study of python, numpy,   tensorflow, and pytorch implementations",
        "authors": [
            "Benjamin Antunes",
            "David R. C Hill"
        ],
        "summary": "Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine learning technologies because they are interesting for numerous methods. The field of machine learning holds the potential for substantial advancements across various domains, as exemplified by recent breakthroughs in Large Language Models (LLMs). However, despite the growing interest, persistent concerns include issues related to reproducibility and energy consumption. Reproducibility is crucial for robust scientific inquiry and explainability, while energy efficiency underscores the imperative to conserve finite global resources. This study delves into the investigation of whether the leading Pseudo-Random Number Generators (PRNGs) employed in machine learning languages, libraries, and frameworks uphold statistical quality and numerical reproducibility when compared to the original C implementation of the respective PRNG algorithms. Additionally, we aim to evaluate the time efficiency and energy consumption of various implementations. Our experiments encompass Python, NumPy, TensorFlow, and PyTorch, utilizing the Mersenne Twister, PCG, and Philox algorithms. Remarkably, we verified that the temporal performance of machine learning technologies closely aligns with that of C-based implementations, with instances of achieving even superior performances. On the other hand, it is noteworthy that ML technologies consumed only 10% more energy than their C-implementation counterparts. However, while statistical quality was found to be comparable, achieving numerical reproducibility across different platforms for identical seeds and algorithms was not achieved.",
        "published": "2024-01-30T15:44:14Z",
        "link": "http://arxiv.org/abs/2401.17345v2",
        "categories": [
            "cs.MS",
            "cs.LG"
        ]
    },
    {
        "title": "Rigorous Error Analysis for Logarithmic Number Systems",
        "authors": [
            "Thanh Son Nguyen",
            "Alexey Solovyev",
            "Ganesh Gopalakrishnan"
        ],
        "summary": "Logarithmic Number Systems (LNS) hold considerable promise in helping reduce the number of bits needed to represent a high dynamic range of real-numbers with finite precision, and also efficiently support multiplication and division. However, under LNS, addition and subtraction turn into non-linear functions that must be approximated - typically using precomputed table-based functions. Additionally, multiple layers of error correction are typically needed to improve result accuracy. Unfortunately, previous efforts have not characterized the resulting error bound. We provide the first rigorous analysis of LNS, covering detailed techniques such as co-transformation that are crucial to implementing subtraction with reasonable accuracy. We provide theorems capturing the error due to table interpolations, the finite precision of pre-computed values in the tables, and the error introduced by fix-point multiplications involved in LNS implementations. We empirically validate our analysis using a Python implementation, showing that our analytical bounds are tight, and that our testing campaign generates inputs diverse-enough to almost match (but not exceed) the analytical bounds. We close with discussions on how to adapt our analysis to LNS systems with different bases and also discuss many pragmatic ramifications of our work in the broader arena of scientific computing and machine learning.",
        "published": "2024-01-30T17:12:56Z",
        "link": "http://arxiv.org/abs/2401.17184v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA",
            "65G50",
            "G.1"
        ]
    },
    {
        "title": "cmaes : A Simple yet Practical Python Library for CMA-ES",
        "authors": [
            "Masahiro Nomura",
            "Masashi Shibata"
        ],
        "summary": "The covariance matrix adaptation evolution strategy (CMA-ES) has been highly effective in black-box continuous optimization, as demonstrated by its success in both benchmark problems and various real-world applications. To address the need for an accessible yet potent tool in this domain, we developed cmaes, a simple and practical Python library for CMA-ES. cmaes is characterized by its simplicity, offering intuitive use and high code readability. This makes it suitable for quickly using CMA-ES, as well as for educational purposes and seamless integration into other libraries. Despite its simplistic design, cmaes maintains enhanced functionality. It incorporates recent advancements in CMA-ES, such as learning rate adaptation for challenging scenarios, transfer learning, and mixed-integer optimization capabilities. These advanced features are accessible through a user-friendly API, ensuring that cmaes can be easily adopted in practical applications. We regard cmaes as the first choice for a Python CMA-ES library among practitioners. The software is available under the MIT license at https://github.com/CyberAgentAILab/cmaes.",
        "published": "2024-02-02T12:55:10Z",
        "link": "http://arxiv.org/abs/2402.01373v2",
        "categories": [
            "cs.NE",
            "cs.MS"
        ]
    },
    {
        "title": "Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package   in R and Python",
        "authors": [
            "Giovanni Saraceno",
            "Marianthi Markatou",
            "Raktim Mukhopadhyay",
            "Mojgan Golzy"
        ],
        "summary": "We introduce the QuadratiK package that incorporates innovative data analysis methodologies. The presented software, implemented in both R and Python, offers a comprehensive set of goodness-of-fit tests and clustering techniques using kernel-based quadratic distances, thereby bridging the gap between the statistical and machine learning literatures. Our software implements one, two and k-sample tests for goodness of fit, providing an efficient and mathematically sound way to assess the fit of probability distributions. Expanded capabilities of our software include supporting tests for uniformity on the d-dimensional Sphere based on Poisson kernel densities. Particularly noteworthy is the incorporation of a unique clustering algorithm specifically tailored for spherical data that leverages a mixture of Poisson kernel-based densities on the sphere. Alongside this, our software includes additional graphical functions, aiding the users in validating, as well as visualizing and representing clustering results. This enhances interpretability and usability of the analysis. In summary, our R and Python packages serve as a powerful suite of tools, offering researchers and practitioners the means to delve deeper into their data, draw robust inference, and conduct potentially impactful analyses and inference across a wide array of disciplines.",
        "published": "2024-02-03T23:04:32Z",
        "link": "http://arxiv.org/abs/2402.02290v2",
        "categories": [
            "stat.CO",
            "cs.LG",
            "cs.MS",
            "stat.AP",
            "stat.ML"
        ]
    },
    {
        "title": "MATLAB Simulator of Level-Index Arithmetic",
        "authors": [
            "Mantas Mikaitis"
        ],
        "summary": "Level-index arithmetic appeared in the 1980s. One of its principal purposes is to abolish the issues caused by underflows and overflows in floating point. However, level-index arithmetic does not expand the set of numbers but spaces out the numbers of large magnitude even more than floating-point representations to move the infinities further away from zero: gaps between numbers on both ends of the range become very large. We revisit level index by presenting a custom precision simulator in MATLAB. This toolbox is useful for exploring performance of level-index arithmetic in research projects, such as using 8-bit and 16-bit representations in machine learning algorithms where narrow bit-width is desired but overflow/underflow of floating-point representations causes difficulties.",
        "published": "2024-02-03T23:49:08Z",
        "link": "http://arxiv.org/abs/2402.02301v2",
        "categories": [
            "cs.MS",
            "cs.AR",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "TopoX: A Suite of Python Packages for Machine Learning on Topological   Domains",
        "authors": [
            "Mustafa Hajij",
            "Mathilde Papillon",
            "Florian Frantzen",
            "Jens Agerberg",
            "Ibrahem AlJabea",
            "Rubén Ballester",
            "Claudio Battiloro",
            "Guillermo Bernárdez",
            "Tolga Birdal",
            "Aiden Brent",
            "Peter Chin",
            "Sergio Escalera",
            "Simone Fiorellino",
            "Odin Hoff Gardaa",
            "Gurusankar Gopalakrishnan",
            "Devendra Govil",
            "Josef Hoppe",
            "Maneel Reddy Karri",
            "Jude Khouja",
            "Manuel Lecha",
            "Neal Livesay",
            "Jan Meißner",
            "Soham Mukherjee",
            "Alexander Nikitin",
            "Theodore Papamarkou",
            "Jaro Prílepok",
            "Karthikeyan Natesan Ramamurthy",
            "Paul Rosen",
            "Aldo Guzmán-Sáenz",
            "Alessandro Salatiello",
            "Shreyas N. Samaga",
            "Simone Scardapane",
            "Michael T. Schaub",
            "Luca Scofano",
            "Indro Spinelli",
            "Lev Telyatnikov",
            "Quang Truong",
            "Robin Walters",
            "Maosheng Yang",
            "Olga Zaghen",
            "Ghada Zamzmi",
            "Ali Zia",
            "Nina Miolane"
        ],
        "summary": "We introduce TopoX, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. TopoX consists of three packages: TopoNetX facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; TopoEmbedX provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; TopoModelX is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of TopoX is available under MIT license at https://pyt-team.github.io/}{https://pyt-team.github.io/.",
        "published": "2024-02-04T10:41:40Z",
        "link": "http://arxiv.org/abs/2402.02441v5",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MS",
            "stat.CO"
        ]
    },
    {
        "title": "FEniCSx Preconditioning Tools (FEniCSx-pctools)",
        "authors": [
            "Martin Řehoř",
            "Jack S. Hale"
        ],
        "summary": "FEniCSx Preconditioning Tools (FEniCSx-pctools) is a software package for easing the specification of PETSc-based block preconditioning strategies in the DOLFINx finite element solver of the FEniCS Project. It attaches all of the necessary metadata to the block-structured linear systems in order that block-structured preconditioners can be applied straightforwardly via PETSc's options-based configuration system. Fast prototyping is facilitated thanks to the implementation in Python, and all intensive operations are executed in C/C++. FEniCSx-pctools is available under the LGPLv3 or later license.",
        "published": "2024-02-04T15:11:04Z",
        "link": "http://arxiv.org/abs/2402.02523v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA",
            "65N22, 65F08, 65F10"
        ]
    },
    {
        "title": "Optimistix: modular optimisation in JAX and Equinox",
        "authors": [
            "Jason Rader",
            "Terry Lyons",
            "Patrick Kidger"
        ],
        "summary": "We introduce Optimistix: a nonlinear optimisation library built in JAX and Equinox. Optimistix introduces a novel, modular approach for its minimisers and least-squares solvers. This modularity relies on new practical abstractions for optimisation which we call search and descent, and which generalise classical notions of line search, trust-region, and learning-rate algorithms. It provides high-level APIs and solvers for minimisation, nonlinear least-squares, root-finding, and fixed-point iteration. Optimistix is available at https://github.com/patrick-kidger/optimistix.",
        "published": "2024-02-15T14:49:18Z",
        "link": "http://arxiv.org/abs/2402.09983v1",
        "categories": [
            "math.OC",
            "cs.MS"
        ]
    },
    {
        "title": "BlackJAX: Composable Bayesian inference in JAX",
        "authors": [
            "Alberto Cabezas",
            "Adrien Corenflos",
            "Junpeng Lao",
            "Rémi Louf",
            "Antoine Carnec",
            "Kaustubh Chaudhari",
            "Reuben Cohn-Gordon",
            "Jeremie Coullon",
            "Wei Deng",
            "Sam Duffield",
            "Gerardo Durán-Martín",
            "Marcin Elantkowski",
            "Dan Foreman-Mackey",
            "Michele Gregori",
            "Carlos Iguaran",
            "Ravin Kumar",
            "Martin Lysy",
            "Kevin Murphy",
            "Juan Camilo Orduz",
            "Karm Patel",
            "Xi Wang",
            "Rob Zinkov"
        ],
        "summary": "BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation. It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use. It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.",
        "published": "2024-02-16T16:21:02Z",
        "link": "http://arxiv.org/abs/2402.10797v2",
        "categories": [
            "cs.MS",
            "cs.LG",
            "stat.CO",
            "stat.ML"
        ]
    },
    {
        "title": "Recent Extensions of the ZKCM Library for Parallel and Accurate MPS   Simulation of Quantum Circuits",
        "authors": [
            "Akira SaiToh"
        ],
        "summary": "A C++ library ZKCM and its extension library ZKCM_QC have been developed since 2011 for multiple-precision matrix computation and accurate matrix-product-state (MPS) quantum circuit simulation, respectively. In this report, a recent progress in the extensions of these libraries is described, which are mainly for parallel processing with the OpenMP and CUDA frameworks.",
        "published": "2024-02-19T06:24:25Z",
        "link": "http://arxiv.org/abs/2402.11868v1",
        "categories": [
            "physics.comp-ph",
            "cs.MS",
            "quant-ph",
            "97N80, 81-04",
            "G.4"
        ]
    },
    {
        "title": "Democratizing Uncertainty Quantification",
        "authors": [
            "Linus Seelinger",
            "Anne Reinarz",
            "Mikkel B. Lykkegaard",
            "Robert Akers",
            "Amal M. A. Alghamdi",
            "David Aristoff",
            "Wolfgang Bangerth",
            "Jean Bénézech",
            "Matteo Diez",
            "Kurt Frey",
            "John D. Jakeman",
            "Jakob S. Jørgensen",
            "Ki-Tae Kim",
            "Benjamin M. Kent",
            "Massimiliano Martinelli",
            "Matthew Parno",
            "Riccardo Pellegrini",
            "Noemi Petra",
            "Nicolai A. B. Riis",
            "Katherine Rosenfeld",
            "Andrea Serani",
            "Lorenzo Tamellini",
            "Umberto Villa",
            "Tim J. Dodwell",
            "Robert Scheichl"
        ],
        "summary": "Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale.   In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge. These benchmarks support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.",
        "published": "2024-02-21T12:43:41Z",
        "link": "http://arxiv.org/abs/2402.13768v5",
        "categories": [
            "cs.MS",
            "stat.AP"
        ]
    },
    {
        "title": "UMAT4COMSOL: An Abaqus user material (UMAT) subroutine wrapper for   COMSOL",
        "authors": [
            "S. Lucarini",
            "E. Martínez-Pañeda"
        ],
        "summary": "We present a wrapper that allows Abaqus user material subroutines (UMATs) to be used as an External Material library in the software COMSOL Multiphysics. The wrapper, written in C language, transforms COMSOL's external material subroutine inputs and outputs into Fortran-coded Abaqus UMAT inputs and outputs, by means of a consistent variable transformation. This significantly facilitates conducting coupled, multi-physics studies employing the advanced material models that the solid mechanics community has developed over the past decades. We exemplify the potential of our new framework, UMAT4COMSOL, by conducting numerical experiments in the areas of elastoplasticity, hyperelasticity and crystal plasticity. The source code, detailed documentation and example tutorials are made freely available to download at www.empaneda.com/codes.",
        "published": "2024-02-21T16:45:02Z",
        "link": "http://arxiv.org/abs/2402.13925v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "cs.SE"
        ]
    },
    {
        "title": "High-performance finite elements with MFEM",
        "authors": [
            "Julian Andrej",
            "Nabil Atallah",
            "Jan-Phillip Bäcker",
            "John Camier",
            "Dylan Copeland",
            "Veselin Dobrev",
            "Yohann Dudouit",
            "Tobias Duswald",
            "Brendan Keith",
            "Dohyun Kim",
            "Tzanio Kolev",
            "Boyan Lazarov",
            "Ketan Mittal",
            "Will Pazner",
            "Socratis Petrides",
            "Syun'ichi Shiraiwa",
            "Mark Stowell",
            "Vladimir Tomov"
        ],
        "summary": "The MFEM (Modular Finite Element Methods) library is a high-performance C++ library for finite element discretizations. MFEM supports numerous types of finite element methods and is the discretization engine powering many computational physics and engineering applications across a number of domains. This paper describes some of the recent research and development in MFEM, focusing on performance portability across leadership-class supercomputing facilities, including exascale supercomputers, as well as new capabilities and functionality, enabling a wider range of applications. Much of this work was undertaken as part of the Department of Energy's Exascale Computing Project (ECP) in collaboration with the Center for Efficient Exascale Discretizations (CEED).",
        "published": "2024-02-25T00:01:00Z",
        "link": "http://arxiv.org/abs/2402.15940v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "Image-To-Mesh Conversion for Biomedical Simulations",
        "authors": [
            "Fotis Drakopoulos",
            "Kevin Garner",
            "Christopher Rector",
            "Nikos Chrisochoides"
        ],
        "summary": "Converting a three-dimensional medical image into a 3D mesh that satisfies both the quality and fidelity constraints of predictive simulations and image-guided surgical procedures remains a critical problem. Presented is an image-to-mesh conversion method called CBC3D. It first discretizes a segmented image by generating an adaptive Body-Centered Cubic (BCC) mesh of high-quality elements. Next, the tetrahedral mesh is converted into a mixed-element mesh of tetrahedra, pentahedra, and hexahedra to decrease element count while maintaining quality. Finally, the mesh surfaces are deformed to their corresponding physical image boundaries, improving the mesh's fidelity. The deformation scheme builds upon the ITK open-source library and is based on the concept of energy minimization, relying on a multi-material point-based registration. It uses non-connectivity patterns to implicitly control the number of extracted feature points needed for the registration and, thus, adjusts the trade-off between the achieved mesh fidelity and the deformation speed. We compare CBC3D with four widely used and state-of-the-art homegrown image-to-mesh conversion methods from industry and academia. Results indicate that the CBC3D meshes (i) achieve high fidelity, (ii) keep the element count reasonably low, and (iii) exhibit good element quality.",
        "published": "2024-02-27T04:43:35Z",
        "link": "http://arxiv.org/abs/2402.18596v1",
        "categories": [
            "cs.GR",
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "A Constraint-based Mathematical Modeling Library in Prolog with Answer   Constraint Semantics",
        "authors": [
            "François Fages"
        ],
        "summary": "Constraint logic programming emerged in the late 80's as a highly declarative class of programming languages based on first-order logic and theories with decidable constraint languages, thereby subsuming Prolog restricted to equality constraints over the Herbrand's term domain. This approach has proven extremely successfull in solving combinatorial problems in the industry which quickly led to the development of a variety of constraint solving libraries in standard programming languages. Later came the design of a purely declarative front-end constraint-based modeling language, MiniZinc, independent of the constraint solvers, in order to compare their performances and create model benchmarks. Beyond that purpose, the use of a high-level modeling language such as MiniZinc to develop complete applications, or to teach constraint programming, is limited by the impossibility to program search strategies, or new constraint solvers, in a modeling language, as well as by the absence of an integrated development environment for both levels of constraint-based modeling and constraint solving. In this paper, we propose to solve those issues by taking Prolog with its constraint solving libraries, as a unified relation-based modeling and programming language. We present a Prolog library for high-level constraint-based mathematical modeling, inspired by MiniZinc, using subscripted variables (arrays) in addition to lists and terms, quantifiers and iterators in addition to recursion, together with a patch of constraint libraries in order to allow array functional notations in constraints. We show that this approach does not come with a significant computation time overhead, and presents several advantages in terms of the possibility of focussing on mathematical modeling, getting answer constraints in addition to ground solutions, programming search or constraint solvers if needed, and debugging models within a unique modeling and programming environment.",
        "published": "2024-02-27T07:57:40Z",
        "link": "http://arxiv.org/abs/2402.17286v1",
        "categories": [
            "cs.LO",
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Analytic continuations and numerical evaluation of the Appell $F_1$,   $F_3$, Lauricella $F_D^{(3)}$ and Lauricella-Saran $F_S^{(3)}$ and their   Application to Feynman Integrals",
        "authors": [
            "Souvik Bera",
            "Tanay Pathak"
        ],
        "summary": "We present our investigation of the study of two variable hypergeometric series, namely Appell $F_{1}$ and $F_{3}$ series, and obtain a comprehensive list of its analytic continuations enough to cover the whole real $(x,y)$ plane, except on their singular loci. We also derive analytic continuations of their 3-variable generalization, the Lauricella $F_{D}^{(3)}$ series and the Lauricella-Saran $F_{S}^{(3)}$ series, leveraging the analytic continuations of $F_{1}$ and $F_{3}$, which ensures that the whole real $(x,y,z)$ space is covered, except on the singular loci of these functions. While these studies are motivated by the frequent occurrence of these multivariable hypergeometric functions in Feynman integral evaluation, they can also be used whenever they appear in other branches of mathematical physics. To facilitate their practical use, we provide four packages: $\\texttt{AppellF1.wl}$, $\\texttt{AppellF3.wl}$, $\\texttt{LauricellaFD.wl}$, and $\\texttt{LauricellaSaranFS.wl}$ in $\\textit{MATHEMATICA}$. These packages are applicable for generic as well as non-generic values of parameters, keeping in mind their utilities in the evaluation of the Feynman integrals. We explicitly present various physical applications of these packages in the context of Feynman integral evaluation and compare the results using other means as well. Various $\\textit{MATHEMATICA}$ notebooks demonstrating different numerical results are also provided along with this paper.",
        "published": "2024-03-04T17:29:25Z",
        "link": "http://arxiv.org/abs/2403.02237v1",
        "categories": [
            "hep-ph",
            "cs.MS",
            "hep-th",
            "math-ph",
            "math.MP"
        ]
    },
    {
        "title": "GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise",
        "authors": [
            "Xiang Qu",
            "Hui Zhao",
            "Wenjie Cai",
            "Gongyi Wang",
            "Zihan Huang"
        ],
        "summary": "Mittag-Leffler correlated noise (M-L noise) plays a crucial role in the dynamics of complex systems, yet the scientific community has lacked tools for its direct generation. Addressing this gap, our work introduces GenML, a Python library specifically designed for generating M-L noise. We detail the architecture and functionalities of GenML and its underlying algorithmic approach, which enables the precise simulation of M-L noise. The effectiveness of GenML is validated through quantitative analyses of autocorrelation functions and diffusion behaviors, showcasing its capability to accurately replicate theoretical noise properties. Our contribution with GenML enables the effective application of M-L noise data in numerical simulation and data-driven methods for describing complex systems, moving beyond mere theoretical modeling.",
        "published": "2024-03-07T07:13:12Z",
        "link": "http://arxiv.org/abs/2403.04273v2",
        "categories": [
            "cs.MS",
            "cond-mat.stat-mech",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Efficient Calculations for Inverse of $k$-diagonal Circulant Matrices   and Cyclic Banded Matrices",
        "authors": [
            "Chen Wang",
            "Hailong Yu",
            "Chao Wang"
        ],
        "summary": "$k$-diagonal circulant matrices and cyclic banded matrices are widely used in numerical simulations and signal processing of circular linear systems. Algorithms that directly involve or specify linear or quadratic complexity for the inverses of these two types of matrices are rare. We find that the inverse of a $k$-diagonal circulant matrix can be uniquely determined by a recursive formula, which can be derived within $O(k^3 \\log n+k^4)$. Similarly for the inverse of a cyclic banded matrix, its inverse can be uniquely determined by a series of recursive formulas, with the initial terms of these recursions computable within $O(k^3 n+k^5)$. The additional costs for solving the complete inverses of these two types of matrices are $kn$ and $kn^2$. Our calculations enable rapid representation with most processes defined by explicit formulas. Additionally, most algorithms for inverting $k$-diagonal circulant matrices rely on the Fast Fourier Transform, which is not applicable to finite fields, while our algorithms can be applied to computations in finite fields.",
        "published": "2024-03-08T04:50:04Z",
        "link": "http://arxiv.org/abs/2403.05048v2",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial   Surrogates",
        "authors": [
            "Janina Schreiber",
            "Pau Batlle",
            "Damar Wicaksono",
            "Michael Hecht"
        ],
        "summary": "We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions.   The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among surrogate-based optimization methods when addressing low-dimensional optimization problems. Hereby, the simple nature of polynomials opens the opportunity for interpretation and analysis of the inferred surrogate model, providing a macroscopic perspective on the landscape of the objective function.",
        "published": "2024-03-12T10:21:21Z",
        "link": "http://arxiv.org/abs/2403.07485v1",
        "categories": [
            "math.OC",
            "cs.LG",
            "cs.MS"
        ]
    },
    {
        "title": "Binomial sums and Mellin asymptotics with explicit error bounds: a case   study",
        "authors": [
            "Benjamin Hackl",
            "Stephan Wagner"
        ],
        "summary": "Making use of a newly developed package in the computer algebra system SageMath, we show how to perform a full asymptotic analysis by means of the Mellin transform with explicit error bounds. As an application of the method, we answer a question of B\\'ona and DeJonge on 132-avoiding permutations with a unique longest increasing subsequence that can be translated into an inequality for a certain binomial sum.",
        "published": "2024-03-14T14:01:19Z",
        "link": "http://arxiv.org/abs/2403.09408v2",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.MS",
            "05A15, 05A16, 05-04",
            "G.2.1; G.4"
        ]
    },
    {
        "title": "GeoFlood: Computational model for overland flooding",
        "authors": [
            "Brian Kyanjo",
            "Donna Calhoun",
            "David L. George"
        ],
        "summary": "This paper presents GeoFlood, a new open-source software package for solving shallow water equations (SWE) on a quadtree hierarchy of mapped, logically Cartesian grids managed by the parallel, adaptive library ForestClaw (Calhoun and Burstedde, 2017). The GeoFlood model is validated using standard benchmark tests from Neelz and Pender (2013) and against George (2011) results obtained from the GeoClaw software (Clawpack Development Team, 2020) for the historical Malpasset dam failure problem. The benchmark test results are compared against GeoClaw and software package HEC-RAS (Hydraulic Engineering Center - River Analysis System, Army Corp of Engineers) results (Brunner, 2018). This comparison demonstrates the capability of GeoFlood to accurately and efficiently predict flood wave propagation on complex terrain. The results from comparisons with the Malpasset dam break show good agreement with the GeoClaw results and are consistent with the historical records of the event.",
        "published": "2024-03-15T18:48:13Z",
        "link": "http://arxiv.org/abs/2403.15435v2",
        "categories": [
            "physics.geo-ph",
            "cs.MS"
        ]
    },
    {
        "title": "On a vectorized basic linear algebra package for prototyping codes in   MATLAB",
        "authors": [
            "Alexej Moskovka",
            "Talal Rahman",
            "Jan Valdman",
            "Jon Eivind Vatne"
        ],
        "summary": "When writing high-performance code for numerical computation in a scripting language like MATLAB, it is crucial to have the operations in a large for-loop vectorized. If not, the code becomes too slow to use, even for a moderately large problem. However, in the process of vectorizing, the code often loses its original structure and becomes less readable. This is particularly true in the case of a finite element implementation, even though finite element methods are inherently structured. A basic remedy to this is the separation of the vectorization part from the mathematics part of the code, which is easily achieved through building the code on top of the basic linear algebra subprograms that are already vectorized codes, an idea that has been used in a series of papers over the last fifteen years, developing codes that are fast and still structured and readable. We discuss the vectorized basic linear algebra package and introduce a formalism using multi-linear algebra to explain and define formally the functions in the package, as well as MATLAB pagetime functions. We provide examples from computations of varying complexity, including the computation of normal vectors, volumes, and finite element methods. Benchmarking shows that we also get fast computations. Using the library, we can write codes that closely follow our mathematical thinking, making writing, following, reusing, and extending the code easier.",
        "published": "2024-03-15T19:57:37Z",
        "link": "http://arxiv.org/abs/2404.16039v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "A Comprehensive Review of Latent Space Dynamics Identification   Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling",
        "authors": [
            "Christophe Bonneville",
            "Xiaolong He",
            "April Tran",
            "Jun Sur Park",
            "William Fries",
            "Daniel A. Messenger",
            "Siu Wun Cheung",
            "Yeonjong Shin",
            "David M. Bortz",
            "Debojyoti Ghosh",
            "Jiun-Shyan Chen",
            "Jonathan Belof",
            "Youngsoo Choi"
        ],
        "summary": "Numerical solvers of partial differential equations (PDEs) have been widely employed for simulating physical systems. However, the computational cost remains a major bottleneck in various scientific and engineering applications, which has motivated the development of reduced-order models (ROMs). Recently, machine-learning-based ROMs have gained significant popularity and are promising for addressing some limitations of traditional ROM methods, especially for advection dominated systems. In this chapter, we focus on a particular framework known as Latent Space Dynamics Identification (LaSDI), which transforms the high-fidelity data, governed by a PDE, to simpler and low-dimensional latent-space data, governed by ordinary differential equations (ODEs). These ODEs can be learned and subsequently interpolated to make ROM predictions. Each building block of LaSDI can be easily modulated depending on the application, which makes the LaSDI framework highly flexible. In particular, we present strategies to enforce the laws of thermodynamics into LaSDI models (tLaSDI), enhance robustness in the presence of noise through the weak form (WLaSDI), select high-fidelity training data efficiently through active learning (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty through Gaussian processes (GPLaSDI). We demonstrate the performance of different LaSDI approaches on Burgers equation, a non-linear heat conduction problem, and a plasma physics problem, showing that LaSDI algorithms can achieve relative errors of less than a few percent and up to thousands of times speed-ups.",
        "published": "2024-03-16T00:45:06Z",
        "link": "http://arxiv.org/abs/2403.10748v1",
        "categories": [
            "cs.CE",
            "cs.LG",
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "On the Average Runtime of an Open Source Binomial Random Variate   Generation Algorithm",
        "authors": [
            "Vincent A. Cicirello"
        ],
        "summary": "The BTPE algorithm (Binomial, Triangle, Parallelogram, Exponential) of Kachitvichyanukul and Schmeiser is one of the faster and more widely utilized algorithms for generating binomial random variates. Cicirello's open source Java library, $\\rho\\mu$, includes an implementation of BTPE as well as a variety of other random number related utilities. In this report, I explore the average case runtime of the BTPE algorithm when generating random values from binomial distribution $B(n,p)$. Beginning with Kachitvichyanukul and Schmeiser's formula for the expected number of acceptance-rejection sampling iterations, I analyze the limit behavior as $n$ approaches infinity, and show that the average runtime of BTPE converges to a constant. I instrument the open source Java implementation from the $\\rho\\mu$ library to experimentally validate the analysis.",
        "published": "2024-03-16T21:10:22Z",
        "link": "http://arxiv.org/abs/2403.11018v1",
        "categories": [
            "cs.DS",
            "cs.MS",
            "68Q25, 68Q87, 68W40, 60-04",
            "F.2.1; G.3; G.4; I.1.2"
        ]
    },
    {
        "title": "A Lie Group Approach to Riemannian Batch Normalization",
        "authors": [
            "Ziheng Chen",
            "Yue Song",
            "Yunmei Liu",
            "Nicu Sebe"
        ],
        "summary": "Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization layers induced by these Lie groups are then proposed for SPD neural networks. We demonstrate the effectiveness of our approach through three sets of experiments: radar recognition, human action recognition, and electroencephalography (EEG) classification. The code is available at https://github.com/GitZH-Chen/LieBN.git.",
        "published": "2024-03-17T16:24:07Z",
        "link": "http://arxiv.org/abs/2403.11261v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MS"
        ]
    },
    {
        "title": "PETScML: Second-order solvers for training regression problems in   Scientific Machine Learning",
        "authors": [
            "Stefano Zampini",
            "Umberto Zerbinati",
            "George Turkiyyah",
            "David Keyes"
        ],
        "summary": "In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications. At the core of these methods is the supervised training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods. However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization. We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization. We empirically demonstrate the superior efficacy of a trust region method based on the Gauss-Newton approximation of the Hessian in improving the generalization errors arising from regression tasks when learning surrogate models for a wide range of scientific machine-learning techniques and test cases. All the conventional second-order solvers tested, including L-BFGS and inexact Newton with line-search, compare favorably, either in terms of cost or accuracy, with the adaptive first-order methods used to validate the surrogate models.",
        "published": "2024-03-18T18:59:42Z",
        "link": "http://arxiv.org/abs/2403.12188v1",
        "categories": [
            "cs.LG",
            "cs.MS",
            "math.OC",
            "65K10, 68T07, 65M70, 65Y05",
            "I.2.5; D.2.m; G.4; G.1.6; J.2"
        ]
    },
    {
        "title": "Extrapolating Solution Paths of Polynomial Homotopies towards   Singularities with PHCpack and phcpy",
        "authors": [
            "Jan Verschelde",
            "Kylash Viswanathan"
        ],
        "summary": "PHCpack is a software package for polynomial homotopy continuation, which provides a robust path tracker [Telen, Van Barel, Verschelde, SISC 2020]. This tracker computes the radius of convergence of Newton's method, estimates the distance to the nearest path, and then applies Pad\\'{e} approximants to predict the next point on the path. A priori step size control is less sensitive to finely tuned tolerances than a posteriori step size control, and is therefore robust. The Python interface phcpy is extended with a new step-by-step tracker and is applied to experiment with extrapolation methods to accurately locate the singular points at the end of solution paths.",
        "published": "2024-03-21T21:28:47Z",
        "link": "http://arxiv.org/abs/2403.14844v2",
        "categories": [
            "cs.MS",
            "cs.NA",
            "cs.SC",
            "math.CV",
            "math.NA"
        ]
    },
    {
        "title": "FlowFPX: Nimble Tools for Debugging Floating-Point Exceptions",
        "authors": [
            "Taylor Allred",
            "Xinyi Li",
            "Ashton Wiersdorf",
            "Ben Greenman",
            "Ganesh Gopalakrishnan"
        ],
        "summary": "Reliable numerical computations are central to scientific computing, but the floating-point arithmetic that enables large-scale models is error-prone. Numeric exceptions are a common occurrence and can propagate through code, leading to flawed results. This paper presents FlowFPX, a toolkit for systematically debugging floating-point exceptions by recording their flow, coalescing exception contexts, and fuzzing in select locations. These tools help scientists discover when exceptions happen and track down their origin, smoothing the way to a reliable codebase.",
        "published": "2024-03-22T22:02:36Z",
        "link": "http://arxiv.org/abs/2403.15632v1",
        "categories": [
            "cs.PL",
            "cs.MS"
        ]
    },
    {
        "title": "Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for   Computations in Matlab",
        "authors": [
            "Manel Velasco",
            "Isiah Zaplana",
            "Arnau Dória-Cerezo",
            "Pau Martí"
        ],
        "summary": "Geometric algebra (GA) is a mathematical tool for geometric computing, providing a framework that allows a unified and compact approach to geometric relations which in other mathematical systems are typically described using different more complicated elements. This fact has led to an increasing adoption of GA in applied mathematics and engineering problems. However, the scarcity of symbolic implementations of GA and its inherent complexity, requiring a specific mathematical background, make it challenging and less intuitive for engineers to work with. This prevents wider adoption among more applied professionals. To address this challenge, this paper introduces SUGAR (Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox designed for Matlab and licensed under the MIT License. SUGAR facilitates the translation of GA concepts into Matlab and provides a collection of user-friendly functions tailored for GA computations, including support for symbolic operations. It supports both numeric and symbolic computations in high-dimensional GAs. Specifically tailored for applied mathematics and engineering applications, SUGAR has been meticulously engineered to represent geometric elements and transformations within two and three-dimensional projective and conformal geometric algebras, aligning with established computational methodologies in the literature. Furthermore, SUGAR efficiently handles functions of multivectors, such as exponential, logarithmic, sinusoidal, and cosine functions, enhancing its applicability across various engineering domains, including robotics, control systems, and power electronics. Finally, this work includes four distinct validation examples, demonstrating SUGAR's capabilities across the above-mentioned fields and its practical utility in addressing real-world applied mathematics and engineering problems.",
        "published": "2024-03-25T11:22:38Z",
        "link": "http://arxiv.org/abs/2403.16634v1",
        "categories": [
            "cs.MS",
            "cs.RO",
            "cs.SY",
            "eess.SY",
            "G.4"
        ]
    },
    {
        "title": "EinExprs: Contraction Paths of Tensor Networks as Symbolic Expressions",
        "authors": [
            "Sergio Sanchez-Ramirez",
            "Jofre Vallès-Muns",
            "Artur Garcia-Saez"
        ],
        "summary": "Tensor Networks are graph representations of summation expressions in which vertices represent tensors and edges represent tensor indices or vector spaces. In this work, we present EinExprs.jl, a Julia package for contraction path optimization that offers state-of-art optimizers. We propose a representation of the contraction path of a Tensor Network based on symbolic expressions. Using this package the user may choose among a collection of different methods such as Greedy algorithms, or an approach based on the hypergraph partitioning problem. We benchmark this library with examples obtained from the simulation of Random Quantum Circuits (RQC), a well known example where Tensor Networks provide state-of-the-art methods.",
        "published": "2024-03-26T18:38:00Z",
        "link": "http://arxiv.org/abs/2403.18030v1",
        "categories": [
            "quant-ph",
            "cs.MS",
            "81-04",
            "G.4; J.2; I.1.1"
        ]
    },
    {
        "title": "Inexactness and Correction of Floating-Point Reciprocal, Division and   Square Root",
        "authors": [
            "Lucas M. Dutton",
            "Christopher Kumar Anand",
            "Robert Enenkel",
            "Silvia Melitta Müller"
        ],
        "summary": "Floating-point arithmetic performance determines the overall performance of important applications, from graphics to AI. Meeting the IEEE-754 specification for floating-point requires that final results of addition, subtraction, multiplication, division, and square root are correctly rounded based on the user-selected rounding mode. A frustrating fact for implementers is that naive rounding methods will not produce correctly rounded results even when intermediate results with greater accuracy and precision are available. In contrast, our novel algorithm can correct approximations of reciprocal, division and square root, even ones with slightly lower than target precision. In this paper, we present a family of algorithms that can both increase the accuracy (and potentially the precision) of an estimate and correctly round it according to all binary IEEE-754 rounding modes. We explain how it may be efficiently implemented in hardware, and for completeness, we present proofs that it is not necessary to include equality tests associated with round-to-nearest-even mode for reciprocal, division and square root functions, because it is impossible for input(s) in a given precision to have exact answers exactly midway between representable floating-point numbers in that precision. In fact, our simpler proofs are sometimes stronger.",
        "published": "2024-03-30T15:02:03Z",
        "link": "http://arxiv.org/abs/2404.00387v1",
        "categories": [
            "cs.MS",
            "cs.AR"
        ]
    },
    {
        "title": "A shared compilation stack for distributed-memory parallelism in stencil   DSLs",
        "authors": [
            "George Bisbas",
            "Anton Lydike",
            "Emilien Bauer",
            "Nick Brown",
            "Mathieu Fehr",
            "Lawrence Mitchell",
            "Gabriel Rodriguez-Canal",
            "Maurice Jamieson",
            "Paul H. J. Kelly",
            "Michel Steuwer",
            "Tobias Grosser"
        ],
        "summary": "Domain Specific Languages (DSLs) increase programmer productivity and provide high performance. Their targeted abstractions allow scientists to express problems at a high level, providing rich details that optimizing compilers can exploit to target current- and next-generation supercomputers. The convenience and performance of DSLs come with significant development and maintenance costs. The siloed design of DSL compilers and the resulting inability to benefit from shared infrastructure cause uncertainties around longevity and the adoption of DSLs at scale. By tailoring the broadly-adopted MLIR compiler framework to HPC, we bring the same synergies that the machine learning community already exploits across their DSLs (e.g. Tensorflow, PyTorch) to the finite-difference stencil HPC community. We introduce new HPC-specific abstractions for message passing targeting distributed stencil computations. We demonstrate the sharing of common components across three distinct HPC stencil-DSL compilers: Devito, PSyclone, and the Open Earth Compiler, showing that our framework generates high-performance executables based upon a shared compiler ecosystem.",
        "published": "2024-04-02T18:11:55Z",
        "link": "http://arxiv.org/abs/2404.02218v1",
        "categories": [
            "cs.DC",
            "cs.MS"
        ]
    },
    {
        "title": "Discrete Event Simulation: It's Easy with SimPy!",
        "authors": [
            "Dmitry Zinoviev"
        ],
        "summary": "This paper introduces the practicalities and benefits of using SimPy, a discrete event simulation (DES) module written in Python, for modeling and simulating complex systems. Through a step-by-step exploration of the classical Dining Philosophers Problem, we demonstrate how SimPy enables the efficient construction of discrete event models, emphasizing system states, transitions, and event handling. We extend the scenario to introduce resources, such as chopsticks, to model contention and deadlock conditions, and showcase SimPy's capabilities in managing these scenarios. Furthermore, we explore the integration of SimPy with other Python libraries for statistical analysis, showcasing how simulation results inform system design and optimization. The versatility of SimPy is further highlighted through additional modeling scenarios, including resource constraints and customer service interactions, providing insights into the process of building, debugging, simulating, and optimizing models for a wide range of applications. This paper aims to make DES accessible to practitioners and researchers alike, emphasizing the ease with which complex simulations can be constructed, analyzed, and visualized using SimPy and the broader Python ecosystem.",
        "published": "2024-04-03T06:03:09Z",
        "link": "http://arxiv.org/abs/2405.01562v1",
        "categories": [
            "cs.MS",
            "cs.MA"
        ]
    },
    {
        "title": "SARIS: Accelerating Stencil Computations on Energy-Efficient RISC-V   Compute Clusters with Indirect Stream Registers",
        "authors": [
            "Paul Scheffler",
            "Luca Colagrande",
            "Luca Benini"
        ],
        "summary": "Stencil codes are performance-critical in many compute-intensive applications, but suffer from significant address calculation and irregular memory access overheads. This work presents SARIS, a general and highly flexible methodology for stencil acceleration using register-mapped indirect streams. We demonstrate SARIS for various stencil codes on an eight-core RISC-V compute cluster with indirect stream registers, achieving significant speedups of 2.72x, near-ideal FPU utilizations of 81%, and energy efficiency improvements of 1.58x over an RV32G baseline on average. Scaling out to a 256-core manycore system, we estimate an average FPU utilization of 64%, an average speedup of 2.14x, and up to 15% higher fractions of peak compute than a leading GPU code generator.",
        "published": "2024-04-08T08:46:40Z",
        "link": "http://arxiv.org/abs/2404.05303v1",
        "categories": [
            "cs.MS",
            "cs.AR"
        ]
    },
    {
        "title": "Predefined Software Environment Runtimes As A Measure For   Reproducibility",
        "authors": [
            "Aaruni Kaushik"
        ],
        "summary": "As part of Mathematical Research Data Initiative (MaRDI), we have developed a way to preserve a software package into an easy to deploy and use sandbox environment we call a \"runtime\", via a program we developed called MaPS : MaRDI Packaging System. The program relies on Linux user namespaces to isolate a library environment from the host system, making the sandboxed software reproducible on other systems, with minimal effort. Moreover an overlay filesystem makes local edits persistent. This project will aid reproducibility efforts of research papers: both mathematical and from other disciplines. As a proof of concept, we provide runtimes for the OSCAR Computer Algebra System, polymake software for research in polyhedral geometry, and VIBRANT Virus Identification By iteRative ANnoTation. The software is in a prerelease state: the interface for creating, deploying, and executing runtimes is final, and an interface for easily publishing runtimes is under active development. We thus propose publishing predefined, distributable software environment runtimes along with research papers in an effort to make research with software based results reproducible.",
        "published": "2024-04-08T14:36:45Z",
        "link": "http://arxiv.org/abs/2404.05563v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Confirmable Workflows in OSCAR",
        "authors": [
            "Michael Joswig",
            "Lars Kastner",
            "Benjamin Lorenz"
        ],
        "summary": "We discuss what is special about the reproducibility of workflows in computer algebra. It is emphasized how the programming language Julia and the new computer algebra system OSCAR support such a reproducibility, and how users can benefit for their own work.",
        "published": "2024-04-09T12:08:24Z",
        "link": "http://arxiv.org/abs/2404.06241v1",
        "categories": [
            "cs.MS",
            "68N30"
        ]
    },
    {
        "title": "Raster Forge: Interactive Raster Manipulation Library and GUI for Python",
        "authors": [
            "Afonso Oliveira",
            "Nuno Fachada",
            "João P. Matos-Carvalho"
        ],
        "summary": "Raster Forge is a Python library and graphical user interface for raster data manipulation and analysis. The tool is focused on remote sensing applications, particularly in wildfire management. It allows users to import, visualize, and process raster layers for tasks such as image compositing or topographical analysis. For wildfire management, it generates fuel maps using predefined models. Its impact extends from disaster management to hydrological modeling, agriculture, and environmental monitoring. Raster Forge can be a valuable asset for geoscientists and researchers who rely on raster data analysis, enhancing geospatial data processing and visualization across various disciplines.",
        "published": "2024-04-09T15:31:48Z",
        "link": "http://arxiv.org/abs/2404.06389v2",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.CY",
            "cs.MS",
            "I.4; I.5; J.2; D.2; H.5.2"
        ]
    },
    {
        "title": "Massively Parallel Computation of Similarity Matrices from Piecewise   Constant Invariants",
        "authors": [
            "Björn H. Wehlin"
        ],
        "summary": "We present a computational framework for piecewise constant functions (PCFs) and use this for several types of computations that are useful in statistics, e.g., averages, similarity matrices, and so on. We give a linear-time, allocation-free algorithm for working with pairs of PCFs at machine precision. From this, we derive algorithms for computing reductions of several PCFs. The algorithms have been implemented in a highly scalable fashion for parallel execution on CPU and, in some cases, (multi-)GPU, and are provided in a \\proglang{Python} package. In addition, we provide support for multidimensional arrays of PCFs and vectorized operations on these. As a stress test, we have computed a distance matrix from 500,000 PCFs using 8 GPUs.",
        "published": "2024-04-10T17:35:36Z",
        "link": "http://arxiv.org/abs/2404.07183v1",
        "categories": [
            "stat.CO",
            "cs.MS",
            "math.AT",
            "62-04 (Primary) 62R40 (Secondary)",
            "G.3; G.4"
        ]
    },
    {
        "title": "sCWatter: Open source coupled wave scattering simulation for   spectroscopy and microscopy",
        "authors": [
            "Ruijiao Sun",
            "Rohith Reddy",
            "David Mayerich"
        ],
        "summary": "Several emerging microscopy imaging methods rely on complex interactions between the incident light and the sample. These include interferometry, spectroscopy, and nonlinear optics. Reconstructing a sample from the measured scattered field relies on fast and accurate optical models. Fast approaches like ray tracing and the Born approximation have limitations that are limited when working with high numerical apertures. This paper presents sCWatter, an open-source tool that utilizes coupled wave theory (CWT) to simulate and visualize the 3D electric field scattered by complex samples. The sample refractive index is specified on a volumetric grid, while the incident field is provided as a 2D image orthogonal to the optical path. We introduce connection equations between layers that significantly reduce the dimensionality of the CW linear system, enabling efficient parallel processing on consumer hardware. Further optimizations using Intel MKL and CUDA significantly accelerate both field simulation and visualization.",
        "published": "2024-04-10T18:40:58Z",
        "link": "http://arxiv.org/abs/2404.07293v1",
        "categories": [
            "physics.optics",
            "cs.MS"
        ]
    },
    {
        "title": "Algorithm xxx: Faster Randomized SVD with Dynamic Shifts",
        "authors": [
            "Xu Feng",
            "Wenjian Yu",
            "Yuyang Xie",
            "Jie Tang"
        ],
        "summary": "Aiming to provide a faster and convenient truncated SVD algorithm for large sparse matrices from real applications (i.e. for computing a few of largest singular values and the corresponding singular vectors), a dynamically shifted power iteration technique is applied to improve the accuracy of the randomized SVD method. This results in a dynamic shifts based randomized SVD (dashSVD) algorithm, which also collaborates with the skills for handling sparse matrices. An accuracy-control mechanism is included in the dashSVD algorithm to approximately monitor the per vector error bound of computed singular vectors with negligible overhead. Experiments on real-world data validate that the dashSVD algorithm largely improves the accuracy of randomized SVD algorithm or attains same accuracy with fewer passes over the matrix, and provides an efficient accuracy-control mechanism to the randomized SVD computation, while demonstrating the advantages on runtime and parallel efficiency. A bound of the approximation error of the randomized SVD with the shifted power iteration is also proved.",
        "published": "2024-04-14T14:58:02Z",
        "link": "http://arxiv.org/abs/2404.09276v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "Computing with Hypergeometric-Type Terms",
        "authors": [
            "Bertrand Teguia Tabuguia"
        ],
        "summary": "Take a multiplicative monoid of sequences in which the multiplication is given by Hadamard product. The set of linear combinations of interleaving monoid elements then yields a ring. For hypergeometric sequences, the resulting ring is a subring of the ring of holonomic sequences. We present two algorithms in this setting: one for computing holonomic recurrence equations from hypergeometric-type normal forms and the other for finding products of hypergeometric-type terms. These are newly implemented commands in our Maple package $HyperTypeSeq$, available at \\url{https://github.com/T3gu1a/HyperTypeSeq}, which we also describe.",
        "published": "2024-04-15T21:24:18Z",
        "link": "http://arxiv.org/abs/2404.10143v2",
        "categories": [
            "cs.SC",
            "cs.DM",
            "cs.MS",
            "33F10, 39A06 (Primary), 68R01, 05A19 (Secondary)",
            "I.1.1; I.1.2; G.2.1; G.4"
        ]
    },
    {
        "title": "GALÆXI: Solving complex compressible flows with high-order   discontinuous Galerkin methods on accelerator-based systems",
        "authors": [
            "Daniel Kempf",
            "Marius Kurz",
            "Marcel Blind",
            "Patrick Kopper",
            "Philipp Offenhäuser",
            "Anna Schwarz",
            "Spencer Starr",
            "Jens Keim",
            "Andrea Beck"
        ],
        "summary": "This work presents GALAEXI as a novel, energy-efficient flow solver for the simulation of compressible flows on unstructured meshes leveraging the parallel computing power of modern Graphics Processing Units (GPUs). GALAEXI implements the high-order Discontinuous Galerkin Spectral Element Method (DGSEM) using shock capturing with a finite-volume subcell approach to ensure the stability of the high-order scheme near shocks. This work provides details on the general code design, the parallelization strategy, and the implementation approach for the compute kernels with a focus on the element local mappings between volume and surface data due to the unstructured mesh. GALAEXI exhibits excellent strong scaling properties up to 1024 GPUs if each GPU is assigned a minimum of one million degrees of freedom degrees of freedom. To verify its implementation, a convergence study is performed that recovers the theoretical order of convergence of the implemented numerical schemes. Moreover, the solver is validated using both the incompressible and compressible formulation of the Taylor-Green-Vortex at a Mach number of 0.1 and 1.25, respectively. A mesh convergence study shows that the results converge to the high-fidelity reference solution and that the results match the original CPU implementation. Finally, GALAEXI is applied to a large-scale wall-resolved large eddy simulation of a linear cascade of the NASA Rotor 37. Here, the supersonic region and shocks at the leading edge are captured accurately and robustly by the implemented shock-capturing approach. It is demonstrated that GALAEXI requires less than half of the energy to carry out this simulation in comparison to the reference CPU implementation. This renders GALAEXI as a potent tool for accurate and efficient simulations of compressible flows in the realm of exascale computing and the associated new HPC architectures.",
        "published": "2024-04-19T08:21:05Z",
        "link": "http://arxiv.org/abs/2404.12703v2",
        "categories": [
            "cs.MS",
            "cs.CE"
        ]
    },
    {
        "title": "Conversion of Boolean and Integer FlatZinc Builtins to Quadratic or   Linear Integer Problems",
        "authors": [
            "Armin Wolf"
        ],
        "summary": "Constraint satisfaction or optimisation models -- even if they are formulated in high-level modelling languages -- need to be reduced into an equivalent format before they can be solved by the use of Quantum Computing. In this paper we show how Boolean and integer FlatZinc builtins over finite-domain integer variables can be equivalently reformulated as linear equations, linear inequalities or binary products of those variables, i.e. as finite-domain quadratic integer programs. Those quadratic integer programs can be further transformed into equivalent Quadratic Unconstrained Binary Optimisation problem models, i.e. a general format for optimisation problems to be solved on Quantum Computers especially on Quantum Annealers.",
        "published": "2024-04-19T11:24:03Z",
        "link": "http://arxiv.org/abs/2404.12797v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Robustness and Accuracy in Pipelined Bi-Conjugate Gradient Stabilized   Method: A Comparative Study",
        "authors": [
            "Mykhailo Havdiak",
            "Jose I. Aliaga",
            "Roman Iakymchuk"
        ],
        "summary": "In this article, we propose an accuracy-assuring technique for finding a solution for unsymmetric linear systems. Such problems are related to different areas such as image processing, computer vision, and computational fluid dynamics. Parallel implementation of Krylov subspace methods speeds up finding approximate solutions for linear systems. In this context, the refined approach in pipelined BiCGStab enhances scalability on distributed memory machines, yielding to substantial speed improvements compared to the standard BiCGStab method. However, it's worth noting that the pipelined BiCGStab algorithm sacrifices some accuracy, which is stabilized with the residual replacement technique. This paper aims to address this issue by employing the ExBLAS-based reproducible approach. We validate the idea on a set of matrices from the SuiteSparse Matrix Collection.",
        "published": "2024-04-19T23:49:04Z",
        "link": "http://arxiv.org/abs/2404.13216v1",
        "categories": [
            "cs.MS",
            "cs.DC"
        ]
    },
    {
        "title": "Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs   Tree LSTMs",
        "authors": [
            "Rashid Barket",
            "Matthew England",
            "Jürgen Gerhard"
        ],
        "summary": "Computer Algebra Systems (e.g. Maple) are used in research, education, and industrial settings. One of their key functionalities is symbolic integration, where there are many sub-algorithms to choose from that can affect the form of the output integral, and the runtime. Choosing the right sub-algorithm for a given problem is challenging: we hypothesise that Machine Learning can guide this sub-algorithm choice. A key consideration of our methodology is how to represent the mathematics to the ML model: we hypothesise that a representation which encodes the tree structure of mathematical expressions would be well suited. We trained both an LSTM and a TreeLSTM model for sub-algorithm prediction and compared them to Maple's existing approach. Our TreeLSTM performs much better than the LSTM, highlighting the benefit of using an informed representation of mathematical expressions. It is able to produce better outputs than Maple's current state-of-the-art meta-algorithm, giving a strong basis for further research.",
        "published": "2024-04-23T12:27:20Z",
        "link": "http://arxiv.org/abs/2404.14973v1",
        "categories": [
            "cs.LG",
            "cs.MS",
            "cs.SC"
        ]
    },
    {
        "title": "Finch: Sparse and Structured Array Programming with Control Flow",
        "authors": [
            "Willow Ahrens",
            "Teodoro Fields Collin",
            "Radha Patel",
            "Kyle Deeds",
            "Changwan Hong",
            "Saman Amarasinghe"
        ],
        "summary": "From FORTRAN to NumPy, arrays have revolutionized how we express computation. However, arrays in these, and almost all prominent systems, can only handle dense rectilinear integer grids. Real world arrays often contain underlying structure, such as sparsity, runs of repeated values, or symmetry. Support for structured data is fragmented and incomplete. Existing frameworks limit the array structures and program control flow they support to better simplify the problem.   In this work, we propose a new programming language, Finch, which supports both flexible control flow and diverse data structures. Finch facilitates a programming model which resolves the challenges of computing over structured arrays by combining control flow and data structures into a common representation where they can be co-optimized. Finch automatically specializes control flow to data so that performance engineers can focus on experimenting with many algorithms. Finch supports a familiar programming language of loops, statements, ifs, breaks, etc., over a wide variety of array structures, such as sparsity, run-length-encoding, symmetry, triangles, padding, or blocks. Finch reliably utilizes the key properties of structure, such as structural zeros, repeated values, or clustered non-zeros. We show that this leads to dramatic speedups in operations such as SpMV and SpGEMM, image processing, graph analytics, and a high-level tensor operator fusion interface.",
        "published": "2024-04-25T16:41:12Z",
        "link": "http://arxiv.org/abs/2404.16730v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Differentiating Through Linear Solvers",
        "authors": [
            "Paul Hovland",
            "Jan Hückelheim"
        ],
        "summary": "Computer programs containing calls to linear solvers are a known challenge for automatic differentiation. Previous publications advise against differentiating through the low-level solver implementation, and instead advocate for high-level approaches that express the derivative in terms of a modified linear system that can be solved with a separate solver call. Despite this ubiquitous advice, we are not aware of prior work comparing the accuracy of both approaches. With this article we thus empirically study a simple question: What happens if we ignore common wisdom, and differentiate through linear solvers?",
        "published": "2024-04-25T21:05:01Z",
        "link": "http://arxiv.org/abs/2404.17039v2",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "Xabclib:A Fully Auto-tuned Sparse Iterative Solver",
        "authors": [
            "Takahiro Katagiri",
            "Takao Sakurai",
            "Mitsuyoshi Igai",
            "Shoji Itoh",
            "Satoshi Ohshima",
            "Hisayasu Kuroda",
            "Ken Naono",
            "Kengo Nakajima"
        ],
        "summary": "In this paper, we propose a general application programming interface named OpenATLib for auto-tuning (AT). OpenATLib is designed to establish the reusability of AT functions. By using OpenATLib, we develop a fully auto-tuned sparse iterative solver named Xabclib. Xabclib has several novel run-time AT functions. First, the following new implementations of sparse matrix-vector multiplication (SpMV) for thread processing are implemented:(1) non-zero elements; (2) omission of zero-elements computation for vector reduction; (3) branchless segmented scan (BSS). According to the performance evaluation and the comparison with conventional implementations, the following results are obtained: (1) 14x speedup for non-zero elements and zero-elements computation omission for symmetric SpMV; (2) 4.62x speedup by using BSS. We also develop a \"numerical computation policy\" that can optimize memory space and computational accuracy. Using the policy, we obtain the following: (1) an averaged 1/45 memory space reduction; (2) avoidance of the \"fault convergence\" situation, which is a problem of conventional solvers.",
        "published": "2024-05-01T00:14:47Z",
        "link": "http://arxiv.org/abs/2405.01599v1",
        "categories": [
            "cs.MS",
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "A Communication Avoiding and Reducing Algorithm for Symmetric   Eigenproblem for Very Small Matrices",
        "authors": [
            "Takahiro Katagiri",
            "Jun'ichi Iwata",
            "Kazuyuki Uchida"
        ],
        "summary": "In this paper, a parallel symmetric eigensolver with very small matrices in massively parallel processing is considered. We define very small matrices that fit the sizes of caches per node in a supercomputer. We assume that the sizes also fit the exa-scale computing requirements of current production runs of an application. To minimize communication time, we added several communication avoiding and communication reducing algorithms based on Message Passing Interface (MPI) non-blocking implementations. A performance evaluation with up to full nodes of the FX10 system indicates that (1) the MPI non-blocking implementation is 3x as efficient as the baseline implementation, (2) the hybrid MPI execution is 1.9x faster than the pure MPI execution, (3) our proposed solver is 2.3x and 22x faster than a ScaLAPACK routine with optimized blocking size and cyclic-cyclic distribution, respectively.",
        "published": "2024-05-01T05:19:49Z",
        "link": "http://arxiv.org/abs/2405.00326v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "cs.PF"
        ]
    },
    {
        "title": "Minimization of Nonlinear Energies in Python Using FEM and Automatic   Differentiation Tools",
        "authors": [
            "Michal Béreš",
            "Jan Valdman"
        ],
        "summary": "This contribution examines the capabilities of the Python ecosystem to solve nonlinear energy minimization problems, with a particular focus on transitioning from traditional MATLAB methods to Python's advanced computational tools, such as automatic differentiation. We demonstrate Python's streamlined approach to minimizing nonlinear energies by analyzing three problem benchmarks - the p-Laplacian, the Ginzburg-Landau model, and the Neo-Hookean hyperelasticity. This approach merely requires the provision of the energy functional itself, making it a simple and efficient way to solve this category of problems. The results show that the implementation is about ten times faster than the MATLAB implementation for large-scale problems. Our findings highlight Python's efficiency and ease of use in scientific computing, establishing it as a preferable choice for implementing sophisticated mathematical models and accelerating the development of numerical simulations.",
        "published": "2024-05-03T12:51:59Z",
        "link": "http://arxiv.org/abs/2407.04706v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Performance of H-Matrix-Vector Multiplication with Floating Point   Compression",
        "authors": [
            "Ronald Kriemann"
        ],
        "summary": "Matrix-vector multiplication forms the basis of many iterative solution algorithms and as such is an important algorithm also for hierarchical matrices. However, due to its low computational intensity, its performance is typically limited by the available memory bandwidth. By optimizing the storage representation of the data within such matrices, this limitation can be lifted and the performance increased. This applies not only to hierarchical matrices but for also for other low-rank approximation schemes, e.g. block low-rank matrices.",
        "published": "2024-05-06T13:29:14Z",
        "link": "http://arxiv.org/abs/2405.03456v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "65Y05, 65Y20, 68W10, 68W25, 68P30"
        ]
    },
    {
        "title": "An efficient active-set method with applications to sparse   approximations and risk minimization",
        "authors": [
            "Spyridon Pougkakiotis",
            "Jacek Gondzio",
            "Dionysis Kalogerias"
        ],
        "summary": "In this paper we present an efficient active-set method for the solution of convex quadratic programming problems with general piecewise-linear terms in the objective, with applications to sparse approximations and risk-minimization. The algorithm is derived by combining a proximal method of multipliers (PMM) with a standard semismooth Newton method (SSN), and is shown to be globally convergent under minimal assumptions. Further local linear (and potentially superlinear) convergence is shown under standard additional conditions. The major computational bottleneck of the proposed approach arises from the solution of the associated SSN linear systems. These are solved using a Krylov-subspace method, accelerated by certain novel general-purpose preconditioners which are shown to be optimal with respect to the proximal penalty parameters. The preconditioners are easy to store and invert, since they exploit the structure of the nonsmooth terms appearing in the problem's objective to significantly reduce their memory requirements. We showcase the efficiency, robustness, and scalability of the proposed solver on a variety of problems arising in risk-averse portfolio selection, $L^1$-regularized partial differential equation constrained optimization, quantile regression, and binary classification via linear support vector machines. We provide computational evidence, on real-world datasets, to demonstrate the ability of the solver to efficiently and competitively handle a diverse set of medium- and large-scale optimization instances.",
        "published": "2024-05-07T10:14:33Z",
        "link": "http://arxiv.org/abs/2405.04172v1",
        "categories": [
            "math.OC",
            "cs.MS"
        ]
    },
    {
        "title": "Optimizing Tensor Contraction Paths: A Greedy Algorithm Approach With   Improved Cost Functions",
        "authors": [
            "Sheela Orgler",
            "Mark Blacher"
        ],
        "summary": "Finding efficient tensor contraction paths is essential for a wide range of problems, including model counting, quantum circuits, graph problems, and language models. There exist several approaches to find efficient paths, such as the greedy and random greedy algorithm by Optimized Einsum (opt_einsum), and the greedy algorithm and hypergraph partitioning approach employed in cotengra. However, these algorithms require a lot of computational time and resources to find efficient contraction paths. In this paper, we introduce a novel approach based on the greedy algorithm by opt_einsum that computes efficient contraction paths in less time. Moreover, with our approach, we are even able to compute paths for large problems where modern algorithms fail.",
        "published": "2024-05-08T09:25:39Z",
        "link": "http://arxiv.org/abs/2405.09644v1",
        "categories": [
            "quant-ph",
            "cs.DM",
            "cs.MS"
        ]
    },
    {
        "title": "A Sparse Tensor Generator with Efficient Feature Extraction",
        "authors": [
            "Tugba Torun",
            "Eren Yenigul",
            "Ameer Taweel",
            "Didem Unat"
        ],
        "summary": "Sparse tensor operations are gaining attention in emerging applications such as social networks, deep learning, diagnosis, crime, and review analysis. However, a major obstacle for research in sparse tensor operations is the deficiency of a broad-scale sparse tensor dataset. Another challenge in sparse tensor operations is examining the sparse tensor features, which are not only important for revealing its nonzero pattern but also have a significant impact on determining the best-suited storage format, the decomposition algorithm, and the reordering methods. However, due to the large sizes of real tensors, even extracting these features becomes costly without caution. To address these gaps in the literature, we have developed a smart sparse tensor generator that mimics the substantial features of real sparse tensors. Moreover, we propose various methods for efficiently extracting an extensive set of features for sparse tensors. The effectiveness of our generator is validated through the quality of features and the performance of decomposition in the generated tensors. Both the sparse tensor feature extractor and the tensor generator are open source with all the artifacts available at https://github.com/sparcityeu/feaTen and https://github.com/sparcityeu/genTen, respectively.",
        "published": "2024-05-08T10:28:20Z",
        "link": "http://arxiv.org/abs/2405.04944v1",
        "categories": [
            "cs.MS",
            "cs.LG",
            "G.4"
        ]
    },
    {
        "title": "Experience and Analysis of Scalable High-Fidelity Computational Fluid   Dynamics on Modular Supercomputing Architectures",
        "authors": [
            "Martin Karp",
            "Estela Suarez",
            "Jan H. Meinke",
            "Måns I. Andersson",
            "Philipp Schlatter",
            "Stefano Markidis",
            "Niclas Jansson"
        ],
        "summary": "The never-ending computational demand from simulations of turbulence makes computational fluid dynamics (CFD) a prime application use case for current and future exascale systems. High-order finite element methods, such as the spectral element method, have been gaining traction as they offer high performance on both multicore CPUs and modern GPU-based accelerators. In this work, we assess how high-fidelity CFD using the spectral element method can exploit the modular supercomputing architecture at scale through domain partitioning, where the computational domain is split between a Booster module powered by GPUs and a Cluster module with conventional CPU nodes. We investigate several different flow cases and computer systems based on the modular supercomputing architecture (MSA). We observe that for our simulations, the communication overhead and load balancing issues incurred by incorporating different computing architectures are seldom worthwhile, especially when I/O is also considered, but when the simulation at hand requires more than the combined global memory on the GPUs, utilizing additional CPUs to increase the available memory can be fruitful. We support our results with a simple performance model to assess when running across modules might be beneficial. As MSA is becoming more widespread and efforts to increase system utilization are growing more important our results give insight into when and how a monolithic application can utilize and spread out to more than one module and obtain a faster time to solution.",
        "published": "2024-05-09T09:28:43Z",
        "link": "http://arxiv.org/abs/2405.05640v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "physics.flu-dyn",
            "J.2; C.1.4; G.4"
        ]
    },
    {
        "title": "Hybrid parallel discrete adjoints in SU2",
        "authors": [
            "Johannes Blühdorn",
            "Pedro Gomes",
            "Max Aehle",
            "Nicolas R. Gauger"
        ],
        "summary": "The open-source multiphysics suite SU2 features discrete adjoints by means of operator overloading automatic differentiation (AD). While both primal and discrete adjoint solvers support MPI parallelism, hybrid parallelism using both MPI and OpenMP has only been introduced for the primal solvers so far. In this work, we enable hybrid parallel discrete adjoint solvers. Coupling SU2 with OpDiLib, an add-on for operator overloading AD tools that extends AD to OpenMP parallelism, marks a key step in this endeavour. We identify the affected parts of SU2's advanced AD workflow and discuss the required changes and their tradeoffs. Detailed performance studies compare MPI parallel and hybrid parallel discrete adjoints in terms of memory and runtime and unveil key performance characteristics. We showcase the effectiveness of performance optimizations and highlight perspectives for future improvements. At the same time, this study demonstrates the applicability of OpDiLib in a large code base and its scalability on large test cases, providing valuable insights for future applications both within and beyond SU2.",
        "published": "2024-05-09T19:00:48Z",
        "link": "http://arxiv.org/abs/2405.06056v2",
        "categories": [
            "cs.MS",
            "D.1.3; G.1.4; G.4; J.2"
        ]
    },
    {
        "title": "Adaptation of XAI to Auto-tuning for Numerical Libraries",
        "authors": [
            "Shota Aoki",
            "Takahiro Katagiri",
            "Satoshi Ohshima",
            "Masatoshi Kawai",
            "Toru Nagai",
            "Tetsuya Hoshino"
        ],
        "summary": "Concerns have arisen regarding the unregulated utilization of artificial intelligence (AI) outputs, potentially leading to various societal issues. While humans routinely validate information, manually inspecting the vast volumes of AI-generated results is impractical. Therefore, automation and visualization are imperative. In this context, Explainable AI (XAI) technology is gaining prominence, aiming to streamline AI model development and alleviate the burden of explaining AI outputs to users. Simultaneously, software auto-tuning (AT) technology has emerged, aiming to reduce the man-hours required for performance tuning in numerical calculations. AT is a potent tool for cost reduction during parameter optimization and high-performance programming for numerical computing. The synergy between AT mechanisms and AI technology is noteworthy, with AI finding extensive applications in AT. However, applying AI to AT mechanisms introduces challenges in AI model explainability. This research focuses on XAI for AI models when integrated into two different processes for practical numerical computations: performance parameter tuning of accuracy-guaranteed numerical calculations and sparse iterative algorithm.",
        "published": "2024-05-12T09:00:56Z",
        "link": "http://arxiv.org/abs/2405.10973v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LG",
            "cs.MS"
        ]
    },
    {
        "title": "Local Adjoints for Simultaneous Preaccumulations with Shared Inputs",
        "authors": [
            "Johannes Blühdorn",
            "Nicolas R. Gauger"
        ],
        "summary": "In shared-memory parallel automatic differentiation, inputs that are shared among simultaneous thread-local preaccumulations lead to data races if Jacobians are accumulated with a single, shared vector of adjoint variables. In this work, we discuss the benefits and tradeoffs of re-enabling such preaccumulations by a transition to suitable local adjoints. We propose different vector- and map-based approaches for storing local adjoint variables and analyze them with respect to memory consumption, memory allocation, and adjoint variable access times in the context of simultaneous preaccumulations in multiple threads. We implement the approaches in CoDiPack and benchmark them in parallel discrete adjoint computations in the multiphysics simulation suite SU2.",
        "published": "2024-05-13T15:01:18Z",
        "link": "http://arxiv.org/abs/2405.07819v2",
        "categories": [
            "cs.MS",
            "D.1.3; G.1.4; G.4; J.2"
        ]
    },
    {
        "title": "A Fast and Scalable Pathwise-Solver for Group Lasso and Elastic Net   Penalized Regression via Block-Coordinate Descent",
        "authors": [
            "James Yang",
            "Trevor Hastie"
        ],
        "summary": "We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path. Special attention is given when the loss is the usual least squares loss (Gaussian loss). We show that each block-coordinate update can be solved efficiently using Newton's method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate. Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets. Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet.",
        "published": "2024-05-14T14:10:48Z",
        "link": "http://arxiv.org/abs/2405.08631v1",
        "categories": [
            "stat.CO",
            "cs.LG",
            "cs.MS",
            "cs.SE"
        ]
    },
    {
        "title": "PyOptInterface: Design and implementation of an efficient modeling   language for mathematical optimization",
        "authors": [
            "Yue Yang",
            "Chenhui Lin",
            "Luo Xu",
            "Wenchuan Wu"
        ],
        "summary": "This paper introduces the design and implementation of PyOptInterface, a modeling language for mathematical optimization embedded in Python programming language. PyOptInterface uses lightweight and compact data structure to bridge high-level entities in optimization models like variables and constraints to internal indices of optimizers efficiently. It supports a variety of optimization solvers and a range of common problem classes. We provide benchmarks to exhibit the competitive performance of PyOptInterface compared with other state-of-the-art modeling languages.",
        "published": "2024-05-16T14:29:02Z",
        "link": "http://arxiv.org/abs/2405.10130v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "GridapTopOpt.jl: A scalable Julia toolbox for level set-based topology   optimisation",
        "authors": [
            "Zachary J. Wegert",
            "Jordi Manyer",
            "Connor Mallon",
            "Santiago Badia",
            "Vivien J. Challis"
        ],
        "summary": "In this paper we present GridapTopOpt, an extendable framework for level set-based topology optimisation that can be readily distributed across a personal computer or high-performance computing cluster. The package is written in Julia and uses the Gridap package ecosystem for parallel finite element assembly from arbitrary weak formulations of partial differential equation (PDEs) along with the scalable solvers from the Portable and Extendable Toolkit for Scientific Computing (PETSc). The resulting user interface is intuitive and easy-to-use, allowing for the implementation of a wide range of topology optimisation problems with a syntax that is near one-to-one with the mathematical notation. Furthermore, we implement automatic differentiation to help mitigate the bottleneck associated with the analytic derivation of sensitivities for complex problems. GridapTopOpt is capable of solving a range of benchmark and research topology optimisation problems with large numbers of degrees of freedom. This educational article demonstrates the usability and versatility of the package by describing the formulation and step-by-step implementation of several distinct topology optimisation problems. The driver scripts for these problems are provided and the package source code is available at https://github$.$com/zjwegert/GridapTopOpt.jl.",
        "published": "2024-05-17T00:36:26Z",
        "link": "http://arxiv.org/abs/2405.10478v2",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Enabling mixed-precision with the help of tools: A Nekbone case study",
        "authors": [
            "Yanxiang Chen",
            "Pablo de Oliveira Castro",
            "Paolo Bientinesi",
            "Roman Iakymchuk"
        ],
        "summary": "Mixed-precision computing has the potential to significantly reduce the cost of exascale computations, but determining when and how to implement it in programs can be challenging. In this article, we consider Nekbone, a mini-application for the CFD solver Nek5000, as a case study, and propose a methodology for enabling mixed-precision with the help of computer arithmetic tools and roofline model. We evaluate the derived mixed-precision program by combining metrics in three dimensions: accuracy, time-to-solution, and energy-to-solution. Notably, the introduction of mixed-precision in Nekbone, reducing time-to-solution by 40.7% and energy-to-solution by 47% on 128 MPI ranks.",
        "published": "2024-05-17T19:42:10Z",
        "link": "http://arxiv.org/abs/2405.11065v1",
        "categories": [
            "cs.MS",
            "cs.DC",
            "cs.SE"
        ]
    },
    {
        "title": "An 808 Line Phasor-Based Dehomogenisation Matlab Code For Multi-Scale   Topology Optimisation",
        "authors": [
            "Rebekka Varum Woldseth",
            "Ole Sigmund",
            "Peter Dørffler Ladegaard Jensen"
        ],
        "summary": "This work presents an 808-line Matlab educational code for combined multi-scale topology optimisation and phasor-based dehomogenisation titled deHomTop808. The multi-scale formulation utilises homogenisation of optimal microstructures to facilitate efficient coarse-scale optimisation. Dehomogenisation allows for a high-resolution single-scale reconstruction of the optimised multi-scale structure, achieving minor losses in structural performance, at a fraction of the computational cost, compared to its large-scale topology optimisation counterpart. The presented code utilises stiffness optimal Rank-2 microstructures to minimise the compliance of a single-load case problem, subject to a volume fraction constraint. By exploiting the inherent efficiency benefits of the phasor-based dehomogenisation procedure, on-the-fly dehomogenisation to a single-scale structure is obtained. The presented code includes procedures for structural verification of the final dehomogenised structure by comparison to the multi-scale solution. The code is introduced in terms of the underlying theory and its major components, including examples and potential extensions, and can be downloaded from https://github.com/peterdorffler/deHomTop808.git.",
        "published": "2024-05-23T08:53:24Z",
        "link": "http://arxiv.org/abs/2405.14321v2",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA",
            "math.OC"
        ]
    },
    {
        "title": "GPU Implementations for Midsize Integer Addition and Multiplication",
        "authors": [
            "Cosmin E. Oancea",
            "Stephen M. Watt"
        ],
        "summary": "This paper explores practical aspects of using a high-level functional language for GPU-based arithmetic on ``midsize'' integers. By this we mean integers of up to about a quarter million bits, which is sufficient for most practical purposes. The goal is to understand whether it is possible to support efficient nested-parallel programs with a small, flexible code base. We report on GPU implementations for addition and multiplication of integers that fit in one CUDA block, thus leveraging temporal reuse from scratchpad memories. Our key contribution resides in the simplicity of the proposed solutions: We recognize that addition is a straightforward application of scan, which is known to allow efficient GPU implementation. For quadratic multiplication we employ a simple work-partitioning strategy that offers good temporal locality. For FFT multiplication, we efficiently map the computation in the domain of integral fields by finding ``good'' primes that enable almost-full utilization of machine words. In comparison, related work uses complex tiling strategies -- which feel too big a hammer for the job -- or uses the computational domain of reals, which may degrade the magnitude of the base in which the computation is carried. We evaluate the performance in comparison to the state-of-the-art CGBN library, authored by NvidiaLab, and report that our CUDA prototype outperforms CGBN for integer sizes higher than 32K bits, while offering comparable performance for smaller sizes. Moreover, we are, to our knowledge, the first to report that FFT multiplication outperforms the classical one on the larger sizes that still fit in a CUDA block. Finally, we examine Futhark's strengths and weaknesses for efficiently supporting such computations and find out that a compiler pass aimed at efficient sequentialization of excess parallelism would significantly improve performance.",
        "published": "2024-05-23T14:44:49Z",
        "link": "http://arxiv.org/abs/2405.14642v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Uniform $\\mathcal{H}$-matrix Compression with Applications to Boundary   Integral Equations",
        "authors": [
            "Kobe Bruyninckx",
            "Daan Huybrechs",
            "Karl Meerbergen"
        ],
        "summary": "Boundary integral equation formulations of elliptic partial differential equations lead to dense system matrices when discretized, yet they are data-sparse. Using the $\\mathcal{H}$-matrix format, this sparsity is exploited to achieve $\\mathcal{O}(N\\log N)$ complexity for storage and multiplication by a vector. This is achieved purely algebraically, based on low-rank approximations of subblocks, and hence the format is also applicable to a wider range of problems. The $\\mathcal{H}^2$-matrix format improves the complexity to $\\mathcal{O}(N)$ by introducing a recursive structure onto subblocks on multiple levels. However, in practice this comes with a large proportionality constant, making the $\\mathcal{H}^2$-matrix format advantageous mostly for large problems. In this paper we investigate the usefulness of a matrix format that lies in between these two: Uniform $\\mathcal{H}$-matrices. An algebraic compression algorithm is introduced to transform a regular $\\mathcal{H}$-matrix into a uniform $\\mathcal{H}$-matrix, which maintains the asymptotic complexity.",
        "published": "2024-05-24T14:03:42Z",
        "link": "http://arxiv.org/abs/2405.15573v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "An efficient optimization model and tabu search-based global   optimization approach for continuous p-dispersion problem",
        "authors": [
            "Xiangjing Lai",
            "Zhenheng Lin",
            "Jin-Kao Hao",
            "Qinghua Wu"
        ],
        "summary": "Continuous p-dispersion problems with and without boundary constraints are NP-hard optimization problems with numerous real-world applications, notably in facility location and circle packing, which are widely studied in mathematics and operations research. In this work, we concentrate on general cases with a non-convex multiply-connected region that are rarely studied in the literature due to their intractability and the absence of an efficient optimization model. Using the penalty function approach, we design a unified and almost everywhere differentiable optimization model for these complex problems and propose a tabu search-based global optimization (TSGO) algorithm for solving them. Computational results over a variety of benchmark instances show that the proposed model works very well, allowing popular local optimization methods (e.g., the quasi-Newton methods and the conjugate gradient methods) to reach high-precision solutions due to the differentiability of the model. These results further demonstrate that the proposed TSGO algorithm is very efficient and significantly outperforms several popular global optimization algorithms in the literature, improving the best-known solutions for several existing instances in a short computational time. Experimental analyses are conducted to show the influence of several key ingredients of the algorithm on computational performance.",
        "published": "2024-05-26T16:25:55Z",
        "link": "http://arxiv.org/abs/2405.16618v1",
        "categories": [
            "math.OC",
            "cs.DM",
            "cs.MS"
        ]
    },
    {
        "title": "Scorch: A Library for Sparse Deep Learning",
        "authors": [
            "Bobby Yan",
            "Alexander J. Root",
            "Trevor Gale",
            "David Broman",
            "Fredrik Kjolstad"
        ],
        "summary": "The rapid growth in the size of deep learning models strains the capabilities of traditional dense computation paradigms. Leveraging sparse computation has become increasingly popular for training and deploying large-scale models, but existing deep learning frameworks lack extensive support for sparse operations. To bridge this gap, we introduce Scorch, a library that seamlessly integrates efficient sparse tensor computation into the PyTorch ecosystem, with an initial focus on inference workloads on CPUs. Scorch provides a flexible and intuitive interface for sparse tensors, supporting diverse sparse data structures. Scorch introduces a compiler stack that automates key optimizations, including automatic loop ordering, tiling, and format inference. Combined with a runtime that adapts its execution to both dense and sparse data, Scorch delivers substantial speedups over hand-written PyTorch Sparse (torch.sparse) operations without sacrificing usability. More importantly, Scorch enables efficient computation of complex sparse operations that lack hand-optimized PyTorch implementations. This flexibility is crucial for exploring novel sparse architectures. We demonstrate Scorch's ease of use and performance gains on diverse deep learning models across multiple domains. With only minimal code changes, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end tasks. Scorch's seamless integration and performance gains make it a valuable addition to the PyTorch ecosystem. We believe Scorch will enable wider exploration of sparsity as a tool for scaling deep learning and inform the development of other sparse libraries.",
        "published": "2024-05-27T06:59:20Z",
        "link": "http://arxiv.org/abs/2405.16883v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "An Open-Source Framework for Efficient Numerically-Tailored Computations",
        "authors": [
            "Louis Ledoux",
            "Marc Casas"
        ],
        "summary": "We present a versatile open-source framework designed to facilitate efficient, numerically-tailored Matrix-Matrix Multiplications (MMMs). The framework offers two primary contributions: first, a fine-tuned, automated pipeline for arithmetic datapath generation, enabling highly customizable systolic MMM kernels; second, seamless integration of the generated kernels into user code, irrespective of the programming language employed, without necessitating modifications.   The framework demonstrates a systematic enhancement in accuracy per energy cost across diverse High Performance Computing (HPC) workloads displaying a variety of numerical requirements, such as Artificial Intelligence (AI) inference and Sea Surface Height (SSH) computation. For AI inference, we consider a set of state-of-the-art neural network models, namely ResNet18, ResNet34, ResNet50, DenseNet121, DenseNet161, DenseNet169, and VGG11, in conjunction with two datasets, two computer formats, and 27 distinct intermediate arithmetic datapaths. Our approach consistently reduces energy consumption across all cases, with a notable example being the reduction by factors of $3.3\\times$ for IEEE754-32 and $1.4\\times$ for Bfloat16 during ImageNet inference with ResNet50. This is accomplished while maintaining accuracies of $82.3\\%$ and $86\\%$, comparable to those achieved with conventional Floating-Point Units (FPUs). In the context of SSH computation, our method achieves fully-reproducible results using double-precision words, surpassing the accuracy of conventional double- and quad-precision arithmetic in FPUs. Our approach enhances SSH computation accuracy by a minimum of $5\\times$ and $27\\times$ compared to IEEE754-64 and IEEE754-128, respectively, resulting in $5.6\\times$ and $15.1\\times$ improvements in accuracy per power cost.",
        "published": "2024-05-29T10:10:53Z",
        "link": "http://arxiv.org/abs/2406.02579v1",
        "categories": [
            "cs.MS",
            "cs.AI",
            "cs.AR",
            "cs.LG",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "svds-C: A Multi-Thread C Code for Computing Truncated Singular Value   Decomposition",
        "authors": [
            "Xu Feng",
            "Wenjian Yu",
            "Yuyang Xie"
        ],
        "summary": "This article presents svds-C, an open-source and high-performance C program for accurately and robustly computing truncated SVD, e.g. computing several largest singular values and corresponding singular vectors. We have re-implemented the algorithm of svds in Matlab in C based on MKL or OpenBLAS and multi-thread computing to obtain the parallel program named svds-C. svds-C running on shared-memory computer consumes less time and memory than svds thanks to careful implementation of multi-thread parallelization and memory management. Numerical experiments on different test cases which are synthetically generated or directly from real world datasets show that, svds-C runs remarkably faster than svds with averagely 4.7X and at most 12X speedup for 16-thread parallel computing on a computer with Intel CPU, while preserving same accuracy and consuming about half memory space. Experimental results also demonstrate that svds-C has similar advantages over svds on the computer with AMD CPU, and outperforms other state-of-the-art algorithms for truncated SVD on computing time and robustness.",
        "published": "2024-05-29T10:24:56Z",
        "link": "http://arxiv.org/abs/2405.18966v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Parallel Redundancy Removal in lrslib with Application to Projections",
        "authors": [
            "David Avis",
            "Charles Jordan"
        ],
        "summary": "We describe a parallel implementation in lrslib for removing redundant halfspaces and finding a minimum representation for an H-representation of a convex polyhedron. By a standard transformation, the same code works for V-representations. We use this approach to speed up the redundancy removal step in Fourier-Motzkin elimination. Computational results are given including a comparison with Clarkson's algorithm, which is particularly fast on highly redundant inputs.",
        "published": "2024-05-30T11:24:11Z",
        "link": "http://arxiv.org/abs/2406.00065v1",
        "categories": [
            "math.OC",
            "cs.CG",
            "cs.MS"
        ]
    },
    {
        "title": "Spectral Toolkit of Algorithms for Graphs: Technical Report (2)",
        "authors": [
            "Peter Macgregor",
            "He Sun"
        ],
        "summary": "Spectral Toolkit of Algorithms for Graphs (STAG) is an open-source library for efficient graph algorithms. This technical report presents the newly implemented component on locality sensitive hashing, kernel density estimation, and fast spectral clustering. The report includes a user's guide to the newly implemented algorithms, experiments and demonstrations of the new functionality, and several technical considerations behind our development.",
        "published": "2024-06-06T15:32:37Z",
        "link": "http://arxiv.org/abs/2407.07096v1",
        "categories": [
            "cs.DS",
            "cs.LG",
            "cs.MS",
            "cs.SI"
        ]
    },
    {
        "title": "Flexible Multi-Dimensional FFTs for Plane Wave Density Functional Theory   Codes",
        "authors": [
            "Doru Thom Popovici",
            "Mauro del Ben",
            "Osni Marques",
            "Andrew Canning"
        ],
        "summary": "Multi-dimensional Fourier transforms are key mathematical building blocks that appear in a wide range of applications from materials science, physics, chemistry and even machine learning. Over the past years, a multitude of software packages targeting distributed multi-dimensional Fourier transforms have been developed. Most variants attempt to offer efficient implementations for single transforms applied on data mapped onto rectangular grids. However, not all scientific applications conform to this pattern, i.e. plane wave Density Functional Theory codes require multi-dimensional Fourier transforms applied on data represented as batches of spheres. Typically, the implementations for this use case are hand-coded and tailored for the requirements of each application. In this work, we present the Fastest Fourier Transform from Berkeley (FFTB) a distributed framework that offers flexible implementations for both regular/non-regular data grids and batched/non-batched transforms. We provide a flexible implementations with a user-friendly API that captures most of the use cases. Furthermore, we provide implementations for both CPU and GPU platforms, showing that our approach offers improved execution time and scalability on the HP Cray EX supercomputer. In addition, we outline the need for flexible implementations for different use cases of the software package.",
        "published": "2024-06-08T21:17:35Z",
        "link": "http://arxiv.org/abs/2406.05577v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "68W15",
            "G.4"
        ]
    },
    {
        "title": "An extension of C++ with memory-centric specifications for HPC to reduce   memory footprints and streamline MPI development",
        "authors": [
            "Pawel K. Radtke",
            "Cristian G. Barrera-Hinojosa",
            "Mladen Ivkovic",
            "Tobias Weinzierl"
        ],
        "summary": "The C++ programming language and its cousins lean towards a memory-inefficient storage of structs: The compiler inserts helper bits into the struct such that individual attributes align with bytes, and it adds additional bytes aligning attributes with cache lines, while it is not able to exploit knowledge about the range of integers, enums or bitsets to bring the memory footprint down. Furthermore, the language provides neither support for data exchange via MPI nor for arbitrary floating-point precision formats. If developers need to have a low memory footprint and MPI datatypes over structs which exchange only minimal data, they have to manipulate the data and to write MPI datatypes manually. We propose a C++ language extension based upon C++ attributes through which developers can guide the compiler what memory arrangements would be beneficial: Can multiple booleans be squeezed into one bit field, do floats hold fewer significant bits than in the IEEE standard, or does the code require a user-defined MPI datatype for certain subsets of attributes? The extension offers the opportunity to fall back to normal alignment and padding rules via plain C++ assignments, no dependencies upon external libraries are introduced, and the resulting code remains standard C++. Our work implements the language annotations within LLVM and demonstrates their potential impact, both upon the runtime and the memory footprint, through smoothed particle hydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of performance and development productivity.",
        "published": "2024-06-10T08:26:27Z",
        "link": "http://arxiv.org/abs/2406.06095v2",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Enhancing non-Perl bioinformatic applications with Perl: Building novel,   component based applications using Object Orientation, PDL, Alien, FFI,   Inline and OpenMP",
        "authors": [
            "Christos Argyropoulos"
        ],
        "summary": "Component-Based Software Engineering (CBSE) is a methodology that assembles pre-existing, re-usable software components into new applications, which is particularly relevant for fast moving, data-intensive fields such as bioinformatics. While Perl was used extensively in this field until a decade ago, more recent applications opt for a Bioconductor/R or Python. This trend represents a significantly missed opportunity for the rapid generation of novel bioinformatic applications out of pre-existing components since Perl offers a variety of abstractions that can facilitate composition. In this paper, we illustrate the utility of Perl for CBSE through a combination of Object Oriented frameworks, the Perl Data Language and facilities for interfacing with non-Perl code through Foreign Function Interfaces and inlining of foreign source code. To do so, we enhance Polyester, a RNA sequencing simulator written in R, and edlib a fast sequence similarity search library based on the edit distance. The first case study illustrates the near effortless authoring of new, highly performant Perl modules for the simulation of random numbers using the GNU Scientific Library and PDL, and proposes Perl and Perl/C alternatives to the Python tool cutadapt that is used to \"trim\" polyA tails from biological sequences. For the edlib case, we leverage the power of metaclass programming to endow edlib with coarse, process based parallelism, through the Many Core Engine (MCE) module and fine grained parallelism through OpenMP, a C/C++/Fortran Application Programming Interface for shared memory multithreaded processing. These use cases provide proof-of-concept for the Bio::SeqAlignment framework, which can organize heterogeneous components in complex memory and command-line based workflows for the construction of novel bionformatic tools to analyze data from long-read sequencing, e.g. Nanopore, sequencing platforms.",
        "published": "2024-06-11T18:32:50Z",
        "link": "http://arxiv.org/abs/2406.10271v1",
        "categories": [
            "cs.MS",
            "cs.SE"
        ]
    },
    {
        "title": "A square root algorithm faster than Newton's method for multiprecision   numbers, using floating-point arithmetic",
        "authors": [
            "Fabio Romano"
        ],
        "summary": "In this paper, an optimized version of classical Bombelli's algorithm for computing integer square roots is presented. In particular, floating-point arithmetic is used to compute the initial guess of each digit of the root, following similar ideas to those used in \"The Art of Computer Programming\" Vol. 2, p. 4.3.1 for division. A program with an implementation of the algorithm in Java is also presented, and its running time is compared with that of the algorithm provided by the Java standard library, which uses the Newton's method. From tests, the algorithm presented here turns out to be much faster.",
        "published": "2024-06-11T22:22:13Z",
        "link": "http://arxiv.org/abs/2406.07751v1",
        "categories": [
            "cs.MS",
            "cs.DS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "Hiperwalk: Simulation of Quantum Walks with Heterogeneous   High-Performance Computing",
        "authors": [
            "Paulo Motta",
            "Gustavo A. Bezerra",
            "Anderson F. P. Santos",
            "Renato Portugal"
        ],
        "summary": "The Hiperwalk package is designed to facilitate the simulation of quantum walks using heterogeneous high-performance computing, taking advantage of the parallel processing power of diverse processors such as CPUs, GPUs, and acceleration cards. This package enables the simulation of both the continuous-time and discrete-time quantum walk models, effectively modeling the behavior of quantum systems on large graphs. Hiperwalk features a user-friendly Python package frontend with comprehensive documentation, as well as a high-performance C-based inner core that leverages parallel computing for efficient linear algebra calculations. This versatile tool empowers researchers to better understand quantum walk behavior, optimize implementation, and explore a wide range of potential applications, including spatial search algorithms.",
        "published": "2024-06-12T13:17:05Z",
        "link": "http://arxiv.org/abs/2406.08186v1",
        "categories": [
            "quant-ph",
            "cs.MS"
        ]
    },
    {
        "title": "PETSc/TAO Developments for GPU-Based Early Exascale Systems",
        "authors": [
            "Richard Tran Mills",
            "Mark Adams",
            "Satish Balay",
            "Jed Brown",
            "Jacob Faibussowitsch",
            "Toby Isaac",
            "Matthew Knepley",
            "Todd Munson",
            "Hansol Suh",
            "Stefano Zampini",
            "Hong Zhang",
            "Junchao Zhang"
        ],
        "summary": "The Portable Extensible Toolkit for Scientific Computation (PETSc) library provides scalable solvers for nonlinear time-dependent differential and algebraic equations and for numerical optimization via the Toolkit for Advanced Optimization (TAO). PETSc is used in dozens of scientific fields and is an important building block for many simulation codes. During the U.S. Department of Energy's Exascale Computing Project, the PETSc team has made substantial efforts to enable efficient utilization of the massive fine-grain parallelism present within exascale compute nodes and to enable performance portability across exascale architectures. We recap some of the challenges that designers of numerical libraries face in such an endeavor, and then discuss the many developments we have made, which include the addition of new GPU backends, features supporting efficient on-device matrix assembly, better support for asynchronicity and GPU kernel concurrency, and new communication infrastructure. We evaluate the performance of these developments on some pre-exascale systems as well the early exascale systems Frontier and Aurora, using compute kernel, communication layer, solver, and mini-application benchmark studies, and then close with a few observations drawn from our experiences on the tension between portable performance and other goals of numerical libraries.",
        "published": "2024-06-12T21:11:46Z",
        "link": "http://arxiv.org/abs/2406.08646v2",
        "categories": [
            "cs.MS",
            "cs.DC",
            "00A69"
        ]
    },
    {
        "title": "A Symbolic Computing Perspective on Software Systems",
        "authors": [
            "Arthur C. Norman",
            "Stephen M. Watt"
        ],
        "summary": "Symbolic mathematical computing systems have served as a canary in the coal mine of software systems for more than sixty years. They have introduced or have been early adopters of programming language ideas such ideas as dynamic memory management, arbitrary precision arithmetic and dependent types. These systems have the feature of being highly complex while at the same time operating in a domain where results are well-defined and clearly verifiable. These software systems span multiple layers of abstraction with concerns ranging from instruction scheduling and cache pressure up to algorithmic complexity of constructions in algebraic geometry. All of the major symbolic mathematical computing systems include low-level code for arithmetic, memory management and other primitives, a compiler or interpreter for a bespoke programming language, a library of high level mathematical algorithms, and some form of user interface. Each of these parts invokes multiple deep issues.   We present some lessons learned from this environment and free flowing opinions on topics including:   * Portability of software across architectures and decades;   * Infrastructure to embrace and infrastructure to avoid;   * Choosing base abstractions upon which to build;   * How to get the most out of a small code base;   * How developments in compilers both to optimise and to validate code have always been and remain of critical importance, with plenty of remaining challenges;   * The way in which individuals including in particular Alan Mycroft who has been able to span from hand-crafting Z80 machine code up to the most abstruse high level code analysis techniques are needed, and   * Why it is important to teach full-stack thinking to the next generation.",
        "published": "2024-06-13T13:10:47Z",
        "link": "http://arxiv.org/abs/2406.09085v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "cs.SE",
            "I.1.3"
        ]
    },
    {
        "title": "SySTeC: A Symmetric Sparse Tensor Compiler",
        "authors": [
            "Radha Patel",
            "Willow Ahrens",
            "Saman Amarasinghe"
        ],
        "summary": "Symmetric and sparse tensors arise naturally in many domains including linear algebra, statistics, physics, chemistry, and graph theory. Symmetric tensors are equal to their transposes, so in the $n$-dimensional case we can save up to a factor of $n!$ by avoiding redundant operations. Sparse tensors, on the other hand, are mostly zero, and we can save asymptotically by processing only nonzeros. Unfortunately, specializing for both symmetry and sparsity at the same time is uniquely challenging. Optimizing for symmetry requires consideration of $n!$ transpositions of a triangular kernel, which can be complex and error prone. Considering multiple transposed iteration orders and triangular loop bounds also complicates iteration through intricate sparse tensor formats. Additionally, since each combination of symmetry and sparse tensor formats requires a specialized implementation, this leads to a combinatorial number of cases. A compiler is needed, but existing compilers cannot take advantage of both symmetry and sparsity within the same kernel. In this paper, we describe the first compiler which can automatically generate symmetry-aware code for sparse or structured tensor kernels. We introduce a taxonomy for symmetry in tensor kernels, and show how to target each kind of symmetry. Our implementation demonstrates significant speedups ranging from 1.36x for SSYMV to 30.4x for a 5-dimensional MTTKRP over the non-symmetric state of the art.",
        "published": "2024-06-13T16:06:29Z",
        "link": "http://arxiv.org/abs/2406.09266v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "12 Labours tools for developing Functional Tissue Units",
        "authors": [
            "Jagir R. Hussan"
        ],
        "summary": "A brief introduction of the technical approach to model FTUs as an aggregate of cells, whose state transition dynamics are mathematically represented as port-hamiltonians or Differential Algebraic equations is presented. A python library and browser based tool to enable modellers to compose the FTU graph, specify the cellular equations and the interconnection between the cells at the level of physical quantities they exchange consistent with the technical approach is discussed.",
        "published": "2024-06-13T22:55:40Z",
        "link": "http://arxiv.org/abs/2406.10301v1",
        "categories": [
            "q-bio.TO",
            "cs.MS",
            "math.DS"
        ]
    },
    {
        "title": "Learning from landmarks, curves, surfaces, and shapes in Geomstats",
        "authors": [
            "Luís F. Pereira",
            "Alice Le Brigant",
            "Adele Myers",
            "Emmanuel Hartman",
            "Amil Khan",
            "Malik Tuerkoen",
            "Trey Dold",
            "Mengyang Gu",
            "Pablo Suárez-Serrato",
            "Nina Miolane"
        ],
        "summary": "We introduce the shape module of the Python package Geomstats to analyze shapes of objects represented as landmarks, curves and surfaces across fields of natural sciences and engineering. The shape module first implements widely used shape spaces, such as the Kendall shape space, as well as elastic spaces of discrete curves and surfaces. The shape module further implements the abstract mathematical structures of group actions, fiber bundles, quotient spaces and associated Riemannian metrics which allow users to build their own shape spaces. The Riemannian geometry tools enable users to compare, average, interpolate between shapes inside a given shape space. These essential operations can then be leveraged to perform statistics and machine learning on shape data. We present the object-oriented implementation of the shape module along with illustrative examples and show how it can be used to perform statistics and machine learning on shape spaces.",
        "published": "2024-06-14T22:59:03Z",
        "link": "http://arxiv.org/abs/2406.10437v1",
        "categories": [
            "cs.GR",
            "cs.MS",
            "math.DG"
        ]
    },
    {
        "title": "OpenCAEPoro: A Parallel Simulation Framework for Multiphase and   Multicomponent Porous Media Flows",
        "authors": [
            "Shizhe Li",
            "Chen-Song Zhang"
        ],
        "summary": "OpenCAEPoro is a parallel numerical simulation software developed in C++ for simulating multiphase and multicomponent flows in porous media. The software utilizes a set of general-purpose compositional model equations, enabling it to handle a diverse range of fluid dynamics, including the black oil model, compositional model, and thermal recovery models. OpenCAEPoro establishes a unified solving framework that integrates many widely used methods, such as IMPEC, FIM, and AIM. This framework allows dynamic collaboration between different methods. Specifically, based on this framework, we have developed an adaptively coupled domain decomposition method, which can provide initial solutions for global methods to accelerate the simulation. The reliability of OpenCAEPoro has been validated through benchmark testing with the SPE comparative solution project. Furthermore, its robust parallel efficiency has been tested in distributed parallel environments, demonstrating its suitability for large-scale simulation problems.",
        "published": "2024-06-16T09:12:54Z",
        "link": "http://arxiv.org/abs/2406.10862v1",
        "categories": [
            "cs.MS",
            "G.4"
        ]
    },
    {
        "title": "Utilization of G-Programming Language for Educational Control   Application: Case Study of Magnetic Levitation of Elastic Beam",
        "authors": [
            "Abdallah Amr",
            "Mostafa Eshra",
            "Ayman A. Nada"
        ],
        "summary": "This paper presents the practical employment of G-Programming tools to demonstrate, design, and implement traditional control algorithms upon magnetic levitation system. The complexity of controlling this type of fast dynamic and sensitive system is vital for highlighting the capabilities of LabVIEW G-programming in control education. PID and Lead-Lag controllers are designed and implemented within the LabVIEW environment, with the ability to tune and optimize the controllers utilizing the Virtual Instruments (VIs) of the control design and simulation toolkit. The paper enables the reader to understand the modelling, testing the control action, and dynamic simulation of the system. Then, deploying the control law in real time. It can be concluded that the G programming shows a suitable and easy tool for facilitating hands-on, experiential learning and validation in control systems engineering.",
        "published": "2024-06-20T20:44:10Z",
        "link": "http://arxiv.org/abs/2406.14726v1",
        "categories": [
            "eess.SY",
            "cs.MS",
            "cs.SY"
        ]
    },
    {
        "title": "ASTERIX: Module for modelling the water flow on vegetated hillslopes",
        "authors": [
            "Stelian Ion",
            "Dorin Marinescu",
            "Stefan-Gicu Cruceanu"
        ],
        "summary": "The paper presents ASTERIX, an open source software for numerical integration of an extended Saint-Venant system of equations used as a mathematical tool to model the water flow from laboratory up to large-scale spatial domains applying physically-based principles of fluid mechanics. Many in-situ observations have shown that the plant cover plays a key roll in controlling the hydrological flux at a catchment scale. The plant roots facilitate the infiltration processes, the canopy intercept some proportion of rain, and plant stems slow down the flow. In case of heavy rains, the infiltration and interception processes cease in a short time, the remaining rainfall gives rise to the Hortonian overland flow and the flash flood is thus initiated. In this context, the following problem is also addressed in the article: how do the gradient of soil surface and the density of the plant cover influence the water dynamics in the Hortonian flow? The mathematical model and ASTERIX were kept as simple as possible in order to be accessible to a wide range of stakeholders interested in understanding the complex processes behind the water flow on hillslopes covered by plants.   The software is written in C programming language and it is free under GNU license. It was tested on a series of benchmark problems, laboratory experiments, and theoretical problems; and the results have shown a good agreement with the theoretical or measured data.",
        "published": "2024-06-21T07:41:46Z",
        "link": "http://arxiv.org/abs/2406.14933v1",
        "categories": [
            "cs.MS",
            "physics.flu-dyn",
            "35-04, 76-04 (Primary), 76-10, 35Q35, 74F10, 65M08 (Secondary)",
            "G.1.8; G.4"
        ]
    },
    {
        "title": "Introducing Moment: A toolkit for semi-definite programming with moment   matrices",
        "authors": [
            "Andrew J. P. Garner",
            "Mateus Araújo"
        ],
        "summary": "Non-commutative polynomial optimization is a powerful technique with numerous applications in quantum nonlocality, quantum key distribution, causal inference, many-body physics, amongst others. The standard approach is to reduce such optimizations to a hierarchy of semi-definite programs, which can be solved numerically using well-understood interior-point methods. A key, but computationally costly, step is the formulation of moment matrices, whose size (and hence cost) grows exponentially with the depth of the hierarchy. It is therefore essential to have highly-optimized software to construct moment matrices. Here, we introduce Moment: a toolkit that produces moment matrix relaxations from the specification of a non-commutative optimization problem. In order to obtain the absolute best performance, Moment is written in C++, and for convenience of use provides an interface via MATLAB. We benchmark Moment's performance, and see that it can be up to four orders of magnitude faster than current software with similar functionality.",
        "published": "2024-06-21T18:00:08Z",
        "link": "http://arxiv.org/abs/2406.15559v1",
        "categories": [
            "quant-ph",
            "cs.MS",
            "math.OC"
        ]
    },
    {
        "title": "Automating Variational Differentiation",
        "authors": [
            "Kangbo Li",
            "Anil Damle"
        ],
        "summary": "Many problems in Physics and Chemistry are formulated as the minimization of a functional. Therefore, methods for solving these problems typically require differentiating maps whose input and/or output are functions -- commonly referred to as variational differentiation. Such maps are not addressed at the mathematical level by the chain rule, which underlies modern symbolic and algorithmic differentiation (AD) systems. Although there are algorithmic solutions such as tracing and reverse accumulation, they do not provide human readability and introduce strict programming constraints that bottleneck performance, especially in high-performance computing (HPC) environments. In this manuscript, we propose a new computer theoretic model of differentiation by combining the pullback of the $\\mathbf{B}$ and $\\mathbf{C}$ combinators from the combinatory logic. Unlike frameworks based on the chain rule, this model differentiates a minimal complete basis for the space of computable functions. Consequently, the model is capable of analytic backpropagation and variational differentiation while supporting complex numbers. To demonstrate the generality of this approach we build a system named CombDiff, which can differentiate nontrivial variational problems such as Hartree-Fock (HF) theory and multilayer perceptrons.",
        "published": "2024-06-23T16:28:46Z",
        "link": "http://arxiv.org/abs/2406.16154v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "cs.PL",
            "math.NA",
            "68V99, 49-04, 15-04"
        ]
    },
    {
        "title": "Structured Sketching for Linear Systems",
        "authors": [
            "Johannes J Brust",
            "Michael A Saunders"
        ],
        "summary": "For linear systems $Ax=b$ we develop iterative algorithms based on a sketch-and-project approach. By using judicious choices for the sketch, such as the history of residuals, we develop weighting strategies that enable short recursive formulas. The proposed algorithms have a low memory footprint and iteration complexity compared to regular sketch-and-project methods. In a set of numerical experiments the new methods compare well to GMRES, SYMMLQ and state-of-the-art randomized solvers.",
        "published": "2024-06-30T16:12:36Z",
        "link": "http://arxiv.org/abs/2407.00746v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA",
            "stat.CO",
            "15A06, 15B52, 65F10, 68W20, 65Y20, 90C20"
        ]
    },
    {
        "title": "Enabling MPI communication within Numba/LLVM JIT-compiled Python code   using numba-mpi v1.0",
        "authors": [
            "Kacper Derlatka",
            "Maciej Manna",
            "Oleksii Bulenok",
            "David Zwicker",
            "Sylwester Arabas"
        ],
        "summary": "The numba-mpi package offers access to the Message Passing Interface (MPI) routines from Python code that uses the Numba just-in-time (JIT) compiler. As a result, high-performance and multi-threaded Python code may utilize MPI communication facilities without leaving the JIT-compiled code blocks, which is not possible with the mpi4py package, a higher-level Python interface to MPI. For debugging purposes, numba-mpi retains full functionality of the code even if the JIT compilation is disabled. The numba-mpi API constitutes a thin wrapper around the C API of MPI and is built around Numpy arrays including handling of non-contiguous views over array slices. Project development is hosted at GitHub leveraging the mpi4py/setup-mpi workflow enabling continuous integration tests on Linux (MPICH, OpenMPI & Intel MPI), macOS (MPICH & OpenMPI) and Windows (MS MPI). The paper covers an overview of the package features, architecture and performance. As of v1.0, the following MPI routines are exposed and covered by unit tests: size/rank, [i]send/[i]recv, wait[all|any], test[all|any], allreduce, bcast, barrier, scatter/[all]gather & wtime. The package is implemented in pure Python and depends on numpy, numba and mpi4py (the latter used at initialization and as a source of utility routines only). The performance advantage of using numba-mpi compared to mpi4py is depicted with a simple example, with entirety of the code included in listings discussed in the text. Application of numba-mpi for handling domain decomposition in numerical solvers for partial differential equations is presented using two external packages that depend on numba-mpi: py-pde and PyMPDATA-MPI.",
        "published": "2024-07-01T17:12:34Z",
        "link": "http://arxiv.org/abs/2407.13712v1",
        "categories": [
            "cs.DC",
            "cs.MS"
        ]
    },
    {
        "title": "Construct accurate multi-continuum micromorphic homogenisations in   multi-D space-time with computer algebra",
        "authors": [
            "A. J. Roberts"
        ],
        "summary": "Homogenisation empowers the efficient macroscale system level prediction of physical problems with intricate microscale structures. Here we develop an innovative powerful, rigorous and flexible framework for asymptotic homogenisation of dynamics at the finite scale separation of real physics, with proven results underpinned by modern dynamical systems theory. The novel systematic approach removes most of the usual assumptions, whether implicit or explicit, of other methodologies. By no longer assuming averages the methodology constructs so-called multi-continuum or micromorphic homogenisations systematically based upon the microscale physics. The developed framework and approach enables a user to straightforwardly choose and create such homogenisations with clear physical and theoretical support, and of highly controllable accuracy and fidelity.",
        "published": "2024-07-03T20:11:02Z",
        "link": "http://arxiv.org/abs/2407.03483v2",
        "categories": [
            "math.DS",
            "cs.MS",
            "math.AP",
            "35B27, 74H10, 37L10, 35B40"
        ]
    },
    {
        "title": "Disciplined Geodesically Convex Programming",
        "authors": [
            "Andrew Cheng",
            "Vaibhav Dixit",
            "Melanie Weber"
        ],
        "summary": "Convex programming plays a fundamental role in machine learning, data science, and engineering. Testing convexity structure in nonlinear programs relies on verifying the convexity of objectives and constraints. \\citet{grant2006disciplined} introduced a framework, Disciplined Convex Programming (DCP), for automating this verification task for a wide range of convex functions that can be decomposed into basic convex functions (atoms) using convexity-preserving compositions and transformations (rules). However, the restriction to Euclidean convexity concepts can limit the applicability of the framework. For instance, many notable instances of statistical estimators and matrix-valued (sub)routines in machine learning applications are Euclidean non-convex, but exhibit geodesic convexity through a more general Riemannian lens. In this work, we extend disciplined programming to this setting by introducing Disciplined Geodesically Convex Programming (DGCP). We determine convexity-preserving compositions and transformations for geodesically convex functions on general Cartan-Hadamard manifolds, as well as for the special case of symmetric positive definite matrices, a common setting in matrix-valued optimization. For the latter, we also define a basic set of atoms. Our paper is accompanied by a Julia package SymbolicAnalysis.jl, which provides functionality for testing and certifying DGCP-compliant expressions. Our library interfaces with manifold optimization software, which allows for directly solving verified geodesically convex programs.",
        "published": "2024-07-07T05:13:51Z",
        "link": "http://arxiv.org/abs/2407.05261v1",
        "categories": [
            "math.OC",
            "cs.LG",
            "cs.MS",
            "stat.ML"
        ]
    },
    {
        "title": "Acceleration of Tensor-Product Operations with Tensor Cores",
        "authors": [
            "Cu Cui"
        ],
        "summary": "In this paper, we explore the acceleration of tensor product operations in finite element methods, leveraging the computational power of the NVIDIA A100 GPU Tensor Cores. We provide an accessible overview of the necessary mathematical background and discuss our implementation strategies. Our study focuses on two common programming approaches for NVIDIA Tensor Cores: the C++ Warp Matrix Functions in nvcuda::wmma and the inline Parallel Thread Execution (PTX) instructions mma.sync.aligned. A significant focus is placed on the adoption of the versatile inline PTX instructions combined with a conflict-free shared memory access pattern, a key to unlocking superior performance. When benchmarked against traditional CUDA Cores, our approach yields a remarkable 2.3-fold increase in double precision performance, achieving 8 TFLOPS/s-45% of the theoretical maximum. Furthermore, in half-precision computations, numerical experiments demonstrate a fourfold enhancement in solving the Poisson equation using the flexible GMRES (FGMRES) method, preconditioned by a multigrid method in 3D. This is achieved while maintaining the same discretization error as observed in double precision computations. These results highlight the considerable benefits of using Tensor Cores for finite element operators with tensor products, achieving an optimal balance between computational speed and precision.",
        "published": "2024-07-12T18:16:38Z",
        "link": "http://arxiv.org/abs/2407.09621v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "cs.PF",
            "math.NA",
            "G.1.8; G.4"
        ]
    },
    {
        "title": "MPAT: Modular Petri Net Assembly Toolkit",
        "authors": [
            "Stefano Chiaradonna",
            "Petar Jevtic",
            "Beckett Sterner"
        ],
        "summary": "We present a Python package called Modular Petri Net Assembly Toolkit (MPAT) that empowers users to easily create large-scale, modular Petri Nets for various spatial configurations, including extensive spatial grids or those derived from shape files, augmented with heterogeneous information layers. Petri Nets are powerful discrete event system modeling tools in computational biology and engineering. However, their utility for automated construction of large-scale spatial models has been limited by gaps in existing modeling software packages. MPAT addresses this gap by supporting the development of modular Petri Net models with flexible spatial geometries.",
        "published": "2024-07-15T00:41:02Z",
        "link": "http://arxiv.org/abs/2407.10372v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Compressing Structured Tensor Algebra",
        "authors": [
            "Mahdi Ghorbani",
            "Emilien Bauer",
            "Tobias Grosser",
            "Amir Shaikhha"
        ],
        "summary": "Tensor algebra is a crucial component for data-intensive workloads such as machine learning and scientific computing. As the complexity of data grows, scientists often encounter a dilemma between the highly specialized dense tensor algebra and efficient structure-aware algorithms provided by sparse tensor algebra. In this paper, we introduce DASTAC, a framework to propagate the tensors's captured high-level structure down to low-level code generation by incorporating techniques such as automatic data layout compression, polyhedral analysis, and affine code generation. Our methodology reduces memory footprint by automatically detecting the best data layout, heavily benefits from polyhedral optimizations, leverages further optimizations, and enables parallelization through MLIR. Through extensive experimentation, we show that DASTAC achieves 1 to 2 orders of magnitude speedup over TACO, a state-of-the-art sparse tensor compiler, and StructTensor, a state-of-the-art structured tensor algebra compiler, with a significantly lower memory footprint.",
        "published": "2024-07-18T17:25:17Z",
        "link": "http://arxiv.org/abs/2407.13726v1",
        "categories": [
            "cs.PL",
            "cs.LG",
            "cs.MS"
        ]
    },
    {
        "title": "Mixed Precision Block-Jacobi Preconditioner: Algorithms, Performance   Evaluation and Feature Analysis",
        "authors": [
            "Ningxi Tian",
            "Silu Huang",
            "Xiaowen Xu"
        ],
        "summary": "In this paper, we propose two mixed precision algorithms for Block-Jacobi preconditioner(BJAC): a fixed low precision strategy and an adaptive precision strategy. We evaluate the performance improvement of the proposed mixed precision BJAC preconditioners combined with the preconditioned conjugate gradient algorithm using problems including diffusion equations and radiation hydrodynamics equations. Numerical results show that, compared to the uniform high precision PCG algorithm, the mixed precision preconditioners can achieve speedups from 1.3 to 1.8 without sacrificing accuracy. Furthermore, we observe the phenomenon of convergence delay in some test cases for the mixed precision preconditioners, and further analyse the matrix features associate with the convergence delay behavior.",
        "published": "2024-07-22T18:35:05Z",
        "link": "http://arxiv.org/abs/2407.15973v3",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Implementing a Restricted Function Space Class in Firedrake",
        "authors": [
            "Emma Rothwell"
        ],
        "summary": "The implementation process of a $\\texttt{RestrictedFunctionSpace}$ class in Firedrake, a Python library which numerically solves partial differential equations through the use of the finite element method, is documented. This includes an introduction to the current $\\texttt{FunctionSpace}$ class in Firedrake, and the key features that it has. With the current $\\texttt{FunctionSpace}$ class, the limitations of the capabilities of the solvers in Firedrake when imposing Dirichlet boundary conditions are explored, as well as what the $\\texttt{RestrictedFunctionSpace}$ class does differently to remove these issues. These will be considered in both a mathematical way, and in the code as an abstraction of the mathematical ideas presented. Finally, the benefits to the user of the $\\texttt{RestrictedFunctionSpace}$ class are considered, and demonstrated through tests and comparisons. This leads to the conclusion that in particular, the eigensolver in Firedrake is improved through the use of the $\\texttt{RestrictedFunctionSpace}$, through the removal of eigenvalues associated with the Dirichlet boundary conditions for a system.",
        "published": "2024-07-24T16:47:01Z",
        "link": "http://arxiv.org/abs/2408.05217v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "HOBOTAN: Efficient Higher Order Binary Optimization Solver with Tensor   Networks and PyTorch",
        "authors": [
            "Shoya Yasuda",
            "Shunsuke Sotobayashi",
            "Yuichiro Minato"
        ],
        "summary": "In this study, we introduce HOBOTAN, a new solver designed for Higher Order Binary Optimization (HOBO). HOBOTAN supports both CPU and GPU, with the GPU version developed based on PyTorch, offering a fast and scalable system. This solver utilizes tensor networks to solve combinatorial optimization problems, employing a HOBO tensor that maps the problem and performs tensor contractions as needed. Additionally, by combining techniques such as batch processing for tensor optimization and binary-based integer encoding, we significantly enhance the efficiency of combinatorial optimization. In the future, the utilization of increased GPU numbers is expected to harness greater computational power, enabling efficient collaboration between multiple GPUs for high scalability. Moreover, HOBOTAN is designed within the framework of quantum computing, thus providing insights for future quantum computer applications. This paper details the design, implementation, performance evaluation, and scalability of HOBOTAN, demonstrating its effectiveness.",
        "published": "2024-07-29T13:20:11Z",
        "link": "http://arxiv.org/abs/2407.19987v1",
        "categories": [
            "cs.MS",
            "quant-ph"
        ]
    },
    {
        "title": "JAX-SSO: Differentiable Finite Element Analysis Solver for Structural   Optimization and Seamless Integration with Neural Networks",
        "authors": [
            "Gaoyuan Wu"
        ],
        "summary": "Differentiable numerical simulations of physical systems have gained rising attention in the past few years with the development of automatic differentiation tools. This paper presents JAX-SSO, a differentiable finite element analysis solver built with JAX, Google's high-performance computing library, to assist efficient structural design in the built environment. With the adjoint method and automatic differentiation feature, JAX-SSO can efficiently evaluate gradients of physical quantities in an automatic way, enabling accurate sensitivity calculation in structural optimization problems. Written in Python and JAX, JAX-SSO is naturally within the machine learning ecosystem so it can be seamlessly integrated with neural networks to train machine learning models with inclusion of physics. Moreover, JAX-SSO supports GPU acceleration to further boost finite element analysis. Several examples are presented to showcase the capabilities and efficiency of JAX-SSO: i) shape optimization of grid-shells and continuous shells; ii) size (thickness) optimization of continuous shells; iii) simultaneous shape and topology optimization of continuous shells; and iv) training of physics-informed neural networks for structural optimization. We believe that JAX-SSO can facilitate research related to differentiable physics and machine learning to further address problems in structural and architectural design.",
        "published": "2024-07-29T14:02:33Z",
        "link": "http://arxiv.org/abs/2407.20026v1",
        "categories": [
            "cs.MS",
            "math.OC"
        ]
    },
    {
        "title": "A Multi-Reference Relaxation Enforced Neighborhood Search Heuristic in   SCIP",
        "authors": [
            "Suresh Bolusani",
            "Gioni Mexi",
            "Mathieu Besançon",
            "Mark Turner"
        ],
        "summary": "This paper proposes and evaluates a Multi-Reference Relaxation Enforced Neighborhood Search (MRENS) heuristic within the SCIP solver. This study marks the first integration and evaluation of MRENS in a full-fledged MILP solver, specifically coupled with the recently-introduced Lagromory separator for generating multiple reference solutions. Computational experiments on the MIPLIB 2017 benchmark set show that MRENS, with multiple reference solutions, improves the solver's ability to find higher-quality feasible solutions compared to single-reference approaches. This study highlights the potential of multi-reference heuristics in enhancing primal heuristics in MILP solvers.",
        "published": "2024-08-01T17:03:02Z",
        "link": "http://arxiv.org/abs/2408.00718v1",
        "categories": [
            "math.OC",
            "cs.MS",
            "90-08, 90C11, 90C57, 90C59,"
        ]
    },
    {
        "title": "Matrix-Free Finite Volume Kernels on a Dataflow Architecture",
        "authors": [
            "Ryuichi Sai",
            "Francois P. Hamon",
            "John Mellor-Crummey",
            "Mauricio Araya-Polo"
        ],
        "summary": "Fast and accurate numerical simulations are crucial for designing large-scale geological carbon storage projects ensuring safe long-term CO2 containment as a climate change mitigation strategy. These simulations involve solving numerous large and complex linear systems arising from the implicit Finite Volume (FV) discretization of PDEs governing subsurface fluid flow. Compounded with highly detailed geomodels, solving linear systems is computationally and memory expensive, and accounts for the majority of the simulation time. Modern memory hierarchies are insufficient to meet the latency and bandwidth needs of large-scale numerical simulations. Therefore, exploring algorithms that can leverage alternative and balanced paradigms, such as dataflow and in-memory computing is crucial. This work introduces a matrix-free algorithm to solve FV-based linear systems using a dataflow architecture to significantly minimize memory latency and bandwidth bottlenecks. Our implementation achieves two orders of magnitude speedup compared to a GPGPU-based reference implementation, and up to 1.2 PFlops on a single dataflow device.",
        "published": "2024-08-06T21:18:51Z",
        "link": "http://arxiv.org/abs/2408.03452v1",
        "categories": [
            "cs.MS",
            "physics.comp-ph"
        ]
    },
    {
        "title": "UGrid: An Efficient-And-Rigorous Neural Multigrid Solver for Linear PDEs",
        "authors": [
            "Xi Han",
            "Fei Hou",
            "Hong Qin"
        ],
        "summary": "Numerical solvers of Partial Differential Equations (PDEs) are of fundamental significance to science and engineering. To date, the historical reliance on legacy techniques has circumscribed possible integration of big data knowledge and exhibits sub-optimal efficiency for certain PDE formulations, while data-driven neural methods typically lack mathematical guarantee of convergence and correctness. This paper articulates a mathematically rigorous neural solver for linear PDEs. The proposed UGrid solver, built upon the principled integration of U-Net and MultiGrid, manifests a mathematically rigorous proof of both convergence and correctness, and showcases high numerical accuracy, as well as strong generalization power to various input geometry/values and multiple PDE formulations. In addition, we devise a new residual loss metric, which enables unsupervised training and affords more stability and a larger solution space over the legacy losses.",
        "published": "2024-08-09T03:46:35Z",
        "link": "http://arxiv.org/abs/2408.04846v1",
        "categories": [
            "math.NA",
            "cs.AI",
            "cs.LG",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Portability of Fortran's `do concurrent' on GPUs",
        "authors": [
            "Ronald M. Caplan",
            "Miko M. Stulajter",
            "Jon A. Linker",
            "Jeff Larkin",
            "Henry A. Gabb",
            "Shiquan Su",
            "Ivan Rodriguez",
            "Zachary Tschirhart",
            "Nicholas Malaya"
        ],
        "summary": "There is a continuing interest in using standard language constructs for accelerated computing in order to avoid (sometimes vendor-specific) external APIs. For Fortran codes, the {\\tt do concurrent} (DC) loop has been successfully demonstrated on the NVIDIA platform. However, support for DC on other platforms has taken longer to implement. Recently, Intel has added DC GPU offload support to its compiler, as has HPE for AMD GPUs. In this paper, we explore the current portability of using DC across GPU vendors using the in-production solar surface flux evolution code, HipFT. We discuss implementation and compilation details, including when/where using directive APIs for data movement is needed/desired compared to using a unified memory system. The performance achieved on both data center and consumer platforms is shown.",
        "published": "2024-08-14T22:45:46Z",
        "link": "http://arxiv.org/abs/2408.07843v1",
        "categories": [
            "cs.PL",
            "astro-ph.SR",
            "cs.CE",
            "cs.MS",
            "cs.PF"
        ]
    },
    {
        "title": "CEopt: A MATLAB Package for Non-convex Optimization with the   Cross-Entropy Method",
        "authors": [
            "Americo Cunha Jr",
            "Marcos Vinicius Issa",
            "Julio Cesar Basilio",
            "José Geraldo Telles Ribeiro"
        ],
        "summary": "This paper introduces CEopt (https://ceopt.org), a MATLAB tool leveraging the Cross-Entropy method for non-convex optimization. Due to the relative simplicity of the algorithm, it provides a kind of transparent ``gray-box'' optimization solver, with intuitive control parameters. Unique in its approach, CEopt effectively handles both equality and inequality constraints using an augmented Lagrangian method, offering robustness and scalability for moderately sized complex problems. Through select case studies, the package's applicability and effectiveness in various optimization scenarios are showcased, marking CEopt as a practical addition to optimization research and application toolsets.",
        "published": "2024-08-15T23:53:50Z",
        "link": "http://arxiv.org/abs/2409.00013v1",
        "categories": [
            "stat.CO",
            "cs.MS",
            "math.OC",
            "stat.ME",
            "90-04",
            "G.4"
        ]
    },
    {
        "title": "ideal.II: a Galerkin Space-Time Extension to the Finite Element Library   deal.II",
        "authors": [
            "Jan Philipp Thiele"
        ],
        "summary": "The C++ library deal.II provides classes and functions to solve stationary problems with finite elements on one- to threedimensional domains. It also supports the typical way to solve time-dependent problems using time-stepping schemes, either with an implementation by hand or through the use of external libraries like SUNDIALS. A different approach is the usage of finite elements in time as well, which results in space-time finite element schemes. The library ideal.II (short for instationary deal.II) aims to extend deal.II to simplify implementations of the second approach.",
        "published": "2024-08-16T16:46:42Z",
        "link": "http://arxiv.org/abs/2408.08840v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA",
            "35-04, 65-04, 65M60,"
        ]
    },
    {
        "title": "cpp11armadillo: An R Package to Use the Armadillo C++ Library",
        "authors": [
            "Mauricio Vargas Sepúlveda",
            "Jonathan Schneider Malamud"
        ],
        "summary": "This article introduces 'cpp11armadillo', a new R package that integrates the powerful Armadillo C++ library for linear algebra into the R programming environment. Targeted primarily at social scientists and other non-programmers, this article explains the computational benefits of moving code to C++ in terms of speed and syntax. We provide a comprehensive overview of Armadillo's capabilities, highlighting its user-friendly syntax akin to MATLAB and its efficiency for computationally intensive tasks. The 'cpp11armadillo' package simplifies a part of the process of using C++ within R by offering additional ease of integration for those who require high-performance linear algebra operations in their R workflows. This work aims to bridge the gap between computational efficiency and accessibility, making advanced linear algebra operations more approachable for R users without extensive programming backgrounds.",
        "published": "2024-08-19T00:07:39Z",
        "link": "http://arxiv.org/abs/2408.11074v3",
        "categories": [
            "cs.MS",
            "cs.PL",
            "stat.CO",
            "D.1.5; D.3.3; F.2.1"
        ]
    },
    {
        "title": "Fast Algorithms and Implementations for Computing the Minimum Distance   of Quantum Codes",
        "authors": [
            "Fernando Hernando",
            "Gregorio Quintana-Ortí",
            "Markus Grassl"
        ],
        "summary": "The distance of a stabilizer quantum code is a very important feature since it determines the number of errors that can be detected and corrected. We present three new fast algorithms and implementations for computing the symplectic distance of the associated classical code. Our new algorithms are based on the Brouwer-Zimmermann algorithm. Our experimental study shows that these new implementations are much faster than current state-of-the-art licensed implementations on single-core processors, multicore processors, and shared-memory multiprocessors. In the most computationally-demanding cases, the performance gain in the computational time can be larger than one order of magnitude. The experimental study also shows a good scalability on shared-memory parallel architectures.",
        "published": "2024-08-20T11:24:30Z",
        "link": "http://arxiv.org/abs/2408.10743v1",
        "categories": [
            "quant-ph",
            "cs.CE",
            "cs.IT",
            "cs.MS",
            "math.IT",
            "81-04, 81-08, 94B05, 94B60, 94B99",
            "G.4"
        ]
    },
    {
        "title": "Solving the Convex Flow Problem",
        "authors": [
            "Theo Diamandis",
            "Guillermo Angeris"
        ],
        "summary": "In this paper, we introduce the solver ConvexFlows for the convex flow problem first defined in the authors' previous work. In this problem, we aim to optimize a concave utility function depending on the flows over a graph. However, unlike the classic network flows literature, we also allow for a concave relationship between the input and output flows of edges. This nonlinear gain describes many physical phenomena, including losses in power network transmission lines. We outline an efficient algorithm for solving this problem which parallelizes over the graph edges. We provide an open source implementation of this algorithm in the Julia programming language package ConvexFlows.jl. This package includes an interface to easily specify these flow problems. We conclude by walking through an example of solving for an optimal power flow using ConvexFlows.",
        "published": "2024-08-20T17:48:38Z",
        "link": "http://arxiv.org/abs/2408.11040v1",
        "categories": [
            "math.OC",
            "cs.MS"
        ]
    },
    {
        "title": "RAO-SS: A Prototype of Run-time Auto-tuning Facility for Sparse Direct   Solvers",
        "authors": [
            "Takahiro Katagiri",
            "Yoshinori Ishii",
            "Hiroki Honda"
        ],
        "summary": "In this paper, a run-time auto-tuning method for performance parameters according to input matrices is proposed. RAO-SS (Run-time Auto-tuning Optimizer for Sparse Solvers), which is a prototype of auto-tuning software using the proposed method, is also evaluated. The RAO-SS is implemented with the Autopilot, which is middle-ware to support run-time auto-tuning with fuzzy logic function. The target numerical library is the SuperLU, which is a sparse direct solver for linear equations. The result indicated that: (1) the speedup factors of 1.2 for average and 3.6 for maximum to default executions were obtained; (2) the software overhead of the Autopilot can be ignored in RAO-SS.",
        "published": "2024-08-21T03:05:27Z",
        "link": "http://arxiv.org/abs/2408.11880v1",
        "categories": [
            "cs.MS",
            "cs.PF"
        ]
    },
    {
        "title": "PySLSQP: A transparent Python package for the SLSQP optimization   algorithm modernized with utilities for visualization and post-processing",
        "authors": [
            "Anugrah Jo Joshy",
            "John T. Hwang"
        ],
        "summary": "PySLSQP is a seamless interface for using the SLSQP algorithm from Python. It wraps the original SLSQP Fortran code sourced from the SciPy repository and provides a host of new features to improve the research utility of the original algorithm. Some of the additional features offered by PySLSQP include auto-generation of unavailable derivatives using finite differences, independent scaling of the problem variables and functions, access to internal optimization data, live-visualization, saving optimization data from each iteration, warm/hot restarting of optimization, and various other utilities for post-processing.",
        "published": "2024-08-24T01:24:11Z",
        "link": "http://arxiv.org/abs/2408.13420v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA",
            "G.1.6; J.2"
        ]
    },
    {
        "title": "The applicability of equal area partitions of the unit sphere",
        "authors": [
            "Paul Leopardi"
        ],
        "summary": "This paper addresses the idea of the applicability of mathematics, using, as a case study, a construction and software package that partition the unit sphere into regions of equal area. The paper assesses the applicability of this construction and software by examining citing works, including papers, dissertations and software.",
        "published": "2024-08-24T02:12:31Z",
        "link": "http://arxiv.org/abs/2408.13434v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA",
            "41-02, 65-02, 65D15"
        ]
    },
    {
        "title": "TorchDA: A Python package for performing data assimilation with deep   learning forward and transformation functions",
        "authors": [
            "Sibo Cheng",
            "Jinyang Min",
            "Che Liu",
            "Rossella Arcucci"
        ],
        "summary": "Data assimilation techniques are often confronted with challenges handling complex high dimensional physical systems, because high precision simulation in complex high dimensional physical systems is computationally expensive and the exact observation functions that can be applied in these systems are difficult to obtain. It prompts growing interest in integrating deep learning models within data assimilation workflows, but current software packages for data assimilation cannot handle deep learning models inside. This study presents a novel Python package seamlessly combining data assimilation with deep neural networks to serve as models for state transition and observation functions. The package, named TorchDA, implements Kalman Filter, Ensemble Kalman Filter (EnKF), 3D Variational (3DVar), and 4D Variational (4DVar) algorithms, allowing flexible algorithm selection based on application requirements. Comprehensive experiments conducted on the Lorenz 63 and a two-dimensional shallow water system demonstrate significantly enhanced performance over standalone model predictions without assimilation. The shallow water analysis validates data assimilation capabilities mapping between different physical quantity spaces in either full space or reduced order space. Overall, this innovative software package enables flexible integration of deep learning representations within data assimilation, conferring a versatile tool to tackle complex high dimensional dynamical systems across scientific domains.",
        "published": "2024-08-30T20:30:34Z",
        "link": "http://arxiv.org/abs/2409.00244v1",
        "categories": [
            "cs.MS",
            "cs.LG"
        ]
    },
    {
        "title": "Welding R and C++: A Tale of Two Programming Languages",
        "authors": [
            "Mauricio Vargas Sepulveda"
        ],
        "summary": "This article compares `cpp11armadillo` and `cpp11eigen`, new R packages that integrate the powerful Armadillo and Eigen C++ libraries for linear algebra into the R programming environment. This article provides a detailed comparison between Armadillo and Eigen speed and syntax. The goal of these packages is to simplify a part of the process of solving bottlenecks by using C++ within R, these offer additional ease of integration for users who require high-performance linear algebra operations in their R workflows. This document aims to discuss the tradeoff between computational efficiency and accessibility.",
        "published": "2024-09-01T00:09:00Z",
        "link": "http://arxiv.org/abs/2409.00568v2",
        "categories": [
            "cs.MS",
            "cs.PL",
            "stat.CO"
        ]
    },
    {
        "title": "A prony method variant which surpasses the Adaptive LMS filter in the   output signal's representation of input",
        "authors": [
            "Parthasarathy Srinivasan"
        ],
        "summary": "The Prony method for approximating signals comprising sinusoidal/exponential components is known through the pioneering work of Prony in his seminal dissertation in the year 1795. However, the Prony method saw the light of real world application only upon the advent of the computational era, which made feasible the extensive numerical intricacies and labor which the method demands inherently. The Adaptive LMS Filter which has been the most pervasive method for signal filtration and approximation since its inception in 1965 does not provide a consistently assured level of highly precise results as the extended experiment in this work proves. As a remedy this study improvises upon the Prony method by observing that a better (more precise) computational approximation can be obtained under the premise that adjustment can be made for computational error , in the autoregressive model setup in the initial step of the Prony computation itself. This adjustment is in proportion to the deviation of the coefficients in the same autoregressive model. The results obtained by this improvisation live up to the expectations of obtaining consistency and higher value in the precision of the output (recovered signal) approximations as shown in this current work and as compared with the results obtained using the Adaptive LMS Filter.",
        "published": "2024-09-02T14:10:26Z",
        "link": "http://arxiv.org/abs/2409.01272v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Toward Capturing Genetic Epistasis From Multivariate Genome-Wide   Association Studies Using Mixed-Precision Kernel Ridge Regression",
        "authors": [
            "Hatem Ltaief",
            "Rabab Alomairy",
            "Qinglei Cao",
            "Jie Ren",
            "Lotfi Slim",
            "Thorsten Kurth",
            "Benedikt Dorschner",
            "Salim Bougouffa",
            "Rached Abdelkhalak",
            "David E. Keyes"
        ],
        "summary": "We exploit the widening margin in tensor-core performance between [FP64/FP32/FP16/INT8,FP64/FP32/FP16/FP8/INT8] on NVIDIA [Ampere,Hopper] GPUs to boost the performance of output accuracy-preserving mixed-precision computation of Genome-Wide Association Studies (GWAS) of 305K patients from the UK BioBank, the largest-ever GWAS cohort studied for genetic epistasis using a multivariate approach. Tile-centric adaptive-precision linear algebraic techniques motivated by reducing data motion gain enhanced significance with low-precision GPU arithmetic. At the core of Kernel Ridge Regression (KRR) techniques for GWAS lie compute-bound cubic-complexity matrix operations that inhibit scaling to aspirational dimensions of the population, genotypes, and phenotypes. We accelerate KRR matrix generation by redesigning the computation for Euclidean distances to engage INT8 tensor cores while exploiting symmetry.We accelerate solution of the regularized KRR systems by deploying a new four-precision Cholesky-based solver, which, at 1.805 mixed-precision ExaOp/s on a nearly full Alps system, outperforms the state-of-the-art CPU-only REGENIE GWAS software by five orders of magnitude.",
        "published": "2024-09-03T08:50:42Z",
        "link": "http://arxiv.org/abs/2409.01712v1",
        "categories": [
            "q-bio.GN",
            "cs.AR",
            "cs.LG",
            "cs.MS",
            "cs.PF"
        ]
    },
    {
        "title": "LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch",
        "authors": [
            "Xiaoyuan Zhang",
            "Liang Zhao",
            "Yingying Yu",
            "Xi Lin",
            "Yifan Chen",
            "Han Zhao",
            "Qingfu Zhang"
        ],
        "summary": "Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, learning under fairness or robustness constraints, etc. Instead of reducing multiple objective functions into a scalar objective, MOPs aim to optimize for the so-called Pareto optimality or Pareto set learning, which involves optimizing more than one objective function simultaneously, over models with thousands / millions of parameters. Existing benchmark libraries for MOPs mainly focus on evolutionary algorithms, most of which are zeroth-order / meta-heuristic methods that do not effectively utilize higher-order information from objectives and cannot scale to large-scale models with thousands / millions of parameters. In light of the above gap, this paper introduces LibMOON, the first multiobjective optimization library that supports state-of-the-art gradient-based methods, provides a fair benchmark, and is open-sourced for the community.",
        "published": "2024-09-04T07:44:43Z",
        "link": "http://arxiv.org/abs/2409.02969v3",
        "categories": [
            "cs.MS",
            "cs.LG",
            "math.OC"
        ]
    },
    {
        "title": "QHDOPT: A Software for Nonlinear Optimization with Quantum Hamiltonian   Descent",
        "authors": [
            "Samuel Kushnir",
            "Jiaqi Leng",
            "Yuxiang Peng",
            "Lei Fan",
            "Xiaodi Wu"
        ],
        "summary": "We develop an open-source, end-to-end software (named QHDOPT), which can solve nonlinear optimization problems using the quantum Hamiltonian descent (QHD) algorithm. QHDOPT offers an accessible interface and automatically maps tasks to various supported quantum backends (i.e., quantum hardware machines). These features enable users, even those without prior knowledge or experience in quantum computing, to utilize the power of existing quantum devices for nonlinear and nonconvex optimization tasks. In its intermediate compilation layer, QHDOPT employs SimuQ, an efficient interface for Hamiltonian-oriented programming, to facilitate multiple algorithmic specifications and ensure compatible cross-hardware deployment. The detailed documentation of QHDOPT is available at https://github.com/jiaqileng/QHDOPT.",
        "published": "2024-09-04T23:11:25Z",
        "link": "http://arxiv.org/abs/2409.03121v1",
        "categories": [
            "quant-ph",
            "cs.MS",
            "math.OC"
        ]
    },
    {
        "title": "OGRePy: An Object-Oriented General Relativity Package for Python",
        "authors": [
            "Barak Shoshany"
        ],
        "summary": "We present OGRePy, the official Python port of the popular Mathematica tensor calculus package OGRe (Object-Oriented General Relativity) - a powerful, yet user-friendly, tool for advanced tensor calculations in mathematics and physics, especially suitable for general relativity. The Python port uses the same robust and performance-oriented algorithms as the original package, and retains its core design principles. However, its truly object-oriented interface, enabled by Python, is more intuitive and flexible than the original Mathematica implementation. It utilizes SymPy for symbolic computations and Jupyter as a notebook interface. OGRePy allows calculating arbitrary tensor formulas using any combination of addition, multiplication by scalar, trace, contraction, partial derivative, covariant derivative, and permutation of indices. Transformations of the tensor components between different index configurations and/or coordinate systems are performed seamlessly behind the scenes as needed, eliminating user error due to combining incompatible representations, and guaranteeing consistent results. In addition, the package provides facilities for easily calculating various curvature tensors and geodesic equations in multiple representations. This paper presents the main features of the package in great detail, including many examples of its use in the context of general relativity research.",
        "published": "2024-09-05T03:40:27Z",
        "link": "http://arxiv.org/abs/2409.03803v1",
        "categories": [
            "gr-qc",
            "cs.MS",
            "cs.SC",
            "math.DG",
            "G.4; I.1; J.2"
        ]
    },
    {
        "title": "forester: A Tree-Based AutoML Tool in R",
        "authors": [
            "Hubert Ruczyński",
            "Anna Kozak"
        ],
        "summary": "The majority of automated machine learning (AutoML) solutions are developed in Python, however a large percentage of data scientists are associated with the R language. Unfortunately, there are limited R solutions available. Moreover high entry level means they are not accessible to everyone, due to required knowledge about machine learning (ML). To fill this gap, we present the forester package, which offers ease of use regardless of the user's proficiency in the area of machine learning.   The forester is an open-source AutoML package implemented in R designed for training high-quality tree-based models on tabular data. It fully supports binary and multiclass classification, regression, and partially survival analysis tasks. With just a few functions, the user is capable of detecting issues regarding the data quality, preparing the preprocessing pipeline, training and tuning tree-based models, evaluating the results, and creating the report for further analysis.",
        "published": "2024-09-07T10:39:10Z",
        "link": "http://arxiv.org/abs/2409.04789v1",
        "categories": [
            "cs.LG",
            "cs.MS",
            "stat.ME"
        ]
    },
    {
        "title": "Differentiable programming across the PDE and Machine Learning barrier",
        "authors": [
            "Nacime Bouziani",
            "David A. Ham",
            "Ado Farsi"
        ],
        "summary": "The combination of machine learning and physical laws has shown immense potential for solving scientific problems driven by partial differential equations (PDEs) with the promise of fast inference, zero-shot generalisation, and the ability to discover new physics. Examples include the use of fundamental physical laws as inductive bias to machine learning algorithms, also referred to as physics-driven machine learning, and the application of machine learning to represent features not represented in the differential equations such as closures for unresolved spatiotemporal scales. However, the simulation of complex physical systems by coupling advanced numerics for PDEs with state-of-the-art machine learning demands the composition of specialist PDE solving frameworks with industry-standard machine learning tools. Hand-rolling either the PDE solver or the neural net will not cut it. In this work, we introduce a generic differentiable programming abstraction that provides scientists and engineers with a highly productive way of specifying end-to-end differentiable models coupling machine learning and PDE-based components, while relying on code generation for high performance. Our interface automates the coupling of arbitrary PDE-based systems and machine learning models and unlocks new applications that could not hitherto be tackled, while only requiring trivial changes to existing code. Our framework has been adopted in the Firedrake finite-element library and supports the PyTorch and JAX ecosystems, as well as downstream libraries.",
        "published": "2024-09-09T21:36:38Z",
        "link": "http://arxiv.org/abs/2409.06085v1",
        "categories": [
            "cs.LG",
            "cs.MS",
            "cs.NA",
            "math.NA",
            "physics.comp-ph"
        ]
    },
    {
        "title": "A tutorial on automatic differentiation with complex numbers",
        "authors": [
            "Nicholas Krämer"
        ],
        "summary": "Automatic differentiation is everywhere, but there exists only minimal documentation of how it works in complex arithmetic beyond stating \"derivatives in $\\mathbb{C}^d$\" $\\cong$ \"derivatives in $\\mathbb{R}^{2d}$\" and, at best, shallow references to Wirtinger calculus. Unfortunately, the equivalence $\\mathbb{C}^d \\cong \\mathbb{R}^{2d}$ becomes insufficient as soon as we need to derive custom gradient rules, e.g., to avoid differentiating \"through\" expensive linear algebra functions or differential equation simulators. To combat such a lack of documentation, this article surveys forward- and reverse-mode automatic differentiation with complex numbers, covering topics such as Wirtinger derivatives, a modified chain rule, and different gradient conventions while explicitly avoiding holomorphicity and the Cauchy--Riemann equations (which would be far too restrictive). To be precise, we will derive, explain, and implement a complex version of Jacobian-vector and vector-Jacobian products almost entirely with linear algebra without relying on complex analysis or differential geometry. This tutorial is a call to action, for users and developers alike, to take complex values seriously when implementing custom gradient propagation rules -- the manuscript explains how.",
        "published": "2024-09-10T14:04:58Z",
        "link": "http://arxiv.org/abs/2409.06752v3",
        "categories": [
            "cs.MS",
            "cs.LG",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "Introducing UNIQuE: The Unconventional Noiseless Intermediate Quantum   Emulator",
        "authors": [
            "Reece Robertson",
            "Dan Ventura"
        ],
        "summary": "We implement the first open-source quantum computing emulator that includes arithmetic operations, the quantum Fourier transform, and quantum phase estimation. The emulator provides significant savings in both temporal and spatial resources compared to simulation, and these computational advantages are verified through comparison to the Intel Quantum Simulator. We also demonstrate how to use the emulator to implement Shor's algorithm and use it to solve a nontrivial factoring problem. This demonstrates that emulation can make quantum computing more accessible than simulation or noisy hardware by allowing researchers to study the behavior of algorithms on large problems in a noiseless environment.",
        "published": "2024-09-11T04:24:51Z",
        "link": "http://arxiv.org/abs/2409.07000v1",
        "categories": [
            "quant-ph",
            "cs.MS"
        ]
    },
    {
        "title": "MPPI-Generic: A CUDA Library for Stochastic Optimization",
        "authors": [
            "Bogdan Vlahov",
            "Jason Gibson",
            "Manan Gandhi",
            "Evangelos A. Theodorou"
        ],
        "summary": "This paper introduces a new C++/CUDA library for GPU-accelerated stochastic optimization called MPPI-Generic. It provides implementations of Model Predictive Path Integral control, Tube-Model Predictive Path Integral Control, and Robust Model Predictive Path Integral Control, and allows for these algorithms to be used across many pre-existing dynamics models and cost functions. Furthermore, researchers can create their own dynamics models or cost functions following our API definitions without needing to change the actual Model Predictive Path Integral Control code. Finally, we compare computational performance to other popular implementations of Model Predictive Path Integral Control over a variety of GPUs to show the real-time capabilities our library can allow for. Library code can be found at: https://acdslab.github.io/mppi-generic-website/ .",
        "published": "2024-09-11T18:31:33Z",
        "link": "http://arxiv.org/abs/2409.07563v1",
        "categories": [
            "cs.MS",
            "cs.DC",
            "cs.RO",
            "cs.SY",
            "eess.SY"
        ]
    },
    {
        "title": "A Unified Funnel Restoration SQP Algorithm",
        "authors": [
            "David Kiessling",
            "Sven Leyffer",
            "Charlie Vanaret"
        ],
        "summary": "We consider nonlinearly constrained optimization problems and discuss a generic double-loop framework consisting of four algorithmic ingredients that unifies a broad range of nonlinear optimization solvers. This framework has been implemented in the open-source solver Uno, a Swiss Army knife-like C++ optimization framework that unifies many nonlinearly constrained nonconvex optimization solvers. We illustrate the framework with a sequential quadratic programming (SQP) algorithm that maintains an acceptable upper bound on the constraint violation, called a funnel, that is monotonically decreased to control the feasibility of the iterates. Infeasible quadratic subproblems are handled by a feasibility restoration strategy. Globalization is controlled by a line search or a trust-region method. We prove global convergence of the trust-region funnel SQP method, building on known results from filter methods. We implement the algorithm in Uno, and we provide extensive test results for the trust-region line-search funnel SQP on small CUTEst instances.",
        "published": "2024-09-13T21:42:06Z",
        "link": "http://arxiv.org/abs/2409.09208v1",
        "categories": [
            "math.OC",
            "cs.MS"
        ]
    },
    {
        "title": "Computing Arrangements of Hypersurfaces",
        "authors": [
            "Paul Breiding",
            "Bernd Sturmfels",
            "Kexin Wang"
        ],
        "summary": "We present a Julia package HypersurfaceRegions.jl for computing all connected components in the complement of an arrangement of real algebraic hypersurfaces in $\\mathbb{R}^n$.",
        "published": "2024-09-15T06:11:57Z",
        "link": "http://arxiv.org/abs/2409.09622v1",
        "categories": [
            "cs.MS",
            "cs.CG",
            "math.AG"
        ]
    },
    {
        "title": "OpenACC offloading of the MFC compressible multiphase flow solver on AMD   and NVIDIA GPUs",
        "authors": [
            "Benjamin Wilfong",
            "Anand Radhakrishnan",
            "Henry A. Le Berre",
            "Steve Abbott",
            "Reuben D. Budiardja",
            "Spencer H. Bryngelson"
        ],
        "summary": "GPUs are the heart of the latest generations of supercomputers. We efficiently accelerate a compressible multiphase flow solver via OpenACC on NVIDIA and AMD Instinct GPUs. Optimization is accomplished by specifying the directive clauses 'gang vector' and 'collapse'. Further speedups of six and ten times are achieved by packing user-defined types into coalesced multidimensional arrays and manual inlining via metaprogramming. Additional optimizations yield seven-times speedup in array packing and thirty-times speedup of select kernels on Frontier. Weak scaling efficiencies of 97% and 95% are observed when scaling to 50% of Summit and 95% of Frontier. Strong scaling efficiencies of 84% and 81% are observed when increasing the device count by a factor of 8 and 16 on V100 and MI250X hardware. The strong scaling efficiency of AMD's MI250X increases to 92% when increasing the device count by a factor of 16 when GPU-aware MPI is used for communication.",
        "published": "2024-09-16T21:05:45Z",
        "link": "http://arxiv.org/abs/2409.10729v1",
        "categories": [
            "physics.flu-dyn",
            "cs.MS",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Fast Symbolic Integer-Linear Spectra",
        "authors": [
            "Jonny Luntzel",
            "Abraham Miller"
        ],
        "summary": "Here we contribute a fast symbolic eigenvalue solver for matrices whose eigenvalues are $\\mathbb{Z}$-linear combinations of their entries, alongside efficient general and stochastic $M^{X}$ generators. Users can interact with a few degrees of freedom to create linear operators, making high-dimensional symbolic analysis feasible for when numerical analyses are insufficient.",
        "published": "2024-09-18T06:59:39Z",
        "link": "http://arxiv.org/abs/2410.09053v2",
        "categories": [
            "math.RA",
            "cs.MS",
            "cs.NA",
            "cs.SC",
            "math.NA"
        ]
    },
    {
        "title": "Fitting Multilevel Factor Models",
        "authors": [
            "Tetiana Parshakova",
            "Trevor Hastie",
            "Stephen Boyd"
        ],
        "summary": "We examine a special case of the multilevel factor model, with covariance given by multilevel low rank (MLR) matrix~\\cite{parshakova2023factor}. We develop a novel, fast implementation of the expectation-maximization (EM) algorithm, tailored for multilevel factor models, to maximize the likelihood of the observed data. This method accommodates any hierarchical structure and maintains linear time and storage complexities per iteration. This is achieved through a new efficient technique for computing the inverse of the positive definite MLR matrix. We show that the inverse of an invertible PSD MLR matrix is also an MLR matrix with the same sparsity in factors, and we use the recursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of the inverse. Additionally, we present an algorithm that computes the Cholesky factorization of an expanded matrix with linear time and space complexities, yielding the covariance matrix as its Schur complement. This paper is accompanied by an open-source package that implements the proposed methods.",
        "published": "2024-09-18T15:39:12Z",
        "link": "http://arxiv.org/abs/2409.12067v2",
        "categories": [
            "stat.ML",
            "cs.LG",
            "cs.MS",
            "stat.CO",
            "62H12",
            "G.4"
        ]
    },
    {
        "title": "Some new techniques to use in serial sparse Cholesky factorization   algorithms",
        "authors": [
            "M. Ozan Karsavuran",
            "Esmond G. Ng",
            "Barry W. Peyton",
            "Jonathan L. Peyton"
        ],
        "summary": "We present a new variant of serial right-looking supernodal sparse Cholesky factorization (RL). Our comparison of RL with the multifrontal method confirms that RL is simpler, slightly faster, and requires slightly less storage. The key to the rest of the work in this paper is recent work on reordering columns within supernodes so that the dense off-diagonal blocks in the factor matrix joining pairs of supernodes are fewer and larger. We present a second new variant of serial right-looking supernodal sparse Cholesky factorization (RLB), where this one is specifically designed to exploit fewer and larger off-diagonal blocks in the factor matrix obtained by reordering within supernodes. A key distinction found in RLB is that it uses no floating-point working storage and performs no assembly operations. Our key finding is that RLB is unequivocally faster than its competitors. Indeed, RLB is consistently, but modestly, faster than its competitors whenever Intel's MKL sequential BLAS are used. More importantly, RLB is substantially faster than its competitors whenever Intel's MKL multithreaded BLAS are used. Finally, RLB using the multithreaded BLAS achieves impressive speedups over RLB using the sequential BLAS.",
        "published": "2024-09-19T21:20:00Z",
        "link": "http://arxiv.org/abs/2409.13090v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "QHyper: an integration library for hybrid quantum-classical optimization",
        "authors": [
            "Tomasz Lamża",
            "Justyna Zawalska",
            "Kacper Jurek",
            "Mariusz Sterzel",
            "Katarzyna Rycerz"
        ],
        "summary": "We propose the QHyper library, which is aimed at researchers working on computational experiments with a variety of quantum combinatorial optimization solvers. The library offers a simple and extensible interface for formulating combinatorial optimization problems, selecting and running solvers, and optimizing hyperparameters. The supported solver set includes variational gate-based algorithms, quantum annealers, and classical solutions. The solvers can be combined with provided local and global (hyper)optimizers. The main features of the library are its extensibility on different levels of use as well as a straightforward and flexible experiment configuration format presented in the paper.",
        "published": "2024-09-24T09:47:28Z",
        "link": "http://arxiv.org/abs/2409.15926v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "A method of using RSVD in residual calculation of LowBit GEMM",
        "authors": [
            "Hongyaoxing Gu"
        ],
        "summary": "The advancements of hardware technology in recent years has brought many possibilities for low-precision applications. However, the use of low precision can introduce significant computational errors, posing a considerable challenge to maintaining the computational accuracy.   We propose low-rank residuals quantized matrix multiplication(LRQMM) method which introduces low-rank approximation in residual compensation for dense low precision quantization matrix multiplication. It can bring several times accuracy improvement with only BLAS-2 level extra time overhead. Moreover, LRQMM is a completely data-free quantization method that does not require additional data for pre-training. And it only works with low precision GEMM operator, which is easy to couple with other methods.   Through experimentation, LRQMM can reduce the error of direct quantized matrix multiplication by 1~2 orders of magnitude, when dealing with larger matrix sizes, the computational speed is only reduced by approximately 20\\%. In deep learning networks, LRQMM-4bit achieves 61.8% ImageNet Top-1 accuracy in Resnet-50, while the Direct Quant accuracy is only 8.3%.",
        "published": "2024-09-27T14:16:35Z",
        "link": "http://arxiv.org/abs/2409.18772v1",
        "categories": [
            "cs.MS",
            "cs.LG"
        ]
    },
    {
        "title": "BDDC Preconditioning on GPUs for Cardiac Simulations",
        "authors": [
            "Fritz Goebel",
            "Terry Cojean",
            "Hartwig Anzt"
        ],
        "summary": "In order to understand cardiac arrhythmia, computer models for electrophysiology are essential. In the EuroHPC MicroCARD project, we adapt the current models and leverage modern computing resources to model diseased hearts and their microstructure accurately. Towards this objective, we develop a portable, highly efficient, and performing BDDC preconditioner and solver implementation, demonstrating scalability with over 90% efficiency on up to 100 GPUs.",
        "published": "2024-10-01T08:21:10Z",
        "link": "http://arxiv.org/abs/2410.14786v1",
        "categories": [
            "cs.CE",
            "cs.MS"
        ]
    },
    {
        "title": "Efficient $1$-bit tensor approximations",
        "authors": [
            "Alex W. Neal Riasanovsky",
            "Sarah El Kazdadi"
        ],
        "summary": "We present a spatially efficient decomposition of matrices and arbitrary-order tensors as linear combinations of tensor products of $\\{-1, 1\\}$-valued vectors. For any matrix $A \\in \\mathbb{R}^{m \\times n}$, $$A - R_w = S_w C_w T_w^\\top = \\sum_{j=1}^w c_j \\cdot \\mathbf{s}_j \\mathbf{t}_j^\\top$$ is a {\\it $w$-width signed cut decomposition of $A$}. Here $C_w = \"diag\"(\\mathbf{c}_w)$ for some $\\mathbf{c}_w \\in \\mathbb{R}^w,$ and $S_w, T_w$, and the vectors $\\mathbf{s}_j, \\mathbf{t}_j$ are $\\{-1, 1\\}$-valued. To store $(S_w, T_w, C_w)$, we may pack $w \\cdot (m + n)$ bits, and require only $w$ floating point numbers. As a function of $w$, $\\|R_w\\|_F$ exhibits exponential decay when applied to #f32 matrices with i.i.d. $\\mathcal N (0, 1)$ entries. Choosing $w$ so that $(S_w, T_w, C_w)$ has the same memory footprint as a \\textit{f16} or \\textit{bf16} matrix, the relative error is comparable. Our algorithm yields efficient signed cut decompositions in $20$ lines of pseudocode. It reflects a simple modification from a celebrated 1999 paper [1] of Frieze and Kannan. As a first application, we approximate the weight matrices in the open \\textit{Mistral-7B-v0.1} Large Language Model to a $50\\%$ spatial compression. Remarkably, all $226$ remainder matrices have a relative error $<6\\%$ and the expanded model closely matches \\textit{Mistral-7B-v0.1} on the {\\it huggingface} leaderboard [2]. Benchmark performance degrades slowly as we reduce the spatial compression from $50\\%$ to $25\\%$. We optimize our open source \\textit{rust} implementation [3] with \\textit{simd} instructions on \\textit{avx2} and \\textit{avx512} architectures. We also extend our algorithm from matrices to tensors of arbitrary order and use it to compress a picture of the first author's cat Angus.",
        "published": "2024-10-02T17:56:32Z",
        "link": "http://arxiv.org/abs/2410.01799v1",
        "categories": [
            "math.CO",
            "cs.LG",
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "A C++ implementation of the discrete adjoint sensitivity analysis method   for explicit adaptive Runge-Kutta methods enabled by automatic adjoint   differentiation and SIMD vectorization",
        "authors": [
            "Rui Martins",
            "Evgeny Lakshtanov"
        ],
        "summary": "A C++ library for sensitivity analysis of optimisation problems involving ordinary differential equations (ODEs) enabled by automatic differentiation (AD) and SIMD (Single Instruction, Multiple data) vectorization is presented. The discrete adjoint sensitivity analysis method is implemented for adaptive explicit Runge-Kutta (ERK) methods. Automatic adjoint differentiation (AAD) is employed for efficient evaluations of products of vectors and the Jacobian matrix of the right hand side of the ODE system. This approach avoids the low-level drawbacks of the black box approach of employing AAD on the entire ODE solver and opens the possibility to leverage parallelization. SIMD vectorization is employed to compute the vector-Jacobian products concurrently. We study the performance of other methods and implementations of sensitivity analysis and we find that our algorithm presents a small advantage compared to equivalent existing software.",
        "published": "2024-10-02T18:09:48Z",
        "link": "http://arxiv.org/abs/2410.01911v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA",
            "34-04 (Primary) 65L06, 65K10, 90C31 (Secondary)",
            "G.1; G.4"
        ]
    },
    {
        "title": "Exact sensitivity analysis of Markov reward processes via algebraic   geometry",
        "authors": [
            "Timothy C. Y. Chan",
            "Muhammad Maaz"
        ],
        "summary": "We introduce a new approach for deterministic sensitivity analysis of Markov reward processes, commonly used in cost-effectiveness analyses, via reformulation into a polynomial system. Our approach leverages cylindrical algebraic decomposition (CAD), a technique arising from algebraic geometry that provides an exact description of all solutions to a polynomial system. While it is typically intractable to build a CAD for systems with more than a few variables, we show that a special class of polynomial systems, which includes the polynomials arising from Markov reward processes, can be analyzed much more tractably. We establish several theoretical results about such systems and develop a specialized algorithm to construct their CAD, which allows us to perform exact, multi-way sensitivity analysis for common health economic analyses. We develop an open-source software package that implements our algorithm. Finally, we apply it to two case studies, one with synthetic data and one that re-analyzes a previous cost-effectiveness analysis from the literature, demonstrating advantages of our approach over standard techniques. Our software and code are available at: \\url{https://github.com/mmaaz-git/markovag}.",
        "published": "2024-10-07T20:08:02Z",
        "link": "http://arxiv.org/abs/2410.05471v1",
        "categories": [
            "math.OC",
            "cs.MS",
            "math.AG",
            "math.PR"
        ]
    },
    {
        "title": "lintsampler: Easy random sampling via linear interpolation",
        "authors": [
            "Aneesh P. Naik",
            "Michael S. Petersen"
        ],
        "summary": "'lintsampler' provides a Python implementation of a technique we term 'linear interpolant sampling': an algorithm to efficiently draw pseudo-random samples from an arbitrary probability density function (PDF). First, the PDF is evaluated on a grid-like structure. Then, it is assumed that the PDF can be approximated between grid vertices by the (multidimensional) linear interpolant. With this assumption, random samples can be efficiently drawn via inverse transform sampling. lintsampler is primarily written with 'numpy', drawing some additional functionality from 'scipy'. Under the most basic usage of lintsampler, the user provides a Python function defining the target PDF and some parameters describing a grid-like structure to the 'LintSampler' class, and is then able to draw samples via the 'sample' method. Additionally, there is functionality for the user to set the random seed, employ quasi-Monte Carlo sampling, or sample within a premade grid ('DensityGrid') or tree ('DensityTree') structure.",
        "published": "2024-10-08T08:42:29Z",
        "link": "http://arxiv.org/abs/2410.05811v1",
        "categories": [
            "stat.CO",
            "astro-ph.IM",
            "cs.MS",
            "math.PR"
        ]
    },
    {
        "title": "BLAS-like Interface for Binary Tensor Contractions",
        "authors": [
            "Niklas Hörnblad"
        ],
        "summary": "In the world of linear algebra computation, a well-established standard exists called BLAS(Basic Linear Algebra Subprograms). This standard has been crucial for the development of software using linear algebra operations. Its benefits include portability with efficiency and mitigation of suboptimal re-implementations of linear algebra operations. Multilinear algebra is an extension of linear algebra in which the central objects are tensors, which are generalizations of vectors and matrices. Though tensor operations are becoming more common, they do not have a standard like BLAS. Such standardization would be beneficial and decrease the now-visible replication of work, as many libraries nowadays use their own implementations. This master thesis aims to work towards such a standard by discovering whether or not a BLAS-like interface is possible for the operation binary tensor contraction. To answer this, an interface has been developed in the programming language C together with an implementation and tested to see if it would be sufficient. The interface developed is:   xGETT(RANKA, EXTA, INCA, A, RANKB, EXTB, INCB, B, CONTS, CONTA, CONTB, PERM, INCC, C)   with the implementation and tests, it has been deemed sufficient as a BLAS-like interface for binary tensor contractions and possible to use in a BLAS-like standardization for tensor operations.",
        "published": "2024-10-09T11:03:14Z",
        "link": "http://arxiv.org/abs/2410.06770v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Fast Sound Error Bounds for Mixed-Precision Real Evaluation",
        "authors": [
            "Artem Yadrov",
            "Pavel Panchekha"
        ],
        "summary": "Evaluating real-valued expressions to high precision is a key building block in computational mathematics, physics, and numerics. A typical implementation uses a uniform precision for each operation, and doubles that precision until the real result can be bounded to some sufficiently narrow interval. However, this is wasteful: usually only a few operations really need to be performed at high precision, and the bulk of the expression could use much lower precision. Uniform precision can also waste iterations discovering the necessary precision and then still overestimate by up to a factor of two. We propose to instead use mixed-precision interval arithmetic to evaluate real-valued expressions. A key challenge is deriving the mixed-precision assignment both soundly and quickly. To do so, we introduce a sound variation of error Taylor series and condition numbers, specialized to interval arithmetic, that can be evaluated with minimal overhead thanks to an \"exponent trick\". Our implementation, Reval, achieves an average speed-up of 1.47x compared to the state-of-the-art Sollya tool, with the speed-up increasing to 4.92x on the most difficult input points. An examination of the precisions used with and without precision tuning shows that the speed-up results come from quickly assigning lower precisions for the majority of operations.",
        "published": "2024-10-09T22:10:51Z",
        "link": "http://arxiv.org/abs/2410.07468v2",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Methods for Few-View CT Image Reconstruction",
        "authors": [
            "Kyle M. Champley",
            "Michael B. Zellner",
            "Joseph W. Tringe",
            "Harry E. Martz Jr"
        ],
        "summary": "Computed Tomography (CT) is an essential non-destructive three dimensional imaging modality used in medicine, security screening, and inspection of manufactured components. Typical CT data acquisition entails the collection of a thousand or more projections through the object under investigation through a range of angles covering one hundred eighty degrees or more. It may be desirable or required that the number of projections angles be reduced by one or two orders of magnitude for reasons such as acquisition time or dose. Unless specialized reconstruction algorithms are applied, reconstructing with fewer views will result in streak artifacts and failure to resolve object boundaries at certain orientations. These artifacts may substantially diminish the usefulness of the reconstructed CT volumes.   Here we develop constrained and regularized numerical optimization methods to reconstruct CT volumes from 4-28 projections. These methods entail utilization of novel data fidelity and convex and non-convex regularization terms. In addition, the methods outlined here are usually carried out by a sequence of two or three numerical optimization methods in sequence.   The efficacy of our methods is demonstrated on four measured and three simulated few-view CT data sets. We show that these methods outperform other state of the art few-view numerical optimization methods.",
        "published": "2024-10-10T02:49:06Z",
        "link": "http://arxiv.org/abs/2410.07552v1",
        "categories": [
            "physics.med-ph",
            "cs.MS",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
        "authors": [
            "Konstantin Burlachenko",
            "Peter Richtárik"
        ],
        "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent agents to collaboratively train Machine Learning (ML) models in a distributed manner, eliminating the need for sharing their local data. The recent work (arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL) algorithms, marking a significant step towards applying second-order methods to FL and large-scale optimization. However, the reference FedNL prototype exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch a single experiment in a sever-grade workstation; (ii) The prototype only simulates multi-node setting; (iii) Prototype integration into resource-constrained applications is challenging. To bridge the gap between theory and practice, we present a self-contained implementation of FedNL, FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves the aforementioned issues and reduces the wall clock time by x1000. With this FedNL outperforms alternatives for training logistic regression in a single-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark (arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose two practical-orientated compressors for FedNL - adaptive TopLEK and cache-aware RandSeqK, which fulfill the theory of FedNL.",
        "published": "2024-10-11T12:19:18Z",
        "link": "http://arxiv.org/abs/2410.08760v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MS",
            "cs.PF",
            "math.OC",
            "G.4; C.3; I.2.11"
        ]
    },
    {
        "title": "Multigrid methods for the Stokes problem on GPU systems",
        "authors": [
            "Cu Cui",
            "Guido Kanschat"
        ],
        "summary": "This paper presents a matrix-free multigrid method for solving the Stokes problem, discretized using $H^{\\text{div}}$-conforming discontinuous Galerkin methods. We employ a Schur complement method combined with the fast diagonalization method for the efficient evaluation of the local solver within the multiplicative Schwarz smoother. This approach operates directly on both the velocity and pressure spaces, eliminating the need for a global Schur complement approximation. By leveraging the tensor product structure of Raviart-Thomas elements and an optimized, conflict-free shared memory access pattern, the matrix-free operator evaluation demonstrates excellent performance numbers, reaching over one billion degrees of freedom per second on a single NVIDIA A100 GPU. Numerical results indicate efficiency comparable to that of the three-dimensional Poisson problem.",
        "published": "2024-10-12T11:22:43Z",
        "link": "http://arxiv.org/abs/2410.09497v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Adaptive finite element methods based on flux and stress equilibration   using FEniCSx",
        "authors": [
            "Maximilian Brodbeck",
            "Fleurianne Bertrand",
            "Tim Ricken"
        ],
        "summary": "This contribution shows how a-posteriori error estimators based on equilibrated fluxes - H(div) functions fulfilling the underlying conservation law - can be implemented in FEniCSx. Therefore, dolfinx_eqlb is introduced, its algorithmic structure is described and classical benchmarks for adaptive solution procedures for the Poisson problem and linear elasticity are presented.",
        "published": "2024-10-13T07:41:44Z",
        "link": "http://arxiv.org/abs/2410.09764v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Tomographic Model Based Iterative Reconstruction of Symmetric Objects",
        "authors": [
            "Kyle M. Champley",
            "Ibrahim Oksuz",
            "Matthew G. Bisbee",
            "Joseph W. Tringe",
            "Brian Maddox"
        ],
        "summary": "Computed Tomography (CT) reconstruction of objects with cylindrical symmetry can be performed with a single projection. When the measured rays are parallel, and the axis of symmetry is perpendicular to the optical axis, the data can be modeled with the so-called Abel Transform. The Abel Transform has been extensively studied and many methods exist for accurate reconstruction. However, most CT geometries are cone-beam rather than parallel-beam. Using Abel methods for reconstruction in these cases can lead to distortions and reconstruction artifacts. Here, we develop analytic and model-based iterative reconstruction (MBIR) methods to reconstruct symmetric objects with an arbitrary axis of symmetry from a cone-beam geometry. The MBIR methods demonstrate superior results relative to the analytic inversion methods by mitigating artifacts and reducing noise while retaining fine image features. We demonstrate the efficacy of our methods using simulated and experimentally-acquired x-ray and neutron projections.",
        "published": "2024-10-13T13:35:30Z",
        "link": "http://arxiv.org/abs/2410.09837v1",
        "categories": [
            "physics.med-ph",
            "cs.MS",
            "eess.IV"
        ]
    },
    {
        "title": "The State of Julia for Scientific Machine Learning",
        "authors": [
            "Edward Berman",
            "Jacob Ginesin"
        ],
        "summary": "Julia has been heralded as a potential successor to Python for scientific machine learning and numerical computing, boasting ergonomic and performance improvements. Since Julia's inception in 2012 and declaration of language goals in 2017, its ecosystem and language-level features have grown tremendously. In this paper, we take a modern look at Julia's features and ecosystem, assess the current state of the language, and discuss its viability and pitfalls as a replacement for Python as the de-facto scientific machine learning language. We call for the community to address Julia's language-level issues that are preventing further adoption.",
        "published": "2024-10-14T01:43:23Z",
        "link": "http://arxiv.org/abs/2410.10908v1",
        "categories": [
            "cs.LG",
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Mixed-precision finite element kernels and assembly: Rounding error   analysis and hardware acceleration",
        "authors": [
            "M. Croci",
            "G. N. Wells"
        ],
        "summary": "In this paper we develop the first fine-grained rounding error analysis of finite element (FE) cell kernels and assembly. The theory includes mixed-precision implementations and accounts for hardware-acceleration via matrix multiplication units, thus providing theoretical guidance for designing reduced- and mixed-precision FE algorithms on CPUs and GPUs. Guided by this analysis, we introduce hardware-accelerated mixed-precision implementation strategies which are provably robust to low-precision computations. Indeed, these algorithms are accurate to the lower-precision unit roundoff with an error constant that is independent from: the conditioning of FE basis function evaluations, the ill-posedness of the cell, the polynomial degree, and the number of quadrature nodes. Consequently, we present the first AMX-accelerated FE kernel implementations on Intel Sapphire Rapids CPUs. Numerical experiments demonstrate that the proposed mixed- (single/half-) precision algorithms are up to 60 times faster than their double precision equivalent while being orders of magnitude more accurate than their fully half-precision counterparts.",
        "published": "2024-10-16T14:32:10Z",
        "link": "http://arxiv.org/abs/2410.12614v1",
        "categories": [
            "math.NA",
            "cs.AR",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "modOpt: A modular development environment and library for optimization   algorithms",
        "authors": [
            "Anugrah Jo Joshy",
            "John T. Hwang"
        ],
        "summary": "Recent advances in computing hardware and modeling software have given rise to new applications for numerical optimization. These new applications occasionally uncover bottlenecks in existing optimization algorithms and necessitate further specialization of the algorithms. However, such specialization requires expert knowledge of the underlying mathematical theory and the software implementation of existing algorithms. To address this challenge, we present modOpt, an open-source software framework that facilitates the construction of optimization algorithms from modules. The modular environment provided by modOpt enables developers to tailor an existing algorithm for a new application by only altering the relevant modules. modOpt is designed as a platform to support students and beginner developers in quickly learning and developing their own algorithms. With that aim, the entirety of the framework is written in Python, and it is well-documented, well-tested, and hosted open-source on GitHub. Several additional features are embedded into the framework to assist both beginner and advanced developers. In addition to providing stock modules, the framework also includes fully transparent implementations of pedagogical optimization algorithms in Python. To facilitate testing and benchmarking of new algorithms, the framework features built-in visualization and recording capabilities, interfaces to modeling frameworks such as OpenMDAO and CSDL, interfaces to general-purpose optimization algorithms such as SNOPT and SLSQP, an interface to the CUTEst test problem set, etc. In this paper, we present the underlying software architecture of modOpt, review its various features, discuss several educational and performance-oriented algorithms within modOpt, and present numerical studies illustrating its unique benefits.",
        "published": "2024-10-16T18:30:23Z",
        "link": "http://arxiv.org/abs/2410.12942v1",
        "categories": [
            "cs.MS",
            "cs.CE",
            "cs.NA",
            "math.NA",
            "math.OC",
            "D.2.2; D.2.13; G.1.6; G.4; J.2"
        ]
    },
    {
        "title": "An Efficient Local Optimizer-Tracking Solver for Differential-Algebriac   Equations with Optimization Criteria",
        "authors": [
            "Alexander Fleming",
            "Jens Deussen",
            "Uwe Naumann"
        ],
        "summary": "A sequential solver for differential-algebraic equations with embedded optimization criteria (DAEOs) was developed to take advantage of the theoretical work done by Deussen et al. Solvers of this type separate the optimization problem from the differential equation and solve each individually. The new solver relies on the reduction of a DAEO to a sequence of differential inclusions separated by jump events. These jump events occur when the global solution to the optimization problem jumps to a new value. Without explicit treatment, these events will reduce the order of convergence of the integration step to one. The solver implements a \"local optimizer tracking\" procedure to detect and correct these jump events. Local optimizer tracking is much less expensive than running a deterministic global optimizer at every time step. This preserves the order of convergence of the integrator component without sacrificing performance to perform deterministic global optimization at every time step. The newly developed solver produces correct solutions to DAEOs and runs much faster than sequential DAEO solvers that rely only on global optimization.",
        "published": "2024-10-21T12:48:12Z",
        "link": "http://arxiv.org/abs/2410.15963v1",
        "categories": [
            "math.OC",
            "cs.MS",
            "cs.NA",
            "math.NA",
            "G.1.7; G.1.4"
        ]
    },
    {
        "title": "Matrix-by-matrix multiplication algorithm with $O(N^2log_2N)$   computational complexity for variable precision arithmetic",
        "authors": [
            "Maciej Paszyński"
        ],
        "summary": "We show that assuming the availability of the processor with variable precision arithmetic, we can compute matrix-by-matrix multiplications in $O(N^2log_2N)$ computational complexity. We replace the standard matrix-by-matrix multiplications algorithm $\\begin{bmatrix}A_{11}&A_{12}\\\\A_{21}&A_{22}\\end{bmatrix}\\begin{bmatrix}B_{11}&B_{12}\\\\B_{21}&B_{22}\\end{bmatrix}=\\begin{bmatrix}A_{11}B_{11}+A_{12}B_{21}&A_{11}B_{12}+A_{12}B_{22}\\\\A_{21}B_{11}+A_{22}B_{21}&A_{21}B_{12}+A_{22}B_{22}\\end{bmatrix}$ by $\\begin{bmatrix}A_{11}&A_{12}\\\\A_{21}&A_{22}\\end{bmatrix}\\begin{bmatrix}B_{11}&B_{12}\\\\B_{21}&B_{22}\\end{bmatrix}=\\Bigl\\lfloor\\begin{bmatrix} (A_{11}+\\epsilon A_{12})(B_{11}+1/{\\epsilon}B_{21})&(A_{11}+\\epsilon A_{12})(B_{12}+1/{\\epsilon}B_{22})\\\\(A_{21}+\\epsilon A_{22})(B_{11}+1/{\\epsilon}B_{21})&(A_{21}+\\epsilon A_{22})(B_{12}+1/{\\epsilon}B_{22})\\end{bmatrix}\\Bigr\\rfloor \\mod \\frac{1}{\\epsilon}$. The resulting computational complexity for $N\\times N$ matrices can be estimated from recursive equation $T(N)=4(N/2)^2$ (multiplication of a matrix by number)+$4(N/2)^2$ (additions of matrices)+$2N^2$ (floor and modulo)+$4T(N/2)$ (recursive calls) as $O(N^2log_2N)$. The novelty of the method lies in the observation, somehow ignored by other matrix-by-matrix multiplication algorithms, that we can multiply matrix entries by non-integer numbers to improve computational complexity. In other words, while having a processor that can compute multiplications, additions, modulo and floor operations with variable precision arithmetic in $O(1)$, we can obtain a matrix-by-matrix multiplication algorithm with $O(N^2log_2N)$ computational complexity. We also present a MATLAB code using VPA variable precision arithmetic emulator that can multiply matrices of size $N\\times N$ using $(4log_2N+1)N^2$ variable precision arithmetic operations. This emulator uses $O(N)$ digits to run our algorithm.",
        "published": "2024-10-28T14:06:12Z",
        "link": "http://arxiv.org/abs/2410.21050v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.MS",
            "68",
            "F.2.1; G.4"
        ]
    },
    {
        "title": "A Bellman-Ford algorithm for the path-length-weighted distance in graphs",
        "authors": [
            "R. Arnau",
            "J. M. Calabuig",
            "L. M. García Raffi",
            "E. A. Sánchez Pérez",
            "S. Sanjuan"
        ],
        "summary": "Consider a finite directed graph without cycles in which the arrows are weighted. We present an algorithm for the computation of a new distance, called path-length-weighted distance, which has proven useful for graph analysis in the context of fraud detection. The idea is that the new distance explicitly takes into account the size of the paths in the calculations. Thus, although our algorithm is based on arguments similar to those at work for the Bellman-Ford and Dijkstra methods, it is in fact essentially different. We lay out the appropriate framework for its computation, showing the constraints and requirements for its use, along with some illustrative examples.",
        "published": "2024-10-28T15:31:34Z",
        "link": "http://arxiv.org/abs/2411.00819v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "cs.MS",
            "05C38 (Primary) 90C35 (Secondary)"
        ]
    },
    {
        "title": "$\\texttt{skwdro}$: a library for Wasserstein distributionally robust   machine learning",
        "authors": [
            "Florian Vincent",
            "Waïss Azizian",
            "Franck Iutzeler",
            "Jérôme Malick"
        ],
        "summary": "We present skwdro, a Python library for training robust machine learning models. The library is based on distributionally robust optimization using optimal transport distances. For ease of use, it features both scikit-learn compatible estimators for popular objectives, as well as a wrapper for PyTorch modules, enabling researchers and practitioners to use it in a wide range of models with minimal code changes. Its implementation relies on an entropic smoothing of the original robust objective in order to ensure maximal model flexibility. The library is available at https://github.com/iutzeler/skwdro",
        "published": "2024-10-28T17:16:00Z",
        "link": "http://arxiv.org/abs/2410.21231v1",
        "categories": [
            "cs.LG",
            "cs.MS",
            "math.OC",
            "90C17, 90C15",
            "I.2.6; I.2.5; G.4; G.1.6"
        ]
    },
    {
        "title": "Development of a Python-Based Software for Calculating the Jones   Polynomial: Insights into the Behavior of Polymers and Biopolymers",
        "authors": [
            "Caleb Musfeldt"
        ],
        "summary": "This thesis details a Python-based software designed to calculate the Jones polynomial, a vital mathematical tool from Knot Theory used for characterizing the topological and geometrical complexity of curves in \\( \\mathbb{R}^3 \\), which is essential in understanding physical systems of filaments, including the behavior of polymers and biopolymers. The Jones polynomial serves as a topological invariant capable of distinguishing between different knot structures. This capability is fundamental to characterizing the architecture of molecular chains, such as proteins and DNA. Traditional computational methods for deriving the Jones polynomial have been limited by closure-schemes and high execution costs, which can be impractical for complex structures like those that appear in real life. This software implements methods that significantly reduce calculation times, allowing for more efficient and practical applications in the study of biological polymers. It utilizes a divide-and-conquer approach combined with parallel computing and applies recursive Reidemeister moves to optimize the computation, transitioning from an exponential to a near-linear runtime for specific configurations. This thesis provides an overview of the software's functions, detailed performance evaluations using protein structures as test cases, and a discussion of the implications for future research and potential algorithmic improvements.",
        "published": "2024-10-30T02:41:42Z",
        "link": "http://arxiv.org/abs/2410.22652v1",
        "categories": [
            "cs.MS",
            "math.GN"
        ]
    },
    {
        "title": "The Python LevelSet Toolbox (LevelSetPy)",
        "authors": [
            "Lekan Molu"
        ],
        "summary": "This paper describes open-source scientific contributions in python surrounding the numerical solutions to hyperbolic Hamilton-Jacobi (HJ) partial differential equations viz., their implicit representation on co-dimension one surfaces; dynamics evolution with levelsets; spatial derivatives; total variation diminishing Runge-Kutta integration schemes; and their applications to the theory of reachable sets. They are increasingly finding applications in multiple research domains such as reinforcement learning, robotics, control engineering and automation. We describe the library components, illustrate usage with an example, and provide comparisons with existing implementations. This GPU-accelerated package allows for easy portability to many modern libraries for the numerical analyses of the HJ equations. We also provide a CPU implementation in python that is significantly faster than existing alternatives.",
        "published": "2024-11-05T20:31:23Z",
        "link": "http://arxiv.org/abs/2411.03501v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "cs.SY",
            "eess.SY",
            "math.NA"
        ]
    },
    {
        "title": "On a probabilistic global optimizer derived from the Walker slice   sampling",
        "authors": [
            "Aditya Gupta",
            "Souvik Das",
            "Debasish Chatterjee"
        ],
        "summary": "This article presents a zeroth order probabilistic global optimization algorithm -- SwiftNav -- for (not necessarily convex) functions over a compact domain. A discretization procedure is deployed on the compact domain, starting with a small step-size $h > 0$ and subsequently adaptively refining it in the course of a simulated annealing routine utilizing the Walker slice and the Gibbs sampler, in order to identify a set of global optimizers up to good precision. SwiftNav is parallelizable, which helps with scalability as the dimension of decision variables increases. Several numerical experiments are included here to demonstrate the effectiveness and accuracy of SwiftNav in high-dimensional benchmark optimization problems.",
        "published": "2024-11-06T11:40:43Z",
        "link": "http://arxiv.org/abs/2411.03851v1",
        "categories": [
            "math.OC",
            "cs.MS"
        ]
    },
    {
        "title": "SequentialSamplingModels.jl: Simulating and Evaluating Cognitive Models   of Response Times in Julia",
        "authors": [
            "Kianté Fernandez",
            "Dominique Makowski",
            "Christopher Fisher"
        ],
        "summary": "Sequential sampling models (SSMs) are a widely used framework describing decision-making as a stochastic, dynamic process of evidence accumulation. SSMs popularity across cognitive science has driven the development of various software packages that lower the barrier for simulating, estimating, and comparing existing SSMs. Here, we present a software tool, SequentialSamplingModels.jl (SSM.jl), designed to make SSM simulations more accessible to Julia users, and to integrate with the Julia ecosystem. We demonstrate the basic use of SSM.jl for simulation, plotting, and Bayesian inference.",
        "published": "2024-11-10T23:46:37Z",
        "link": "http://arxiv.org/abs/2411.06631v1",
        "categories": [
            "cs.MS",
            "q-bio.NC",
            "stat.CO",
            "stat.ME"
        ]
    },
    {
        "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
        "authors": [
            "Ishna Satyarth",
            "Chao Yin",
            "RuQing G. Xu",
            "Devin A. Matthews"
        ],
        "summary": "The factorization of skew-symmetric matrices is a critically understudied area of dense linear algebra (DLA), particularly in comparison to that of symmetric matrices. While some algorithms can be adapted from the symmetric case, the cost of algorithms can be reduced by exploiting skew-symmetry. A motivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix $X$, which is used in practical applications as a means of determining the determinant of $X$ as the square of the (cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$, for example in fields such as quantum electronic structure and machine learning. Such applications also often require pivoting in order to improve numerical stability. In this work we explore a combination of known literature algorithms and new algorithms recently derived using formal methods. High-performance parallel CPU implementations are created, leveraging the concept of fusion at multiple levels in order to reduce memory traffic overhead, as well as the BLIS framework which provides high-performance GEMM kernels, hierarchical parallelism, and cache blocking. We find that operation fusion and improved use of available bandwidth via parallelization of bandwidth-bound (level-2 BLAS) operations are essential for obtaining high performance, while a concise C++ implementation provides a clear and close connection to the formal derivation process without sacrificing performance.",
        "published": "2024-11-15T00:37:31Z",
        "link": "http://arxiv.org/abs/2411.09859v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Cascaded Prediction and Asynchronous Execution of Iterative Algorithms   on Heterogeneous Platforms",
        "authors": [
            "Jianhua Gao",
            "Bingjie Liu",
            "Yizhuo Wang",
            "Weixing Ji",
            "Hua Huang"
        ],
        "summary": "Owing to the diverse scales and varying distributions of sparse matrices arising from practical problems, a multitude of choices are present in the design and implementation of sparse matrix-vector multiplication (SpMV). Researchers have proposed many machine learning-based optimization methods for SpMV. However, these efforts only support one area of sparse matrix format selection, SpMV algorithm selection, or parameter configuration, and rarely consider a large amount of time overhead associated with feature extraction, model inference, and compression format conversion. This paper introduces a machine learning-based cascaded prediction method for SpMV computations that spans various computing stages and hierarchies. Besides, an asynchronous and concurrent computing model has been designed and implemented for runtime model prediction and iterative algorithm solving on heterogeneous computing platforms. It not only offers comprehensive support for the iterative algorithm-solving process leveraging machine learning technology, but also effectively mitigates the preprocessing overheads. Experimental results demonstrate that the cascaded prediction introduced in this paper accelerates SpMV by 1.33x on average, and the iterative algorithm, enhanced by cascaded prediction and asynchronous execution, optimizes by 2.55x on average.",
        "published": "2024-11-15T12:33:58Z",
        "link": "http://arxiv.org/abs/2411.10143v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "68-02, 68W10, 65F50",
            "A.1; D.1.3; G.1.3"
        ]
    },
    {
        "title": "Invariant Polydiagonal Subspaces of Matrices and Constraint Programming",
        "authors": [
            "John M. Neuberger",
            "Nándor Sieben",
            "James W. Swift"
        ],
        "summary": "In a polydiagonal subspace of the Euclidean space, certain components of the vectors are equal (synchrony) or opposite (anti-synchrony). Polydiagonal subspaces invariant under a matrix have many applications in graph theory and dynamical systems, especially coupled cell networks. We describe invariant polydiagonal subspaces in terms of coloring vectors. This approach gives an easy formulation of a constraint satisfaction problem for finding invariant polydiagonal subspaces. Solving the resulting problem with existing state-of-the-art constraint solvers greatly outperforms the currently known algorithms.",
        "published": "2024-11-16T22:38:40Z",
        "link": "http://arxiv.org/abs/2411.10904v2",
        "categories": [
            "math.DS",
            "cs.DM",
            "cs.MS",
            "90C35, 34C14, 37C80"
        ]
    },
    {
        "title": "Interface for Sparse Linear Algebra Operations",
        "authors": [
            "Ahmad Abdelfattah",
            "Willow Ahrens",
            "Hartwig Anzt",
            "Chris Armstrong",
            "Ben Brock",
            "Aydin Buluc",
            "Federico Busato",
            "Terry Cojean",
            "Tim Davis",
            "Jim Demmel",
            "Grace Dinh",
            "David Gardener",
            "Jan Fiala",
            "Mark Gates",
            "Azzam Haider",
            "Toshiyuki Imamura",
            "Pedro Valero Lara",
            "Jose Moreira",
            "Sherry Li",
            "Piotr Luszczek",
            "Max Melichenko",
            "Jose Moeira",
            "Yvan Mokwinski",
            "Riley Murray",
            "Spencer Patty",
            "Slaven Peles",
            "Tobias Ribizel",
            "Jason Riedy",
            "Siva Rajamanickam",
            "Piyush Sao",
            "Manu Shantharam",
            "Keita Teranishi",
            "Stan Tomov",
            "Yu-Hsiang Tsai",
            "Heiko Weichelt"
        ],
        "summary": "The standardization of an interface for dense linear algebra operations in the BLAS standard has enabled interoperability between different linear algebra libraries, thereby boosting the success of scientific computing, in particular in scientific HPC. Despite numerous efforts in the past, the community has not yet agreed on a standardization for sparse linear algebra operations due to numerous reasons. One is the fact that sparse linear algebra objects allow for many different storage formats, and different hardware may favor different storage formats. This makes the definition of a FORTRAN-style all-circumventing interface extremely challenging. Another reason is that opposed to dense linear algebra functionality, in sparse linear algebra, the size of the sparse data structure for the operation result is not always known prior to the information. Furthermore, as opposed to the standardization effort for dense linear algebra, we are late in the technology readiness cycle, and many production-ready software libraries using sparse linear algebra routines have implemented and committed to their own sparse BLAS interface. At the same time, there exists a demand for standardization that would improve interoperability, and sustainability, and allow for easier integration of building blocks. In an inclusive, cross-institutional effort involving numerous academic institutions, US National Labs, and industry, we spent two years designing a hardware-portable interface for basic sparse linear algebra functionality that serves the user needs and is compatible with the different interfaces currently used by different vendors. In this paper, we present a C++ API for sparse linear algebra functionality, discuss the design choices, and detail how software developers preserve a lot of freedom in terms of how to implement functionality behind this API.",
        "published": "2024-11-20T12:20:45Z",
        "link": "http://arxiv.org/abs/2411.13259v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "AD-HOC: A C++ Expression Template package for high-order derivatives   backpropagation",
        "authors": [
            "Juan Lucas Rey"
        ],
        "summary": "This document presents a new C++ Automatic Differentiation (AD) tool, AD-HOC (Automatic Differentiation for High-Order Calculations). This tool aims to have the following features: -Calculation of user specified derivatives of arbitrary order -To be able to run with similar speeds as handwritten code -All derivatives calculations are computed in a single backpropagation tree pass -No source code generation is used, relying heavily on the C++ compiler to statically build the computation tree before runtime -A simple interface -The ability to be used \\textit{in conjunction} with other established, general-purpose dynamic AD tools -Header-only library, with no external dependencies -Open source, with a business-friendly license",
        "published": "2024-11-25T10:24:29Z",
        "link": "http://arxiv.org/abs/2412.05300v2",
        "categories": [
            "cs.MS",
            "q-fin.CP"
        ]
    },
    {
        "title": "Jaya R Package -- A Parameter-Free Solution for Advanced Single and   Multi-Objective Optimization",
        "authors": [
            "Neeraj Dhanraj Bokde"
        ],
        "summary": "The Jaya R package offers a robust and versatile implementation of the parameter-free Jaya optimization algorithm, suitable for solving both single-objective and multi-objective optimization problems. By integrating advanced features such as constraint handling, adaptive population management, Pareto front tracking for multi-objective trade-offs, and parallel processing for computational efficiency, the package caters to a wide range of optimization challenges. Its intuitive design and flexibility allow users to solve complex, real-world problems across various domains. To demonstrate its practical utility, a case study on energy modeling explores the optimization of renewable energy shares, showcasing the package's ability to minimize carbon emissions and costs while enhancing system reliability. The Jaya R package is an invaluable tool for researchers and practitioners seeking efficient and adaptive optimization solutions.",
        "published": "2024-11-25T15:46:54Z",
        "link": "http://arxiv.org/abs/2411.16509v1",
        "categories": [
            "cs.MS",
            "cs.LG"
        ]
    },
    {
        "title": "Learning optimal objective values for MILP",
        "authors": [
            "Lara Scavuzzo",
            "Karen Aardal",
            "Neil Yorke-Smith"
        ],
        "summary": "Modern Mixed Integer Linear Programming (MILP) solvers use the Branch-and-Bound algorithm together with a plethora of auxiliary components that speed up the search. In recent years, there has been an explosive development in the use of machine learning for enhancing and supporting these algorithmic components. Within this line, we propose a methodology for predicting the optimal objective value, or, equivalently, predicting if the current incumbent is optimal. For this task, we introduce a predictor based on a graph neural network (GNN) architecture, together with a set of dynamic features. Experimental results on diverse benchmarks demonstrate the efficacy of our approach, achieving high accuracy in the prediction task and outperforming existing methods. These findings suggest new opportunities for integrating ML-driven predictions into MILP solvers, enabling smarter decision-making and improved performance.",
        "published": "2024-11-27T13:22:31Z",
        "link": "http://arxiv.org/abs/2411.18321v1",
        "categories": [
            "math.OC",
            "cs.AI",
            "cs.LG",
            "cs.MS"
        ]
    },
    {
        "title": "Open Source Evolutionary Computation with Chips-n-Salsa",
        "authors": [
            "Vincent A. Cicirello"
        ],
        "summary": "When it was first introduced, the Chips-n-Salsa Java library provided stochastic local search and related algorithms, with a focus on self-adaptation and parallel execution. For the past four years, we expanded its scope to include evolutionary computation. This paper concerns the evolutionary algorithms that Chips-n-Salsa now provides, which includes multiple evolutionary models, common problem representations, a wide range of mutation and crossover operators, and a variety of benchmark problems. Well-defined Java interfaces enable easily integrating custom representations and evolutionary operators, as well as defining optimization problems. Chips-n-Salsa's evolutionary algorithms include implementations with adaptive mutation and crossover rates, as well as both sequential and parallel execution. Source code is maintained on GitHub, and immutable artifacts are regularly published to the Maven Central Repository to enable easily importing into projects for reproducible builds. Effective development processes such as test-driven development, as well as a variety of static analysis tools help ensure code quality.",
        "published": "2024-12-02T22:18:31Z",
        "link": "http://arxiv.org/abs/2412.02004v1",
        "categories": [
            "cs.NE",
            "cs.MS",
            "68W50, 68T05, 68T20, 68W20",
            "I.2.8; F.2.2"
        ]
    },
    {
        "title": "iSEEtree: interactive explorer for hierarchical data",
        "authors": [
            "Giulio Benedetti",
            "Ely Seraidarian",
            "Theotime Pralas",
            "Akewak Jeba",
            "Tuomas Borman",
            "Leo Lahti"
        ],
        "summary": "$\\textbf{Motivation:}$ Hierarchical data structures are prevalent across several fields of research, as they represent an organised and efficient approach to study complex interconnected systems. Their significance is particularly evident in microbiome analysis, where microbial communities are classified at various taxonomic levels along the phylogenetic tree. In light of this trend, the R/Bioconductor community has established a reproducible analytical framework for hierarchical data, which relies on the highly generic and optimised TreeSummarizedExperiment data container. However, using this framework requires basic proficiency in programming.   $\\textbf{Results:}$ To reduce the entry requirements, we developed iSEEtree, an R shiny app which provides a visual interface for the analysis and exploration of TreeSummarizedExperiment objects, thereby expanding the interactive graphics capabilities of related work to hierarchical structures. This way, users can interactively explore several aspects of their data without the need for extensive knowledge of R programming. We describe how iSEEtree enables the exploration of hierarchical multi-table data and demonstrate its functionality with applications to microbiome analysis.   $\\textbf{Availability and Implementation:}$ iSEEtree was implemented in the R programming language and is available on Bioconductor at https://bioconductor.org/packages/iSEEtree under an Artistic 2.0 license.   $\\textbf{Contact:}$ giulio.benedetti@utu.fi or leo.lahti@utu.fi.",
        "published": "2024-12-03T22:34:38Z",
        "link": "http://arxiv.org/abs/2412.02882v2",
        "categories": [
            "cs.MS",
            "cs.GR",
            "q-bio.GN",
            "G.4, H.5.2"
        ]
    },
    {
        "title": "Monte Carlo Analysis of Boid Simulations with Obstacles: A Physics-Based   Perspective",
        "authors": [
            "Quoc Chuong Nguyen"
        ],
        "summary": "Boids, developed by Craig W. Reynolds in 1986, is one of the earliest emergent models where the global pattern emerges from the interaction between many individuals within the local scale. In the original model, Boids follow three rules: separation, alignment, and cohesion; which allow them to move around and create a flock without intention in the empty environment. In the real world, however, the Boids' movement also faces obstacles preventing the flock's direction. In this project, I propose two new simple rules of the Boids model to represent the more realistic movement in nature and analyze the model from the physics perspective using the Monte Carlo method. From those results, the physics metrics related to the forming of the flocking phenomenon show that it is reasonable to explain why birds or fishes prefer to move in a flock, rather than sole movement.",
        "published": "2024-12-10T04:34:23Z",
        "link": "http://arxiv.org/abs/2412.10420v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.MS",
            "math.DS"
        ]
    },
    {
        "title": "Optimization-Driven Design of Monolithic Soft-Rigid Grippers",
        "authors": [
            "Pierluigi Mansueto",
            "Mihai Dragusanu",
            "Anjum Saeed",
            "Monica Malvezzi",
            "Matteo Lapucci",
            "Gionata Salvietti"
        ],
        "summary": "Sim-to-real transfer remains a significant challenge in soft robotics due to the unpredictability introduced by common manufacturing processes such as 3D printing and molding. These processes often result in deviations from simulated designs, requiring multiple prototypes before achieving a functional system. In this study, we propose a novel methodology to address these limitations by combining advanced rapid prototyping techniques and an efficient optimization strategy. Firstly, we employ rapid prototyping methods typically used for rigid structures, leveraging their precision to fabricate compliant components with reduced manufacturing errors. Secondly, our optimization framework minimizes the need for extensive prototyping, significantly reducing the iterative design process. The methodology enables the identification of stiffness parameters that are more practical and achievable within current manufacturing capabilities. The proposed approach demonstrates a substantial improvement in the efficiency of prototype development while maintaining the desired performance characteristics. This work represents a step forward in bridging the sim-to-real gap in soft robotics, paving the way towards a faster and more reliable deployment of soft robotic systems.",
        "published": "2024-12-10T14:47:09Z",
        "link": "http://arxiv.org/abs/2412.07556v1",
        "categories": [
            "cs.RO",
            "cs.MS"
        ]
    },
    {
        "title": "Direct Low-Dose CT Image Reconstruction on GPU using Out-Of-Core:   Precision and Quality Study",
        "authors": [
            "M. Chillarón",
            "G. Quintana-Ortí",
            "V. Vidal",
            "G. Verdú"
        ],
        "summary": "Algebraic methods applied to the reconstruction of Sparse-view Computed Tomography (CT) can provide both a high image quality and a decrease in the dose received by patients, although with an increased reconstruction time since their computational costs are higher. In our work, we present a new algebraic implementation that obtains an exact solution to the system of linear equations that models the problem and based on single-precision floating-point arithmetic. By applying Out-Of-Core (OOC) techniques, the dimensions of the system can be increased regardless of the main memory size and as long as there is enough secondary storage (disk). These techniques have allowed to process images of 768 x 768 pixels. A comparative study of our method on a GPU using both single-precision and double-precision arithmetic has been carried out. The goal is to assess the single-precision arithmetic implementation both in terms of time improvement and quality of the reconstructed images to determine if it is sufficient to consider it a viable option. Results using single-precision arithmetic approximately halves the reconstruction time of the double-precision implementation, whereas the obtained images retain all internal structures despite having higher noise levels.",
        "published": "2024-12-10T16:11:51Z",
        "link": "http://arxiv.org/abs/2412.07631v2",
        "categories": [
            "physics.med-ph",
            "cs.DC",
            "cs.MS",
            "15A23, 15A30, 65F05, 65K05, 65Y20, 68W10, 92C55",
            "D.1; F.2.1; G.1.3; G.4; I.4; I.4.5; J.3"
        ]
    },
    {
        "title": "TIGRE v3: Efficient and easy to use iterative computed tomographic   reconstruction toolbox for real datasets",
        "authors": [
            "Ander Biguri",
            "Tomoyuki Sadakane",
            "Reuben Lindroos",
            "Yi Liu",
            "Malena Sabaté Landman",
            "Yi Du",
            "Manasavee Lohvithee",
            "Stefanie Kaser",
            "Sepideh Hatamikia",
            "Robert Bryll",
            "Emilien Valat",
            "Sarinrat Wonglee",
            "Thomas Blumensath",
            "Carola-Bibiane Schönlieb"
        ],
        "summary": "Computed Tomography (CT) has been widely adopted in medicine and it is increasingly being used in scientific and industrial applications. Parallelly, research in different mathematical areas concerning discrete inverse problems has led to the development of new sophisticated numerical solvers that can be applied in the context of CT. The Tomographic Iterative GPU-based Reconstruction (TIGRE) toolbox was born almost a decade ago precisely in the gap between mathematics and high performance computing for real CT data, providing user-friendly open-source software tools for image reconstruction. However, since its inception, the tools' features and codebase have had over a twenty-fold increase, and are now including greater geometric flexibility, a variety of modern algorithms for image reconstruction, high-performance computing features and support for other CT modalities, like proton CT. The purpose of this work is two-fold: first, it provides a structured overview of the current version of the TIGRE toolbox, providing appropriate descriptions and references, and serving as a comprehensive and peer-reviewed guide for the user; second, it is an opportunity to illustrate the performance of several of the available solvers showcasing real CT acquisitions, which are typically not be openly available to algorithm developers.",
        "published": "2024-12-13T13:21:47Z",
        "link": "http://arxiv.org/abs/2412.10129v1",
        "categories": [
            "physics.med-ph",
            "cs.MS",
            "math.OC"
        ]
    },
    {
        "title": "Towards Real-time Adaptive Anisotropic Image-to-mesh Conversion for   Vascular Flow Simulations",
        "authors": [
            "Kevin Garner",
            "Fotis Drakopoulos",
            "Chander Sadasivan",
            "Nikos Chrisochoides"
        ],
        "summary": "Presented is a path towards a fast and robust adaptive anisotropic mesh generation method that is designed to help streamline the discretization of complex vascular geometries within the Computational Fluid Dynamics (CFD) modeling process. The proposed method combines multiple software tools into a single pipeline to provide the following: (1) image-to-mesh conversion which satisfies quality, fidelity, and smoothness requirements, (2) the generation of a boundary layer grid over the high fidelity surface, (3) a parallel adaptive anisotropic meshing procedure which satisfies real-time requirements, and (4) robustness, which is satisfied by the pipeline's ability to process segmented images and CAD models. The proposed approach is tested with two brain aneurysm cases and is shown to satisfy all the aforementioned requirements. The next steps are to fully parallelize the remaining components of the pipeline to maximize potential performance and to test its integration within a CFD vascular flow simulation. Just as the parallel anisotropic adaptation procedure was tested within aerospace CFD simulations using CAD models, the method is expected to provide accurate results for CFD vascular flow simulations in real-time when executed on multicore cc-NUMA architectures.",
        "published": "2024-12-16T18:51:22Z",
        "link": "http://arxiv.org/abs/2412.13222v1",
        "categories": [
            "physics.flu-dyn",
            "cs.DC",
            "cs.GR",
            "cs.MS",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "The Ramanujan Library -- Automated Discovery on the Hypergraph of   Integer Relations",
        "authors": [
            "Itay Beit-Halachmi",
            "Ido Kaminer"
        ],
        "summary": "Fundamental mathematical constants appear in nearly every field of science, from physics to biology. Formulas that connect different constants often bring great insight by hinting at connections between previously disparate fields. Discoveries of such relations, however, have remained scarce events, relying on sporadic strokes of creativity by human mathematicians. Recent developments of algorithms for automated conjecture generation have accelerated the discovery of formulas for specific constants. Yet, the discovery of connections between constants has not been addressed. In this paper, we present the first library dedicated to mathematical constants and their interrelations. This library can serve as a central repository of knowledge for scientists from different areas, and as a collaborative platform for development of new algorithms. The library is based on a new representation that we propose for organizing the formulas of mathematical constants: a hypergraph, with each node representing a constant and each edge representing a formula. Using this representation, we propose and demonstrate a systematic approach for automatically enriching this library using PSLQ, an integer relation algorithm based on QR decomposition and lattice construction. During its development and testing, our strategy led to the discovery of 75 previously unknown connections between constants, including a new formula for the `first continued fraction' constant $C_1$, novel formulas for natural logarithms, and new formulas connecting $\\pi$ and $e$. The latter formulas generalize a century-old relation between $\\pi$ and $e$ by Ramanujan, which until now was considered a singular formula and is now found to be part of a broader mathematical structure. The code supporting this library is a public, open-source API that can serve researchers in experimental mathematics and other fields of science.",
        "published": "2024-12-16T21:18:44Z",
        "link": "http://arxiv.org/abs/2412.12361v1",
        "categories": [
            "cs.AI",
            "cs.MS",
            "math.NT"
        ]
    },
    {
        "title": "A Fully Adaptive Radau Method for the Efficient Solution of Stiff   Ordinary Differential Equations at Low Tolerances",
        "authors": [
            "Shreyas Ekanathan",
            "Oscar Smith",
            "Christopher Rackauckas"
        ],
        "summary": "Radau IIA methods, specifically the adaptive order radau method in Fortran due to Hairer, are known to be state-of-the-art for the high-accuracy solution of highly stiff ordinary differential equations (ODEs). However, the traditional implementation was specialized to a specific range of tolerance, in particular only supporting 5th, 9th, and 13th order versions of the tableau and only derived in double precision floating point, thus limiting the ability to be truly general purpose for highly accurate scenarios. To alleviate these constraints, we implement an adaptive-time adaptive-order Radau method which can derive the coefficients for the Radau IIA embedded tableau to any order on the fly to any precision. Additionally, our Julia-based implementation includes many modernizations to improve performance, including improvements to the order adaptation scheme and improved linear algebra integrations. In a head-to-head benchmark against the classic Fortran implementation, we demonstrate our implementation is approximately 2x across a range of stiff ODEs. We benchmark our algorithm against several well-reputed numerical integrators for stiff ODEs and find state-of-the-art performance on several test problems, with a 1.5-times speed-up over common numerical integrators for stiff ODEs when low error tolerance is required. The newly implemented method is distributed in open source software for free usage on stiff ODEs.",
        "published": "2024-12-18T22:00:25Z",
        "link": "http://arxiv.org/abs/2412.14362v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "cs.NA"
        ]
    },
    {
        "title": "Accelerated Patient-Specific Calibration via Differentiable Hemodynamics   Simulations",
        "authors": [
            "Diego Renner",
            "Georgios Kissas"
        ],
        "summary": "One of the goals of personalized medicine is to tailor diagnostics to individual patients. Diagnostics are performed in practice by measuring quantities, called biomarkers, that indicate the existence and progress of a disease. In common cardiovascular diseases, such as hypertension, biomarkers that are closely related to the clinical representation of a patient can be predicted using computational models. Personalizing computational models translates to considering patient-specific flow conditions, for example, the compliance of blood vessels that cannot be a priori known and quantities such as the patient geometry that can be measured using imaging. Therefore, a patient is identified by a set of measurable and nonmeasurable parameters needed to well-define a computational model; else, the computational model is not personalized, meaning it is prone to large prediction errors. Therefore, to personalize a computational model, sufficient information needs to be extracted from the data. The current methods by which this is done are either inefficient, due to relying on slow-converging optimization methods, or hard to interpret, due to using `black box` deep-learning algorithms. We propose a personalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes reduced order model solver and fast parameter inference methods that take advantage of gradients through the solver. By providing a faster method for performing parameter inference and sensitivity analysis through differentiability while maintaining the interpretability of well-understood mathematical models and numerical methods, the best of both worlds is combined. The performance of the proposed solver is validated against a well-established process on different geometries, and different parameter inference processes are successfully performed.",
        "published": "2024-12-19T06:42:57Z",
        "link": "http://arxiv.org/abs/2412.14572v1",
        "categories": [
            "physics.med-ph",
            "cs.LG",
            "cs.MS",
            "physics.comp-ph",
            "q-bio.QM"
        ]
    },
    {
        "title": "Algorithm for globally identifiable reparametrizations of ODEs",
        "authors": [
            "Sebastian Falkensteiner",
            "Alexey Ovchinnikov",
            "J. Rafael Sendra"
        ],
        "summary": "Structural global parameter identifiability indicates whether one can determine a parameter's value in an ODE model from given inputs and outputs. If a given model has parameters for which there is exactly one value, such parameters are called globally identifiable. Given an ODE model involving not globally identifiable parameters, first we transform the system into one with locally identifiable parameters. As a main contribution of this paper, then we present a procedure for replacing, if possible, the ODE model with an equivalent one that has globally identifiable parameters. We first derive this as an algorithm for one-dimensional ODE models and then reuse this approach for higher-dimensional models.",
        "published": "2024-01-01T14:04:32Z",
        "link": "http://arxiv.org/abs/2401.00762v3",
        "categories": [
            "eess.SY",
            "cs.SC",
            "cs.SY",
            "math.AP",
            "93C15, 93B25, 93B30, 34A55, 14E08, 14M20, 14Q20, 12H05, 92B05"
        ]
    },
    {
        "title": "Persistent components in Canny's Generalized Characteristic Polynomial",
        "authors": [
            "Gleb Pogudin"
        ],
        "summary": "When using resultants for elimination, one standard issue is that the resultant vanishes if the variety contains components of dimension larger than the expected dimension. J. Canny proposed an elegant construction, generalized characteristic polynomial, to address this issue by symbolically perturbing the system before the resultant computation. Such perturbed resultant would typically involve artefact components only loosely related to the geometry of the variety of interest. For removing these components, J.M. Rojas proposed to take the greatest common divisor of the results of two different perturbations. In this paper, we investigate this construction, and show that the extra components persistent under taking different perturbations must come either from singularities or from positive-dimensional fibers.",
        "published": "2024-01-03T19:17:55Z",
        "link": "http://arxiv.org/abs/2401.01948v2",
        "categories": [
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Finite Expression Method for Learning Dynamics on Complex Networks",
        "authors": [
            "Zezheng Song",
            "Chunmei Wang",
            "Haizhao Yang"
        ],
        "summary": "Complex network data pervades various real-world domains, including physical, technological, and biological systems. Despite the prevalence of such data, predicting trends and understanding behavioral patterns in complex systems remains challenging due to poorly understood underlying mechanisms. While data-driven methods have made strides in uncovering governing equations from time series data, efforts to extract physical laws from network data are limited and often struggle with incomplete or noisy data. To address these challenges, we introduce a novel approach called the Finite Expression Method (FEX) and its fast algorithm for this learning problem on complex networks. FEX represents dynamics on complex networks using binary trees composed of finite mathematical operators. The nodes within these trees are trained through a combinatorial optimization process guided by reinforcement learning techniques. This unique configuration allows FEX to capture complex dynamics with minimal prior knowledge of the system and a small dictionary of mathematical operators. Our extensive numerical experiments demonstrate that FEX excels in accurately identifying dynamics across diverse network topologies and dynamic behaviors.",
        "published": "2024-01-05T23:47:37Z",
        "link": "http://arxiv.org/abs/2401.03092v2",
        "categories": [
            "cs.SC",
            "cs.NA",
            "math.NA",
            "physics.app-ph"
        ]
    },
    {
        "title": "A semi-numerical algorithm for the homology lattice and periods of   complex elliptic surfaces over the projective line",
        "authors": [
            "Eric Pichon-Pharabod"
        ],
        "summary": "We provide an algorithm for computing an effective basis of homology of elliptic surfaces over the complex projective line on which integration of periods can be carried out. This allows the heuristic recovery of several algebraic invariants of the surface, notably the N\\'eron-Severi lattice, the transcendental lattice, the Mordell-Weil group and the Mordell-Weil lattice. This algorithm comes with a SageMath implementation.",
        "published": "2024-01-10T12:54:03Z",
        "link": "http://arxiv.org/abs/2401.05131v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "Primary 14Q10, Secondary 32G20, 14D05"
        ]
    },
    {
        "title": "Universal Analytic Gr{ö}bner Bases and Tropical Geometry",
        "authors": [
            "Tristan Vaccon",
            "Thibaut Verron"
        ],
        "summary": "A universal analytic Gr{\\\"o}bner basis (UAGB) of an ideal of a Tate algebra is a set containing a local Gr{\\\"o}bner basis for all suitable convergence radii. In a previous article, the authors proved the existence of finite UAGB's for polynomial ideals, leaving open the question of how to compute them. In this paper, we provide an algorithm computing a UAGB for a given polynomial ideal, by traversing the Gr{\\\"o}bner fan of the ideal. As an application, it offers a new point of view on algorithms for computing tropical varieties of homogeneous polynomial ideals, which typically rely on lifting the computations to an algebra of power series. Motivated by effective computations in tropical analytic geometry, we also examine local bases for more general convergence conditions, constraining the radii to a convex polyhedron. In this setting, we provide an algorithm to compute local Gr{\\\"o}bner bases and discuss obstacles towards proving the existence of finite UAGBs. CCS CONCEPTS $\\bullet$ Computing methodologies $\\rightarrow$ Algebraic algorithms.",
        "published": "2024-01-11T09:08:34Z",
        "link": "http://arxiv.org/abs/2401.05759v1",
        "categories": [
            "cs.SC",
            "math.AG",
            "math.NT"
        ]
    },
    {
        "title": "Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents",
        "authors": [
            "Quentin Delfosse",
            "Sebastian Sztwiertnia",
            "Mark Rothermel",
            "Wolfgang Stammer",
            "Kristian Kersting"
        ],
        "summary": "Goal misalignment, reward sparsity and difficult credit assignment are only a few of the many issues that make it difficult for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep neural networks impedes the inclusion of domain experts for inspecting the model and revising suboptimal policies. To this end, we introduce *Successive Concept Bottleneck Agents* (SCoBots), that integrate consecutive concept bottleneck (CB) layers. In contrast to current CB models, SCoBots do not just represent concepts as properties of individual objects, but also as relations between objects which is crucial for many RL tasks. Our experimental results provide evidence of SCoBots' competitive performances, but also of their potential for domain experts to understand and regularize their behavior. Among other things, SCoBots enabled us to identify a previously unknown misalignment problem in the iconic video game, Pong, and resolve it. Overall, SCoBots thus result in more human-aligned RL agents. Our code is available at https://github.com/k4ntz/SCoBots .",
        "published": "2024-01-11T10:38:22Z",
        "link": "http://arxiv.org/abs/2401.05821v4",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "On Hilbert-Poincaré series of affine semi-regular polynomial   sequences and related Gröbner bases",
        "authors": [
            "Momonari Kudo",
            "Kazuhiro Yokoyama"
        ],
        "summary": "Gr\\\"{o}bner bases are nowadays central tools for solving various problems in commutative algebra and algebraic geometry. A typical use of Gr\\\"{o}bner bases is the multivariate polynomial system solving, which enables us to construct algebraic attacks against post-quantum cryptographic protocols. Therefore, the determination of the complexity of computing Gr\\\"{o}bner bases is very important both in theory and in practice: One of the most important cases is the case where input polynomials compose an (overdetermined) affine semi-regular sequence. The first part of this paper aims to present a survey on Gr\\\"{o}bner basis computation and its complexity. In the second part, we shall give an explicit formula on the (truncated) Hilbert-Poincar\\'{e} series associated to the homogenization of an affine semi-regular sequence. Based on the formula, we also study (reduced) Gr\\\"{o}bner bases of the ideals generated by an affine semi-regular sequence and its homogenization. Some of our results are considered to give mathematically rigorous proofs of the correctness of methods for computing Gr\\\"{o}bner bases of the ideal generated by an affine semi-regular sequence.",
        "published": "2024-01-15T15:26:52Z",
        "link": "http://arxiv.org/abs/2401.07768v2",
        "categories": [
            "cs.SC",
            "math.AC",
            "math.AG"
        ]
    },
    {
        "title": "Submodule approach to creative telescoping",
        "authors": [
            "Mark van Hoeij"
        ],
        "summary": "This paper proposes ideas to speed up the process of creative telescoping, particularly when the telescoper is reducible. One can interpret telescoping as computing an annihilator $L \\in D$ for an element $m$ in a $D$-module $M$. The main idea is to look for submodules of $M$. If $N$ is a non-trivial submodule of $M$, constructing the minimal operator $R$ of the image of $m$ in $M/N$ gives a right-factor of $L$ in $D$. Then $L = L' R$ where the left-factor $L'$ is the telescoper of $R(m) \\in N$. To expedite computing $L'$, compute the action of $D$ on a natural basis of $N$, then obtain $L'$ with a cyclic vector computation.   The next main idea is that when $N$ has automorphisms, use them to construct submodules. An automorphism with distinct eigenvalues can be used to decompose $N$ as a direct sum $N_1 \\oplus \\cdots \\oplus N_k$. Then $L'$ is the LCLM (Least Common Left Multiple) of $L_1, \\ldots, L_k$ where $L_i$ is the telescoper of the projection of $R(m)$ on $N_i$. An LCLM can greatly increase the degrees of coefficients, so $L'$ and $L$ can be much larger expressions than the factors $L_1,\\ldots,L_k$ and $R$. Examples show that computing each factor $L_i$ and $R$ seperately can save a lot of CPU time compared to computing $L$ in expanded form with standard creative telescoping.",
        "published": "2024-01-16T15:59:40Z",
        "link": "http://arxiv.org/abs/2401.08455v2",
        "categories": [
            "cs.SC",
            "39A04",
            "G.2.1"
        ]
    },
    {
        "title": "Hypergeometric Solutions of Linear Difference Systems",
        "authors": [
            "Moulay Barkatou",
            "Mark van Hoeij",
            "Johannes Middeke",
            "Yi Zhou"
        ],
        "summary": "We extend Petkov\\v{s}ek's algorithm for computing hypergeometric solutions of scalar difference equations to the case of difference systems $\\tau(Y) = M Y$, with $M \\in {\\rm GL}_n(C(x))$, where $\\tau$ is the shift operator. Hypergeometric solutions are solutions of the form $\\gamma P$ where $P \\in C(x)^n$ and $\\gamma$ is a hypergeometric term over $C(x)$, i.e. ${\\tau(\\gamma)}/{\\gamma} \\in C(x)$. Our contributions concern efficient computation of a set of candidates for ${\\tau(\\gamma)}/{\\gamma}$ which we write as $\\lambda = c\\frac{A}{B}$ with monic $A, B \\in C[x]$, $c \\in C^*$. Factors of the denominators of $M^{-1}$ and $M$ give candidates for $A$ and $B$, while another algorithm is needed for $c$. We use the super-reduction algorithm to compute candidates for $c$, as well as other ingredients to reduce the list of candidates for $A/B$. To further reduce the number of candidates $A/B$, we bound the so-called type of $A/B$ by bounding local types. Our algorithm has been implemented in Maple and experiments show that our implementation can handle systems of high dimension, which is useful for factoring operators.",
        "published": "2024-01-16T16:19:09Z",
        "link": "http://arxiv.org/abs/2401.08470v2",
        "categories": [
            "cs.SC",
            "39A04",
            "G.2.1"
        ]
    },
    {
        "title": "Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive   Symbolic Regression Framework",
        "authors": [
            "Tianhao Chen",
            "Pengbo Xu",
            "Haibiao Zheng"
        ],
        "summary": "In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal information mining behind the data, missing a multimodal framework akin to that in the image-text domain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-text domain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.",
        "published": "2024-01-18T06:19:05Z",
        "link": "http://arxiv.org/abs/2401.09748v1",
        "categories": [
            "cs.SC",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "EFO: the Emotion Frame Ontology",
        "authors": [
            "Stefano De Giorgis",
            "Aldo Gangemi"
        ],
        "summary": "Emotions are a subject of intense debate in various disciplines. Despite the proliferation of theories and definitions, there is still no consensus on what emotions are, and how to model the different concepts involved when we talk about - or categorize - them. In this paper, we propose an OWL frame-based ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as semantic frames, with a set of semantic roles that capture the different aspects of emotional experience. EFO follows pattern-based ontology design, and is aligned to the DOLCE foundational ontology. EFO is used to model multiple emotion theories, which can be cross-linked as modules in an Emotion Ontology Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE) Theory as an EFO-BE module, and demonstrate how to perform automated inferences on the representation of emotion situations. EFO-BE has been evaluated by lexicalizing the BE emotion frames from within the Framester knowledge graph, and implementing a graph-based emotion detector from text. In addition, an EFO integration of multimodal datasets, including emotional speech and emotional face expressions, has been performed to enable further inquiry into crossmodal emotion semantics.",
        "published": "2024-01-19T15:20:57Z",
        "link": "http://arxiv.org/abs/2401.10751v1",
        "categories": [
            "cs.AI",
            "cs.CY",
            "cs.SC"
        ]
    },
    {
        "title": "Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming   for Policy Optimization in Mixed Discrete-Continuous MDPs",
        "authors": [
            "Michael Gimelfarb",
            "Ayal Taitler",
            "Scott Sanner"
        ],
        "summary": "We propose Constraint-Generation Policy Optimization (CGPO) for optimizing policy parameters within compact and interpretable policy classes for mixed discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able to provide bounded policy error guarantees over an infinite range of initial states for many DC-MDPs with expressive nonlinear dynamics, but it can also provably derive optimal policies in cases where it terminates with zero error. Furthermore, CGPO can generate worst-case state trajectories to diagnose policy deficiencies and provide counterfactual explanations of optimal actions. To achieve such results, CGPO proposes a bi-level mixed-integer nonlinear optimization framework for optimizing policies within defined expressivity classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint generation methodology that adversarially generates worst-case state trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can obtain solutions with bounded optimality gap guarantees. We handle stochastic transitions through explicit marginalization (where applicable) or chance-constraints, providing high-probability policy performance guarantees. We also present a road-map for understanding the computational complexities associated with different expressivity classes of policy, reward, and transition dynamics. We experimentally demonstrate the applicability of CGPO in diverse domains, including inventory control, management of a system of water reservoirs, and physics control. In summary, we provide a solution for deriving structured, compact, and explainable policies with bounded performance guarantees, enabling worst-case scenario generation and counterfactual policy diagnostics.",
        "published": "2024-01-20T07:12:57Z",
        "link": "http://arxiv.org/abs/2401.12243v1",
        "categories": [
            "math.OC",
            "cs.LG",
            "cs.RO",
            "cs.SC",
            "cs.SY",
            "eess.SY"
        ]
    },
    {
        "title": "Towards Automatic Transformations of Coq Proof Scripts",
        "authors": [
            "Nicolas Magaud"
        ],
        "summary": "Proof assistants like Coq are increasingly popular to help mathematicians carry out proofs of the results they conjecture. However, formal proofs remain highly technical and are especially difficult to reuse. In this paper, we present a framework to carry out a posteriori script transformations. These transformations are meant to be applied as an automated post-processing step, once the proof has been completed. As an example, we present a transformation which takes an arbitrary large proof script and produces an equivalent single-line proof script, which can be executed by Coq in one single step. Other applications, such as fully expanding a proof script (for debugging purposes), removing all named hypotheses, etc. could be developed within this framework. We apply our tool to various Coq proof scripts, including some from the GeoCoq library.",
        "published": "2024-01-22T12:48:34Z",
        "link": "http://arxiv.org/abs/2401.11897v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "cs.SE"
        ]
    },
    {
        "title": "Showing Proofs, Assessing Difficulty with GeoGebra Discovery",
        "authors": [
            "Zoltán Kovács",
            "Tomás Recio",
            "M. Pilar Vélez"
        ],
        "summary": "In our contribution we describe some on-going improvements concerning the Automated Reasoning Tools developed in GeoGebra Discovery, providing different examples of the performance of these new features. We describe the new ShowProof command, that outputs both the sequence of the different steps performed by GeoGebra Discovery to confirm a certain statement, as well as a number intending to grade the difficulty or interest of the assertion. The proposal of this assessment measure, involving the comparison of the expression of the thesis (or conclusion) as a combination of the hypotheses, will be developed.",
        "published": "2024-01-22T12:50:12Z",
        "link": "http://arxiv.org/abs/2401.11900v1",
        "categories": [
            "cs.SC",
            "cs.AI",
            "cs.CG"
        ]
    },
    {
        "title": "Open Source Prover in the Attic",
        "authors": [
            "Zoltán Kovács",
            "Alexander Vujic"
        ],
        "summary": "The well known JGEX program became open source a few years ago, but seemingly, further development of the program can only be done without the original authors. In our project, we are looking at whether it is possible to continue such a large project as a newcomer without the involvement of the original authors. Is there a way to internationalize, fix bugs, improve the code base, add new features? In other words, to save a relic found in the attic and polish it into a useful everyday tool.",
        "published": "2024-01-22T12:50:29Z",
        "link": "http://arxiv.org/abs/2401.13702v1",
        "categories": [
            "cs.PL",
            "cs.MS",
            "cs.SC",
            "cs.SE"
        ]
    },
    {
        "title": "Solving with GeoGebra Discovery an Austrian Mathematics Olympiad   problem: Lessons Learned",
        "authors": [
            "Belén Ariño-Morera",
            "Zoltán Kovács",
            "Tomás Recio",
            "Piedad Tolmos"
        ],
        "summary": "We address, through the automated reasoning tools in GeoGebra Discovery, a problem from a regional phase of the Austrian Mathematics Olympiad 2023. Trying to solve this problem gives rise to four different kind of feedback: the almost instantaneous, automated solution of the proposed problem; the measure of its complexity, according to some recent proposals; the automated discovery of a generalization of the given assertion, showing that the same statement is true over more general polygons than those mentioned in the problem; and the difficulties associated to the analysis of the surprising and involved high number of degenerate cases that appear when using the LocusEquation command in this problem. In our communication we will describe and reflect on these diverse issues, enhancing its exemplar role for showing some of the advantages, problems, and current fields of development of GeoGebra Discovery.",
        "published": "2024-01-22T12:51:35Z",
        "link": "http://arxiv.org/abs/2401.11906v1",
        "categories": [
            "cs.SC",
            "cs.AI",
            "cs.CG"
        ]
    },
    {
        "title": "Solving Some Geometry Problems of the Náboj 2023 Contest with   Automated Deduction in GeoGebra Discovery",
        "authors": [
            "Amela Hota",
            "Zoltán Kovács",
            "Alexander Vujic"
        ],
        "summary": "In this article, we solve some of the geometry problems of the N\\'aboj 2023 competition with the help of a computer, using examples that the software tool GeoGebra Discovery can calculate. In each case, the calculation requires symbolic computations. We analyze the difficulty of feeding the problem into the machine and set further goals to make the problems of this type of contests even more tractable in the future.",
        "published": "2024-01-22T12:51:51Z",
        "link": "http://arxiv.org/abs/2401.13703v1",
        "categories": [
            "math.HO",
            "cs.AI",
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "Using Java Geometry Expert as Guide in the Preparations for Math   Contests",
        "authors": [
            "Ines Ganglmayr",
            "Zoltán Kovács"
        ],
        "summary": "We give an insight into Java Geometry Expert (JGEX) in use in a school context, focusing on the Austrian school system. JGEX can offer great support in some classroom situations, especially for solving mathematical competition tasks. Also, we discuss some limitations of the program.",
        "published": "2024-01-22T12:52:07Z",
        "link": "http://arxiv.org/abs/2401.13704v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "The Locus Story of a Rocking Camel in a Medical Center in the City of   Freistadt",
        "authors": [
            "Anna Käferböck",
            "Zoltán Kovács"
        ],
        "summary": "We give an example of automated geometry reasoning for an imaginary classroom project by using the free software package GeoGebra Discovery. The project is motivated by a publicly available toy, a rocking camel, installed at a medical center in Upper Austria. We explain how the process of a false conjecture, experimenting, modeling, a precise mathematical setup, and then a proof by automated reasoning could help extend mathematical knowledge at secondary school level and above.",
        "published": "2024-01-22T12:52:22Z",
        "link": "http://arxiv.org/abs/2401.11908v1",
        "categories": [
            "cs.RO",
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "3D Space Trajectories and beyond: Abstract Art Creation with 3D Printing",
        "authors": [
            "Thierry Dana-Picard",
            "Matias Tejera",
            "Eva Ulbrich"
        ],
        "summary": "We present simple models of trajectories in space, both in 2D and in 3D. The first examples, which model bicircular moves in the same direction, are classical curves (epicycloids, etc.). Then, we explore bicircular moves in reverse direction and tricircular moves in 2D and 3D, to explore complex visualisations of extraplanetary movements. These moves are studied in a plane setting. Then, adding increasing complexity, we explore them in a non planar setting (which is a closer model of the real situation). The exploration is followed by using these approaches for creating mathematical art in 2D and 3D printed objects, providing new ways of mathematical representations. Students' activities are organized around this exploration.",
        "published": "2024-01-22T12:52:38Z",
        "link": "http://arxiv.org/abs/2401.11909v1",
        "categories": [
            "cs.CG",
            "cs.SC"
        ]
    },
    {
        "title": "Computation of classical and $v$-adic $L$-series of $t$-motives",
        "authors": [
            "Xavier Caruso",
            "Quentin Gazda"
        ],
        "summary": "We design an algorithm for computing the $L$-series associated to an Anderson $t$-motives, exhibiting quasilinear complexity with respect to the target precision. Based on experiments, we conjecture that the order of vanishing at $T=1$ of the $v$-adic $L$-series of a given Anderson $t$-motive with good reduction does not depend on the finite place $v$.",
        "published": "2024-01-23T10:16:47Z",
        "link": "http://arxiv.org/abs/2401.12618v1",
        "categories": [
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "Local Hamiltonian decomposition and classical simulation of parametrized   quantum circuits",
        "authors": [
            "Bibhas Adhikari",
            "Aryan Jha"
        ],
        "summary": "In this paper we develop a classical algorithm of complexity $O(K \\, 2^n)$ to simulate parametrized quantum circuits (PQCs) of $n$ qubits, where $K$ is the total number of one-qubit and two-qubit control gates. The algorithm is developed by finding $2$-sparse unitary matrices of order $2^n$ explicitly corresponding to any single-qubit and two-qubit control gates in an $n$-qubit system. Finally, we determine analytical expression of Hamiltonians for any such gate and consequently a local Hamiltonian decomposition of any PQC is obtained. All results are validated with numerical simulations.",
        "published": "2024-01-24T00:30:31Z",
        "link": "http://arxiv.org/abs/2401.13156v2",
        "categories": [
            "quant-ph",
            "cs.SC"
        ]
    },
    {
        "title": "Lessons on Datasets and Paradigms in Machine Learning for Symbolic   Computation: A Case Study on CAD",
        "authors": [
            "Tereso del Río",
            "Matthew England"
        ],
        "summary": "Symbolic Computation algorithms and their implementation in computer algebra systems often contain choices which do not affect the correctness of the output but can significantly impact the resources required: such choices can benefit from having them made separately for each problem via a machine learning model. This study reports lessons on such use of machine learning in symbolic computation, in particular on the importance of analysing datasets prior to machine learning and on the different machine learning paradigms that may be utilised. We present results for a particular case study, the selection of variable ordering for cylindrical algebraic decomposition, but expect that the lessons learned are applicable to other decisions in symbolic computation.   We utilise an existing dataset of examples derived from applications which was found to be imbalanced with respect to the variable ordering decision. We introduce an augmentation technique for polynomial systems problems that allows us to balance and further augment the dataset, improving the machine learning results by 28\\% and 38\\% on average, respectively. We then demonstrate how the existing machine learning methodology used for the problem $-$ classification $-$ might be recast into the regression paradigm. While this does not have a radical change on the performance, it does widen the scope in which the methodology can be applied to make choices.",
        "published": "2024-01-24T10:12:43Z",
        "link": "http://arxiv.org/abs/2401.13343v2",
        "categories": [
            "cs.SC",
            "cs.LG",
            "68W30, 68T05, 03C10",
            "I.2.6; I.1.0"
        ]
    },
    {
        "title": "Symbolic Equation Solving via Reinforcement Learning",
        "authors": [
            "Lennart Dabelow",
            "Masahito Ueda"
        ],
        "summary": "Machine-learning methods are gradually being adopted in a wide variety of social, economic, and scientific contexts, yet they are notorious for struggling with exact mathematics. A typical example is computer algebra, which includes tasks like simplifying mathematical terms, calculating formal derivatives, or finding exact solutions of algebraic equations. Traditional software packages for these purposes are commonly based on a huge database of rules for how a specific operation (e.g., differentiation) transforms a certain term (e.g., sine function) into another one (e.g., cosine function). These rules have usually needed to be discovered and subsequently programmed by humans. Efforts to automate this process by machine-learning approaches are faced with challenges like the singular nature of solutions to mathematical problems, when approximations are unacceptable, as well as hallucination effects leading to flawed reasoning. We propose a novel deep-learning interface involving a reinforcement-learning agent that operates a symbolic stack calculator to explore mathematical relations. By construction, this system is capable of exact transformations and immune to hallucination. Using the paradigmatic example of solving linear equations in symbolic form, we demonstrate how our reinforcement-learning agent autonomously discovers elementary transformation rules and step-by-step solutions.",
        "published": "2024-01-24T13:42:24Z",
        "link": "http://arxiv.org/abs/2401.13447v2",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Symbolic-numeric algorithm for parameter estimation in discrete-time   models with $\\exp$",
        "authors": [
            "Yosef Berman",
            "Joshua Forrest",
            "Matthew Grote",
            "Alexey Ovchinnikov",
            "Sonia Rueda"
        ],
        "summary": "Dynamic models describe phenomena across scientific disciplines, yet to make these models useful in application the unknown parameter values of the models must be determined. Discrete-time dynamic models are widely used to model biological processes, but it is often difficult to determine these parameters. In this paper, we propose a symbolic-numeric approach for parameter estimation in discrete-time models that involve univariate non-algebraic (locally) analytic functions such as exp. We illustrate the performance (precision) of our approach by applying our approach to two archetypal discrete-time models in biology (the flour beetle 'LPA' model and discrete Lotka-Volterra competition model). Unlike optimization-based methods, our algorithm guarantees to find all solutions of the parameter values up to a specified precision given time-series data for the measured variables provided that there are finitely many parameter values that fit the data and that the used polynomial system solver can find all roots of the associated polynomial system with interval coefficients.",
        "published": "2024-01-29T15:19:16Z",
        "link": "http://arxiv.org/abs/2401.16220v2",
        "categories": [
            "q-bio.QM",
            "cs.SC",
            "cs.SY",
            "eess.SY",
            "math.AC",
            "math.DS",
            "92B05, 68W30, 14Q20, 39A60, 13P15"
        ]
    },
    {
        "title": "Creative Telescoping for Hypergeometric Double Sums",
        "authors": [
            "Peter Paule",
            "Carsten Schneider"
        ],
        "summary": "We present efficient methods for calculating linear recurrences of hypergeometric double sums and, more generally, of multiple sums. In particular, we supplement this approach with the algorithmic theory of contiguous relations, which guarantees the applicability of our method for many input sums. In addition, we elaborate new techniques to optimize the underlying key task of our method to compute rational solutions of parameterized linear recurrences.",
        "published": "2024-01-29T17:18:47Z",
        "link": "http://arxiv.org/abs/2401.16314v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "On the Algorithmic Verification of Nonlinear Superposition for Systems   of First Order Ordinary Differential Equations",
        "authors": [
            "Veronika Treumova",
            "Dmitry A. Lyakhov",
            "Dominik L. Michels"
        ],
        "summary": "This paper belongs to a group of work in the intersection of symbolic computation and group analysis aiming for the symbolic analysis of differential equations. The goal is to extract important properties without finding the explicit general solution. In this contribution, we introduce the algorithmic verification of nonlinear superposition properties and its implementation. More exactly, for a system of nonlinear ordinary differential equations of first order with a polynomial right-hand side, we check if the differential system admits a general solution by means of a superposition rule and a certain number of particular solutions. It is based on the theory of Newton polytopes and associated symbolic computation. The developed method provides the basis for the identification of nonlinear superpositions within a given system and for the construction of numerical methods which preserve important algebraic properties at the numerical level.",
        "published": "2024-01-30T13:47:46Z",
        "link": "http://arxiv.org/abs/2401.17012v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "math.NA",
            "math.RA"
        ]
    },
    {
        "title": "Validated numerics for algebraic path tracking",
        "authors": [
            "Alexandre Guillemot",
            "Pierre Lairez"
        ],
        "summary": "Using validated numerical methods, interval arithmetic and Taylor models, we propose a certified predictor-corrector loop for tracking zeros of polynomial systems with a parameter. We provide a Rust implementation which shows tremendous improvement over existing software for certified path tracking.",
        "published": "2024-01-31T16:28:17Z",
        "link": "http://arxiv.org/abs/2401.17973v2",
        "categories": [
            "math.NA",
            "cs.NA",
            "cs.SC"
        ]
    },
    {
        "title": "SymbolicAI: A framework for logic-based approaches combining generative   models and solvers",
        "authors": [
            "Marius-Constantin Dinu",
            "Claudiu Leoveanu-Condrei",
            "Markus Holzleitner",
            "Werner Zellinger",
            "Sepp Hochreiter"
        ],
        "summary": "We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.",
        "published": "2024-02-01T18:50:50Z",
        "link": "http://arxiv.org/abs/2402.00854v4",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC",
            "cs.SE"
        ]
    },
    {
        "title": "SMLP: Symbolic Machine Learning Prover",
        "authors": [
            "Franz Brauße",
            "Zurab Khasidashvili",
            "Konstantin Korovin"
        ],
        "summary": "Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors. SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods. SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level. SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models.",
        "published": "2024-02-02T13:53:29Z",
        "link": "http://arxiv.org/abs/2402.01415v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.LO",
            "cs.SC",
            "math.OC"
        ]
    },
    {
        "title": "Computing roadmaps in unbounded smooth real algebraic sets II: algorithm   and complexity",
        "authors": [
            "Rémi Prébet",
            "Mohab Safey El Din",
            "Éric Schost"
        ],
        "summary": "A roadmap for an algebraic set $V$ defined by polynomials with coefficients in some real field, say $\\mathbb{R}$, is an algebraic curve contained in $V$ whose intersection with all connected components of $V\\cap\\mathbb{R}^{n}$ is connected. These objects, introduced by Canny, can be used to answer connectivity queries over $V\\cap \\mathbb{R}^{n}$ provided that they are required to contain the finite set of query points $\\mathcal{P}\\subset V$; in this case,we say that the roadmap is associated to $(V, \\mathcal{P})$.   In this paper, we make effective a connectivity result we previously proved, to design a Monte Carlo algorithm which, on input (i) a finite sequence of polynomials defining $V$ (and satisfying some regularity assumptions) and (ii) an algebraic representation of finitely many query points $\\mathcal{P}$ in $V$, computes a roadmap for $(V, \\mathcal{P})$. This algorithm generalizes the nearly optimal one introduced by the last two authors by dropping a boundedness assumption on the real trace of $V$.   The output size and running times of our algorithm are both polynomial in $(nD)^{n\\log d}$, where $D$ is the maximal degree of the input equations and $d$ is the dimension of $V$. As far as we know, the best previously known algorithm dealing with such sets has an output size and running time polynomial in $(nD)^{n\\log^2 n}$.",
        "published": "2024-02-05T15:44:16Z",
        "link": "http://arxiv.org/abs/2402.03111v1",
        "categories": [
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Computing Generic Fibers of Polynomial Ideals with FGLM and Hensel   Lifting",
        "authors": [
            "Jérémy Berthomieu",
            "Rafael Mohr"
        ],
        "summary": "We describe a version of the FGLM algorithm that can be used to compute generic fibers of positive-dimensional polynomial ideals. It combines the FGLM algorithm with a Hensel lifting strategy. In analogy with Hensel lifting, we show that this algorithm has a complexity quasi-linear in the number of terms of certain $\\mathfrak{m}$-adic expansions we compute. Some provided experimental data also demonstrates the practical efficacy of our algorithm.",
        "published": "2024-02-05T16:12:27Z",
        "link": "http://arxiv.org/abs/2402.03144v2",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "A quasi-optimal lower bound for skew polynomial multiplication",
        "authors": [
            "Qiyuan Chen",
            "Ke Ye"
        ],
        "summary": "We establish a lower bound for the complexity of multiplying two skew polynomials. The lower bound coincides with the upper bound conjectured by Caruso and Borgne in 2017, up to a log factor. We present algorithms for three special cases, indicating that the aforementioned lower bound is quasi-optimal. In fact, our lower bound is quasi-optimal in the sense of bilinear complexity. In addition, we discuss the average bilinear complexity of simultaneous multiplication of skew polynomials and the complexity of skew polynomial multiplication in the case of towers of extensions.",
        "published": "2024-02-06T16:38:11Z",
        "link": "http://arxiv.org/abs/2402.04134v1",
        "categories": [
            "cs.CC",
            "cs.SC",
            "math.RA"
        ]
    },
    {
        "title": "Algebraic identifiability of partial differential equation models",
        "authors": [
            "Helen Byrne",
            "Heather Harrington",
            "Alexey Ovchinnikov",
            "Gleb Pogudin",
            "Hamid Rahkooy",
            "Pedro Soto"
        ],
        "summary": "Differential equation models are crucial to scientific processes. The values of model parameters are important for analyzing the behaviour of solutions. A parameter is called globally identifiable if its value can be uniquely determined from the input and output functions. To determine if a parameter estimation problem is well-posed for a given model, one must check if the model parameters are globally identifiable. This problem has been intensively studied for ordinary differential equation models, with theory and several efficient algorithms and software packages developed. A comprehensive theory of algebraic identifiability for PDEs has hitherto not been developed due to the complexity of initial and boundary conditions. Here, we provide theory and algorithms, based on differential algebra, for testing identifiability of polynomial PDE models. We showcase this approach on PDE models arising in the sciences.",
        "published": "2024-02-06T18:49:51Z",
        "link": "http://arxiv.org/abs/2402.04241v1",
        "categories": [
            "q-bio.QM",
            "cs.SC",
            "cs.SY",
            "eess.SY",
            "math.AP",
            "92B05, 12H05, 35R30, 93C20, 93B25, 93B30"
        ]
    },
    {
        "title": "Factorial Basis Method for q-Series Applications",
        "authors": [
            "Antonio Jiménez-Pastor",
            "Ali Kemal Uncu"
        ],
        "summary": "The Factorial Basis method, initially designed for quasi-triangular, shift-compatible factorial bases, provides solutions to linear recurrence equations in the form of definite-sums. This paper extends the Factorial Basis method to its q-analog, enabling its application in q-calculus. We demonstrate the adaptation of the method to q-sequences and its utility in the realm of q-combinatorics. The extended technique is employed to automatically prove established identities and unveil novel ones, particularly some associated with the Rogers-Ramanujan identities.",
        "published": "2024-02-06T20:47:58Z",
        "link": "http://arxiv.org/abs/2402.04392v1",
        "categories": [
            "cs.SC",
            "math.CO",
            "05A15, 05A30, 68R05, 05-04",
            "G.2.1; I.1.4; F.2.2"
        ]
    },
    {
        "title": "Parallel Summation in P-Recursive Extensions",
        "authors": [
            "Shaoshi Chen",
            "Ruyong Feng",
            "Manuel Kauers",
            "Xiuyun Li"
        ],
        "summary": "We propose investigating a summation analog of the paradigm for parallel integration. We make some first steps towards an indefinite summation method applicable to summands that rationally depend on the summation index and a P-recursive sequence and its shifts. There is a distinction between so-called normal and so-called special polynomials. Under the assumption that the corresponding difference field has no unnatural constants, we are able to predict the normal polynomials appearing in the denominator of a potential closed form. We can also handle the numerator. Our method is incomplete so far as we cannot predict the special polynomials appearing in the denominator. However, we do have some structural results about special polynomials for the setting under consideration.",
        "published": "2024-02-07T09:19:01Z",
        "link": "http://arxiv.org/abs/2402.04684v2",
        "categories": [
            "math.CO",
            "cs.SC"
        ]
    },
    {
        "title": "Automated Data-Driven Discovery of Material Models Based on Symbolic   Regression: A Case Study on Human Brain Cortex",
        "authors": [
            "Jixin Hou",
            "Xianyan Chen",
            "Taotao Wu",
            "Ellen Kuhl",
            "Xianqiao Wang"
        ],
        "summary": "We introduce a data-driven framework to automatically identify interpretable and physically meaningful hyperelastic constitutive models from sparse data. Leveraging symbolic regression, an algorithm based on genetic programming, our approach generates elegant hyperelastic models that achieve accurate data fitting through parsimonious mathematic formulae, while strictly adhering to hyperelasticity constraints such as polyconvexity. Our investigation spans three distinct hyperelastic models -- invariant-based, principal stretch-based, and normal strain-based -- and highlights the versatility of symbolic regression. We validate our new approach using synthetic data from five classic hyperelastic models and experimental data from the human brain to demonstrate algorithmic efficacy. Our results suggest that our symbolic regression robustly discovers accurate models with succinct mathematic expressions in invariant-based, stretch-based, and strain-based scenarios. Strikingly, the strain-based model exhibits superior accuracy, while both stretch- and strain-based models effectively capture the nonlinearity and tension-compression asymmetry inherent to human brain tissue. Polyconvexity examinations affirm the rigor of convexity within the training regime and demonstrate excellent extrapolation capabilities beyond this regime for all three models. However, the stretch-based models raise concerns regarding potential convexity loss under large deformations. Finally, robustness tests on noise-embedded data underscore the reliability of our symbolic regression algorithms. Our study confirms the applicability and accuracy of symbolic regression in the automated discovery of hyperelastic models for the human brain and gives rise to a wide variety of applications in other soft matter systems.",
        "published": "2024-02-07T20:31:07Z",
        "link": "http://arxiv.org/abs/2402.05238v1",
        "categories": [
            "cs.SC",
            "q-bio.QM",
            "q-bio.TO"
        ]
    },
    {
        "title": "Quantifier Elimination for Normal Cone Computations",
        "authors": [
            "Michael Mandlmayr",
            "Ali Kemal Uncu"
        ],
        "summary": "We present effective procedures to calculate regular normal cones and other related objects using quantifier elimination. This method of normal cone calculations is complementary to computing Lagrangians and it works best at points where the constraint qualifications fail and extra work for other methods becomes inevitable. This method also serves as a tool to calculate the regular co-derivative for semismooth* Newton methods. We list algorithms and their demonstrations of different use cases for this approach.",
        "published": "2024-02-08T11:27:58Z",
        "link": "http://arxiv.org/abs/2402.05579v1",
        "categories": [
            "math.OC",
            "cs.NA",
            "cs.SC",
            "math.NA",
            "49J53, 03C10, 49J52, 68V15, 74P10, 90C23, 90C30, 90C31, 90C53",
            "I.1.4"
        ]
    },
    {
        "title": "Strassen's algorithm is not optimally accurate",
        "authors": [
            "Jean-Guillaume Dumas",
            "Clément Pernet",
            "Alexandre Sedoglavic"
        ],
        "summary": "We propose a non-commutative algorithm for multiplying 2x2 matrices using 7 coefficient products. This algorithm reaches simultaneously a better accuracy in practice compared to previously known such fast algorithms, and a time complexity bound with the best currently known leading term (obtained via alternate basis sparsification). To build this algorithm, we consider matrix and tensor norms bounds governing the stability and accuracy of numerical matrix multiplication. First, we reduce those bounds by minimizing a growth factor along the unique orbit of Strassen's 2x2-matrix multiplication tensor decomposition. Second, we develop heuristics for minimizing the number of operations required to realize a given bilinear formula, while further improving its accuracy. Third, we perform an alternate basis sparsification that improves on the time complexity constant and mostly preserves the overall accuracy.",
        "published": "2024-02-08T12:36:35Z",
        "link": "http://arxiv.org/abs/2402.05630v2",
        "categories": [
            "math.NA",
            "cs.NA",
            "cs.SC"
        ]
    },
    {
        "title": "Subalgebra and Khovanskii bases equivalence",
        "authors": [
            "Colin Alstad",
            "Michael Burr",
            "Oliver Clarke",
            "Timothy Duff"
        ],
        "summary": "The main results of this paper establish a partial correspondence between two previously-studied analogues of Groebner bases in the setting of algebras: namely, subalgebra (aka SAGBI) bases for quotients of polynomial rings and Khovanskii bases for valued algebras. We aim to bridge the gap between the concrete, computational aspects of the former and the more abstract theory of the latter. Our philosophy is that most interesting examples of Khovanskii bases can also be realized as subalgebra bases and vice-versa. We also discuss the computation of Newton-Okounkov bodies, illustrating how interpreting Khovanskii bases as subalgebra bases makes them more amenable to the existing computer algebra tools.",
        "published": "2024-02-08T21:00:13Z",
        "link": "http://arxiv.org/abs/2402.06057v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "14Q20 (Primary)"
        ]
    },
    {
        "title": "Certified homotopy tracking using the Krawczyk method",
        "authors": [
            "Timothy Duff",
            "Kisun Lee"
        ],
        "summary": "We revisit the problem of certifying the correctness of approximate solution paths computed by numerical homotopy continuation methods. We propose a conceptually simple approach based on a parametric variant of the Krawczyk method from interval arithmetic. Unlike most previous methods for certified path-tracking, our approach is applicable in the general setting of parameter homotopies commonly used to solve polynomial systems of equations. We also describe a novel preconditioning strategy and give theoretical correctness and termination results. Experiments using a preliminary implementation of the method indicate that our approach is competitive with specialized methods appearing previously in the literature, in spite of our more general setting.",
        "published": "2024-02-10T21:53:06Z",
        "link": "http://arxiv.org/abs/2402.07053v2",
        "categories": [
            "math.NA",
            "cs.NA",
            "cs.SC",
            "math.AG",
            "65H14"
        ]
    },
    {
        "title": "Reading Rational Univariate Representations on lexicographic Groebner   bases",
        "authors": [
            "Alexander Demin",
            "Fabrice Rouillier",
            "Joao Ruiz"
        ],
        "summary": "In this contribution, we consider a zero-dimensional polynomial system in $n$ variables defined over a field $\\mathbb{K}$. In the context of computing a Rational Univariate Representation (RUR) of its solutions, we address the problem of certifying a separating linear form and, once certified, calculating the RUR that comes from it, without any condition on the ideal else than being zero-dimensional. Our key result is that the RUR can be read (closed formula) from lexicographic Groebner bases of bivariate elimination ideals, even in the case where the original ideal that is not in shape position, so that one can use the same core as the well known FGLM method to propose a simple algorithm. Our first experiments, either with a very short code (300 lines) written in Maple or with a Julia code using straightforward implementations performing only classical Gaussian reductions in addition to Groebner bases for the degree reverse lexicographic ordering, show that this new method is already competitive with sophisticated state of the art implementations which do not certify the parameterizations.",
        "published": "2024-02-11T09:50:49Z",
        "link": "http://arxiv.org/abs/2402.07141v1",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Computing discrete residues of rational functions",
        "authors": [
            "Carlos E. Arreche",
            "Hari P. Sitaula"
        ],
        "summary": "In 2012 Chen and Singer introduced the notion of discrete residues for rational functions as a complete obstruction to rational summability. More explicitly, for a given rational function f(x), there exists a rational function g(x) such that f(x) = g(x+1) - g(x) if and only if every discrete residue of f(x) is zero. Discrete residues have many important further applications beyond summability: to creative telescoping problems, thence to the determination of (differential-)algebraic relations among hypergeometric sequences, and subsequently to the computation of (differential) Galois groups of difference equations. However, the discrete residues of a rational function are defined in terms of its complete partial fraction decomposition, which makes their direct computation impractical due to the high complexity of completely factoring arbitrary denominator polynomials into linear factors. We develop a factorization-free algorithm to compute discrete residues of rational functions, relying only on gcd computations and linear algebra.",
        "published": "2024-02-11T23:28:31Z",
        "link": "http://arxiv.org/abs/2402.07328v1",
        "categories": [
            "cs.SC",
            "math.AC",
            "math.CO",
            "39A06, 33F10, 68W30, 40C15, 11Y50",
            "I.1.2; F.2.1"
        ]
    },
    {
        "title": "Computing Krylov iterates in the time of matrix multiplication",
        "authors": [
            "Vincent Neiger",
            "Clément Pernet",
            "Gilles Villard"
        ],
        "summary": "Krylov methods rely on iterated matrix-vector products $A^k u_j$ for an $n\\times n$ matrix $A$ and vectors $u_1,\\ldots,u_m$. The space spanned by all iterates $A^k u_j$ admits a particular basis -- the \\emph{maximal Krylov basis} -- which consists of iterates of the first vector $u_1, Au_1, A^2u_1,\\ldots$, until reaching linear dependency, then iterating similarly the subsequent vectors until a basis is obtained. Finding minimal polynomials and Frobenius normal forms is closely related to computing maximal Krylov bases. The fastest way to produce these bases was, until this paper, Keller-Gehrig's 1985 algorithm whose complexity bound $O(n^\\omega \\log(n))$ comes from repeated squarings of $A$ and logarithmically many Gaussian eliminations. Here $\\omega>2$ is a feasible exponent for matrix multiplication over the base field. We present an algorithm computing the maximal Krylov basis in $O(n^\\omega\\log\\log(n))$ field operations when $m \\in O(n)$, and even $O(n^\\omega)$ as soon as $m\\in O(n/\\log(n)^c)$ for some fixed real $c>0$. As a consequence, we show that the Frobenius normal form together with a transformation matrix can be computed deterministically in $O(n^\\omega (\\log\\log(n))^2)$, and therefore matrix exponentiation~$A^k$ can be performed in the latter complexity if $\\log(k) \\in O(n^{\\omega-1-\\varepsilon})$ for some fixed $\\varepsilon>0$. A key idea for these improvements is to rely on fast algorithms for $m\\times m$ polynomial matrices of average degree $n/m$, involving high-order lifting and minimal kernel bases.",
        "published": "2024-02-12T00:23:41Z",
        "link": "http://arxiv.org/abs/2402.07345v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Optimized Gröbner basis algorithms for maximal determinantal ideals   and critical point computations",
        "authors": [
            "Sriram Gopalakrishnan",
            "Vincent Neiger",
            "Mohab Safey El Din"
        ],
        "summary": "Given polynomials $g$ and $f_1,\\dots,f_p$, all in $\\Bbbk[x_1,\\dots,x_n]$ for some field $\\Bbbk$, we consider the problem of computing the critical points of the restriction of $g$ to the variety defined by $f_1=\\cdots=f_p=0$. These are defined by the simultaneous vanishing of the $f_i$'s and all maximal minors of the Jacobian matrix associated to $(g,f_1, \\ldots, f_p)$. We use the Eagon-Northcott complex associated to the ideal generated by these maximal minors to gain insight into the syzygy module of the system defining these critical points. We devise new $F_5$-type criteria to predict and avoid more reductions to zero when computing a Gr\\\"obner basis for the defining system of this critical locus. We give a bound for the arithmetic complexity of this enhanced $F_5$ algorithm and compare it to the best previously known bound for computing critical points using Gr\\\"obner bases.",
        "published": "2024-02-12T01:02:33Z",
        "link": "http://arxiv.org/abs/2402.07353v1",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Ensuring trustworthy and ethical behaviour in intelligent logical agents",
        "authors": [
            "Stefania Costantini"
        ],
        "summary": "Autonomous Intelligent Agents are employed in many applications upon which the life and welfare of living beings and vital social functions may depend. Therefore, agents should be trustworthy. A priori certification techniques (i.e., techniques applied prior to system's deployment) can be useful, but are not sufficient for agents that evolve, and thus modify their epistemic and belief state, and for open Multi-Agent Systems, where heterogeneous agents can join or leave the system at any stage of its operation. In this paper, we propose/refine/extend dynamic (runtime) logic-based self-checking techniques, devised in order to be able to ensure agents' trustworthy and ethical behaviour.",
        "published": "2024-02-12T10:19:17Z",
        "link": "http://arxiv.org/abs/2402.07547v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.LO",
            "cs.SC",
            "I.2.4"
        ]
    },
    {
        "title": "Solving parameter-dependent semi-algebraic systems",
        "authors": [
            "Louis Gaillard",
            "Mohab Safey El Din"
        ],
        "summary": "We consider systems of polynomial equations and inequalities in $\\mathbb{Q}[\\boldsymbol{y}][\\boldsymbol{x}]$ where $\\boldsymbol{x} = (x_1, \\ldots, x_n)$ and $\\boldsymbol{y} = (y_1, \\ldots,y_t)$. The $\\boldsymbol{y}$ indeterminates are considered as parameters and we assume that when specialising them generically, the set of common complex solutions, to the obtained equations, is finite. We consider the problem of real root classification for such parameter-dependent problems, i.e. identifying the possible number of real solutions depending on the values of the parameters and computing a description of the regions of the space of parameters over which the number of real roots remains invariant.   We design an algorithm for solving this problem. The formulas it outputs enjoy a determinantal structure. Under genericity assumptions, we show that its arithmetic complexity is polynomial in both the maximum degree $d$ and the number $s$ of the input inequalities and exponential in $nt+t^2$. The output formulas consist of polynomials of degree bounded by $(2s+n)d^{n+1}$. This is the first algorithm with such a singly exponential complexity. We report on practical experiments showing that a first implementation of this algorithm can tackle examples which were previously out of reach.",
        "published": "2024-02-12T16:47:13Z",
        "link": "http://arxiv.org/abs/2402.07782v1",
        "categories": [
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Fast interpolation and multiplication of unbalanced polynomials",
        "authors": [
            "Pascal Giorgi",
            "Bruno Grenet",
            "Armelle Perret du Cray",
            "Daniel S. Roche"
        ],
        "summary": "We consider the classical problems of interpolating a polynomial given a black box for evaluation, and of multiplying two polynomials, in the setting where the bit-lengths of the coefficients may vary widely, so-called unbalanced polynomials. Writing s for the total bit-length and D for the degree, our new algorithms have expected running time $\\tilde{O}(s \\log D)$, whereas previous methods for (resp.) dense or sparse arithmetic have at least $\\tilde{O}(sD)$ or $\\tilde{O}(s^2)$ bit complexity.",
        "published": "2024-02-15T17:43:22Z",
        "link": "http://arxiv.org/abs/2402.10139v2",
        "categories": [
            "cs.SC",
            "cs.CC"
        ]
    },
    {
        "title": "Optimal Pseudorandom Generators for Low-Degree Polynomials Over   Moderately Large Fields",
        "authors": [
            "Ashish Dwivedi",
            "Zeyu Guo",
            "Ben Lee Volk"
        ],
        "summary": "We construct explicit pseudorandom generators that fool $n$-variate polynomials of degree at most $d$ over a finite field $\\mathbb{F}_q$. The seed length of our generators is $O(d \\log n + \\log q)$, over fields of size exponential in $d$ and characteristic at least $d(d-1)+1$. Previous constructions such as Bogdanov's (STOC 2005) and Derksen and Viola's (FOCS 2022) had either suboptimal seed length or required the field size to depend on $n$.   Our approach follows Bogdanov's paradigm while incorporating techniques from Lecerf's factorization algorithm (J. Symb. Comput. 2007) and insights from the construction of Derksen and Viola regarding the role of indecomposability of polynomials.",
        "published": "2024-02-19T07:59:37Z",
        "link": "http://arxiv.org/abs/2402.11915v1",
        "categories": [
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "Improving Neural-based Classification with Logical Background Knowledge",
        "authors": [
            "Arthur Ledaguenel",
            "Céline Hudelot",
            "Mostepha Khouadjia"
        ],
        "summary": "Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditioning at inference can be used to build more accurate neural-based systems with fewer resources while guaranteeing the semantic consistency of outputs.",
        "published": "2024-02-20T14:01:26Z",
        "link": "http://arxiv.org/abs/2402.13019v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Some Lower Bounds on the Reach of an Algebraic Variety",
        "authors": [
            "Chris La Valle",
            "Josué Tonelli-Cueto"
        ],
        "summary": "Separation bounds are a fundamental measure of the complexity of solving a zero-dimensional system as it measures how difficult it is to separate its zeroes. In the positive dimensional case, the notion of reach takes its place. In this paper, we provide bounds on the reach of a smooth algebraic variety in terms of several invariants of interest: the condition number, Smale's $\\gamma$ and the bit-size. We also provide probabilistic bounds for random algebraic varieties under some general assumptions.",
        "published": "2024-02-23T23:21:35Z",
        "link": "http://arxiv.org/abs/2402.15649v2",
        "categories": [
            "math.AG",
            "cs.CG",
            "cs.SC",
            "14Q20, 14P99, 14Q65"
        ]
    },
    {
        "title": "Optimal Communication Unbalanced Private Set Union",
        "authors": [
            "Jean-Guillaume Dumas",
            "Alexis Galan",
            "Bruno Grenet",
            "Aude Maignan",
            "Daniel S. Roche"
        ],
        "summary": "We present new two-party protocols for the Unbalanced Private Set Union (UPSU) problem.Here, the Sender holds a set of data points, and the Receiver holds another (possibly much larger) set, and they would like for the Receiver to learn the union of the two sets and nothing else. Furthermore, the Sender's computational cost, along with the communication complexity, should be smaller when the Sender has a smaller set.While the UPSU problem has numerous applications and has seen considerable recent attention in the literature, our protocols are the first where the Sender's computational cost and communication volume are linear in the size of the Sender's set only, and do not depend on the size of the Receiver's set.Our constructions combine linearly homomorphic encryption (LHE) withfully homomorphic encryption (FHE). The first construction uses multi-point polynomial evaluation (MEv) on FHE, and achieves optimal linear cost for the Sender, but has higher quadratic computational cost for the Receiver. In the second construction we explore another trade-off: the Receiver computes fast polynomial Euclidean remainder in FHE while the Sender computes a fast MEv, in LHE only. This reduces the Receiver's cost to quasi-linear, with a modest increase in the computational cost for the Sender.Preliminary experimental results using HElib indicate that, for example, a Sender holding 1000 elements can complete our first protocol using less than 2s of computation time and less than 10MB of communication volume, independently of the Receiver's set size.",
        "published": "2024-02-26T08:36:11Z",
        "link": "http://arxiv.org/abs/2402.16393v3",
        "categories": [
            "cs.CR",
            "cs.SC"
        ]
    },
    {
        "title": "Deep Neural Network for Constraint Acquisition through Tailored Loss   Function",
        "authors": [
            "Eduardo Vyhmeister",
            "Rocio Paez",
            "Gabriel Gonzalez"
        ],
        "summary": "The significance of learning constraints from data is underscored by its potential applications in real-world problem-solving. While constraints are popular for modeling and solving, the approaches to learning constraints from data remain relatively scarce. Furthermore, the intricate task of modeling demands expertise and is prone to errors, thus constraint acquisition methods offer a solution by automating this process through learnt constraints from examples or behaviours of solutions and non-solutions. This work introduces a novel approach grounded in Deep Neural Network (DNN) based on Symbolic Regression that, by setting suitable loss functions, constraints can be extracted directly from datasets. Using the present approach, direct formulation of constraints was achieved. Furthermore, given the broad pre-developed architectures and functionalities of DNN, connections and extensions with other frameworks could be foreseen.",
        "published": "2024-03-04T13:47:33Z",
        "link": "http://arxiv.org/abs/2403.02042v1",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "On the arithmetic complexity of computing Gröbner bases of comaximal   determinantal ideals",
        "authors": [
            "Sriram Gopalakrishnan"
        ],
        "summary": "Let $M$ be an $n\\times n$ matrix of homogeneous linear forms over a field $\\Bbbk$. If the ideal $\\mathcal{I}_{n-2}(M)$ generated by minors of size $n-1$ is Cohen-Macaulay, then the Gulliksen-Neg{\\aa}rd complex is a free resolution of $\\mathcal{I}_{n-2}(M)$. It has recently been shown that by taking into account the syzygy modules for $\\mathcal{I}_{n-2}(M)$ which can be obtained from this complex, one can derive a refined signature-based Gr\\\"obner basis algorithm DetGB which avoids reductions to zero when computing a grevlex Gr\\\"obner basis for $\\mathcal{I}_{n-2}(M)$. In this paper, we establish sharp complexity bounds on DetGB. To accomplish this, we prove several results on the sizes of reduced grevlex Gr\\\"obner bases of reverse lexicographic ideals, thanks to which we obtain two main complexity results which rely on conjectures similar to that of Fr\\\"oberg. The first one states that, in the zero-dimensional case, the size of the reduced grevlex Gr\\\"obner basis of $\\mathcal{I}_{n-2}(M)$ is bounded from below by $n^{6}$ asymptotically. The second, also in the zero-dimensional case, states that the complexity of DetGB is bounded from above by $n^{2\\omega+3}$ asymptotically, where $2\\le\\omega\\le 3$ is any complexity exponent for matrix multiplication over $\\Bbbk$.",
        "published": "2024-03-04T16:08:33Z",
        "link": "http://arxiv.org/abs/2403.02160v2",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Saturating Sorting without Sorts",
        "authors": [
            "Pamina Georgiou",
            "Márton Hajdu",
            "Laura Kovács"
        ],
        "summary": "We present a first-order theorem proving framework for establishing the correctness of functional programs implementing sorting algorithms with recursive data structures.   We formalize the semantics of recursive programs in many-sorted first-order logic and integrate sortedness/permutation properties within our first-order formalization. Rather than focusing on sorting lists of elements of specific first-order theories, such as integer arithmetic, our list formalization relies on a sort parameter abstracting (arithmetic) theories and hence concrete sorts. We formalize the permutation property of lists in first-order logic so that we automatically prove verification conditions of such algorithms purely by superpositon-based first-order reasoning. Doing so, we adjust recent efforts for automating inducion in saturation. We advocate a compositional approach for automating proofs by induction required to verify functional programs implementing and preserving sorting and permutation properties over parameterized list structures. Our work turns saturation-based first-order theorem proving into an automated verification engine by (i) guiding automated inductive reasoning with manual proof splits and (ii) fully automating inductive reasoning in saturation. We showcase the applicability of our framework over recursive sorting algorithms, including Mergesort and Quicksort.",
        "published": "2024-03-06T13:54:57Z",
        "link": "http://arxiv.org/abs/2403.03712v1",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Learning Guided Automated Reasoning: A Brief Survey",
        "authors": [
            "Lasse Blaauwbroek",
            "David Cerna",
            "Thibault Gauthier",
            "Jan Jakubův",
            "Cezary Kaliszyk",
            "Martin Suda",
            "Josef Urban"
        ],
        "summary": "Automated theorem provers and formal proof assistants are general reasoning systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical reasoning. In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance. This is an opportunity for trained machine learning predictors, which can guide the work of such reasoning systems. Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large reasoning corpora. Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long reasoning chains and possibly novel proof ideas. In this paper we provide an overview of several automated reasoning and theorem proving domains and the learning and AI methods that have been so far developed for them. These include premise selection, proof guidance in several settings, AI systems and feedback loops iterating between reasoning and learning, and symbolic classification problems.",
        "published": "2024-03-06T19:59:17Z",
        "link": "http://arxiv.org/abs/2403.04017v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.LO",
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Identifying Causal Effects Under Functional Dependencies",
        "authors": [
            "Yizuo Chen",
            "Adnan Darwiche"
        ],
        "summary": "We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.",
        "published": "2024-03-07T22:04:35Z",
        "link": "http://arxiv.org/abs/2403.04919v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.SC",
            "stat.ME"
        ]
    },
    {
        "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM   Disscusion via Argumentation Schemes",
        "authors": [
            "Shengxin Hong",
            "Liang Xiao",
            "Xin Zhang",
            "Jianxia Chen"
        ],
        "summary": "There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Discussion (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, use symbolic solver to identify a series of rational and coherent arguments to support decision. We construct a formal model of ArgMed-Agents and present conjectures for theoretical guarantees. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.",
        "published": "2024-03-10T19:47:00Z",
        "link": "http://arxiv.org/abs/2403.06294v2",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.SC"
        ]
    },
    {
        "title": "Solving the p-Riccati Equations and Applications to the Factorisation of   Differential Operators",
        "authors": [
            "Raphaël Pagès"
        ],
        "summary": "The solutions of the equation f^{ (p--1) }+ f^p = h^p in the unknown function f overan algebraic function field of characteristic p are very closely linked to the structure and fac-torisations of linear differential operators with coefficients in function fields of characteristic p.However, while being able to solve this equation over general algebraic function fields is necessaryeven for operators with rational coefficients, no general resolution method has been developed.We present an algorithm for testing the existence of solutions in polynomial time in the ``size''of h and an algorithm based on the computation of Riemann-Roch spaces and the selection ofelements in the divisor class group, for computing solutions of size polynomial in the ``size'' of hin polynomial time in the size of h and linear in the characteristic p, and discuss its applicationsto the factorisation of linear differential operators in positive characteristic p.",
        "published": "2024-03-11T09:36:54Z",
        "link": "http://arxiv.org/abs/2403.06542v2",
        "categories": [
            "cs.SC",
            "math.NT",
            "math.OA"
        ]
    },
    {
        "title": "Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic   Music Generation",
        "authors": [
            "Keshav Bhandari",
            "Simon Colton"
        ],
        "summary": "Modelling musical structure is vital yet challenging for artificial intelligence systems that generate symbolic music compositions. This literature review dissects the evolution of techniques for incorporating coherent structure, from symbolic approaches to foundational and transformative deep learning methods that harness the power of computation and data across a wide variety of training paradigms. In the later stages, we review an emerging technique which we refer to as \"sub-task decomposition\" that involves decomposing music generation into separate high-level structural planning and content creation stages. Such systems incorporate some form of musical knowledge or neuro-symbolic methods by extracting melodic skeletons or structural templates to guide the generation. Progress is evident in capturing motifs and repetitions across all three eras reviewed, yet modelling the nuanced development of themes across extended compositions in the style of human composers remains difficult. We outline several key future directions to realize the synergistic benefits of combining approaches from all eras examined.",
        "published": "2024-03-12T18:03:08Z",
        "link": "http://arxiv.org/abs/2403.07995v1",
        "categories": [
            "cs.SD",
            "cs.LG",
            "cs.SC",
            "eess.AS"
        ]
    },
    {
        "title": "Complexity Classification of Complex-Weighted Counting Acyclic   Constraint Satisfaction Problems",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "We study the computational complexity of counting constraint satisfaction problems (#CSPs) whose constraints assign complex numbers to Boolean inputs when the corresponding constraint hypergraphs are acyclic. These problems are called acyclic #CSPs or succinctly, #ACSPs. We wish to determine the computational complexity of all such #ACSPs when arbitrary unary constraints are freely available. Depending on whether we further allow or disallow the free use of the specific constraint XOR (binary disequality), we present two complexity classifications of the #ACSPs according to the types of constraints used for the problems. When XOR is freely available, we first obtain a complete dichotomy classification. On the contrary, when XOR is not available for free, we then obtain a trichotomy classification. To deal with an acyclic nature of constraints in those classifications, we develop a new technical tool called acyclic-T-constructibility or AT-constructibility, and we exploit it to analyze a complexity upper bound of each #ACSPs.",
        "published": "2024-03-14T07:47:02Z",
        "link": "http://arxiv.org/abs/2403.09145v1",
        "categories": [
            "cs.CC",
            "cs.FL",
            "cs.SC"
        ]
    },
    {
        "title": "Structural Preprocessing Method for Nonlinear Differential-Algebraic   Equations Using Linear Symbolic Matrices",
        "authors": [
            "Taihei Oki",
            "Yujin Song"
        ],
        "summary": "Differential-algebraic equations (DAEs) have been used in modeling various dynamical systems in science and engineering. Several preprocessing methods for DAEs, such as consistent initialization and index reduction, use structural information on DAEs. Unfortunately, these methods may fail when the system Jacobian, which is a functional matrix, derived from the DAE is singular.   To transform a DAE with a singular system Jacobian into a nonsingular system, several regularization methods have been proposed. Most of all existing regularization methods rely on symbolic computation to eliminate the system Jacobian for finding a certificate of singularity, resulting in much computational time. Iwata--Oki--Takamatsu (2019) proposed a method (IOT-method) to find a certificate without symbolic computations. The IOT method approximates the system Jacobian by a simpler symbolic matrix, called a layered mixed matrix, which admits a fast combinatorial algorithm for singularity testing. However, it often overlooks the singularity of the system Jacobian since the approximation largely discards algebraic relationships among entries in the original system Jacobian.   In this study, we propose a new regularization method extending the idea of the IOT method. Instead of layered mixed matrices, our method approximates the system Jacobian by more expressive symbolic matrices, called rank-1 coefficient mixed (1CM) matrices. This makes our method more widely applicable. We give a fast combinatorial algorithm for finding a singularity certificate of 1CM-matrices, which is free from symbolic elimination. Our method is also advantageous in that it globally preserves the solution set to the DAE. Through numerical experiments, we confirmed that our method runs fast for large-scale DAEs from real instances.",
        "published": "2024-03-15T12:48:41Z",
        "link": "http://arxiv.org/abs/2403.10260v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "First-order factors of linear Mahler operators",
        "authors": [
            "Frédéric Chyzak",
            "Thomas Dreyfus",
            "Philippe Dumas",
            "Marc Mezzarobba"
        ],
        "summary": "We develop and compare two algorithms for computing first-order right-hand factors in the ring of linear Mahler operators$\\ell_r M^r + \\dots + \\ell_1 M + \\ell_0$where $\\ell_0, \\dots, \\ell_r$ are polynomials in~$x$ and $Mx = x^b M$ for some integer $b \\geq 2$. In other words, we give algorithms for finding all formal infinite product solutions of linear functional equations$\\ell_r(x) f(x^{b^r}) + \\dots + \\ell_1(x) f(x^b) + \\ell_0(x) f(x) = 0$. The first of our algorithms is adapted from Petkov\\v{s}ek's classical algorithm forthe analogous problem in the case of linear recurrences. The second one proceeds by computing a basis of generalized power series solutions of the functional equation and by using Hermite-Pad{\\'e} approximants to detect those linear combinations of the solutions that correspond to first-order factors. We present implementations of both algorithms and discuss their use in combination with criteria from the literature to prove the differential transcendence of power series solutions of Mahler equations.",
        "published": "2024-03-18T07:56:49Z",
        "link": "http://arxiv.org/abs/2403.11545v2",
        "categories": [
            "cs.SC",
            "math.NT",
            "math.RA"
        ]
    },
    {
        "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the   Abstraction and Reasoning Corpus",
        "authors": [
            "Seungpil Lee",
            "Woochang Sim",
            "Donghyeon Shin",
            "Wongyu Seo",
            "Jiwon Park",
            "Seokki Lee",
            "Sanha Hwang",
            "Sejin Kim",
            "Sundong Kim"
        ],
        "summary": "The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been predominantly results-centric, making it challenging to assess the inference process comprehensively. We introduce a novel approach using the Abstraction and Reasoning Corpus (ARC) benchmark to evaluate the inference and contextual understanding abilities of LLMs in a process-centric manner, focusing on three key components from the Language of Thought Hypothesis (LoTH): Logical Coherence, Compositionality, and Productivity. Our carefully designed experiments reveal that while LLMs demonstrate some inference capabilities, they still significantly lag behind human-level reasoning in these three aspects. The main contribution of this paper lies in introducing the LoTH perspective, which provides a method for evaluating the reasoning process that conventional results-oriented approaches fail to capture, thereby offering new insights into the development of human-level reasoning in artificial intelligence systems.",
        "published": "2024-03-18T13:50:50Z",
        "link": "http://arxiv.org/abs/2403.11793v3",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.ET",
            "cs.SC"
        ]
    },
    {
        "title": "Routing and Scheduling in Answer Set Programming applied to Multi-Agent   Path Finding: Preliminary Report",
        "authors": [
            "Roland Kaminski",
            "Torsten Schaub",
            "Tran Cao Son",
            "Jiří Švancara",
            "Philipp Wanko"
        ],
        "summary": "We present alternative approaches to routing and scheduling in Answer Set Programming (ASP), and explore them in the context of Multi-agent Path Finding. The idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. This also abolishes the need for fixed upper bounds on the length of plans. The trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. While this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in ASP in a feasible way. This is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. We formally elaborate upon this idea and present several resulting ASP encodings. Finally, we demonstrate their effectiveness via an empirical analysis.",
        "published": "2024-03-18T18:09:47Z",
        "link": "http://arxiv.org/abs/2403.12153v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Self-Attention Based Semantic Decomposition in Vector Symbolic   Architectures",
        "authors": [
            "Calvin Yeung",
            "Prathyush Poduval",
            "Mohsen Imani"
        ],
        "summary": "Vector Symbolic Architectures (VSAs) have emerged as a novel framework for enabling interpretable machine learning algorithms equipped with the ability to reason and explain their decision processes. The basic idea is to represent discrete information through high dimensional random vectors. Complex data structures can be built up with operations over vectors such as the \"binding\" operation involving element-wise vector multiplication, which associates data together. The reverse task of decomposing the associated elements is a combinatorially hard task, with an exponentially large search space. The main algorithm for performing this search is the resonator network, inspired by Hopfield network-based memory search operations.   In this work, we introduce a new variant of the resonator network, based on self-attention based update rules in the iterative search problem. This update rule, based on the Hopfield network with log-sum-exp energy function and norm-bounded states, is shown to substantially improve the performance and rate of convergence. As a result, our algorithm enables a larger capacity for associative memory, enabling applications in many tasks like perception based pattern recognition, scene decomposition, and object reasoning. We substantiate our algorithm with a thorough evaluation and comparisons to baselines.",
        "published": "2024-03-20T00:37:19Z",
        "link": "http://arxiv.org/abs/2403.13218v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Gr{ö}bner bases over polytopal affinoid algebras",
        "authors": [
            "Moulay A. Barkatou",
            "Lucas Legrand",
            "Tristan Vaccon"
        ],
        "summary": "Polyhedral affinoid algebras have been introduced by Einsiedler, Kapranov and Lind to connect rigid analytic geometry (analytic geometry over non-archimedean fields) and tropical geometry.In this article, we present a theory of Gr{\\\"o}bner bases for polytopal affinoid algebras that extends both Caruso et al.'s theory of Gr{\\\"o}bner bases on Tate algebras and Pauer et al.'s theory of Gr{\\\"o}bner bases on Laurent polynomials.We provide effective algorithms to compute Gr{\\\"o}bner bases for both ideals of Laurent polynomials and ideals in polytopal affinoid algebras. Experiments with a Sagemath implementation are provided.",
        "published": "2024-03-20T08:24:35Z",
        "link": "http://arxiv.org/abs/2403.13382v1",
        "categories": [
            "cs.SC",
            "math.AC",
            "math.NT"
        ]
    },
    {
        "title": "OSVAuto: semi-automatic verifier for functional specifications of   operating systems",
        "authors": [
            "Yulun Wu",
            "Bohua Zhan",
            "Bican Xia"
        ],
        "summary": "We present the design and implementation of a tool for semi-automatic verification of functional specifications of operating system modules. Such verification tasks are traditionally done in interactive theorem provers, where the functionalities of the module are specified at abstract and concrete levels using data such as structures, algebraic datatypes, arrays, maps and so on. In this work, we provide encodings to SMT for these commonly occurring data types. This allows verification conditions to be reduced into a form suitable for SMT solvers. The use of SMT solvers combined with a tactic language allows semi-automatic verification of the specification. We apply the tool to verify functional specification for key parts of the uC-OS/II operating system, based on earlier work giving full verification of the system in Coq. We demonstrate a large reduction in the amount of human effort due to increased level of automation.",
        "published": "2024-03-20T10:07:07Z",
        "link": "http://arxiv.org/abs/2403.13457v1",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "Hierarchical NeuroSymbolic Approach for Comprehensive and Explainable   Action Quality Assessment",
        "authors": [
            "Lauren Okamoto",
            "Paritosh Parmar"
        ],
        "summary": "Action quality assessment (AQA) applies computer vision to quantitatively assess the performance or execution of a human action. Current AQA approaches are end-to-end neural models, which lack transparency and tend to be biased because they are trained on subjective human judgements as ground-truth. To address these issues, we introduce a neuro-symbolic paradigm for AQA, which uses neural networks to abstract interpretable symbols from video data and makes quality assessments by applying rules to those symbols. We take diving as the case study. We found that domain experts prefer our system and find it more informative than purely neural approaches to AQA in diving. Our system also achieves state-of-the-art action recognition and temporal segmentation, and automatically generates a detailed report that breaks the dive down into its elements and provides objective scoring with visual evidence. As verified by a group of domain experts, this report may be used to assist judges in scoring, help train judges, and provide feedback to divers. Annotated training data and code: https://github.com/laurenok24/NSAQA.",
        "published": "2024-03-20T17:55:21Z",
        "link": "http://arxiv.org/abs/2403.13798v2",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Extrapolating Solution Paths of Polynomial Homotopies towards   Singularities with PHCpack and phcpy",
        "authors": [
            "Jan Verschelde",
            "Kylash Viswanathan"
        ],
        "summary": "PHCpack is a software package for polynomial homotopy continuation, which provides a robust path tracker [Telen, Van Barel, Verschelde, SISC 2020]. This tracker computes the radius of convergence of Newton's method, estimates the distance to the nearest path, and then applies Pad\\'{e} approximants to predict the next point on the path. A priori step size control is less sensitive to finely tuned tolerances than a posteriori step size control, and is therefore robust. The Python interface phcpy is extended with a new step-by-step tracker and is applied to experiment with extrapolation methods to accurately locate the singular points at the end of solution paths.",
        "published": "2024-03-21T21:28:47Z",
        "link": "http://arxiv.org/abs/2403.14844v2",
        "categories": [
            "cs.MS",
            "cs.NA",
            "cs.SC",
            "math.CV",
            "math.NA"
        ]
    },
    {
        "title": "Applied Category Theory in the Wolfram Language using Categorica I:   Diagrams, Functors and Fibrations",
        "authors": [
            "Jonathan Gorard"
        ],
        "summary": "This article serves as a preliminary introduction to the design of a new, open-source applied and computational category theory framework, named Categorica, built on top of the Wolfram Language. Categorica allows one to configure and manipulate abstract quivers, categories, groupoids, diagrams, functors and natural transformations, and to perform a vast array of automated abstract algebraic computations using (arbitrary combinations of) the above structures; to manipulate and abstractly reason about arbitrary universal properties, including products, coproducts, pullbacks, pushouts, limits and colimits; and to manipulate, visualize and compute with strict (symmetric) monoidal categories, including full support for automated string diagram rewriting and diagrammatic theorem-proving. In so doing, Categorica combines the capabilities of an abstract computer algebra framework (thus allowing one to compute directly with epimorphisms, monomorphisms, retractions, sections, spans, cospans, fibrations, etc.) with those of a powerful automated theorem-proving system (thus allowing one to convert universal properties and other abstract constructions into (higher-order) equational logic statements that can be reasoned about and proved using standard automated theorem-proving methods, as well as to prove category-theoretic statements directly using purely diagrammatic methods). In this first of two articles introducing the design of the framework, we shall focus principally upon its handling of quivers, categories, diagrams, groupoids, functors and natural transformations, including demonstrations of both its algebraic manipulation and theorem-proving capabilities in each case.",
        "published": "2024-03-24T19:16:37Z",
        "link": "http://arxiv.org/abs/2403.16269v1",
        "categories": [
            "math.CT",
            "cs.SC"
        ]
    },
    {
        "title": "Two Algorithms for Computing Rational Univariate Representations of   Zero-Dimensional Ideals with Parameters",
        "authors": [
            "Dingkang Wang",
            "Jingjing Wei",
            "Fanghui Xiao",
            "Xiaopeng Zheng"
        ],
        "summary": "Based on the partition of parameter space, two algorithms for computing the rational univariate representation of zero-dimensional ideals with parameters are presented in the paper. Unlike the rational univariate representation of zero-dimensional ideals without parameters, the number of zeros of zero-dimensional ideals with parameters under various specializations is different, which leads to choosing and checking the separating element, the key to computing the rational univariate representation, is difficult. In order to pick out the separating element, we first ensure that under each branch the ideal has the same number of zeros by partitioning the parameter space. Subsequently two ideas are given to choose and check the separating element. One idea is that by extending the subresultant theorem to parametric cases, we utilize the extended subresultant theorem to choose the separating element with the further partition of parameter space and then with the help of parametric greatest common divisor theory compute rational univariate representations. Another one is that we go straight to choose and check the separating element by the computation of parametric greatest common divisors, then immediately get the rational univariate representations. Based on these, we design two different algorithms for computing rational univariate representations of zero-dimensional ideals with parameters. Furthermore, the algorithms have been implemented on Singular and the performance comparison are presented.",
        "published": "2024-03-25T08:02:31Z",
        "link": "http://arxiv.org/abs/2403.16519v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "On the scaling of random Tamari intervals and Schnyder woods of random   triangulations (with an asymptotic D-finite trick)",
        "authors": [
            "Guillaume Chapuy"
        ],
        "summary": "We consider a Tamari interval of size $n$ (i.e., a pair of Dyck paths which are comparable for the Tamari relation) chosen uniformly at random. We show that the height of a uniformly chosen vertex on the upper or lower path scales as $n^{3/4}$, and has an explicit limit law. By the Bernardi-Bonichon bijection, this result also describes the height of points in the canonical Schnyder trees of a uniform random plane triangulation of size $n$.   The exact solution of the model is based on polynomial equations with one and two catalytic variables. To prove the convergence from the exact solution, we use a version of moment pumping based on D-finiteness, which is essentially automatic and should apply to many other models. We are not sure to have seen this simple trick used before.   It would be interesting to study the universality of this convergence for decomposition trees associated to positive Bousquet-M\\'elou--Jehanne equations.",
        "published": "2024-03-27T16:08:41Z",
        "link": "http://arxiv.org/abs/2403.18719v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.PR"
        ]
    },
    {
        "title": "A Neuro-Symbolic Approach to Monitoring Salt Content in Food",
        "authors": [
            "Anuja Tayal",
            "Barbara Di Eugenio",
            "Devika Salunke",
            "Andrew D. Boyd",
            "Carolyn A Dickens",
            "Eulalia P Abril",
            "Olga Garcia-Bedoya",
            "Paula G Allen-Meares"
        ],
        "summary": "We propose a dialogue system that enables heart failure patients to inquire about salt content in foods and help them monitor and reduce salt intake. Addressing the lack of specific datasets for food-based salt content inquiries, we develop a template-based conversational dataset. The dataset is structured to ask clarification questions to identify food items and their salt content. Our findings indicate that while fine-tuning transformer-based models on the dataset yields limited performance, the integration of Neuro-Symbolic Rules significantly enhances the system's performance. Our experiments show that by integrating neuro-symbolic rules, our system achieves an improvement in joint goal accuracy of over 20% across different data sizes compared to naively fine-tuning transformer-based models.",
        "published": "2024-04-01T15:34:24Z",
        "link": "http://arxiv.org/abs/2404.01182v1",
        "categories": [
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "On the Algorithmic Recovering of Coefficients in Linearizable   Differential Equations",
        "authors": [
            "Dmitry A. Lyakhov",
            "Dominik L. Michels"
        ],
        "summary": "We investigate the problem of recovering coefficients in scalar nonlinear ordinary differential equations that can be exactly linearized. This contribution builds upon prior work by Lyakhov, Gerdt, and Michels, which focused on obtaining a linearizability certificate through point transformations. Our focus is on quasi-linear equations, specifically those solved for the highest derivative with a rational dependence on the variables involved. Our novel algorithm for coefficient recovery relies on basic operations on Lie algebras, such as computing the derived algebra and the dimension of the symmetry algebra. This algorithmic approach is efficient, although finding the linearization transformation necessitates computing at least one solution of the corresponding Bluman-Kumei equation system.",
        "published": "2024-04-02T09:57:54Z",
        "link": "http://arxiv.org/abs/2404.01798v1",
        "categories": [
            "cs.SC",
            "math.GR",
            "math.RA",
            "68W30",
            "I.1"
        ]
    },
    {
        "title": "The solving degrees for computing Gröbner bases of affine   semi-regular polynomial sequences",
        "authors": [
            "Momonari Kudo",
            "Kazuhiro Yokoyama"
        ],
        "summary": "In this paper, we study the solving degrees for affine semi-regular sequences and their homogenized sequences. Some of our results are considered to give mathematically rigorous proofs of the correctness of methods for computing Gr\\\"{o}bner bases of the ideal generated by an affine semi-regular sequence. This paper is a sequel of the authors' previous work and gives additional results on the solving degrees and important behaviors of Gr\\\"obner basis computation.   We also define the generalized degree of regularity for a sequence of homogeneous polynomials. For the ideal generated by the homogenization of an affine semi-regular sequence, we relate its generalized degree of regularity with its maximal Gr\\\"{o}bner basis degree (i.e., the solving degree for the homogenized sequence). The definition of a generalized (cryptographic) semi-regular sequence is also given, and it derives a new cryptographic assumption to estimate the security of cryptosystems. From our experimental observation, we raise a conjecture and some questions related to this generalized semi-regularity. These definitions and our results provide a theoretical formulation of (somehow heuristic) discussions done so far in the cryptographic community.",
        "published": "2024-04-04T15:35:41Z",
        "link": "http://arxiv.org/abs/2404.03530v3",
        "categories": [
            "math.AC",
            "cs.CR",
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Power Series Composition in Near-Linear Time",
        "authors": [
            "Yasunori Kinoshita",
            "Baitian Li"
        ],
        "summary": "We present an algebraic algorithm that computes the composition of two power series in softly linear time complexity. The previous best algorithms are $\\mathop{\\mathrm O}(n^{1+o(1)})$ by Kedlaya and Umans (FOCS 2008) and an $\\mathop{\\mathrm O}(n^{1.43})$ algebraic algorithm by Neiger, Salvy, Schost and Villard (JACM 2023).   Our algorithm builds upon the recent Graeffe iteration approach to manipulate rational power series introduced by Bostan and Mori (SOSA 2021).",
        "published": "2024-04-08T03:57:38Z",
        "link": "http://arxiv.org/abs/2404.05177v2",
        "categories": [
            "cs.SC",
            "I.1.2"
        ]
    },
    {
        "title": "WebPie: A Tiny Slice of Dependent Typing",
        "authors": [
            "Christophe Scholliers"
        ],
        "summary": "Dependently typed programming languages have become increasingly relevant in recent years. They have been adopted in industrial strength programming languages and have been extremely successful as the basis for theorem provers. There are however, very few entry level introductions to the theory of language constructs for dependently typed languages, and even less sources on didactical implementations. In this paper, we present a small dependently typed programming language called WebPie. The main features of the language are inductive types, recursion and case matching. While none of these features are new, we believe this article can provide a step forward towards the understanding and systematic construction of dependently typed languages for researchers new to dependent types.",
        "published": "2024-04-08T12:40:02Z",
        "link": "http://arxiv.org/abs/2404.05457v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Study of Emotion Concept Formation by Integrating Vision, Physiology,   and Word Information using Multilayered Multimodal Latent Dirichlet   Allocation",
        "authors": [
            "Kazuki Tsurumaki",
            "Chie Hieida",
            "Kazuki Miyazawa"
        ],
        "summary": "How are emotions formed? Through extensive debate and the promulgation of diverse theories , the theory of constructed emotion has become prevalent in recent research on emotions. According to this theory, an emotion concept refers to a category formed by interoceptive and exteroceptive information associated with a specific emotion. An emotion concept stores past experiences as knowledge and can predict unobserved information from acquired information. Therefore, in this study, we attempted to model the formation of emotion concepts using a constructionist approach from the perspective of the constructed emotion theory. Particularly, we constructed a model using multilayered multimodal latent Dirichlet allocation , which is a probabilistic generative model. We then trained the model for each subject using vision, physiology, and word information obtained from multiple people who experienced different visual emotion-evoking stimuli. To evaluate the model, we verified whether the formed categories matched human subjectivity and determined whether unobserved information could be predicted via categories. The verification results exceeded chance level, suggesting that emotion concept formation can be explained by the proposed model.",
        "published": "2024-04-12T07:34:46Z",
        "link": "http://arxiv.org/abs/2404.08295v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.LG",
            "cs.RO",
            "cs.SC"
        ]
    },
    {
        "title": "Complexity of Probabilistic Reasoning for Neurosymbolic Classification   Techniques",
        "authors": [
            "Arthur Ledaguenel",
            "Céline Hudelot",
            "Mostepha Khouadjia"
        ],
        "summary": "Neurosymbolic artificial intelligence is a growing field of research aiming to combine neural network learning capabilities with the reasoning abilities of symbolic systems. Informed multi-label classification is a sub-field of neurosymbolic AI which studies how to leverage prior knowledge to improve neural classification systems. A well known family of neurosymbolic techniques for informed classification use probabilistic reasoning to integrate this knowledge during learning, inference or both. Therefore, the asymptotic complexity of probabilistic reasoning is of cardinal importance to assess the scalability of such techniques. However, this topic is rarely tackled in the neurosymbolic literature, which can lead to a poor understanding of the limits of probabilistic neurosymbolic techniques. In this paper, we introduce a formalism for informed supervised classification tasks and techniques. We then build upon this formalism to define three abstract neurosymbolic techniques based on probabilistic reasoning. Finally, we show computational complexity results on several representation languages for prior knowledge commonly found in the neurosymbolic literature.",
        "published": "2024-04-12T11:31:37Z",
        "link": "http://arxiv.org/abs/2404.08404v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Performant Dynamically Typed E-Graphs in Pure Julia",
        "authors": [
            "Alessandro Cheli",
            "Niklas Heim"
        ],
        "summary": "We introduce the third major version of Metatheory.jl, a Julia library for general-purpose metaprogramming and symbolic computation. Metatheory.jl provides a flexible and performant implementation of e-graphs and Equality Saturation (EqSat) that addresses the two-language problem in high-level compiler optimizations, symbolics and metaprogramming. We present results from our ongoing optimization efforts, comparing the state-of-the-art egg Rust library's performance against our system and show that performant EqSat implementations are possible without sacrificing the comfort of a direct 1-1 integration with a dynamic, high-level and an interactive host programming language.",
        "published": "2024-04-12T18:24:44Z",
        "link": "http://arxiv.org/abs/2404.08751v3",
        "categories": [
            "cs.PL",
            "cs.SC",
            "I.1.3"
        ]
    },
    {
        "title": "Connectivity in Symmetric Semi-Algebraic Sets",
        "authors": [
            "Cordian Riener",
            "Robin Schabert",
            "Thi Xuan Vu"
        ],
        "summary": "Semi-algebraic set is a subset of the real space defined by polynomial equations and inequalities. In this paper, we consider the problem of deciding whether two given points in a semi-algebraic set are connected. We restrict to the case when all equations and inequalities are invariant under the action of the symmetric group and their degrees at most $d<n$, where $n$ is the number of variables. Additionally, we assume that the two points are in the same fundamental domain of the action of the symmetric group, by assuming that the coordinates of two given points are sorted in non-decreasing order. We construct and analyze an algorithm that solves this problem, by taking advantage of the group action, and has a complexity being polynomial in $n$.",
        "published": "2024-04-15T12:51:03Z",
        "link": "http://arxiv.org/abs/2404.09749v3",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Computing with Hypergeometric-Type Terms",
        "authors": [
            "Bertrand Teguia Tabuguia"
        ],
        "summary": "Take a multiplicative monoid of sequences in which the multiplication is given by Hadamard product. The set of linear combinations of interleaving monoid elements then yields a ring. For hypergeometric sequences, the resulting ring is a subring of the ring of holonomic sequences. We present two algorithms in this setting: one for computing holonomic recurrence equations from hypergeometric-type normal forms and the other for finding products of hypergeometric-type terms. These are newly implemented commands in our Maple package $HyperTypeSeq$, available at \\url{https://github.com/T3gu1a/HyperTypeSeq}, which we also describe.",
        "published": "2024-04-15T21:24:18Z",
        "link": "http://arxiv.org/abs/2404.10143v2",
        "categories": [
            "cs.SC",
            "cs.DM",
            "cs.MS",
            "33F10, 39A06 (Primary), 68R01, 05A19 (Secondary)",
            "I.1.1; I.1.2; G.2.1; G.4"
        ]
    },
    {
        "title": "Primary Decomposition of Symmetric Ideals",
        "authors": [
            "Yuki Ishihara"
        ],
        "summary": "We propose an effective method for primary decomposition of symmetric ideals. Let $K[X]=K[x_1,\\ldots,x_n]$ be the $n$-valuables polynomial ring over a field $K$ and $\\mathfrak{S}_n$ the symmetric group of order $n$. We consider the canonical action of $\\mathfrak{S}_n$ on $K[X]$ i.e. $\\sigma(f(x_1,\\ldots,x_n))=f(x_{\\sigma(1)},\\ldots,x_{\\sigma(n)})$ for $\\sigma\\in \\mathfrak{S}_n$. For an ideal $I$ of $K[X]$, $I$ is called {\\em symmetric} if $\\sigma(I)=I$ for any $\\sigma\\in \\mathfrak{S}_n$. For a minimal primary decomposition $I=Q_1\\cap \\cdots \\cap Q_r$ of a symmetric ideal $I$, $\\sigma(I)=\\sigma (Q_1)\\cap \\cdots \\cap \\sigma(Q_r)$ is a minimal primary decomposition of $I$ for any $\\sigma\\in \\mathfrak{S}_n$. We utilize this property to compute a full primary decomposition of $I$ efficiently from partial primary components. We investigate the effectiveness of our algorithm by implementing it in the computer algebra system Risa/Asir.",
        "published": "2024-04-16T11:42:19Z",
        "link": "http://arxiv.org/abs/2404.10482v1",
        "categories": [
            "math.AC",
            "cs.SC"
        ]
    },
    {
        "title": "Constant-Depth Arithmetic Circuits for Linear Algebra Problems",
        "authors": [
            "Robert Andrews",
            "Avi Wigderson"
        ],
        "summary": "We design polynomial size, constant depth (namely, $\\mathsf{AC}^0$) arithmetic formulae for the greatest common divisor (GCD) of two polynomials, as well as the related problems of the discriminant, resultant, B\\'ezout coefficients, squarefree decomposition, and the inversion of structured matrices like Sylvester and B\\'ezout matrices. Our GCD algorithm extends to any number of polynomials. Previously, the best known arithmetic formulae for these problems required super-polynomial size, regardless of depth.   These results are based on new algorithmic techniques to compute various symmetric functions in the roots of polynomials, as well as manipulate the multiplicities of these roots, without having access to them. These techniques allow $\\mathsf{AC}^0$ computation of a large class of linear and polynomial algebra problems, which include the above as special cases.   We extend these techniques to problems whose inputs are multivariate polynomials, which are represented by $\\mathsf{AC}^0$ arithmetic circuits. Here too we solve problems such as computing the GCD and squarefree decomposition in $\\mathsf{AC}^0$.",
        "published": "2024-04-16T18:25:34Z",
        "link": "http://arxiv.org/abs/2404.10839v1",
        "categories": [
            "cs.CC",
            "cs.SC"
        ]
    },
    {
        "title": "Towards a Research Community in Interpretable Reinforcement Learning:   the InterpPol Workshop",
        "authors": [
            "Hector Kohler",
            "Quentin Delfosse",
            "Paul Festor",
            "Philippe Preux"
        ],
        "summary": "Embracing the pursuit of intrinsically explainable reinforcement learning raises crucial questions: what distinguishes explainability from interpretability? Should explainable and interpretable agents be developed outside of domains where transparency is imperative? What advantages do interpretable policies offer over neural networks? How can we rigorously define and measure interpretability in policies, without user studies? What reinforcement learning paradigms,are the most suited to develop interpretable agents? Can Markov Decision Processes integrate interpretable state representations? In addition to motivate an Interpretable RL community centered around the aforementioned questions, we propose the first venue dedicated to Interpretable RL: the InterpPol Workshop.",
        "published": "2024-04-16T20:53:17Z",
        "link": "http://arxiv.org/abs/2404.10906v1",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Reduction systems and degree bounds for integration",
        "authors": [
            "Hao Du",
            "Clemens G. Raab"
        ],
        "summary": "In symbolic integration, the Risch--Norman algorithm aims to find closed forms of elementary integrals over differential fields by an ansatz for the integral, which usually is based on heuristic degree bounds. Norman presented an approach that avoids degree bounds and only relies on the completion of reduction systems. We give a formalization of his approach and we develop a refined completion process, which terminates in more instances. In some situations when the algorithm does not terminate, one can detect patterns allowing to still describe infinite reduction systems that are complete. We present such infinite systems for the fields generated by Airy functions and complete elliptic integrals, respectively. Moreover, we show how complete reduction systems can be used to find rigorous degree bounds. In particular, we give a general formula for weighted degree bounds and we apply it to find tight bounds for above examples.",
        "published": "2024-04-19T17:54:58Z",
        "link": "http://arxiv.org/abs/2404.13042v1",
        "categories": [
            "cs.SC",
            "33F10, 68W30, 12H05, 13P99, 15A06"
        ]
    },
    {
        "title": "Reconciling Explanations in Multi-Model Systems through Probabilistic   Argumentation",
        "authors": [
            "Shengxin Hong",
            "Xiuyi Fan"
        ],
        "summary": "Explainable Artificial Intelligence (XAI) has become critical in enhancing the transparency and trustworthiness of AI systems, especially as these systems are increasingly deployed in high-stakes domains such as healthcare and finance. Despite the progress made in developing explanation generation techniques for individual machine learning (ML) models, significant challenges remain in achieving coherent and comprehensive explanations in multi-model systems. This paper addresses these challenges by focusing on the explanation reconciliation problem (ERP) within multi-model systems. Traditional explanation generation technique often fall short in multi-model systems contexts, where explanations from different models can conflict and fail to form a cohesive narrative. Through the use of probabilistic argumentation and knowledge representation techniques, we propose a framework for generating holistic explanations that align with human cognitive processes. Our approach involves mapping uncertain explanation information to probabilistic arguments and introducing criteria for explanation reconciliation based on user perspectives such as optimism, pessimism, fairness. In addition, we introduce the relative independence assumption to optimise the search space for computational explanations.",
        "published": "2024-04-20T16:20:17Z",
        "link": "http://arxiv.org/abs/2404.13419v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Proceedings 18th International Workshop on Logical and Semantic   Frameworks, with Applications and 10th Workshop on Horn Clauses for   Verification and Synthesis",
        "authors": [
            "Temur Kutsia",
            "Daniel Ventura",
            "David Monniaux",
            "José F. Morales"
        ],
        "summary": "This volume contains   * The post-proceedings of the Eighteenth Logical and Semantic Frameworks with Applications (LSFA 2023). The meeting was held on July 1-2, 2023, organised by the Sapienza Universit\\`a di Roma, Italy. LSFA aims to bring researchers and students interested in theoretical and practical aspects of logical and semantic frameworks and their applications. The covered topics include proof theory, type theory and rewriting theory, specification and deduction languages, and formal semantics of languages and systems.   * The post-proceedings of the Tenth Workshop on Horn clauses for Verification and Synthesis (HCVS 2023). The meeting was held on April 23, 2023 at the Institut Henri Poincar\\'e in Paris. HCVS aims to bring together researchers working in the two communities of constraint/ logic programming (e.g., ICLP and CP), program verification (e.g., CAV, TACAS, and VMCAI), and automated deduction (e.g., CADE, IJCAR), on the topics of Horn clause based analysis, verification, and synthesis.",
        "published": "2024-04-21T14:22:53Z",
        "link": "http://arxiv.org/abs/2404.13672v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "cs.SE"
        ]
    },
    {
        "title": "Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs   Tree LSTMs",
        "authors": [
            "Rashid Barket",
            "Matthew England",
            "Jürgen Gerhard"
        ],
        "summary": "Computer Algebra Systems (e.g. Maple) are used in research, education, and industrial settings. One of their key functionalities is symbolic integration, where there are many sub-algorithms to choose from that can affect the form of the output integral, and the runtime. Choosing the right sub-algorithm for a given problem is challenging: we hypothesise that Machine Learning can guide this sub-algorithm choice. A key consideration of our methodology is how to represent the mathematics to the ML model: we hypothesise that a representation which encodes the tree structure of mathematical expressions would be well suited. We trained both an LSTM and a TreeLSTM model for sub-algorithm prediction and compared them to Maple's existing approach. Our TreeLSTM performs much better than the LSTM, highlighting the benefit of using an informed representation of mathematical expressions. It is able to produce better outputs than Maple's current state-of-the-art meta-algorithm, giving a strong basis for further research.",
        "published": "2024-04-23T12:27:20Z",
        "link": "http://arxiv.org/abs/2404.14973v1",
        "categories": [
            "cs.LG",
            "cs.MS",
            "cs.SC"
        ]
    },
    {
        "title": "Evolutionary Causal Discovery with Relative Impact Stratification for   Interpretable Data Analysis",
        "authors": [
            "Ou Deng",
            "Shoji Nishimura",
            "Atsushi Ogihara",
            "Qun Jin"
        ],
        "summary": "This study proposes Evolutionary Causal Discovery (ECD) for causal discovery that tailors response variables, predictor variables, and corresponding operators to research datasets. Utilizing genetic programming for variable relationship parsing, the method proceeds with the Relative Impact Stratification (RIS) algorithm to assess the relative impact of predictor variables on the response variable, facilitating expression simplification and enhancing the interpretability of variable relationships. ECD proposes an expression tree to visualize the RIS results, offering a differentiated depiction of unknown causal relationships compared to conventional causal discovery. The ECD method represents an evolution and augmentation of existing causal discovery methods, providing an interpretable approach for analyzing variable relationships in complex systems, particularly in healthcare settings with Electronic Health Record (EHR) data. Experiments on both synthetic and real-world EHR datasets demonstrate the efficacy of ECD in uncovering patterns and mechanisms among variables, maintaining high accuracy and stability across different noise levels. On the real-world EHR dataset, ECD reveals the intricate relationships between the response variable and other predictive variables, aligning with the results of structural equation modeling and shapley additive explanations analyses.",
        "published": "2024-04-25T06:42:32Z",
        "link": "http://arxiv.org/abs/2404.16361v1",
        "categories": [
            "cs.LG",
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Constrained Neural Networks for Interpretable Heuristic Creation to   Optimise Computer Algebra Systems",
        "authors": [
            "Dorian Florescu",
            "Matthew England"
        ],
        "summary": "We present a new methodology for utilising machine learning technology in symbolic computation research. We explain how a well known human-designed heuristic to make the choice of variable ordering in cylindrical algebraic decomposition may be represented as a constrained neural network. This allows us to then use machine learning methods to further optimise the heuristic, leading to new networks of similar size, representing new heuristics of similar complexity as the original human-designed one. We present this as a form of ante-hoc explainability for use in computer algebra development.",
        "published": "2024-04-26T16:20:04Z",
        "link": "http://arxiv.org/abs/2404.17508v1",
        "categories": [
            "cs.SC",
            "cs.LG",
            "68W30, 68T05, 03C10",
            "I.2.6; I.1.0"
        ]
    },
    {
        "title": "A Basis-preserving Algorithm for Computing the Bezout Matrix of Newton   Polynomials",
        "authors": [
            "Jing Yang",
            "Wei Yang"
        ],
        "summary": "This paper tackles the problem of constructing Bezout matrices for Newton polynomials in a basis-preserving approach that operates directly with the given Newton basis, thus avoiding the need for transformation from Newton basis to monomial basis. This approach significantly reduces the computational cost and also mitigates numerical instability caused by basis transformation. For this purpose, we investigate the internal structure of Bezout matrices in Newton basis and design a basis-preserving algorithm that generates the Bezout matrix in the specified basis used to formulate the input polynomials. Furthermore, we show an application of the proposed algorithm on constructing confederate resultant matrices for Newton polynomials. Experimental results demonstrate that the proposed methods perform superior to the basis-transformation-based ones.",
        "published": "2024-04-28T08:54:57Z",
        "link": "http://arxiv.org/abs/2404.18117v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "On Rational Recursion for Holonomic Sequences",
        "authors": [
            "Bertrand Teguia Tabuguia",
            "James Worrell"
        ],
        "summary": "It was recently conjectured that every component of a discrete-time rational dynamical system is a solution to an algebraic difference equation that is linear in its highest-shift term (a quasi-linear equation). We prove that the conjecture holds in the special case of holonomic sequences, which can straightforwardly be represented by rational dynamical systems. We propose two algorithms for converting holonomic recurrence equations into such quasi-linear equations. The two algorithms differ in their efficiency and the minimality of orders in their outputs.",
        "published": "2024-04-29T22:42:01Z",
        "link": "http://arxiv.org/abs/2404.19136v2",
        "categories": [
            "cs.SC",
            "cs.FL",
            "math.AC",
            "math.DS",
            "68W30, 12H10 (Primary), 13-04, 03C60 (Secondary)",
            "I.1.2"
        ]
    },
    {
        "title": "Symbolic construction of the chemical Jacobian of quasi-steady state   (QSS) chemistries for Exascale computing platforms",
        "authors": [
            "Malik Hassanaly",
            "Nicholas T. Wimer",
            "Anne Felden",
            "Lucas Esclapez",
            "Julia Ream",
            "Marc T. Henry de Frahan",
            "Jon Rood",
            "Marc Day"
        ],
        "summary": "The Quasi-Steady State Approximation (QSSA) can be an effective tool for reducing the size and stiffness of chemical mechanisms for implementation in computational reacting flow solvers. However, for many applications, stiffness remains, and the resulting model requires implicit methods for efficient time integration. In this paper, we outline an approach to formulating the QSSA reduction that is coupled with a strategy to generate C++ source code to evaluate the net species production rate, and the chemical Jacobian. The code-generation component employs a symbolic approach enabling a simple and effective strategy to analytically compute the chemical Jacobian. For computational tractability, the symbolic approach needs to be paired with common subexpression elimination which can negatively affect memory usage. Several solutions are outlined and successfully tested on a 3D multipulse ignition problem, thus allowing portable application across a chemical model sizes and GPU capabilities. The implementation of the proposed method is available at https://github.com/AMReX-Combustion/PelePhysics under an open-source license.",
        "published": "2024-04-30T23:42:12Z",
        "link": "http://arxiv.org/abs/2405.05974v2",
        "categories": [
            "physics.flu-dyn",
            "cs.SC",
            "physics.chem-ph"
        ]
    },
    {
        "title": "Unification in the description logic $\\mathcal{FL}_\\bot$",
        "authors": [
            "Barbara Morawska"
        ],
        "summary": "Description Logics are a formalism used in the knowledge representation, where the knowledge is captured in the form of concepts constructed in a controlled way from a restricted vocabulary. This allows one to test effectively for consistency of and the subsumption between the concepts. Unification of concepts may likewise become a useful tool in analysing the relations between concepts. The unification problem has been solved for the description logics $\\mathcal{FL}_0$ and $\\mathcal{EL}$. These small logics do not provide any means to express negation. Here we show an algorithm solving unification in $\\mathcal{FL}_\\bot$, the logic that extends $\\mathcal{FL}_0$ with the bottom concept. Bottom allows one to express that two concepts are disjoint. Our algorithm runs in exponential time, with respect to the size of the problem.",
        "published": "2024-05-01T23:53:34Z",
        "link": "http://arxiv.org/abs/2405.00912v3",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "Rings with common division, common meadows and their conditional   equational theories",
        "authors": [
            "Jan A Bergstra",
            "John V Tucker"
        ],
        "summary": "We examine the consequences of having a total division operation $\\frac{x}{y}$ on commutative rings. We consider two forms of binary division, one derived from a unary inverse, the other defined directly as a general operation; each are made total by setting $1/0$ equal to an error value $\\bot$, which is added to the ring. Such totalised divisions we call common divisions. In a field the two forms are equivalent and we have a finite equational axiomatisation $E$ that is complete for the equational theory of fields equipped with common division, called common meadows. These equational axioms $E$ turn out to be true of commutative rings with common division but only when defined via inverses. We explore these axioms $E$ and their role in seeking a completeness theorem for the conditional equational theory of common meadows. We prove they are complete for the conditional equational theory of commutative rings with inverse based common division. By adding a new proof rule, we can prove a completeness theorem for the conditional equational theory of common meadows. Although, the equational axioms $E$ fail with common division defined directly, we observe that the direct division does satisfies the equations in $E$ under a new congruence for partial terms called eager equality.",
        "published": "2024-05-02T21:07:00Z",
        "link": "http://arxiv.org/abs/2405.01733v2",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "How to generate all possible rational Wilf-Zeilberger forms?",
        "authors": [
            "Shaoshi Chen",
            "Christoph Koutschan",
            "Yisen Wang"
        ],
        "summary": "Wilf-Zeilberger pairs are fundamental in the algorithmic theory of Wilf and Zeilberger for computer-generated proofs of combinatorial identities. Wilf-Zeilberger forms are their high-dimensional generalizations, which can be used for proving and discovering convergence acceleration formulas. This paper presents a structural description of all possible rational such forms, which can be viewed as an additive analog of the classical Ore-Sato theorem. Based on this analog, we show a structural decomposition of so-called multivariate hyperarithmetic terms, which extend multivariate hypergeometric terms to the additive setting.",
        "published": "2024-05-03T18:52:23Z",
        "link": "http://arxiv.org/abs/2405.02430v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Effective Quadratic Error Bounds for Floating-Point Algorithms Computing   the Hypotenuse Function",
        "authors": [
            "Jean-Michel Muller",
            "Bruno Salvy"
        ],
        "summary": "We provide tools to help automate the error analysis of algorithms that evaluate simple functions over the floating-point numbers. The aim is to obtain tight relative error bounds for these algorithms, expressed as a function of the unit round-off. Due to the discrete nature of the set of floating-point numbers, the largest errors are often intrinsically \"arithmetic\" in the sense that their appearance may depend on specific bit patterns in the binary representations of intermediate variables, which may be present only for some precisions. We focus on generic (i.e., parameterized by the precision) and analytic over-estimations that still capture the correlations between the errors made at each step of the algorithms. Using methods from computer algebra, which we adapt to the particular structure of the polynomial systems that encode the errors, we obtain bounds with a linear term in the unit round-off that is sharp in manycases. An explicit quadratic bound is given, rather than the $O()$-estimate that is more common in this area. This is particularly important when using low precision formats, which are increasingly common in modern processors. Using this approach, we compare five algorithms for computing the hypotenuse function, ranging from elementary to quite challenging.",
        "published": "2024-05-06T15:59:46Z",
        "link": "http://arxiv.org/abs/2405.03588v1",
        "categories": [
            "math.NA",
            "cs.NA",
            "cs.SC"
        ]
    },
    {
        "title": "On $n$-Dimensional Sequences. I",
        "authors": [
            "Graham H. Norton"
        ],
        "summary": "Let $R$ be a commutative ring and let $n \\geq 1.$ We study $\\Gamma(s)$, the generating function and Ann$(s)$, the ideal of characteristic polynomials of $s$, an $n$--dimensional sequence over $R$.   We express $f(X_1,\\ldots,X_n) \\cdot \\Gamma(s)(X_1^{-1},\\ldots ,X_n^{-1})$ as a partitioned sum. That is, we give (i) a $2^n$--fold ``border'' partition (ii) an explicit expression for the product as a $2^n$--fold sum; the support of each summand is contained in precisely one member of the partition. A key summand is $\\beta_0(f,s)$, the ``border polynomial'' of $f$ and $s$, which is divisible by $X_1\\cdots X_n$.   We say that $s$ is {\\em eventually rectilinear} if the elimination ideals Ann$(s)\\cap R[X_i]$ contain an $f_i(X_i)$ for $1 \\leq i \\leq n$. In this case, we show that $\\mbox{Ann}(s)$ is the ideal quotient $(\\sum_{i=1}^n(f_i)\\ :\\ \\beta_0(f,s)/(X_1\\cdots X_n)).$   When $R$ and $R[[X_1,X_2, \\ldots ,X_n]]$ are factorial domains (e.g. $R$ a principal ideal domain or ${\\Bbb F}[X_1,\\ldots,X_n]$), we compute {\\em the monic generator} $\\gamma _i$ of $\\mbox{Ann}(s) \\cap R[X_i]$ from known $f_i \\in \\mbox{Ann}(s) \\cap R[X_i]$ or from a finite number of $1$--dimensional linear recurring sequences over $R$. Over a field ${\\Bbb F}$ this gives an $O(\\prod_{i=1}^n \\delta \\gamma _i^3)$ algorithm to compute an ${\\Bbb F}$--basis for $\\mbox{Ann}(s)$.",
        "published": "2024-05-07T05:49:08Z",
        "link": "http://arxiv.org/abs/2405.04022v1",
        "categories": [
            "math.AC",
            "cs.SC"
        ]
    },
    {
        "title": "Certifying Phase Abstraction",
        "authors": [
            "Nils Froleyks",
            "Emily Yu",
            "Armin Biere",
            "Keijo Heljanko"
        ],
        "summary": "Certification helps to increase trust in formal verification of safety-critical systems which require assurance on their correctness. In hardware model checking, a widely used formal verification technique, phase abstraction is considered one of the most commonly used preprocessing techniques. We present an approach to certify an extended form of phase abstraction using a generic certificate format. As in earlier works our approach involves constructing a witness circuit with an inductive invariant property that certifies the correctness of the entire model checking process, which is then validated by an independent certificate checker. We have implemented and evaluated the proposed approach including certification for various preprocessing configurations on hardware model checking competition benchmarks. As an improvement on previous work in this area, the proposed method is able to efficiently complete certification with an overhead of a fraction of model checking time.",
        "published": "2024-05-07T13:12:49Z",
        "link": "http://arxiv.org/abs/2405.04297v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Effective alpha theory certification using interval arithmetic: alpha   theory over regions",
        "authors": [
            "Kisun Lee"
        ],
        "summary": "We reexamine Smale's alpha theory as a way to certify a numerical solution to an analytic system. For a given point and a system, Smale's alpha theory determines whether Newton's method applied to this point shows the quadratic convergence to an exact solution. We introduce the alpha theory computation using interval arithmetic to avoid costly exact arithmetic. As a straightforward variation of the alpha theory, our work improves computational efficiency compared to software employing the traditional alpha theory.",
        "published": "2024-05-08T06:31:14Z",
        "link": "http://arxiv.org/abs/2405.04842v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "math.AG",
            "math.NA",
            "65H14"
        ]
    },
    {
        "title": "Harmonizing Program Induction with Rate-Distortion Theory",
        "authors": [
            "Hanqi Zhou",
            "David G. Nagy",
            "Charley M. Wu"
        ],
        "summary": "Many aspects of human learning have been proposed as a process of constructing mental programs: from acquiring symbolic number representations to intuitive theories about the world. In parallel, there is a long-tradition of using information processing to model human cognition through Rate Distortion Theory (RDT). Yet, it is still poorly understood how to apply RDT when mental representations take the form of programs. In this work, we adapt RDT by proposing a three way trade-off among rate (description length), distortion (error), and computational costs (search budget). We use simulations on a melody task to study the implications of this trade-off, and show that constructing a shared program library across tasks provides global benefits. However, this comes at the cost of sensitivity to curricula, which is also characteristic of human learners. Finally, we use methods from partial information decomposition to generate training curricula that induce more effective libraries and better generalization.",
        "published": "2024-05-08T10:03:50Z",
        "link": "http://arxiv.org/abs/2405.05294v1",
        "categories": [
            "cs.HC",
            "cs.CL",
            "cs.IT",
            "cs.LG",
            "cs.SC",
            "math.IT",
            "stat.ML"
        ]
    },
    {
        "title": "Transforming the Bootstrap: Using Transformers to Compute Scattering   Amplitudes in Planar N = 4 Super Yang-Mills Theory",
        "authors": [
            "Tianji Cai",
            "Garrett W. Merz",
            "François Charton",
            "Niklas Nolte",
            "Matthias Wilhelm",
            "Kyle Cranmer",
            "Lance J. Dixon"
        ],
        "summary": "We pursue the use of deep learning methods to improve state-of-the-art computations in theoretical high-energy physics. Planar N = 4 Super Yang-Mills theory is a close cousin to the theory that describes Higgs boson production at the Large Hadron Collider; its scattering amplitudes are large mathematical expressions containing integer coefficients. In this paper, we apply Transformers to predict these coefficients. The problem can be formulated in a language-like representation amenable to standard cross-entropy training objectives. We design two related experiments and show that the model achieves high accuracy (> 98%) on both tasks. Our work shows that Transformers can be applied successfully to problems in theoretical physics that require exact solutions.",
        "published": "2024-05-09T21:28:52Z",
        "link": "http://arxiv.org/abs/2405.06107v2",
        "categories": [
            "cs.LG",
            "cs.SC",
            "hep-ph",
            "hep-th",
            "stat.ML"
        ]
    },
    {
        "title": "Scalable Computation of Inter-Core Bounds Through Exact Abstractions",
        "authors": [
            "Mohammed Aristide Foughali",
            "Marius Mikučionis",
            "Maryline Zhang"
        ],
        "summary": "Real-time systems (RTSs) are at the heart of numerous safety-critical applications. An RTS typically consists of a set of real-time tasks (the software) that execute on a multicore shared-memory platform (the hardware) following a scheduling policy. In an RTS, computing inter-core bounds, i.e., bounds separating events produced by tasks on different cores, is crucial. While efficient techniques to over-approximate such bounds exist, little has been proposed to compute their exact values. Given an RTS with a set of cores C and a set of tasks T , under partitioned fixed-priority scheduling with limited preemption, a recent work by Foughali, Hladik and Zuepke (FHZ) models tasks with affinity c (i.e., allocated to core c in C) as a Uppaal timed automata (TA) network Nc. For each core c in C, Nc integrates blocking (due to data sharing) using tight analytical formulae. Through compositional model checking, FHZ achieved a substantial gain in scalability for bounds local to a core. However, computing inter-core bounds for some events of interest E, produced by a subset of tasks TE with different affinities CE, requires model checking the parallel composition of all TA networks Nc for each c in CE, which produces a large, often intractable, state space. In this paper, we present a new scalable approach based on exact abstractions to compute exact inter-core bounds in a schedulable RTS, under the assumption that tasks in TE have distinct affinities. We develop a novel algorithm, leveraging a new query that we implement in Uppaal, that computes for each TA network Nc in NE an abstraction A(Nc) preserving the exact intervals within which events occur on c, therefore drastically reducing the state space. The scalability of our approach is demonstrated on the WATERS 2017 industrial challenge, for which we efficiently compute various types of inter-core bounds where FHZ fails to scale.",
        "published": "2024-05-10T10:49:41Z",
        "link": "http://arxiv.org/abs/2405.06387v2",
        "categories": [
            "cs.FL",
            "cs.SC"
        ]
    },
    {
        "title": "Predictive Modeling of Flexible EHD Pumps using Kolmogorov-Arnold   Networks",
        "authors": [
            "Yanhong Peng",
            "Yuxin Wang",
            "Fangchao Hu",
            "Miao He",
            "Zebing Mao",
            "Xia Huang",
            "Jun Ding"
        ],
        "summary": "We present a novel approach to predicting the pressure and flow rate of flexible electrohydrodynamic pumps using the Kolmogorov-Arnold Network. Inspired by the Kolmogorov-Arnold representation theorem, KAN replaces fixed activation functions with learnable spline-based activation functions, enabling it to approximate complex nonlinear functions more effectively than traditional models like Multi-Layer Perceptron and Random Forest. We evaluated KAN on a dataset of flexible EHD pump parameters and compared its performance against RF, and MLP models. KAN achieved superior predictive accuracy, with Mean Squared Errors of 12.186 and 0.001 for pressure and flow rate predictions, respectively. The symbolic formulas extracted from KAN provided insights into the nonlinear relationships between input parameters and pump performance. These findings demonstrate that KAN offers exceptional accuracy and interpretability, making it a promising alternative for predictive modeling in electrohydrodynamic pumping.",
        "published": "2024-05-13T06:04:26Z",
        "link": "http://arxiv.org/abs/2405.07488v2",
        "categories": [
            "cs.LG",
            "cs.RO",
            "cs.SC"
        ]
    },
    {
        "title": "LLM4ED: Large Language Models for Automatic Equation Discovery",
        "authors": [
            "Mengge Du",
            "Yuntian Chen",
            "Zhongzheng Wang",
            "Longfeng Nie",
            "Dongxiao Zhang"
        ],
        "summary": "Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.",
        "published": "2024-05-13T14:03:49Z",
        "link": "http://arxiv.org/abs/2405.07761v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC",
            "math-ph",
            "math.MP",
            "stat.AP"
        ]
    },
    {
        "title": "Vector-Symbolic Architecture for Event-Based Optical Flow",
        "authors": [
            "Hongzhi You",
            "Yijun Cao",
            "Wei Yuan",
            "Fanjun Wang",
            "Ning Qiao",
            "Yongjie Li"
        ],
        "summary": "From a perspective of feature matching, optical flow estimation for event cameras involves identifying event correspondences by comparing feature similarity across accompanying event frames. In this work, we introduces an effective and robust high-dimensional (HD) feature descriptor for event frames, utilizing Vector Symbolic Architectures (VSA). The topological similarity among neighboring variables within VSA contributes to the enhanced representation similarity of feature descriptors for flow-matching points, while its structured symbolic representation capacity facilitates feature fusion from both event polarities and multiple spatial scales. Based on this HD feature descriptor, we propose a novel feature matching framework for event-based optical flow, encompassing both model-based (VSA-Flow) and self-supervised learning (VSA-SM) methods. In VSA-Flow, accurate optical flow estimation validates the effectiveness of HD feature descriptors. In VSA-SM, a novel similarity maximization method based on the HD feature descriptor is proposed to learn optical flow in a self-supervised way from events alone, eliminating the need for auxiliary grayscale images. Evaluation results demonstrate that our VSA-based method achieves superior accuracy in comparison to both model-based and self-supervised learning methods on the DSEC benchmark, while remains competitive among both methods on the MVSEC benchmark. This contribution marks a significant advancement in event-based optical flow within the feature matching methodology.",
        "published": "2024-05-14T03:50:07Z",
        "link": "http://arxiv.org/abs/2405.08300v2",
        "categories": [
            "cs.CV",
            "cs.SC"
        ]
    },
    {
        "title": "Wronskians form the inverse system of the arcs of a double point",
        "authors": [
            "Rida Ait El Manssour",
            "Gleb Pogudin"
        ],
        "summary": "The ideal of the arc scheme of a double point or, equivalently, the differential ideal generated by the ideal of a double point is a primary ideal in an infinite-dimensional polynomial ring supported at the origin. This ideal has a rich combinatorial structure connecting it to singularity theory, partition identities, representation theory, and differential algebra. Macaulay inverse system is a powerful tool for studying the structure of primary ideals which describes an ideal in terms of certain linear differential operators. In the present paper, we show that the inverse system of the ideal of the arc scheme of a double point is precisely a vector space spanned by all the Wronskians of the variables and their formal derivatives. We then apply this characterization to extend our recent result on Poincar\\'e-type series for such ideals.",
        "published": "2024-05-14T21:10:16Z",
        "link": "http://arxiv.org/abs/2405.08964v1",
        "categories": [
            "math.AC",
            "cs.SC",
            "math.AG",
            "math.CO",
            "12H05, 13D40, 05A17"
        ]
    },
    {
        "title": "Algebraic Tools for Computing Polynomial Loop Invariants",
        "authors": [
            "Erdenebayar Bayarmagnai",
            "Fatemeh Mohammadi",
            "Rémi Prébet"
        ],
        "summary": "Loop invariants are properties of a program loop that hold before and after each iteration of the loop. They are often employed to verify programs and ensure that algorithms consistently produce correct results during execution. Consequently, the generation of invariants becomes a crucial task for loops. We specifically focus on polynomial loops, where both the loop conditions and assignments within the loop are expressed as polynomials. Although computing polynomial invariants for general loops is undecidable, efficient algorithms have been developed for certain classes of loops. For instance, when all assignments within a while loop involve linear polynomials, the loop becomes solvable. In this work, we study the more general case where the polynomials exhibit arbitrary degrees.   Applying tools from algebraic geometry, we present two algorithms designed to generate all polynomial invariants for a while loop, up to a specified degree. These algorithms differ based on whether the initial values of the loop variables are given or treated as parameters. Furthermore, we introduce various methods to address cases where the algebraic problem exceeds the computational capabilities of our methods. In such instances, we identify alternative approaches to generate specific polynomial invariants.",
        "published": "2024-05-15T10:26:24Z",
        "link": "http://arxiv.org/abs/2405.09232v1",
        "categories": [
            "cs.SC",
            "cs.PL",
            "math.AG"
        ]
    },
    {
        "title": "Generalized Holographic Reduced Representations",
        "authors": [
            "Calvin Yeung",
            "Zhuowen Zou",
            "Mohsen Imani"
        ],
        "summary": "Deep learning has achieved remarkable success in recent years. Central to its success is its ability to learn representations that preserve task-relevant structure. However, massive energy, compute, and data costs are required to learn general representations. This paper explores Hyperdimensional Computing (HDC), a computationally and data-efficient brain-inspired alternative. HDC acts as a bridge between connectionist and symbolic approaches to artificial intelligence (AI), allowing explicit specification of representational structure as in symbolic approaches while retaining the flexibility of connectionist approaches. However, HDC's simplicity poses challenges for encoding complex compositional structures, especially in its binding operation. To address this, we propose Generalized Holographic Reduced Representations (GHRR), an extension of Fourier Holographic Reduced Representations (FHRR), a specific HDC implementation. GHRR introduces a flexible, non-commutative binding operation, enabling improved encoding of complex data structures while preserving HDC's desirable properties of robustness and transparency. In this work, we introduce the GHRR framework, prove its theoretical properties and its adherence to HDC properties, explore its kernel and binding characteristics, and perform empirical experiments showcasing its flexible non-commutativity, enhanced decoding accuracy for compositional structures, and improved memorization capacity compared to FHRR.",
        "published": "2024-05-15T20:37:48Z",
        "link": "http://arxiv.org/abs/2405.09689v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Bridging Syntax and Semantics of Lean Expressions in E-Graphs",
        "authors": [
            "Marcus Rossel",
            "Andrés Goens"
        ],
        "summary": "Interactive theorem provers, like Isabelle/HOL, Coq and Lean, have expressive languages that allow the formalization of general mathematical objects and proofs. In this context, an important goal is to reduce the time and effort needed to prove theorems. A significant means of achieving this is by improving proof automation. We have implemented an early prototype of proof automation for equational reasoning in Lean by using equality saturation. To achieve this, we need to bridge the gap between Lean's expression semantics and the syntactically driven e-graphs in equality saturation. This involves handling bound variables, implicit typing, as well as Lean's definitional equality, which is more general than syntactic equality and involves notions like $\\alpha$-equivalence, $\\beta$-reduction, and $\\eta$-reduction. In this extended abstract, we highlight how we attempt to bridge this gap, and which challenges remain to be solved. Notably, while our techniques are partially unsound, the resulting proof automation remains sound by virtue of Lean's proof checking.",
        "published": "2024-05-16T15:31:37Z",
        "link": "http://arxiv.org/abs/2405.10188v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "SMLP: Symbolic Machine Learning Prover (User Manual)",
        "authors": [
            "Franz Brauße",
            "Zurab Khasidashvili",
            "Konstantin Korovin"
        ],
        "summary": "SMLP: Symbolic Machine Learning Prover an open source tool for exploration and optimization of systems represented by machine learning models. SMLP uses symbolic reasoning for ML model exploration and optimization under verification and stability constraints, based on SMT, constraint and NN solvers. In addition its exploration methods are guided by probabilistic and statistical methods. SMLP is a general purpose tool that requires only data suitable for ML modelling in the csv format (usually samples of the system's input/output). SMLP has been applied at Intel for analyzing and optimizing hardware designs at the analog level. Currently SMLP supports NNs, polynomial and tree models, and uses SMT solvers for reasoning and optimization at the backend, integration of specialized NN solvers is in progress.",
        "published": "2024-05-16T16:05:21Z",
        "link": "http://arxiv.org/abs/2405.10215v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.LO",
            "cs.SC",
            "math.OC"
        ]
    },
    {
        "title": "Jacobi Stability Analysis for Systems of ODEs Using Symbolic Computation",
        "authors": [
            "Bo Huang",
            "Dongming Wang",
            "Jing Yang"
        ],
        "summary": "The classical theory of Kosambi-Cartan-Chern (KCC) developed in differential geometry provides a powerful method for analyzing the behaviors of dynamical systems. In the KCC theory, the properties of a dynamical system are described in terms of five geometrical invariants, of which the second corresponds to the so-called Jacobi stability of the system. Different from that of the Lyapunov stability that has been studied extensively in the literature, the analysis of the Jacobi stability has been investigated more recently using geometrical concepts and tools. It turns out that the existing work on the Jacobi stability analysis remains theoretical and the problem of algorithmic and symbolic treatment of Jacobi stability analysis has yet to be addressed. In this paper, we initiate our study on the problem for a class of ODE systems of arbitrary dimension and propose two algorithmic schemes using symbolic computation to check whether a nonlinear dynamical system may exhibit Jacobi stability. The first scheme, based on the construction of the complex root structure of a characteristic polynomial and on the method of quantifier elimination, is capable of detecting the existence of the Jacobi stability of the given dynamical system. The second algorithmic scheme exploits the method of semi-algebraic system solving and allows one to determine conditions on the parameters for a given dynamical system to have a prescribed number of Jacobi stable fixed points. Several examples are presented to demonstrate the effectiveness of the proposed algorithmic schemes.",
        "published": "2024-05-17T07:05:21Z",
        "link": "http://arxiv.org/abs/2405.10578v3",
        "categories": [
            "cs.SC",
            "34C07, 68W30"
        ]
    },
    {
        "title": "Strided Difference Bound Matrices",
        "authors": [
            "Arjun Pitchanathan",
            "Albert Cohen",
            "Oleksandr Zinenko",
            "Tobias Grosser"
        ],
        "summary": "A wide range of symbolic analysis and optimization problems can be formalized using polyhedra. Sub-classes of polyhedra, also known as sub-polyhedral domains, are sought for their lower space and time complexity. We introduce the Strided Difference Bound Matrix (SDBM) domain, which represents a sweet spot in the context of optimizing compilers. Its expressiveness and efficient algorithms are particularly well suited to the construction of machine learning compilers. We present decision algorithms, abstract domain operators and computational complexity proofs for SDBM. We also conduct an empirical study with the MLIR compiler framework to validate the domain's practical applicability. We characterize a sub-class of SDBMs that frequently occurs in practice, and demonstrate even faster algorithms on this sub-class.",
        "published": "2024-05-18T10:05:31Z",
        "link": "http://arxiv.org/abs/2405.11244v2",
        "categories": [
            "cs.SC",
            "cs.PL"
        ]
    },
    {
        "title": "Simulating Petri nets with Boolean Matrix Logic Programming",
        "authors": [
            "Lun Ai",
            "Stephen H. Muggleton",
            "Shi-Shun Liang",
            "Geoff S. Baldwin"
        ],
        "summary": "Recent attention to relational knowledge bases has sparked a demand for understanding how relations change between entities. Petri nets can represent knowledge structure and dynamically simulate interactions between entities, and thus they are well suited for achieving this goal. However, logic programs struggle to deal with extensive Petri nets due to the limitations of high-level symbol manipulations. To address this challenge, we introduce a novel approach called Boolean Matrix Logic Programming (BMLP), utilising boolean matrices as an alternative computation mechanism for Prolog to evaluate logic programs. Within this framework, we propose two novel BMLP algorithms for simulating a class of Petri nets known as elementary nets. This is done by transforming elementary nets into logically equivalent datalog programs. We demonstrate empirically that BMLP algorithms can evaluate these programs 40 times faster than tabled B-Prolog, SWI-Prolog, XSB-Prolog and Clingo. Our work enables the efficient simulation of elementary nets using Prolog, expanding the scope of analysis, learning and verification of complex systems with logic programming techniques.",
        "published": "2024-05-18T23:17:00Z",
        "link": "http://arxiv.org/abs/2405.11412v1",
        "categories": [
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "The Recovery of $λ$ from a Hilbert Polynomial",
        "authors": [
            "Joseph Donato",
            "Monica Lewis"
        ],
        "summary": "In the study of Hilbert schemes, the integer partition $\\lambda$ helps researchers identify some geometric and combinatorial properties of the scheme in question. To aid researchers in extracting such information from a Hilbert polynomial, we describe an efficient algorithm which can identify if $p(x)\\in\\mathbb{Q}[x]$ is a Hilbert polynomial and if so, recover the integer partition $\\lambda$ associated with it.",
        "published": "2024-05-21T15:55:34Z",
        "link": "http://arxiv.org/abs/2405.12886v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Verifying feasibility of degenerate semidefinite programs",
        "authors": [
            "Vladimir Kolmogorov",
            "Simone Naldi",
            "Jeferson Zapata"
        ],
        "summary": "This paper deals with the algorithmic aspects of solving feasibility problems of semidefinite programming (SDP), aka linear matrix inequalities (LMI). Since in some SDP instances all feasible solutions have irrational entries, numerical solvers that work with rational numbers can only find an approximate solution. We study the following question: is it possible to certify feasibility of a given SDP using an approximate solution that is sufficiently close to some exact solution? Existing approaches make the assumption that there exist rational feasible solutions (and use techniques such as rounding and lattice reduction algorithms).   We propose an alternative approach that does not need this assumption. More specifically, we show how to construct a system of polynomial equations whose set of real solutions is guaranteed to have an isolated correct solution (assuming that the target exact solution is maximum-rank). This allows, in particular, to use algorithms from real algebraic geometry for solving systems of polynomial equations, yielding a hybrid (or symbolic-numerical) method for SDPs. We experimentally compare it with a pure symbolic method in [Henrion, Naldi, Safey El Din; SIAM J. Optim., 2016]; the hybrid method was able to certify feasibility of many SDP instances on which [Henrion, Naldi, Safey El Din; SIAM J. Optim., 2016] failed. We argue that our approach may have other uses, such as refining an approximate solution using methods of numerical algebraic geometry for systems of polynomial equations.",
        "published": "2024-05-22T13:23:10Z",
        "link": "http://arxiv.org/abs/2405.13625v1",
        "categories": [
            "math.OC",
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Automated Loss function Search for Class-imbalanced Node Classification",
        "authors": [
            "Xinyu Guo",
            "Kai Wu",
            "Xiaoyu Zhang",
            "Jing Liu"
        ],
        "summary": "Class-imbalanced node classification tasks are prevalent in real-world scenarios. Due to the uneven distribution of nodes across different classes, learning high-quality node representations remains a challenging endeavor. The engineering of loss functions has shown promising potential in addressing this issue. It involves the meticulous design of loss functions, utilizing information about the quantities of nodes in different categories and the network's topology to learn unbiased node representations. However, the design of these loss functions heavily relies on human expert knowledge and exhibits limited adaptability to specific target tasks. In this paper, we introduce a high-performance, flexible, and generalizable automated loss function search framework to tackle this challenge. Across 15 combinations of graph neural networks and datasets, our framework achieves a significant improvement in performance compared to state-of-the-art methods. Additionally, we observe that homophily in graph-structured data significantly contributes to the transferability of the proposed framework.",
        "published": "2024-05-23T03:12:49Z",
        "link": "http://arxiv.org/abs/2405.14133v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "On the configurations of four spheres supporting the vertices of a   tetrahedron",
        "authors": [
            "Marco Longinetti",
            "Simone Naldi"
        ],
        "summary": "A reformulation of the three circles theorem of Johnson with distance coordinates to the vertices of a triangle is explicitly represented in a polynomial system and solved by symbolic computation. A similar polynomial system in distance coordinates to the vertices of a tetrahedron $T \\subset \\mathbb{R}^3$ is introduced to represent the configurations of four spheres of radius $R^*$, which intersect in one point, each sphere containing three vertices of $T$ but not the fourth one. This problem is related to that of computing the largest value $r$ for which the set of vertices of $T$ is an $r$-body. For triangular pyramids we completely describe the set of geometric configurations with the required four balls of radius $R^*$. The solutions obtained by symbolic computation show that triangular pyramids are splitted into two different classes: in the first one $R^*$ is unique, in the second one three values $R^*$ there exist. The first class can be itself subdivided into two subclasses, one of which is related to the family of $r$-bodies.",
        "published": "2024-05-25T10:25:38Z",
        "link": "http://arxiv.org/abs/2405.16167v1",
        "categories": [
            "math.MG",
            "cs.SC",
            "math.AG",
            "68W30, 52A30, 51FXX"
        ]
    },
    {
        "title": "Network reduction and absence of Hopf Bifurcations in dual   phosphorylation networks with three Intermediates",
        "authors": [
            "Elisenda Feliu",
            "Nidhi Kaihnsa"
        ],
        "summary": "Phosphorylation networks, representing the mechanisms by which proteins are phosphorylated at one or multiple sites, are ubiquitous in cell signalling and display rich dynamics such as unlimited multistability. Dual-site phosphorylation networks are known to exhibit oscillations in the form of periodic trajectories, when phosphorylation and dephosphorylation occurs as a mixed mechanism: phosphorylation of the two sites requires one encounter of the kinase, while dephosphorylation of the two sites requires two encounters with the phosphatase. A still open question is whether a mechanism requiring two encounters for both phosphorylation and dephosphorylation also admits oscillations. In this work we provide evidence in favor of the absence of oscillations of this network by precluding Hopf bifurcations in any reduced network comprising three out of its four intermediate protein complexes. Our argument relies on a novel network reduction step that preserves the absence of Hopf bifurcations, and on a detailed analysis of the semi-algebraic conditions precluding Hopf bifurcations obtained from Hurwitz determinants of the characteristic polynomial of the Jacobian of the system. We conjecture that the removal of certain reverse reactions appearing in Michaelis-Menten-type mechanisms does not have an impact on the presence or absence of Hopf bifurcations. We prove an implication of the conjecture under certain favorable scenarios and support the conjecture with additional example-based evidence.",
        "published": "2024-05-25T11:11:17Z",
        "link": "http://arxiv.org/abs/2405.16179v1",
        "categories": [
            "math.DS",
            "cs.SC",
            "q-bio.MN"
        ]
    },
    {
        "title": "Construction of birational trilinear volumes via tensor rank criteria",
        "authors": [
            "Laurent Busé",
            "Pablo Mazón"
        ],
        "summary": "We provide effective methods to construct and manipulate trilinear birational maps $\\phi:(\\mathbb{P}^1)^3\\dashrightarrow \\mathbb{P}^3$ by establishing a novel connection between birationality and tensor rank. These yield four families of nonlinear birational transformations between 3D spaces that can be operated with enough flexibility for applications in computer-aided geometric design. More precisely, we describe the geometric constraints on the defining control points of the map that are necessary for birationality, and present constructions for such configurations. For adequately constrained control points, we prove that birationality is achieved if and only if a certain $2\\times 2\\times 2$ tensor has rank one. As a corollary, we prove that the locus of weights that ensure birationality is $\\mathbb{P}^1\\times\\mathbb{P}^1\\times\\mathbb{P}^1$. Additionally, we provide formulas for the inverse $\\phi^{-1}$ as well as the explicit defining equations of the irreducible components of the base loci. Finally, we introduce a notion of \"distance to birationality\" for trilinear rational maps, and explain how to continuously deform birational maps.",
        "published": "2024-05-27T08:28:09Z",
        "link": "http://arxiv.org/abs/2405.16936v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "14E05, 14Q99, 65D17",
            "G.0; I.1; I.6; J.6"
        ]
    },
    {
        "title": "Learning from Uncertain Data: From Possible Worlds to Possible Models",
        "authors": [
            "Jiongli Zhu",
            "Su Feng",
            "Boris Glavic",
            "Babak Salimi"
        ],
        "summary": "We introduce an efficient method for learning linear models from uncertain data, where uncertainty is represented as a set of possible variations in the data, leading to predictive multiplicity. Our approach leverages abstract interpretation and zonotopes, a type of convex polytope, to compactly represent these dataset variations, enabling the symbolic execution of gradient descent on all possible worlds simultaneously. We develop techniques to ensure that this process converges to a fixed point and derive closed-form solutions for this fixed point. Our method provides sound over-approximations of all possible optimal models and viable prediction ranges. We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data.",
        "published": "2024-05-28T19:36:55Z",
        "link": "http://arxiv.org/abs/2405.18549v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "cs.SC"
        ]
    },
    {
        "title": "Unit-Aware Genetic Programming for the Development of Empirical   Equations",
        "authors": [
            "Julia Reuter",
            "Viktor Martinek",
            "Roland Herzog",
            "Sanaz Mostaghim"
        ],
        "summary": "When developing empirical equations, domain experts require these to be accurate and adhere to physical laws. Often, constants with unknown units need to be discovered alongside the equations. Traditional unit-aware genetic programming (GP) approaches cannot be used when unknown constants with undetermined units are included. This paper presents a method for dimensional analysis that propagates unknown units as ''jokers'' and returns the magnitude of unit violations. We propose three methods, namely evolutive culling, a repair mechanism, and a multi-objective approach, to integrate the dimensional analysis in the GP algorithm. Experiments on datasets with ground truth demonstrate comparable performance of evolutive culling and the multi-objective approach to a baseline without dimensional analysis. Extensive analysis of the results on datasets without ground truth reveals that the unit-aware algorithms make only low sacrifices in accuracy, while producing unit-adherent solutions. Overall, we presented a promising novel approach for developing unit-adherent empirical equations.",
        "published": "2024-05-29T08:57:00Z",
        "link": "http://arxiv.org/abs/2405.18896v1",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "On the Problem of Separating Variables in Multivariate Polynomial Ideals",
        "authors": [
            "Manfred Buchacher",
            "Manuel Kauers"
        ],
        "summary": "For a given ideal I in K[x_1,...,x_n,y_1,...,y_m] in a polynomial ring with n+m variables, we want to find all elements that can be written as f-g for some f in K[x_1,...,x_n] and some g in K[y_1,...,y_m], i.e., all elements of I that contain no term involving at the same time one of the x_1,...,x_n and one of the y_1,...,y_m. For principal ideals and for ideals of dimension zero, we give a algorithms that compute all these polynomials in a finite number of steps.",
        "published": "2024-05-29T16:03:48Z",
        "link": "http://arxiv.org/abs/2405.19223v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "LinApart: optimizing the univariate partial fraction decomposition",
        "authors": [
            "Bakar Chargeishvili",
            "Levente Fekésházy",
            "Gábor Somogyi",
            "Sam Van Thurenhout"
        ],
        "summary": "We present LinApart, a routine designed for efficiently performing the univariate partial fraction decomposition of large symbolic expressions. Our method is based on an explicit closed formula for the decomposition of rational functions with fully factorized denominators. We provide implementations in both the Wolfram Mathematica and C languages, made available at https://github.com/fekeshazy/LinApart . The routine can provide very significant performance gains over available tools such as the Apart command in Mathematica.",
        "published": "2024-05-30T15:10:30Z",
        "link": "http://arxiv.org/abs/2405.20130v1",
        "categories": [
            "cs.SC",
            "hep-ph",
            "hep-th"
        ]
    },
    {
        "title": "ParSEL: Parameterized Shape Editing with Language",
        "authors": [
            "Aditya Ganeshan",
            "Ryan Y. Huang",
            "Xianghao Xu",
            "R. Kenny Jones",
            "Daniel Ritchie"
        ],
        "summary": "The ability to edit 3D assets from natural language presents a compelling paradigm to aid in the democratization of 3D content creation. However, while natural language is often effective at communicating general intent, it is poorly suited for specifying precise manipulation. To address this gap, we introduce ParSEL, a system that enables controllable editing of high-quality 3D assets from natural language. Given a segmented 3D mesh and an editing request, ParSEL produces a parameterized editing program. Adjusting the program parameters allows users to explore shape variations with a precise control over the magnitudes of edits. To infer editing programs which align with an input edit request, we leverage the abilities of large-language models (LLMs). However, while we find that LLMs excel at identifying initial edit operations, they often fail to infer complete editing programs, and produce outputs that violate shape semantics. To overcome this issue, we introduce Analytical Edit Propagation (AEP), an algorithm which extends a seed edit with additional operations until a complete editing program has been formed. Unlike prior methods, AEP searches for analytical editing operations compatible with a range of possible user edits through the integration of computer algebra systems for geometric analysis. Experimentally we demonstrate ParSEL's effectiveness in enabling controllable editing of 3D objects through natural language requests over alternative system designs.",
        "published": "2024-05-30T17:55:46Z",
        "link": "http://arxiv.org/abs/2405.20319v2",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.HC",
            "cs.SC"
        ]
    },
    {
        "title": "Practical Modelling with Bigraphs",
        "authors": [
            "Blair Archibald",
            "Muffy Calder",
            "Michele Sevegnani"
        ],
        "summary": "Bigraphs are a versatile modelling formalism that allows easy expression of placement and connectivity relations in a graphical format. System evolution is user defined as a set of rewrite rules. This paper presents a practical, yet detailed guide to developing, executing, and reasoning about bigraph models, including recent extensions such as parameterised, instantaneous, prioritised and conditional rules, and probabilistic and stochastic rewriting.",
        "published": "2024-05-31T10:17:46Z",
        "link": "http://arxiv.org/abs/2405.20745v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "cs.SE"
        ]
    },
    {
        "title": "Shape Constraints in Symbolic Regression using Penalized Least Squares",
        "authors": [
            "Viktor Martinek",
            "Julia Reuter",
            "Ophelia Frotscher",
            "Sanaz Mostaghim",
            "Markus Richter",
            "Roland Herzog"
        ],
        "summary": "We study the addition of shape constraints (SC) and their consideration during the parameter identification step of symbolic regression (SR). SC serve as a means to introduce prior knowledge about the shape of the otherwise unknown model function into SR. Unlike previous works that have explored SC in SR, we propose minimizing SC violations during parameter identification using gradient-based numerical optimization. We test three algorithm variants to evaluate their performance in identifying three symbolic expressions from synthetically generated data sets. This paper examines two benchmark scenarios: one with varying noise levels and another with reduced amounts of training data. The results indicate that incorporating SC into the expression search is particularly beneficial when data is scarce. Compared to using SC only in the selection process, our approach of minimizing violations during parameter identification shows a statistically significant benefit in some of our test cases, without being significantly worse in any instance.",
        "published": "2024-05-31T14:01:12Z",
        "link": "http://arxiv.org/abs/2405.20800v2",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Discovering an interpretable mathematical expression for a full   wind-turbine wake with artificial intelligence enhanced symbolic regression",
        "authors": [
            "Ding Wang",
            "Yuntian Chen",
            "Shiyi Chen"
        ],
        "summary": "The rapid expansion of wind power worldwide underscores the critical significance of engineering-focused analytical wake models in both the design and operation of wind farms. These theoretically-derived ana lytical wake models have limited predictive capabilities, particularly in the near-wake region close to the turbine rotor, due to assumptions that do not hold. Knowledge discovery methods can bridge these gaps by extracting insights, adjusting for theoretical assumptions, and developing accurate models for physical processes. In this study, we introduce a genetic symbolic regression (SR) algorithm to discover an interpretable mathematical expression for the mean velocity deficit throughout the wake, a previously unavailable insight. By incorporating a double Gaussian distribution into the SR algorithm as domain knowledge and designing a hierarchical equation structure, the search space is reduced, thus efficiently finding a concise, physically informed, and robust wake model. The proposed mathematical expression (equation) can predict the wake velocity deficit at any location in the full-wake region with high precision and stability. The model's effectiveness and practicality are validated through experimental data and high-fidelity numerical simulations.",
        "published": "2024-06-02T10:17:54Z",
        "link": "http://arxiv.org/abs/2406.00695v1",
        "categories": [
            "physics.flu-dyn",
            "cs.LG",
            "cs.SC",
            "stat.AP"
        ]
    },
    {
        "title": "A Synergistic Approach In Network Intrusion Detection By Neurosymbolic   AI",
        "authors": [
            "Alice Bizzarri",
            "Chung-En Yu",
            "Brian Jalaian",
            "Fabrizio Riguzzi",
            "Nathaniel D. Bastian"
        ],
        "summary": "The prevailing approaches in Network Intrusion Detection Systems (NIDS) are often hampered by issues such as high resource consumption, significant computational demands, and poor interpretability. Furthermore, these systems generally struggle to identify novel, rapidly changing cyber threats. This paper delves into the potential of incorporating Neurosymbolic Artificial Intelligence (NSAI) into NIDS, combining deep learning's data-driven strengths with symbolic AI's logical reasoning to tackle the dynamic challenges in cybersecurity, which also includes detailed NSAI techniques introduction for cyber professionals to explore the potential strengths of NSAI in NIDS. The inclusion of NSAI in NIDS marks potential advancements in both the detection and interpretation of intricate network threats, benefiting from the robust pattern recognition of neural networks and the interpretive prowess of symbolic reasoning. By analyzing network traffic data types and machine learning architectures, we illustrate NSAI's distinctive capability to offer more profound insights into network behavior, thereby improving both detection performance and the adaptability of the system. This merging of technologies not only enhances the functionality of traditional NIDS but also sets the stage for future developments in building more resilient, interpretable, and dynamic defense mechanisms against advanced cyber threats. The continued progress in this area is poised to transform NIDS into a system that is both responsive to known threats and anticipatory of emerging, unseen ones.",
        "published": "2024-06-03T02:24:01Z",
        "link": "http://arxiv.org/abs/2406.00938v1",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Polynomial Bounds of CFLOBDDs against BDDs",
        "authors": [
            "Xusheng Zhi",
            "Thomas Reps"
        ],
        "summary": "Binary Decision Diagrams (BDDs) are widely used for the representation of Boolean functions. Context-Free-Language Ordered Decision Diagrams (CFLOBDDs) are a plug-compatible replacement for BDDs -- roughly, they are BDDs augmented with a certain form of procedure call. A natural question to ask is, ``For a given family of Boolean functions $F$, what is the relationship between the size of a BDD for $f \\in F$ and the size of a CFLOBDD for $f$?'' Sistla et al. established that there are best-case families of functions, which demonstrate an inherently exponential separation between CFLOBDDs and BDDs. They showed that there are families of functions $\\{ f_n \\}$ for which, for all $n = 2^k$, the CFLOBDD for $f_n$ (using a particular variable order) is exponentially more succinct than any BDD for $f_n$ (i.e., using any variable order). However, they did not give a worst-case bound -- i.e., they left open the question, ``Is there a family of functions $\\{ g_i \\}$ for which the size of a CFLOBDD for $g_i$ must be substantially larger than a BDD for $g_i$?'' For instance, it could be that there is a family of functions for which the BDDs are exponentially more succinct than any corresponding CFLOBDDs.   This paper studies such questions, and answers the second question posed above in the negative. In particular, we show that by using the same variable ordering in the CFLOBDD that is used in the BDD, the size of a CFLOBDD for any function $h$ cannot be far worse than the size of the BDD for $h$. The bound that relates their sizes is polynomial: If BDD $B$ for function $h$ is of size $|B|$ and uses variable ordering $\\textit{Ord}$, then the size of the CFLOBDD $C$ for $h$ that also uses $\\textit{Ord}$ is bounded by $O(|B|^3)$.   The paper also shows that the bound is tight: there is a family of functions for which $|C|$ grows as $\\Omega(|B|^3)$.",
        "published": "2024-06-03T16:55:59Z",
        "link": "http://arxiv.org/abs/2406.01525v2",
        "categories": [
            "cs.SC",
            "cs.DM",
            "cs.DS",
            "cs.FL",
            "I.1.1; G.2.2; F.4.3"
        ]
    },
    {
        "title": "clauseSMT: A NLSAT-Based Clause-Level Framework for Satisfiability   Modulo Nonlinear Real Arithmetic Theory",
        "authors": [
            "Zhonghan Wang"
        ],
        "summary": "Model-constructing satisfiability calculus (MCSAT) framework has been applied to SMT problems on different arithmetic theories. NLSAT, an implementation using cylindrical algebraic decomposition for explanation, is especially competitive among nonlinear real arithmetic constraints. However, current Conflict-Driven Clause Learning (CDCL)-style algorithms only consider literal information for decision, and thus ignore clause-level influence on arithmetic variables. As a consequence, NLSAT encounters unnecessary conflicts caused by improper literal decisions.   In this work, we analyze the literal decision caused conflicts, and introduce clause-level information with a direct effect on arithmetic variables. Two main algorithm improvements are presented: clause-level feasible-set based look-ahead mechanism and arithmetic propagation based branching heuristic. We implement our solver named clauseSMT on our dynamic variable ordering framework. Experiments show that clauseSMT is competitive on nonlinear real arithmetic theory against existing SMT solvers (cvc5, Z3, Yices2), and outperforms all these solvers on satisfiable instances of SMT(QF_NRA) in SMT-LIB. The effectiveness of our proposed methods are also studied.",
        "published": "2024-06-04T09:05:09Z",
        "link": "http://arxiv.org/abs/2406.02122v3",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "Representing Piecewise-Linear Functions by Functions with Minimal Arity",
        "authors": [
            "Christoph Koutschan",
            "Anton Ponomarchuk",
            "Josef Schicho"
        ],
        "summary": "Any continuous piecewise-linear function $F\\colon \\mathbb{R}^{n}\\to \\mathbb{R}$ can be represented as a linear combination of $\\max$ functions of at most $n+1$ affine-linear functions. In our previous paper [``Representing piecewise linear functions by functions with small arity'', AAECC, 2023], we showed that this upper bound of $n+1$ arguments is tight. In the present paper, we extend this result by establishing a correspondence between the function $F$ and the minimal number of arguments that are needed in any such decomposition. We show that the tessellation of the input space $\\mathbb{R}^{n}$ induced by the function $F$ has a direct connection to the number of arguments in the $\\max$ functions.",
        "published": "2024-06-04T15:39:08Z",
        "link": "http://arxiv.org/abs/2406.02421v1",
        "categories": [
            "cs.DM",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Expressive Symbolic Regression for Interpretable Models of Discrete-Time   Dynamical Systems",
        "authors": [
            "Adarsh Iyer",
            "Nibodh Boddupalli",
            "Jeff Moehlis"
        ],
        "summary": "Interpretable mathematical expressions defining discrete-time dynamical systems (iterated maps) can model many phenomena of scientific interest, enabling a deeper understanding of system behaviors. Since formulating governing expressions from first principles can be difficult, it is of particular interest to identify expressions for iterated maps given only their data streams. In this work, we consider a modified Symbolic Artificial Neural Network-Trained Expressions (SymANNTEx) architecture for this task, an architecture more expressive than others in the literature. We make a modification to the model pipeline to optimize the regression, then characterize the behavior of the adjusted model in identifying several classical chaotic maps. With the goal of parsimony, sparsity-inducing weight regularization and information theory-informed simplification are implemented. We show that our modified SymANNTEx model properly identifies single-state maps and achieves moderate success in approximating a dual-state attractor. These performances offer significant promise for data-driven scientific discovery and interpretation.",
        "published": "2024-06-05T05:05:29Z",
        "link": "http://arxiv.org/abs/2406.06585v1",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Amortized Equation Discovery in Hybrid Dynamical Systems",
        "authors": [
            "Yongtuo Liu",
            "Sara Magliacane",
            "Miltiadis Kofinas",
            "Efstratios Gavves"
        ],
        "summary": "Hybrid dynamical systems are prevalent in science and engineering to express complex systems with continuous and discrete states. To learn the laws of systems, all previous methods for equation discovery in hybrid systems follow a two-stage paradigm, i.e. they first group time series into small cluster fragments and then discover equations in each fragment separately through methods in non-hybrid systems. Although effective, these methods do not fully take advantage of the commonalities in the shared dynamics of multiple fragments that are driven by the same equations. Besides, the two-stage paradigm breaks the interdependence between categorizing and representing dynamics that jointly form hybrid systems. In this paper, we reformulate the problem and propose an end-to-end learning framework, i.e. Amortized Equation Discovery (AMORE), to jointly categorize modes and discover equations characterizing the dynamics of each mode by all segments of the mode. Experiments on four hybrid and six non-hybrid systems show that our method outperforms previous methods on equation discovery, segmentation, and forecasting.",
        "published": "2024-06-06T07:49:02Z",
        "link": "http://arxiv.org/abs/2406.03818v1",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.MA",
            "cs.SC"
        ]
    },
    {
        "title": "Whitney Stratification of Algebraic Boundaries of Convex Semi-algebraic   Sets",
        "authors": [
            "Zihao Dai",
            "Zijia Li",
            "Zhi-Hong Yang",
            "Lihong Zhi"
        ],
        "summary": "Algebraic boundaries of convex semi-algebraic sets are closely related to polynomial optimization problems. Building upon Rainer Sinn's work, we refine the stratification of iterated singular loci to a Whitney (a) stratification, which gives a list of candidates of varieties whose dual is an irreducible component of the algebraic boundary of the dual convex body. We also present an algorithm based on Teissier's criterion to compute Whitney (a) stratifications, which employs conormal spaces and prime decomposition.",
        "published": "2024-06-07T06:51:22Z",
        "link": "http://arxiv.org/abs/2406.04681v1",
        "categories": [
            "math.OC",
            "cs.SC",
            "math.AG",
            "52A99, 14N05, 14P10, 51N35, 14Q15"
        ]
    },
    {
        "title": "Differential equations satisfied by generating functions of 5-, 6-, and   7-regular labelled graphs: a reduction-based approach",
        "authors": [
            "Frédéric Chyzak",
            "Marni Mishna"
        ],
        "summary": "By a classic result of Gessel, the exponential generating functions for $k$-regular graphs are D-finite. Using Gr\\\"obner bases in Weyl algebras, we compute the linear differential equations satisfied by the generating function for 5-, 6-, and 7- regular graphs. The method is sufficiently robust to consider variants such as graphs with multiple edges, loops, and graphs whose degrees are limited to fixed sets of values.",
        "published": "2024-06-07T08:53:04Z",
        "link": "http://arxiv.org/abs/2406.04753v2",
        "categories": [
            "math.CO",
            "cs.SC",
            "05C30, 12H05"
        ]
    },
    {
        "title": "Linear equations with monomial constraints and decision problems in   abelian-by-cyclic groups",
        "authors": [
            "Ruiwen Dong"
        ],
        "summary": "We show that it is undecidable whether a system of linear equations over the Laurent polynomial ring $\\mathbb{Z}[X^{\\pm}]$ admit solutions where a specified subset of variables take value in the set of monomials $\\{X^z \\mid z \\in \\mathbb{Z}\\}$. In particular, we construct a finitely presented $\\mathbb{Z}[X^{\\pm}]$-module, where it is undecidable whether a linear equation $X^{z_1} \\boldsymbol{f}_1 + \\cdots + X^{z_n} \\boldsymbol{f}_n = \\boldsymbol{f}_0$ has solutions $z_1, \\ldots, z_n \\in \\mathbb{Z}$. This contrasts the decidability of the case $n = 1$, which can be deduced from Noskov's Lemma.   We apply this result to settle a number of problems in computational group theory. We show that it is undecidable whether a system of equations has solutions in the wreath product $\\mathbb{Z} \\wr \\mathbb{Z}$, providing a negative answer to an open problem of Kharlampovich, L\\'{o}pez and Miasnikov (2020). We show that there exists a finitely generated abelian-by-cyclic group in which the problem of solving a single quadratic equation is undecidable. We also construct a finitely generated abelian-by-cyclic group, different to that of Mishchenko and Treier (2017), in which the Knapsack Problem is undecidable. In contrast, we show that the problem of Coset Intersection is decidable in all finitely generated abelian-by-cyclic groups.",
        "published": "2024-06-12T17:59:14Z",
        "link": "http://arxiv.org/abs/2406.08480v3",
        "categories": [
            "cs.SC",
            "cs.LO",
            "math.AC",
            "math.GR"
        ]
    },
    {
        "title": "A Symbolic Computing Perspective on Software Systems",
        "authors": [
            "Arthur C. Norman",
            "Stephen M. Watt"
        ],
        "summary": "Symbolic mathematical computing systems have served as a canary in the coal mine of software systems for more than sixty years. They have introduced or have been early adopters of programming language ideas such ideas as dynamic memory management, arbitrary precision arithmetic and dependent types. These systems have the feature of being highly complex while at the same time operating in a domain where results are well-defined and clearly verifiable. These software systems span multiple layers of abstraction with concerns ranging from instruction scheduling and cache pressure up to algorithmic complexity of constructions in algebraic geometry. All of the major symbolic mathematical computing systems include low-level code for arithmetic, memory management and other primitives, a compiler or interpreter for a bespoke programming language, a library of high level mathematical algorithms, and some form of user interface. Each of these parts invokes multiple deep issues.   We present some lessons learned from this environment and free flowing opinions on topics including:   * Portability of software across architectures and decades;   * Infrastructure to embrace and infrastructure to avoid;   * Choosing base abstractions upon which to build;   * How to get the most out of a small code base;   * How developments in compilers both to optimise and to validate code have always been and remain of critical importance, with plenty of remaining challenges;   * The way in which individuals including in particular Alan Mycroft who has been able to span from hand-crafting Z80 machine code up to the most abstruse high level code analysis techniques are needed, and   * Why it is important to teach full-stack thinking to the next generation.",
        "published": "2024-06-13T13:10:47Z",
        "link": "http://arxiv.org/abs/2406.09085v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "cs.SE",
            "I.1.3"
        ]
    },
    {
        "title": "Sparse Tensors and Subdivision Methods for Finding the Zero Set of   Polynomial Equations",
        "authors": [
            "Guillaume Moroz"
        ],
        "summary": "Finding the solutions to a system of multivariate polynomial equations is a fundamental problem in mathematics and computer science. It involves evaluating the polynomials at many points, often chosen from a grid. In most current methods, such as subdivision, homotopy continuation, or marching cube algorithms, polynomial evaluation is treated as a black box, repeating the process for each point. We propose a new approach that partially evaluates the polynomials, allowing us to efficiently reuse computations across multiple points in a grid. Our method leverages the Compressed Sparse Fiber data structure to efficiently store and process subsets of grid points. We integrated our amortized evaluation scheme into a subdivision algorithm. Experimental results show that our approach is efficient in practice. Notably, our software \\texttt{voxelize} can successfully enclose curves defined by two trivariate polynomial equations of degree $100$, a problem that was previously intractable.",
        "published": "2024-06-14T09:18:23Z",
        "link": "http://arxiv.org/abs/2406.09857v1",
        "categories": [
            "cs.CG",
            "cs.NA",
            "cs.SC",
            "math.NA"
        ]
    },
    {
        "title": "Neural Concept Binder",
        "authors": [
            "Wolfgang Stammer",
            "Antonia Wüst",
            "David Steinmann",
            "Kristian Kersting"
        ],
        "summary": "The challenge in object-based visual reasoning lies in generating concept representations that are both descriptive and distinct. Achieving this in an unsupervised manner requires human users to understand the model's learned concepts and, if necessary, revise incorrect ones. To address this challenge, we introduce the Neural Concept Binder (NCB), a novel framework for deriving both discrete and continuous concept representations, which we refer to as \"concept-slot encodings\". NCB employs two types of binding: \"soft binding\", which leverages the recent SysBinder mechanism to obtain object-factor encodings, and subsequent \"hard binding\", achieved through hierarchical clustering and retrieval-based inference. This enables obtaining expressive, discrete representations from unlabeled images. Moreover, the structured nature of NCB's concept representations allows for intuitive inspection and the straightforward integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism preserves model performance while enabling seamless integration into both neural and symbolic modules for complex reasoning tasks. We validate the effectiveness of NCB through evaluations on our newly introduced CLEVR-Sudoku dataset.",
        "published": "2024-06-14T11:52:09Z",
        "link": "http://arxiv.org/abs/2406.09949v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "The Liouville Generator for Producing Integrable Expressions",
        "authors": [
            "Rashid Barket",
            "Matthew England",
            "Jürgen Gerhard"
        ],
        "summary": "There has been a growing need to devise processes that can create comprehensive datasets in the world of Computer Algebra, both for accurate benchmarking and for new intersections with machine learning technology. We present here a method to generate integrands that are guaranteed to be integrable, dubbed the LIOUVILLE method. It is based on Liouville's theorem and the Parallel Risch Algorithm for symbolic integration.   We show that this data generation method retains the best qualities of previous data generation methods, while overcoming some of the issues built into that prior work. The LIOUVILLE generator is able to generate sufficiently complex and realistic integrands, and could be used for benchmarking or machine learning training tasks related to symbolic integration.",
        "published": "2024-06-17T15:13:36Z",
        "link": "http://arxiv.org/abs/2406.11631v1",
        "categories": [
            "cs.SC",
            "cs.LG"
        ]
    },
    {
        "title": "VeriFlow: Modeling Distributions for Neural Network Verification",
        "authors": [
            "Faried Abu Zaid",
            "Daniel Neider",
            "Mustafa Yalçıner"
        ],
        "summary": "Formal verification has emerged as a promising method to ensure the safety and reliability of neural networks. Naively verifying a safety property amounts to ensuring the safety of a neural network for the whole input space irrespective of any training or test set. However, this also implies that the safety of the neural network is checked even for inputs that do not occur in the real-world and have no meaning at all, often resulting in spurious errors. To tackle this shortcoming, we propose the VeriFlow architecture as a flow based density model tailored to allow any verification approach to restrict its search to the some data distribution of interest. We argue that our architecture is particularly well suited for this purpose because of two major properties. First, we show that the transformation and log-density function that are defined by our model are piece-wise affine. Therefore, the model allows the usage of verifiers based on SMT with linear arithmetic. Second, upper density level sets (UDL) of the data distribution take the shape of an $L^p$-ball in the latent space. As a consequence, representations of UDLs specified by a given probability are effectively computable in latent space. This allows for SMT and abstract interpretation approaches with fine-grained, probabilistically interpretable, control regarding on how (a)typical the inputs subject to verification are.",
        "published": "2024-06-20T12:41:39Z",
        "link": "http://arxiv.org/abs/2406.14265v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Combining Neural Networks and Symbolic Regression for Analytical   Lyapunov Function Discovery",
        "authors": [
            "Jie Feng",
            "Haohan Zou",
            "Yuanyuan Shi"
        ],
        "summary": "We propose CoNSAL (Combining Neural networks and Symbolic regression for Analytical Lyapunov function) to construct analytical Lyapunov functions for nonlinear dynamic systems. This framework contains a neural Lyapunov function and a symbolic regression component, where symbolic regression is applied to distill the neural network to precise analytical forms. Our approach utilizes symbolic regression not only as a tool for translation but also as a means to uncover counterexamples. This procedure terminates when no counterexamples are found in the analytical formulation. Compared with previous results, CoNSAL directly produces an analytical form of the Lyapunov function with improved interpretability in both the learning process and the final results. We apply CoNSAL to 2-D inverted pendulum, path following, Van Der Pol Oscillator, 3-D trig dynamics, 4-D rotating wheel pendulum, 6-D 3-bus power system, and demonstrate that our algorithm successfully finds their valid Lyapunov functions. Code examples are available at https://github.com/HaohanZou/CoNSAL.",
        "published": "2024-06-21T22:31:06Z",
        "link": "http://arxiv.org/abs/2406.15675v3",
        "categories": [
            "eess.SY",
            "cs.AI",
            "cs.SC",
            "cs.SY"
        ]
    },
    {
        "title": "A Local Search Algorithm for MaxSMT(LIA)",
        "authors": [
            "Xiang He",
            "Bohan Li",
            "Mengyu Zhao",
            "Shaowei Cai"
        ],
        "summary": "MaxSAT modulo theories (MaxSMT) is an important generalization of Satisfiability modulo theories (SMT) with various applications. In this paper, we focus on MaxSMT with the background theory of Linear Integer Arithmetic, denoted as MaxSMT(LIA). We design the first local search algorithm for MaxSMT(LIA) called PairLS, based on the following novel ideas. A novel operator called pairwise operator is proposed for integer variables. It extends the original local search operator by simultaneously operating on two variables, enriching the search space. Moreover, a compensation-based picking heuristic is proposed to determine and distinguish the pairwise operations. Experiments are conducted to evaluate our algorithm on massive benchmarks. The results show that our solver is competitive with state-of-the-art MaxSMT solvers. Furthermore, we also apply the pairwise operation to enhance the local search algorithm of SMT, which shows its extensibility.",
        "published": "2024-06-22T08:22:34Z",
        "link": "http://arxiv.org/abs/2406.15782v1",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "A New Algorithm for Whitney Stratification of Varieties",
        "authors": [
            "Martin Helmer",
            "Rafael Mohr"
        ],
        "summary": "We describe a new algorithm to compute Whitney stratifications of real and complex algebraic varieties. This algorithm is a modification of the algorithm of Helmer and Nanda (HN), but is made more efficient by using techniques for equidimensional decomposition rather than computing the set of associated primes of a polynomial ideal at a key step in the HN algorithm. We note that this modified algorithm may fail to produce a minimal Whitney stratification even when the HN algorithm would produce a minimal stratification. We, additionally, present an algorithm to coarsen any Whitney stratification of a complex variety to a minimal Whitney stratification; the theoretical basis for our approach is a classical result of Teissier.",
        "published": "2024-06-24T20:17:06Z",
        "link": "http://arxiv.org/abs/2406.17122v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AC",
            "math.AT",
            "14B05, 14Q20, 32S60, 32S15"
        ]
    },
    {
        "title": "Large Language Models are Interpretable Learners",
        "authors": [
            "Ruochen Wang",
            "Si Si",
            "Felix Yu",
            "Dorothea Wiesmann",
            "Cho-Jui Hsieh",
            "Inderjit Dhillon"
        ],
        "summary": "The trade-off between expressiveness and interpretability remains a core challenge when building human-centric predictive models for classification and decision-making. While symbolic rules offer interpretability, they often lack expressiveness, whereas neural networks excel in performance but are known for being black boxes. In this paper, we show a combination of Large Language Models (LLMs) and symbolic programs can bridge this gap. In the proposed LLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language prompts provides a massive set of interpretable modules that can transform raw input into natural language concepts. Symbolic programs then integrate these modules into an interpretable decision rule. To train LSPs, we develop a divide-and-conquer approach to incrementally build the program from scratch, where the learning process of each step is guided by LLMs. To evaluate the effectiveness of LSPs in extracting interpretable and accurate knowledge from data, we introduce IL-Bench, a collection of diverse tasks, including both synthetic and real-world scenarios across different modalities. Empirical results demonstrate LSP's superior performance compared to traditional neurosymbolic programs and vanilla automatic prompt tuning methods. Moreover, as the knowledge learned by LSP is a combination of natural language descriptions and symbolic rules, it is easily transferable to humans (interpretable), and other LLMs, and generalizes well to out-of-distribution samples.",
        "published": "2024-06-25T02:18:15Z",
        "link": "http://arxiv.org/abs/2406.17224v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.SC",
            "68T05"
        ]
    },
    {
        "title": "YAQQ: Yet Another Quantum Quantizer -- Design Space Exploration of   Quantum Gate Sets using Novelty Search",
        "authors": [
            "Aritra Sarkar",
            "Akash Kundu",
            "Matthew Steinberg",
            "Sibasish Mishra",
            "Sebastiaan Fauquenot",
            "Tamal Acharya",
            "Jarosław A. Miszczak",
            "Sebastian Feld"
        ],
        "summary": "In the standard circuit model of quantum computation, the number and quality of the quantum gates composing the circuit influence the runtime and fidelity of the computation. The fidelity of the decomposition of quantum algorithms, represented as unitary matrices, to bounded depth quantum circuits depends strongly on the set of gates available for the decomposition routine. To investigate this dependence, we explore the design space of discrete quantum gate sets and present a software tool for comparative analysis of quantum processing units and control protocols based on their native gates. The evaluation is conditioned on a set of unitary transformations representing target use cases on the quantum processors. The cost function considers three key factors: (i) the statistical distribution of the decomposed circuits' depth, (ii) the statistical distribution of process fidelities for the approximate decomposition, and (iii) the relative novelty of a gate set compared to other gate sets in terms of the aforementioned properties. The developed software, YAQQ (Yet Another Quantum Quantizer), enables the discovery of an optimized set of quantum gates through this tunable joint cost function. To identify these gate sets, we use the novelty search algorithm, circuit decomposition techniques, and stochastic optimization to implement YAQQ within the Qiskit quantum simulator environment. YAQQ exploits reachability tradeoffs conceptually derived from quantum algorithmic information theory. Our results demonstrate the pragmatic application of identifying gate sets that are advantageous to popularly used quantum gate sets in representing quantum algorithms. Consequently, we demonstrate pragmatic use cases of YAQQ in comparing transversal logical gate sets in quantum error correction codes, designing optimal quantum instruction sets, and compiling to specific quantum processors.",
        "published": "2024-06-25T14:55:35Z",
        "link": "http://arxiv.org/abs/2406.17610v1",
        "categories": [
            "quant-ph",
            "cs.IT",
            "cs.SC",
            "math.IT"
        ]
    },
    {
        "title": "Solving Hard Mizar Problems with Instantiation and Strategy Invention",
        "authors": [
            "Jan Jakubův",
            "Mikoláš Janota",
            "Josef Urban"
        ],
        "summary": "In this work, we prove over 3000 previously ATP-unproved Mizar/MPTP problems by using several ATP and AI methods, raising the number of ATP-solved Mizar problems from 75\\% to above 80\\%. First, we start to experiment with the cvc5 SMT solver which uses several instantiation-based heuristics that differ from the superposition-based systems, that were previously applied to Mizar,and add many new solutions. Then we use automated strategy invention to develop cvc5 strategies that largely improve cvc5's performance on the hard problems. In particular, the best invented strategy solves over 14\\% more problems than the best previously available cvc5 strategy. We also show that different clausification methods have a high impact on such instantiation-based methods, again producing many new solutions. In total, the methods solve 3021 (21.3\\%) of the 14163 previously unsolved hard Mizar problems. This is a new milestone over the Mizar large-theory benchmark and a large strengthening of the hammer methods for Mizar.",
        "published": "2024-06-25T17:47:13Z",
        "link": "http://arxiv.org/abs/2406.17762v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Reasoning About Action and Change",
        "authors": [
            "Florence Dupin de Saint-Cyr",
            "Andreas Herzig",
            "Jérôme Lang",
            "Pierre Marquis"
        ],
        "summary": "The purpose of this book is to provide an overview of AI research, ranging from basic work to interfaces and applications, with as much emphasis on results as on current issues. It is aimed at an audience of master students and Ph.D. students, and can be of interest as well for researchers and engineers who want to know more about AI. The book is split into three volumes.",
        "published": "2024-06-27T06:53:28Z",
        "link": "http://arxiv.org/abs/2406.18930v1",
        "categories": [
            "cs.AI",
            "cs.DM",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "DNLSAT: A Dynamic Variable Ordering MCSAT Framework for Nonlinear Real   Arithmetic",
        "authors": [
            "Zhonghan Wang"
        ],
        "summary": "Satisfiability modulo nonlinear real arithmetic theory (SMT(NRA)) solving is essential to multiple applications, including program verification, program synthesis and software testing. In this context, recently model constructing satisfiability calculus (MCSAT) has been invented to directly search for models in the theory space. Although following papers discussed practical directions and updates on MCSAT, less attention has been paid to the detailed implementation. In this paper, we present an efficient implementation of dynamic variable orderings of MCSAT, called dnlsat. We show carefully designed data structures and promising mechanisms, such as branching heuristic, restart, and lemma management. Besides, we also give a theoretical study of potential influences brought by the dynamic variablr ordering. The experimental evaluation shows that dnlsat accelerates the solving speed and solves more satisfiable instances than other state-of-the-art SMT solvers.   Demonstration Video: https://youtu.be/T2Z0gZQjnPw   Code: https://github.com/yogurt-shadow/dnlsat/tree/master/code   Benchmark https://zenodo.org/records/10607722/files/QF_NRA.tar.zst?download=1",
        "published": "2024-06-27T07:49:03Z",
        "link": "http://arxiv.org/abs/2406.18964v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Towards Learning Abductive Reasoning using VSA Distributed   Representations",
        "authors": [
            "Giacomo Camposampiero",
            "Michael Hersche",
            "Aleksandar Terzić",
            "Roger Wattenhofer",
            "Abu Sebastian",
            "Abbas Rahimi"
        ],
        "summary": "We introduce the Abductive Rule Learner with Context-awareness (ARLC), a model that solves abstract reasoning tasks based on Learn-VRF. ARLC features a novel and more broadly applicable training objective for abductive reasoning, resulting in better interpretability and higher accuracy when solving Raven's progressive matrices (RPM). ARLC allows both programming domain knowledge and learning the rules underlying a data distribution. We evaluate ARLC on the I-RAVEN dataset, showcasing state-of-the-art accuracy across both in-distribution and out-of-distribution (unseen attribute-rule pairs) tests. ARLC surpasses neuro-symbolic and connectionist baselines, including large language models, despite having orders of magnitude fewer parameters. We show ARLC's robustness to post-programming training by incrementally learning from examples on top of programmed knowledge, which only improves its performance and does not result in catastrophic forgetting of the programmed solution. We validate ARLC's seamless transfer learning from a 2x2 RPM constellation to unseen constellations. Our code is available at https://github.com/IBM/abductive-rule-learner-with-context-awareness.",
        "published": "2024-06-27T12:05:55Z",
        "link": "http://arxiv.org/abs/2406.19121v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "SAT and Lattice Reduction for Integer Factorization",
        "authors": [
            "Yameen Ajani",
            "Curtis Bright"
        ],
        "summary": "The difficulty of factoring large integers into primes is the basis for cryptosystems such as RSA. Due to the widespread popularity of RSA, there have been many proposed attacks on the factorization problem such as side-channel attacks where some bits of the prime factors are available. When enough bits of the prime factors are known, two methods that are effective at solving the factorization problem are satisfiability (SAT) solvers and Coppersmith's method. The SAT approach reduces the factorization problem to a Boolean satisfiability problem, while Coppersmith's approach uses lattice basis reduction. Both methods have their advantages, but they also have their limitations: Coppersmith's method does not apply when the known bit positions are randomized, while SAT-based methods can take advantage of known bits in arbitrary locations, but have no knowledge of the algebraic structure exploited by Coppersmith's method. In this paper we describe a new hybrid SAT and computer algebra approach to efficiently solve random leaked-bit factorization problems. Specifically, Coppersmith's method is invoked by a SAT solver to determine whether a partial bit assignment can be extended to a complete assignment. Our hybrid implementation solves random leaked-bit factorization problems significantly faster than either a pure SAT or pure computer algebra approach.",
        "published": "2024-06-28T17:30:20Z",
        "link": "http://arxiv.org/abs/2406.20071v1",
        "categories": [
            "cs.CR",
            "cs.DM",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "SHA-256 Collision Attack with Programmatic SAT",
        "authors": [
            "Nahiyan Alamgir",
            "Saeed Nejati",
            "Curtis Bright"
        ],
        "summary": "Cryptographic hash functions play a crucial role in ensuring data security, generating fixed-length hashes from variable-length inputs. The hash function SHA-256 is trusted for data security due to its resilience after over twenty years of intense scrutiny. One of its critical properties is collision resistance, meaning that it is infeasible to find two different inputs with the same hash. Currently, the best SHA-256 collision attacks use differential cryptanalysis to find collisions in simplified versions of SHA-256 that are reduced to have fewer steps, making it feasible to find collisions.   In this paper, we use a satisfiability (SAT) solver as a tool to search for step-reduced SHA-256 collisions, and dynamically guide the solver with the aid of a computer algebra system (CAS) used to detect inconsistencies and deduce information that the solver would otherwise not detect on its own. Our hybrid SAT + CAS solver significantly outperformed a pure SAT approach, enabling us to find collisions in step-reduced SHA-256 with significantly more steps. Using SAT + CAS, we find a 38-step collision of SHA-256 with a modified initialization vector -- something first found by a highly sophisticated search tool of Mendel, Nad, and Schl\\\"affer. Conversely, a pure SAT approach could find collisions for no more than 28 steps. However, our work only uses the SAT solver CaDiCaL and its programmatic interface IPASIR-UP.",
        "published": "2024-06-28T17:30:20Z",
        "link": "http://arxiv.org/abs/2406.20072v1",
        "categories": [
            "cs.CR",
            "cs.DM",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "We-Math: Does Your Large Multimodal Model Achieve Human-like   Mathematical Reasoning?",
        "authors": [
            "Runqi Qiao",
            "Qiuna Tan",
            "Guanting Dong",
            "Minhui Wu",
            "Chong Sun",
            "Xiaoshuai Song",
            "Zhuoma GongQue",
            "Shanglin Lei",
            "Zhe Wei",
            "Miaoxuan Zhang",
            "Runfeng Qiao",
            "Yifan Zhang",
            "Xiao Zong",
            "Yida Xu",
            "Muxi Diao",
            "Zhimin Bao",
            "Chen Li",
            "Honggang Zhang"
        ],
        "summary": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks, such as MathVista and MathVerse, focus more on the result-oriented performance but neglect the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond end-to-end performance. We meticulously collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and five layers of knowledge granularity. We decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM), to hierarchically assess inherent issues in LMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and problem-specific performance. We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategies. More notably, the primary challenge of GPT-4o has significantly transitioned from IK to IG, establishing it as the first LMM advancing towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorization - they correctly solve composite problems involving multiple knowledge concepts yet fail to answer sub-problems. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs. The WE-MATH data and evaluation code are available at https://github.com/We-Math/We-Math.",
        "published": "2024-07-01T13:39:08Z",
        "link": "http://arxiv.org/abs/2407.01284v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Terminating Differentiable Tree Experts",
        "authors": [
            "Jonathan Thomm",
            "Michael Hersche",
            "Giacomo Camposampiero",
            "Aleksandar Terzić",
            "Bernhard Schölkopf",
            "Abbas Rahimi"
        ],
        "summary": "We advance the recently proposed neuro-symbolic Differentiable Tree Machine, which learns tree operations using a combination of transformers and Tensor Product Representations. We investigate the architecture and propose two key components. We first remove a series of different transformer layers that are used in every step by introducing a mixture of experts. This results in a Differentiable Tree Experts model with a constant number of parameters for any arbitrary number of steps in the computation, compared to the previous method in the Differentiable Tree Machine with a linear growth. Given this flexibility in the number of steps, we additionally propose a new termination algorithm to provide the model the power to choose how many steps to make automatically. The resulting Terminating Differentiable Tree Experts model sluggishly learns to predict the number of steps without an oracle. It can do so while maintaining the learning capabilities of the model, converging to the optimal amount of steps.",
        "published": "2024-07-02T08:45:38Z",
        "link": "http://arxiv.org/abs/2407.02060v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Algorithms for Recursive Block Matrices",
        "authors": [
            "Stephen M. Watt"
        ],
        "summary": "We study certain linear algebra algorithms for recursive block matrices. This representation has useful practical and theoretical properties. We summarize some previous results for block matrix inversion and present some results on triangular decomposition of block matrices. The case of inverting matrices over a ring that is neither formally real nor formally complex was inspired by Gonzalez-Vega et al.",
        "published": "2024-07-04T14:50:27Z",
        "link": "http://arxiv.org/abs/2407.03976v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Computing Clipped Products",
        "authors": [
            "Arthur C. Norman",
            "Stephen M. Watt"
        ],
        "summary": "Sometimes only some digits of a numerical product or some terms of a polynomial or series product are required. Frequently these constitute the most significant or least significant part of the value, for example when computing initial values or refinement steps in iterative approximation schemes. Other situations require the middle portion. In this paper we provide algorithms for the general problem of computing a given span of coefficients within a product, that is the terms within a range of degrees for univariate polynomials or range digits of an integer. This generalizes the \"middle product\" concept of Hanrot, Quercia and Zimmerman. We are primarily interested in problems of modest size where constant speed up factors can improve overall system performance, and therefore focus the discussion on classical and Karatsuba multiplication and how methods may be combined.",
        "published": "2024-07-04T19:38:34Z",
        "link": "http://arxiv.org/abs/2407.04133v1",
        "categories": [
            "cs.SC",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "Towards Automated Functional Equation Proving: A Benchmark Dataset and A   Domain-Specific In-Context Agent",
        "authors": [
            "Mahdi Buali",
            "Robert Hoehndorf"
        ],
        "summary": "Automated Theorem Proving (ATP) faces challenges due to its complexity and computational demands. Recent work has explored using Large Language Models (LLMs) for ATP action selection, but these methods can be resource-intensive. This study introduces FEAS, an agent that enhances the COPRA in-context learning framework within Lean. FEAS refines prompt generation, response parsing, and incorporates domain-specific heuristics for functional equations. It introduces FunEq, a curated dataset of functional equation problems with varying difficulty. FEAS outperforms baselines on FunEq, particularly with the integration of domain-specific heuristics. The results demonstrate FEAS's effectiveness in generating and formalizing high-level proof strategies into Lean proofs, showcasing the potential of tailored approaches for specific ATP challenges.",
        "published": "2024-07-05T15:59:16Z",
        "link": "http://arxiv.org/abs/2407.14521v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "Probabilistic Shoenfield Machines",
        "authors": [
            "Maksymilian Bujok",
            "Adam Mata"
        ],
        "summary": "This article provides the theoretical framework of Probabilistic Shoenfield Machines (PSMs), an extension of the classical Shoenfield Machine that models randomness in the computation process. PSMs are brought in contexts where deterministic computation is insufficient, such as randomized algorithms. By allowing transitions to multiple possible states with certain probabilities, PSMs can solve problems and make decisions based on probabilistic outcomes, hence expanding the variety of possible computations. We provide an overview of PSMs, detailing their formal definitions as well as the computation mechanism and their equivalence with Non-deterministic Shoenfield Machines (NSM).",
        "published": "2024-07-08T09:36:11Z",
        "link": "http://arxiv.org/abs/2407.05777v1",
        "categories": [
            "cs.SC",
            "F.4.1"
        ]
    },
    {
        "title": "On the equivalence problem of Smith forms for multivariate polynomial   matrices",
        "authors": [
            "Dong Lu",
            "Dingkang Wang",
            "Fanghui Xiao",
            "Xiaopeng Zheng"
        ],
        "summary": "This paper delves into the equivalence problem of Smith forms for multivariate polynomial matrices. Generally speaking, multivariate ($n \\geq 2$) polynomial matrices and their Smith forms may not be equivalent. However, under certain specific condition, we derive the necessary and sufficient condition for their equivalence. Let $F\\in K[x_1,\\ldots,x_n]^{l\\times m}$ be of rank $r$, $d_r(F)\\in K[x_1]$ be the greatest common divisor of all the $r\\times r$ minors of $F$, where $K$ is a field, $x_1,\\ldots,x_n$ are variables and $1 \\leq r \\leq \\min\\{l,m\\}$. Our key findings reveal the result: $F$ is equivalent to its Smith form if and only if all the $i\\times i$ reduced minors of $F$ generate $K[x_1,\\ldots,x_n]$ for $i=1,\\ldots,r$.",
        "published": "2024-07-09T08:20:44Z",
        "link": "http://arxiv.org/abs/2407.06649v1",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "A Neurosymbolic Approach to Adaptive Feature Extraction in SLAM",
        "authors": [
            "Yasra Chandio",
            "Momin A. Khan",
            "Khotso Selialia",
            "Luis Garcia",
            "Joseph DeGol",
            "Fatima M. Anwar"
        ],
        "summary": "Autonomous robots, autonomous vehicles, and humans wearing mixed-reality headsets require accurate and reliable tracking services for safety-critical applications in dynamically changing real-world environments. However, the existing tracking approaches, such as Simultaneous Localization and Mapping (SLAM), do not adapt well to environmental changes and boundary conditions despite extensive manual tuning. On the other hand, while deep learning-based approaches can better adapt to environmental changes, they typically demand substantial data for training and often lack flexibility in adapting to new domains. To solve this problem, we propose leveraging the neurosymbolic program synthesis approach to construct adaptable SLAM pipelines that integrate the domain knowledge from traditional SLAM approaches while leveraging data to learn complex relationships. While the approach can synthesize end-to-end SLAM pipelines, we focus on synthesizing the feature extraction module. We first devise a domain-specific language (DSL) that can encapsulate domain knowledge on the important attributes for feature extraction and the real-world performance of various feature extractors. Our neurosymbolic architecture then undertakes adaptive feature extraction, optimizing parameters via learning while employing symbolic reasoning to select the most suitable feature extractor. Our evaluations demonstrate that our approach, neurosymbolic Feature EXtraction (nFEX), yields higher-quality features. It also reduces the pose error observed for the state-of-the-art baseline feature extractors ORB and SIFT by up to 90% and up to 66%, respectively, thereby enhancing the system's efficiency and adaptability to novel environments.",
        "published": "2024-07-09T14:18:35Z",
        "link": "http://arxiv.org/abs/2407.06889v3",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.SC"
        ]
    },
    {
        "title": "Hyperion -- A fast, versatile symbolic Gaussian Belief Propagation   framework for Continuous-Time SLAM",
        "authors": [
            "David Hug",
            "Ignacio Alzugaray",
            "Margarita Chli"
        ],
        "summary": "Continuous-Time Simultaneous Localization And Mapping (CTSLAM) has become a promising approach for fusing asynchronous and multi-modal sensor suites. Unlike discrete-time SLAM, which estimates poses discretely, CTSLAM uses continuous-time motion parametrizations, facilitating the integration of a variety of sensors such as rolling-shutter cameras, event cameras and Inertial Measurement Units (IMUs). However, CTSLAM approaches remain computationally demanding and are conventionally posed as centralized Non-Linear Least Squares (NLLS) optimizations. Targeting these limitations, we not only present the fastest SymForce-based [Martiros et al., RSS 2022] B- and Z-Spline implementations achieving speedups between 2.43x and 110.31x over Sommer et al. [CVPR 2020] but also implement a novel continuous-time Gaussian Belief Propagation (GBP) framework, coined Hyperion, which targets decentralized probabilistic inference across agents. We demonstrate the efficacy of our method in motion tracking and localization settings, complemented by empirical ablation studies.",
        "published": "2024-07-09T17:46:53Z",
        "link": "http://arxiv.org/abs/2407.07074v1",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.SC"
        ]
    },
    {
        "title": "Complex reflection groups as differential Galois groups",
        "authors": [
            "Carlos E. Arreche",
            "Avery Bainbridge",
            "Benjamin Obert",
            "Alavi Ullah"
        ],
        "summary": "Complex reflection groups comprise a generalization of Weyl groups of semisimple Lie algebras, and even more generally of finite Coxeter groups. They have been heavily studied since their introduction and complete classification in the 1950s by Shephard and Todd, due to their many applications to combinatorics, representation theory, knot theory, and mathematical physics, to name a few examples. For each given complex reflection group G, we explain a new recipe for producing an integrable system of linear differential equations whose differential Galois group is precisely G. We exhibit these systems explicitly for many (low-rank) irreducible complex reflection groups in the Shephard-Todd classification.",
        "published": "2024-07-11T11:55:52Z",
        "link": "http://arxiv.org/abs/2407.08419v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.CO",
            "20F55 (Primary), 12F12, 34F50 (Secondary)",
            "I.1.2; F.2.1"
        ]
    },
    {
        "title": "Puzzle Ideals for Grassmannians",
        "authors": [
            "Chenqi Mou",
            "Weifeng Shang"
        ],
        "summary": "Puzzles are a versatile combinatorial tool to interpret the Littlewood-Richardson coefficients for Grassmannians. In this paper, we propose the concept of puzzle ideals whose varieties one-one correspond to the tilings of puzzles and present an algebraic framework to construct the puzzle ideals which works with the Knutson-Tao-Woodward puzzle and its $T$-equivariant and $K$-theoretic variants for Grassmannians. For puzzles for which one side is free, we propose the side-free puzzle ideals whose varieties one-one correspond to the tilings of side-free puzzles, and the elimination ideals of the side-free puzzle ideals contain all the information of the structure constants for Grassmannians with respect to the free side.   Besides the underlying algebraic importance of the introduction of these puzzle ideals is the computational feasibility to find all the tilings of the puzzles for Grassmannians by solving the defining polynomial systems, demonstrated with illustrative puzzles via computation of Gr\\\"obner bases.",
        "published": "2024-07-15T17:26:49Z",
        "link": "http://arxiv.org/abs/2407.10927v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.AC",
            "05E14 (Primary) 13F20, 14N15 (Secondary)"
        ]
    },
    {
        "title": "Evaluating Task-Oriented Dialogue Consistency through Constraint   Satisfaction",
        "authors": [
            "Tiziano Labruna",
            "Bernardo Magnini"
        ],
        "summary": "Task-oriented dialogues must maintain consistency both within the dialogue itself, ensuring logical coherence across turns, and with the conversational domain, accurately reflecting external knowledge. We propose to conceptualize dialogue consistency as a Constraint Satisfaction Problem (CSP), wherein variables represent segments of the dialogue referencing the conversational domain, and constraints among variables reflect dialogue properties, including linguistic, conversational, and domain-based aspects. To demonstrate the feasibility of the approach, we utilize a CSP solver to detect inconsistencies in dialogues re-lexicalized by an LLM. Our findings indicate that: (i) CSP is effective to detect dialogue inconsistencies; and (ii) consistent dialogue re-lexicalization is challenging for state-of-the-art LLMs, achieving only a 0.15 accuracy rate when compared to a CSP solver. Furthermore, through an ablation study, we reveal that constraints derived from domain knowledge pose the greatest difficulty in being respected. We argue that CSP captures core properties of dialogue consistency that have been poorly considered by approaches based on component pipelines.",
        "published": "2024-07-16T15:38:41Z",
        "link": "http://arxiv.org/abs/2407.11857v1",
        "categories": [
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "A SageMath Package for Elementary and Sign Vectors with Applications to   Chemical Reaction Networks",
        "authors": [
            "Marcus S. Aichmayr",
            "Stefan Müller",
            "Georg Regensburger"
        ],
        "summary": "We present our SageMath package elementary_vectors for computing elementary and sign vectors of real subspaces. In this setting, elementary vectors are support-minimal vectors that can be determined from maximal minors of a real matrix representing a subspace. By applying the sign function, we obtain the cocircuits of the corresponding oriented matroid, which in turn allow the computation of all sign vectors of a real subspace.   As an application, we discuss sign vector conditions for existence and uniqueness of complex-balanced equilibria of chemical reaction networks with generalized mass-action kinetics. The conditions are formulated in terms of sign vectors of two subspaces arising from the stoichiometric coefficients and the kinetic orders of the reactions. We discuss how these conditions can be checked algorithmically, and we demonstrate the functionality of our package sign_vector_conditions in several examples.",
        "published": "2024-07-17T15:44:53Z",
        "link": "http://arxiv.org/abs/2407.12660v1",
        "categories": [
            "cs.SC",
            "math.DS"
        ]
    },
    {
        "title": "From Words to Worlds: Compositionality for Cognitive Architectures",
        "authors": [
            "Ruchira Dhar",
            "Anders Søgaard"
        ],
        "summary": "Large language models (LLMs) are very performant connectionist systems, but do they exhibit more compositionality? More importantly, is that part of why they perform so well? We present empirical analyses across four LLM families (12 models) and three task categories, including a novel task introduced below. Our findings reveal a nuanced relationship in learning of compositional strategies by LLMs -- while scaling enhances compositional abilities, instruction tuning often has a reverse effect. Such disparity brings forth some open issues regarding the development and improvement of large language models in alignment with human cognitive capacities.",
        "published": "2024-07-18T11:42:13Z",
        "link": "http://arxiv.org/abs/2407.13419v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Error Detection and Constraint Recovery in Hierarchical Multi-Label   Classification without Prior Knowledge",
        "authors": [
            "Joshua Shay Kricheli",
            "Khoa Vo",
            "Aniruddha Datta",
            "Spencer Ozgur",
            "Paulo Shakarian"
        ],
        "summary": "Recent advances in Hierarchical Multi-label Classification (HMC), particularly neurosymbolic-based approaches, have demonstrated improved consistency and accuracy by enforcing constraints on a neural model during training. However, such work assumes the existence of such constraints a-priori. In this paper, we relax this strong assumption and present an approach based on Error Detection Rules (EDR) that allow for learning explainable rules about the failure modes of machine learning models. We show that these rules are not only effective in detecting when a machine learning classifier has made an error but also can be leveraged as constraints for HMC, thereby allowing the recovery of explainable constraints even if they are not provided. We show that our approach is effective in detecting machine learning errors and recovering constraints, is noise tolerant, and can function as a source of knowledge for neurosymbolic models on multiple datasets, including a newly introduced military vehicle recognition dataset.",
        "published": "2024-07-21T15:12:19Z",
        "link": "http://arxiv.org/abs/2407.15192v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Algebraic anti-unification",
        "authors": [
            "Christian Antić"
        ],
        "summary": "Abstraction is key to human and artificial intelligence as it allows one to see common structure in otherwise distinct objects or situations and as such it is a key element for generality in AI. Anti-unification (or generalization) is \\textit{the} part of theoretical computer science and AI studying abstraction. It has been successfully applied to various AI-related problems, most importantly inductive logic programming. Up to this date, anti-unification is studied only from a syntactic perspective in the literature. The purpose of this paper is to initiate an algebraic (i.e. semantic) theory of anti-unification within general algebras. This is motivated by recent applications to similarity and analogical proportions.",
        "published": "2024-07-22T09:49:46Z",
        "link": "http://arxiv.org/abs/2407.15510v1",
        "categories": [
            "cs.AI",
            "cs.DM",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Equality of morphic sequences",
        "authors": [
            "Hans Zantema"
        ],
        "summary": "Morphic sequences form a natural class of infinite sequences, typically defined as the coding of a fixed point of a morphism. Different morphisms and codings may yield the same morphic sequence. This paper investigates how to prove that two such representations of a morphic sequence by morphisms represent the same sequence. In particular, we focus on the smallest representations of the subsequences of the binary Fibonacci sequence obtained by only taking the even or odd elements. The proofs we give are induction proofs of several properties simultaneously, and are typically found fully automatically by a tool that we developed.",
        "published": "2024-07-22T15:25:05Z",
        "link": "http://arxiv.org/abs/2407.15721v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Recent Developments in Real Quantifier Elimination and Cylindrical   Algebraic Decomposition",
        "authors": [
            "Matthew England"
        ],
        "summary": "This extended abstract accompanies an invited talk at CASC 2024, which surveys recent developments in Real Quantifier Elimination (QE) and Cylindrical Algebraic Decomposition (CAD). After introducing these concepts we will first consider adaptations of CAD inspired by computational logic, in particular the algorithms which underpin modern SAT solvers. CAD theory has found use in collaboration with these via the Satisfiability Modulo Theory (SMT) paradigm; while the ideas behind SAT/SMT have led to new algorithms for Real QE. Second we will consider the optimisation of CAD through the use of Machine Learning (ML). The choice of CAD variable ordering has become a key case study for the use of ML to tune algorithms in computer algebra. We will also consider how explainable AI techniques might give insight for improved computer algebra software without any reliance on ML in the final code.",
        "published": "2024-07-29T08:23:58Z",
        "link": "http://arxiv.org/abs/2407.19781v1",
        "categories": [
            "cs.SC",
            "68W30",
            "I.1.2"
        ]
    },
    {
        "title": "Integrability and Linearizability of a Family of Three-Dimensional   Polynomial Systems",
        "authors": [
            "Bo Huang",
            "Ivan Mastev",
            "Valery Romanovski"
        ],
        "summary": "We investigate the local integrability and linearizability of a family of three-dimensional polynomial systems with the matrix of the linear approximation having the eigenvalues $1, \\zeta, \\zeta^2 $, where $\\zeta$ is a primitive cubic root of unity. We establish a criterion for the convergence of the Poincar\\'e--Dulac normal form of the systems and examine the relationship between the normal form and integrability. Additionally, we introduce an efficient algorithm to determine the necessary conditions for the integrability of the systems. This algorithm is then applied to a quadratic subfamily of the systems to analyze its integrability and linearizability. Our findings offer insights into the integrability properties of three-dimensional polynomial systems.",
        "published": "2024-07-30T03:32:56Z",
        "link": "http://arxiv.org/abs/2407.20521v1",
        "categories": [
            "math.DS",
            "cs.SC"
        ]
    },
    {
        "title": "Discovery of Green's function based on symbolic regression with physical   hard constraints",
        "authors": [
            "Jianghang Gu",
            "Mengge Du",
            "Yuntian Chen",
            "Shiyi Chen"
        ],
        "summary": "The Green's function, serving as a kernel function that delineates the interaction relationships of physical quantities within a field, holds significant research implications across various disciplines. It forms the foundational basis for the renowned Biot-Savart formula in fluid dynamics, the theoretical solution of the pressure Poisson equation, and et al. Despite their importance, the theoretical derivation of the Green's function is both time-consuming and labor-intensive. In this study, we employed DISCOVER, an advanced symbolic regression method leveraging symbolic binary trees and reinforcement learning, to identify unknown Green's functions for several elementary partial differential operators, including Laplace operators, Helmholtz operators, and second-order differential operators with jump conditions. The Laplace and Helmholtz operators are particularly vital for resolving the pressure Poisson equation, while second-order differential operators with jump conditions are essential for analyzing multiphase flows and shock waves. By incorporating physical hard constraints, specifically symmetry properties inherent to these self-adjoint operators, we significantly enhanced the performance of the DISCOVER framework, potentially doubling its efficacy. Notably, the Green's functions discovered for the Laplace and Helmholtz operators precisely matched the true Green's functions. Furthermore, for operators without known exact Green's functions, such as the periodic Helmholtz operator and second-order differential operators with jump conditions, we identified potential Green's functions with solution error on the order of 10^(-10). This application of symbolic regression to the discovery of Green's functions represents a pivotal advancement in leveraging artificial intelligence to accelerate scientific discoveries, particularly in fluid dynamics and related fields.",
        "published": "2024-08-01T10:48:01Z",
        "link": "http://arxiv.org/abs/2408.00811v1",
        "categories": [
            "physics.comp-ph",
            "cs.SC",
            "physics.flu-dyn"
        ]
    },
    {
        "title": "Algebraic power series and their automatic complexity II: modulo prime   powers",
        "authors": [
            "Eric Rowland",
            "Reem Yassawi"
        ],
        "summary": "Christol and, independently, Denef and Lipshitz showed that an algebraic sequence of $p$-adic integers (or integers) is $p$-automatic when reduced modulo $p^\\alpha$. Previously, the best known bound on the minimal automaton size for such a sequence was doubly exponential in $\\alpha$. We improve this bound to the order of $p^{\\alpha^3 h d}$, where $h$ and $d$ are the height and degree of the minimal annihilating polynomial modulo $p$. We achieve this bound by showing that all states in the automaton are naturally represented in a new numeration system. This significantly restricts the set of possible states. Since our approach embeds algebraic sequences as diagonals of rational functions, we also obtain bounds more generally for diagonals of multivariate rational functions.",
        "published": "2024-08-01T17:52:24Z",
        "link": "http://arxiv.org/abs/2408.00750v2",
        "categories": [
            "math.NT",
            "cs.FL",
            "cs.SC"
        ]
    },
    {
        "title": "An Abstraction-Preserving Block Matrix Implementation in Maple",
        "authors": [
            "David J. Jeffrey",
            "Stephen M. Watt"
        ],
        "summary": "A Maple implementation of partitioned matrices is described. A recursive block data structure is used, with all operations preserving the block abstraction. These include constructor functions, ring operations such as addition and product, and inversion. The package is demonstrated by calculating the PLU factorization of a block matrix.",
        "published": "2024-08-04T18:51:41Z",
        "link": "http://arxiv.org/abs/2408.02112v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Rational Curves on Real Classical Groups",
        "authors": [
            "Zijia Li",
            "Ke Ye"
        ],
        "summary": "This paper is concerned with rational curves on real classical groups. Our contributions are three-fold: (i) We determine the structure of quadratic rational curves on real classical groups. As a consequence, we completely classify quadratic rational curves on $\\mathrm{U}_n$, $\\mathrm{O}_n(\\mathbb{R})$, $\\mathrm{O}_{n-1,1}(\\mathbb{R})$ and $\\mathrm{O}_{n-2,2}(\\mathbb{R})$. (ii) We prove a decomposition theorem for rational curves on real classical groups, which can be regarded as a non-commutative generalization of the fundamental theorem of algebra and partial fraction decomposition. (iii) As an application of (i) and (ii), we generalize Kempe's Universality Theorem to rational curves on homogeneous spaces.",
        "published": "2024-08-08T13:27:42Z",
        "link": "http://arxiv.org/abs/2408.04453v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.GR",
            "14H45, 20G20, 26C15, 14L35, 14L30, 70B05"
        ]
    },
    {
        "title": "First steps towards Computational Polynomials in Lean",
        "authors": [
            "James Harold Davenport"
        ],
        "summary": "The proof assistant Lean has support for abstract polynomials, but this is not necessarily the same as support for computations with polynomials. Lean is also a functional programming language, so it should be possible to implement computational polynomials in Lean. It turns out not to be as easy as the naive author thought.",
        "published": "2024-08-08T16:22:48Z",
        "link": "http://arxiv.org/abs/2408.04564v2",
        "categories": [
            "cs.SC",
            "68W30",
            "G.4"
        ]
    },
    {
        "title": "MathPartner: An Artificial Intelligence Cloud Service",
        "authors": [
            "Gennadi Malaschonok",
            "Alexandr Seliverstov"
        ],
        "summary": "In a broad sense, artificial intelligence is a service to find a solution to complex intellectual problems. In this sense, the MathPartner service provides artificial intelligence that allows us to formulate questions and receive answers to questions formulated in a mathematical language. For mathematicians and physicists today, such a language is \\LaTeX. The MathPartner service uses a dialect of \\LaTeX, which is called Mathpar. The service is a cloud-based computer algebra system and provides users with the opportunity to solve many mathematical problems. In this publication, we focus only on a small class of extremum problems, which are widely applied in economics, management, logistics, and in many engineering fields. In particular, we consider the shortest path problem and discuss an algorithm that is based on the tropical mathematics. The ability to work with many types of classical and tropical algebras, which are freely available to users, is an important distinguishing feature of this intelligent tool for symbolic-numerical calculations. We also consider the use of the simplex algorithm for solving optimization problems.",
        "published": "2024-08-09T11:19:01Z",
        "link": "http://arxiv.org/abs/2408.04999v1",
        "categories": [
            "cs.SC",
            "68T01",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Nonlinear Propagation of Non-Gaussian Uncertainties",
        "authors": [
            "Giacomo Acciarini",
            "Nicola Baresi",
            "David Lloyd",
            "Dario Izzo"
        ],
        "summary": "This paper presents a novel approach for propagating uncertainties in dynamical systems building on high-order Taylor expansions of the flow and moment-generating functions (MGFs). Unlike prior methods that focus on Gaussian distributions, our approach leverages the relationship between MGFs and distribution moments to extend high-order uncertainty propagation techniques to non-Gaussian scenarios. This significantly broadens the applicability of these methods to a wider range of problems and uncertainty types. High-order moment computations are performed one-off and symbolically, reducing the computational burden of the technique to the calculation of Taylor series coefficients around a nominal trajectory, achieved by efficiently integrating the system's variational equations. Furthermore, the use of the proposed approach in combination with event transition tensors, allows for accurate propagation of uncertainties at specific events, such as the landing surface of a celestial body, the crossing of a predefined Poincar\\'e section, or the trigger of an arbitrary event during the propagation. Via numerical simulations we demonstrate the effectiveness of our method in various astrodynamics applications, including the unperturbed and perturbed two-body problem, and the circular restricted three-body problem, showing that it accurately propagates non-Gaussian uncertainties both at future times and at event manifolds.",
        "published": "2024-08-09T23:46:01Z",
        "link": "http://arxiv.org/abs/2408.05384v2",
        "categories": [
            "physics.space-ph",
            "cs.SC",
            "math.PR",
            "nlin.CD"
        ]
    },
    {
        "title": "Challenges for analytic calculations of the massive three-loop form   factors",
        "authors": [
            "J Blümlein",
            "A. De Freitas",
            "P. Marquard",
            "C. Schneider"
        ],
        "summary": "The calculation of massive three-loop QCD form factors using in particular the large moments method has been successfully applied to quarkonic contributions in [1]. We give a brief review of the different steps of the calculation and report on improvements of our methods that enabled us to push forward the calculations of the gluonic contributions to the form factors.",
        "published": "2024-08-13T17:21:00Z",
        "link": "http://arxiv.org/abs/2408.07046v1",
        "categories": [
            "hep-ph",
            "cs.SC"
        ]
    },
    {
        "title": "Algebraic Representations for Faster Predictions in Convolutional Neural   Networks",
        "authors": [
            "Johnny Joyce",
            "Jan Verschelde"
        ],
        "summary": "Convolutional neural networks (CNNs) are a popular choice of model for tasks in computer vision. When CNNs are made with many layers, resulting in a deep neural network, skip connections may be added to create an easier gradient optimization problem while retaining model expressiveness. In this paper, we show that arbitrarily complex, trained, linear CNNs with skip connections can be simplified into a single-layer model, resulting in greatly reduced computational requirements during prediction time. We also present a method for training nonlinear models with skip connections that are gradually removed throughout training, giving the benefits of skip connections without requiring computational overhead during during prediction time. These results are demonstrated with practical examples on Residual Networks (ResNet) architecture.",
        "published": "2024-08-14T21:10:05Z",
        "link": "http://arxiv.org/abs/2408.07815v1",
        "categories": [
            "cs.CV",
            "cs.SC"
        ]
    },
    {
        "title": "ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction   Based on Large Language Model",
        "authors": [
            "Xuanqing Yu",
            "Wangtao Sun",
            "Jingwei Li",
            "Kang Liu",
            "Chengbao Liu",
            "Jie Tan"
        ],
        "summary": "In the realm of event prediction, temporal knowledge graph forecasting (TKGF) stands as a pivotal technique. Previous approaches face the challenges of not utilizing experience during testing and relying on a single short-term history, which limits adaptation to evolving data. In this paper, we introduce the Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by integrating dynamic causal rule mining (DCRM) and dual history augmented generation (DHAG). DCRM dynamically constructs causal rules from real-time data, allowing for swift adaptation to new causal relationships. In parallel, DHAG merges short-term and long-term historical contexts, leveraging a bi-branch approach to enrich event prediction. Our framework demonstrates notable performance enhancements across diverse datasets, with significant Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language models (LLMs) for event prediction without necessitating extensive retraining. The ONSEP framework not only advances the field of TKGF but also underscores the potential of neural-symbolic approaches in adapting to dynamic data environments.",
        "published": "2024-08-14T22:28:19Z",
        "link": "http://arxiv.org/abs/2408.07840v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large   Language Models for Manufacturing Decision-making",
        "authors": [
            "Siyu Wu",
            "Alessandro Oltramari",
            "Jonathan Francis",
            "C. Lee Giles",
            "Frank E. Ritter"
        ],
        "summary": "Resolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models (LLMs) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing LLMs with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use LLMs for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference -- reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce LLM-ACTR, a novel neuro-symbolic architecture that provides human-aligned and versatile decision-making by integrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts and embeds knowledge of ACT-R's internal decision-making process as latent neural representations, injects this information into trainable LLM adapter layers, and fine-tunes the LLMs for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.",
        "published": "2024-08-17T11:49:53Z",
        "link": "http://arxiv.org/abs/2408.09176v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "Active learning of digenic functions with boolean matrix logic   programming",
        "authors": [
            "Lun Ai",
            "Stephen H. Muggleton",
            "Shi-shun Liang",
            "Geoff S. Baldwin"
        ],
        "summary": "We apply logic-based machine learning techniques to facilitate cellular engineering and drive biological discovery, based on comprehensive databases of metabolic processes called genome-scale metabolic network models (GEMs). Predicted host behaviours are not always correctly described by GEMs. Learning the intricate genetic interactions within GEMs presents computational and empirical challenges. To address these, we describe a novel approach called Boolean Matrix Logic Programming (BMLP) by leveraging boolean matrices to evaluate large logic programs. We introduce a new system, $BMLP_{active}$, which efficiently explores the genomic hypothesis space by guiding informative experimentation through active learning. In contrast to sub-symbolic methods, $BMLP_{active}$ encodes a state-of-the-art GEM of a widely accepted bacterial host in an interpretable and logical representation using datalog logic programs. Notably, $BMLP_{active}$ can successfully learn the interaction between a gene pair with fewer training examples than random experimentation, overcoming the increase in experimental design space. $BMLP_{active}$ enables rapid optimisation of metabolic models and offers a realistic approach to a self-driving lab for microbial engineering.",
        "published": "2024-08-19T18:47:07Z",
        "link": "http://arxiv.org/abs/2408.14487v3",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.SC",
            "q-bio.MN"
        ]
    },
    {
        "title": "Boolean Matrix Logic Programming",
        "authors": [
            "Lun Ai",
            "Stephen H. Muggleton"
        ],
        "summary": "We describe a datalog query evaluation approach based on efficient and composable boolean matrix manipulation modules. We first define an overarching problem, Boolean Matrix Logic Programming (BMLP), which uses boolean matrices as an alternative computation to evaluate datalog programs. We develop two novel BMLP modules for bottom-up inferences on linear dyadic recursive datalog programs, and show how additional modules can extend this capability to compute both linear and non-linear recursive datalog programs of arity two. Our empirical results demonstrate that these modules outperform general-purpose and specialised systems by factors of 30x and 9x, respectively, when evaluating large programs with millions of facts. This boolean matrix approach significantly enhances the efficiency of datalog querying to support logic programming techniques.",
        "published": "2024-08-19T19:26:49Z",
        "link": "http://arxiv.org/abs/2408.10369v2",
        "categories": [
            "cs.SC",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "New Results on Periodic Golay Pairs",
        "authors": [
            "Tyler Lumsden",
            "Ilias Kotsireas",
            "Curtis Bright"
        ],
        "summary": "In this paper, we provide algorithmic methods for conducting exhaustive searches for periodic Golay pairs. Our methods enumerate several lengths beyond the currently known state-of-the-art available searches: we conducted exhaustive searches for periodic Golay pairs of all lengths $v \\leq 72$ using our methods, while only lengths $v \\leq 34$ had previously been exhaustively enumerated. Our methods are applicable to periodic complementary sequences in general. We utilize sequence compression, a method of sequence generation derived in 2013 by Djokovi\\'c and Kotsireas. We also introduce and implement a new method of \"multi-level\" compression, where sequences are uncompressed in several steps. This method allowed us to exhaustively search all lengths $v \\leq 72$ using less than 10 CPU years. For cases of complementary sequences where uncompression is not possible, we introduce some new methods of sequence generation inspired by the isomorph-free exhaustive generation algorithm of orderly generation. Finally, we pose a conjecture regarding the structure of periodic Golay pairs and prove it holds in many lengths, including all lengths $v \\lt 100$. We demonstrate the usefulness of our algorithms by providing the first ever examples of periodic Golay pairs of length $v = 90$. The smallest length for which the existence of periodic Golay pairs is undecided is now $106$.",
        "published": "2024-08-28T08:06:16Z",
        "link": "http://arxiv.org/abs/2408.15611v2",
        "categories": [
            "cs.DM",
            "cs.SC",
            "math.CO",
            "11B83, 05B20, 94A55, 68W30"
        ]
    },
    {
        "title": "Comprehensive Systems for Primary Decompositions of Parametric Ideals",
        "authors": [
            "Yuki Ishihara",
            "Kazuhiro Yokoyama"
        ],
        "summary": "We present an effective method for computing parametric primary decomposition via comprehensive Gr\\\"obner systems. In general, it is very difficult to compute a parametric primary decomposition of a given ideal in the polynomial ring with rational coefficients $\\mathbb{Q}[A,X]$ where $A$ is the set of parameters and $X$ is the set of ordinary variables. One cause of the difficulty is related to the irreducibility of the specialized polynomial. Thus, we introduce a new notion of ``feasibility'' on the stability of the structure of the ideal in terms of its primary decomposition, and we give a new algorithm for computing a so-called comprehensive system consisting of pairs $(C, \\mathcal{Q})$, where for each parameter value in $C$, the ideal has the stable decomposition $\\mathcal{Q}$. We may call this comprehensive system a parametric primary decomposition of the ideal. Also, one can also compute a dense set $\\mathcal{O}$ such that $\\varphi_\\alpha(\\mathcal{Q})$ is a primary decomposition for any $\\alpha\\in C\\cap \\mathcal{O}$ via irreducible polynomials. In addition, we give several computational examples to examine the effectiveness of our new decomposition.",
        "published": "2024-08-28T16:31:26Z",
        "link": "http://arxiv.org/abs/2408.15917v1",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Role of Data-driven Regional Growth Model in Shaping Brain Folding   Patterns",
        "authors": [
            "Jixin Hou",
            "Zhengwang Wu",
            "Xianyan Chen",
            "Li Wang",
            "Dajiang Zhu",
            "Tianming Liu",
            "Gang Li",
            "Xianqiao Wang"
        ],
        "summary": "The surface morphology of the developing mammalian brain is crucial for understanding brain function and dysfunction. Computational modeling offers valuable insights into the underlying mechanisms for early brain folding. Recent findings indicate significant regional variations in brain tissue growth, while the role of these variations in cortical development remains unclear. In this study, we unprecedently explored how regional cortical growth affects brain folding patterns using computational simulation. We first developed growth models for typical cortical regions using machine learning (ML)-assisted symbolic regression, based on longitudinal real surface expansion and cortical thickness data from prenatal and infant brains derived from over 1,000 MRI scans of 735 pediatric subjects with ages ranging from 29 post-menstrual weeks to 24 months. These models were subsequently integrated into computational software to simulate cortical development with anatomically realistic geometric models. We comprehensively quantified the resulting folding patterns using multiple metrics such as mean curvature, sulcal depth, and gyrification index. Our results demonstrate that regional growth models generate complex brain folding patterns that more closely match actual brains structures, both quantitatively and qualitatively, compared to conventional uniform growth models. Growth magnitude plays a dominant role in shaping folding patterns, while growth trajectory has a minor influence. Moreover, multi-region models better capture the intricacies of brain folding than single-region models. Our results underscore the necessity and importance of incorporating regional growth heterogeneity into brain folding simulations, which could enhance early diagnosis and treatment of cortical malformations and neurodevelopmental disorders such as cerebral palsy and autism.",
        "published": "2024-08-30T14:49:10Z",
        "link": "http://arxiv.org/abs/2408.17334v2",
        "categories": [
            "q-bio.NC",
            "cs.CE",
            "cs.SC",
            "q-bio.TO"
        ]
    },
    {
        "title": "Active Symbolic Discovery of Ordinary Differential Equations via Phase   Portrait Sketching",
        "authors": [
            "Nan Jiang",
            "Md Nasim",
            "Yexiang Xue"
        ],
        "summary": "Discovering Ordinary Differential Equations (ODEs) from trajectory data is a crucial task in AI-driven scientific discovery. Recent methods for symbolic discovery of ODEs primarily rely on fixed training datasets collected a-priori, often leading to suboptimal performance, as observed in our experiments in Figure 1. Inspired by active learning, we explore methods for querying informative trajectory data to evaluate predicted ODEs, where data are obtained by the specified initial conditions of the trajectory. Chaos theory indicates that small changes in the initial conditions of a dynamical system can result in vastly different trajectories, necessitating the maintenance of a large set of initial conditions of the trajectory. To address this challenge, we introduce Active Symbolic Discovery of Ordinary Differential Equations via Phase Portrait Sketching (APPS). Instead of directly selecting individual initial conditions, APPS first identifies an informative region and samples a batch of initial conditions within that region. Compared to traditional active learning methods, APPS eliminates the need for maintaining a large amount of data. Extensive experiments demonstrate that APPS consistently discovers more accurate ODE expressions than baseline methods using passively collected datasets.",
        "published": "2024-09-02T18:24:39Z",
        "link": "http://arxiv.org/abs/2409.01416v1",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "OGRePy: An Object-Oriented General Relativity Package for Python",
        "authors": [
            "Barak Shoshany"
        ],
        "summary": "We present OGRePy, the official Python port of the popular Mathematica tensor calculus package OGRe (Object-Oriented General Relativity) - a powerful, yet user-friendly, tool for advanced tensor calculations in mathematics and physics, especially suitable for general relativity. The Python port uses the same robust and performance-oriented algorithms as the original package, and retains its core design principles. However, its truly object-oriented interface, enabled by Python, is more intuitive and flexible than the original Mathematica implementation. It utilizes SymPy for symbolic computations and Jupyter as a notebook interface. OGRePy allows calculating arbitrary tensor formulas using any combination of addition, multiplication by scalar, trace, contraction, partial derivative, covariant derivative, and permutation of indices. Transformations of the tensor components between different index configurations and/or coordinate systems are performed seamlessly behind the scenes as needed, eliminating user error due to combining incompatible representations, and guaranteeing consistent results. In addition, the package provides facilities for easily calculating various curvature tensors and geodesic equations in multiple representations. This paper presents the main features of the package in great detail, including many examples of its use in the context of general relativity research.",
        "published": "2024-09-05T03:40:27Z",
        "link": "http://arxiv.org/abs/2409.03803v1",
        "categories": [
            "gr-qc",
            "cs.MS",
            "cs.SC",
            "math.DG",
            "G.4; I.1; J.2"
        ]
    },
    {
        "title": "Symbolic Regression with a Learned Concept Library",
        "authors": [
            "Arya Grayeli",
            "Atharva Sehgal",
            "Omar Costilla-Reyes",
            "Miles Cranmer",
            "Swarat Chaudhuri"
        ],
        "summary": "We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered, hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LaSR can be used to discover a novel and powerful scaling law for LLMs.",
        "published": "2024-09-14T08:17:30Z",
        "link": "http://arxiv.org/abs/2409.09359v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Towards Verified Polynomial Factorisation",
        "authors": [
            "James H. Davenport"
        ],
        "summary": "Computer algebra systems are really good at factoring polynomials, i.e. writing f as a product of irreducible factors. It is relatively easy to verify that we have a factorisation, but verifying that these factors are irreducible is a much harder problem. This paper reports work-in-progress to do such verification in Lean.",
        "published": "2024-09-14T21:08:19Z",
        "link": "http://arxiv.org/abs/2409.09533v1",
        "categories": [
            "cs.SC",
            "68W30",
            "G.4"
        ]
    },
    {
        "title": "Introducing Quantification into a Hierarchical Graph Rewriting Language",
        "authors": [
            "Haruto Mishina",
            "Kazunori Ueda"
        ],
        "summary": "LMNtal is a programming and modeling language based on hierarchical graph rewriting that uses logical variables to represent connectivity and membranes to represent hierarchy. On the theoretical side, it allows logical interpretation based on intuitionistic linear logic; on the practical side, its full-fledged implementation supports a graph-based parallel model checker and has been used to model diverse applications including various computational models. This paper discuss how we extend LMNtal to QLMNtal (LMNtal with Quantification) to further enhance the usefulness of hierarchical graph rewriting for high-level modeling by introducing quantifiers into rewriting as well as matching. Those quantifiers allows us to express universal quantification, cardinality and non-existence in an integrated manner. Unlike other attempts to introduce quantifiers into graph rewriting, QLMNtal has term-based syntax, whose semantics is smoothly integrated into the small-step semantics of the base language LMNtal. The proposed constructs allow combined and nested use of quantifiers within individual rewrite rules.",
        "published": "2024-09-17T09:25:42Z",
        "link": "http://arxiv.org/abs/2409.11015v1",
        "categories": [
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "Fast Symbolic Integer-Linear Spectra",
        "authors": [
            "Jonny Luntzel",
            "Abraham Miller"
        ],
        "summary": "Here we contribute a fast symbolic eigenvalue solver for matrices whose eigenvalues are $\\mathbb{Z}$-linear combinations of their entries, alongside efficient general and stochastic $M^{X}$ generators. Users can interact with a few degrees of freedom to create linear operators, making high-dimensional symbolic analysis feasible for when numerical analyses are insufficient.",
        "published": "2024-09-18T06:59:39Z",
        "link": "http://arxiv.org/abs/2410.09053v2",
        "categories": [
            "math.RA",
            "cs.MS",
            "cs.NA",
            "cs.SC",
            "math.NA"
        ]
    },
    {
        "title": "Synthesizing Evolving Symbolic Representations for Autonomous Systems",
        "authors": [
            "Gabriele Sartor",
            "Angelo Oddi",
            "Riccardo Rasconi",
            "Vieri Giuliano Santucci",
            "Rosa Meo"
        ],
        "summary": "Recently, AI systems have made remarkable progress in various tasks. Deep Reinforcement Learning(DRL) is an effective tool for agents to learn policies in low-level state spaces to solve highly complex tasks. Researchers have introduced Intrinsic Motivation(IM) to the RL mechanism, which simulates the agent's curiosity, encouraging agents to explore interesting areas of the environment. This new feature has proved vital in enabling agents to learn policies without being given specific goals. However, even though DRL intelligence emerges through a sub-symbolic model, there is still a need for a sort of abstraction to understand the knowledge collected by the agent. To this end, the classical planning formalism has been used in recent research to explicitly represent the knowledge an autonomous agent acquires and effectively reach extrinsic goals. Despite classical planning usually presents limited expressive capabilities, PPDDL demonstrated usefulness in reviewing the knowledge gathered by an autonomous system, making explicit causal correlations, and can be exploited to find a plan to reach any state the agent faces during its experience. This work presents a new architecture implementing an open-ended learning system able to synthesize from scratch its experience into a PPDDL representation and update it over time. Without a predefined set of goals and tasks, the system integrates intrinsic motivations to explore the environment in a self-directed way, exploiting the high-level knowledge acquired during its experience. The system explores the environment and iteratively: (a) discover options, (b) explore the environment using options, (c) abstract the knowledge collected and (d) plan. This paper proposes an alternative approach to implementing open-ended learning architectures exploiting low-level and high-level representations to extend its knowledge in a virtuous loop.",
        "published": "2024-09-18T07:23:26Z",
        "link": "http://arxiv.org/abs/2409.11756v1",
        "categories": [
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "A Generalization of Habicht's Theorem for Subresultants of Several   Univariate Polynomials",
        "authors": [
            "Hoon Hong",
            "Jiaqi Meng",
            "Jing Yang"
        ],
        "summary": "Subresultants of two univariate polynomials are one of the most classic and ubiquitous objects in computational algebra and algebraic geometry. In 1948, Habicht discovered and proved interesting relationships among subresultants. Those relationships were found to be useful for both structural understanding and efficient computation. Often one needs to consider several (possibly more than two) polynomials. It is rather straightforward to generalize the notion of subresultants to several polynomials. However, it is not obvious (in fact, quite challenging) to generalize the Habicht's result to several polynomials. The main contribution of this paper is to provide such a generalization.",
        "published": "2024-09-19T12:51:56Z",
        "link": "http://arxiv.org/abs/2409.12727v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "A Syzygial Method for Equidimensional Decomposition",
        "authors": [
            "Rafael Mohr"
        ],
        "summary": "Based on a theorem by Vasconcelos, we give an algorithm for equidimensional decomposition of algebraic sets using syzygy computations via Gr\\\"obner bases. This algorithm avoids the use of elimination, homological algebra and processing the input equations one-by-one present in previous algorithms. We experimentally demonstrate the practical interest of our algorithm compared to the state of the art.",
        "published": "2024-09-26T12:24:54Z",
        "link": "http://arxiv.org/abs/2409.17785v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Proceedings 13th International Workshop on Developments in Computational   Models",
        "authors": [
            "Sandra Alves",
            "Ian Mackie"
        ],
        "summary": "This volume contains the proceedings of DCM 2023, the 13th International Workshop on Developments in Computational Models held on 2 July 2023 in Rome, Italy. DCM 2023 was organised as a one-day satellite event of FSCD 2023, the 8th International Conference on Formal Structures for Computation and Deduction. The aim of this workshop is to bring together researchers who are currently developing new computation models or new features for traditional computation models, in order to foster their interaction, to provide a forum for presenting new ideas and work in progress, and to enable newcomers to learn about current activities in this area.",
        "published": "2024-09-28T09:53:44Z",
        "link": "http://arxiv.org/abs/2409.19298v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "AlphaIntegrator: Transformer Action Search for Symbolic Integration   Proofs",
        "authors": [
            "Mert Ünsal",
            "Timon Gehr",
            "Martin Vechev"
        ],
        "summary": "We present the first correct-by-construction learning-based system for step-by-step mathematical integration. The key idea is to learn a policy, represented by a GPT transformer model, which guides the search for the right mathematical integration rule, to be carried out by a symbolic solver. Concretely, we introduce a symbolic engine with axiomatically correct actions on mathematical expressions, as well as the first dataset for step-by-step integration. Our GPT-style transformer model, trained on this synthetic data, demonstrates strong generalization by surpassing its own data generator in accuracy and efficiency, using 50% fewer search steps. Our experimental results with SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs on a set of question-answer pairs is insufficient for solving this mathematical task. This motivates the importance of discovering creative methods for combining LLMs with symbolic reasoning engines, of which our work is an instance.",
        "published": "2024-10-03T16:50:30Z",
        "link": "http://arxiv.org/abs/2410.02666v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Learning to Solve Abstract Reasoning Problems with Neurosymbolic Program   Synthesis and Task Generation",
        "authors": [
            "Jakub Bednarek",
            "Krzysztof Krawiec"
        ],
        "summary": "The ability to think abstractly and reason by analogy is a prerequisite to rapidly adapt to new conditions, tackle newly encountered problems by decomposing them, and synthesize knowledge to solve problems comprehensively. We present TransCoder, a method for solving abstract problems based on neural program synthesis, and conduct a comprehensive analysis of decisions made by the generative module of the proposed architecture. At the core of TransCoder is a typed domain-specific language, designed to facilitate feature engineering and abstract reasoning. In training, we use the programs that failed to solve tasks to generate new tasks and gather them in a synthetic dataset. As each synthetic task created in this way has a known associated program (solution), the model is trained on them in supervised mode. Solutions are represented in a transparent programmatic form, which can be inspected and verified. We demonstrate TransCoder's performance using the Abstract Reasoning Corpus dataset, for which our framework generates tens of thousands of synthetic problems with corresponding solutions and facilitates systematic progress in learning.",
        "published": "2024-10-06T13:42:53Z",
        "link": "http://arxiv.org/abs/2410.04480v1",
        "categories": [
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Jet Expansions of Residual Computation",
        "authors": [
            "Yihong Chen",
            "Xiangxiang Xu",
            "Yao Lu",
            "Pontus Stenetorp",
            "Luca Franceschi"
        ],
        "summary": "We introduce a framework for expanding residual computational graphs using jets, operators that generalize truncated Taylor series. Our method provides a systematic approach to disentangle contributions of different computational paths to model predictions. In contrast to existing techniques such as distillation, probing, or early decoding, our expansions rely solely on the model itself and requires no data, training, or sampling from the model. We demonstrate how our framework grounds and subsumes logit lens, reveals a (super-)exponential path structure in the recursive residual depth and opens up several applications. These include sketching a transformer large language model with $n$-gram statistics extracted from its computations, and indexing the models' levels of toxicity knowledge. Our approach enables data-free analysis of residual computation for model interpretability, development, and evaluation.",
        "published": "2024-10-08T13:25:08Z",
        "link": "http://arxiv.org/abs/2410.06024v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "Generalized Fixed-Depth Prefix and Postfix Symbolic Regression Grammars",
        "authors": [
            "Edward Finkelstein"
        ],
        "summary": "We develop faultless, fixed-depth, string-based, prefix and postfix symbolic regression grammars, capable of producing \\emph{any} expression from a set of operands, unary operators and/or binary operators. Using these grammars, we outline simplified forms of 5 popular heuristic search strategies: Brute Force Search, Monte Carlo Tree Search, Particle Swarm Optimization, Genetic Programming, and Simulated Annealing. For each algorithm, we compare the relative performance of prefix vs postfix for ten ground-truth expressions implemented entirely within a common C++/Eigen framework. Our experiments show a comparatively strong correlation between the average number of nodes per layer of the ground truth expression tree and the relative performance of prefix vs postfix. The fixed-depth grammars developed herein can enhance scientific discovery by increasing the efficiency of symbolic regression, enabling faster identification of accurate mathematical models across various disciplines.",
        "published": "2024-10-10T17:21:32Z",
        "link": "http://arxiv.org/abs/2410.08137v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "UniGlyph: A Seven-Segment Script for Universal Language Representation",
        "authors": [
            "G. V. Bency Sherin",
            "A. Abijesh Euphrine",
            "A. Lenora Moreen",
            "L. Arun Jose"
        ],
        "summary": "UniGlyph is a constructed language (conlang) designed to create a universal transliteration system using a script derived from seven-segment characters. The goal of UniGlyph is to facilitate cross-language communication by offering a flexible and consistent script that can represent a wide range of phonetic sounds. This paper explores the design of UniGlyph, detailing its script structure, phonetic mapping, and transliteration rules. The system addresses imperfections in the International Phonetic Alphabet (IPA) and traditional character sets by providing a compact, versatile method to represent phonetic diversity across languages. With pitch and length markers, UniGlyph ensures accurate phonetic representation while maintaining a small character set. Applications of UniGlyph include artificial intelligence integrations, such as natural language processing and multilingual speech recognition, enhancing communication across different languages. Future expansions are discussed, including the addition of animal phonetic sounds, where unique scripts are assigned to different species, broadening the scope of UniGlyph beyond human communication. This study presents the challenges and solutions in developing such a universal script, demonstrating the potential of UniGlyph to bridge linguistic gaps in cross-language communication, educational phonetics, and AI-driven applications.",
        "published": "2024-10-11T16:46:09Z",
        "link": "http://arxiv.org/abs/2410.08974v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "cs.SC",
            "cs.SD",
            "eess.AS",
            "68T50, 68T01",
            "H.5.2; I.2.7"
        ]
    },
    {
        "title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical   Reasoning",
        "authors": [
            "Joshua Ong Jun Leang",
            "Aryo Pradipta Gema",
            "Shay B. Cohen"
        ],
        "summary": "Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",
        "published": "2024-10-14T09:48:41Z",
        "link": "http://arxiv.org/abs/2410.10336v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Neural Symbolic Regression of Complex Network Dynamics",
        "authors": [
            "Haiquan Qiu",
            "Shuzhi Liu",
            "Quanming Yao"
        ],
        "summary": "Complex networks describe important structures in nature and society, composed of nodes and the edges that connect them. The evolution of these networks is typically described by dynamics, which are labor-intensive and require expert knowledge to derive. However, because the complex network involves noisy observations from multiple trajectories of nodes, existing symbolic regression methods are either not applicable or ineffective on its dynamics. In this paper, we propose Physically Inspired Neural Dynamics Symbolic Regression (PI-NDSR), a method based on neural networks and genetic programming to automatically learn the symbolic expression of dynamics. Our method consists of two key components: a Physically Inspired Neural Dynamics (PIND) to augment and denoise trajectories through observed trajectory interpolation; and a coordinated genetic search algorithm to derive symbolic expressions. This algorithm leverages references of node dynamics and edge dynamics from neural dynamics to avoid overfitted expressions in symbolic space. We evaluate our method on synthetic datasets generated by various dynamics and real datasets on disease spreading. The results demonstrate that PI-NDSR outperforms the existing method in terms of both recovery probability and error.",
        "published": "2024-10-15T02:02:30Z",
        "link": "http://arxiv.org/abs/2410.11185v1",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Algorithmic reduction of polynomially nonlinear PDE systems to   parametric ODE systems",
        "authors": [
            "Siyuan Deng",
            "Michelle Hatzel",
            "Gregory Reid",
            "Wenqiang Yang",
            "Wenyuan Wu"
        ],
        "summary": "Differential-elimination algorithms apply a finite number of differentiations and eliminations to systems of partial differential equations. For systems that are polynomially nonlinear with rational number coefficients, they guarantee the inclusion of missing integrability conditions and the statement of of existence and uniqueness theorems for local analytic solutions of such systems. Further, they are useful in obtaining systems in a form more amenable to exact and approximate solution methods.   Maple's \\maple{dsolve} and \\maple{pdsolve} algorithms for solving PDE and ODE often automatically call such routines during applications. Indeed, even casual users of Maple's dsolve and pdsolve commands have probably unknowingly used Maple's differential-elimination algorithms.   Suppose that a system of PDE has been reduced by differential-elimination to a system whose automatic existence and uniqueness algorithm has been determined to be finite-dimensional. We present an algorithm for rewriting the output as a system of parameterized ODE. Exact methods and numerical methods for solving ODE and DAE can be applied to this form.",
        "published": "2024-10-15T23:17:39Z",
        "link": "http://arxiv.org/abs/2410.12110v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "A Remedy to Compute-in-Memory with Dynamic Random Access Memory:   1FeFET-1C Technology for Neuro-Symbolic AI",
        "authors": [
            "Xunzhao Yin",
            "Hamza Errahmouni Barkam",
            "Franz Müller",
            "Yuxiao Jiang",
            "Mohsen Imani",
            "Sukhrob Abdulazhanov",
            "Alptekin Vardar",
            "Nellie Laleni",
            "Zijian Zhao",
            "Jiahui Duan",
            "Zhiguo Shi",
            "Siddharth Joshi",
            "Michael Niemier",
            "Xiaobo Sharon Hu",
            "Cheng Zhuo",
            "Thomas Kämpfe",
            "Kai Ni"
        ],
        "summary": "Neuro-symbolic artificial intelligence (AI) excels at learning from noisy and generalized patterns, conducting logical inferences, and providing interpretable reasoning. Comprising a 'neuro' component for feature extraction and a 'symbolic' component for decision-making, neuro-symbolic AI has yet to fully benefit from efficient hardware accelerators. Additionally, current hardware struggles to accommodate applications requiring dynamic resource allocation between these two components. To address these challenges-and mitigate the typical data-transfer bottleneck of classical Von Neumann architectures-we propose a ferroelectric charge-domain compute-in-memory (CiM) array as the foundational processing element for neuro-symbolic AI. This array seamlessly handles both the critical multiply-accumulate (MAC) operations of the 'neuro' workload and the parallel associative search operations of the 'symbolic' workload. To enable this approach, we introduce an innovative 1FeFET-1C cell, combining a ferroelectric field-effect transistor (FeFET) with a capacitor. This design, overcomes the destructive sensing limitations of DRAM in CiM applications, while capable of capitalizing decades of DRAM expertise with a similar cell structure as DRAM, achieves high immunity against FeFET variation-crucial for neuro-symbolic AI-and demonstrates superior energy efficiency. The functionalities of our design have been successfully validated through SPICE simulations and prototype fabrication and testing. Our hardware platform has been benchmarked in executing typical neuro-symbolic AI reasoning tasks, showing over 2x improvement in latency and 1000x improvement in energy efficiency compared to GPU-based implementations.",
        "published": "2024-10-20T05:52:03Z",
        "link": "http://arxiv.org/abs/2410.15296v1",
        "categories": [
            "cs.ET",
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Integer Polynomial Factorization by Recombination of Real Factors:   Re-evaluating an Old Technique in Modern Era",
        "authors": [
            "Shahriar Iravanian"
        ],
        "summary": "Polynomial factorization over $ZZ$ is of great historical and practical importance. Currently, the standard technique is to factor the polynomial over finite fields first and then to lift to integers. Factorization over finite fields can be done in polynomial time using Berlekamp or Cantor-Zassenhaus algorithms. Lifting from the finite field to $ZZ$ requires a combinatorial algorithm. The van Hoeij algorithm casts the combinatorial problem as a knapsack-equivalent problem, which is then solved using lattice-reduction (the LLL algorithm) in polynomial time, which is implemented in many computer algebra systems (CAS).   In this paper, we revisit the old idea of starting with factorization over $RR$ instead of a finite field, followed by recombination of the resulting linear and quadratic factors. We transform the problem into an integer subset sum problem, which is then solved using the Horowizt-Sinha algorithm. This algorithm can factor a random integer polynomial of degree $d$ in a time complexity of $O(2^(d slash 4))$.   While the resulting algorithm is exponential, consistent with the integer subset sum problem being in NP, it has a few advantages. First, it is simple and easy to implement. Second, it is almost embarrassingly parallelizable. We demonstrate this by implementing the algorithm in a Graphic Processing Unit (GPU). The resulting code can factor a degree 100 polynomial is a few tenths of a second, comparable to some standard CAS. This shows that it is possible to use current hardware, especially massively parallel systems like GPU, to the benefit of symbolic algebra.",
        "published": "2024-10-21T11:01:12Z",
        "link": "http://arxiv.org/abs/2410.15880v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "05-08, 12-08",
            "I.1.1; I.1.2; F.2.2"
        ]
    },
    {
        "title": "On Recurrence Relations of Multi-dimensional Sequences",
        "authors": [
            "Hamid Rahkooy"
        ],
        "summary": "In this paper, we present a new algorithm for computing the linear recurrence relations of multi-dimensional sequences. Existing algorithms for computing these relations arise in computational algebra and include constructing structured matrices and computing their kernels. The challenging problem is to reduce the size of the corresponding matrices. In this paper, we show how to convert the problem of computing recurrence relations of multi-dimensional sequences into computing the orthogonal of certain ideals as subvector spaces of the dual module of polynomials. We propose an algorithm using efficient dual module computation algorithms. We present a complexity bound for this algorithm, carry on experiments using Maple implementation, and discuss the cases when using this algorithm is much faster than the existing approaches.",
        "published": "2024-10-22T17:28:50Z",
        "link": "http://arxiv.org/abs/2410.17208v1",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Mechanisms of Symbol Processing for In-Context Learning in Transformer   Networks",
        "authors": [
            "Paul Smolensky",
            "Roland Fernandez",
            "Zhenghao Herbert Zhou",
            "Mattia Opper",
            "Jianfeng Gao"
        ],
        "summary": "Large Language Models (LLMs) have demonstrated impressive abilities in symbol processing through in-context learning (ICL). This success flies in the face of decades of predictions that artificial neural networks cannot master abstract symbol manipulation. We seek to understand the mechanisms that can enable robust symbol processing in transformer networks, illuminating both the unanticipated success, and the significant limitations, of transformers in symbol processing. Borrowing insights from symbolic AI on the power of Production System architectures, we develop a high-level language, PSL, that allows us to write symbolic programs to do complex, abstract symbol processing, and create compilers that precisely implement PSL programs in transformer networks which are, by construction, 100% mechanistically interpretable. We demonstrate that PSL is Turing Universal, so the work can inform the understanding of transformer ICL in general. The type of transformer architecture that we compile from PSL programs suggests a number of paths for enhancing transformers' capabilities at symbol processing. (Note: The first section of the paper gives an extended synopsis of the entire paper.)",
        "published": "2024-10-23T01:38:10Z",
        "link": "http://arxiv.org/abs/2410.17498v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.NE",
            "cs.SC",
            "F.1; I.2"
        ]
    },
    {
        "title": "Library Learning Doesn't: The Curious Case of the Single-Use \"Library\"",
        "authors": [
            "Ian Berlot-Attwell",
            "Frank Rudzicz",
            "Xujie Si"
        ],
        "summary": "Advances in Large Language Models (LLMs) have spurred a wave of LLM library learning systems for mathematical reasoning. These systems aim to learn a reusable library of tools, such as formal Isabelle lemmas or Python programs that are tailored to a family of tasks. Many of these systems are inspired by the human structuring of knowledge into reusable and extendable concepts, but do current methods actually learn reusable libraries of tools?   We study two library learning systems for mathematics which both reported increased accuracy: LEGO-Prover and TroVE. We find that function reuse is extremely infrequent on miniF2F and MATH. Our followup ablation experiments suggest that, rather than reuse, self-correction and self-consistency are the primary drivers of the observed performance gains. Our code and data are available at https://github.com/ikb-a/curious-case",
        "published": "2024-10-26T21:05:08Z",
        "link": "http://arxiv.org/abs/2410.20274v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "Simplest Mechanism Builder Algorithm (SiMBA): An Automated Microkinetic   Model Discovery Tool",
        "authors": [
            "Miguel Ángel de Carvalho Servia",
            "King Kuok",
            "Hii",
            "Klaus Hellgardt",
            "Dongda Zhang",
            "Ehecatl Antonio del Rio Chanona"
        ],
        "summary": "Microkinetic models are key for evaluating industrial processes' efficiency and chemicals' environmental impact. Manual construction of these models is difficult and time-consuming, prompting a shift to automated methods. This study introduces SiMBA (Simplest Mechanism Builder Algorithm), a novel approach for generating microkinetic models from kinetic data. SiMBA operates through four phases: mechanism generation, mechanism translation, parameter estimation, and model comparison. Our approach systematically proposes reaction mechanisms, using matrix representations and a parallelized backtracking algorithm to manage complexity. These mechanisms are then translated into microkinetic models represented by ordinary differential equations, and optimized to fit available data. Models are compared using information criteria to balance accuracy and complexity, iterating until convergence to an optimal model is reached. Case studies on an aldol condensation reaction, and the dehydration of fructose demonstrate SiMBA's effectiveness in distilling complex kinetic behaviors into simple yet accurate models. While SiMBA predicts intermediates correctly for all case studies, it does not chemically identify intermediates, requiring expert input for complex systems. Despite this, SiMBA significantly enhances mechanistic exploration, offering a robust initial mechanism that accelerates the development and modeling of chemical processes.",
        "published": "2024-10-28T16:51:50Z",
        "link": "http://arxiv.org/abs/2410.21205v1",
        "categories": [
            "cs.CE",
            "cs.SC"
        ]
    },
    {
        "title": "Accelerated Relaxation Engines for Optimizing to Minimum Energy Path",
        "authors": [
            "Sandra Liz Simon",
            "Nitin Kaistha",
            "Vishal Agarwal"
        ],
        "summary": "In the last few decades, several novel algorithms have been designed for finding critical points on PES and the minimum energy paths connecting them. This has led to considerably improve our understanding of reaction mechanisms and kinetics of the underlying processes. These methods implicitly rely on computation of energy and forces on the PES, which are usually obtained by computationally demanding wave-function or density-function based ab initio methods. To mitigate the computational cost, efficient optimization algorithms are needed. Herein, we present two new optimization algorithms: adaptively accelerated relaxation engine (AARE), an enhanced molecular dynamics (MD) scheme, and accelerated conjugate-gradient method (Acc-CG), an improved version of the traditional conjugate gradient (CG) algorithm. We show the efficacy of these algorithms for unconstrained optimization on 2D and 4D test functions. Additionally, we also show the efficacy of these algorithms for optimizing an elastic band of images to the minimum energy path on two analytical potentials (LEPS-I and LEPS-II) and for HCN/CNH isomerization reaction. In all cases, we find that the new algorithms outperforms the standard and popular fast inertial relaxation engine (FIRE).",
        "published": "2024-10-29T08:05:02Z",
        "link": "http://arxiv.org/abs/2410.21837v1",
        "categories": [
            "cs.CE",
            "cs.SC"
        ]
    },
    {
        "title": "A Theoretical Review on Solving Algebra Problems",
        "authors": [
            "Xinguo Yu",
            "Weina Cheng",
            "Chuanzhi Yang",
            "Ting Zhang"
        ],
        "summary": "Solving algebra problems (APs) continues to attract significant research interest as evidenced by the large number of algorithms and theories proposed over the past decade. Despite these important research contributions, however, the body of work remains incomplete in terms of theoretical justification and scope. The current contribution intends to fill the gap by developing a review framework that aims to lay a theoretical base, create an evaluation scheme, and extend the scope of the investigation. This paper first develops the State Transform Theory (STT), which emphasizes that the problem-solving algorithms are structured according to states and transforms unlike the understanding that underlies traditional surveys which merely emphasize the progress of transforms. The STT, thus, lays the theoretical basis for a new framework for reviewing algorithms. This new construct accommodates the relation-centric algorithms for solving both word and diagrammatic algebra problems. The latter not only highlights the necessity of introducing new states but also allows revelation of contributions of individual algorithms obscured in prior reviews without this approach.",
        "published": "2024-10-29T08:16:49Z",
        "link": "http://arxiv.org/abs/2411.00031v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Generalized cryptographic semi-regular sequences: A variant of   Fröberg conjecture and a simple complexity estimation for Gröbner   basis computation",
        "authors": [
            "Momonari Kudo",
            "Kazuhiro Yokoyama"
        ],
        "summary": "In this paper, we study generalized cryptographic semi-regular sequences, which are expected to generic in the space of homogeneous polynomial sequences on which the coordinate rings have Krull dimension one. We provide an upper-bound on the complexity of the Gr\\\"{o}bner basis computation for such a sequence.",
        "published": "2024-10-30T17:00:28Z",
        "link": "http://arxiv.org/abs/2410.23211v1",
        "categories": [
            "math.AC",
            "cs.SC",
            "math.AG"
        ]
    },
    {
        "title": "Efficient Performance Analysis of Modular Rewritable Petri Nets",
        "authors": [
            "Lorenzo Capra",
            "Marco Gribaudo"
        ],
        "summary": "Petri Nets (PN) are extensively used as a robust formalism to model concurrent and distributed systems; however, they encounter difficulties in accurately modeling adaptive systems. To address this issue, we defined rewritable PT nets (RwPT) using Maude, a declarative language that ensures consistent rewriting logic semantics. Recently, we proposed a modular approach that employs algebraic operators to build extensive RwPT models. This methodology uses composite node labeling to maintain hierarchical organization through net rewrites and has been shown to be effective. Once stochastic parameters are integrated into the formalism, we introduce an automated procedure to derive a lumped CTMC from the quotient graph generated by a modular RwPT model. To demonstrate the effectiveness of our method, we present a fault-tolerant manufacturing system as a case study.",
        "published": "2024-10-31T09:27:08Z",
        "link": "http://arxiv.org/abs/2410.23762v1",
        "categories": [
            "cs.PF",
            "cs.SC"
        ]
    },
    {
        "title": "Tracer: A Tool for Race Detection in Software Defined Network Models",
        "authors": [
            "Georgiana Caltais",
            "Mahboobeh Zangiabady",
            "Ervin Zvirbulis"
        ],
        "summary": "Software Defined Networking (SDN) has become a new paradigm in computer networking, introducing a decoupled architecture that separates the network into the data plane and the control plane. The control plane acts as the centralized brain, managing configuration updates and network management tasks, while the data plane handles traffic based on the configurations provided by the control plane. Given its asynchronous distributed nature, SDN can experience data races due to message passing between the control and data planes. This paper presents Tracer, a tool designed to automatically detect and explain the occurrence of data races in DyNetKAT SDN models. DyNetKAT is a formal framework for modeling and analyzing SDN behaviors, with robust operational semantics and a complete axiomatization implemented in Maude. Built on NetKAT, a language leveraging Kleene Algebra with Tests to express data plane forwarding behavior, DyNetKAT extends these capabilities by adding primitives for communication between the control and data planes. Tracer exploits the DyNetKAT axiomatization and enables race detection in SDNs based on Lamport vector clocks. Tracer is a publicly available tool.",
        "published": "2024-10-31T09:27:23Z",
        "link": "http://arxiv.org/abs/2410.23763v1",
        "categories": [
            "cs.FL",
            "cs.NI",
            "cs.SC"
        ]
    },
    {
        "title": "Transformers to Predict the Applicability of Symbolic Integration   Routines",
        "authors": [
            "Rashid Barket",
            "Uzma Shafiq",
            "Matthew England",
            "Juergen Gerhard"
        ],
        "summary": "Symbolic integration is a fundamental problem in mathematics: we consider how machine learning may be used to optimise this task in a Computer Algebra System (CAS). We train transformers that predict whether a particular integration method will be successful, and compare against the existing human-made heuristics (called guards) that perform this task in a leading CAS. We find the transformer can outperform these guards, gaining up to 30% accuracy and 70% precision. We further show that the inference time of the transformer is inconsequential which shows that it is well-suited to include as a guard in a CAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions that the transformer is making. If guided by a subject-matter expert, the technique can explain some of the predictions based on the input tokens, which can lead to further optimisations.",
        "published": "2024-10-31T14:03:37Z",
        "link": "http://arxiv.org/abs/2410.23948v1",
        "categories": [
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "Integrating Fuzzy Logic into Deep Symbolic Regression",
        "authors": [
            "Wout Gerdes",
            "Erman Acar"
        ],
        "summary": "Credit card fraud detection is a critical concern for financial institutions, intensified by the rise of contactless payment technologies. While deep learning models offer high accuracy, their lack of explainability poses significant challenges in financial settings. This paper explores the integration of fuzzy logic into Deep Symbolic Regression (DSR) to enhance both performance and explainability in fraud detection. We investigate the effectiveness of different fuzzy logic implications, specifically {\\L}ukasiewicz, G\\\"odel, and Product, in handling the complexity and uncertainty of fraud detection datasets. Our analysis suggest that the {\\L}ukasiewicz implication achieves the highest F1-score and overall accuracy, while the Product implication offers a favorable balance between performance and explainability. Despite having a performance lower than state-of-the-art (SOTA) models due to information loss in data transformation, our approach provides novelty and insights into into integrating fuzzy logic into DSR for fraud detection, providing a comprehensive comparison between different implications and methods.",
        "published": "2024-11-01T07:55:17Z",
        "link": "http://arxiv.org/abs/2411.00431v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Guiding Genetic Programming with Graph Neural Networks",
        "authors": [
            "Piotr Wyrwiński",
            "Krzysztof Krawiec"
        ],
        "summary": "In evolutionary computation, it is commonly assumed that a search algorithm acquires knowledge about a problem instance by sampling solutions from the search space and evaluating them with a fitness function. This is necessarily inefficient because fitness reveals very little about solutions -- yet they contain more information that can be potentially exploited. To address this observation in genetic programming, we propose EvoNUDGE, which uses a graph neural network to elicit additional knowledge from symbolic regression problems. The network is queried on the problem before an evolutionary run to produce a library of subprograms, which is subsequently used to seed the initial population and bias the actions of search operators. In an extensive experiment on a large number of problem instances, EvoNUDGE is shown to significantly outperform multiple baselines, including the conventional tree-based genetic programming and the purely neural variant of the method.",
        "published": "2024-11-03T20:43:31Z",
        "link": "http://arxiv.org/abs/2411.05820v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "cs.SC",
            "stat.ML"
        ]
    },
    {
        "title": "Extensions of the Cylindrical Algebraic Covering Method for Quantifiers",
        "authors": [
            "Jasper Nalbach",
            "Gereon Kremer"
        ],
        "summary": "The cylindrical algebraic covering method was originally proposed to decide the satisfiability of a set of non-linear real arithmetic constraints. We reformulate and extend the cylindrical algebraic covering method to allow for checking the truth of arbitrary non-linear arithmetic formulas, adding support for both quantifiers and Boolean structure. Furthermore, we also propose a variant to perform quantifier elimination on such formulas. After introducing the algorithm, we elaborate on various extensions, optimizations and heuristics. Finally, we present an experimental evaluation of our implementation and provide a comparison with state-of-the-art SMT solvers and quantifier elimination tools.",
        "published": "2024-11-05T13:06:52Z",
        "link": "http://arxiv.org/abs/2411.03070v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Symbolic Algorithm for Solving SLAEs with Multi-Diagonal Coefficient   Matrices",
        "authors": [
            "Milena Veneva"
        ],
        "summary": "This paper presents a generalised symbolic algorithm for solving systems of linear algebraic equations with multi-diagonal coefficient matrices. The algorithm is given in a pseudocode. A theorem which gives the condition for correctness of the algorithm is formulated and proven. Formula for the complexity of the multi-diagonal numerical algorithm is obtained.",
        "published": "2024-11-06T10:43:28Z",
        "link": "http://arxiv.org/abs/2411.11889v1",
        "categories": [
            "cs.SC",
            "65F50, 65K05, 65Y20",
            "G.1.3"
        ]
    },
    {
        "title": "Learning Interpretable Network Dynamics via Universal Neural Symbolic   Regression",
        "authors": [
            "Jiao Hu",
            "Jiaxu Cui",
            "Bo Yang"
        ],
        "summary": "Discovering governing equations of complex network dynamics is a fundamental challenge in contemporary science with rich data, which can uncover the mysterious patterns and mechanisms of the formation and evolution of complex phenomena in various fields and assist in decision-making. In this work, we develop a universal computational tool that can automatically, efficiently, and accurately learn the symbolic changing patterns of complex system states by combining the excellent fitting ability from deep learning and the equation inference ability from pre-trained symbolic regression. We conduct intensive experimental verifications on more than ten representative scenarios from physics, biochemistry, ecology, epidemiology, etc. Results demonstrate the outstanding effectiveness and efficiency of our tool by comparing with the state-of-the-art symbolic regression techniques for network dynamics. The application to real-world systems including global epidemic transmission and pedestrian movements has verified its practical applicability. We believe that our tool can serve as a universal solution to dispel the fog of hidden mechanisms of changes in complex phenomena, advance toward interpretability, and inspire more scientific discoveries.",
        "published": "2024-11-11T09:51:22Z",
        "link": "http://arxiv.org/abs/2411.06833v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MA",
            "cs.SC"
        ]
    },
    {
        "title": "Case Frames and Case-Based Arguments in Statutory Interpretation",
        "authors": [
            "Michal Araszkiewicz"
        ],
        "summary": "We introduce a novel conceptual Case Frame model that represents the content of cases involving statutory interpretation within civil law frameworks, accompanied by an associated argument scheme enriched with critical questions. By validating our approach with a modest dataset, we demonstrate its robustness and practical applicability. Our model not only provides a structured method for analyzing statutory interpretation but also highlights the distinct needs of lawyers operating under statutory law compared to those reasoning with common law precedents. The model presented here is a step towards developing a hybrid Machine Learning and Argumentation system that includes a module for constructing well-structured arguments from textual datasets.",
        "published": "2024-11-11T11:14:01Z",
        "link": "http://arxiv.org/abs/2411.06873v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "On Minimal and Minimum Cylindrical Algebraic Decompositions",
        "authors": [
            "Lucas Michel",
            "Pierre Mathonet",
            "Naïm Zénaïdi"
        ],
        "summary": "We consider cylindrical algebraic decompositions (CADs) as a tool for representing semi-algebraic subsets of $\\mathbb{R}^n$. In this framework, a CAD $\\mathscr{C}$ is adapted to a given set $S$ if $S$ is a union of cells of $\\mathscr{C}$. Different algorithms computing an adapted CAD may produce different outputs, usually with redundant cell divisions. In this paper we analyse the possibility to remove the superfluous data. More precisely we consider the set CAD$(S)$ of CADs that are adapted to $S$, endowed with the refinement partial order and we study the existence of minimal and minimum elements in this poset.   We show that for every semi-algebraic set $S$ of $\\mathbb{R}^n$ and every CAD $\\mathscr{C}$ adapted to $S$, there is a minimal CAD adapted to $S$ and smaller (i.e. coarser) than or equal to $\\mathscr{C}$. Moreover, when $n=1$ or $n=2$, we strengthen this result by proving the existence of a minimum element in CAD$(S)$. Astonishingly for $n \\geq 3$, there exist semi-algebraic sets whose associated poset of adapted CADs does not admit a minimum. We prove this result by providing explicit examples. We finally use a reduction relation on CAD$(S)$ to define an algorithm for the computation of minimal CADs. We conclude with a characterization of those semi-algebraic sets $S$ for which CAD$(S)$ has a minimum by means of confluence of the associated reduction system.",
        "published": "2024-11-20T11:24:49Z",
        "link": "http://arxiv.org/abs/2411.13218v1",
        "categories": [
            "cs.SC",
            "14P10",
            "I.1.0"
        ]
    },
    {
        "title": "On Projective Delineability",
        "authors": [
            "Lucas Michel",
            "Jasper Nalbach",
            "Pierre Mathonet",
            "Naïm Zénaïdi",
            "Christopher W. Brown",
            "Erika Ábrahám",
            "James H. Davenport",
            "Matthew England"
        ],
        "summary": "We consider cylindrical algebraic decomposition (CAD) and the key concept of delineability which underpins CAD theory. We introduce the novel concept of projective delineability which is easier to guarantee computationally. We prove results about this which can allow reduced CAD computations.",
        "published": "2024-11-20T13:12:34Z",
        "link": "http://arxiv.org/abs/2411.13300v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "14Q20, 14Q30",
            "I.1.0"
        ]
    },
    {
        "title": "CryptoFormalEval: Integrating LLMs and Formal Verification for Automated   Cryptographic Protocol Vulnerability Detection",
        "authors": [
            "Cristian Curaba",
            "Denis D'Ambrosi",
            "Alessandro Minisini",
            "Natalia Pérez-Campanero Antolín"
        ],
        "summary": "Cryptographic protocols play a fundamental role in securing modern digital infrastructure, but they are often deployed without prior formal verification. This could lead to the adoption of distributed systems vulnerable to attack vectors. Formal verification methods, on the other hand, require complex and time-consuming techniques that lack automatization. In this paper, we introduce a benchmark to assess the ability of Large Language Models (LLMs) to autonomously identify vulnerabilities in new cryptographic protocols through interaction with Tamarin: a theorem prover for protocol verification. We created a manually validated dataset of novel, flawed, communication protocols and designed a method to automatically verify the vulnerabilities found by the AI agents. Our results about the performances of the current frontier models on the benchmark provides insights about the possibility of cybersecurity applications by integrating LLMs with symbolic reasoning systems.",
        "published": "2024-11-20T14:16:55Z",
        "link": "http://arxiv.org/abs/2411.13627v1",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Union of Finitely Generated Congruences on Ground Term Algebra",
        "authors": [
            "Sándor Vágvölgyi"
        ],
        "summary": "We show that for any ground term equation systems $E$ and $F$, (1) the union of the generated congruences by $E$ and $F$ is a congruence on the ground term algebra if and only if there exists a ground term equation system $H$ such that the congruence generated by $H$ is equal to the union of the congruences generated by $E$ and $F$ if and only if the congruence generated by the union of $E $ and $F$ is equal to the union of the congruences generated by $E $ and $F$, and (2) it is decidable in square time whether the congruence generated by the union of $E$ and $F$ is equal to the union of the congruences generated by $E $ and $F$, where the size of the input is the number of occurrences of symbols in $E$ plus the number of occurrences of symbols in $F$.",
        "published": "2024-11-21T20:13:10Z",
        "link": "http://arxiv.org/abs/2411.14559v1",
        "categories": [
            "cs.SC",
            "cs.LO",
            "08A70"
        ]
    },
    {
        "title": "Bio-inspired AI: Integrating Biological Complexity into Artificial   Intelligence",
        "authors": [
            "Nima Dehghani",
            "Michael Levin"
        ],
        "summary": "The pursuit of creating artificial intelligence (AI) mirrors our longstanding fascination with understanding our own intelligence. From the myths of Talos to Aristotelian logic and Heron's inventions, we have sought to replicate the marvels of the mind. While recent advances in AI hold promise, singular approaches often fall short in capturing the essence of intelligence. This paper explores how fundamental principles from biological computation--particularly context-dependent, hierarchical information processing, trial-and-error heuristics, and multi-scale organization--can guide the design of truly intelligent systems. By examining the nuanced mechanisms of biological intelligence, such as top-down causality and adaptive interaction with the environment, we aim to illuminate potential limitations in artificial constructs. Our goal is to provide a framework inspired by biological systems for designing more adaptable and robust artificial intelligent systems.",
        "published": "2024-11-22T02:55:39Z",
        "link": "http://arxiv.org/abs/2411.15243v1",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.NE",
            "cs.SC"
        ]
    },
    {
        "title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework   Leveraging Knowledge Distillation from Large Language Models",
        "authors": [
            "Tianhao Chen",
            "Pengbo Xu",
            "Pengbo Xu"
        ],
        "summary": "In recent years, the introduction of AI technologies has brought transformative changes to scientific computing. However, AI models typically focus on single-task and single-modal data processing, limiting their application. To address this, multimodal scientific computing frameworks have become a trend. The Botfip framework aligns function images with symbolic operation trees through multimodal training, extracting deep scientific information. However, Botfip struggles with processing Formula Strings, leading to inadequate understanding in multimodal learning. To enhance Botfip's learning of Formula Strings and expand its applicability to related tasks, we propose the Botfip-LLM framework based on knowledge distillation, incorporating pre-trained large language models for aligning symbolic tree data. Experimental analysis shows that the choice of LLM is crucial, with ChatGLM-2 outperforming others in training and testing. Botfip-LLM not only improves performance, generalization, and extrapolation over the original Botfip model but also significantly enhances applicability to Formula String-related tasks, enabling more diverse task handling.",
        "published": "2024-11-23T11:33:16Z",
        "link": "http://arxiv.org/abs/2411.15525v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Extracting Linear Relations from Gröbner Bases for Formal Verification   of And-Inverter Graphs",
        "authors": [
            "Daniela Kaufmann",
            "Jérémy Berthomieu"
        ],
        "summary": "Formal verification techniques based on computer algebra have proven highly effective for circuit verification. The circuit, given as an and-inverter graph, is encoded as a set of polynomials that automatically generates a Gr\\\"obner basis with respect to a lexicographic term ordering. Correctness of the circuit can be derived by computing the polynomial remainder of the specification. However, the main obstacle is the monomial blow-up during the rewriting of the specification, which leads to the development of dedicated heuristics to overcome this issue. In this paper, we investigate an orthogonal approach and focus the computational effort on rewriting the Gr\\\"obner basis itself. Our goal is to ensure the basis contains linear polynomials that can be effectively used to rewrite the linearized specification. We first prove the soundness and completeness of this technique and then demonstrate its practical application. Our implementation of this method shows promising results on benchmarks related to multiplier verification.",
        "published": "2024-11-25T12:55:49Z",
        "link": "http://arxiv.org/abs/2411.16348v1",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "Relations, Negations, and Numbers: Looking for Logic in Generative   Text-to-Image Models",
        "authors": [
            "Colin Conwell",
            "Rupert Tawiah-Quashie",
            "Tomer Ullman"
        ],
        "summary": "Despite remarkable progress in multi-modal AI research, there is a salient domain in which modern AI continues to lag considerably behind even human children: the reliable deployment of logical operators. Here, we examine three forms of logical operators: relations, negations, and discrete numbers. We asked human respondents (N=178 in total) to evaluate images generated by a state-of-the-art image-generating AI (DALL-E 3) prompted with these `logical probes', and find that none reliably produce human agreement scores greater than 50\\%. The negation probes and numbers (beyond 3) fail most frequently. In a 4th experiment, we assess a `grounded diffusion' pipeline that leverages targeted prompt engineering and structured intermediate representations for greater compositional control, but find its performance is judged even worse than that of DALL-E 3 across prompts. To provide further clarity on potential sources of success and failure in these text-to-image systems, we supplement our 4 core experiments with multiple auxiliary analyses and schematic diagrams, directly quantifying, for example, the relationship between the N-gram frequency of relational prompts and the average match to generated images; the success rates for 3 different prompt modification strategies in the rendering of negation prompts; and the scalar variability / ratio dependence (`approximate numeracy') of prompts involving integers. We conclude by discussing the limitations inherent to `grounded' multimodal learning systems whose grounding relies heavily on vector-based semantics (e.g. DALL-E 3), or under-specified syntactical constraints (e.g. `grounded diffusion'), and propose minimal modifications (inspired by development, based in imagery) that could help to bridge the lingering compositional gap between scale and structure. All data and code is available at https://github.com/ColinConwell/T2I-Probology",
        "published": "2024-11-26T03:06:52Z",
        "link": "http://arxiv.org/abs/2411.17066v1",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "Large Language Models as Mirrors of Societal Moral Standards",
        "authors": [
            "Evi Papadopoulou",
            "Hadi Mohammadi",
            "Ayoub Bagheri"
        ],
        "summary": "Prior research has demonstrated that language models can, to a limited extent, represent moral norms in a variety of cultural contexts. This research aims to replicate these findings and further explore their validity, concentrating on issues like 'homosexuality' and 'divorce'. This study evaluates the effectiveness of these models using information from two surveys, the WVS and the PEW, that encompass moral perspectives from over 40 countries. The results show that biases exist in both monolingual and multilingual models, and they typically fall short of accurately capturing the moral intricacies of diverse cultures. However, the BLOOM model shows the best performance, exhibiting some positive correlations, but still does not achieve a comprehensive moral understanding. This research underscores the limitations of current PLMs in processing cross-cultural differences in values and highlights the importance of developing culturally aware AI systems that better align with universal human values.",
        "published": "2024-12-01T20:20:35Z",
        "link": "http://arxiv.org/abs/2412.00956v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "LLMs as mirrors of societal moral standards: reflection of cultural   divergence and agreement across ethical topics",
        "authors": [
            "Mijntje Meijer",
            "Hadi Mohammadi",
            "Ayoub Bagheri"
        ],
        "summary": "Large language models (LLMs) have become increasingly pivotal in various domains due the recent advancements in their performance capabilities. However, concerns persist regarding biases in LLMs, including gender, racial, and cultural biases derived from their training data. These biases raise critical questions about the ethical deployment and societal impact of LLMs. Acknowledging these concerns, this study investigates whether LLMs accurately reflect cross-cultural variations and similarities in moral perspectives. In assessing whether the chosen LLMs capture patterns of divergence and agreement on moral topics across cultures, three main methods are employed: (1) comparison of model-generated and survey-based moral score variances, (2) cluster alignment analysis to evaluate the correspondence between country clusters derived from model-generated moral scores and those derived from survey data, and (3) probing LLMs with direct comparative prompts. All three methods involve the use of systematic prompts and token pairs designed to assess how well LLMs understand and reflect cultural variations in moral attitudes. The findings of this study indicate overall variable and low performance in reflecting cross-cultural differences and similarities in moral values across the models tested, highlighting the necessity for improving models' accuracy in capturing these nuances effectively. The insights gained from this study aim to inform discussions on the ethical development and deployment of LLMs in global contexts, emphasizing the importance of mitigating biases and promoting fair representation across diverse cultural perspectives.",
        "published": "2024-12-01T20:39:42Z",
        "link": "http://arxiv.org/abs/2412.00962v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.SC"
        ]
    },
    {
        "title": "Semantics of Division for Polynomial Solvers",
        "authors": [
            "Christopher W. Brown"
        ],
        "summary": "How to handle division in systems that compute with logical formulas involving what would otherwise be polynomial constraints over the real numbers is a surprisingly difficult question. This paper argues that existing approaches from both the computer algebra and computational logic communities are unsatisfactory for systems that consider the satisfiability of formulas with quantifiers or that perform quantifier elimination. To address this, we propose the notion of the fair-satisfiability of a formula, use it to characterize formulas with divisions that are well-defined, meaning that they adequately guard divisions against division by zero, and provide a translation algorithm that converts a formula with divisions into a purely polynomial formula that is satisfiable if and only if the original formula is fair-satisfiable. This provides a semantics for division with some nice properties, which we describe and prove in the paper.",
        "published": "2024-12-01T20:40:51Z",
        "link": "http://arxiv.org/abs/2412.00963v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Training Stiff Neural Ordinary Differential Equations with Explicit   Exponential Integration Methods",
        "authors": [
            "Colby Fronk",
            "Linda Petzold"
        ],
        "summary": "Stiff ordinary differential equations (ODEs) are common in many science and engineering fields, but standard neural ODE approaches struggle to accurately learn these stiff systems, posing a significant barrier to widespread adoption of neural ODEs. In our earlier work, we addressed this challenge by utilizing single-step implicit methods for solving stiff neural ODEs. While effective, these implicit methods are computationally costly and can be complex to implement. This paper expands on our earlier work by exploring explicit exponential integration methods as a more efficient alternative. We evaluate the potential of these explicit methods to handle stiff dynamics in neural ODEs, aiming to enhance their applicability to a broader range of scientific and engineering problems. We found the integrating factor Euler (IF Euler) method to excel in stability and efficiency. While implicit schemes failed to train the stiff Van der Pol oscillator, the IF Euler method succeeded, even with large step sizes. However, IF Euler's first-order accuracy limits its use, leaving the development of higher-order methods for stiff neural ODEs an open research problem.",
        "published": "2024-12-02T06:40:08Z",
        "link": "http://arxiv.org/abs/2412.01181v1",
        "categories": [
            "math.NA",
            "cs.AI",
            "cs.LG",
            "cs.NA",
            "cs.SC"
        ]
    },
    {
        "title": "Asymptotics for the reciprocal and shifted quotient of the partition   function",
        "authors": [
            "Koustav Banerjee",
            "Peter Paule",
            "Cristian-Silviu Radu",
            "Carsten Schneider"
        ],
        "summary": "Let $p(n)$ denote the partition function. In this paper our main goal is to derive an asymptotic expansion up to order $N$ (for any fixed positive integer $N$) along with estimates for error bounds for the shifted quotient of the partition function, namely $p(n+k)/p(n)$ with $k\\in \\mathbb{N}$, which generalizes a result of Gomez, Males, and Rolen. In order to do so, we derive asymptotic expansions with error bounds for the shifted version $p(n+k)$ and the multiplicative inverse $1/p(n)$, which is of independent interest.",
        "published": "2024-12-03T08:32:37Z",
        "link": "http://arxiv.org/abs/2412.02257v1",
        "categories": [
            "math.NT",
            "cs.SC",
            "math.CO",
            "05A16, 05A20, 11P82"
        ]
    },
    {
        "title": "Summa Summarum: Moessner's Theorem without Dynamic Programming",
        "authors": [
            "Olivier Danvy"
        ],
        "summary": "Seventy years on, Moessner's theorem and Moessner's process -- i.e., the additive computation of integral powers -- continue to fascinate. They have given rise to a variety of elegant proofs, to an implementation in hardware, to generalizations, and now even to a popular video, \"The Moessner Miracle.'' The existence of this video, and even more its title, indicate that while the \"what'' of Moessner's process is understood, its \"how'' and even more its \"why'' are still elusive. And indeed all the proofs of Moessner's theorem involve more complicated concepts than both the theorem and the process. This article identifies that Moessner's process implements an additive function with dynamic programming. A version of this implementation without dynamic programming (1) gives rise to a simpler statement of Moessner's theorem and (2) can be abstracted and then instantiated into related additive computations. The simpler statement also suggests a simpler and more efficient implementation to compute integral powers as well as simple additive functions to compute, e.g., Factorial numbers. It also reveals the source of -- to quote John Conway and Richard Guy -- Moessner's magic.",
        "published": "2024-12-04T08:44:02Z",
        "link": "http://arxiv.org/abs/2412.03127v1",
        "categories": [
            "cs.DM",
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.1.1;D.2.4;D.3.2;F.2.1;F.3.1;G.1.0;G.2.0;I.1.1;I.2.3;I.2.8"
        ]
    },
    {
        "title": "Hahn series and Mahler equations: Algorithmic aspects",
        "authors": [
            "C. Faverjon",
            "Julien Roques"
        ],
        "summary": "Many articles have recently been devoted to Mahler equations, partly because of their links with other branches of mathematics such as automata theory. Hahn series (a generalization of the Puiseux series allowing arbitrary exponents of the indeterminate as long as the set that supports them is well-ordered) play a central role in the theory of Mahler equations. In this paper, we address the following fundamental question: is there an algorithm to calculate the Hahn series solutions of a given linear Mahler equation? What makes this question interesting is the fact that the Hahn series appearing in this context can have complicated supports with infinitely many accumulation points. Our (positive) answer to the above question involves among other things the construction of a computable well-ordered receptacle for the supports of the potential Hahn series solutions.",
        "published": "2024-12-06T10:32:37Z",
        "link": "http://arxiv.org/abs/2412.04928v1",
        "categories": [
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "Towards Learning to Reason: Comparing LLMs with Neuro-Symbolic on   Arithmetic Relations in Abstract Reasoning",
        "authors": [
            "Michael Hersche",
            "Giacomo Camposampiero",
            "Roger Wattenhofer",
            "Abu Sebastian",
            "Abbas Rahimi"
        ],
        "summary": "This work compares large language models (LLMs) and neuro-symbolic approaches in solving Raven's progressive matrices (RPM), a visual abstract reasoning test that involves the understanding of mathematical rules such as progression or arithmetic addition. Providing the visual attributes directly as textual prompts, which assumes an oracle visual perception module, allows us to measure the model's abstract reasoning capability in isolation. Despite providing such compositionally structured representations from the oracle visual perception and advanced prompting techniques, both GPT-4 and Llama-3 70B cannot achieve perfect accuracy on the center constellation of the I-RAVEN dataset. Our analysis reveals that the root cause lies in the LLM's weakness in understanding and executing arithmetic rules. As a potential remedy, we analyze the Abductive Rule Learner with Context-awareness (ARLC), a neuro-symbolic approach that learns to reason with vector-symbolic architectures (VSAs). Here, concepts are represented with distributed vectors s.t. dot products between encoded vectors define a similarity kernel, and simple element-wise operations on the vectors perform addition/subtraction on the encoded values. We find that ARLC achieves almost perfect accuracy on the center constellation of I-RAVEN, demonstrating a high fidelity in arithmetic rules. To stress the length generalization capabilities of the models, we extend the RPM tests to larger matrices (3x10 instead of typical 3x3) and larger dynamic ranges of the attribute values (from 10 up to 1000). We find that the LLM's accuracy of solving arithmetic rules drops to sub-10%, especially as the dynamic range expands, while ARLC can maintain a high accuracy due to emulating symbolic computations on top of properly distributed representations. Our code is available at https://github.com/IBM/raven-large-language-models.",
        "published": "2024-12-07T08:45:39Z",
        "link": "http://arxiv.org/abs/2412.05586v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.SC"
        ]
    },
    {
        "title": "A Monadic Calculus with Episodic Flows",
        "authors": [
            "Sotirios Henning"
        ],
        "summary": "We define computational atoms named \"actions\" equipped primarily with three operations: reduction, collection, and inspection. We show how actions can be used for decision-making algorithms from simple axioms. We describe the encodings of typical data structures as actions, and provide a method of analysis for algorithms on the basis of data mutation.",
        "published": "2024-12-10T21:51:52Z",
        "link": "http://arxiv.org/abs/2412.07939v1",
        "categories": [
            "cs.SC",
            "68W30"
        ]
    },
    {
        "title": "The B2Scala Tool: Integrating Bach in Scala with Security in Mind",
        "authors": [
            "Doha Ouardi",
            "Manel Barkallah",
            "Jean-Marie Jacquet"
        ],
        "summary": "Process algebras have been widely used to verify security protocols in a formal manner. However they mostly focus on synchronous communication based on the exchange of messages. We present an alternative approach relying on asynchronous communication obtained through information available on a shared space. More precisely this paper first proposes an embedding in Scala of a Linda-like language, called Bach. It consists of a Domain Specific Language, internal to Scala, that allows us to experiment programs developed in Bach while benefiting from the Scala eco-system, in particular from its type system as well as program fragments developed in Scala. Moreover, we introduce a logic that allows to restrict the executions of programs to those meeting logic formulae. Our work is illustrated on the Needham-Schroeder security protocol, for which we manage to automatically rediscover the man-in-the-middle attack first put in evidence by G. Lowe.",
        "published": "2024-12-11T09:32:11Z",
        "link": "http://arxiv.org/abs/2412.08235v1",
        "categories": [
            "cs.PL",
            "cs.MA",
            "cs.SC",
            "D.1.3; D.2.4; D.3.3"
        ]
    },
    {
        "title": "Positivity Proofs for Linear Recurrences through Contracted Cones",
        "authors": [
            "Alaa Ibrahim",
            "Bruno Salvy"
        ],
        "summary": "Deciding the positivity of a sequence defined by a linear recurrence with polynomial coefficients and initial condition is difficult in general. Even in the case of recurrences with constant coefficients, it is known to be decidable only for order up to~5. We consider a large class of linear recurrences of arbitrary order, with polynomial coefficients, for which an algorithm decides positivity for initial conditions outside of a hyperplane. The underlying algorithm constructs a cone, contracted by the recurrence operator, that allows a proof of positivity by induction. The existence and construction of such cones relies on the extension of the classical Perron-Frobenius theory to matrices leaving a cone invariant.",
        "published": "2024-12-11T17:47:49Z",
        "link": "http://arxiv.org/abs/2412.08576v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "ViSymRe: Vision-guided Multimodal Symbolic Regression",
        "authors": [
            "Da Li",
            "Junping Yin",
            "Jin Xu",
            "Xinxin Li",
            "Juan Zhang"
        ],
        "summary": "Symbolic regression automatically searches for mathematical equations to reveal underlying mechanisms within datasets, offering enhanced interpretability compared to black box models. Traditionally, symbolic regression has been considered to be purely numeric-driven, with insufficient attention given to the potential contributions of visual information in augmenting this process. When dealing with high-dimensional and complex datasets, existing symbolic regression models are often inefficient and tend to generate overly complex equations, making subsequent mechanism analysis complicated. In this paper, we propose the vision-guided multimodal symbolic regression model, called ViSymRe, that systematically explores how visual information can improve various metrics of symbolic regression. Compared to traditional models, our proposed model has the following innovations: (1) It integrates three modalities: vision, symbol and numeric to enhance symbolic regression, enabling the model to benefit from the strengths of each modality; (2) It establishes a meta-learning framework that can learn from historical experiences to efficiently solve new symbolic regression problems; (3) It emphasizes the simplicity and structural rationality of the equations rather than merely numerical fitting. Extensive experiments show that our proposed model exhibits strong generalization capability and noise resistance. The equations it generates outperform state-of-the-art numeric-only baselines in terms of fitting effect, simplicity and structural accuracy, thus being able to facilitate accurate mechanism analysis and the development of theoretical models.",
        "published": "2024-12-15T10:05:31Z",
        "link": "http://arxiv.org/abs/2412.11139v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "Invariants: Computation and Applications",
        "authors": [
            "Irina A. Kogan"
        ],
        "summary": "Invariants withstand transformations and, therefore, represent the essence of objects or phenomena. In mathematics, transformations often constitute a group action. Since the 19th century, studying the structure of various types of invariants and designing methods and algorithms to compute them remains an active area of ongoing research with an abundance of applications. In this incredibly vast topic, we focus on two particular themes displaying a fruitful interplay between the differential and algebraic invariant theories. First, we show how an algebraic adaptation of the moving frame method from differential geometry leads to a practical algorithm for computing a generating set of rational invariants. Then we discuss the notion of differential invariant signature, its role in solving equivalence problems in geometry and algebra, and some successes and challenges in designing algorithms based on this notion.",
        "published": "2024-12-17T20:18:03Z",
        "link": "http://arxiv.org/abs/2412.13306v1",
        "categories": [
            "cs.SC",
            "13A50, 14L24, 53A55",
            "I.1.2"
        ]
    },
    {
        "title": "Algebraic Tools for Computing Polynomial Loop Invariants (Extended   Version)",
        "authors": [
            "Erdenebayar Bayarmagnai",
            "Fatemeh Mohammadi",
            "Rémi Prébet"
        ],
        "summary": "Loop invariants are properties of a program loop that hold both before and after each iteration of the loop. They are often used to verify programs and ensure that algorithms consistently produce correct results during execution. Consequently, generating invariants becomes a crucial task for loops. We specifically focus on polynomial loops, where both the loop conditions and the assignments within the loop are expressed as polynomials. Although computing polynomial invariants for general loops is undecidable, efficient algorithms have been developed for certain classes of loops. For instance, when all assignments within a while loop involve linear polynomials, the loop becomes solvable. In this work, we study the more general case, where the polynomials can have arbitrary degrees.   Using tools from algebraic geometry, we present two algorithms designed to generate all polynomial invariants within a given vector subspace, for a branching loop with nondeterministic conditional statements. These algorithms combine linear algebra subroutines with computations on polynomial ideals. They differ depending on whether the initial values of the loop variables are specified or treated as parameters. Additionally, we present a much more efficient algorithm for generating polynomial invariants of a specific form, applicable to all initial values. This algorithm avoids expensive ideal computations.",
        "published": "2024-12-18T16:58:48Z",
        "link": "http://arxiv.org/abs/2412.14043v1",
        "categories": [
            "cs.SC",
            "cs.PL",
            "math.AG"
        ]
    },
    {
        "title": "Using SimTeEx to simplify polynomial expressions with tensors",
        "authors": [
            "Renato M. Fonseca"
        ],
        "summary": "Computations with tensors are ubiquitous in fundamental physics, and so is the usage of Einstein's dummy index convention for the contraction of indices. For instance, $T_{ia}U_{aj}$ is readily recognized as the same as $T_{ib}U_{bj}$, but a computer does not know that T[i,a]U[a,j] is equal to T[i,b]U[b,j]. Furthermore, tensors may have symmetries which can be used to simply expressions: if $U_{ij}$ is antisymmetric, then $\\alpha T_{ia}U_{aj}+\\beta T_{ib}U_{jb}=\\left(\\alpha-\\beta\\right)T_{ia}U_{aj}$. The fact that tensors can have elaborate symmetries, together with the problem of dummy indices, makes it complicated to simplify polynomial expressions with tensors. In this work I will present an algorithm for doing so, which was implemented in the Mathematica package SimTeEx (Simplify Tensor Expressions). It can handle any kind of tensor symmetry.",
        "published": "2024-12-18T22:49:22Z",
        "link": "http://arxiv.org/abs/2412.14390v1",
        "categories": [
            "hep-ph",
            "cs.SC",
            "gr-qc",
            "hep-th"
        ]
    },
    {
        "title": "Answer Set Networks: Casting Answer Set Programming into Deep Learning",
        "authors": [
            "Arseny Skryagin",
            "Daniel Ochs",
            "Phillip Deibert",
            "Simon Kohaut",
            "Devendra Singh Dhami",
            "Kristian Kersting"
        ],
        "summary": "Although Answer Set Programming (ASP) allows constraining neural-symbolic (NeSy) systems, its employment is hindered by the prohibitive costs of computing stable models and the CPU-bound nature of state-of-the-art solvers. To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep Probabilistic Logic Programming (DPPL). Specifically, we show how to translate ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded problem by leveraging GPU's batching and parallelization capabilities. Our experimental evaluations demonstrate that ASNs outperform state-of-the-art CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following two contributions based on the strengths of ASNs. Namely, we are the first to show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs to guide the training with logic. Further, we show the \"constitutional navigation\" of drones, i.e., encoding public aviation laws in an ASN for routing Unmanned Aerial Vehicles in uncertain environments.",
        "published": "2024-12-19T13:09:06Z",
        "link": "http://arxiv.org/abs/2412.14814v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.SC",
            "68T37, 68T30, 68T27",
            "I.2.4; I.2.5"
        ]
    },
    {
        "title": "Folding One Polyhedral Metric Graph into Another",
        "authors": [
            "Lily Chung",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Markus Hecher",
            "Rebecca Lin",
            "Jayson Lynch",
            "Chie Nara"
        ],
        "summary": "We analyze the problem of folding one polyhedron, viewed as a metric graph of its edges, into the shape of another, similar to 1D origami. We find such foldings between all pairs of Platonic solids and prove corresponding lower bounds, establishing the optimal scale factor when restricted to integers. Further, we establish that our folding problem is also NP-hard, even if the source graph is a tree. It turns out that the problem is hard to approximate, as we obtain NP-hardness even for determining the existence of a scale factor 1.5-{\\epsilon}. Finally, we prove that, in general, the optimal scale factor has to be rational. This insight then immediately results in NP membership. In turn, verifying whether a given scale factor is indeed the smallest possible, requires two independent calls to an NP oracle, rendering the problem DP-complete.",
        "published": "2024-12-19T18:01:35Z",
        "link": "http://arxiv.org/abs/2412.15121v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "cs.DM",
            "cs.SC",
            "68R10, 68Q17, 68U05",
            "G.2.2; F.2.2"
        ]
    }
]