[
    {
        "title": "Incremental Recompilation of Knowledge",
        "authors": [
            "G. Gogic",
            "C. H. Papadimitriou",
            "M. Sideri"
        ],
        "summary": "Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of ``knowledge compilation,'' supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.",
        "published": "1998-01-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9801101v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Monotonicity and Persistence in Preferential Logics",
        "authors": [
            "J. Engelfriet"
        ],
        "summary": "An important characteristic of many logics for Artificial Intelligence is their nonmonotonicity. This means that adding a formula to the premises can invalidate some of the consequences. There may, however, exist formulae that can always be safely added to the premises without destroying any of the consequences: we say they respect monotonicity. Also, there may be formulae that, when they are a consequence, can not be invalidated when adding any formula to the premises: we call them conservative. We study these two classes of formulae for preferential logics, and show that they are closely linked to the formulae whose truth-value is preserved along the (preferential) ordering. We will consider some preferential logics for illustration, and prove syntactic characterization results for them. The results in this paper may improve the efficiency of theorem provers for preferential logics.",
        "published": "1998-01-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9801102v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Synthesizing Customized Planners from Specifications",
        "authors": [
            "B. Srivastava",
            "S. Kambhampati"
        ],
        "summary": "Existing plan synthesis approaches in artificial intelligence fall into two categories -- domain independent and domain dependent. The domain independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain dependent approaches need to be (re)designed for each domain separately, but can be very efficient in the domain for which they are designed. One enticing alternative to these approaches is to automatically synthesize domain independent planners given the knowledge about the domain and the theory of planning. In this paper, we investigate the feasibility of using existing automated software synthesis tools to support such synthesis. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System (KIDS) is used to derive a domain-customized planner through a semi-automatic combination of a declarative theory of planning, and the declarative control knowledge specific to a given domain, to semi-automatically combine them to derive domain-customized planners. We discuss what it means to write a declarative theory of planning and control knowledge for KIDS, and illustrate our approach by generating a class of domain-specific planners using state space refinements. Our experiments show that the synthesized planners can outperform classical refinement planners (implemented as instantiations of UCP, Kambhampati & Srivastava, 1995), using the same control knowledge. We will contrast the costs and benefits of the synthesis approach with conventional methods for customizing domain independent planners.",
        "published": "1998-03-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9803101v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Cached Sufficient Statistics for Efficient Machine Learning with Large   Datasets",
        "authors": [
            "A. Moore",
            "M. S. Lee"
        ],
        "summary": "This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.",
        "published": "1998-03-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9803102v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Tractability of Theory Patching",
        "authors": [
            "S. Argamon-Engelson",
            "M. Koppel"
        ],
        "summary": "In this paper we consider the problem of `theory patching', in which we are given a domain theory, some of whose components are indicated to be possibly flawed, and a set of labeled training examples for the domain concept. The theory patching problem is to revise only the indicated components of the theory, such that the resulting theory correctly classifies all the training examples. Theory patching is thus a type of theory revision in which revisions are made to individual components of the theory. Our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable. We consider both propositional and first-order domain theories, and show that the theory patching problem is equivalent to that of determining what information contained in a theory is `stable' regardless of what revisions might be performed to the theory. We show that determining stability is tractable if the input theory satisfies two conditions: that revisions to each theory component have monotonic effects on the classification of examples, and that theory components act independently in the classification of examples in the theory. We also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms.",
        "published": "1998-03-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9803103v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Integrative Windowing",
        "authors": [
            "J. FÃ¼rnkranz"
        ],
        "summary": "In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise.",
        "published": "1998-05-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9805101v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Model-Based Diagnosis using Structured System Descriptions",
        "authors": [
            "A. Darwiche"
        ],
        "summary": "This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.",
        "published": "1998-06-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9806101v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Selective Macro-learning Algorithm and its Application to the NxN   Sliding-Tile Puzzle",
        "authors": [
            "L. Finkelstein",
            "S. Markovitch"
        ],
        "summary": "One of the most common mechanisms used for speeding up problem solvers is macro-learning. Macros are sequences of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problem-solving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of NxN sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puzzles of any size.",
        "published": "1998-06-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9806102v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The Computational Complexity of Probabilistic Planning",
        "authors": [
            "M. L. Littman",
            "J. Goldsmith",
            "M. Mundhenk"
        ],
        "summary": "We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both flat and propositional representations. The complexity of plan evaluation and existence varies with the plan type sought; we examine totally ordered plans, acyclic plans, and looping plans, and partially ordered plans under three natural definitions of plan value. We show that problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the process of proving that certain planning problems are complete for NP^PP, we introduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the standard Boolean satisfiability problem to computations involving probabilistic quantities; our results suggest that the development of good heuristics for E-MAJSAT could be important for the creation of efficient algorithms for a wide variety of problems.",
        "published": "1998-08-01T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9808101v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Chess Pure Strategies are Probably Chaotic",
        "authors": [
            "M. Chaves"
        ],
        "summary": "It is odd that chess grandmasters often disagree in their analysis of positions, sometimes even of simple ones, and that a grandmaster can hold his own against an powerful analytic machine such as Deep Blue. The fact that there must exist pure winning strategies for chess is used to construct a control strategy function. It is then shown that chess strategy is equivalent to an autonomous system of differential equations, and conjectured that the system is chaotic. If true the conjecture would explain the forenamed peculiarities and would also imply that there cannot exist a static evaluator for chess.",
        "published": "1998-08-21T19:13:51Z",
        "link": "http://arxiv.org/abs/cs/9808001v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.2.0; I.2.0"
        ]
    },
    {
        "title": "First-Order Conditional Logic Revisited",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern",
            "Daphne Koller"
        ],
        "summary": "Conditional logics play an important role in recent attempts to formulate theories of default reasoning. This paper investigates first-order conditional logic. We show that, as for first-order probabilistic logic, it is important not to confound statistical conditionals over the domain (such as ``most birds fly''), and subjective conditionals over possible worlds (such as ``I believe that Tweety is unlikely to fly''). We then address the issue of ascribing semantics to first-order conditional logic. As in the propositional case, there are many possible semantics. To study the problem in a coherent way, we use plausibility structures. These provide us with a general framework in which many of the standard approaches can be embedded. We show that while these standard approaches are all the same at the propositional level, they are significantly different in the context of a first-order language. Furthermore, we show that plausibilities provide the most natural extension of conditional logic to the first-order case: We provide a sound and complete axiomatization that contains only the KLM properties and standard axioms of first-order modal logic. We show that most of the other approaches have additional properties, which result in an inappropriate treatment of an infinitary version of the lottery paradox.",
        "published": "1998-08-28T00:16:49Z",
        "link": "http://arxiv.org/abs/cs/9808005v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Set-Theoretic Completeness for Epistemic and Conditional Logic",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "The standard approach to logic in the literature in philosophy and mathematics, which has also been adopted in computer science, is to define a language (the syntax), an appropriate class of models together with an interpretation of formulas in the language (the semantics), a collection of axioms and rules of inference characterizing reasoning (the proof theory), and then relate the proof theory to the semantics via soundness and completeness results. Here we consider an approach that is more common in the economics literature, which works purely at the semantic, set-theoretic level. We provide set-theoretic completeness results for a number of epistemic and conditional logics, and contrast the expressive power of the syntactic and set-theoretic approaches",
        "published": "1998-08-28T00:39:47Z",
        "link": "http://arxiv.org/abs/cs/9808006v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Plausibility Measures and Default Reasoning",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "We introduce a new approach to modeling uncertainty based on plausibility measures. This approach is easily seen to generalize other approaches to modeling uncertainty, such as probability measures, belief functions, and possibility measures. We focus on one application of plausibility measures in this paper: default reasoning. In recent years, a number of different semantics for defaults have been proposed, such as preferential structures, $\\epsilon$-semantics, possibilistic structures, and $\\kappa$-rankings, that have been shown to be characterized by the same set of axioms, known as the KLM properties. While this was viewed as a surprise, we show here that it is almost inevitable. In the framework of plausibility measures, we can give a necessary condition for the KLM axioms to be sound, and an additional condition necessary and sufficient to ensure that the KLM axioms are complete. This additional condition is so weak that it is almost always met whenever the axioms are sound. In particular, it is easily seen to hold for all the proposals made in the literature.",
        "published": "1998-08-29T00:12:30Z",
        "link": "http://arxiv.org/abs/cs/9808007v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Reasoning about Noisy Sensors and Effectors in the Situation Calculus",
        "authors": [
            "Fahiem Bacchus",
            "Joseph Y. Halpern",
            "Hector J. Levesque"
        ],
        "summary": "Agents interacting with an incompletely known world need to be able to reason about the effects of their actions, and to gain further information about that world they need to use sensors of some sort. Unfortunately, both the effects of actions and the information returned from sensors are subject to error. To cope with such uncertainties, the agent can maintain probabilistic beliefs about the state of the world. With probabilistic beliefs the agent will be able to quantify the likelihood of the various outcomes of its actions and is better able to utilize the information gathered from its error-prone actions and sensors. In this paper, we present a model in which we can reason about an agent's probabilistic degrees of belief and the manner in which these beliefs change as various actions are executed. We build on a general logical theory of action developed by Reiter and others, formalized in the situation calculus. We propose a simple axiomatization that captures an agent's state of belief and the manner in which these beliefs change when actions are executed. Our model displays a number of intuitively reasonable properties.",
        "published": "1998-09-09T22:28:32Z",
        "link": "http://arxiv.org/abs/cs/9809013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "Stable models and an alternative logic programming paradigm",
        "authors": [
            "Victor W. Marek",
            "Miroslaw Truszczynski"
        ],
        "summary": "In this paper we reexamine the place and role of stable model semantics in logic programming and contrast it with a least Herbrand model approach to Horn programs. We demonstrate that inherent features of stable model semantics naturally lead to a logic programming system that offers an interesting alternative to more traditional logic programming styles of Horn logic programming, stratified logic programming and logic programming with well-founded semantics. The proposed approach is based on the interpretation of program clauses as constraints. In this setting programs do not describe a single intended model, but a family of stable models. These stable models encode solutions to the constraint satisfaction problem described by the program. Our approach imposes restrictions on the syntax of logic programs. In particular, function symbols are eliminated from the language. We argue that the resulting logic programming system is well-attuned to problems in the class NP, has a well-defined domain of applications, and an emerging methodology of programming. We point out that what makes the whole approach viable is recent progress in implementations of algorithms to compute stable models of propositional logic programs.",
        "published": "1998-09-18T20:34:59Z",
        "link": "http://arxiv.org/abs/cs/9809032v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3, I.2.4"
        ]
    },
    {
        "title": "Semantics and Conversations for an Agent Communication Language",
        "authors": [
            "Yannis Labrou",
            "Tim Finin"
        ],
        "summary": "We address the issues of semantics and conversations for agent communication languages and the Knowledge Query Manipulation Language (KQML) in particular. Based on ideas from speech act theory, we present a semantic description for KQML that associates ``cognitive'' states of the agent with the use of the language's primitives (performatives). We have used this approach to describe the semantics for the whole set of reserved KQML performatives. Building on the semantics, we devise the conversation policies, i.e., a formal description of how KQML performatives may be combined into KQML exchanges (conversations), using a Definite Clause Grammar. Our research offers methods for a speech act theory-based semantic description of a language of communication acts and for the specification of the protocols associated with these acts. Languages of communication acts address the issue of communication among software applications at a level of abstraction that is useful to the emerging software agents paradigm.",
        "published": "1998-09-18T21:41:18Z",
        "link": "http://arxiv.org/abs/cs/9809034v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Learning Nested Agent Models in an Information Economy",
        "authors": [
            "Jose M. Vidal",
            "Edmund H. Durfee"
        ],
        "summary": "We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents, and a description of the forms of knowledge required. The agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models. Our results show, among other lessons, how savvy buyers can avoid being ``cheated'' by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior.",
        "published": "1998-09-26T17:43:36Z",
        "link": "http://arxiv.org/abs/cs/9809108v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I 2.11"
        ]
    },
    {
        "title": "Similarity-Based Models of Word Cooccurrence Probabilities",
        "authors": [
            "Ido Dagan",
            "Lillian Lee",
            "Fernando C. N. Pereira"
        ],
        "summary": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words.   We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.   We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.",
        "published": "1998-09-27T18:42:51Z",
        "link": "http://arxiv.org/abs/cs/9809110v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "Using Local Optimality Criteria for Efficient Information Retrieval with   Redundant Information Filters",
        "authors": [
            "Neil C. Rowe"
        ],
        "summary": "We consider information retrieval when the data, for instance multimedia, is coputationally expensive to fetch. Our approach uses \"information filters\" to considerably narrow the universe of possiblities before retrieval. We are especially interested in redundant information filters that save time over more general but more costly filters. Efficient retrieval requires that decision must be made about the necessity, order, and concurrent processing of proposed filters (an \"execution plan\"). We develop simple polynomial-time local criteria for optimal execution plans, and show that most forms of concurrency are suboptimal with information filters. Although the general problem of finding an optimal execution plan is likely exponential in the number of filters, we show experimentally that our local optimality criteria, used in a polynomial-time algorithm, nearly always find the global optimum with 15 filters or less, a sufficient number of filters for most applications. Our methods do not require special hardware and avoid the high processor idleness that is characteristic of massive parallelism solutions to this problem. We apply our ideas to an important application, information retrieval of cpationed data using natural-language understanding, a problem for which the natural-language processing can be the bottleneck if not implemented well.",
        "published": "1998-09-29T21:55:20Z",
        "link": "http://arxiv.org/abs/cs/9809121v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.3.3"
        ]
    },
    {
        "title": "Anytime Coalition Structure Generation with Worst Case Guarantees",
        "authors": [
            "Tuomas Sandholm",
            "Kate Larson",
            "Martin Andersson",
            "Onn Shehory",
            "Fernando Tohme"
        ],
        "summary": "Coalition formation is a key topic in multiagent systems. One would prefer a coalition structure that maximizes the sum of the values of the coalitions, but often the number of coalition structures is too large to allow exhaustive search for the optimal one. But then, can the coalition structure found via a partial search be guaranteed to be within a bound from optimum? We show that none of the previous coalition structure generation algorithms can establish any bound because they search fewer nodes than a threshold that we show necessary for establishing a bound. We present an algorithm that establishes a tight bound within this minimal amount of search, and show that any other algorithm would have to search strictly more. The fraction of nodes needed to be searched approaches zero as the number of agents grows. If additional time remains, our anytime algorithm searches further, and establishes a progressively lower tight bound. Surprisingly, just searching one more node drops the bound in half. As desired, our algorithm lowers the bound rapidly early on, and exhibits diminishing returns to computation. It also drastically outperforms its obvious contenders. Finally, we show how to distribute the desired search across self-interested manipulative agents.",
        "published": "1998-10-05T16:08:41Z",
        "link": "http://arxiv.org/abs/cs/9810005v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Relaxation in graph coloring and satisfiability problems",
        "authors": [
            "Pontus Svenson",
            "Mats G. Nordahl"
        ],
        "summary": "Using T=0 Monte Carlo simulation, we study the relaxation of graph coloring (K-COL) and satisfiability (K-SAT), two hard problems that have recently been shown to possess a phase transition in solvability as a parameter is varied. A change from exponentially fast to power law relaxation, and a transition to freezing behavior are found. These changes take place for smaller values of the parameter than the solvability transition. Results for the coloring problem for colorable and clustered graphs and for the fraction of persistent spins for satisfiability are also presented.",
        "published": "1998-10-13T11:27:32Z",
        "link": "http://arxiv.org/abs/cond-mat/9810144v2",
        "categories": [
            "cond-mat.dis-nn",
            "cs.AI"
        ]
    },
    {
        "title": "SYNERGY: A Linear Planner Based on Genetic Programming",
        "authors": [
            "Ion Muslea"
        ],
        "summary": "In this paper we describe SYNERGY, which is a highly parallelizable, linear planning system that is based on the genetic programming paradigm. Rather than reasoning about the world it is planning for, SYNERGY uses artificial selection, recombination and fitness measure to generate linear plans that solve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase problem and a few variants of the robot navigation problem), and the experimental results show that our planner is capable of handling problem instances that are one to two orders of magnitude larger than the ones solved by UCPOP. In order to facilitate the search reduction and to enhance the expressive power of SYNERGY, we also propose two major extensions to our planning system: a formalism for using hierarchical planning operators, and a framework for planning in dynamic environments.",
        "published": "1998-10-16T22:11:35Z",
        "link": "http://arxiv.org/abs/cs/9810016v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "A Proof Theoretic View of Constraint Programming",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We provide here a proof theoretic account of constraint programming that attempts to capture the essential ingredients of this programming style. We exemplify it by presenting proof rules for linear constraints over interval domains, and illustrate their use by analyzing the constraint propagation process for the {\\tt SEND + MORE = MONEY} puzzle. We also show how this approach allows one to build new constraint solvers.",
        "published": "1998-10-20T11:23:05Z",
        "link": "http://arxiv.org/abs/cs/9810018v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "F.4.1;I.2.3;D.1.0"
        ]
    },
    {
        "title": "Computational Geometry Column 33",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "Several recent SIGGRAPH papers on surface simplification are described.",
        "published": "1998-10-22T20:44:35Z",
        "link": "http://arxiv.org/abs/cs/9810020v1",
        "categories": [
            "cs.CG",
            "cs.AI",
            "cs.GR",
            "F.2.2;I.3"
        ]
    },
    {
        "title": "The Essence of Constraint Propagation",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We show that several constraint propagation algorithms (also called (local) consistency, consistency enforcing, Waltz, filtering or narrowing algorithms) are instances of algorithms that deal with chaotic iteration. To this end we propose a simple abstract framework that allows us to classify and compare these algorithms and to establish in a uniform way their basic properties.",
        "published": "1998-11-13T13:04:02Z",
        "link": "http://arxiv.org/abs/cs/9811024v1",
        "categories": [
            "cs.AI",
            "I.1.2; I.2.2"
        ]
    },
    {
        "title": "A Human - machine interface for teleoperation of arm manipulators in a   complex environment",
        "authors": [
            "I. Ivanisevic",
            "V. Lumelsky"
        ],
        "summary": "This paper discusses the feasibility of using configuration space (C-space) as a means of visualization and control in operator-guided real-time motion of a robot arm manipulator. The motivation is to improve performance of the human operator in tasks involving the manipulator motion in an environment with obstacles. Unlike some other motion planning tasks, operators are known to make expensive mistakes in such tasks, even in a simpler two-dimensional case. They have difficulty learning better procedures and their performance improves very little with practice. Using an example of a two-dimensional arm manipulator, we show that translating the problem into C-space improves the operator performance rather remarkably, on the order of magnitude compared to the usual work space control. An interface that makes the transfer possible is described, and an example of its use in a virtual environment is shown.",
        "published": "1998-11-20T21:06:07Z",
        "link": "http://arxiv.org/abs/cs/9811029v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "Name Strategy: Its Existence and Implications",
        "authors": [
            "Mark D. Roberts"
        ],
        "summary": "It is argued that colour name strategy, object name strategy, and chunking strategy in memory are all aspects of the same general phenomena, called stereotyping. It is pointed out that the Berlin-Kay universal partial ordering of colours and the frequency of traffic accidents classified by colour are surprisingly similar. Some consequences of the existence of a name strategy for the philosophy of language and mathematics are discussed. It is argued that real valued quantities occur {\\it ab initio}. The implication of real valued truth quantities is that the {\\bf Continuum Hypothesis} of pure mathematics is side-stepped. The existence of name strategy shows that thought/sememes and talk/phonemes can be separate, and this vindicates the assumption of thought occurring before talk used in psycholinguistic speech production models.",
        "published": "1998-12-04T12:28:19Z",
        "link": "http://arxiv.org/abs/cs/9812004v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "math.HO",
            "I.2.6;J.4;I.2.7"
        ]
    },
    {
        "title": "Towards a computational theory of human daydreaming",
        "authors": [
            "Erik T. Mueller",
            "Michael G. Dyer"
        ],
        "summary": "This paper examines the phenomenon of daydreaming: spontaneously recalling or imagining personal or vicarious experiences in the past or future. The following important roles of daydreaming in human cognition are postulated: plan preparation and rehearsal, learning from failures and successes, support for processes of creativity, emotion regulation, and motivation.   A computational theory of daydreaming and its implementation as the program DAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based on relaxed planning, 2) a dynamic episodic memory of experiences used by the scenario generator, 3) a collection of personal goals and control goals which guide the scenario generator, 4) an emotion component in which daydreams initiate, and are initiated by, emotional states arising from goal outcomes, and 5) domain knowledge of interpersonal relations and common everyday occurrences.   The role of emotions and control goals in daydreaming is discussed. Four control goals commonly used in guiding daydreaming are presented: rationalization, failure/success reversal, revenge, and preparation. The role of episodic memory in daydreaming is considered, including how daydreamed information is incorporated into memory and later used. An initial version of DAYDREAMER which produces several daydreams (in English) is currently running.",
        "published": "1998-12-10T16:29:07Z",
        "link": "http://arxiv.org/abs/cs/9812010v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "A reusable iterative optimization software library to solve   combinatorial problems with approximate reasoning",
        "authors": [
            "Andreas Raggl",
            "Wolfgang Slany"
        ],
        "summary": "Real world combinatorial optimization problems such as scheduling are typically too complex to solve with exact methods. Additionally, the problems often have to observe vaguely specified constraints of different importance, the available data may be uncertain, and compromises between antagonistic criteria may be necessary. We present a combination of approximate reasoning based constraints and iterative optimization based heuristics that help to model and solve such problems in a framework of C++ software libraries called StarFLIP++. While initially developed to schedule continuous caster units in steel plants, we present in this paper results from reusing the library components in a shift scheduling system for the workforce of an industrial production plant.",
        "published": "1998-12-15T21:45:15Z",
        "link": "http://arxiv.org/abs/cs/9812017v1",
        "categories": [
            "cs.AI",
            "I.2.8; I.2.1; J.6; I.2.4; F.2.2"
        ]
    },
    {
        "title": "Hypertree Decompositions and Tractable Queries",
        "authors": [
            "G. Gottlob",
            "N. Leone",
            "F. Scarcello"
        ],
        "summary": "Several important decision problems on conjunctive queries (CQs) are NP-complete in general but become tractable, and actually highly parallelizable, if restricted to acyclic or nearly acyclic queries. Examples are the evaluation of Boolean CQs and query containment. These problems were shown tractable for conjunctive queries of bounded treewidth and of bounded degree of cyclicity. The so far most general concept of nearly acyclic queries was the notion of queries of bounded query-width introduced by Chekuri and Rajaraman (1997). While CQs of bounded query width are tractable, it remained unclear whether such queries are efficiently recognizable. Chekuri and Rajaraman stated as an open problem whether for each constant k it can be determined in polynomial time if a query has query width less than or equal to k. We give a negative answer by proving this problem NP-complete (specifically, for k=4). In order to circumvent this difficulty, we introduce the new concept of hypertree decomposition of a query and the corresponding notion of hypertree width. We prove: (a) for each k, the class of queries with query width bounded by k is properly contained in the class of queries whose hypertree width is bounded by k; (b) unlike query width, constant hypertree-width is efficiently recognizable; (c) Boolean queries of constant hypertree width can be efficiently evaluated.",
        "published": "1998-12-28T12:30:50Z",
        "link": "http://arxiv.org/abs/cs/9812022v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "F.2.2; H.2.4; I.2.8; G.2.2"
        ]
    },
    {
        "title": "Similarity-Based Models of Word Cooccurrence Probabilities",
        "authors": [
            "Ido Dagan",
            "Lillian Lee",
            "Fernando C. N. Pereira"
        ],
        "summary": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words.   We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.   We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.",
        "published": "1998-09-27T18:42:51Z",
        "link": "http://arxiv.org/abs/cs/9809110v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "Evolution of Neural Networks to Play the Game of Dots-and-Boxes",
        "authors": [
            "Lex Weaver",
            "Terry Bossomaier"
        ],
        "summary": "Dots-and-Boxes is a child's game which remains analytically unsolved. We implement and evolve artificial neural networks to play this game, evaluating them against simple heuristic players. Our networks do not evaluate or predict the final outcome of the game, but rather recommend moves at each stage. Superior generalisation of play by co-evolved populations is found, and a comparison made with networks trained by back-propagation using simple heuristics as an oracle.",
        "published": "1998-09-28T03:48:22Z",
        "link": "http://arxiv.org/abs/cs/9809111v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Practical algorithms for on-line sampling",
        "authors": [
            "Carlos Domingo",
            "Ricard Gavalda",
            "Osamu Watanabe"
        ],
        "summary": "One of the core applications of machine learning to knowledge discovery consists on building a function (a hypothesis) from a given amount of data (for instance a decision tree or a neural network) such that we can use it afterwards to predict new instances of the data. In this paper, we focus on a particular situation where we assume that the hypothesis we want to use for prediction is very simple, and thus, the hypotheses class is of feasible size. We study the problem of how to determine which of the hypotheses in the class is almost the best one. We present two on-line sampling algorithms for selecting hypotheses, give theoretical bounds on the number of necessary examples, and analize them exprimentally. We compare them with the simple batch sampling approach commonly used and show that in most of the situations our algorithms use much fewer number of examples.",
        "published": "1998-09-30T03:44:08Z",
        "link": "http://arxiv.org/abs/cs/9809122v1",
        "categories": [
            "cs.LG",
            "cs.DS",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "A Winnow-Based Approach to Context-Sensitive Spelling Correction",
        "authors": [
            "Andrew R. Golding",
            "Dan Roth"
        ],
        "summary": "A large class of machine-learning problems in natural language require the characterization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts refer to only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good theoretical properties. We present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting \"to\" for \"too\", \"casual\" for \"causal\", etc. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned condition; (2) When compared with other systems in the literature, WinSpell exhibits the highest performance; (3) The primary reason that WinSpell outperforms BaySpell is that WinSpell learns a better linear separator; (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set.",
        "published": "1998-10-31T19:33:50Z",
        "link": "http://arxiv.org/abs/cs/9811003v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Machine Learning of Generic and User-Focused Summarization",
        "authors": [
            "Inderjeet Mani",
            "Eric Bloedorn"
        ],
        "summary": "A key problem in text summarization is finding a salience function which determines what information in the source should be included in the summary. This paper describes the use of machine learning on a training corpus of documents and their abstracts to discover salience functions which describe what combination of features is optimal for a given summarization task. The method addresses both \"generic\" and user-focused summaries.",
        "published": "1998-11-02T18:57:23Z",
        "link": "http://arxiv.org/abs/cs/9811006v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Learning to Resolve Natural Language Ambiguities: A Unified Approach",
        "authors": [
            "Dan Roth"
        ],
        "summary": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be re-cast as learning linear separators in the feature space. Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.   We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.   In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.",
        "published": "1998-11-03T21:14:32Z",
        "link": "http://arxiv.org/abs/cs/9811010v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6 I.2.7"
        ]
    },
    {
        "title": "Forgetting Exceptions is Harmful in Language Learning",
        "authors": [
            "Walter Daelemans",
            "Antal van den Bosch",
            "Jakub Zavrel"
        ],
        "summary": "We show that in language learning, contrary to received wisdom, keeping exceptional training instances in memory can be beneficial for generalization accuracy. We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks: grapheme-to-phoneme conversion, part-of-speech tagging, prepositional-phrase attachment, and base noun phrase chunking. In a first series of experiments we combine memory-based learning with training set editing techniques, in which instances are edited based on their typicality and class prediction strength. Results show that editing exceptional instances (with low typicality or low class prediction strength) tends to harm generalization accuracy. In a second series of experiments we compare memory-based learning and decision-tree learning methods on the same selection of tasks, and find that decision-tree learning often performs worse than memory-based learning. Moreover, the decrease in performance can be linked to the degree of abstraction from exceptions (i.e., pruning or eagerness). We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms.",
        "published": "1998-12-22T16:33:19Z",
        "link": "http://arxiv.org/abs/cs/9812021v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Quantum Lower Bounds by Polynomials",
        "authors": [
            "Robert Beals",
            "Harry Buhrman",
            "Richard Cleve",
            "Michele Mosca",
            "Ronald de Wolf"
        ],
        "summary": "We examine the number T of queries that a quantum network requires to compute several Boolean functions on {0,1}^N in the black-box model. We show that, in the black-box model, the exponential quantum speed-up obtained for partial functions (i.e. problems involving a promise on the input) by Deutsch and Jozsa and by Simon cannot be obtained for any total function: if a quantum algorithm computes some total Boolean function f with bounded-error using T black-box queries then there is a classical deterministic algorithm that computes f exactly with O(T^6) queries.   We also give asymptotically tight characterizations of T for all symmetric f in the exact, zero-error, and bounded-error settings. Finally, we give new precise bounds for AND, OR, and PARITY. Our results are a quantum extension of the so-called polynomial method, which has been successfully applied in classical complexity theory, and also a quantum extension of results by Nisan about a polynomial relationship between randomized and deterministic decision tree complexity.",
        "published": "1998-02-18T17:41:12Z",
        "link": "http://arxiv.org/abs/quant-ph/9802049v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "1-way quantum finite automata: strengths, weaknesses and generalizations",
        "authors": [
            "A. Ambainis",
            "R. Freivalds"
        ],
        "summary": "We study 1-way quantum finite automata (QFAs). First, we compare them with their classical counterparts. We show that, if an automaton is required to give the correct answer with a large probability (over 0.98), then the power of 1-way QFAs is equal to the power of 1-way reversible automata. However, quantum automata giving the correct answer with smaller probabilities are more powerful than reversible automata.   Second, we show that 1-way QFAs can be very space-efficient. Namely, we construct a 1-way QFA which is exponentially smaller than any equivalent classical (even randomized) finite automaton. This construction may be useful for design of other space-efficient quantum algorithms.   Third, we consider several generalizations of 1-way QFAs. Here, our goal is to find a model which is more powerful than 1-way QFAs keeping the quantum part as simple as possible.",
        "published": "1998-02-25T00:34:35Z",
        "link": "http://arxiv.org/abs/quant-ph/9802062v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Dense Quantum Coding and a Lower Bound for 1-way Quantum Automata",
        "authors": [
            "Andris Ambainis",
            "Ashwin Nayak",
            "Amnon Ta-Shma",
            "Umesh Vazirani"
        ],
        "summary": "We consider the possibility of encoding m classical bits into much fewer n quantum bits so that an arbitrary bit from the original m bits can be recovered with a good probability, and we show that non-trivial quantum encodings exist that have no classical counterparts. On the other hand, we show that quantum encodings cannot be much more succint as compared to classical encodings, and we provide a lower bound on such quantum encodings. Finally, using this lower bound, we prove an exponential lower bound on the size of 1-way quantum finite automata for a family of languages accepted by linear sized deterministic finite automata.",
        "published": "1998-04-18T00:39:22Z",
        "link": "http://arxiv.org/abs/quant-ph/9804043v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The quantum query complexity of approximating the median and related   statistics",
        "authors": [
            "Ashwin Nayak",
            "Felix Wu"
        ],
        "summary": "Let X = (x_0,...,x_{n-1})$ be a sequence of n numbers. For \\epsilon > 0, we say that x_i is an \\epsilon-approximate median if the number of elements strictly less than x_i, and the number of elements strictly greater than x_i are each less than (1+\\epsilon)n/2. We consider the quantum query complexity of computing an \\epsilon-approximate median, given the sequence X as an oracle. We prove a lower bound of \\Omega(\\min{{1/\\epsilon},n}) queries for any quantum algorithm that computes an \\epsilon-approximate median with any constant probability greater than 1/2. We also show how an \\epsilon-approximate median may be computed with O({1/\\epsilon}\\log({1\\/\\epsilon}) \\log\\log({1/\\epsilon})) oracle queries, which represents an improvement over an earlier algorithm due to Grover. Thus, the lower bound we obtain is essentially optimal. The upper and the lower bound both hold in the comparison tree model as well.   Our lower bound result is an application of the polynomial paradigm recently introduced to quantum complexity theory by Beals et al. The main ingredient in the proof is a polynomial degree lower bound for real multilinear polynomials that ``approximate'' symmetric partial boolean functions. The degree bound extends a result of Paturi and also immediately yields lower bounds for the problems of approximating the kth-smallest element, approximating the mean of a sequence of numbers, and that of approximately counting the number of ones of a boolean function. All bounds obtained come within polylogarithmic factors of the optimal (as we show by presenting algorithms where no such optimal or near optimal algorithms were known), thus demonstrating the power of the polynomial method.",
        "published": "1998-04-29T02:00:07Z",
        "link": "http://arxiv.org/abs/quant-ph/9804066v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum Oracle Interrogation: Getting all information for almost half   the price",
        "authors": [
            "Wim van Dam"
        ],
        "summary": "Consider a quantum computer in combination with a binary oracle of domain size N. It is shown how N/2+sqrt(N) calls to the oracle are sufficient to guess the whole content of the oracle (being an N bit string) with probability greater than 95%. This contrasts the power of classical computers which would require N calls to achieve the same task. From this result it follows that any function with the N bits of the oracle as input can be calculated using N/2+sqrt(N) queries if we allow a small probability of error. It is also shown that this error probability can be made arbitrary small by using N/2+O(sqrt(N)) oracle queries.   In the second part of the article `approximate interrogation' is considered. This is when only a certain fraction of the N oracle bits are requested. Also for this scenario does the quantum algorithm outperform the classical protocols. An example is given where a quantum procedure with N/10 queries returns a string of which 80% of the bits are correct. Any classical protocol would need 6N/10 queries to establish such a correctness ratio.",
        "published": "1998-05-04T22:46:43Z",
        "link": "http://arxiv.org/abs/quant-ph/9805006v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Two Classical Queries versus One Quantum Query",
        "authors": [
            "Wim van Dam"
        ],
        "summary": "In this note we study the power of so called query-limited computers. We compare the strength of a classical computer that is allowed to ask two questions to an NP-oracle with the strength of a quantum computer that is allowed only one such query. It is shown that any decision problem that requires two parallel (non-adaptive) SAT-queries on a classical computer can also be solved exactly by a quantum computer using only one SAT-oracle call, where both computations have polynomial time-complexity. Such a simulation is generally believed to be impossible for a one-query classical computer. The reduction also does not hold if we replace the SAT-oracle by a general black-box. This result gives therefore an example of how a quantum computer is probably more powerful than a classical computer. It also highlights the potential differences between quantum complexity results for general oracles when compared to results for more structured tasks like the SAT-problem.",
        "published": "1998-06-27T02:53:13Z",
        "link": "http://arxiv.org/abs/quant-ph/9806090v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Chess Pure Strategies are Probably Chaotic",
        "authors": [
            "M. Chaves"
        ],
        "summary": "It is odd that chess grandmasters often disagree in their analysis of positions, sometimes even of simple ones, and that a grandmaster can hold his own against an powerful analytic machine such as Deep Blue. The fact that there must exist pure winning strategies for chess is used to construct a control strategy function. It is then shown that chess strategy is equivalent to an autonomous system of differential equations, and conjectured that the system is chaotic. If true the conjecture would explain the forenamed peculiarities and would also imply that there cannot exist a static evaluator for chess.",
        "published": "1998-08-21T19:13:51Z",
        "link": "http://arxiv.org/abs/cs/9808001v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.2.0; I.2.0"
        ]
    },
    {
        "title": "Creating Strong Total Commutative Associative Complexity-Theoretic   One-Way Functions from Any Complexity-Theoretic One-Way Function",
        "authors": [
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "Rabi and Sherman [RS97] presented novel digital signature and unauthenticated secret-key agreement protocols, developed by themselves and by Rivest and Sherman. These protocols use ``strong,'' total, commutative (in the case of multi-party secret-key agreement), associative one-way functions as their key building blocks. Though Rabi and Sherman did prove that associative one-way functions exist if $\\p \\neq \\np$, they left as an open question whether any natural complexity-theoretic assumption is sufficient to ensure the existence of ``strong,'' total, commutative, associative one-way functions. In this paper, we prove that if $\\p \\neq \\np$ then ``strong,'' total, commutative, associative one-way functions exist.",
        "published": "1998-08-23T18:22:06Z",
        "link": "http://arxiv.org/abs/cs/9808003v1",
        "categories": [
            "cs.CC",
            "cs.CR",
            "F.1.3; E.3"
        ]
    },
    {
        "title": "Downward Collapse from a Weaker Hypothesis",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "Hemaspaandra et al. proved that, for $m > 0$ and $0 < i < k - 1$: if $\\Sigma_i^p \\BoldfaceDelta DIFF_m(\\Sigma_k^p)$ is closed under complementation, then $DIFF_m(\\Sigma_k^p) = coDIFF_m(\\Sigma_k^p)$. This sharply asymmetric result fails to apply to the case in which the hypothesis is weakened by allowing the $\\Sigma_i^p$ to be replaced by any class in its difference hierarchy. We so extend the result by proving that, for $s,m > 0$ and $0 < i < k - 1$: if $DIFF_s(\\Sigma_i^p) \\BoldfaceDelta DIFF_m(\\Sigma_k^p)$ is closed under complementation, then $DIFF_m(\\Sigma_k^p) = coDIFF_m(\\Sigma_k^p)$.",
        "published": "1998-08-24T21:37:54Z",
        "link": "http://arxiv.org/abs/cs/9808002v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Immunity and Simplicity for Exact Counting and Other Counting Classes",
        "authors": [
            "Joerg Rothe"
        ],
        "summary": "Ko [RAIRO 24, 1990] and Bruschi [TCS 102, 1992] showed that in some relativized world, PSPACE (in fact, ParityP) contains a set that is immune to the polynomial hierarchy (PH). In this paper, we study and settle the question of (relativized) separations with immunity for PH and the counting classes PP, C_{=}P, and ParityP in all possible pairwise combinations. Our main result is that there is an oracle A relative to which C_{=}P contains a set that is immune to BPP^{ParityP}. In particular, this C_{=}P^A set is immune to PH^{A} and ParityP^{A}. Strengthening results of Tor\\'{a}n [J.ACM 38, 1991] and Green [IPL 37, 1991], we also show that, in suitable relativizations, NP contains a C_{=}P-immune set, and ParityP contains a PP^{PH}-immune set. This implies the existence of a C_{=}P^{B}-simple set for some oracle B, which extends results of Balc\\'{a}zar et al. [SIAM J.Comp. 14, 1985; RAIRO 22, 1988] and provides the first example of a simple set in a class not known to be contained in PH. Our proof technique requires a circuit lower bound for ``exact counting'' that is derived from Razborov's [Mat. Zametki 41, 1987] lower bound for majority.",
        "published": "1998-09-01T11:16:45Z",
        "link": "http://arxiv.org/abs/cs/9809001v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "Tally NP Sets and Easy Census Functions",
        "authors": [
            "Judy Goldsmith",
            "Mitsunori Ogihara",
            "Joerg Rothe"
        ],
        "summary": "We study the question of whether every P set has an easy (i.e., polynomial-time computable) census function. We characterize this question in terms of unlikely collapses of language and function classes such as the containment of #P_1 in FP, where #P_1 is the class of functions that count the witnesses for tally NP sets. We prove that every #P_{1}^{PH} function can be computed in FP^{#P_{1}^{#P_{1}}}. Consequently, every P set has an easy census function if and only if every set in the polynomial hierarchy does. We show that the assumption of #P_1 being contained in FP implies P = BPP and that PH is contained in MOD_{k}P for each k \\geq 2, which provides further evidence that not all sets in P have an easy census function. We also relate a set's property of having an easy census function to other well-studied properties of sets, such as rankability and scalability (the closure of the rankable sets under P-isomorphisms). Finally, we prove that it is no more likely that the census function of any set in P can be approximated (more precisely, can be n^{\\alpha}-enumerated in time n^{\\beta} for fixed \\alpha and \\beta) than that it can be precisely computed in polynomial time.",
        "published": "1998-09-01T12:15:55Z",
        "link": "http://arxiv.org/abs/cs/9809002v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Uncomputably Large Integral Points on Algebraic Plane Curves?",
        "authors": [
            "J. Maurice Rojas"
        ],
        "summary": "We show that the decidability of an amplification of Hilbert's Tenth Problem in three variables implies the existence of uncomputably large integral points on certain algebraic curves. We obtain this as a corollary of a new positive complexity result: the Diophantine prefixes EAE and EEAE are generically decidable. This means, taking the former prefix as an example, that we give a precise geometric classification of those polynomials f in Z[v,x,y] for which the question...   ``Does there exists a v in N such that for all x in N, there exists a y in N with f(v,x,y)=0?''   ...may be undecidable, and we show that this set of polynomials is quite small in a rigourous sense. (The decidability of EAE was previously an open question.) The analogous result for the prefix EEAE is even stronger. We thus obtain a connection between the decidability of certain Diophantine problems, height bounds for points on curves, and the geometry of certain complex surfaces and 3-folds.",
        "published": "1998-09-02T08:54:06Z",
        "link": "http://arxiv.org/abs/math/9809009v1",
        "categories": [
            "math.NT",
            "cs.CC",
            "cs.SC",
            "math.AG",
            "math.LO",
            "03D35, 11D72, 14G99; 11G30, 14H99, 14J26"
        ]
    },
    {
        "title": "The Complexity of Planar Counting Problems",
        "authors": [
            "Harry B. Hunt III",
            "Madhav V. Marathe",
            "Venkatesh Radhakrishnan",
            "Richard E. Stearns"
        ],
        "summary": "We prove the #P-hardness of the counting problems associated with various satisfiability, graph and combinatorial problems, when restricted to planar instances. These problems include \\begin{romannum} \\item[{}] {\\sc 3Sat, 1-3Sat, 1-Ex3Sat, Minimum Vertex Cover, Minimum Dominating Set, Minimum Feedback Vertex Set, X3C, Partition Into Triangles, and Clique Cover.} \\end{romannum} We also prove the {\\sf NP}-completeness of the {\\sc Ambiguous Satisfiability} problems \\cite{Sa80} and the {\\sf D$^P$}-completeness (with respect to random polynomial reducibility) of the unique satisfiability problems \\cite{VV85} associated with several of the above problems, when restricted to planar instances. Previously, very few {\\sf #P}-hardness results, no {\\sf NP}-hardness results, and no {\\sf D$^P$}-completeness results were known for counting problems, ambiguous satisfiability problems and unique satisfiability problems, respectively, when restricted to planar instances.   Assuming {\\sf P $\\neq $ NP}, one corollary of the above results is   There are no $\\epsilon$-approximation algorithms for the problems of maximizing or minimizing a linear objective function subject to a planar system of linear inequality constraints over the integers.",
        "published": "1998-09-11T17:38:19Z",
        "link": "http://arxiv.org/abs/cs/9809017v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3"
        ]
    },
    {
        "title": "Solving Degenerate Sparse Polynomial Systems Faster",
        "authors": [
            "J. Maurice Rojas"
        ],
        "summary": "Consider a system F of n polynomial equations in n unknowns, over an algebraically closed field of arbitrary characteristic. We present a fast method to find a point in every irreducible component of the zero set Z of F. Our techniques allow us to sharpen and lower prior complexity bounds for this problem by fully taking into account the monomial term structure. As a corollary of our development we also obtain new explicit formulae for the exact number of isolated roots of F and the intersection multiplicity of the positive-dimensional part of Z. Finally, we present a combinatorial construction of non-degenerate polynomial systems, with specified monomial term structure and maximally many isolated roots, which may be of independent interest.",
        "published": "1998-09-14T07:46:19Z",
        "link": "http://arxiv.org/abs/math/9809071v2",
        "categories": [
            "math.AG",
            "cs.CC",
            "cs.NA",
            "math.NA"
        ]
    },
    {
        "title": "On quantification with a finite universe",
        "authors": [
            "Saharon Shelah"
        ],
        "summary": "We consider a finite universe U (more exactly - a family U of them) and second order quantifiers Q_K, where for each U this means quantifying over a family of n(K)-place relations closed under permuting U. We define some natural orders and shed some light on the classification problem of those quantifiers.",
        "published": "1998-09-15T00:00:00Z",
        "link": "http://arxiv.org/abs/math/9809201v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "New Applications of the Incompressibility Method: Part II",
        "authors": [
            "Harry Buhrman",
            "Tao Jiang",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "The incompressibility method is an elementary yet powerful proof technique. It has been used successfully in many areas. To further demonstrate its power and elegance we exhibit new simple proofs using the incompressibility method.",
        "published": "1998-09-23T15:41:42Z",
        "link": "http://arxiv.org/abs/cs/9809060v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; G.2.1"
        ]
    },
    {
        "title": "New Applications of the Incompressibility Method: Part I",
        "authors": [
            "Tao Jiang",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "The incompressibility method is an elementary yet powerful proof technique. It has been used successfully in many areas. To further demonstrate its power and elegance we exhibit new simple proofs using the incompressibility method.",
        "published": "1998-09-23T15:43:54Z",
        "link": "http://arxiv.org/abs/cs/9809061v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; G.2.1"
        ]
    },
    {
        "title": "Approximation Algorithms for PSPACE-Hard Hierarchically and Periodically   Specified Problems",
        "authors": [
            "Madhav V. Marathe",
            "Harry B. Hunt III",
            "Richard E. Stearns",
            "Venkatesh Radhakrishnan"
        ],
        "summary": "We study the efficient approximability of basic graph and logic problems in the literature when instances are specified hierarchically as in \\cite{Le89} or are specified by 1-dimensional finite narrow periodic specifications as in \\cite{Wa93}. We show that, for most of the problems $\\Pi$ considered when specified using {\\bf k-level-restricted} hierarchical specifications or $k$-narrow periodic specifications the following holds:   \\item Let $\\rho$ be any performance guarantee of a polynomial time approximation algorithm for $\\Pi$, when instances are specified using standard specifications. Then $\\forall \\epsilon > 0$, $ \\Pi$ has a polynomial time approximation algorithm with performance guarantee $(1 + \\epsilon) \\rho$. \\item $\\Pi$ has a polynomial time approximation scheme when restricted to planar instances. \\end{romannum}   These are the first polynomial time approximation schemes for PSPACE-hard hierarchically or periodically specified problems. Since several of the problems considered are PSPACE-hard, our results provide the first examples of natural PSPACE-hard optimization problems that have polynomial time approximation schemes. This answers an open question in Condon et. al. \\cite{CF+93}.",
        "published": "1998-09-23T15:58:21Z",
        "link": "http://arxiv.org/abs/cs/9809064v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "Bicriteria Network Design Problems",
        "authors": [
            "Madhav V. Marathe",
            "R. Ravi",
            "Ravi Sundaram",
            "S. S. Ravi",
            "Daniel J. Rosenkrantz",
            "Harry B. Hunt III"
        ],
        "summary": "We study a general class of bicriteria network design problems. A generic problem in this class is as follows: Given an undirected graph and two minimization objectives (under different cost functions), with a budget specified on the first, find a <subgraph \\from a given subgraph-class that minimizes the second objective subject to the budget on the first. We consider three different criteria - the total edge cost, the diameter and the maximum degree of the network. Here, we present the first polynomial-time approximation algorithms for a large class of bicriteria network design problems for the above mentioned criteria. The following general types of results are presented.   First, we develop a framework for bicriteria problems and their approximations. Second, when the two criteria are the same %(note that the cost functions continue to be different) we present a ``black box'' parametric search technique. This black box takes in as input an (approximation) algorithm for the unicriterion situation and generates an approximation algorithm for the bicriteria case with only a constant factor loss in the performance guarantee. Third, when the two criteria are the diameter and the total edge costs we use a cluster-based approach to devise a approximation algorithms --- the solutions output violate both the criteria by a logarithmic factor. Finally, for the class of treewidth-bounded graphs, we provide pseudopolynomial-time algorithms for a number of bicriteria problems using dynamic programming. We show how these pseudopolynomial-time algorithms can be converted to fully polynomial-time approximation schemes using a scaling technique.",
        "published": "1998-09-24T17:48:18Z",
        "link": "http://arxiv.org/abs/cs/9809103v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F,2.2"
        ]
    },
    {
        "title": "The descriptive complexity approach to LOGCFL",
        "authors": [
            "Clemens Lautemann",
            "Pierre McKenzie",
            "Thomas Schwentick",
            "Heribert Vollmer"
        ],
        "summary": "Building upon the known generalized-quantifier-based first-order characterization of LOGCFL, we lay the groundwork for a deeper investigation. Specifically, we examine subclasses of LOGCFL arising from varying the arity and nesting of groupoidal quantifiers. Our work extends the elaborate theory relating monoidal quantifiers to NC1 and its subclasses. In the absence of the BIT predicate, we resolve the main issues: we show in particular that no single outermost unary groupoidal quantifier with FO can capture all the context-free languages, and we obtain the surprising result that a variant of Greibach's ``hardest context-free language'' is LOGCFL-complete under quantifier-free BIT-free projections. We then prove that FO with unary groupoidal quantifiers is strictly more expressive with the BIT predicate than without. Considering a particular groupoidal quantifier, we prove that first-order logic with majority of pairs is strictly more expressive than first-order with majority of individuals. As a technical tool of independent interest, we define the notion of an aperiodic nondeterministic finite automaton and prove that FO translations are precisely the mappings computed by single-valued aperiodic nondeterministic finite transducers.",
        "published": "1998-09-28T07:57:32Z",
        "link": "http://arxiv.org/abs/cs/9809114v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "A Generalized Quantifier Concept in Computational Complexity Theory",
        "authors": [
            "Heribert Vollmer"
        ],
        "summary": "A notion of generalized quantifier in computational complexity theory is explored and used to give a unified treatment of leaf language definability, oracle separations, type 2 operators, and circuits with monoidal gates. Relations to Lindstroem quantifiers are pointed out.",
        "published": "1998-09-28T08:20:25Z",
        "link": "http://arxiv.org/abs/cs/9809115v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "The Complexity of Computing Optimal Assignments of Generalized   Propositional Formulae",
        "authors": [
            "Steffen Reith",
            "Heribert Vollmer"
        ],
        "summary": "We consider the problems of finding the lexicographically minimal (or maximal) satisfying assignment of propositional formulae for different restricted formula classes. It turns out that for each class from our framework, the above problem is either polynomial time solvable or complete for OptP. We also consider the problem of deciding if in the optimal assignment the largest variable gets value 1. We show that this problem is either in P or P^NP complete.",
        "published": "1998-09-28T08:38:27Z",
        "link": "http://arxiv.org/abs/cs/9809116v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Hard instance generation for SAT",
        "authors": [
            "Satoshi Horie",
            "Osamu Watanabe"
        ],
        "summary": "We propose an algorithm of generating hard instances for the Satisfying Assignment Search Problem (in short, SAT). The algorithm transforms instances of the integer factorization problem into SAT instances efficiently by using the Chinese Remainder Theorem. For example, it is possible to construct SAT instances with about 5,600 variables that is as hard as factorizing 100 bit integers.",
        "published": "1998-09-28T10:50:03Z",
        "link": "http://arxiv.org/abs/cs/9809117v2",
        "categories": [
            "cs.CC",
            "F.2"
        ]
    },
    {
        "title": "The Boolean Hierarchy over Level 1/2 of the Straubing-Therien Hierarchy",
        "authors": [
            "Heinz Schmitz",
            "Klaus W. Wagner"
        ],
        "summary": "For some fixed alphabet A, a language L of A* is in the class L(1/2) of the Straubing-Therien hierarchy if and only if it can be expressed as a finite union of languages A*aA*bA*...A*cA*, where a,b,...,c are letters. The class L(1) is defined as the boolean closure of L(1/2). It is known that the classes L(1/2) and L(1) are decidable. We give a membership criterion for the single classes of the boolean hierarchy over L(1/2). From this criterion we can conclude that this boolean hierarchy is proper and that its classes are decidable.In finite model theory the latter implies the decidability of the classes of the boolean hierarchy over the class Sigma(1) of the FO(<)-logic. Moreover we prove a ``forbidden-pattern'' characterization of L(1) of the type: L is in L(1) if and only if a certain pattern does not appear in the transition graph of a deterministic finite automaton accepting L. We discuss complexity theoretical consequences of our results.",
        "published": "1998-09-28T12:35:34Z",
        "link": "http://arxiv.org/abs/cs/9809118v1",
        "categories": [
            "cs.CC",
            "cs.FL",
            "F.1.3; F.4.3"
        ]
    },
    {
        "title": "Writing and Editing Complexity Theory: Tales and Tools",
        "authors": [
            "Lane A. Hemaspaandra",
            "Alan L. Selman"
        ],
        "summary": "Each researcher should have a full shelf---physical or virtual---of books on writing and editing prose. Though we make no claim to any special degree of expertise, we recently edited a book of complexity theory surveys (Complexity Theory Retrospective II, Springer-Verlag, 1997), and in doing so we were brought into particularly close contact with the subject of this article, and with a number of the excellent resources available to writers and editors. In this article, we list some of these resources, and we also relate some of the adventures we had as our book moved from concept to reality.",
        "published": "1998-11-01T17:44:15Z",
        "link": "http://arxiv.org/abs/cs/9811005v1",
        "categories": [
            "cs.GL",
            "cs.CC",
            "K.3.2; F.1.0"
        ]
    },
    {
        "title": "Complexity limitations on quantum computation",
        "authors": [
            "Lance Fortnow",
            "John D. Rogers"
        ],
        "summary": "We use the powerful tools of counting complexity and generic oracles to help understand the limitations of the complexity of quantum computation. We show several results for the probabilistic quantum class BQP.   1. BQP is low for PP, i.e., PP^BQP=PP.   2. There exists a relativized world where P=BQP and the polynomial-time hierarchy is infinite.   3. There exists a relativized world where BQP does not have complete sets.   4. There exists a relativized world where P=BQP but P is not equal to UP intersect coUP and one-way functions exist. This gives a relativized answer to an open question of Simon.",
        "published": "1998-11-12T17:55:06Z",
        "link": "http://arxiv.org/abs/cs/9811023v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.1;F.1.2;F.1.3"
        ]
    },
    {
        "title": "Lower Bounds for Quantum Search and Derandomization",
        "authors": [
            "Harry Buhrman",
            "Ronald de Wolf"
        ],
        "summary": "We prove lower bounds on the error probability of a quantum algorithm for searching through an unordered list of N items, as a function of the number T of queries it makes. In particular, if T=O(sqrt{N}) then the error is lower bounded by a constant. If we want error <1/2^N then we need T=Omega(N) queries. We apply this to show that a quantum computer cannot do much better than a classical computer when amplifying the success probability of an RP-machine. A classical computer can achieve error <=1/2^k using k applications of the RP-machine, a quantum computer still needs at least ck applications for this (when treating the machine as a black-box), where c>0 is a constant independent of k. Furthermore, we prove a lower bound of Omega(sqrt{log N}/loglog N) queries for quantum bounded-error search of an ordered list of N items.",
        "published": "1998-11-18T11:49:46Z",
        "link": "http://arxiv.org/abs/quant-ph/9811046v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A note on quantum black-box complexity of almost all Boolean functions",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We show that, for almost all N-variable Boolean functions f, at least N/4-O(\\sqrt{N} log N) queries are required to compute f in quantum black-box model with bounded error.",
        "published": "1998-11-28T05:09:30Z",
        "link": "http://arxiv.org/abs/quant-ph/9811080v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum simulations of classical random walks and undirected graph   connectivity",
        "authors": [
            "John Watrous"
        ],
        "summary": "It is not currently known if quantum Turing machines can efficiently simulate probabilistic computations in the space-bounded case. In this paper we show that space-bounded quantum Turing machines can efficiently simulate a limited class of random processes: random walks on undirected graphs. By means of such simulations, it is demonstrated that the undirected graph connectivity problem for regular graphs can be solved by one-sided error quantum Turing machines that run in logspace and halt absolutely. It follows that symmetric logspace is contained in the quantum analogue of randomized logspace.",
        "published": "1998-12-11T00:06:09Z",
        "link": "http://arxiv.org/abs/cs/9812012v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.3; F.2.2; G.2.2"
        ]
    },
    {
        "title": "The Self-Organizing Symbiotic Agent",
        "authors": [
            "Babak Hodjat",
            "Makoto Amamiya"
        ],
        "summary": "In [N. A. Baas, Emergence, Hierarchies, and Hyper-structures, in C.G. Langton ed., Artificial Life III, Addison Wesley, 1994.] a general framework for the study of Emergence and hyper-structure was presented. This approach is mostly concerned with the description of such systems. In this paper we will try to bring forth a different aspect of this model we feel will be useful in the engineering of agent based solutions, namely the symbiotic approach. In this approach a self-organizing method of dividing the more complex \"main-problem\" to a hyper-structure of \"sub-problems\" with the aim of reducing complexity is desired. A description of the general problem will be given along with some instances of related work. This paper is intended to serve as an introductory challenge for general solutions to the described problem.",
        "published": "1998-12-11T20:51:38Z",
        "link": "http://arxiv.org/abs/cs/9812013v1",
        "categories": [
            "cs.NE",
            "cs.CC",
            "I.2.8; D.2.11;F.1.13; I.2.11"
        ]
    },
    {
        "title": "NQP_{C} = co-C_{=}P",
        "authors": [
            "Tomoyuki Yamakami",
            "Andrew C. Yao"
        ],
        "summary": "Adleman, DeMarrais, and Huang introduced the nondeterministic quantum polynomial-time complexity class NQP as an analogue of NP. Fortnow and Rogers implicitly showed that, when the amplitudes are rational numbers, NQP is contained in the complement of C_{=}P. Fenner, Green, Homer, and Pruim improved this result by showing that, when the amplitudes are arbitrary algebraic numbers, NQP coincides with co-C_{=}P. In this paper we prove that, even when the amplitudes are arbitrary complex numbers, NQP still remains identical to co-C_{=}P. As an immediate corollary, BQP differs from NQP when the amplitudes are unrestricted.",
        "published": "1998-12-14T22:56:28Z",
        "link": "http://arxiv.org/abs/quant-ph/9812032v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Polynomial time logic: Inability to express",
        "authors": [
            "Saharon Shelah"
        ],
        "summary": "Here we deal with the logic of [GuSh 533], which tries to capture polynomial time (for finite models). There it is proved that the logic cannot say much on models with equality only. Here we prove that it cannot say much on models for which we expect it cannot say much, like random enough graphs. This is the result of having a general criterion.",
        "published": "1998-07-15T00:00:00Z",
        "link": "http://arxiv.org/abs/math/9807179v1",
        "categories": [
            "math.LO",
            "cs.LO"
        ]
    },
    {
        "title": "First-Order Conditional Logic Revisited",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern",
            "Daphne Koller"
        ],
        "summary": "Conditional logics play an important role in recent attempts to formulate theories of default reasoning. This paper investigates first-order conditional logic. We show that, as for first-order probabilistic logic, it is important not to confound statistical conditionals over the domain (such as ``most birds fly''), and subjective conditionals over possible worlds (such as ``I believe that Tweety is unlikely to fly''). We then address the issue of ascribing semantics to first-order conditional logic. As in the propositional case, there are many possible semantics. To study the problem in a coherent way, we use plausibility structures. These provide us with a general framework in which many of the standard approaches can be embedded. We show that while these standard approaches are all the same at the propositional level, they are significantly different in the context of a first-order language. Furthermore, we show that plausibilities provide the most natural extension of conditional logic to the first-order case: We provide a sound and complete axiomatization that contains only the KLM properties and standard axioms of first-order modal logic. We show that most of the other approaches have additional properties, which result in an inappropriate treatment of an infinitary version of the lottery paradox.",
        "published": "1998-08-28T00:16:49Z",
        "link": "http://arxiv.org/abs/cs/9808005v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Set-Theoretic Completeness for Epistemic and Conditional Logic",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "The standard approach to logic in the literature in philosophy and mathematics, which has also been adopted in computer science, is to define a language (the syntax), an appropriate class of models together with an interpretation of formulas in the language (the semantics), a collection of axioms and rules of inference characterizing reasoning (the proof theory), and then relate the proof theory to the semantics via soundness and completeness results. Here we consider an approach that is more common in the economics literature, which works purely at the semantic, set-theoretic level. We provide set-theoretic completeness results for a number of epistemic and conditional logics, and contrast the expressive power of the syntactic and set-theoretic approaches",
        "published": "1998-08-28T00:39:47Z",
        "link": "http://arxiv.org/abs/cs/9808006v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Plausibility Measures and Default Reasoning",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "We introduce a new approach to modeling uncertainty based on plausibility measures. This approach is easily seen to generalize other approaches to modeling uncertainty, such as probability measures, belief functions, and possibility measures. We focus on one application of plausibility measures in this paper: default reasoning. In recent years, a number of different semantics for defaults have been proposed, such as preferential structures, $\\epsilon$-semantics, possibilistic structures, and $\\kappa$-rankings, that have been shown to be characterized by the same set of axioms, known as the KLM properties. While this was viewed as a surprise, we show here that it is almost inevitable. In the framework of plausibility measures, we can give a necessary condition for the KLM axioms to be sound, and an additional condition necessary and sufficient to ensure that the KLM axioms are complete. This additional condition is so weak that it is almost always met whenever the axioms are sound. In particular, it is easily seen to hold for all the proposals made in the literature.",
        "published": "1998-08-29T00:12:30Z",
        "link": "http://arxiv.org/abs/cs/9808007v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Common knowledge revisited",
        "authors": [
            "R. Fagin",
            "J. Y. Halpern",
            "Y. Moses",
            "M. Vardi"
        ],
        "summary": "We consider the common-knowledge paradox raised by Halpern and Moses: common knowledge is necessary for agreement and coordination, but common knowledge is unattainable in the real world because of temporal imprecision. We discuss two solutions to this paradox: (1) modeling the world with a coarser granularity, and (2) relaxing the requirements for coordination.",
        "published": "1998-09-01T21:56:39Z",
        "link": "http://arxiv.org/abs/cs/9809003v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.4.1, C.2.4"
        ]
    },
    {
        "title": "Comparing the expressive power of the Synchronous and the Asynchronous   pi-calculus",
        "authors": [
            "Catuscia Palamidessi"
        ],
        "summary": "The Asynchronous pi-calculus, as recently proposed by Boudol and, independently, by Honda and Tokoro, is a subset of the pi-calculus which contains no explicit operators for choice and output-prefixing. The communication mechanism of this calculus, however, is powerful enough to simulate output-prefixing, as shown by Boudol, and input-guarded choice, as shown recently by Nestmann and Pierce. A natural question arises, then, whether or not it is possible to embed in it the full pi-calculus. We show that this is not possible, i.e. there does not exist any uniform, parallel-preserving, translation from the pi-calculus into the asynchronous pi-calculus, up to any ``reasonable'' notion of equivalence. This result is based on the incapablity of the asynchronous pi-calculus of breaking certain symmetries possibly present in the initial communication graph. By similar arguments, we prove a separation result between the pi-calculus and CCS.",
        "published": "1998-09-02T17:40:46Z",
        "link": "http://arxiv.org/abs/cs/9809008v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3, F.3"
        ]
    },
    {
        "title": "Reasoning about Noisy Sensors and Effectors in the Situation Calculus",
        "authors": [
            "Fahiem Bacchus",
            "Joseph Y. Halpern",
            "Hector J. Levesque"
        ],
        "summary": "Agents interacting with an incompletely known world need to be able to reason about the effects of their actions, and to gain further information about that world they need to use sensors of some sort. Unfortunately, both the effects of actions and the information returned from sensors are subject to error. To cope with such uncertainties, the agent can maintain probabilistic beliefs about the state of the world. With probabilistic beliefs the agent will be able to quantify the likelihood of the various outcomes of its actions and is better able to utilize the information gathered from its error-prone actions and sensors. In this paper, we present a model in which we can reason about an agent's probabilistic degrees of belief and the manner in which these beliefs change as various actions are executed. We build on a general logical theory of action developed by Reiter and others, formalized in the situation calculus. We propose a simple axiomatization that captures an agent's state of belief and the manner in which these beliefs change when actions are executed. Our model displays a number of intuitively reasonable properties.",
        "published": "1998-09-09T22:28:32Z",
        "link": "http://arxiv.org/abs/cs/9809013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "Uniform Provability in Classical Logic",
        "authors": [
            "Gopalan Nadathur"
        ],
        "summary": "Uniform proofs are sequent calculus proofs with the following characteristic: the last step in the derivation of a complex formula at any stage in the proof is always the introduction of the top-level logical symbol of that formula. We investigate the relevance of this uniform proof notion to structuring proof search in classical logic. A logical language in whose context provability is equivalent to uniform provability admits of a goal-directed proof procedure that interprets logical symbols as search directives whose meanings are given by the corresponding inference rules. While this uniform provability property does not hold directly of classical logic, we show that it holds of a fragment of it that only excludes essentially positive occurrences of universal quantifiers under a modest, sound, modification to the set of assumptions: the addition to them of the negation of the formula being proved. We further note that all uses of the added formula can be factored into certain derived rules. The resulting proof system and the uniform provability property that holds of it are used to outline a proof procedure for classical logic. An interesting aspect of this proof procedure is that it incorporates within it previously proposed mechanisms for dealing with disjunctive information in assumptions and for handling hypotheticals. Our analysis sheds light on the relationship between these mechanisms and the notion of uniform proofs.",
        "published": "1998-09-10T15:17:05Z",
        "link": "http://arxiv.org/abs/cs/9809014v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Correspondences between Classical, Intuitionistic and Uniform   Provability",
        "authors": [
            "Gopalan Nadathur"
        ],
        "summary": "Based on an analysis of the inference rules used, we provide a characterization of the situations in which classical provability entails intuitionistic provability. We then examine the relationship of these derivability notions to uniform provability, a restriction of intuitionistic provability that embodies a special form of goal-directedness. We determine, first, the circumstances in which the former relations imply the latter. Using this result, we identify the richest versions of the so-called abstract logic programming languages in classical and intuitionistic logic. We then study the reduction of classical and, derivatively, intuitionistic provability to uniform provability via the addition to the assumption set of the negation of the formula to be proved. Our focus here is on understanding the situations in which this reduction is achieved. However, our discussions indicate the structure of a proof procedure based on the reduction, a matter also considered explicitly elsewhere.",
        "published": "1998-09-10T15:39:30Z",
        "link": "http://arxiv.org/abs/cs/9809015v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "On quantification with a finite universe",
        "authors": [
            "Saharon Shelah"
        ],
        "summary": "We consider a finite universe U (more exactly - a family U of them) and second order quantifiers Q_K, where for each U this means quantifying over a family of n(K)-place relations closed under permuting U. We define some natural orders and shed some light on the classification problem of those quantifiers.",
        "published": "1998-09-15T00:00:00Z",
        "link": "http://arxiv.org/abs/math/9809201v1",
        "categories": [
            "math.LO",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Stable models and an alternative logic programming paradigm",
        "authors": [
            "Victor W. Marek",
            "Miroslaw Truszczynski"
        ],
        "summary": "In this paper we reexamine the place and role of stable model semantics in logic programming and contrast it with a least Herbrand model approach to Horn programs. We demonstrate that inherent features of stable model semantics naturally lead to a logic programming system that offers an interesting alternative to more traditional logic programming styles of Horn logic programming, stratified logic programming and logic programming with well-founded semantics. The proposed approach is based on the interpretation of program clauses as constraints. In this setting programs do not describe a single intended model, but a family of stable models. These stable models encode solutions to the constraint satisfaction problem described by the program. Our approach imposes restrictions on the syntax of logic programs. In particular, function symbols are eliminated from the language. We argue that the resulting logic programming system is well-attuned to problems in the class NP, has a well-defined domain of applications, and an emerging methodology of programming. We point out that what makes the whole approach viable is recent progress in implementations of algorithms to compute stable models of propositional logic programs.",
        "published": "1998-09-18T20:34:59Z",
        "link": "http://arxiv.org/abs/cs/9809032v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3, I.2.4"
        ]
    },
    {
        "title": "A Natural Deduction style proof system for propositional $Î¼$-calculus   and its formalization in inductive type theories",
        "authors": [
            "Marino Miculan"
        ],
        "summary": "In this paper, we present a formalization of Kozen's propositional modal $\\mu$-calculus, in the Calculus of Inductive Constructions. We address several problematic issues, such as the use of higher-order abstract syntax in inductive sets in presence of recursive constructors, the encoding of modal (``proof'') rules and of context sensitive grammars. The encoding can be used in the \\Coq system, providing an experimental computer-aided proof environment for the interactive development of error-free proofs in the $\\mu$-calculus. The techniques we adopted can be readily ported to other languages and proof systems featuring similar problematic issues.",
        "published": "1998-09-29T11:57:32Z",
        "link": "http://arxiv.org/abs/cs/9809120v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1; F.4.1"
        ]
    },
    {
        "title": "On Dart-Zobel Algorithm for Testing Regular Type Inclusion",
        "authors": [
            "Lunjin Lu",
            "John G. Cleary"
        ],
        "summary": "This paper answers open questions about the correctness and the completeness of Dart-Zobel algorithm for testing the inclusion relation between two regular types. We show that the algorithm is incorrect for regular types. We also prove that the algorithm is complete for regular types as well as correct for tuple distributive regular types. Also presented is a simplified version of Dart-Zobel algorithm for tuple distributive regular types.",
        "published": "1998-10-01T02:33:17Z",
        "link": "http://arxiv.org/abs/cs/9810001v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "Axiomatizing Flat Iteration",
        "authors": [
            "R. J. van Glabbeek"
        ],
        "summary": "Flat iteration is a variation on the original binary version of the Kleene star operation P*Q, obtained by restricting the first argument to be a sum of atomic actions. It generalizes prefix iteration, in which the first argument is a single action. Complete finite equational axiomatizations are given for five notions of bisimulation congruence over basic CCS with flat iteration, viz. strong congruence, branching congruence, eta-congruence, delay congruence and weak congruence. Such axiomatizations were already known for prefix iteration and are known not to exist for general iteration. The use of flat iteration has two main advantages over prefix iteration: 1.The current axiomatizations generalize to full CCS, whereas the prefix iteration approach does not allow an elimination theorem for an asynchronous parallel composition operator. 2.The greater expressiveness of flat iteration allows for much shorter completeness proofs.   In the setting of prefix iteration, the most convenient way to obtain the completeness theorems for eta-, delay, and weak congruence was by reduction to the completeness theorem for branching congruence. In the case of weak congruence this turned out to be much simpler than the only direct proof found. In the setting of flat iteration on the other hand, the completeness theorems for delay and weak (but not eta-) congruence can equally well be obtained by reduction to the one for strong congruence, without using branching congruence as an intermediate step. Moreover, the completeness results for prefix iteration can be retrieved from those for flat iteration, thus obtaining a second indirect approach for proving completeness for delay and weak congruence in the setting of prefix iteration.",
        "published": "1998-10-07T22:39:16Z",
        "link": "http://arxiv.org/abs/cs/9810008v1",
        "categories": [
            "cs.LO",
            "D.3.1; F.1.2; F.3.2"
        ]
    },
    {
        "title": "Deriving Abstract Semantics for Forward Analysis of Normal Logic   Programs",
        "authors": [
            "Lunjin Lu"
        ],
        "summary": "The problem of forward abstract interpretation of {\\em normal} logic programs has not been formally addressed in the literature although negation as failure is dealt with through the built-in predicate ! in the way it is implemented in Prolog. This paper proposes a solution to this problem by deriving two generic fixed-point abstract semantics $F^b and $F^\\diamond for forward abstract interpretation of {\\em normal} logic programs. $F^b$ is intended for inferring data descriptions for edges in the program graph where an edge denotes the possibility that the control of execution transfers from its source program point to its destination program point. $F^\\diamond$ is derived from $F^b$ and is intended for inferring data descriptions for textual program points.",
        "published": "1998-11-06T03:37:44Z",
        "link": "http://arxiv.org/abs/cs/9811012v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2;D.3.2"
        ]
    },
    {
        "title": "An Emptiness Algorithm for Regular Types with Set Operators",
        "authors": [
            "Lunjin Lu",
            "John G. Cleary"
        ],
        "summary": "An algorithm to decide the emptiness of a regular type expression with set operators given a set of parameterised type definitions is presented. The algorithm can also be used to decide the equivalence of two regular type expressions and the inclusion of one regular type expression in another. The algorithm strictly generalises previous work in that tuple distributivity is not assumed and set operators are permitted in type expressions.",
        "published": "1998-11-11T04:50:28Z",
        "link": "http://arxiv.org/abs/cs/9811015v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2; F.3.2"
        ]
    },
    {
        "title": "Formulas as Programs",
        "authors": [
            "Krzysztof R. Apt",
            "Marc Bezem"
        ],
        "summary": "We provide here a computational interpretation of first-order logic based on a constructive interpretation of satisfiability w.r.t. a fixed but arbitrary interpretation. In this approach the formulas themselves are programs. This contrasts with the so-called formulas as types approach in which the proofs of the formulas are typed terms that can be taken as programs. This view of computing is inspired by logic programming and constraint logic programming but differs from them in a number of crucial aspects.   Formulas as programs is argued to yield a realistic approach to programming that has been realized in the implemented programming language ALMA-0 (Apt et al.) that combines the advantages of imperative and logic programming. The work here reported can also be used to reason about the correctness of non-recursive ALMA-0 programs that do not include destructive assignment.",
        "published": "1998-11-11T14:05:13Z",
        "link": "http://arxiv.org/abs/cs/9811017v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.3.1;F.4.1"
        ]
    },
    {
        "title": "Symmetries and transitions of bounded Turing machines",
        "authors": [
            "Peter M. Hines"
        ],
        "summary": "We consider the structures given by repeatedly generalising the definition of finite state automata by symmetry considerations, and constructing analogues of transition monoids at each step. This approach first gives us non-deterministic automata, then (non-deterministic) two-way automata and bounded Turing machines --- that is, Turing machines where the read / write head is unable to move past the end of the input word.   In the case of two-way automata, the transition monoids generalise to endomorphism monoids in compact closed categories. These use Girard's resolution formula (from the Geometry of Interaction representation of linear logic) to construct the images of singleton words.   In the case of bounded Turing machines, the transition homomorphism generalises to a monoid homomorphism from the natural numbers to a monoid constructed from the union of endomorphism monoids of a compact closed category, together with an appropriate composition. These use Girard's execution formula (also from the Geometry of Interaction representation of linear logic) to construct images of singletons.",
        "published": "1998-12-16T20:49:46Z",
        "link": "http://arxiv.org/abs/cs/9812019v1",
        "categories": [
            "cs.LO",
            "math.CT",
            "F.1.1;f.4.1"
        ]
    },
    {
        "title": "Novelty and Social Search in the World Wide Web",
        "authors": [
            "Bernardo A. Huberman",
            "Lada A. Adamic"
        ],
        "summary": "The World Wide Web is fast becoming a source of information for a large part of the world's population. Because of its sheer size and complexity users often resort to recommendations from others to decide which sites to visit. We present a dynamical theory of recommendations which predicts site visits by users of the World Wide Web. We show that it leads to a universal power law for the number of users that visit given sites over periods of time, with an exponent related to the rate at which users discover new sites on their own. An extensive empirical study of user behavior in the Web that we conducted confirms the existence of this law of influence while yielding bounds on the rate of novelty encountered by users.",
        "published": "1998-09-18T00:07:03Z",
        "link": "http://arxiv.org/abs/cs/9809025v1",
        "categories": [
            "cs.MA",
            "cs.DL",
            "H.1.1"
        ]
    },
    {
        "title": "Semantics and Conversations for an Agent Communication Language",
        "authors": [
            "Yannis Labrou",
            "Tim Finin"
        ],
        "summary": "We address the issues of semantics and conversations for agent communication languages and the Knowledge Query Manipulation Language (KQML) in particular. Based on ideas from speech act theory, we present a semantic description for KQML that associates ``cognitive'' states of the agent with the use of the language's primitives (performatives). We have used this approach to describe the semantics for the whole set of reserved KQML performatives. Building on the semantics, we devise the conversation policies, i.e., a formal description of how KQML performatives may be combined into KQML exchanges (conversations), using a Definite Clause Grammar. Our research offers methods for a speech act theory-based semantic description of a language of communication acts and for the specification of the protocols associated with these acts. Languages of communication acts address the issue of communication among software applications at a level of abstraction that is useful to the emerging software agents paradigm.",
        "published": "1998-09-18T21:41:18Z",
        "link": "http://arxiv.org/abs/cs/9809034v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Document Archiving, Replication and Migration Container for Mobile Web   Users",
        "authors": [
            "P. Stanski",
            "S. Giles",
            "A. Zaslavsky"
        ],
        "summary": "With the increasing use of mobile workstations for a wide variety of tasks and associated information needs, and with many variations of available networks, access to data becomes a prime consideration. This paper discusses issues of workstation mobility and proposes a solution wherein the data structures are accessed in an encapsulated form - through the Portable File System (PFS) wrapper. The paper discusses an implementation of the Portable File System, highlighting the architecture and commenting upon performance of an experimental system. Although investigations have been focused upon mobile access of WWW documents, this technique could be applied to any mobile data access situation.",
        "published": "1998-09-20T12:48:43Z",
        "link": "http://arxiv.org/abs/cs/9809036v1",
        "categories": [
            "cs.MA",
            "cs.MM",
            "H.3.2; H.5.3; H.5.4"
        ]
    },
    {
        "title": "Learning Nested Agent Models in an Information Economy",
        "authors": [
            "Jose M. Vidal",
            "Edmund H. Durfee"
        ],
        "summary": "We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents, and a description of the forms of knowledge required. The agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models. Our results show, among other lessons, how savvy buyers can avoid being ``cheated'' by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior.",
        "published": "1998-09-26T17:43:36Z",
        "link": "http://arxiv.org/abs/cs/9809108v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I 2.11"
        ]
    },
    {
        "title": "Anytime Coalition Structure Generation with Worst Case Guarantees",
        "authors": [
            "Tuomas Sandholm",
            "Kate Larson",
            "Martin Andersson",
            "Onn Shehory",
            "Fernando Tohme"
        ],
        "summary": "Coalition formation is a key topic in multiagent systems. One would prefer a coalition structure that maximizes the sum of the values of the coalitions, but often the number of coalition structures is too large to allow exhaustive search for the optimal one. But then, can the coalition structure found via a partial search be guaranteed to be within a bound from optimum? We show that none of the previous coalition structure generation algorithms can establish any bound because they search fewer nodes than a threshold that we show necessary for establishing a bound. We present an algorithm that establishes a tight bound within this minimal amount of search, and show that any other algorithm would have to search strictly more. The fraction of nodes needed to be searched approaches zero as the number of agents grows. If additional time remains, our anytime algorithm searches further, and establishes a progressively lower tight bound. Surprisingly, just searching one more node drops the bound in half. As desired, our algorithm lowers the bound rapidly early on, and exhibits diminishing returns to computation. It also drastically outperforms its obvious contenders. Finally, we show how to distribute the desired search across self-interested manipulative agents.",
        "published": "1998-10-05T16:08:41Z",
        "link": "http://arxiv.org/abs/cs/9810005v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "An Adaptive Agent Oriented Software Architecture",
        "authors": [
            "Babak Hodjat",
            "Christopher J. Savoie",
            "Makoto Amamiya"
        ],
        "summary": "A new approach to software design based on an agent-oriented architecture is presented. Unlike current research, we consider software to be designed and implemented with this methodology in mind. In this approach agents are considered adaptively communicating concurrent modules which are divided into a white box module responsible for the communications and learning, and a black box which is the independent specialized processes of the agent. A distributed Learning policy is also introduced for adaptability.",
        "published": "1998-12-11T22:36:37Z",
        "link": "http://arxiv.org/abs/cs/9812014v1",
        "categories": [
            "cs.DC",
            "cs.MA",
            "D.2.11; D.2.1"
        ]
    },
    {
        "title": "Hierarchical Non-Emitting Markov Models",
        "authors": [
            "Eric Sven Ristad",
            "Robert G. Thomas"
        ],
        "summary": "We describe a simple variant of the interpolated Markov model with non-emitting state transitions and prove that it is strictly more powerful than any Markov model. More importantly, the non-emitting model outperforms the classic interpolated model on the natural language texts under a wide range of experimental conditions, with only a modest increase in computational requirements. The non-emitting model is also much less prone to overfitting.   Keywords: Markov model, interpolated Markov model, hidden Markov model, mixture modeling, non-emitting state transitions, state-conditional interpolation, statistical language model, discrete time series, Brown corpus, Wall Street Journal.",
        "published": "1998-01-14T23:20:23Z",
        "link": "http://arxiv.org/abs/cmp-lg/9801001v3",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Identifying Discourse Markers in Spoken Dialog",
        "authors": [
            "Peter A. Heeman",
            "Donna Byron",
            "James F. Allen"
        ],
        "summary": "In this paper, we present a method for identifying discourse marker usage in spontaneous speech based on machine learning. Discourse markers are denoted by special POS tags, and thus the process of POS tagging can be used to identify discourse markers. By incorporating POS tagging into language modeling, discourse markers can be identified during speech recognition, in which the timeliness of the information can be used to help predict the following words. We contrast this approach with an alternative machine learning approach proposed by Litman (1996). This paper also argues that discourse markers can be used to help the hearer predict the role that the upcoming utterance plays in the dialog. Thus discourse markers should provide valuable evidence for automatic dialog act prediction.",
        "published": "1998-01-17T00:57:29Z",
        "link": "http://arxiv.org/abs/cmp-lg/9801002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Do not forget: Full memory in memory-based learning of word   pronunciation",
        "authors": [
            "Antal van den Bosch",
            "Walter Daelemans"
        ],
        "summary": "Memory-based learning, keeping full memory of learning material, appears a viable approach to learning NLP tasks, and is often superior in generalisation accuracy to eager learning approaches that abstract from learning material. Here we investigate three partial memory-based learning approaches which remove from memory specific task instance types estimated to be exceptional. The three approaches each implement one heuristic function for estimating exceptionality of instance types: (i) typicality, (ii) class prediction strength, and (iii) friendly-neighbourhood size. Experiments are performed with the memory-based learning algorithm IB1-IG trained on English word pronunciation. We find that removing instance types with low prediction strength (ii) is the only tested method which does not seriously harm generalisation accuracy. We conclude that keeping full memory of types rather than tokens, and excluding minority ambiguities appear to be the only performance-preserving optimisations of memory-based learning.",
        "published": "1998-01-26T13:51:59Z",
        "link": "http://arxiv.org/abs/cmp-lg/9801003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Modularity in inductively-learned word pronunciation systems",
        "authors": [
            "Antal van den Bosch",
            "Ton Weijters",
            "Walter Daelemans"
        ],
        "summary": "In leading morpho-phonological theories and state-of-the-art text-to-speech systems it is assumed that word pronunciation cannot be learned or performed without in-between analyses at several abstraction levels (e.g., morphological, graphemic, phonemic, syllabic, and stress levels). We challenge this assumption for the case of English word pronunciation. Using IGTree, an inductive-learning decision-tree algorithms, we train and test three word-pronunciation systems in which the number of abstraction levels (implemented as sequenced modules) is reduced from five, via three, to one. The latter system, classifying letter strings directly as mapping to phonemes with stress markers, yields significantly better generalisation accuracies than the two multi-module systems. Analyses of empirical results indicate that positive utility effects of sequencing modules are outweighed by cascading errors passed on between modules.",
        "published": "1998-01-26T14:42:24Z",
        "link": "http://arxiv.org/abs/cmp-lg/9801004v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A General, Sound and Efficient Natural Language Parsing Algorithm based   on Syntactic Constraints Propagation",
        "authors": [
            "Jose F. Quesada"
        ],
        "summary": "This paper presents a new context-free parsing algorithm based on a bidirectional strictly horizontal strategy which incorporates strong top-down predictions (derivations and adjacencies). From a functional point of view, the parser is able to propagate syntactic constraints reducing parsing ambiguity.   From a computational perspective, the algorithm includes different techniques aimed at the improvement of the manipulation and representation of the structures used.",
        "published": "1998-01-26T16:36:27Z",
        "link": "http://arxiv.org/abs/cmp-lg/9801005v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Look-Back and Look-Ahead in the Conversion of Hidden Markov Models into   Finite State Transducers",
        "authors": [
            "Andre Kempe"
        ],
        "summary": "This paper describes the conversion of a Hidden Markov Model into a finite state transducer that closely approximates the behavior of the stochastic model. In some cases the transducer is equivalent to the HMM. This conversion is especially advantageous for part-of-speech tagging because the resulting transducer can be composed with other transducers that encode correction rules for the most frequent tagging errors. The speed of tagging is also improved. The described methods have been implemented and successfully tested.",
        "published": "1998-02-02T08:36:16Z",
        "link": "http://arxiv.org/abs/cmp-lg/9802001v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Hybrid Environment for Syntax-Semantic Tagging",
        "authors": [
            "Lluis Padro"
        ],
        "summary": "The thesis describes the application of the relaxation labelling algorithm to NLP disambiguation. Language is modelled through context constraint inspired on Constraint Grammars. The constraints enable the use of a real value statind \"compatibility\". The technique is applied to POS tagging, Shallow Parsing and Word Sense Disambigation. Experiments and results are reported. The proposed approach enables the use of multi-feature constraint models, the simultaneous resolution of several NL disambiguation tasks, and the collaboration of linguistic and statistical models.",
        "published": "1998-02-11T19:04:29Z",
        "link": "http://arxiv.org/abs/cmp-lg/9802002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Automating Coreference: The Role of Annotated Training Data",
        "authors": [
            "Lynette Hirschman",
            "Patricia Robinson",
            "John Burger",
            "Marc Vilain"
        ],
        "summary": "We report here on a study of interannotator agreement in the coreference task as defined by the Message Understanding Conference (MUC-6 and MUC-7). Based on feedback from annotators, we clarified and simplified the annotation specification. We then performed an analysis of disagreement among several annotators, concluding that only 16% of the disagreements represented genuine disagreement about coreference; the remainder of the cases were mostly typographical errors or omissions, easily reconciled. Initially, we measured interannotator agreement in the low 80s for precision and recall. To try to improve upon this, we ran several experiments. In our final experiment, we separated the tagging of candidate noun phrases from the linking of actual coreferring expressions. This method shows promise - interannotator agreement climbed to the low 90s - but it needs more extensive validation. These results position the research community to broaden the coreference task to multiple languages, and possibly to different kinds of coreference.",
        "published": "1998-03-02T20:32:58Z",
        "link": "http://arxiv.org/abs/cmp-lg/9803001v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Time, Tense and Aspect in Natural Language Database Interfaces",
        "authors": [
            "I. Androutsopoulos",
            "G. D. Ritchie",
            "P. Thanisch"
        ],
        "summary": "Most existing natural language database interfaces (NLDBs) were designed to be used with database systems that provide very limited facilities for manipulating time-dependent data, and they do not support adequately temporal linguistic mechanisms (verb tenses, temporal adverbials, temporal subordinate clauses, etc.). The database community is becoming increasingly interested in temporal database systems, that are intended to store and manipulate in a principled manner information not only about the present, but also about the past and future. When interfacing to temporal databases, supporting temporal linguistic mechanisms becomes crucial.   We present a framework for constructing natural language interfaces for temporal databases (NLTDBs), that draws on research in tense and aspect theories, temporal logics, and temporal databases. The framework consists of a temporal intermediate representation language, called TOP, an HPSG grammar that maps a wide range of questions involving temporal mechanisms to appropriate TOP expressions, and a provably correct method for translating from TOP to TSQL2, TSQL2 being a recently proposed temporal extension of the SQL database language. This framework was employed to implement a prototype NLTDB using ALE and Prolog.",
        "published": "1998-03-22T15:05:40Z",
        "link": "http://arxiv.org/abs/cmp-lg/9803002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Nymble: a High-Performance Learning Name-finder",
        "authors": [
            "Daniel M. Bikel",
            "Scott Miller",
            "Richard Schwartz",
            "Ralph Weischedel"
        ],
        "summary": "This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.",
        "published": "1998-03-27T15:38:37Z",
        "link": "http://arxiv.org/abs/cmp-lg/9803003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Graph Interpolation Grammars: a Rule-based Approach to the Incremental   Parsing of Natural Languages",
        "authors": [
            "John Larcheveque"
        ],
        "summary": "Graph Interpolation Grammars are a declarative formalism with an operational semantics. Their goal is to emulate salient features of the human parser, and notably incrementality. The parsing process defined by GIGs incrementally builds a syntactic representation of a sentence as each successive lexeme is read. A GIG rule specifies a set of parse configurations that trigger its application and an operation to perform on a matching configuration. Rules are partly context-sensitive; furthermore, they are reversible, meaning that their operations can be undone, which allows the parsing process to be nondeterministic. These two factors confer enough expressive power to the formalism for parsing natural languages.",
        "published": "1998-04-02T14:47:07Z",
        "link": "http://arxiv.org/abs/cmp-lg/9804001v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "The Proper Treatment of Optimality in Computational Phonology",
        "authors": [
            "Lauri Karttunen"
        ],
        "summary": "This paper presents a novel formalization of optimality theory. Unlike previous treatments of optimality in computational linguistics, starting with Ellison (1994), the new approach does not require any explicit marking and counting of constraint violations. It is based on the notion of \"lenient composition,\" defined as the combination of ordinary composition and priority union. If an underlying form has outputs that can meet a given constraint, lenient composition enforces the constraint; if none of the output candidates meet the constraint, lenient composition allows all of them. For the sake of greater efficiency, we may \"leniently compose\" the GEN relation and all the constraints into a single finite-state transducer that maps each underlying form directly into its optimal surface realizations, and vice versa, without ever producing any failing candidates. Seen from this perspective, optimality theory is surprisingly similar to the two older strains of finite-state phonology: classical rewrite systems and two-level models. In particular, the ranking of optimality constraints corresponds to the ordering of rewrite rules.",
        "published": "1998-04-26T22:50:01Z",
        "link": "http://arxiv.org/abs/cmp-lg/9804002v2",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Treatment of Epsilon-Moves in Subset Construction",
        "authors": [
            "Gertjan van Noord"
        ],
        "summary": "The paper discusses the problem of determinising finite-state automata containing large numbers of epsilon-moves. Experiments with finite-state approximations of natural language grammars often give rise to very large automata with a very large number of epsilon-moves. The paper identifies three subset construction algorithms which treat epsilon-moves. A number of experiments has been performed which indicate that the algorithms differ considerably in practice. Furthermore, the experiments suggest that the average number of epsilon-moves per state can be used to predict which algorithm is likely to perform best for a given input automaton.",
        "published": "1998-04-28T08:29:54Z",
        "link": "http://arxiv.org/abs/cmp-lg/9804003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Corpus-Based Word Sense Disambiguation",
        "authors": [
            "Atsushi Fujii"
        ],
        "summary": "Resolution of lexical ambiguity, commonly termed ``word sense disambiguation'', is expected to improve the analytical accuracy for tasks which are sensitive to lexical semantics. Such tasks include machine translation, information retrieval, parsing, natural language understanding and lexicography. Reflecting the growth in utilization of machine readable texts, word sense disambiguation techniques have been explored variously in the context of corpus-based approaches. Within one corpus-based framework, that is the similarity-based method, systems use a database, in which example sentences are manually annotated with correct word senses. Given an input, systems search the database for the most similar example to the input. The lexical ambiguity of a word contained in the input is resolved by selecting the sense annotation of the retrieved example. In this research, we apply this method of resolution of verbal polysemy, in which the similarity between two examples is computed as the weighted average of the similarity between complements governed by a target polysemous verb. We explore similarity-based verb sense disambiguation focusing on the following three methods. First, we propose a weighting schema for each verb complement in the similarity computation. Second, in similarity-based techniques, the overhead for manual supervision and searching the large-sized database can be prohibitive. To resolve this problem, we propose a method to select a small number of effective examples, for system usage. Finally, the efficiency of our system is highly dependent on the similarity computation used. To maximize efficiency, we propose a method which integrates the advantages of previous methods for similarity computation.",
        "published": "1998-04-29T12:01:24Z",
        "link": "http://arxiv.org/abs/cmp-lg/9804004v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "On the existence of certain total recursive functions in nontrivial   axiom systems, I",
        "authors": [
            "N. C. A. da Costa",
            "F. A. Doria"
        ],
        "summary": "We investigate the existence of a class of ZFC-provably total recursive unary functions, given certain constraints, and apply some of those results to show that, for $\\Sigma_1$-sound set theory, ZFC$\\not\\vdash P<NP$.",
        "published": "1998-04-30T17:08:57Z",
        "link": "http://arxiv.org/abs/cmp-lg/9804005v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Valence Induction with a Head-Lexicalized PCFG",
        "authors": [
            "Glenn Carroll",
            "Mats Rooth"
        ],
        "summary": "This paper presents an experiment in learning valences (subcategorization frames) from a 50 million word text corpus, based on a lexicalized probabilistic context free grammar. Distributions are estimated using a modified EM algorithm. We evaluate the acquired lexicon both by comparison with a dictionary and by entropy measures. Results show that our model produces highly accurate frame distributions.",
        "published": "1998-05-05T10:18:17Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805001v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Group Theory and Grammatical Description",
        "authors": [
            "Marc Dymetman"
        ],
        "summary": "This paper presents a model for linguistic description based on group theory. A grammar in this model, or \"G-grammar\", is a collection of lexical expressions which are products of logical forms, phonological forms, and their inverses. Phrasal descriptions are obtained by forming products of lexical expressions and by cancelling contiguous elements which are inverses of each other. We show applications of this model to parsing and generation, long-distance movement, and quantifier scoping. We believe that by moving from the free monoid over a vocabulary V --- standard in formal language studies --- to the free group over V, deep affinities between linguistic phenomena and classical algebra come to the surface, and that the consequences of tapping the mathematical connections thus established could be considerable.",
        "published": "1998-05-07T10:58:06Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Annotation Style Guide for the Blinker Project",
        "authors": [
            "I. Dan Melamed"
        ],
        "summary": "This annotation style guide was created by and for the Blinker project at the University of Pennsylvania. The Blinker project was so named after the ``bilingual linker'' GUI, which was created to enable bilingual annotators to ``link'' word tokens that are mutual translations in parallel texts. The parallel text chosen for this project was the Bible, because it is probably the easiest text to obtain in electronic form in multiple languages. The languages involved were English and French, because, of the languages with which the project co-ordinator was familiar, these were the two for which a sufficient number of annotators was likely to be found.",
        "published": "1998-05-08T16:36:56Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805004v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Manual Annotation of Translational Equivalence: The Blinker Project",
        "authors": [
            "I. Dan Melamed"
        ],
        "summary": "Bilingual annotators were paid to link roughly sixteen thousand corresponding words between on-line versions of the Bible in modern French and modern English. These annotations are freely available to the research community from http://www.cis.upenn.edu/~melamed . The annotations can be used for several purposes. First, they can be used as a standard data set for developing and testing translation lexicons and statistical translation models. Second, researchers in lexical semantics will be able to mine the annotations for insights about cross-linguistic lexicalization patterns. Third, the annotations can be used in research into certain recently proposed methods for monolingual word-sense disambiguation. This paper describes the annotated texts, the specially-designed annotation tool, and the strategies employed to increase the consistency of the annotations. The annotation process was repeated five times by different annotators. Inter-annotator agreement rates indicate that the annotations are reasonably reliable and that the method is easy to replicate.",
        "published": "1998-05-08T16:57:08Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805005v2",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Models of Co-occurrence",
        "authors": [
            "I. Dan Melamed"
        ],
        "summary": "A model of co-occurrence in bitext is a boolean predicate that indicates whether a given pair of word tokens co-occur in corresponding regions of the bitext space. Co-occurrence is a precondition for the possibility that two tokens might be mutual translations. Models of co-occurrence are the glue that binds methods for mapping bitext correspondence with methods for estimating translation models into an integrated system for exploiting parallel texts. Different models of co-occurrence are possible, depending on the kind of bitext map that is available, the language-specific information that is available, and the assumptions made about the nature of translational equivalence. Although most statistical translation models are based on models of co-occurrence, modeling co-occurrence correctly is more difficult than it may at first appear.",
        "published": "1998-05-08T18:02:19Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Word-to-Word Models of Translational Equivalence",
        "authors": [
            "I. Dan Melamed"
        ],
        "summary": "Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is noisy. This article presents methods for biasing statistical translation models to reflect these properties. Analysis of the expected behavior of these biases in the presence of sparse data predicts that they will result in more accurate models. The prediction is confirmed by evaluation with respect to a gold standard -- translation models that are biased in this fashion are significantly more accurate than a baseline knowledge-poor model. This article also shows how a statistical translation model can take advantage of various kinds of pre-existing knowledge that might be available about particular language pairs. Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, is shown to reliably boost translation model performance on some tasks. Statistical models that are informed by pre-existing knowledge about the model domain combine the best of both the rationalist and empiricist traditions.",
        "published": "1998-05-11T16:07:46Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805006v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Parsing Inside-Out",
        "authors": [
            "Joshua Goodman"
        ],
        "summary": "The inside-outside probabilities are typically used for reestimating Probabilistic Context Free Grammars (PCFGs), just as the forward-backward probabilities are typically used for reestimating HMMs. I show several novel uses, including improving parser accuracy by matching parsing algorithms to evaluation criteria; speeding up DOP parsing by 500 times; and 30 times faster PCFG thresholding at a given accuracy level. I also give an elegant, state-of-the-art grammar formalism, which can be used to compute inside-outside probabilities; and a parser description formalism, which makes it easy to derive inside-outside formulas and many others.",
        "published": "1998-05-19T16:43:26Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805007v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Descriptive Characterization of Tree-Adjoining Languages (Full   Version)",
        "authors": [
            "James Rogers"
        ],
        "summary": "Since the early Sixties and Seventies it has been known that the regular and context-free languages are characterized by definability in the monadic second-order theory of certain structures. More recently, these descriptive characterizations have been used to obtain complexity results for constraint- and principle-based theories of syntax and to provide a uniform model-theoretic framework for exploring the relationship between theories expressed in disparate formal terms. These results have been limited, to an extent, by the lack of descriptive characterizations of language classes beyond the context-free. Recently, we have shown that tree-adjoining languages (in a mildly generalized form) can be characterized by recognition by automata operating on three-dimensional tree manifolds, a three-dimensional analog of trees. In this paper, we exploit these automata-theoretic results to obtain a characterization of the tree-adjoining languages by definability in the monadic second-order theory of these three-dimensional tree manifolds. This not only opens the way to extending the tools of model-theoretic syntax to the level of TALs, but provides a highly flexible mechanism for defining TAGs in terms of logical constraints.   This is the full version of a paper to appear in the proceedings of COLING-ACL'98 as a project note.",
        "published": "1998-05-21T03:39:19Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805008v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Discovery of Linguistic Relations Using Lexical Attraction",
        "authors": [
            "Deniz Yuret"
        ],
        "summary": "This work has been motivated by two long term goals: to understand how humans learn language and to build programs that can understand language. Using a representation that makes the relevant features explicit is a prerequisite for successful learning and understanding. Therefore, I chose to represent relations between individual words explicitly in my model. Lexical attraction is defined as the likelihood of such relations. I introduce a new class of probabilistic language models named lexical attraction models which can represent long distance relations between words and I formalize this new class of models using information theory.   Within the framework of lexical attraction, I developed an unsupervised language acquisition program that learns to identify linguistic relations in a given sentence. The only explicitly represented linguistic knowledge in the program is lexical attraction. There is no initial grammar or lexicon built in and the only input is raw text. Learning and processing are interdigitated. The processor uses the regularities detected by the learner to impose structure on the input. This structure enables the learner to detect higher level regularities. Using this bootstrapping procedure, the program was trained on 100 million words of Associated Press material and was able to achieve 60% precision and 50% recall in finding relations between content-words. Using knowledge of lexical attraction, the program can identify the correct relations in syntactically ambiguous sentences such as ``I saw the Statue of Liberty flying over New York.''",
        "published": "1998-05-27T23:40:44Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805009v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Integrating Text Plans for Conciseness and Coherence",
        "authors": [
            "Terrence Harvey",
            "Sandra Carberry"
        ],
        "summary": "Our experience with a critiquing system shows that when the system detects problems with the user's performance, multiple critiques are often produced. Analysis of a corpus of actual critiques revealed that even though each individual critique is concise and coherent, the set of critiques as a whole may exhibit several problems that detract from conciseness and coherence, and consequently assimilation. Thus a text planner was needed that could integrate the text plans for individual communicative goals to produce an overall text plan representing a concise, coherent message.   This paper presents our general rule-based system for accomplishing this task. The system takes as input a \\emph{set} of individual text plans represented as RST-style trees, and produces a smaller set of more complex trees representing integrated messages that still achieve the multiple communicative goals of the individual text plans. Domain-independent rules are used to capture strategies across domains, while the facility for addition of domain-dependent rules enables the system to be tuned to the requirements of a particular domain. The system has been tested on a corpus of critiques in the domain of trauma care.",
        "published": "1998-05-28T18:14:12Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805010v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Automatic summarising: factors and directions",
        "authors": [
            "Karen Sparck Jones"
        ],
        "summary": "This position paper suggests that progress with automatic summarising demands a better research methodology and a carefully focussed research strategy. In order to develop effective procedures it is necessary to identify and respond to the context factors, i.e. input, purpose, and output factors, that bear on summarising and its evaluation. The paper analyses and illustrates these factors and their implications for evaluation. It then argues that this analysis, together with the state of the art and the intrinsic difficulty of summarising, imply a nearer-term strategy concentrating on shallow, but not surface, text analysis and on indicative summarising. This is illustrated with current work, from which a potentially productive research programme can be developed.",
        "published": "1998-05-29T10:57:56Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805011v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Recognizing Syntactic Errors in the Writing of Second Language Learners",
        "authors": [
            "David A. Schneider",
            "Kathleen F. McCoy"
        ],
        "summary": "This paper reports on the recognition component of an intelligent tutoring system that is designed to help foreign language speakers learn standard English. The system models the grammar of the learner, with this instantiation of the system tailored to signers of American Sign Language (ASL). We discuss the theoretical motivations for the system, various difficulties that have been encountered in the implementation, as well as the methods we have used to overcome these problems. Our method of capturing ungrammaticalities involves using mal-rules (also called 'error productions'). However, the straightforward addition of some mal-rules causes significant performance problems with the parser. For instance, the ASL population has a strong tendency to drop pronouns and the auxiliary verb `to be'. Being able to account for these as sentences results in an explosion in the number of possible parses for each sentence. This explosion, left unchecked, greatly hampers the performance of the system. We discuss how this is handled by taking into account expectations from the specific population (some of which are captured in our unique user model). The different representations of lexical items at various points in the acquisition process are modeled by using mal-rules, which obviates the need for multiple lexicons. The grammar is evaluated on its ability to correctly diagnose agreement problems in actual sentences produced by ASL native speakers.",
        "published": "1998-05-29T17:15:39Z",
        "link": "http://arxiv.org/abs/cmp-lg/9805012v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Learning Correlations between Linguistic Indicators and Semantic   Constraints: Reuse of Context-Dependent Descriptions of Entities",
        "authors": [
            "Dragomir R. Radev"
        ],
        "summary": "This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators. We show how such indicators are computed and how correlations between them and the choice of a noun phrase description of a named entity can be automatically established using supervised learning. Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation. We discuss the underlying relationship between the pragmatics of choosing an appropriate description that serves a specific purpose in the automatically generated text and the semantics of the description itself. We present our work in the framework of the more general concept of reuse of linguistic structures that are automatically extracted from large corpora. We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method.",
        "published": "1998-05-31T21:10:44Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806001v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Computing Dialogue Acts from Features with Transformation-Based Learning",
        "authors": [
            "Ken Samuel",
            "Sandra Carberry",
            "K. Vijay-Shanker"
        ],
        "summary": "To interpret natural language at the discourse level, it is very useful to accurately recognize dialogue acts, such as SUGGEST, in identifying speaker intentions. Our research explores the utility of a machine learning method called Transformation-Based Learning (TBL) in computing dialogue acts, because TBL has a number of advantages over alternative approaches for this application. We have identified some extensions to TBL that are necessary in order to address the limitations of the original algorithm and the particular demands of discourse processing. We use a Monte Carlo strategy to increase the applicability of the TBL method, and we select features of utterances that can be used as input to improve the performance of TBL. Our system is currently being tested on the VerbMobil corpora of spoken dialogues, producing promising preliminary results.",
        "published": "1998-06-02T14:28:17Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Lazy Transformation-Based Learning",
        "authors": [
            "Ken Samuel"
        ],
        "summary": "We introduce a significant improvement for a relatively new machine learning method called Transformation-Based Learning. By applying a Monte Carlo strategy to randomly sample from the space of rules, rather than exhaustively analyzing all possible rules, we drastically reduce the memory and time costs of the algorithm, without compromising accuracy on unseen data. This enables Transformation- Based Learning to apply to a wider range of domains, as it can effectively consider a larger number of different features and feature interactions in the data. In addition, the Monte Carlo improvement decreases the labor demands on the human developer, who no longer needs to develop a minimal set of rule templates to maintain tractability.",
        "published": "1998-06-03T16:47:37Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Rationality, Cooperation and Conversational Implicature",
        "authors": [
            "Mark Lee"
        ],
        "summary": "Conversational implicatures are usually described as being licensed by the disobeying or flouting of a Principle of Cooperation. However, the specification of this principle has proved computationally elusive. In this paper we suggest that a more useful concept is rationality. Such a concept can be specified explicitely in planning terms and we argue that speakers perform utterances as part of the optimal plan for their particular communicative goals. Such an assumption can be used by the hearer to infer conversational implicatures implicit in the speaker's utterance.",
        "published": "1998-06-05T10:52:33Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806004v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Eliminating deceptions and mistaken belief to infer conversational   implicature",
        "authors": [
            "Mark Lee",
            "Yorick Wilks"
        ],
        "summary": "Conversational implicatures are usually described as being licensed by the disobeying or flouting of some principle by the speaker in cooperative dialogue. However, such work has failed to distinguish cases of the speaker flouting such a principle from cases where the speaker is either deceptive or holds a mistaken belief. In this paper, we demonstrate how the three different cases can be distinguished in terms of the beliefs ascribed to the speaker of the utterance. We argue that in the act of distinguishing the speaker's intention and ascribing such beliefs, the intended inference can be made by the hearer. This theory is implemented in ViewGen, a pre-existing belief modelling system used in a medical counselling domain.",
        "published": "1998-06-05T16:38:28Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806005v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Dialogue Act Tagging with Transformation-Based Learning",
        "authors": [
            "Ken Samuel",
            "Sandra Carberry",
            "K. Vijay-Shanker"
        ],
        "summary": "For the task of recognizing dialogue acts, we are applying the Transformation-Based Learning (TBL) machine learning algorithm. To circumvent a sparse data problem, we extract values of well-motivated features of utterances, such as speaker direction, punctuation marks, and a new feature, called dialogue act cues, which we find to be more effective than cue phrases and word n-grams in practice. We present strategies for constructing a set of dialogue act cues automatically by minimizing the entropy of the distribution of dialogue acts in a training corpus, filtering out irrelevant dialogue act cues, and clustering semantically-related words. In addition, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task.",
        "published": "1998-06-08T16:06:59Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806006v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "An Investigation of Transformation-Based Learning in Discourse",
        "authors": [
            "Ken Samuel",
            "Sandra Carberry",
            "K. Vijay-Shanker"
        ],
        "summary": "This paper presents results from the first attempt to apply Transformation-Based Learning to a discourse-level Natural Language Processing task. To address two limitations of the standard algorithm, we developed a Monte Carlo version of Transformation-Based Learning to make the method tractable for a wider range of problems without degradation in accuracy, and we devised a committee method for assigning confidence measures to tags produced by Transformation-Based Learning. The paper describes these advances, presents experimental evidence that Transformation-Based Learning is as effective as alternative approaches (such as Decision Trees and N-Grams) for a discourse task called Dialogue Act Tagging, and argues that Transformation-Based Learning has desirable features that make it particularly appealing for the Dialogue Act Tagging task.",
        "published": "1998-06-09T23:10:27Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806007v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Unlimited Vocabulary Grapheme to Phoneme Conversion for Korean TTS",
        "authors": [
            "Byeongchang Kim",
            "WonIl Lee",
            "Geunbae Lee",
            "Jong-Hyeok Lee"
        ],
        "summary": "This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules. The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check.   The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes. The phrase-break detector assigns phrase breaks using part-of-speech (POS) information. In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes. Graphemes within a morpheme are grouped into CCV patterns and converted into phonemes by the CCV conversion rules. The phoneme connectivity table supports grammaticality checking of the adjacent two phonetic morphemes.   In the experiments with a corpus of 4,973 sentences, we achieved 99.9% of the grapheme-to-phoneme conversion performance and 97.5% of the sentence conversion performance. The full Korean TTS system is now being implemented using this conversion method.",
        "published": "1998-06-10T13:23:54Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806008v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Methods and Tools for Building the Catalan WordNet",
        "authors": [
            "Laura Benitez",
            "Sergi Cervell",
            "Gerard Escudero",
            "Monica Lopez",
            "German Rigau",
            "Mariona Taule"
        ],
        "summary": "In this paper we introduce the methodology used and the basic phases we followed to develop the Catalan WordNet, and shich lexical resources have been employed in its building. This methodology, as well as the tools we made use of, have been thought in a general way so that they could be applied to any other language.",
        "published": "1998-06-11T11:05:19Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806009v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Towards a single proposal is spelling correction",
        "authors": [
            "E. Agirre",
            "K. Gojenola",
            "K. Sarasola"
        ],
        "summary": "The study presented here relies on the integrated use of different kinds of knowledge in order to improve first-guess accuracy in non-word context-sensitive correction for general unrestricted texts. State of the art spelling correction systems, e.g. ispell, apart from detecting spelling errors, also assist the user by offering a set of candidate corrections that are close to the misspelled word. Based on the correction proposals of ispell, we built several guessers, which were combined in different ways. Firstly, we evaluated all possibilities and selected the best ones in a corpus with artificially generated typing errors. Secondly, the best combinations were tested on texts with genuine spelling errors. The results for the latter suggest that we can expect automatic non-word correction for all the errors in a free running text with 80% precision and a single proposal 98% of the times (1.02 proposals on average).",
        "published": "1998-06-15T19:38:11Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806010v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Memory-Based Approach to Learning Shallow Natural Language Patterns",
        "authors": [
            "Shlomo Argamon",
            "Ido Dagan",
            "Yuval Krymolowski"
        ],
        "summary": "Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction.",
        "published": "1998-06-16T11:17:41Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806011v3",
        "categories": [
            "cmp-lg",
            "cs.CL",
            "H.3.1;I.2.6;I.2.7"
        ]
    },
    {
        "title": "Bayesian Stratified Sampling to Assess Corpus Utility",
        "authors": [
            "Judith Hochberg",
            "Clint Scovel",
            "Timothy Thomas",
            "Sam Hall"
        ],
        "summary": "This paper describes a method for asking statistical questions about a large text corpus. We exemplify the method by addressing the question, \"What percentage of Federal Register documents are real documents, of possible interest to a text researcher or analyst?\" We estimate an answer to this question by evaluating 200 documents selected from a corpus of 45,820 Federal Register documents. Stratified sampling is used to reduce the sampling uncertainty of the estimate from over 3100 documents to fewer than 1000. The stratification is based on observed characteristics of real documents, while the sampling procedure incorporates a Bayesian version of Neyman allocation. A possible application of the method is to establish baseline statistics used to estimate recall rates for information retrieval systems.",
        "published": "1998-06-19T20:58:37Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806012v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Can Subcategorisation Probabilities Help a Statistical Parser?",
        "authors": [
            "John Carroll",
            "Guido Minnen",
            "Ted Briscoe"
        ],
        "summary": "Research into the automatic acquisition of lexical information from corpora is starting to produce large-scale computational lexicons containing data on the relative frequencies of subcategorisation alternatives for individual verbal predicates. However, the empirical question of whether this type of frequency information can in practice improve the accuracy of a statistical parser has not yet been answered. In this paper we describe an experiment with a wide-coverage statistical grammar and parser for English and subcategorisation frequencies acquired from ten million words of text which shows that this information can significantly improve parse accuracy.",
        "published": "1998-06-21T05:35:53Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806013v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Word Sense Disambiguation using Optimised Combinations of Knowledge   Sources",
        "authors": [
            "Yorick Wilks",
            "Mark Stevenson"
        ],
        "summary": "Word sense disambiguation algorithms, with few exceptions, have made use of only one lexical knowledge source. We describe a system which performs unrestricted word sense disambiguation (on all content words in free text) by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags. The usefulness of these sources is optimised by means of a learning algorithm. We also describe the creation of a new sense tagged corpus by combining existing resources. Tested accuracy of our approach on this corpus exceeds 92%, demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample.",
        "published": "1998-06-22T16:20:22Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806014v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Building Accurate Semantic Taxonomies from Monolingual MRDs",
        "authors": [
            "German Rigau",
            "Horacio Rodriguez",
            "Eneko Agirre"
        ],
        "summary": "This paper presents a method that combines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic exraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE.",
        "published": "1998-06-23T15:41:13Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806015v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Using WordNet for Building WordNets",
        "authors": [
            "Xavier Farreres",
            "German Rigau",
            "Horacio Rodriguez"
        ],
        "summary": "This paper summarises a set of methodologies and techniques for the fast construction of multilingual WordNets. The English WordNet is used in this approach as a backbone for Catalan and Spanish WordNets and as a lexical knowledge resource for several subtasks.",
        "published": "1998-06-23T17:26:41Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806016v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Anchoring a Lexicalized Tree-Adjoining Grammar for Discourse",
        "authors": [
            "Bonnie Lynn Webber",
            "Aravind K. Joshi"
        ],
        "summary": "We here explore a ``fully'' lexicalized Tree-Adjoining Grammar for discourse that takes the basic elements of a (monologic) discourse to be not simply clauses, but larger structures that are anchored on variously realized discourse cues. This link with intra-sentential grammar suggests an account for different patterns of discourse cues, while the different structures and operations suggest three separate sources for elements of discourse meaning: (1) a compositional semantics tied to the basic trees and operations; (2) a presuppositional semantics carried by cue phrases that freely adjoin to trees; and (3) general inference, that draws additional, defeasible conclusions that flesh out what is conveyed compositionally.",
        "published": "1998-06-24T21:22:15Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806017v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Never Look Back: An Alternative to Centering",
        "authors": [
            "Michael Strube"
        ],
        "summary": "I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word.",
        "published": "1998-06-25T17:31:47Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806018v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "An Empirical Investigation of Proposals in Collaborative Dialogues",
        "authors": [
            "Barbara Di Eugenio",
            "Pamela W. Jordan",
            "Johanna D. Moore",
            "Richmond H. Thomason"
        ],
        "summary": "We describe a corpus-based investigation of proposals in dialogue. First, we describe our DRI compliant coding scheme and report our inter-coder reliability results. Next, we test several hypotheses about what constitutes a well-formed proposal.",
        "published": "1998-06-25T17:55:20Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806019v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Textual Economy through Close Coupling of Syntax and Semantics",
        "authors": [
            "Matthew Stone",
            "Bonnie Webber"
        ],
        "summary": "We focus on the production of efficient descriptions of objects, actions and events. We define a type of efficiency, textual economy, that exploits the hearer's recognition of inferential links to material elsewhere within a sentence. Textual economy leads to efficient descriptions because the material that supports such inferences has been included to satisfy independent communicative goals, and is therefore overloaded in Pollack's sense. We argue that achieving textual economy imposes strong requirements on the representation and reasoning used in generating sentences. The representation must support the generator's simultaneous consideration of syntax and semantics. Reasoning must enable the generator to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its (incomplete) syntax and semantics. We show that these representational and reasoning requirements are met in the SPUD system for sentence planning and realization.",
        "published": "1998-06-29T16:54:17Z",
        "link": "http://arxiv.org/abs/cmp-lg/9806020v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Evaluating a Focus-Based Approach to Anaphora Resolution",
        "authors": [
            "Saliha Azzam",
            "Kevin Humphreys",
            "Robert Gaizauskas"
        ],
        "summary": "We present an approach to anaphora resolution based on a focusing algorithm, and implemented within an existing MUC (Message Understanding Conference) Information Extraction system, allowing quantitative evaluation against a substantial corpus of annotated real-world texts. Extensions to the basic focusing mechanism can be easily tested, resulting in refinements to the mechanism and resolution rules. Results are compared with the results of a simpler heuristic-based approach.",
        "published": "1998-07-06T17:28:35Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807001v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "The Role of Verbs in Document Analysis",
        "authors": [
            "Judith L. Klavans",
            "Min-Yen Kan"
        ],
        "summary": "We present results of two methods for assessing the event profile of news articles as a function of verb type. The unique contribution of this research is the focus on the role of verbs, rather than nouns. Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile. The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases. An evaluation is performed on the results using Kendall's Tau. We present convincing evidence for using verb semantic classes as a discriminant in document classification.",
        "published": "1998-07-13T13:29:26Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Centering in Dynamic Semantics",
        "authors": [
            "Daniel Hardt"
        ],
        "summary": "Centering theory posits a discourse center, a distinguished discourse entity that is the topic of a discourse. A simplified version of this theory is developed in a Dynamic Semantics framework. In the resulting system, the mechanism of center shift allows a simple, elegant analysis of a variety of phenomena involving sloppy identity in ellipsis and ``paycheck pronouns''.",
        "published": "1998-07-14T09:08:33Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Word Clustering and Disambiguation Based on Co-occurrence Data",
        "authors": [
            "Hang Li",
            "Naoki Abe"
        ],
        "summary": "We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation. We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs. We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution. Our method is a natural extension of those proposed in (Brown et al 92) and (Li & Abe 96), and overcomes their drawbacks while retaining their advantages. We then combined this clustering method with the disambiguation method of (Li & Abe 95) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a hand-made thesaurus. The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill & Resnik 94).",
        "published": "1998-07-17T08:22:43Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807004v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Graph Interpolation Grammars as Context-Free Automata",
        "authors": [
            "John Larcheveque"
        ],
        "summary": "A derivation step in a Graph Interpolation Grammar has the effect of scanning an input token. This feature, which aims at emulating the incrementality of the natural parser, restricts the formal power of GIGs. This contrasts with the fact that the derivation mechanism involves a context-sensitive device similar to tree adjunction in TAGs. The combined effect of input-driven derivation and restricted context-sensitiveness would be conceivably unfortunate if it turned out that Graph Interpolation Languages did not subsume Context Free Languages while being partially context-sensitive. This report sets about examining relations between CFGs and GIGs, and shows that GILs are a proper superclass of CFLs. It also brings out a strong equivalence between CFGs and GIGs for the class of CFLs. Thus, it lays the basis for meaningfully investigating the amount of context-sensitiveness supported by GIGs, but leaves this investigation for further research.",
        "published": "1998-07-17T12:31:51Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807005v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Maximum-Entropy Partial Parser for Unrestricted Text",
        "authors": [
            "Wojciech Skut",
            "Thorsten Brants"
        ],
        "summary": "This paper describes a partial parser that assigns syntactic structures to sequences of part-of-speech tags. The program uses the maximum entropy parameter estimation method, which allows a flexible combination of different knowledge sources: the hierarchical structure, parts of speech and phrasal categories. In effect, the parser goes beyond simple bracketing and recognises even fairly complex structures. We give accuracy figures for different applications of the parser.",
        "published": "1998-07-17T13:56:23Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807006v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Chunk Tagger - Statistical Recognition of Noun Phrases",
        "authors": [
            "Wojciech Skut",
            "Thorsten Brants"
        ],
        "summary": "We describe a stochastic approach to partial parsing, i.e., the recognition of syntactic structures of limited depth. The technique utilises Markov Models, but goes beyond usual bracketing approaches, since it is capable of recognising not only the boundaries, but also the internal structure and syntactic category of simple as well as complex NP's, PP's, AP's and adverbials. We compare tagging accuracy for different applications and encoding schemes.",
        "published": "1998-07-17T14:23:10Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807007v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Linguistically Interpreted Corpus of German Newspaper Text",
        "authors": [
            "Wojciech Skut",
            "Thorsten Brants",
            "Brigitte Krenn",
            "Hans Uszkoreit"
        ],
        "summary": "In this paper, we report on the development of an annotation scheme and annotation tools for unrestricted German text. Our representation format is based on argument structure, but also permits the extraction of other kinds of representations. We discuss several methodological issues and the analysis of some phenomena. Additional focus is on the tools developed in our project and their applications.",
        "published": "1998-07-17T18:04:07Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807008v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Projection Architecture for Dependency Grammar and How it Compares to   LFG",
        "authors": [
            "Norbert Broeker"
        ],
        "summary": "This paper explores commonalities and differences between \\dachs, a variant of Dependency Grammar, and Lexical-Functional Grammar. \\dachs\\ is based on traditional linguistic insights, but on modern mathematical tools, aiming to integrate different knowledge systems (from syntax and semantics) via their coupling to an abstract syntactic primitive, the dependency relation. These knowledge systems correspond rather closely to projections in LFG. We will investigate commonalities arising from the usage of the projection approach in both theories, and point out differences due to the incompatible linguistic premises. The main difference to LFG lies in the motivation and status of the dimensions, and the information coded there. We will argue that LFG confounds different information in one projection, preventing it to achieve a good separation of alternatives and calling the motivation of the projection into question.",
        "published": "1998-07-20T15:34:57Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807009v2",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Automatically Creating Bilingual Lexicons for Machine Translation from   Bilingual Text",
        "authors": [
            "Davide Turcato"
        ],
        "summary": "A method is presented for automatically augmenting the bilingual lexicon of an existing Machine Translation system, by extracting bilingual entries from aligned bilingual text. The proposed method only relies on the resources already available in the MT system itself. It is based on the use of bilingual lexical templates to match the terminal symbols in the parses of the aligned sentences.",
        "published": "1998-07-20T22:34:56Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807010v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Statistical Models for Unsupervised Prepositional Phrase Attachment",
        "authors": [
            "Adwait Ratnaparkhi"
        ],
        "summary": "We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithms proposed for this task. We present results for prepositional phrase attachment in both English and Spanish.",
        "published": "1998-07-22T22:05:04Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807011v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Character design for soccer commmentary",
        "authors": [
            "Kim Binsted"
        ],
        "summary": "In this paper we present early work on an animated talking head commentary system called {\\bf Byrne}\\footnote{David Byrne is the lead singer of the Talking Heads.}. The goal of this project is to develop a system which can take the output from the RoboCup soccer simulator, and generate appropriate affective speech and facial expressions, based on the character's personality, emotional state, and the state of play. Here we describe a system which takes pre-analysed simulator output as input, and which generates text marked-up for use by a speech generator and a face animation system. We make heavy use of inter-system standards, so that future versions of Byrne will be able to take advantage of advances in the technologies that it incorporates.",
        "published": "1998-07-31T06:50:56Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807012v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Improving Data Driven Wordclass Tagging by System Combination",
        "authors": [
            "Hans van Halteren",
            "Jakub Zavrel",
            "Walter Daelemans"
        ],
        "summary": "In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.",
        "published": "1998-07-31T10:30:08Z",
        "link": "http://arxiv.org/abs/cmp-lg/9807013v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion   Grammars",
        "authors": [
            "Rebecca Hwa"
        ],
        "summary": "We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing. Comparing the performance of PLTIGs with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its non-lexicalized counterpart. Furthermore, training of PLTIGs displays faster convergence than PCFGs.",
        "published": "1998-08-04T02:15:25Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808001v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Indexing with WordNet synsets can improve Text Retrieval",
        "authors": [
            "Julio Gonzalo",
            "Felisa Verdejo",
            "Irina Chugur",
            "Juan Cigarran"
        ],
        "summary": "The classical, vector space model for text retrieval is shown to give better results (up to 29% better in our experiments) if WordNet synsets are chosen as the indexing space, instead of word forms. This result is obtained for a manually disambiguated test collection (of queries and documents) derived from the Semcor semantic concordance. The sensitivity of retrieval performance to (automatic) disambiguation errors when indexing documents is also measured. Finally, it is observed that if queries are not disambiguated, indexing by synsets performs (at best) only as good as standard word indexing.",
        "published": "1998-08-05T14:13:08Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Parallel Strands: A Preliminary Investigation into Mining the Web for   Bilingual Text",
        "authors": [
            "Philip Resnik"
        ],
        "summary": "Parallel corpora are a valuable resource for machine translation, but at present their availability and utility is limited by genre- and domain-specificity, licensing restrictions, and the basic difficulty of locating parallel texts in all but the most dominant of the world's languages. A parallel corpus resource not yet explored is the World Wide Web, which hosts an abundance of pages in parallel translation, offering a potential solution to some of these problems and unique opportunities of its own. This paper presents the necessary first step in that exploration: a method for automatically finding parallel translated documents on the Web. The technique is conceptually simple, fully language independent, and scalable, and preliminary evaluation results indicate that the method may be accurate enough to apply without human intervention.",
        "published": "1998-08-07T20:16:09Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Word Length Frequency and Distribution in English: Observations, Theory,   and Implications for the Construction of Verse Lines",
        "authors": [
            "Hideaki Aoyama",
            "John Constable"
        ],
        "summary": "Recent observations in the theory of verse and empirical metrics have suggested that constructing a verse line involves a pattern-matching search through a source text, and that the number of found elements (complete words totaling a specified number of syllables) is given by dividing the total number of words by the mean number of syllables per word in the source text. This paper makes this latter point explicit mathematically, and in the course of this demonstration shows that the word length frequency totals in English output are distributed geometrically (previous researchers reported an adjusted Poisson distribution), and that the sequential distribution is random at the global level, with significant non-randomness in the fine structure. Data from a corpus of just under two million words, and a syllable-count lexicon of 71,000 word-forms is reported. The pattern-matching theory is shown to be internally coherent, and it is observed that some of the analytic techniques described here form a satisfactory test for regular (isometric) lineation in a text.",
        "published": "1998-08-12T09:07:46Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808004v2",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Combining Expression and Content in Domains for Dialog Managers",
        "authors": [
            "Bernd Ludwig",
            "Guenther Goerz",
            "Heinrich Niemann"
        ],
        "summary": "We present work in progress on abstracting dialog managers from their domain in order to implement a dialog manager development tool which takes (among other data) a domain description as input and delivers a new dialog manager for the described domain as output. Thereby we will focus on two topics; firstly, the construction of domain descriptions with description logics and secondly, the interpretation of utterances in a given domain.",
        "published": "1998-08-13T07:25:22Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808005v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Isometric Lineation in English Texts: An Empirical and Mathematical   Examination of its Character and Consequences",
        "authors": [
            "Hideaki Aoyama",
            "John Constable"
        ],
        "summary": "In this paper we build on earlier observations and theory regarding word length frequency and sequential distribution to develop a mathematical characterization of some of the language features distinguishing isometrically lineated text from unlineated text, in other words the features distinguishing isometrical verse from prose. It is shown that the frequency of syllables making complete words produces a flat distribution for prose, while that for verse exhibits peaks at the line length position and subsequent multiples of that position. Data from several verse authors is presented, including a detailed mathematical analysis of the dynamics underlying peak creation, and comments are offered on the processes by which authors construct lines. We note that the word-length sequence of prose is random, whereas lineation necessitates non-random word-length sequencing, and that this has the probable consequence of introducing a degree of randomness into the otherwise highly ordered grammatical sequence. In addition we observe that this effect can be ameliorated by a reduction in the mean word length of the text (confirming earlier observations that verse tends to use shorter words) and the use of lines varying from the core isometrical set. The frequency of variant lines is shown to be coincident with the frequency of polysyllables, suggesting that the use of variant lines is motivated by polysyllabic word placement. The restrictive effects of different line lengths, the relationship between metrical restriction and poetic effect, and the general character of metrical rules are also discussed.",
        "published": "1998-08-14T00:44:26Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808006v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Some Properties of Preposition and Subordinate Conjunction Attachments",
        "authors": [
            "Alexander S. Yeh",
            "Marc B. Vilain"
        ],
        "summary": "Determining the attachments of prepositions and subordinate conjunctions is a key problem in parsing natural language. This paper presents a trainable approach to making these attachments through transformation sequences and error-driven learning. Our approach is broad coverage, and accounts for roughly three times the attachment cases that have previously been handled by corpus-based techniques. In addition, our approach is based on a simplified model of syntax that is more consistent with the practice in current state-of-the-art language processing systems. This paper sketches syntactic and algorithmic details, and presents experimental results on data sets derived from the Penn Treebank. We obtain an attachment accuracy of 75.4% for the general case, the first such corpus-based result to be reported. For the restricted cases previously studied with corpus-based methods, our approach yields an accuracy comparable to current work (83.1%).",
        "published": "1998-08-19T23:53:22Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808007v2",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Deriving the Predicate-Argument Structure for a Free Word Order Language",
        "authors": [
            "Cem Bozsahin"
        ],
        "summary": "In relatively free word order languages, grammatical functions are intricately related to case marking. Assuming an ordered representation of the predicate-argument structure, this work proposes a Combinatory Categorial Grammar formulation of relating surface case cues to categories and types for correctly placing the arguments in the predicate-argument structure. This is achieved by assigning case markers GF-encoding categories. Unlike other CG formulations, type shifting does not proliferate or cause spurious ambiguity. Categories of all argument-encoding grammatical functions follow from the same principle of category assignment. Normal order evaluation of the combinatory form reveals the predicate-argument structure. Application of the method to Turkish is shown.",
        "published": "1998-08-20T14:26:11Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808008v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "How to define a context-free backbone for DGs: Implementing a DG in the   LFG formalism",
        "authors": [
            "Norbert Broeker"
        ],
        "summary": "This paper presents a multidimensional Dependency Grammar (DG), which decouples the dependency tree from word order, such that surface ordering is not determined by traversing the dependency tree. We develop the notion of a \\emph{word order domain structure}, which is linked but structurally dissimilar to the syntactic dependency tree. We then discuss the implementation of such a DG using constructs from a unification-based phrase-structure approach, namely Lexical-Functional Grammar (LFG). Particular attention is given to the analysis of discontinuities in DG in terms of LFG's functional uncertainty.",
        "published": "1998-08-21T14:04:30Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808009v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Letter to Sound Rules for Accented Lexicon Compression",
        "authors": [
            "V. Pagel",
            "K. Lenzo",
            "A. Black"
        ],
        "summary": "This paper presents trainable methods for generating letter to sound rules from a given lexicon for use in pronouncing out-of-vocabulary words and as a method for lexicon compression.   As the relationship between a string of letters and a string of phonemes representing its pronunciation for many languages is not trivial, we discuss two alignment procedures, one fully automatic and one hand-seeded which produce reasonable alignments of letters to phones.   Top Down Induction Tree models are trained on the aligned entries. We show how combined phoneme/stress prediction is better than separate prediction processes, and still better when including in the model the last phonemes transcribed and part of speech information. For the lexicons we have tested, our models have a word accuracy (including stress) of 78% for OALD, 62% for CMU and 94% for BRULEX. The extremely high scores on the training sets allow substantial size reductions (more than 1/20).   WWW site: http://tcts.fpms.ac.be/synthesis/mbrdico",
        "published": "1998-08-21T15:26:44Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808010v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Primitive Part-of-Speech Tagging using Word Length and Sentential   Structure",
        "authors": [
            "Simon Cozens"
        ],
        "summary": "It has been argued that, when learning a first language, babies use a series of small clues to aid recognition and comprehension, and that one of these clues is word length. In this paper we present a statistical part of speech tagger which trains itself solely on the number of letters in each word in a sentence.",
        "published": "1998-08-23T20:14:21Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808011v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Separating Surface Order and Syntactic Relations in a Dependency Grammar",
        "authors": [
            "Norbert Broeker"
        ],
        "summary": "This paper proposes decoupling the dependency tree from word order, such that surface ordering is not determined by traversing the dependency tree. We develop the notion of a \\emph{word order domain structure}, which is linked but structurally dissimilar to the syntactic dependency tree. The proposal results in a lexicalized, declarative, and formally precise description of word order; features which lack previous proposals for dependency grammars. Contrary to other lexicalized approaches to word order, our proposal does not require lexical ambiguities for ordering alternatives.",
        "published": "1998-08-25T08:06:45Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808012v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Partial Evaluation for Efficient Access to Inheritance Lexicons",
        "authors": [
            "Sven Hartrumpf"
        ],
        "summary": "Multiple default inheritance formalisms for lexicons have attracted much interest in recent years. I propose a new efficient method to access such lexicons. After showing two basic strategies for lookup in inheritance lexicons, a compromise is developed which combines to a large degree (from a practical point of view) the advantages of both strategies and avoids their disadvantages. The method is a kind of (off-line) partial evaluation that makes a subset of inherited information explicit before using the lexicon. I identify the parts of a lexicon which should be evaluated, and show how partial evaluation works for inheritance lexicons. Finally, the theoretical results are confirmed by a complete implementation. Speedups by a factor of 10-100 are reached.",
        "published": "1998-08-25T16:52:31Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808013v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Spotting Prosodic Boundaries in Continuous Speech in French",
        "authors": [
            "V. Pagel",
            "N. Carbonell",
            "Y. Laprie",
            "J. Vaissiere"
        ],
        "summary": "A radio speech corpus of 9mn has been prosodically marked by a phonetician expert, and non expert listeners. this corpus is large enough to train and test an automatic boundary spotting system, namely a time delay neural network fed with F0 values, vowels and pseudo-syllable durations. Results validate both prosodic marking and automatic spotting of prosodic events.",
        "published": "1998-08-26T11:12:30Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808014v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Error-Driven Pruning of Treebank Grammars for Base Noun Phrase   Identification",
        "authors": [
            "Claire Cardie",
            "David Pierce"
        ],
        "summary": "Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a ``treebank'' corpus; then the grammar is improved by selecting rules with high ``benefit'' scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.",
        "published": "1998-08-26T17:39:07Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808015v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Segregatory Coordination and Ellipsis in Text Generation",
        "authors": [
            "James Shaw"
        ],
        "summary": "In this paper, we provide an account of how to generate sentences with coordination constructions from clause-sized semantic representations. An algorithm is developed to generate sentences with ellipsis, gapping, right-node-raising, and non-constituent coordination constructions. Various examples from linguistic literature will be used to demonstrate that the algorithm does its job well.",
        "published": "1998-08-27T19:59:49Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808016v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Variant of Earley Parsing",
        "authors": [
            "Mark-Jan Nederhof",
            "Giorgio Satta"
        ],
        "summary": "The Earley algorithm is a widely used parsing method in natural language processing applications. We introduce a variant of Earley parsing that is based on a ``delayed'' recognition of constituents. This allows us to start the recognition of a constituent only in cases in which all of its subconstituents have been found within the input string. This is particularly advantageous in several cases in which partial analysis of a constituent cannot be completed and in general in all cases of productions sharing some suffix of their right-hand sides (even for different left-hand side nonterminals). Although the two algorithms result in the same asymptotic time and space complexity, from a practical perspective our algorithm improves the time and space requirements of the original method, as shown by reported experimental results.",
        "published": "1998-08-31T11:59:40Z",
        "link": "http://arxiv.org/abs/cmp-lg/9808017v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Towards an implementable dependency grammar",
        "authors": [
            "Timo Jarvinen",
            "Pasi Tapanainen"
        ],
        "summary": "The aim of this paper is to define a dependency grammar framework which is both linguistically motivated and computationally parsable. See the demo at http://www.conexor.fi/analysers.html#testing",
        "published": "1998-09-01T18:26:06Z",
        "link": "http://arxiv.org/abs/cmp-lg/9809001v2",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Some Ontological Principles for Designing Upper Level Lexical Resources",
        "authors": [
            "Nicola Guarino"
        ],
        "summary": "The purpose of this paper is to explore some semantic problems related to the use of linguistic ontologies in information systems, and to suggest some organizing principles aimed to solve such problems. The taxonomic structure of current ontologies is unfortunately quite complicated and hard to understand, especially for what concerns the upper levels. I will focus here on the problem of ISA overloading, which I believe is the main responsible of these difficulties. To this purpose, I will carefully analyze the ontological nature of the categories used in current upper-level structures, considering the necessity of splitting them according to more subtle distinctions or the opportunity of excluding them because of their limited organizational role.",
        "published": "1998-09-09T14:49:01Z",
        "link": "http://arxiv.org/abs/cmp-lg/9809002v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "A Comparison of WordNet and Roget's Taxonomy for Measuring Semantic   Similarity",
        "authors": [
            "Michael Mc Hale"
        ],
        "summary": "This paper presents the results of using Roget's International Thesaurus as the taxonomy in a semantic similarity measurement task. Four similarity metrics were taken from the literature and applied to Roget's The experimental evaluation suggests that the traditional edge counting approach does surprisingly well (a correlation of r=0.88 with a benchmark set of human similarity judgements, with an upper bound of r=0.90 for human subjects performing the same task.)",
        "published": "1998-09-14T15:46:40Z",
        "link": "http://arxiv.org/abs/cmp-lg/9809003v1",
        "categories": [
            "cmp-lg",
            "cs.CL"
        ]
    },
    {
        "title": "Linear Segmentation and Segment Significance",
        "authors": [
            "Min-Yen Kan",
            "Judith L. Klavans",
            "Kathleen R. McKeown"
        ],
        "summary": "We present a new method for discovering a segmental discourse structure of a document while categorizing segment function. We demonstrate how retrieval of noun phrases and pronominal forms, along with a zero-sum weighting scheme, determines topicalized segmentation. Futhermore, we use term distribution to aid in identifying the role that the segment performs in the document. Finally, we present results of evaluation in terms of precision and recall which surpass earlier approaches.",
        "published": "1998-09-15T23:49:32Z",
        "link": "http://arxiv.org/abs/cs/9809020v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Producing NLP-based On-line Contentware",
        "authors": [
            "Francis Wolinski",
            "Frantz Vichot",
            "Olivier Gremont"
        ],
        "summary": "For its internal needs as well as for commercial purposes, CDC Group has produced several NLP-based on-line contentware applications for years. The development process of such applications is subject to numerous constraints such as quality of service, integration of new advances in NLP, direct reactions from users, continuous versioning, short delivery deadlines and cost control. Following this industrial and commercial experience, malleability of the applications, their openness towards foreign components, efficiency of applications and their ease of exploitation have appeared to be key points. In this paper, we describe TalLab, a powerful architecture for on-line contentware which fulfils these requirements.",
        "published": "1998-09-16T14:22:35Z",
        "link": "http://arxiv.org/abs/cs/9809021v1",
        "categories": [
            "cs.CL",
            "cs.AR",
            "I.2.7"
        ]
    },
    {
        "title": "Modelling Users, Intentions, and Structure in Spoken Dialog",
        "authors": [
            "Bernd Ludwig",
            "Guenther Goerz",
            "Heinrich Niemann"
        ],
        "summary": "We outline how utterances in dialogs can be interpreted using a partial first order logic. We exploit the capability of this logic to talk about the truth status of formulae to define a notion of coherence between utterances and explain how this coherence relation can serve for the construction of AND/OR trees that represent the segmentation of the dialog. In a BDI model we formalize basic assumptions about dialog and cooperative behaviour of participants. These assumptions provide a basis for inferring speech acts from coherence relations between utterances and attitudes of dialog participants. Speech acts prove to be useful for determining dialog segments defined on the notion of completing expectations of dialog participants. Finally, we sketch how explicit segmentation signalled by cue phrases and performatives is covered by our dialog model.",
        "published": "1998-09-17T11:10:14Z",
        "link": "http://arxiv.org/abs/cs/9809022v1",
        "categories": [
            "cs.CL",
            "H.5.2"
        ]
    },
    {
        "title": "A Lexicalized Tree Adjoining Grammar for English",
        "authors": [
            "XTAG Research Group"
        ],
        "summary": "This document describes a sizable grammar of English written in the TAG formalism and implemented for use with the XTAG system. This report and the grammar described herein supersedes the TAG grammar described in an earlier 1995 XTAG technical report. The English grammar described in this report is based on the TAG formalism which has been extended to include lexicalization, and unification-based feature structures. The range of syntactic phenomena that can be handled is large and includes auxiliaries (including inversion), copula, raising and small clause constructions, topicalization, relative clauses, infinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO constructions, noun-noun modifications, extraposition, determiner sequences, genitives, negation, noun-verb contractions, sentential adjuncts and imperatives. This technical report corresponds to the XTAG Release 8/31/98. The XTAG grammar is continuously updated with the addition of new analyses and modification of old ones, and an online version of this report can be found at the XTAG web page at http://www.cis.upenn.edu/~xtag/",
        "published": "1998-09-18T00:33:47Z",
        "link": "http://arxiv.org/abs/cs/9809024v2",
        "categories": [
            "cs.CL",
            "I.2.7; D.3.1"
        ]
    },
    {
        "title": "Prefix Probabilities from Stochastic Tree Adjoining Grammars",
        "authors": [
            "Mark-Jan Nederhof",
            "Anoop Sarkar",
            "Giorgio Satta"
        ],
        "summary": "Language models for speech recognition typically use a probability model of the form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other hand, are typically used to assign structure to utterances. A language model of the above form is constructed from such grammars by computing the prefix probability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all possible terminations of the prefix a_1 ... a_n. The main result in this paper is an algorithm to compute such prefix probabilities given a stochastic Tree Adjoining Grammar (TAG). The algorithm achieves the required computation in O(n^6) time. The probability of subderivations that do not derive any words in the prefix, but contribute structurally to its derivation, are precomputed to achieve termination. This algorithm enables existing corpus-based estimation techniques for stochastic TAGs to be used for language modelling.",
        "published": "1998-09-18T03:45:45Z",
        "link": "http://arxiv.org/abs/cs/9809026v1",
        "categories": [
            "cs.CL",
            "I.2.7; D.3.1"
        ]
    },
    {
        "title": "Conditions on Consistency of Probabilistic Tree Adjoining Grammars",
        "authors": [
            "Anoop Sarkar"
        ],
        "summary": "Much of the power of probabilistic methods in modelling language comes from their ability to compare several derivations for the same string in the language. An important starting point for the study of such cross-derivational properties is the notion of _consistency_. The probability model defined by a probabilistic grammar is said to be _consistent_ if the probabilities assigned to all the strings in the language sum to one. From the literature on probabilistic context-free grammars (CFGs), we know precisely the conditions which ensure that consistency is true for a given CFG. This paper derives the conditions under which a given probabilistic Tree Adjoining Grammar (TAG) can be shown to be consistent. It gives a simple algorithm for checking consistency and gives the formal justification for its correctness. The conditions derived here can be used to ensure that probability models that use TAGs can be checked for _deficiency_ (i.e. whether any probability mass is assigned to strings that cannot be generated).",
        "published": "1998-09-18T03:58:57Z",
        "link": "http://arxiv.org/abs/cs/9809027v1",
        "categories": [
            "cs.CL",
            "I.2.7; D.3.1"
        ]
    },
    {
        "title": "Separating Dependency from Constituency in a Tree Rewriting System",
        "authors": [
            "Anoop Sarkar"
        ],
        "summary": "In this paper we present a new tree-rewriting formalism called Link-Sharing Tree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using LSTAG we define an approach towards coordination where linguistic dependency is distinguished from the notion of constituency. Such an approach towards coordination that explicitly distinguishes dependencies from constituency gives a better formal understanding of its representation when compared to previous approaches that use tree-rewriting systems which conflate the two issues.",
        "published": "1998-09-18T04:44:02Z",
        "link": "http://arxiv.org/abs/cs/9809028v1",
        "categories": [
            "cs.CL",
            "I.2.7; D.3.1"
        ]
    },
    {
        "title": "Incremental Parser Generation for Tree Adjoining Grammars",
        "authors": [
            "Anoop Sarkar"
        ],
        "summary": "This paper describes the incremental generation of parse tables for the LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented handles modifications to the input grammar by updating the parser generated so far. In this paper, a lazy generation of LR-type parsers for TALs is defined in which parse tables are created by need while parsing. We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far.",
        "published": "1998-09-18T05:03:48Z",
        "link": "http://arxiv.org/abs/cs/9809029v1",
        "categories": [
            "cs.CL",
            "I.2.7; D.3.1"
        ]
    },
    {
        "title": "A Freely Available Morphological Analyzer, Disambiguator and Context   Sensitive Lemmatizer for German",
        "authors": [
            "Wolfgang Lezius",
            "Reinhard Rapp",
            "Manfred Wettler"
        ],
        "summary": "In this paper we present Morphy, an integrated tool for German morphology, part-of-speech tagging and context-sensitive lemmatization. Its large lexicon of more than 320,000 word forms plus its ability to process German compound nouns guarantee a wide morphological coverage. Syntactic ambiguities can be resolved with a standard statistical part-of-speech tagger. By using the output of the tagger, the lemmatizer can determine the correct root even for ambiguous word forms. The complete package is freely available and can be downloaded from the World Wide Web.",
        "published": "1998-09-23T12:59:39Z",
        "link": "http://arxiv.org/abs/cs/9809050v1",
        "categories": [
            "cs.CL",
            "H.3.4"
        ]
    },
    {
        "title": "Spoken Language Dialogue Systems and Components: Best practice in   development and evaluation (DISC 24823) - Periodic Progress Report 1: Basic   Details of the Action",
        "authors": [
            "Niels Ole Bernsen",
            "Laila Dybkjaer",
            "eds."
        ],
        "summary": "The DISC project aims to (a) build an in-depth understanding of the state-of-the-art in spoken language dialogue systems (SLDSs) and components development and evaluation with the purpose of (b) developing a first best practice methodology in the field. The methodology will be accompanied by (c) a series of development and evaluation support tools. To the limited extent possible within the duration of the project, the draft versions of the methodology and the tools will be (d) tested by SLDS developers from industry and research, and will be (e) packaged to best suit their needs. In the first year of DISC, (a) has been accomplished, and (b) and (c) have started. A proposal to complete the work proposed above by adding 12 months to the 18 months of the present project, has been submitted to Esprit Long-Term Research in March 1998.",
        "published": "1998-09-23T14:47:49Z",
        "link": "http://arxiv.org/abs/cs/9809051v1",
        "categories": [
            "cs.CL",
            "cs.SE",
            "I.2.7; H.5.2; D.2.2; I.3.6"
        ]
    },
    {
        "title": "Processing Unknown Words in HPSG",
        "authors": [
            "Petra Barg",
            "Markus Walther"
        ],
        "summary": "The lexical acquisition system presented in this paper incrementally updates linguistic properties of unknown words inferred from their surrounding context by parsing sentences with an HPSG grammar for German. We employ a gradual, information-based concept of ``unknownness'' providing a uniform treatment for the range of completely known to maximally unknown lexical entries. ``Unknown'' information is viewed as revisable information, which is either generalizable or specializable. Updating takes place after parsing, which only requires a modified lexical lookup. Revisable pieces of information are identified by grammar-specified declarations which provide access paths into the parse feature structure. The updating mechanism revises the corresponding places in the lexical feature structures iff the context actually provides new information. For revising generalizable information, type union is required. A worked-out example demonstrates the inferential capacity of our implemented system.",
        "published": "1998-09-25T11:02:08Z",
        "link": "http://arxiv.org/abs/cs/9809106v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Computing Declarative Prosodic Morphology",
        "authors": [
            "Markus Walther"
        ],
        "summary": "This paper describes a computational, declarative approach to prosodic morphology that uses inviolable constraints to denote small finite candidate sets which are filtered by a restrictive incremental optimization mechanism. The new approach is illustrated with an implemented fragment of Modern Hebrew verbs couched in MicroCUF, an expressive constraint logic formalism. For generation and parsing of word forms, I propose a novel off-line technique to eliminate run-time optimization. It produces a finite-state oracle that efficiently restricts the constraint interpreter's search space. As a byproduct, unknown words can be analyzed without special mechanisms. Unlike pure finite-state transducer approaches, this hybrid setup allows for more expressivity in constraints to specify e.g. token identity for reduplication or arithmetic constraints for phonetics.",
        "published": "1998-09-25T14:32:38Z",
        "link": "http://arxiv.org/abs/cs/9809107v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Similarity-Based Models of Word Cooccurrence Probabilities",
        "authors": [
            "Ido Dagan",
            "Lillian Lee",
            "Fernando C. N. Pereira"
        ],
        "summary": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words.   We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.   We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.",
        "published": "1998-09-27T18:42:51Z",
        "link": "http://arxiv.org/abs/cs/9809110v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.7;I.2.6"
        ]
    },
    {
        "title": "On the Evaluation and Comparison of Taggers: The Effect of Noise in   Testing Corpora",
        "authors": [
            "L. Padro",
            "L. Marquez"
        ],
        "summary": "This paper addresses the issue of {\\sc pos} tagger evaluation. Such evaluation is usually performed by comparing the tagger output with a reference test corpus, which is assumed to be error-free. Currently used corpora contain noise which causes the obtained performance to be a distortion of the real value. We analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improvement given by a new system. The main conclusion is that a more rigorous testing experimentation setting/designing is needed to reliably evaluate and compare tagger accuracies.",
        "published": "1998-09-28T07:49:11Z",
        "link": "http://arxiv.org/abs/cs/9809112v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Improving Tagging Performance by Using Voting Taggers",
        "authors": [
            "L. Marquez",
            "L. Padro",
            "H. Rodriguez"
        ],
        "summary": "We present a bootstrapping method to develop an annotated corpus, which is specially useful for languages with few available resources. The method is being applied to develop a corpus of Spanish of over 5Mw. The method consists on taking advantage of the collaboration of two different POS taggers. The cases in which both taggers agree present a higher accuracy and are used to retrain the taggers.",
        "published": "1998-09-28T07:50:55Z",
        "link": "http://arxiv.org/abs/cs/9809113v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Ultrametric Distance in Syntax",
        "authors": [
            "Mark D. Roberts"
        ],
        "summary": "Phrase structure trees have a hierarchical structure. In many subjects, most notably in Taxonomy such tree structures have been studied using ultrametrics. Here syntactical hierarchical phrase trees are subject to a similar analysis, which is much siompler as the branching structure is more readily discernible and switched. The occurence of hierarchical structure elsewhere in linguistics is mentioned. The phrase tree can be represented by a matrix and the elements of the matrix can be represented by triangles. The height at which branching occurs is not prescribed in previous syntatic models, but it is by using the ultrametric matrix. The ambiguity of which branching height to choose is resolved by postulating that branching occurs at the lowest height available. An ultrametric produces a measure of the complexity of sentences: presumably the complexity of sentence increases as a language is aquired so that this can be tested. A All ultrametric triangles are equilateral or isocles, here it is shown that X structur implies that there are no equilateral triangles. Restricting attention to simple syntax a minium ultrametric distance between lexical categories is calculatex. This ultrametric distance is shown to be different than the matrix obtasined from feaures. It is shown that the definition of c-commabnd can be replaced by an equivalent ultrametric definition. The new definition invokes a minimum distance between nodes and this is more aesthetically satisfing than previouv varieties of definitions.   From the new definition of c-command follows a new definition of government.",
        "published": "1998-10-13T10:24:56Z",
        "link": "http://arxiv.org/abs/cs/9810012v4",
        "categories": [
            "cs.CL",
            "q-bio.NC",
            "I.2.7;J.4;I.2.6"
        ]
    },
    {
        "title": "Resources for Evaluation of Summarization Techniques",
        "authors": [
            "Judith L. Klavans",
            "Kathleen R. McKeown",
            "Min-Yen Kan",
            "Susan Lee"
        ],
        "summary": "We report on two corpora to be used in the evaluation of component systems for the tasks of (1) linear segmentation of text and (2) summary-directed sentence extraction. We present characteristics of the corpora, methods used in the collection of user judgments, and an overview of the application of the corpora to evaluating the component system. Finally, we discuss the problems and issues with construction of the test set which apply broadly to the construction of evaluation resources for language technologies.",
        "published": "1998-10-13T20:33:05Z",
        "link": "http://arxiv.org/abs/cs/9810014v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Restrictions on Tree Adjoining Languages",
        "authors": [
            "Giorgio Satta",
            "William Schuler"
        ],
        "summary": "Several methods are known for parsing languages generated by Tree Adjoining Grammars (TAGs) in O(n^6) worst case running time. In this paper we investigate which restrictions on TAGs and TAG derivations are needed in order to lower this O(n^6) time complexity, without introducing large runtime constants, and without losing any of the generative power needed to capture the syntactic constructions in natural language that can be handled by unrestricted TAGs. In particular, we describe an algorithm for parsing a strict subclass of TAG in O(n^5), and attempt to show that this subclass retains enough generative power to make it useful in the general case.",
        "published": "1998-10-13T21:17:13Z",
        "link": "http://arxiv.org/abs/cs/9810015v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Winnow-Based Approach to Context-Sensitive Spelling Correction",
        "authors": [
            "Andrew R. Golding",
            "Dan Roth"
        ],
        "summary": "A large class of machine-learning problems in natural language require the characterization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts refer to only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good theoretical properties. We present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting \"to\" for \"too\", \"casual\" for \"causal\", etc. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned condition; (2) When compared with other systems in the literature, WinSpell exhibits the highest performance; (3) The primary reason that WinSpell outperforms BaySpell is that WinSpell learns a better linear separator; (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set.",
        "published": "1998-10-31T19:33:50Z",
        "link": "http://arxiv.org/abs/cs/9811003v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Does Meaning Evolve?",
        "authors": [
            "Mark D. Roberts"
        ],
        "summary": "A common method of making a theory more understandable, is by comparing it to another theory which has been better developed. Radical interpretation is a theory which attempts to explain how communication has meaning. Radical interpretation is treated as another time-dependent theory and compared to the time dependent theory of biological evolution. The main reason for doing this is to find the nature of the time dependence; producing analogs between the two theories is a necessary prerequisite to this and brings up many problems. Once the nature of the time dependence is better known it might allow the underlying mechanism to be uncovered. Several similarities and differences are uncovered, there appear to be more differences than similarities.",
        "published": "1998-11-01T11:18:22Z",
        "link": "http://arxiv.org/abs/cs/9811004v4",
        "categories": [
            "cs.CL",
            "q-bio.PE",
            "I.2.7; J.4; I.2.0"
        ]
    },
    {
        "title": "Machine Learning of Generic and User-Focused Summarization",
        "authors": [
            "Inderjeet Mani",
            "Eric Bloedorn"
        ],
        "summary": "A key problem in text summarization is finding a salience function which determines what information in the source should be included in the summary. This paper describes the use of machine learning on a training corpus of documents and their abstracts to discover salience functions which describe what combination of features is optimal for a given summarization task. The method addresses both \"generic\" and user-focused summaries.",
        "published": "1998-11-02T18:57:23Z",
        "link": "http://arxiv.org/abs/cs/9811006v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Translating near-synonyms: Possibilities and preferences in the   interlingua",
        "authors": [
            "Philip Edmonds"
        ],
        "summary": "This paper argues that an interlingual representation must explicitly represent some parts of the meaning of a situation as possibilities (or preferences), not as necessary or definite components of meaning (or constraints). Possibilities enable the analysis and generation of nuance, something required for faithful translation. Furthermore, the representation of the meaning of words, especially of near-synonyms, is crucial, because it specifies which nuances words can convey in which contexts.",
        "published": "1998-11-02T21:29:41Z",
        "link": "http://arxiv.org/abs/cs/9811008v1",
        "categories": [
            "cs.CL",
            "I.2.7; I.2.4"
        ]
    },
    {
        "title": "Choosing the Word Most Typical in Context Using a Lexical Co-occurrence   Network",
        "authors": [
            "Philip Edmonds"
        ],
        "summary": "This paper presents a partial solution to a component of the problem of lexical choice: choosing the synonym most typical, or expected, in context. We apply a new statistical approach to representing the context of a word through lexical co-occurrence networks. The implementation was trained and evaluated on a large corpus, and results show that the inclusion of second-order co-occurrence relations improves the performance of our implemented lexical choice program.",
        "published": "1998-11-02T23:06:19Z",
        "link": "http://arxiv.org/abs/cs/9811009v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Learning to Resolve Natural Language Ambiguities: A Unified Approach",
        "authors": [
            "Dan Roth"
        ],
        "summary": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be re-cast as learning linear separators in the feature space. Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.   We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.   In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.",
        "published": "1998-11-03T21:14:32Z",
        "link": "http://arxiv.org/abs/cs/9811010v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6 I.2.7"
        ]
    },
    {
        "title": "Comparing a statistical and a rule-based tagger for German",
        "authors": [
            "Martin Volk",
            "Gerold Schneider"
        ],
        "summary": "In this paper we present the results of comparing a statistical tagger for German based on decision trees and a rule-based Brill-Tagger for German. We used the same training corpus (and therefore the same tag-set) to train both taggers. We then applied the taggers to the same test corpus and compared their respective behavior and in particular their error rates. Both taggers perform similarly with an error rate of around 5%. From the detailed error analysis it can be seen that the rule-based tagger has more problems with unknown words than the statistical tagger. But the results are opposite for tokens that are many-ways ambiguous. If the unknown words are fed into the taggers with the help of an external lexicon (such as the Gertwol system) the error rate of the rule-based tagger drops to 4.7%, and the respective rate of the statistical taggers drops to around 3.7%. Combining the taggers by using the output of one tagger to help the other did not lead to any further improvement.",
        "published": "1998-11-11T11:06:34Z",
        "link": "http://arxiv.org/abs/cs/9811016v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "P-model Alternative to the T-model",
        "authors": [
            "Mark D. Roberts"
        ],
        "summary": "Standard linguistic analysis of syntax uses the T-model. This model requires the ordering: D-structure $>$ S-structure $>$ LF. Between each of these representations there is movement which alters the order of the constituent words; movement is achieved using the principles and parameters of syntactic theory. Psychological serial models do not accommodate the T-model immediately so that here a new model called the P-model is introduced. Here it is argued that the LF representation should be replaced by a variant of Frege's three qualities. In the F-representation the order of elements is not necessarily the same as that in LF and it is suggested that the correct ordering is: F-representation $>$ D-structure $>$ S-structure. Within this framework movement originates as the outcome of emphasis applied to the sentence.",
        "published": "1998-11-11T14:17:09Z",
        "link": "http://arxiv.org/abs/cs/9811018v3",
        "categories": [
            "cs.CL",
            "q-bio.NC",
            "I.2.7;J.4;I.2.6"
        ]
    },
    {
        "title": "Expoiting Syntactic Structure for Language Modeling",
        "authors": [
            "Ciprian Chelba",
            "Frederick Jelinek"
        ],
        "summary": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words--binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.",
        "published": "1998-11-12T17:31:17Z",
        "link": "http://arxiv.org/abs/cs/9811022v2",
        "categories": [
            "cs.CL",
            "G.3, I.2.7, I.5.1, I.5.4"
        ]
    },
    {
        "title": "A Structured Language Model",
        "authors": [
            "Ciprian Chelba"
        ],
        "summary": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words - binary-parse-structure with headword annotation. The model, its probabilistic parametrization, and a set of experiments meant to evaluate its predictive power are presented.",
        "published": "1998-11-13T16:53:15Z",
        "link": "http://arxiv.org/abs/cs/9811025v2",
        "categories": [
            "cs.CL",
            "G.3, I.2.7, I.5.1, I.5.4"
        ]
    },
    {
        "title": "A Probabilistic Approach to Lexical Semantic Knowledge Acquisition and S   tructural Disambiguation",
        "authors": [
            "Hang LI"
        ],
        "summary": "In this thesis, I address the problem of automatically acquiring lexical semantic knowledge, especially that of case frame patterns, from large corpus data and using the acquired knowledge in structural disambiguation. The approach I adopt has the following characteristics: (1) dividing the problem into three subproblems: case slot generalization, case dependency learning, and word clustering (thesaurus construction). (2) viewing each subproblem as that of statistical estimation and defining probability models for each subproblem, (3) adopting the Minimum Description Length (MDL) principle as learning strategy, (4) employing efficient learning algorithms, and (5) viewing the disambiguation problem as that of statistical prediction. Major contributions of this thesis include: (1) formalization of the lexical knowledge acquisition problem, (2) development of a number of learning methods for lexical knowledge acquisition, and (3) development of a high-performance disambiguation method.",
        "published": "1998-12-01T11:43:32Z",
        "link": "http://arxiv.org/abs/cs/9812001v3",
        "categories": [
            "cs.CL",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "Name Strategy: Its Existence and Implications",
        "authors": [
            "Mark D. Roberts"
        ],
        "summary": "It is argued that colour name strategy, object name strategy, and chunking strategy in memory are all aspects of the same general phenomena, called stereotyping. It is pointed out that the Berlin-Kay universal partial ordering of colours and the frequency of traffic accidents classified by colour are surprisingly similar. Some consequences of the existence of a name strategy for the philosophy of language and mathematics are discussed. It is argued that real valued quantities occur {\\it ab initio}. The implication of real valued truth quantities is that the {\\bf Continuum Hypothesis} of pure mathematics is side-stepped. The existence of name strategy shows that thought/sememes and talk/phonemes can be separate, and this vindicates the assumption of thought occurring before talk used in psycholinguistic speech production models.",
        "published": "1998-12-04T12:28:19Z",
        "link": "http://arxiv.org/abs/cs/9812004v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "math.HO",
            "I.2.6;J.4;I.2.7"
        ]
    },
    {
        "title": "Optimal Multi-Paragraph Text Segmentation by Dynamic Programming",
        "authors": [
            "Oskari Heinonen"
        ],
        "summary": "There exist several methods of calculating a similarity curve, or a sequence of similarity values, representing the lexical cohesion of successive text constituents, e.g., paragraphs. Methods for deciding the locations of fragment boundaries are, however, scarce. We propose a fragmentation method based on dynamic programming. The method is theoretically sound and guaranteed to provide an optimal splitting on the basis of a similarity curve, a preferred fragment length, and a cost function defined. The method is especially useful when control on fragment size is of importance.",
        "published": "1998-12-04T16:16:35Z",
        "link": "http://arxiv.org/abs/cs/9812005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Flexible Shallow Approach to Text Generation",
        "authors": [
            "Stephan Busemann",
            "Helmut Horacek"
        ],
        "summary": "In order to support the efficient development of NL generation systems, two orthogonal methods are currently pursued with emphasis: (1) reusable, general, and linguistically motivated surface realization components, and (2) simple, task-oriented template-based techniques. In this paper we argue that, from an application-oriented perspective, the benefits of both are still limited. In order to improve this situation, we suggest and evaluate shallow generation methods associated with increased flexibility. We advise a close connection between domain-motivated and linguistic ontologies that supports the quick adaptation to new tasks and domains, rather than the reuse of general resources. Our method is especially designed for generating reports with limited linguistic variations.",
        "published": "1998-12-16T16:37:01Z",
        "link": "http://arxiv.org/abs/cs/9812018v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Forgetting Exceptions is Harmful in Language Learning",
        "authors": [
            "Walter Daelemans",
            "Antal van den Bosch",
            "Jakub Zavrel"
        ],
        "summary": "We show that in language learning, contrary to received wisdom, keeping exceptional training instances in memory can be beneficial for generalization accuracy. We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks: grapheme-to-phoneme conversion, part-of-speech tagging, prepositional-phrase attachment, and base noun phrase chunking. In a first series of experiments we combine memory-based learning with training set editing techniques, in which instances are edited based on their typicality and class prediction strength. Results show that editing exceptional instances (with low typicality or low class prediction strength) tends to harm generalization accuracy. In a second series of experiments we compare memory-based learning and decision-tree learning methods on the same selection of tasks, and find that decision-tree learning often performs worse than memory-based learning. Moreover, the decrease in performance can be linked to the degree of abstraction from exceptions (i.e., pruning or eagerness). We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms.",
        "published": "1998-12-22T16:33:19Z",
        "link": "http://arxiv.org/abs/cs/9812021v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Using Local Optimality Criteria for Efficient Information Retrieval with   Redundant Information Filters",
        "authors": [
            "Neil C. Rowe"
        ],
        "summary": "We consider information retrieval when the data, for instance multimedia, is coputationally expensive to fetch. Our approach uses \"information filters\" to considerably narrow the universe of possiblities before retrieval. We are especially interested in redundant information filters that save time over more general but more costly filters. Efficient retrieval requires that decision must be made about the necessity, order, and concurrent processing of proposed filters (an \"execution plan\"). We develop simple polynomial-time local criteria for optimal execution plans, and show that most forms of concurrency are suboptimal with information filters. Although the general problem of finding an optimal execution plan is likely exponential in the number of filters, we show experimentally that our local optimality criteria, used in a polynomial-time algorithm, nearly always find the global optimum with 15 filters or less, a sufficient number of filters for most applications. Our methods do not require special hardware and avoid the high processor idleness that is characteristic of massive parallelism solutions to this problem. We apply our ideas to an important application, information retrieval of cpationed data using natural-language understanding, a problem for which the natural-language processing can be the bottleneck if not implemented well.",
        "published": "1998-09-29T21:55:20Z",
        "link": "http://arxiv.org/abs/cs/9809121v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.3.3"
        ]
    },
    {
        "title": "Comparing the expressive power of the Synchronous and the Asynchronous   pi-calculus",
        "authors": [
            "Catuscia Palamidessi"
        ],
        "summary": "The Asynchronous pi-calculus, as recently proposed by Boudol and, independently, by Honda and Tokoro, is a subset of the pi-calculus which contains no explicit operators for choice and output-prefixing. The communication mechanism of this calculus, however, is powerful enough to simulate output-prefixing, as shown by Boudol, and input-guarded choice, as shown recently by Nestmann and Pierce. A natural question arises, then, whether or not it is possible to embed in it the full pi-calculus. We show that this is not possible, i.e. there does not exist any uniform, parallel-preserving, translation from the pi-calculus into the asynchronous pi-calculus, up to any ``reasonable'' notion of equivalence. This result is based on the incapablity of the asynchronous pi-calculus of breaking certain symmetries possibly present in the initial communication graph. By similar arguments, we prove a separation result between the pi-calculus and CCS.",
        "published": "1998-09-02T17:40:46Z",
        "link": "http://arxiv.org/abs/cs/9809008v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3, F.3"
        ]
    },
    {
        "title": "Scoping Constructs in Logic Programming: Implementation Problems and   their Solution",
        "authors": [
            "Gopalan Nadathur",
            "Bharat Jayaraman",
            "Keehang Kwon"
        ],
        "summary": "The inclusion of universal quantification and a form of implication in goals in logic programming is considered. These additions provide a logical basis for scoping but they also raise new implementation problems. When universal and existential quantifiers are permitted to appear in mixed order in goals, the devices of logic variables and unification that are employed in solving existential goals must be modified to ensure that constraints arising out of the order of quantification are respected. Suitable modifications that are based on attaching numerical tags to constants and variables and on using these tags in unification are described. The resulting devices are amenable to an efficient implementation and can, in fact, be assimilated easily into the usual machinery of the Warren Abstract Machine (WAM). The provision of implications in goals results in the possibility of program clauses being added to the program for the purpose of solving specific subgoals. A naive scheme based on asserting and retracting program clauses does not suffice for implementing such additions for two reasons. First, it is necessary to also support the resurrection of an earlier existing program in the face of backtracking. Second, the possibility for implication goals to be surrounded by quantifiers requires a consideration of the parameterization of program clauses by bindings for their free variables. Devices for supporting these additional requirements are described as also is the integration of these devices into the WAM. Further extensions to the machine are outlined for handling higher-order additions to the language. The ideas presented here are relevant to the implementation of the higher-order logic programming language lambda Prolog.",
        "published": "1998-09-10T16:54:05Z",
        "link": "http://arxiv.org/abs/cs/9809016v1",
        "categories": [
            "cs.PL",
            "D.3.2"
        ]
    },
    {
        "title": "On Dart-Zobel Algorithm for Testing Regular Type Inclusion",
        "authors": [
            "Lunjin Lu",
            "John G. Cleary"
        ],
        "summary": "This paper answers open questions about the correctness and the completeness of Dart-Zobel algorithm for testing the inclusion relation between two regular types. We show that the algorithm is incorrect for regular types. We also prove that the algorithm is complete for regular types as well as correct for tuple distributive regular types. Also presented is a simplified version of Dart-Zobel algorithm for tuple distributive regular types.",
        "published": "1998-10-01T02:33:17Z",
        "link": "http://arxiv.org/abs/cs/9810001v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "Active Libraries: Rethinking the roles of compilers and libraries",
        "authors": [
            "Todd L. Veldhuizen",
            "Dennis Gannon"
        ],
        "summary": "We describe Active Libraries, which take an active role in compilation. Unlike traditional libraries which are passive collections of functions and objects, Active Libraries may generate components, specialize algorithms, optimize code, configure and tune themselves for a target machine, and describe themselves to tools (such as profilers and debuggers) in an intelligible way. Several such libraries are described, as are implementation technologies.",
        "published": "1998-10-05T15:27:42Z",
        "link": "http://arxiv.org/abs/math/9810022v1",
        "categories": [
            "math.NA",
            "cs.PL"
        ]
    },
    {
        "title": "C++ Templates as Partial Evaluation",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "This paper explores the relationship between C++ templates and partial evaluation. Templates were designed to support generic programming, but unintentionally provided the ability to perform compile-time computations and code generation. These features are completely accidental, and as a result their syntax is awkward. By recasting these features in terms of partial evaluation, a much simpler syntax can be achieved. C++ may be regarded as a two-level language in which types are first-class values. Template instantiation resembles an offline partial evaluator. This paper describes preliminary work toward a single mechanism based on Partial Evaluation which unifies generic programming, compile-time computation and code generation. The language Catat is introduced to illustrate these ideas.",
        "published": "1998-10-09T20:29:43Z",
        "link": "http://arxiv.org/abs/cs/9810010v2",
        "categories": [
            "cs.PL",
            "cs.PF",
            "F.3.2; D.3.3; D.3.4"
        ]
    },
    {
        "title": "Early Experience with ASDL in lcc",
        "authors": [
            "David R. Hanson"
        ],
        "summary": "The Abstract Syntax Description Language (ASDL) is a language for specifying the tree data structures often found in compiler intermediate representations. The ASDL generator reads an ASDL specification and generates code to construct, read, and write instances of the trees specified. Using ASDL permits a compiler to be decomposed into semi-independent components that communicate by reading and writing trees. Each component can be written in a different language, because the ASDL generator can emit code in several languages, and the files written by ASDL-generated code are machine- and language-independent. ASDL is part of the National Compiler Infrastructure project, which seeks to reduce dramatically the overhead of computer systems research by making it much easier to build high-quality compilers. This paper describes dividing lcc, a widely used retargetable C compiler, into two components that communicate via trees defined in ASDL. As the first use of ASDL in a `real' compiler, this experience reveals much about the effort required to retrofit an existing compiler to use ASDL, the overheads involved, and the strengths and weaknesses of ASDL itself and, secondarily, of lcc.",
        "published": "1998-10-13T20:38:55Z",
        "link": "http://arxiv.org/abs/cs/9810013v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.4"
        ]
    },
    {
        "title": "A Proof Theoretic View of Constraint Programming",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We provide here a proof theoretic account of constraint programming that attempts to capture the essential ingredients of this programming style. We exemplify it by presenting proof rules for linear constraints over interval domains, and illustrate their use by analyzing the constraint propagation process for the {\\tt SEND + MORE = MONEY} puzzle. We also show how this approach allows one to build new constraint solvers.",
        "published": "1998-10-20T11:23:05Z",
        "link": "http://arxiv.org/abs/cs/9810018v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "F.4.1;I.2.3;D.1.0"
        ]
    },
    {
        "title": "Linguistic Reflection in Java",
        "authors": [
            "G. N. C. Kirby",
            "R. Morrison",
            "D. W. Stemple"
        ],
        "summary": "Reflective systems allow their own structures to be altered from within. Here we are concerned with a style of reflection, called linguistic reflection, which is the ability of a running program to generate new program fragments and to integrate these into its own execution. In particular we describe how this kind of reflection may be provided in the compiler-based, strongly typed object-oriented programming language Java. The advantages of the programming technique include attaining high levels of genericity and accommodating system evolution. These advantages are illustrated by an example taken from persistent programming which shows how linguistic reflection allows functionality (program code) to be generated on demand (Just-In-Time) from a generic specification and integrated into the evolving running program. The technique is evaluated against alternative implementation approaches with respect to efficiency, safety and ease of use.",
        "published": "1998-10-29T14:30:59Z",
        "link": "http://arxiv.org/abs/cs/9810027v1",
        "categories": [
            "cs.PL",
            "D.1.0"
        ]
    },
    {
        "title": "A Polymorphic Groundness Analysis of Logic Programs",
        "authors": [
            "Lunjin Lu"
        ],
        "summary": "A polymorphic analysis is an analysis whose input and output contain parameters which serve as placeholders for information that is unknown before analysis but provided after analysis. In this paper, we present a polymorphic groundness analysis that infers parameterised groundness descriptions of the variables of interest at a program point. The polymorphic groundness analysis is designed by replacing two primitive operators used in a monomorphic groundness analysis and is shown to be as precise as the monomorphic groundness analysis for any possible values for mode parameters. Experimental results of a prototype implementation of the polymorphic groundness analysis are given.",
        "published": "1998-10-31T07:20:37Z",
        "link": "http://arxiv.org/abs/cs/9811001v1",
        "categories": [
            "cs.PL",
            "F.3.2;D.3.2"
        ]
    },
    {
        "title": "Deriving Abstract Semantics for Forward Analysis of Normal Logic   Programs",
        "authors": [
            "Lunjin Lu"
        ],
        "summary": "The problem of forward abstract interpretation of {\\em normal} logic programs has not been formally addressed in the literature although negation as failure is dealt with through the built-in predicate ! in the way it is implemented in Prolog. This paper proposes a solution to this problem by deriving two generic fixed-point abstract semantics $F^b and $F^\\diamond for forward abstract interpretation of {\\em normal} logic programs. $F^b$ is intended for inferring data descriptions for edges in the program graph where an edge denotes the possibility that the control of execution transfers from its source program point to its destination program point. $F^\\diamond$ is derived from $F^b$ and is intended for inferring data descriptions for textual program points.",
        "published": "1998-11-06T03:37:44Z",
        "link": "http://arxiv.org/abs/cs/9811012v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2;D.3.2"
        ]
    },
    {
        "title": "An Emptiness Algorithm for Regular Types with Set Operators",
        "authors": [
            "Lunjin Lu",
            "John G. Cleary"
        ],
        "summary": "An algorithm to decide the emptiness of a regular type expression with set operators given a set of parameterised type definitions is presented. The algorithm can also be used to decide the equivalence of two regular type expressions and the inclusion of one regular type expression in another. The algorithm strictly generalises previous work in that tuple distributivity is not assumed and set operators are permitted in type expressions.",
        "published": "1998-11-11T04:50:28Z",
        "link": "http://arxiv.org/abs/cs/9811015v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2; F.3.2"
        ]
    },
    {
        "title": "Automatic Hardware Synthesis for a Hybrid Reconfigurable CPU Featuring   Philips CPLDs",
        "authors": [
            "Bernardo Kastrup"
        ],
        "summary": "A high-level architecture of a Hybrid Reconfigurable CPU, based on a Philips-supported core processor, is introduced. It features the Philips XPLA2 CPLD as a reconfigurable functional unit. A compilation chain is presented, in which automatic implementation of time-critical program segments in custom hardware is performed. The entire process is transparent from the programmer's point of view. The hardware synthesis module of the chain, which translates segments of assembly code into a hardware netlist, is discussed in details. Application examples are also presented.",
        "published": "1998-11-12T09:49:57Z",
        "link": "http://arxiv.org/abs/cs/9811021v1",
        "categories": [
            "cs.PL",
            "cs.AR",
            "D.3.4; C.1.3"
        ]
    },
    {
        "title": "Computational Geometry Column 34",
        "authors": [
            "Pankaj K. Agarwal",
            "Joseph O'Rourke"
        ],
        "summary": "Problems presented at the open-problem session of the 14th Annual ACM Symposium on Computational Geometry are listed.",
        "published": "1998-08-31T19:04:14Z",
        "link": "http://arxiv.org/abs/cs/9808008v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Separation-Sensitive Collision Detection for Convex Objects",
        "authors": [
            "Jeff Erickson",
            "Leonidas J. Guibas",
            "Jorge Stolfi",
            "Li Zhang"
        ],
        "summary": "We develop a class of new kinetic data structures for collision detection between moving convex polytopes; the performance of these structures is sensitive to the separation of the polytopes during their motion. For two convex polygons in the plane, let $D$ be the maximum diameter of the polygons, and let $s$ be the minimum distance between them during their motion. Our separation certificate changes $O(\\log(D/s))$ times when the relative motion of the two polygons is a translation along a straight line or convex curve, $O(\\sqrt{D/s})$ for translation along an algebraic trajectory, and $O(D/s)$ for algebraic rigid motion (translation and rotation). Each certificate update is performed in $O(\\log(D/s))$ time. Variants of these data structures are also shown that exhibit \\emph{hysteresis}---after a separation certificate fails, the new certificate cannot fail again until the objects have moved by some constant fraction of their current separation. We can then bound the number of events by the combinatorial size of a certain cover of the motion path by balls.",
        "published": "1998-09-18T23:16:06Z",
        "link": "http://arxiv.org/abs/cs/9809035v2",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2;I.3.5"
        ]
    },
    {
        "title": "Regression Depth and Center Points",
        "authors": [
            "Nina Amenta",
            "Marshall Bern",
            "David Eppstein",
            "Shang-Hua Teng"
        ],
        "summary": "We show that, for any set of n points in d dimensions, there exists a hyperplane with regression depth at least ceiling(n/(d+1)). as had been conjectured by Rousseeuw and Hubert. Dually, for any arrangement of n hyperplanes in d dimensions there exists a point that cannot escape to infinity without crossing at least ceiling(n/(d+1)) hyperplanes. We also apply our approach to related questions on the existence of partitions of the data into subsets such that a common plane has nonzero regression depth in each subset, and to the computational complexity of regression depth problems.",
        "published": "1998-09-21T21:55:49Z",
        "link": "http://arxiv.org/abs/cs/9809037v2",
        "categories": [
            "cs.CG",
            "math.CO",
            "G.3"
        ]
    },
    {
        "title": "Incremental and Decremental Maintenance of Planar Width",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We present an algorithm for maintaining the width of a planar point set dynamically, as points are inserted or deleted. Our algorithm takes time O(kn^epsilon) per update, where k is the amount of change the update causes in the convex hull, n is the number of points in the set, and epsilon is any arbitrarily small constant. For incremental or decremental update sequences, the amortized time per update is O(n^epsilon).",
        "published": "1998-09-21T23:31:16Z",
        "link": "http://arxiv.org/abs/cs/9809038v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Optimal Point Placement for Mesh Smoothing",
        "authors": [
            "Nina Amenta",
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We study the problem of moving a vertex in an unstructured mesh of triangular, quadrilateral, or tetrahedral elements to optimize the shapes of adjacent elements. We show that many such problems can be solved in linear time using generalized linear programming. We also give efficient algorithms for some mesh smoothing problems that do not fit into the generalized linear programming paradigm.",
        "published": "1998-09-24T21:02:45Z",
        "link": "http://arxiv.org/abs/cs/9809081v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Linear Complexity Hexahedral Mesh Generation",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show that any polyhedron forming a topological ball with an even number of quadrilateral sides can be partitioned into O(n) topological cubes, meeting face to face. The result generalizes to non-simply-connected polyhedra satisfying an additional bipartiteness condition. The same techniques can also be used to reduce the geometric version of the hexahedral mesh generation problem to a finite case analysis amenable to machine solution.",
        "published": "1998-09-26T23:44:27Z",
        "link": "http://arxiv.org/abs/cs/9809109v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "A role of constraint in self-organization",
        "authors": [
            "Carlos Domingo",
            "Osamu Watanabe",
            "Tadashi Yamazaki"
        ],
        "summary": "In this paper we introduce a neural network model of self-organization. This model uses a variation of Hebb rule for updating its synaptic weights, and surely converges to the equilibrium status. The key point of the convergence is the update rule that constrains the total synaptic weight and this seems to make the model stable. We investigate the role of the constraint and show that it is the constraint that makes the model stable. For analyzing this setting, we propose a simple probabilistic game that models the neural network and the self-organization process. Then, we investigate the characteristics of this game, namely, the probability that the game becomes stable and the number of the steps it takes.",
        "published": "1998-09-30T04:06:36Z",
        "link": "http://arxiv.org/abs/cs/9809123v1",
        "categories": [
            "cs.NE",
            "cs.CG",
            "I.2.6;J.3"
        ]
    },
    {
        "title": "Randomization yields simple O(n log star n) algorithms for difficult   Omega(n) problems",
        "authors": [
            "Olivier Devillers"
        ],
        "summary": "We use here the results on the influence graph by Boissonnat et al. to adapt them for particular cases where additional information is available. In some cases, it is possible to improve the expected randomized complexity of algorithms from O(n log n) to O(n log star n).   This technique applies in the following applications: triangulation of a simple polygon, skeleton of a simple polygon, Delaunay triangulation of points knowing the EMST (euclidean minimum spanning tree).",
        "published": "1998-10-07T08:03:32Z",
        "link": "http://arxiv.org/abs/cs/9810007v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Object-Oriented Design of Graph Oriented Data Structures",
        "authors": [
            "Maurizio Pizzonia",
            "Giuseppe Di Battista"
        ],
        "summary": "Applied research in graph algorithms and combinatorial structures needs comprehensive and versatile software libraries. However, the design and the implementation of flexible libraries are challenging activities. Among the other problems involved in such a difficult field, a very special role is played by graph classification issues.   We propose new techniques devised to help the designer and the programmer in the development activities. Such techniques are especially suited for dealing with graph classification problems and rely on an extension of the usual object-oriented paradigm. In order to support the usage of our approach, we devised an extension of the C++ programming language and implemented the corresponding pre-compiler.",
        "published": "1998-10-08T23:54:51Z",
        "link": "http://arxiv.org/abs/cs/9810009v1",
        "categories": [
            "cs.SE",
            "cs.CG",
            "cs.DS",
            "D.2.3; D.2.11; D.2.13; E.1"
        ]
    },
    {
        "title": "Computational Geometry Column 33",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "Several recent SIGGRAPH papers on surface simplification are described.",
        "published": "1998-10-22T20:44:35Z",
        "link": "http://arxiv.org/abs/cs/9810020v1",
        "categories": [
            "cs.CG",
            "cs.AI",
            "cs.GR",
            "F.2.2;I.3"
        ]
    },
    {
        "title": "Computational Geometry Column 32",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The proof of Dey's new k-set bound is illustrated.",
        "published": "1998-10-22T21:01:36Z",
        "link": "http://arxiv.org/abs/cs/9810021v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2"
        ]
    },
    {
        "title": "Locked and Unlocked Polygonal Chains in 3D",
        "authors": [
            "T. Biedl",
            "E. Demaine",
            "M. Demaine",
            "S. Lazard",
            "A. Lubiw",
            "J. O'Rourke",
            "M. Overmars",
            "S. Robbins",
            "I. Streinu",
            "G. Toussaint",
            "S. Whitesides"
        ],
        "summary": "In this paper, we study movements of simple polygonal chains in 3D. We say that an open, simple polygonal chain can be straightened if it can be continuously reconfigured to a straight sequence of segments in such a manner that both the length of each link and the simplicity of the chain are maintained throughout the movement. The analogous concept for closed chains is convexification: reconfiguration to a planar convex polygon. Chains that cannot be straightened or convexified are called locked. While there are open chains in 3D that are locked, we show that if an open chain has a simple orthogonal projection onto some plane, it can be straightened. For closed chains, we show that there are unknotted but locked closed chains, and we provide an algorithm for convexifying a planar simple polygon in 3D with a polynomial number of moves.",
        "published": "1998-11-11T20:36:50Z",
        "link": "http://arxiv.org/abs/cs/9811019v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.RO",
            "F.2.2; I.2.9"
        ]
    },
    {
        "title": "Separation-Sensitive Collision Detection for Convex Objects",
        "authors": [
            "Jeff Erickson",
            "Leonidas J. Guibas",
            "Jorge Stolfi",
            "Li Zhang"
        ],
        "summary": "We develop a class of new kinetic data structures for collision detection between moving convex polytopes; the performance of these structures is sensitive to the separation of the polytopes during their motion. For two convex polygons in the plane, let $D$ be the maximum diameter of the polygons, and let $s$ be the minimum distance between them during their motion. Our separation certificate changes $O(\\log(D/s))$ times when the relative motion of the two polygons is a translation along a straight line or convex curve, $O(\\sqrt{D/s})$ for translation along an algebraic trajectory, and $O(D/s)$ for algebraic rigid motion (translation and rotation). Each certificate update is performed in $O(\\log(D/s))$ time. Variants of these data structures are also shown that exhibit \\emph{hysteresis}---after a separation certificate fails, the new certificate cannot fail again until the objects have moved by some constant fraction of their current separation. We can then bound the number of events by the combinatorial size of a certain cover of the motion path by balls.",
        "published": "1998-09-18T23:16:06Z",
        "link": "http://arxiv.org/abs/cs/9809035v2",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2;I.3.5"
        ]
    },
    {
        "title": "Droems: experimental mathematics, informatics and infinite dimensional   geometry",
        "authors": [
            "Denis V. Juriev"
        ],
        "summary": "The article is devoted to a problem of elaboration of the real-time interactive videosystems for accelerated nonverbal cognitive computer and telecommunications. The proposed approach is based on the using of droems (dynamically reconstructed objects of experimental mathematics) and interpretational figures as pointers to them. Four paragraphs of the article are devoted to (1) an exposition of basic notions of the interpretational geometry, (2) the operator methods in the theory of interactive dynamical videosystems, (3) the general concept of organization of the integrated interactive real-time videocognitive systems, (4) the droems and processes of their dynamical reconstruction, where the general notions are illustrated by a concrete example related to the infinite dimensional geometry. The exposition is presumably heuristic and conceptual (the first and the third paragraphs) though some particular aspects such as content of the second and the fourth paragraphs, which allow deeper formalization and detailing in present, are exposed on the mathematical level of rigor.",
        "published": "1998-09-29T07:06:31Z",
        "link": "http://arxiv.org/abs/cs/9809119v1",
        "categories": [
            "cs.HC",
            "cs.GR",
            "math.RT",
            "H.1.2; I.3.8"
        ]
    },
    {
        "title": "The Design of EzWindows: A Graphics API for an Introductory Programming   Course",
        "authors": [
            "Bruce R. Childers",
            "James P. Cohoon",
            "Jack W. Davidson",
            "Peter Valle"
        ],
        "summary": "Teaching object-oriented programming in an introductory programming course poses considerable challenges to the instructor. An often advocated approach to meeting this challenge is the use of a simple, object-oriented graphics library. We have developed a simple, portable graphics library for teaching object-oriented programming using C++. The library, EzWindows, allows beginning programmers to design and write programs that use the graphical display found on all modern desktop computers. In addition to providing simple graphical objects such as windows, geometric shapes, and bitmaps, EzWindows provides facilities for introducing event-based programming using the mouse and timers. EzWindows has proven to be extremely popular; it is currently in use at over 200 universities, colleges, and high schools. This paper describes the rationale for EzWindows and its high-level design.",
        "published": "1998-10-03T12:04:29Z",
        "link": "http://arxiv.org/abs/cs/9810004v1",
        "categories": [
            "cs.CY",
            "cs.GR",
            "K.3.1;K.3.2;I.3.4"
        ]
    },
    {
        "title": "Computational Geometry Column 33",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "Several recent SIGGRAPH papers on surface simplification are described.",
        "published": "1998-10-22T20:44:35Z",
        "link": "http://arxiv.org/abs/cs/9810020v1",
        "categories": [
            "cs.CG",
            "cs.AI",
            "cs.GR",
            "F.2.2;I.3"
        ]
    },
    {
        "title": "Computational Geometry Column 32",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The proof of Dey's new k-set bound is illustrated.",
        "published": "1998-10-22T21:01:36Z",
        "link": "http://arxiv.org/abs/cs/9810021v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2"
        ]
    },
    {
        "title": "Locked and Unlocked Polygonal Chains in 3D",
        "authors": [
            "T. Biedl",
            "E. Demaine",
            "M. Demaine",
            "S. Lazard",
            "A. Lubiw",
            "J. O'Rourke",
            "M. Overmars",
            "S. Robbins",
            "I. Streinu",
            "G. Toussaint",
            "S. Whitesides"
        ],
        "summary": "In this paper, we study movements of simple polygonal chains in 3D. We say that an open, simple polygonal chain can be straightened if it can be continuously reconfigured to a straight sequence of segments in such a manner that both the length of each link and the simplicity of the chain are maintained throughout the movement. The analogous concept for closed chains is convexification: reconfiguration to a planar convex polygon. Chains that cannot be straightened or convexified are called locked. While there are open chains in 3D that are locked, we show that if an open chain has a simple orthogonal projection onto some plane, it can be straightened. For closed chains, we show that there are unknotted but locked closed chains, and we provide an algorithm for convexifying a planar simple polygon in 3D with a polynomial number of moves.",
        "published": "1998-11-11T20:36:50Z",
        "link": "http://arxiv.org/abs/cs/9811019v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.RO",
            "F.2.2; I.2.9"
        ]
    },
    {
        "title": "A Human - machine interface for teleoperation of arm manipulators in a   complex environment",
        "authors": [
            "I. Ivanisevic",
            "V. Lumelsky"
        ],
        "summary": "This paper discusses the feasibility of using configuration space (C-space) as a means of visualization and control in operator-guided real-time motion of a robot arm manipulator. The motivation is to improve performance of the human operator in tasks involving the manipulator motion in an environment with obstacles. Unlike some other motion planning tasks, operators are known to make expensive mistakes in such tasks, even in a simpler two-dimensional case. They have difficulty learning better procedures and their performance improves very little with practice. Using an example of a two-dimensional arm manipulator, we show that translating the problem into C-space improves the operator performance rather remarkably, on the order of magnitude compared to the usual work space control. An interface that makes the transfer possible is described, and an example of its use in a virtual environment is shown.",
        "published": "1998-11-20T21:06:07Z",
        "link": "http://arxiv.org/abs/cs/9811029v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "I.2.9"
        ]
    },
    {
        "title": "Performance / Price Sort",
        "authors": [
            "Jim Gray",
            "Joshua Coates",
            "Chris Nyberg"
        ],
        "summary": "NTsort is an external sort on WindowsNT 5.0. It has minimal functionality but excellent price performance. In particular, running on mail-order hardware it can sort 1.5 GB for a penny. For commercially available sorts, Postman Sort from Robert Ramey Software Development has elapsed time performance comparable to NTsort, while using less processor time. It can sort 1.27 GB for a penny (12.7 million records.) These sorts set new price-performance records. This paper documents this and proposes that the PennySort benchmark be revised to Performance/Price sort: a simple GB/$ sort metric based on a two-pass external sort.",
        "published": "1998-09-02T00:49:25Z",
        "link": "http://arxiv.org/abs/cs/9809004v1",
        "categories": [
            "cs.DB",
            "cs.PF",
            "D.5;H.2"
        ]
    },
    {
        "title": "The Five-Minute Rule Ten Years Later, and Other Computer Storage Rules   of Thumb",
        "authors": [
            "Jim Gray",
            "Goetz Graefe"
        ],
        "summary": "Simple economic and performance arguments suggest appropriate lifetimes for main memory pages and suggest optimal page sizes. The fundamental tradeoffs are the prices and bandwidths of RAMs and disks. The analysis indicates that with today's technology, five minutes is a good lifetime for randomly accessed pages, one minute is a good lifetime for two-pass sequentially accessed pages, and 16 KB is a good size for index pages. These rules-of-thumb change in predictable ways as technology ratios change. They also motivate the importance of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.",
        "published": "1998-09-02T01:49:32Z",
        "link": "http://arxiv.org/abs/cs/9809005v1",
        "categories": [
            "cs.DB",
            "H.3.4"
        ]
    },
    {
        "title": "Microsoft TerraServer",
        "authors": [
            "Tom Barclay",
            "Robert Eberl",
            "Jim Gray",
            "John Nordlinger",
            "Guru Raghavendran",
            "Don Slutz",
            "Greg Smith",
            "Phil Smoot",
            "John Hoffman",
            "Natt Robb III",
            "Hedy Rossmeissl",
            "Beth Duff",
            "George Lee",
            "Theresa Mathesmier",
            "Randall Sunne"
        ],
        "summary": "The Microsoft TerraServer stores aerial and satellite images of the earth in a SQL Server Database served to the public via the Internet. It is the world's largest atlas, combining five terabytes of image data from the United States Geodetic Survey, Sovinformsputnik, and Encarta Virtual Globe. Internet browsers provide intuitive spatial and gazetteer interfaces to the data. The TerraServer is also an E-Commerce application. Users can buy the right to use the imagery using Microsoft Site Servers managed by the USGS and Aerial Images. This paper describes the TerraServer's design and implementation.",
        "published": "1998-09-05T00:29:54Z",
        "link": "http://arxiv.org/abs/cs/9809011v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2.4;H.2.8;H.3.5"
        ]
    },
    {
        "title": "Similarity-Based Queries for Time Series Data",
        "authors": [
            "Davood Rafiei",
            "Alberto Mendelzon"
        ],
        "summary": "We study a set of linear transformations on the Fourier series representation of a sequence that can be used as the basis for similarity queries on time-series data. We show that our set of transformations is rich enough to formulate operations such as moving average and time warping. We present a query processing algorithm that uses the underlying R-tree index of a multidimensional data set to answer similarity queries efficiently. Our experiments show that the performance of this algorithm is competitive to that of processing ordinary (exact match) queries using the index, and much faster than sequential scanning. We relate our transformations to the general framework for similarity queries of Jagadish et al.",
        "published": "1998-09-17T14:41:07Z",
        "link": "http://arxiv.org/abs/cs/9809023v2",
        "categories": [
            "cs.DB",
            "H.2.2"
        ]
    },
    {
        "title": "Efficient Retrieval of Similar Time Sequences Using DFT",
        "authors": [
            "Davood Rafiei",
            "Alberto Mendelzon"
        ],
        "summary": "We propose an improvement of the known DFT-based indexing technique for fast retrieval of similar time sequences. We use the last few Fourier coefficients in the distance computation without storing them in the index since every coefficient at the end is the complex conjugate of a coefficient at the beginning and as strong as its counterpart. We show analytically that this observation can accelerate the search time of the index by more than a factor of two. This result was confirmed by our experiments, which were carried out on real stock prices and synthetic data.",
        "published": "1998-09-18T21:24:23Z",
        "link": "http://arxiv.org/abs/cs/9809033v2",
        "categories": [
            "cs.DB",
            "H.2;H.3"
        ]
    },
    {
        "title": "Pre-fetching tree-structured data in distributed memory",
        "authors": [
            "Lex Weaver",
            "Chris Johnson"
        ],
        "summary": "A distributed heap storage manager has been implemented on the Fujitsu AP1000 multicomputer. The performance of various pre-fetching strategies is experimentally compared. Subjective programming benefits and objective performance benefits of up to 10% in pre-fetching are found for certain applications, but not for all. The performance benefits of pre-fetching depend on the specific data structure and access patterns. We suggest that control of pre-fetching strategy be dynamically under the control of the application.",
        "published": "1998-10-02T04:41:03Z",
        "link": "http://arxiv.org/abs/cs/9810002v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.3.4"
        ]
    },
    {
        "title": "The Asilomar Report on Database Research",
        "authors": [
            "Phil Bernstein",
            "Michael Brodie",
            "Stefano Ceri",
            "David DeWitt",
            "Mike Franklin",
            "Hector Garcia-Molina",
            "Jim Gray",
            "Jerry Held",
            "Joe Hellerstein",
            "H. V. Jagadish",
            "Michael Lesk",
            "Dave Maier",
            "Jeff Naughton",
            "Hamid Pirahesh",
            "Mike Stonebraker",
            "Jeff Ullman"
        ],
        "summary": "The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda -- broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.",
        "published": "1998-11-09T05:09:36Z",
        "link": "http://arxiv.org/abs/cs/9811013v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.0;H.2;H.3;H.4;H.5"
        ]
    },
    {
        "title": "Hypertree Decompositions and Tractable Queries",
        "authors": [
            "G. Gottlob",
            "N. Leone",
            "F. Scarcello"
        ],
        "summary": "Several important decision problems on conjunctive queries (CQs) are NP-complete in general but become tractable, and actually highly parallelizable, if restricted to acyclic or nearly acyclic queries. Examples are the evaluation of Boolean CQs and query containment. These problems were shown tractable for conjunctive queries of bounded treewidth and of bounded degree of cyclicity. The so far most general concept of nearly acyclic queries was the notion of queries of bounded query-width introduced by Chekuri and Rajaraman (1997). While CQs of bounded query width are tractable, it remained unclear whether such queries are efficiently recognizable. Chekuri and Rajaraman stated as an open problem whether for each constant k it can be determined in polynomial time if a query has query width less than or equal to k. We give a negative answer by proving this problem NP-complete (specifically, for k=4). In order to circumvent this difficulty, we introduce the new concept of hypertree decomposition of a query and the corresponding notion of hypertree width. We prove: (a) for each k, the class of queries with query width bounded by k is properly contained in the class of queries whose hypertree width is bounded by k; (b) unlike query width, constant hypertree-width is efficiently recognizable; (c) Boolean queries of constant hypertree width can be efficiently evaluated.",
        "published": "1998-12-28T12:30:50Z",
        "link": "http://arxiv.org/abs/cs/9812022v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "F.2.2; H.2.4; I.2.8; G.2.2"
        ]
    },
    {
        "title": "Concatenating Decoherence Free Subspaces with Quantum Error Correcting   Codes",
        "authors": [
            "D. A. Lidar",
            "D. Bacon",
            "K. B. Whaley"
        ],
        "summary": "An operator sum representation is derived for a decoherence-free subspace (DFS) and used to (i) show that DFSs are the class of quantum error correcting codes (QECCs) with fixed, unitary recovery operators, and (ii) find explicit representations for the Kraus operators of collective decoherence. We demonstrate how this can be used to construct a concatenated DFS-QECC code which protects against collective decoherence perturbed by independent decoherence. The code yields an error threshold which depends only on the perturbing independent decoherence rate.",
        "published": "1998-09-28T02:08:00Z",
        "link": "http://arxiv.org/abs/quant-ph/9809081v2",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math-ph",
            "math.IT",
            "math.MP"
        ]
    },
    {
        "title": "On the classifiability of cellular automata",
        "authors": [
            "John T. Baldwin",
            "Saharon Shelah"
        ],
        "summary": "Based on computer simulations Wolfram presented in several papers conjectured classifications of cellular automata into 4 types. He distinguishes the 4 classes of cellular automata by the evolution of the pattern generated by applying a cellular automaton to a finite input. Wolfram's qualitative classification is based on the examination of a large number of simulations. In addition to this classification based on the rate of growth, he conjectured a similar classification according to the eventual pattern. We consider here one formalization of his rate of growth suggestion. After completing our major results (based only on Wolfram's work), we investigated other contributions to the area and we report the relation of some of them to our discoveries.",
        "published": "1998-01-15T00:00:00Z",
        "link": "http://arxiv.org/abs/math/9801152v1",
        "categories": [
            "math.LO",
            "cs.NE"
        ]
    },
    {
        "title": "Development and Evolution of Neural Networks in an Artificial Chemistry",
        "authors": [
            "Jens C. Astor",
            "Christoph Adami"
        ],
        "summary": "We present a model of decentralized growth for Artificial Neural Networks (ANNs) inspired by the development and the physiology of real nervous systems. In this model, each individual artificial neuron is an autonomous unit whose behavior is determined only by the genetic information it harbors and local concentrations of substrates modeled by a simple artificial chemistry. Gene expression is manifested as axon and dendrite growth, cell division and differentiation, substrate production and cell stimulation. We demonstrate the model's power with a hand-written genome that leads to the growth of a simple network which performs classical conditioning. To evolve more complex structures, we implemented a platform-independent, asynchronous, distributed Genetic Algorithm (GA) that allows users to participate in evolutionary experiments via the World Wide Web.",
        "published": "1998-07-17T01:07:50Z",
        "link": "http://arxiv.org/abs/adap-org/9807003v1",
        "categories": [
            "adap-org",
            "cs.NE",
            "nlin.AO",
            "q-bio.PE"
        ]
    },
    {
        "title": "Genetic Algorithm for SU(N) gauge theory on a lattice",
        "authors": [
            "Yamaguchi Azusa"
        ],
        "summary": "An Algorithm is proposed for the simulation of pure SU(N) lattice gauge theories based on Genetic Algorithms(GAs). Main difference between GAs and Metropolis methods(MPs) is that GAs treat a population of points at once, while MPs treat only one point in the searching space. This provides GAs with information about the assortment as well as the fitness of the evolution function and producting a better solution. We apply GAs to SU(2) pure gauge theory on a 2 dimensional lattice and show the results are consistent with those given by MP and Heatbath methods(HBs). Thermalization speed of GAs is especially faster than the simple MPs.",
        "published": "1998-08-02T13:26:31Z",
        "link": "http://arxiv.org/abs/hep-lat/9808001v2",
        "categories": [
            "hep-lat",
            "cs.NE"
        ]
    },
    {
        "title": "Genetic Algorithm for SU(2) Gauge Theory on a 2-dimensional Lattice",
        "authors": [
            "A. Yamaguchi"
        ],
        "summary": "An algorithm is proposed for the simulation of pure SU(N) lattice gauge theories based on Genetic Algorithms(GAs). We apply GAs to SU(2) pure gauge theory on a 2 dimensional lattice and show the results, the action per plaquette and Wilson loops, are consistent with those by Metropolis method(MP)s and Heatbath method(HB)s. Thermalization speed of GAs is especially faster than the simple MPs.",
        "published": "1998-09-11T05:44:42Z",
        "link": "http://arxiv.org/abs/hep-lat/9809068v1",
        "categories": [
            "hep-lat",
            "cs.NE"
        ]
    },
    {
        "title": "Distributed Computation as Hierarchy",
        "authors": [
            "Michael Manthey"
        ],
        "summary": "This paper presents a new distributed computational model of distributed systems called the phase web that extends V. Pratt's orthocurrence relation from 1986. The model uses mutual-exclusion to express sequence, and a new kind of hierarchy to replace event sequences, posets, and pomsets. The model explicitly connects computation to a discrete Clifford algebra that is in turn extended into homology and co-homology, wherein the recursive nature of objects and boundaries becomes apparent and itself subject to hierarchical recursion. Topsy, a programming environment embodying the phase web, is available from www.cs.auc.dk/topsy.",
        "published": "1998-09-14T15:25:42Z",
        "link": "http://arxiv.org/abs/cs/9809019v1",
        "categories": [
            "cs.DC",
            "cs.NE",
            "F.1; E.2; D.1; I.4"
        ]
    },
    {
        "title": "Aspects of Evolutionary Design by Computers",
        "authors": [
            "Peter J Bentley"
        ],
        "summary": "This paper examines the four main types of Evolutionary Design by computers: Evolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial Life Forms and Creative Evolutionary Design. Definitions for all four areas are provided. A review of current work in each of these areas is given, with examples of the types of applications that have been tackled. The different properties and requirements of each are examined. Descriptions of typical representations and evolutionary algorithms are provided and examples of designs evolved using these techniques are shown. The paper then discusses how the boundaries of these areas are beginning to merge, resulting in four new 'overlapping' types of Evolutionary Design: Integral Evolutionary Design, Artificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and Aesthetic Evolutionary Design. Finally, the last part of the paper discusses some common problems faced by creators of Evolutionary Design systems, including: interdependent elements in designs, epistasis, and constraint handling.",
        "published": "1998-09-23T11:01:55Z",
        "link": "http://arxiv.org/abs/cs/9809049v1",
        "categories": [
            "cs.NE",
            "A.1;E.2;F.4.1;I.2.0;I.2.6;I.2.8;I.2.9;I.2.11;I.3.5;I.6.0;J.6"
        ]
    },
    {
        "title": "Evolution of Neural Networks to Play the Game of Dots-and-Boxes",
        "authors": [
            "Lex Weaver",
            "Terry Bossomaier"
        ],
        "summary": "Dots-and-Boxes is a child's game which remains analytically unsolved. We implement and evolve artificial neural networks to play this game, evaluating them against simple heuristic players. Our networks do not evaluate or predict the final outcome of the game, but rather recommend moves at each stage. Superior generalisation of play by co-evolved populations is found, and a comparison made with networks trained by back-propagation using simple heuristics as an oracle.",
        "published": "1998-09-28T03:48:22Z",
        "link": "http://arxiv.org/abs/cs/9809111v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "A role of constraint in self-organization",
        "authors": [
            "Carlos Domingo",
            "Osamu Watanabe",
            "Tadashi Yamazaki"
        ],
        "summary": "In this paper we introduce a neural network model of self-organization. This model uses a variation of Hebb rule for updating its synaptic weights, and surely converges to the equilibrium status. The key point of the convergence is the update rule that constrains the total synaptic weight and this seems to make the model stable. We investigate the role of the constraint and show that it is the constraint that makes the model stable. For analyzing this setting, we propose a simple probabilistic game that models the neural network and the self-organization process. Then, we investigate the characteristics of this game, namely, the probability that the game becomes stable and the number of the steps it takes.",
        "published": "1998-09-30T04:06:36Z",
        "link": "http://arxiv.org/abs/cs/9809123v1",
        "categories": [
            "cs.NE",
            "cs.CG",
            "I.2.6;J.3"
        ]
    },
    {
        "title": "Generating Segment Durations in a Text-To-Speech System: A Hybrid   Rule-Based/Neural Network Approach",
        "authors": [
            "Gerald Corrigan",
            "Noel Massey",
            "Orhan Karaali"
        ],
        "summary": "A combination of a neural network with rule firing information from a rule-based system is used to generate segment durations for a text-to-speech system. The system shows a slight improvement in performance over a neural network system without the rule firing information. Synthesized speech using segment durations was accepted by listeners as having about the same quality as speech generated using segment durations extracted from natural speech.",
        "published": "1998-11-24T22:51:20Z",
        "link": "http://arxiv.org/abs/cs/9811030v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Speech Synthesis with Neural Networks",
        "authors": [
            "Orhan Karaali",
            "Gerald Corrigan",
            "Ira Gerson"
        ],
        "summary": "Text-to-speech conversion has traditionally been performed either by concatenating short samples of speech or by using rule-based systems to convert a phonetic representation of speech into an acoustic representation, which is then converted into speech. This paper describes a system that uses a time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping, with another neural network to control the timing of the generated speech. The neural network system requires less memory than a concatenation system, and performed well in tests comparing it to commercial systems using other technologies.",
        "published": "1998-11-24T23:33:12Z",
        "link": "http://arxiv.org/abs/cs/9811031v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Text-To-Speech Conversion with Neural Networks: A Recurrent TDNN   Approach",
        "authors": [
            "Orhan Karaali",
            "Gerald Corrigan",
            "Ira Gerson",
            "Noel Massey"
        ],
        "summary": "This paper describes the design of a neural network that performs the phonetic-to-acoustic mapping in a speech synthesis system. The use of a time-domain neural network architecture limits discontinuities that occur at phone boundaries. Recurrent data input also helps smooth the output parameter tracks. Independent testing has demonstrated that the voice quality produced by this system compares favorably with speech from existing commercial text-to-speech systems.",
        "published": "1998-11-24T23:51:56Z",
        "link": "http://arxiv.org/abs/cs/9811032v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Training Reinforcement Neurocontrollers Using the Polytope Algorithm",
        "authors": [
            "A. Likas",
            "I. E. Lagaris"
        ],
        "summary": "A new training algorithm is presented for delayed reinforcement learning problems that does not assume the existence of a critic model and employs the polytope optimization algorithm to adjust the weights of the action network so that a simple direct measure of the training performance is maximized. Experimental results from the application of the method to the pole balancing problem indicate improved training performance compared with critic-based and genetic reinforcement approaches.",
        "published": "1998-12-03T09:08:03Z",
        "link": "http://arxiv.org/abs/cs/9812002v1",
        "categories": [
            "cs.NE",
            "C.1.3"
        ]
    },
    {
        "title": "Neural Network Methods for Boundary Value Problems Defined in   Arbitrarily Shaped Domains",
        "authors": [
            "I. E. Lagaris",
            "A. Likas",
            "D. G. Papageorgiou"
        ],
        "summary": "Partial differential equations (PDEs) with Dirichlet boundary conditions defined on boundaries with simple geometry have been succesfuly treated using sigmoidal multilayer perceptrons in previous works. This article deals with the case of complex boundary geometry, where the boundary is determined by a number of points that belong to it and are closely located, so as to offer a reasonable representation. Two networks are employed: a multilayer perceptron and a radial basis function network. The later is used to account for the satisfaction of the boundary conditions. The method has been successfuly tested on two-dimensional and three-dimensional PDEs and has yielded accurate solutions.",
        "published": "1998-12-03T10:09:19Z",
        "link": "http://arxiv.org/abs/cs/9812003v1",
        "categories": [
            "cs.NE",
            "cond-mat.dis-nn",
            "cs.NA",
            "math-ph",
            "math.MP",
            "math.NA",
            "physics.comp-ph",
            "C.1.3"
        ]
    },
    {
        "title": "A High Quality Text-To-Speech System Composed of Multiple Neural   Networks",
        "authors": [
            "Orhan Karaali",
            "Gerald Corrigan",
            "Noel Massey",
            "Corey Miller",
            "Otto Schnurr",
            "Andrew Mackie"
        ],
        "summary": "While neural networks have been employed to handle several different text-to-speech tasks, ours is the first system to use neural networks throughout, for both linguistic and acoustic processing. We divide the text-to-speech task into three subtasks, a linguistic module mapping from text to a linguistic representation, an acoustic module mapping from the linguistic representation to speech, and a video module mapping from the linguistic representation to animated images. The linguistic module employs a letter-to-sound neural network and a postlexical neural network. The acoustic module employs a duration neural network and a phonetic neural network. The visual neural network is employed in parallel to the acoustic module to drive a talking head. The use of neural networks that can be retrained on the characteristics of different voices and languages affords our system a degree of adaptability and naturalness heretofore unavailable.",
        "published": "1998-12-05T00:00:57Z",
        "link": "http://arxiv.org/abs/cs/9812006v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "The Self-Organizing Symbiotic Agent",
        "authors": [
            "Babak Hodjat",
            "Makoto Amamiya"
        ],
        "summary": "In [N. A. Baas, Emergence, Hierarchies, and Hyper-structures, in C.G. Langton ed., Artificial Life III, Addison Wesley, 1994.] a general framework for the study of Emergence and hyper-structure was presented. This approach is mostly concerned with the description of such systems. In this paper we will try to bring forth a different aspect of this model we feel will be useful in the engineering of agent based solutions, namely the symbiotic approach. In this approach a self-organizing method of dividing the more complex \"main-problem\" to a hyper-structure of \"sub-problems\" with the aim of reducing complexity is desired. A description of the general problem will be given along with some instances of related work. This paper is intended to serve as an introductory challenge for general solutions to the described problem.",
        "published": "1998-12-11T20:51:38Z",
        "link": "http://arxiv.org/abs/cs/9812013v1",
        "categories": [
            "cs.NE",
            "cs.CC",
            "I.2.8; D.2.11;F.1.13; I.2.11"
        ]
    },
    {
        "title": "Linear probing and graphs",
        "authors": [
            "Donald E. Knuth"
        ],
        "summary": "Mallows and Riordan showed in 1968 that labeled trees with a small number of inversions are related to labeled graphs that are connected and sparse. Wright enumerated sparse connected graphs in 1977, and Kreweras related the inversions of trees to the so-called ``parking problem'' in 1980. A~combination of these three results leads to a surprisingly simple analysis of the behavior of hashing by linear probing, including higher moments of the cost of successful search.",
        "published": "1998-01-15T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9801103v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Fully Polynomial Randomized Approximation Scheme for the All Terminal   Network Reliability Problem",
        "authors": [
            "David R. Karger"
        ],
        "summary": "The classic all-terminal network reliability problem posits a graph, each of whose edges fails independently with some given probability.",
        "published": "1998-09-09T02:38:56Z",
        "link": "http://arxiv.org/abs/cs/9809012v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Approximation Algorithms for PSPACE-Hard Hierarchically and Periodically   Specified Problems",
        "authors": [
            "Madhav V. Marathe",
            "Harry B. Hunt III",
            "Richard E. Stearns",
            "Venkatesh Radhakrishnan"
        ],
        "summary": "We study the efficient approximability of basic graph and logic problems in the literature when instances are specified hierarchically as in \\cite{Le89} or are specified by 1-dimensional finite narrow periodic specifications as in \\cite{Wa93}. We show that, for most of the problems $\\Pi$ considered when specified using {\\bf k-level-restricted} hierarchical specifications or $k$-narrow periodic specifications the following holds:   \\item Let $\\rho$ be any performance guarantee of a polynomial time approximation algorithm for $\\Pi$, when instances are specified using standard specifications. Then $\\forall \\epsilon > 0$, $ \\Pi$ has a polynomial time approximation algorithm with performance guarantee $(1 + \\epsilon) \\rho$. \\item $\\Pi$ has a polynomial time approximation scheme when restricted to planar instances. \\end{romannum}   These are the first polynomial time approximation schemes for PSPACE-hard hierarchically or periodically specified problems. Since several of the problems considered are PSPACE-hard, our results provide the first examples of natural PSPACE-hard optimization problems that have polynomial time approximation schemes. This answers an open question in Condon et. al. \\cite{CF+93}.",
        "published": "1998-09-23T15:58:21Z",
        "link": "http://arxiv.org/abs/cs/9809064v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "Bicriteria Network Design Problems",
        "authors": [
            "Madhav V. Marathe",
            "R. Ravi",
            "Ravi Sundaram",
            "S. S. Ravi",
            "Daniel J. Rosenkrantz",
            "Harry B. Hunt III"
        ],
        "summary": "We study a general class of bicriteria network design problems. A generic problem in this class is as follows: Given an undirected graph and two minimization objectives (under different cost functions), with a budget specified on the first, find a <subgraph \\from a given subgraph-class that minimizes the second objective subject to the budget on the first. We consider three different criteria - the total edge cost, the diameter and the maximum degree of the network. Here, we present the first polynomial-time approximation algorithms for a large class of bicriteria network design problems for the above mentioned criteria. The following general types of results are presented.   First, we develop a framework for bicriteria problems and their approximations. Second, when the two criteria are the same %(note that the cost functions continue to be different) we present a ``black box'' parametric search technique. This black box takes in as input an (approximation) algorithm for the unicriterion situation and generates an approximation algorithm for the bicriteria case with only a constant factor loss in the performance guarantee. Third, when the two criteria are the diameter and the total edge costs we use a cluster-based approach to devise a approximation algorithms --- the solutions output violate both the criteria by a logarithmic factor. Finally, for the class of treewidth-bounded graphs, we provide pseudopolynomial-time algorithms for a number of bicriteria problems using dynamic programming. We show how these pseudopolynomial-time algorithms can be converted to fully polynomial-time approximation schemes using a scaling technique.",
        "published": "1998-09-24T17:48:18Z",
        "link": "http://arxiv.org/abs/cs/9809103v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F,2.2"
        ]
    },
    {
        "title": "Practical algorithms for on-line sampling",
        "authors": [
            "Carlos Domingo",
            "Ricard Gavalda",
            "Osamu Watanabe"
        ],
        "summary": "One of the core applications of machine learning to knowledge discovery consists on building a function (a hypothesis) from a given amount of data (for instance a decision tree or a neural network) such that we can use it afterwards to predict new instances of the data. In this paper, we focus on a particular situation where we assume that the hypothesis we want to use for prediction is very simple, and thus, the hypotheses class is of feasible size. We study the problem of how to determine which of the hypotheses in the class is almost the best one. We present two on-line sampling algorithms for selecting hypotheses, give theoretical bounds on the number of necessary examples, and analize them exprimentally. We compare them with the simple batch sampling approach commonly used and show that in most of the situations our algorithms use much fewer number of examples.",
        "published": "1998-09-30T03:44:08Z",
        "link": "http://arxiv.org/abs/cs/9809122v1",
        "categories": [
            "cs.LG",
            "cs.DS",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "Object-Oriented Design of Graph Oriented Data Structures",
        "authors": [
            "Maurizio Pizzonia",
            "Giuseppe Di Battista"
        ],
        "summary": "Applied research in graph algorithms and combinatorial structures needs comprehensive and versatile software libraries. However, the design and the implementation of flexible libraries are challenging activities. Among the other problems involved in such a difficult field, a very special role is played by graph classification issues.   We propose new techniques devised to help the designer and the programmer in the development activities. Such techniques are especially suited for dealing with graph classification problems and rely on an extension of the usual object-oriented paradigm. In order to support the usage of our approach, we devised an extension of the C++ programming language and implemented the corresponding pre-compiler.",
        "published": "1998-10-08T23:54:51Z",
        "link": "http://arxiv.org/abs/cs/9810009v1",
        "categories": [
            "cs.SE",
            "cs.CG",
            "cs.DS",
            "D.2.3; D.2.11; D.2.13; E.1"
        ]
    },
    {
        "title": "An exact representation of the fermion dynamics in terms of Poisson   processes and its connection with Monte Carlo algorithms",
        "authors": [
            "Matteo Beccaria",
            "Carlo Presilla",
            "Gian Fabrizio De Angelis",
            "Giovanni Jona-Lasinio"
        ],
        "summary": "We present a simple derivation of a Feynman-Kac type formula to study fermionic systems. In this approach the real time or the imaginary time dynamics is expressed in terms of the evolution of a collection of Poisson processes. A computer implementation of this formula leads to a family of algorithms parametrized by the values of the jump rates of the Poisson processes. From these an optimal algorithm can be chosen which coincides with the Green Function Monte Carlo method in the limit when the latter becomes exact.",
        "published": "1998-10-26T19:22:47Z",
        "link": "http://arxiv.org/abs/cond-mat/9810347v3",
        "categories": [
            "cond-mat",
            "cs.DS",
            "hep-lat",
            "math-ph",
            "math.MP",
            "quant-ph"
        ]
    },
    {
        "title": "Locked and Unlocked Polygonal Chains in 3D",
        "authors": [
            "T. Biedl",
            "E. Demaine",
            "M. Demaine",
            "S. Lazard",
            "A. Lubiw",
            "J. O'Rourke",
            "M. Overmars",
            "S. Robbins",
            "I. Streinu",
            "G. Toussaint",
            "S. Whitesides"
        ],
        "summary": "In this paper, we study movements of simple polygonal chains in 3D. We say that an open, simple polygonal chain can be straightened if it can be continuously reconfigured to a straight sequence of segments in such a manner that both the length of each link and the simplicity of the chain are maintained throughout the movement. The analogous concept for closed chains is convexification: reconfiguration to a planar convex polygon. Chains that cannot be straightened or convexified are called locked. While there are open chains in 3D that are locked, we show that if an open chain has a simple orthogonal projection onto some plane, it can be straightened. For closed chains, we show that there are unknotted but locked closed chains, and we provide an algorithm for convexifying a planar simple polygon in 3D with a polynomial number of moves.",
        "published": "1998-11-11T20:36:50Z",
        "link": "http://arxiv.org/abs/cs/9811019v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "cs.RO",
            "F.2.2; I.2.9"
        ]
    },
    {
        "title": "Minimum Cuts in Near-Linear Time",
        "authors": [
            "David R. Karger"
        ],
        "summary": "We significantly improve known time bounds for solving the minimum cut problem on undirected graphs. We use a ``semi-duality'' between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques. We give a randomized algorithm that finds a minimum cut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O(n^2 log n) time. This variant has an optimal RNC parallelization. Both variants improve on the previous best time bound of O(n^2 log^3 n). Other applications of the tree-packing approach are new, nearly tight bounds on the number of near minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner.",
        "published": "1998-12-08T21:29:20Z",
        "link": "http://arxiv.org/abs/cs/9812007v1",
        "categories": [
            "cs.DS",
            "F.2.2;G.2.2;G.3"
        ]
    },
    {
        "title": "Approximate Graph Coloring by Semidefinite Programming",
        "authors": [
            "David Karger",
            "Rajeev Motwani",
            "Madhu Sudan"
        ],
        "summary": "We consider the problem of coloring k-colorable graphs with the fewest possible colors. We present a randomized polynomial time algorithm that colors a 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log n), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any vertex. Besides giving the best known approximation ratio in terms of n, this marks the first non-trivial approximation result as a function of the maximum degree Delta. This result can be generalized to k-colorable graphs to obtain a coloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)} log^{1/2} n) colors. Our results are inspired by the recent work of Goemans and Williamson who used an algorithm for semidefinite optimization problems, which generalize linear programs, to obtain improved approximations for the MAX CUT and MAX 2-SAT problems. An intriguing outcome of our work is a duality relationship established between the value of the optimum solution to our semidefinite program and the Lovasz theta-function. We show lower bounds on the gap between the optimum solution of our semidefinite program and the actual chromatic number; by duality this also demonstrates interesting new facts about the theta-function.",
        "published": "1998-12-08T22:03:36Z",
        "link": "http://arxiv.org/abs/cs/9812008v1",
        "categories": [
            "cs.DS",
            "F.2.2;G.2.2;G.3"
        ]
    },
    {
        "title": "Microsoft TerraServer",
        "authors": [
            "Tom Barclay",
            "Robert Eberl",
            "Jim Gray",
            "John Nordlinger",
            "Guru Raghavendran",
            "Don Slutz",
            "Greg Smith",
            "Phil Smoot",
            "John Hoffman",
            "Natt Robb III",
            "Hedy Rossmeissl",
            "Beth Duff",
            "George Lee",
            "Theresa Mathesmier",
            "Randall Sunne"
        ],
        "summary": "The Microsoft TerraServer stores aerial and satellite images of the earth in a SQL Server Database served to the public via the Internet. It is the world's largest atlas, combining five terabytes of image data from the United States Geodetic Survey, Sovinformsputnik, and Encarta Virtual Globe. Internet browsers provide intuitive spatial and gazetteer interfaces to the data. The TerraServer is also an E-Commerce application. Users can buy the right to use the imagery using Microsoft Site Servers managed by the USGS and Aerial Images. This paper describes the TerraServer's design and implementation.",
        "published": "1998-09-05T00:29:54Z",
        "link": "http://arxiv.org/abs/cs/9809011v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2.4;H.2.8;H.3.5"
        ]
    },
    {
        "title": "Novelty and Social Search in the World Wide Web",
        "authors": [
            "Bernardo A. Huberman",
            "Lada A. Adamic"
        ],
        "summary": "The World Wide Web is fast becoming a source of information for a large part of the world's population. Because of its sheer size and complexity users often resort to recommendations from others to decide which sites to visit. We present a dynamical theory of recommendations which predicts site visits by users of the World Wide Web. We show that it leads to a universal power law for the number of users that visit given sites over periods of time, with an exponent related to the rate at which users discover new sites on their own. An extensive empirical study of user behavior in the Web that we conducted confirms the existence of this law of influence while yielding bounds on the rate of novelty encountered by users.",
        "published": "1998-09-18T00:07:03Z",
        "link": "http://arxiv.org/abs/cs/9809025v1",
        "categories": [
            "cs.MA",
            "cs.DL",
            "H.1.1"
        ]
    },
    {
        "title": "The Asilomar Report on Database Research",
        "authors": [
            "Phil Bernstein",
            "Michael Brodie",
            "Stefano Ceri",
            "David DeWitt",
            "Mike Franklin",
            "Hector Garcia-Molina",
            "Jim Gray",
            "Jerry Held",
            "Joe Hellerstein",
            "H. V. Jagadish",
            "Michael Lesk",
            "Dave Maier",
            "Jeff Naughton",
            "Hamid Pirahesh",
            "Mike Stonebraker",
            "Jeff Ullman"
        ],
        "summary": "The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda -- broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.",
        "published": "1998-11-09T05:09:36Z",
        "link": "http://arxiv.org/abs/cs/9811013v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.0;H.2;H.3;H.4;H.5"
        ]
    },
    {
        "title": "Digitizing Legacy Documents: A Knowledge-Base Preservation Project",
        "authors": [
            "Elizabeth Anderson",
            "Robert Atkinson",
            "Cynthia Crego",
            "Jean Slisz",
            "Sara Tompson"
        ],
        "summary": "This paper addresses the issue of making legacy information (that material held in paper format only) electronically searchable and retrievable. We used proprietary software and commercial hardware to create a process for scanning, cataloging, archiving and electronically disseminating full-text documents. This process is relatively easy to implement and reasonably affordable.",
        "published": "1998-11-11T22:12:18Z",
        "link": "http://arxiv.org/abs/cs/9811020v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Vocal Access to a Newspaper Archive: Design Issues and Preliminary   Investigation",
        "authors": [
            "Fabio Crestani"
        ],
        "summary": "This paper presents the design and the current prototype implementation of an interactive vocal Information Retrieval system that can be used to access articles of a large newspaper archive using a telephone. The results of preliminary investigation into the feasibility of such a system are also presented.",
        "published": "1998-12-10T13:49:57Z",
        "link": "http://arxiv.org/abs/cs/9812009v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Making the most of electronic journals",
        "authors": [
            "Steve Hitchcock",
            "Les Carr",
            "Wendy Hall"
        ],
        "summary": "As most electronic journals available today have been derived from print originals, print journals have become a vital element in the broad development of electronic journals publishing. Further dependence on the print publishing model, however, will be a constraint on the continuing development of e-journals, and a series of conflicts are likely to arise. Making the most of e-journals requires that a distinctive new publishing model is developed. We consider some of the issues that will be fundamental in this new model, starting with user motivations and some reported publisher experiences, both of which suggest a broadening desire for comprehensive linked archives. This leads in turn to questions about the impact of rights assignment by authors, in particular the common practice of giving exlusive rights to publishers for individual works. Some non-prescriptive solutions are suggested, and four steps towards optimum e-journals are proposed.",
        "published": "1998-12-14T18:24:14Z",
        "link": "http://arxiv.org/abs/cs/9812016v1",
        "categories": [
            "cs.DL",
            "I.7.4"
        ]
    },
    {
        "title": "The Computing Research Repository: Promoting the Rapid Dissemination and   Archiving of Computer Science Research",
        "authors": [
            "Joseph Y. Halpern",
            "Carl Lagoze"
        ],
        "summary": "We describe the Computing Research Repository (CoRR), a new electronic archive for rapid dissemination and archiving of computer science research results. CoRR was initiated in September 1998 through the cooperation of ACM, LANL (Los Alamos National Laboratory) e-Print archive, and NCSTRL (Networked Computer Science Technical Research Library. Through its implementation of the Dienst protocol, CoRR combines the open and extensible architecture of NCSTRL with the reliable access and well-established management practices of the LANL XXX e-Print repository. This architecture will allow integration with other e-Print archives and provides a foundation for a future broad-based scholarly digital library. We describe the decisions that were made in creating CoRR, the architecture of the CoRR/NCSTRL interoperation, and issues that have arisen during the operation of CoRR.",
        "published": "1998-12-22T11:49:29Z",
        "link": "http://arxiv.org/abs/cs/9812020v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Spoken Language Dialogue Systems and Components: Best practice in   development and evaluation (DISC 24823) - Periodic Progress Report 1: Basic   Details of the Action",
        "authors": [
            "Niels Ole Bernsen",
            "Laila Dybkjaer",
            "eds."
        ],
        "summary": "The DISC project aims to (a) build an in-depth understanding of the state-of-the-art in spoken language dialogue systems (SLDSs) and components development and evaluation with the purpose of (b) developing a first best practice methodology in the field. The methodology will be accompanied by (c) a series of development and evaluation support tools. To the limited extent possible within the duration of the project, the draft versions of the methodology and the tools will be (d) tested by SLDS developers from industry and research, and will be (e) packaged to best suit their needs. In the first year of DISC, (a) has been accomplished, and (b) and (c) have started. A proposal to complete the work proposed above by adding 12 months to the 18 months of the present project, has been submitted to Esprit Long-Term Research in March 1998.",
        "published": "1998-09-23T14:47:49Z",
        "link": "http://arxiv.org/abs/cs/9809051v1",
        "categories": [
            "cs.CL",
            "cs.SE",
            "I.2.7; H.5.2; D.2.2; I.3.6"
        ]
    },
    {
        "title": "Object-Oriented Design of Graph Oriented Data Structures",
        "authors": [
            "Maurizio Pizzonia",
            "Giuseppe Di Battista"
        ],
        "summary": "Applied research in graph algorithms and combinatorial structures needs comprehensive and versatile software libraries. However, the design and the implementation of flexible libraries are challenging activities. Among the other problems involved in such a difficult field, a very special role is played by graph classification issues.   We propose new techniques devised to help the designer and the programmer in the development activities. Such techniques are especially suited for dealing with graph classification problems and rely on an extension of the usual object-oriented paradigm. In order to support the usage of our approach, we devised an extension of the C++ programming language and implemented the corresponding pre-compiler.",
        "published": "1998-10-08T23:54:51Z",
        "link": "http://arxiv.org/abs/cs/9810009v1",
        "categories": [
            "cs.SE",
            "cs.CG",
            "cs.DS",
            "D.2.3; D.2.11; D.2.13; E.1"
        ]
    },
    {
        "title": "Early Experience with ASDL in lcc",
        "authors": [
            "David R. Hanson"
        ],
        "summary": "The Abstract Syntax Description Language (ASDL) is a language for specifying the tree data structures often found in compiler intermediate representations. The ASDL generator reads an ASDL specification and generates code to construct, read, and write instances of the trees specified. Using ASDL permits a compiler to be decomposed into semi-independent components that communicate by reading and writing trees. Each component can be written in a different language, because the ASDL generator can emit code in several languages, and the files written by ASDL-generated code are machine- and language-independent. ASDL is part of the National Compiler Infrastructure project, which seeks to reduce dramatically the overhead of computer systems research by making it much easier to build high-quality compilers. This paper describes dividing lcc, a widely used retargetable C compiler, into two components that communicate via trees defined in ASDL. As the first use of ASDL in a `real' compiler, this experience reveals much about the effort required to retrofit an existing compiler to use ASDL, the overheads involved, and the strengths and weaknesses of ASDL itself and, secondarily, of lcc.",
        "published": "1998-10-13T20:38:55Z",
        "link": "http://arxiv.org/abs/cs/9810013v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.4"
        ]
    },
    {
        "title": "Broy-Lamport Specification Problem: A Gurevich Abstract State Machine   Solution",
        "authors": [
            "James K. Huggins"
        ],
        "summary": "We apply the Gurevich Abstract State Machine methodology to a benchmark specification problem of Broy and Lamport.",
        "published": "1998-10-26T16:52:34Z",
        "link": "http://arxiv.org/abs/cs/9810022v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "Equivalence is in the Eye of the Beholder",
        "authors": [
            "Yuri Gurevich",
            "James K. Huggins"
        ],
        "summary": "In a recent provocative paper, Lamport points out \"the insubstantiality of processes\" by proving the equivalence of two different decompositions of the same intuitive algorithm by means of temporal formulas. We point out that the correct equivalence of algorithms is itself in the eye of the beholder. We discuss a number of related issues and, in particular, whether algorithms can be proved equivalent directly.",
        "published": "1998-10-26T18:46:06Z",
        "link": "http://arxiv.org/abs/cs/9810023v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "Evolving Algebras and Partial Evaluation",
        "authors": [
            "Yuri Gurevich",
            "James K. Huggins"
        ],
        "summary": "We describe an automated partial evaluator for evolving algebras implemented at the University of Michigan.",
        "published": "1998-10-26T19:13:12Z",
        "link": "http://arxiv.org/abs/cs/9810024v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "An Offline Partial Evaluator for Evolving Algebras",
        "authors": [
            "James K. Huggins"
        ],
        "summary": "We describe the architecture of an evolving algebra partial evaluator, a program which specializes an evolving algebra with respect to a portion of its input. We discuss the particular analysis, specialization, and optimization techniques used and show an example of its use.",
        "published": "1998-10-26T19:19:04Z",
        "link": "http://arxiv.org/abs/cs/9810025v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "The Railroad Crossing Problem: An Experiment with Instantaneous Actions   and Immediate Reactions",
        "authors": [
            "Yuri Gurevich",
            "James K. Huggins"
        ],
        "summary": "We give an evolving algebra solution for the well-known railroad crossing problem and use the occasion to experiment with agents that perform instantaneous actions in continuous time and in particular with agents that fire at the moment they are enabled.",
        "published": "1998-10-26T19:24:37Z",
        "link": "http://arxiv.org/abs/cs/9810026v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "Second Product Line Practice Workshop Report",
        "authors": [
            "L. Bass",
            "G. Chastek",
            "P. Clements",
            "L. Northrop",
            "D. Smith",
            "J. Withey"
        ],
        "summary": "The second Software Engineering Institute Product Line Practice Workshop was a hands-on meeting held in November 1997 to share industry practices in software product lines and to explore the technical and non-technical issues involved. This report synthesizes the workshop presentations and discussions, which identified factors involved in product line practices and analyzed issues in the areas of software engineering, technical management, and enterprise management.",
        "published": "1998-11-02T19:34:17Z",
        "link": "http://arxiv.org/abs/cs/9811007v1",
        "categories": [
            "cs.SE",
            "A.0"
        ]
    },
    {
        "title": "Case Study in Survivable Network System Analysis",
        "authors": [
            "Robert Ellison",
            "Rick Linger",
            "Thomas Longstaff",
            "Nancy Mead"
        ],
        "summary": "This paper presents a method for analyzing the survivability of distributed network systems and an example of its application.",
        "published": "1998-11-04T14:32:22Z",
        "link": "http://arxiv.org/abs/cs/9811011v1",
        "categories": [
            "cs.SE",
            "C.2.3"
        ]
    },
    {
        "title": "Abstract State Machines 1988-1998: Commented ASM Bibliography",
        "authors": [
            "Egon Boerger",
            "James K. Huggins"
        ],
        "summary": "An annotated bibliography of papers which deal with or use Abstract State Machines (ASMs), as of January 1998.",
        "published": "1998-11-09T19:32:38Z",
        "link": "http://arxiv.org/abs/cs/9811014v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "Developing numerical libraries in Java",
        "authors": [
            "Ronald F. Boisvert",
            "Jack J. Dongarra",
            "Roldan Pozo",
            "Karin Remington",
            "G. W. Stewart"
        ],
        "summary": "The rapid and widespread adoption of Java has created a demand for reliable and reusable mathematical software components to support the growing number of compute-intensive applications now under development, particularly in science and engineering. In this paper we address practical issues of the Java language and environment which have an effect on numerical library design and development. Benchmarks which illustrate the current levels of performance of key numerical kernels on a variety of Java platforms are presented. Finally, a strategy for the development of a fundamental numerical toolkit for Java is proposed and its current status is described.",
        "published": "1998-09-02T19:27:07Z",
        "link": "http://arxiv.org/abs/cs/9809009v1",
        "categories": [
            "cs.MS",
            "G.4"
        ]
    },
    {
        "title": "Hyper-Systolic Matrix Multiplication",
        "authors": [
            "Thomas Lippert",
            "Nikolay Petkov",
            "Paolo Palazzari",
            "Klaus Schilling"
        ],
        "summary": "A novel parallel algorithm for matrix multiplication is presented. The hyper-systolic algorithm makes use of a one-dimensional processor abstraction. The procedure can be implemented on all types of parallel systems. It can handle matrix-vector multiplications as well as transposed matrix products.",
        "published": "1998-09-24T20:56:11Z",
        "link": "http://arxiv.org/abs/cs/9809105v1",
        "categories": [
            "cs.MS",
            "D.1.3; G.4"
        ]
    },
    {
        "title": "Uncomputably Large Integral Points on Algebraic Plane Curves?",
        "authors": [
            "J. Maurice Rojas"
        ],
        "summary": "We show that the decidability of an amplification of Hilbert's Tenth Problem in three variables implies the existence of uncomputably large integral points on certain algebraic curves. We obtain this as a corollary of a new positive complexity result: the Diophantine prefixes EAE and EEAE are generically decidable. This means, taking the former prefix as an example, that we give a precise geometric classification of those polynomials f in Z[v,x,y] for which the question...   ``Does there exists a v in N such that for all x in N, there exists a y in N with f(v,x,y)=0?''   ...may be undecidable, and we show that this set of polynomials is quite small in a rigourous sense. (The decidability of EAE was previously an open question.) The analogous result for the prefix EEAE is even stronger. We thus obtain a connection between the decidability of certain Diophantine problems, height bounds for points on curves, and the geometry of certain complex surfaces and 3-folds.",
        "published": "1998-09-02T08:54:06Z",
        "link": "http://arxiv.org/abs/math/9809009v1",
        "categories": [
            "math.NT",
            "cs.CC",
            "cs.SC",
            "math.AG",
            "math.LO",
            "03D35, 11D72, 14G99; 11G30, 14H99, 14J26"
        ]
    },
    {
        "title": "Factorization of linear partial differential operators and Darboux   integrability of nonlinear PDEs",
        "authors": [
            "Serguei P. Tsarev"
        ],
        "summary": "Using a new definition of generalized divisors we prove that the lattice of such divisors for a given linear partial differential operator is modular and obtain analogues of the well-known theorems of the Loewy-Ore theory of factorization of linear ordinary differential operators. Possible applications to factorized Groebner bases computations in the commutative and non-commutative cases are discussed, an application to finding criterions of Darboux integrability of nonlinear PDEs is given.",
        "published": "1998-10-31T09:09:05Z",
        "link": "http://arxiv.org/abs/cs/9811002v1",
        "categories": [
            "cs.SC",
            "nlin.SI",
            "solv-int",
            "I.1"
        ]
    },
    {
        "title": "Formulas as Programs",
        "authors": [
            "Krzysztof R. Apt",
            "Marc Bezem"
        ],
        "summary": "We provide here a computational interpretation of first-order logic based on a constructive interpretation of satisfiability w.r.t. a fixed but arbitrary interpretation. In this approach the formulas themselves are programs. This contrasts with the so-called formulas as types approach in which the proofs of the formulas are typed terms that can be taken as programs. This view of computing is inspired by logic programming and constraint logic programming but differs from them in a number of crucial aspects.   Formulas as programs is argued to yield a realistic approach to programming that has been realized in the implemented programming language ALMA-0 (Apt et al.) that combines the advantages of imperative and logic programming. The work here reported can also be used to reason about the correctness of non-recursive ALMA-0 programs that do not include destructive assignment.",
        "published": "1998-11-11T14:05:13Z",
        "link": "http://arxiv.org/abs/cs/9811017v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.3.1;F.4.1"
        ]
    },
    {
        "title": "Common knowledge revisited",
        "authors": [
            "R. Fagin",
            "J. Y. Halpern",
            "Y. Moses",
            "M. Vardi"
        ],
        "summary": "We consider the common-knowledge paradox raised by Halpern and Moses: common knowledge is necessary for agreement and coordination, but common knowledge is unattainable in the real world because of temporal imprecision. We discuss two solutions to this paradox: (1) modeling the world with a coarser granularity, and (2) relaxing the requirements for coordination.",
        "published": "1998-09-01T21:56:39Z",
        "link": "http://arxiv.org/abs/cs/9809003v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.4.1, C.2.4"
        ]
    },
    {
        "title": "The Design and Architecture of the Microsoft Cluster Service -- A   Practical Approach to High-Availability and Scalability",
        "authors": [
            "Werner Vogels",
            "Dan Dumitriu",
            "Ken Birman",
            "Rod Gamache",
            "Mike Massa",
            "Rob Short",
            "John Vert",
            "Joe Barrera"
        ],
        "summary": "Microsoft Cluster Service (MSCS) extends the Win-dows NT operating system to support high-availability services. The goal is to offer an execution environment where off-the-shelf server applications can continue to operate, even in the presence of node failures. Later ver-sions of MSCS will provide scalability via a node and application management system that allows applications to scale to hundreds of nodes. This paper provides a de-tailed description of the MSCS architecture and the de-sign decisions that have driven the implementation of the service. The paper also describes how some major appli-cations use the MSCS features, and describes features added to make it easier to implement and manage fault-tolerant applications on MSCS.",
        "published": "1998-09-02T17:11:54Z",
        "link": "http://arxiv.org/abs/cs/9809006v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "C.4; C.5;D.4.5"
        ]
    },
    {
        "title": "Locally Served Network Computers",
        "authors": [
            "Jim Gray"
        ],
        "summary": "NCs are the natural evolution of PCs, ubiquitous computers everywhere. The current vision of NCs requires two improbable developments: (1) inexpensive high-bandwidth WAN links to the Internet, and (2) inexpensive centralized servers. The large NC bandwidth requirements will force each home or office to have a local server LAN attached to the NCs. These servers will be much less expensive to purchase and manage than a centralized solution. Centralized staff are expensive and unresponsive.",
        "published": "1998-09-02T17:30:05Z",
        "link": "http://arxiv.org/abs/cs/9809007v1",
        "categories": [
            "cs.AR",
            "cs.DC",
            "D.4.7"
        ]
    },
    {
        "title": "Distributed Computation, the Twisted Isomorphism, and Auto-Poiesis",
        "authors": [
            "Michael Manthey"
        ],
        "summary": "This paper presents a synchronization-based, multi-process computational model of anticipatory systems called the Phase Web. It describes a self-organizing paradigm that explicitly recognizes and exploits the existence of a boundary between inside and outside, accepts and exploits intentionality, and uses explicit self-reference to describe eg. auto-poiesis. The model explicitly connects computation to a discrete Clifford algebraic formalization that is in turn extended into homology and co-homology, wherein the recursive nature of objects and boundaries becomes apparent and itself subject to hierarchical recursion. Topsy, a computer program embodying the Phase Web, is available at www.cs.auc.dk/topsy.",
        "published": "1998-09-14T14:42:26Z",
        "link": "http://arxiv.org/abs/cs/9809125v1",
        "categories": [
            "cs.DC",
            "F.1"
        ]
    },
    {
        "title": "Distributed Computation as Hierarchy",
        "authors": [
            "Michael Manthey"
        ],
        "summary": "This paper presents a new distributed computational model of distributed systems called the phase web that extends V. Pratt's orthocurrence relation from 1986. The model uses mutual-exclusion to express sequence, and a new kind of hierarchy to replace event sequences, posets, and pomsets. The model explicitly connects computation to a discrete Clifford algebra that is in turn extended into homology and co-homology, wherein the recursive nature of objects and boundaries becomes apparent and itself subject to hierarchical recursion. Topsy, a programming environment embodying the phase web, is available from www.cs.auc.dk/topsy.",
        "published": "1998-09-14T15:25:42Z",
        "link": "http://arxiv.org/abs/cs/9809019v1",
        "categories": [
            "cs.DC",
            "cs.NE",
            "F.1; E.2; D.1; I.4"
        ]
    },
    {
        "title": "Pre-fetching tree-structured data in distributed memory",
        "authors": [
            "Lex Weaver",
            "Chris Johnson"
        ],
        "summary": "A distributed heap storage manager has been implemented on the Fujitsu AP1000 multicomputer. The performance of various pre-fetching strategies is experimentally compared. Subjective programming benefits and objective performance benefits of up to 10% in pre-fetching are found for certain applications, but not for all. The performance benefits of pre-fetching depend on the specific data structure and access patterns. We suggest that control of pre-fetching strategy be dynamically under the control of the application.",
        "published": "1998-10-02T04:41:03Z",
        "link": "http://arxiv.org/abs/cs/9810002v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.3.4"
        ]
    },
    {
        "title": "Gryphon: An Information Flow Based Approach to Message Brokering",
        "authors": [
            "Robert Strom",
            "Guruduth Banavar",
            "Tushar Chandra",
            "Marc Kaplan",
            "Kevan Miller",
            "Bodhi Mukherjee",
            "Daniel Sturman",
            "Michael Ward"
        ],
        "summary": "Gryphon is a distributed computing paradigm for message brokering, which is the transferring of information in the form of streams of events from information providers to information consumers. This extended abstract outlines the major problems in message brokering and Gryphon's approach to solving them.",
        "published": "1998-10-21T18:43:47Z",
        "link": "http://arxiv.org/abs/cs/9810019v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "A nested transaction mechanism for LOCUS",
        "authors": [
            "Erik T. Mueller",
            "Johanna D. Moore",
            "Gerald J. Popek"
        ],
        "summary": "A working implementation of nested transactions has been produced for LOCUS, an integrated distributed operating system which provides a high degree of network transparency. Several aspects of our mechanism are novel. First, the mechanism allows a transaction to access objects directly without regard to the location of the object. Second, processes running on behalf of a single transaction may be located at many sites. Thus there is no need to invoke a new transaction to perform processing or access objects at a remote site. Third, unlike other environments, LOCUS allows replication of data objects at more than one site in the network, and this capability is incorporated into the transaction mechanism. If the copy of an object that is currently being accessed becomes unavailable, it is possible to continue work by using another one of the replicated copies. Finally, an efficient orphan removal algorithm is presented, and the problem of providing continued operation during network partitions is addressed in detail.",
        "published": "1998-12-10T16:49:04Z",
        "link": "http://arxiv.org/abs/cs/9812011v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "H.2.4"
        ]
    },
    {
        "title": "An Adaptive Agent Oriented Software Architecture",
        "authors": [
            "Babak Hodjat",
            "Christopher J. Savoie",
            "Makoto Amamiya"
        ],
        "summary": "A new approach to software design based on an agent-oriented architecture is presented. Unlike current research, we consider software to be designed and implemented with this methodology in mind. In this approach agents are considered adaptively communicating concurrent modules which are divided into a white box module responsible for the communications and learning, and a black box which is the independent specialized processes of the agent. A distributed Learning policy is also introduced for adaptability.",
        "published": "1998-12-11T22:36:37Z",
        "link": "http://arxiv.org/abs/cs/9812014v1",
        "categories": [
            "cs.DC",
            "cs.MA",
            "D.2.11; D.2.1"
        ]
    },
    {
        "title": "Adaptive Interaction Using the Adaptive Agent Oriented Software   Architecture (AAOSA)",
        "authors": [
            "Babak Hodjat",
            "Makoto Amamiya"
        ],
        "summary": "User interfaces that adapt their characteristics to those of the user are referred to as adaptive interfaces. We propose Adaptive Agent Oriented Software Architecture (AAOSA) as a new way of designing adaptive interfaces. AAOSA is a new approach to software design based on an agent-oriented architecture. In this approach agents are considered adaptively communicating concurrent modules which are divided into a white box module responsible for the communications and learning, and a black box which is responsible for the independent specialized processes of the agent. A distributed learning policy that makes use of this architecture is used for purposes of system adaptability.",
        "published": "1998-12-11T23:05:51Z",
        "link": "http://arxiv.org/abs/cs/9812015v1",
        "categories": [
            "cs.HC",
            "cs.DC",
            "D.2.2; I.2.7; J.7; I.2.11"
        ]
    },
    {
        "title": "Parallelization of a Dynamic Monte Carlo Algorithm: a Partially   Rejection-Free Conservative Approach",
        "authors": [
            "G. Korniss",
            "M. A. Novotny",
            "P. A. Rikvold"
        ],
        "summary": "We experiment with a massively parallel implementation of an algorithm for simulating the dynamics of metastable decay in kinetic Ising models. The parallel scheme is directly applicable to a wide range of stochastic cellular automata where the discrete events (updates) are Poisson arrivals. For high performance, we utilize a continuous-time, asynchronous parallel version of the n-fold way rejection-free algorithm. Each processing element carries an lxl block of spins, and we employ the fast SHMEM-library routines on the Cray T3E distributed-memory parallel architecture. Different processing elements have different local simulated times. To ensure causality, the algorithm handles the asynchrony in a conservative fashion. Despite relatively low utilization and an intricate relationship between the average time increment and the size of the spin blocks, we find that for sufficiently large l the algorithm outperforms its corresponding parallel Metropolis (non-rejection-free) counterpart. As an example application, we present results for metastable decay in a model ferromagnetic or ferroelectric film, observed with a probe of area smaller than the total system.",
        "published": "1998-12-21T23:57:48Z",
        "link": "http://arxiv.org/abs/cond-mat/9812344v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.mtrl-sci",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Droems: experimental mathematics, informatics and infinite dimensional   geometry",
        "authors": [
            "Denis V. Juriev"
        ],
        "summary": "The article is devoted to a problem of elaboration of the real-time interactive videosystems for accelerated nonverbal cognitive computer and telecommunications. The proposed approach is based on the using of droems (dynamically reconstructed objects of experimental mathematics) and interpretational figures as pointers to them. Four paragraphs of the article are devoted to (1) an exposition of basic notions of the interpretational geometry, (2) the operator methods in the theory of interactive dynamical videosystems, (3) the general concept of organization of the integrated interactive real-time videocognitive systems, (4) the droems and processes of their dynamical reconstruction, where the general notions are illustrated by a concrete example related to the infinite dimensional geometry. The exposition is presumably heuristic and conceptual (the first and the third paragraphs) though some particular aspects such as content of the second and the fourth paragraphs, which allow deeper formalization and detailing in present, are exposed on the mathematical level of rigor.",
        "published": "1998-09-29T07:06:31Z",
        "link": "http://arxiv.org/abs/cs/9809119v1",
        "categories": [
            "cs.HC",
            "cs.GR",
            "math.RT",
            "H.1.2; I.3.8"
        ]
    },
    {
        "title": "Designing an interface to optimize reading with small display windows",
        "authors": [
            "Tarjin Rahman",
            "Paul Muter"
        ],
        "summary": "The electronic presentation of text in small display windows is mushrooming. In the present paper, four ways of presenting text in a small display window were examined and compared to a Normal Page condition: rapid serial visual presentation (RSVP), RSVP with a Completion Meter, Sentence-by-Sentence presentation, and Sentence-by-Sentence presentation with a Completion Meter. Dependent measures were reading efficiency - speed and comprehension - and preference. For designers of hardware or software with small display windows, the results suggest the following: (1) Though RSVP is disliked by readers, the present methods of allowing self-pacing and regressions in RSVP, unlike earlier tested methods, are efficient and feasible. (2) Slower reading in RSVP should be achieved by increasing pauses between sentences or by repeating sentences, not by decreasing the within-sentence rate. (3) Completion meters do not interfere with performance, and are usually preferred. (4) The space-saving Sentence-by-Sentence format is as efficient and as preferred as the Normal Page format.",
        "published": "1998-11-16T19:42:30Z",
        "link": "http://arxiv.org/abs/cs/9811026v2",
        "categories": [
            "cs.HC",
            "H.1.2"
        ]
    },
    {
        "title": "Generating Segment Durations in a Text-To-Speech System: A Hybrid   Rule-Based/Neural Network Approach",
        "authors": [
            "Gerald Corrigan",
            "Noel Massey",
            "Orhan Karaali"
        ],
        "summary": "A combination of a neural network with rule firing information from a rule-based system is used to generate segment durations for a text-to-speech system. The system shows a slight improvement in performance over a neural network system without the rule firing information. Synthesized speech using segment durations was accepted by listeners as having about the same quality as speech generated using segment durations extracted from natural speech.",
        "published": "1998-11-24T22:51:20Z",
        "link": "http://arxiv.org/abs/cs/9811030v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Speech Synthesis with Neural Networks",
        "authors": [
            "Orhan Karaali",
            "Gerald Corrigan",
            "Ira Gerson"
        ],
        "summary": "Text-to-speech conversion has traditionally been performed either by concatenating short samples of speech or by using rule-based systems to convert a phonetic representation of speech into an acoustic representation, which is then converted into speech. This paper describes a system that uses a time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping, with another neural network to control the timing of the generated speech. The neural network system requires less memory than a concatenation system, and performed well in tests comparing it to commercial systems using other technologies.",
        "published": "1998-11-24T23:33:12Z",
        "link": "http://arxiv.org/abs/cs/9811031v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Text-To-Speech Conversion with Neural Networks: A Recurrent TDNN   Approach",
        "authors": [
            "Orhan Karaali",
            "Gerald Corrigan",
            "Ira Gerson",
            "Noel Massey"
        ],
        "summary": "This paper describes the design of a neural network that performs the phonetic-to-acoustic mapping in a speech synthesis system. The use of a time-domain neural network architecture limits discontinuities that occur at phone boundaries. Recurrent data input also helps smooth the output parameter tracks. Independent testing has demonstrated that the voice quality produced by this system compares favorably with speech from existing commercial text-to-speech systems.",
        "published": "1998-11-24T23:51:56Z",
        "link": "http://arxiv.org/abs/cs/9811032v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "A High Quality Text-To-Speech System Composed of Multiple Neural   Networks",
        "authors": [
            "Orhan Karaali",
            "Gerald Corrigan",
            "Noel Massey",
            "Corey Miller",
            "Otto Schnurr",
            "Andrew Mackie"
        ],
        "summary": "While neural networks have been employed to handle several different text-to-speech tasks, ours is the first system to use neural networks throughout, for both linguistic and acoustic processing. We divide the text-to-speech task into three subtasks, a linguistic module mapping from text to a linguistic representation, an acoustic module mapping from the linguistic representation to speech, and a video module mapping from the linguistic representation to animated images. The linguistic module employs a letter-to-sound neural network and a postlexical neural network. The acoustic module employs a duration neural network and a phonetic neural network. The visual neural network is employed in parallel to the acoustic module to drive a talking head. The use of neural networks that can be retrained on the characteristics of different voices and languages affords our system a degree of adaptability and naturalness heretofore unavailable.",
        "published": "1998-12-05T00:00:57Z",
        "link": "http://arxiv.org/abs/cs/9812006v1",
        "categories": [
            "cs.NE",
            "cs.HC",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Adaptive Interaction Using the Adaptive Agent Oriented Software   Architecture (AAOSA)",
        "authors": [
            "Babak Hodjat",
            "Makoto Amamiya"
        ],
        "summary": "User interfaces that adapt their characteristics to those of the user are referred to as adaptive interfaces. We propose Adaptive Agent Oriented Software Architecture (AAOSA) as a new way of designing adaptive interfaces. AAOSA is a new approach to software design based on an agent-oriented architecture. In this approach agents are considered adaptively communicating concurrent modules which are divided into a white box module responsible for the communications and learning, and a black box which is responsible for the independent specialized processes of the agent. A distributed learning policy that makes use of this architecture is used for purposes of system adaptability.",
        "published": "1998-12-11T23:05:51Z",
        "link": "http://arxiv.org/abs/cs/9812015v1",
        "categories": [
            "cs.HC",
            "cs.DC",
            "D.2.2; I.2.7; J.7; I.2.11"
        ]
    },
    {
        "title": "Virtual Kathakali : Gesture Driven Metamorphosis",
        "authors": [
            "Soumyadeep Paul",
            "Sudipta N. Sinha",
            "Amitabha Mukerjee"
        ],
        "summary": "Training in motor skills such as athletics, dance, or gymnastics is not possible today except in the direct presence of the coach/instructor. This paper describes a computer vision based gesture recognition system which is used to metamorphose the user into a Virtual person, e.g. as a Kathakali dancer, which is graphically recreated at a near or diatant location. Thus this can be seen by an off-site coach using low-bandwidth joint-motion data which permits real time animation. The metamorphosis involves altering the appearance and identity of the user and also creating a specific environment possibly in interaction with other virtual creatures.   A robust vision module is used to identify the user, based on very simple binary image processing in real time which also manages to resolve self-occlusion, correct for clothing/colour and other variations among users. Gestures are identified by locating key points at the shoulder, elbow and wrist joint, which are then recreated in an articulated humanoid model, which in this instance, representes a Kathakali dancer in elaborate traditional dress. Unlike glove based or other and movement tracking systems, this application requires the user to wear no hardwire devices and is aimed at making gesture tracking simpler, cheaper, and more user friendly.",
        "published": "1998-12-29T16:50:27Z",
        "link": "http://arxiv.org/abs/cs/9812023v1",
        "categories": [
            "cs.HC",
            "K.3.1; I.3.7; H.5.2; H.5.1"
        ]
    }
]