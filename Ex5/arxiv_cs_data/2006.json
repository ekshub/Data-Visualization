[
    {
        "title": "Truecluster: robust scalable clustering with model selection",
        "authors": [
            "Jens Oehlschlägel"
        ],
        "summary": "Data-based classification is fundamental to most branches of science. While recent years have brought enormous progress in various areas of statistical computing and clustering, some general challenges in clustering remain: model selection, robustness, and scalability to large datasets. We consider the important problem of deciding on the optimal number of clusters, given an arbitrary definition of space and clusteriness. We show how to construct a cluster information criterion that allows objective model selection. Differing from other approaches, our truecluster method does not require specific assumptions about underlying distributions, dissimilarity definitions or cluster models. Truecluster puts arbitrary clustering algorithms into a generic unified (sampling-based) statistical framework. It is scalable to big datasets and provides robust cluster assignments and case-wise diagnostics. Truecluster will make clustering more objective, allows for automation, and will save time and costs. Free R software is available.",
        "published": "2006-01-02T13:17:09Z",
        "link": "http://arxiv.org/abs/cs/0601001v2",
        "categories": [
            "cs.AI",
            "G.3; I.5.3"
        ]
    },
    {
        "title": "Integration of navigation and action selection functionalities in a   computational model of cortico-basal ganglia-thalamo-cortical loops",
        "authors": [
            "Benoît Girard",
            "David Filliat",
            "Jean-Arcady Meyer",
            "Alain Berthoz",
            "Agnès Guillot"
        ],
        "summary": "This article describes a biomimetic control architecture affording an animat both action selection and navigation functionalities. It satisfies the survival constraint of an artificial metabolism and supports several complementary navigation strategies. It builds upon an action selection model based on the basal ganglia of the vertebrate brain, using two interconnected cortico-basal ganglia-thalamo-cortical loops: a ventral one concerned with appetitive actions and a dorsal one dedicated to consummatory actions. The performances of the resulting model are evaluated in simulation. The experiments assess the prolonged survival permitted by the use of high level navigation strategies and the complementarity of navigation strategies in dynamic environments. The correctness of the behavioral choices in situations of antagonistic or synergetic internal states are also tested. Finally, the modelling choices are discussed with regard to their biomimetic plausibility, while the experimental results are estimated in terms of animat adaptivity.",
        "published": "2006-01-03T10:39:24Z",
        "link": "http://arxiv.org/abs/cs/0601004v1",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Divide-and-Evolve: a New Memetic Scheme for Domain-Independent Temporal   Planning",
        "authors": [
            "Marc Schoenauer",
            "Pierre Savéant",
            "Vincent Vidal"
        ],
        "summary": "An original approach, termed Divide-and-Evolve is proposed to hybridize Evolutionary Algorithms (EAs) with Operational Research (OR) methods in the domain of Temporal Planning Problems (TPPs). Whereas standard Memetic Algorithms use local search methods to improve the evolutionary solutions, and thus fail when the local method stops working on the complete problem, the Divide-and-Evolve approach splits the problem at hand into several, hopefully easier, sub-problems, and can thus solve globally problems that are intractable when directly fed into deterministic OR algorithms. But the most prominent advantage of the Divide-and-Evolve approach is that it immediately opens up an avenue for multi-objective optimization, even though the OR method that is used is single-objective. Proof of concept approach on the standard (single-objective) Zeno transportation benchmark is given, and a small original multi-objective benchmark is proposed in the same Zeno framework to assess the multi-objective capabilities of the proposed methodology, a breakthrough in Temporal Planning.",
        "published": "2006-01-09T16:57:08Z",
        "link": "http://arxiv.org/abs/cs/0601031v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Efficient Open World Reasoning for Planning",
        "authors": [
            "Tamara Babaian",
            "James G. Schmolze"
        ],
        "summary": "We consider the problem of reasoning and planning with incomplete knowledge and deterministic actions. We introduce a knowledge representation scheme called PSIPLAN that can effectively represent incompleteness of an agent's knowledge while allowing for sound, complete and tractable entailment in domains where the set of all objects is either unknown or infinite. We present a procedure for state update resulting from taking an action in PSIPLAN that is correct, complete and has only polynomial complexity. State update is performed without considering the set of all possible worlds corresponding to the knowledge state. As a result, planning with PSIPLAN is done without direct manipulation of possible worlds. PSIPLAN representation underlies the PSIPOP planning algorithm that handles quantified goals with or without exceptions that no other domain independent planner has been shown to achieve. PSIPLAN has been implemented in Common Lisp and used in an application on planning in a collaborative interface.",
        "published": "2006-01-09T18:07:37Z",
        "link": "http://arxiv.org/abs/cs/0601032v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.2.8; F.4.1; F.2.2"
        ]
    },
    {
        "title": "LPAR-05 Workshop: Empirically Successfull Automated Reasoning in   Higher-Order Logic (ESHOL)",
        "authors": [
            "Christoph Benzmueller",
            "John Harrison",
            "Carsten Schuermann"
        ],
        "summary": "This workshop brings together practioners and researchers who are involved in the everyday aspects of logical systems based on higher-order logic. We hope to create a friendly and highly interactive setting for discussions around the following four topics. Implementation and development of proof assistants based on any notion of impredicativity, automated theorem proving tools for higher-order logic reasoning systems, logical framework technology for the representation of proofs in higher-order logic, formal digital libraries for storing, maintaining and querying databases of proofs.   We envision attendees that are interested in fostering the development and visibility of reasoning systems for higher-order logics. We are particularly interested in a discusssion on the development of a higher-order version of the TPTP and in comparisons of the practical strengths of automated higher-order reasoning systems. Additionally, the workshop includes system demonstrations.   ESHOL is the successor of the ESCAR and ESFOR workshops held at CADE 2005 and IJCAR 2004.",
        "published": "2006-01-10T18:43:59Z",
        "link": "http://arxiv.org/abs/cs/0601042v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Combining Relational Algebra, SQL, Constraint Modelling, and Local   Search",
        "authors": [
            "Marco Cadoli",
            "Toni Mancini"
        ],
        "summary": "The goal of this paper is to provide a strong integration between constraint modelling and relational DBMSs. To this end we propose extensions of standard query languages such as relational algebra and SQL, by adding constraint modelling capabilities to them. In particular, we propose non-deterministic extensions of both languages, which are specially suited for combinatorial problems. Non-determinism is introduced by means of a guessing operator, which declares a set of relations to have an arbitrary extension. This new operator results in languages with higher expressive power, able to express all problems in the complexity class NP. Some syntactical restrictions which make data complexity polynomial are shown. The effectiveness of both extensions is demonstrated by means of several examples. The current implementation, written in Java using local search techniques, is described. To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2006-01-11T14:29:44Z",
        "link": "http://arxiv.org/abs/cs/0601043v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "A Constructive Semantic Characterization of Aggregates in ASP",
        "authors": [
            "Tran Cao Son",
            "Enrico Pontelli"
        ],
        "summary": "This technical note describes a monotone and continuous fixpoint operator to compute the answer sets of programs with aggregates. The fixpoint operator relies on the notion of aggregate solution. Under certain conditions, this operator behaves identically to the three-valued immediate consequence operator $\\Phi^{aggr}_P$ for aggregate programs, independently proposed Pelov et al. This operator allows us to closely tie the computational complexity of the answer set checking and answer sets existence problems to the cost of checking a solution of the aggregates in the program. Finally, we relate the semantics described by the operator to other proposals for logic programming with aggregates.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2006-01-13T16:09:36Z",
        "link": "http://arxiv.org/abs/cs/0601051v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.1.6; D.3.1; D.3.2; D.3.3"
        ]
    },
    {
        "title": "Artificial and Biological Intelligence",
        "authors": [
            "Subhash Kak"
        ],
        "summary": "This article considers evidence from physical and biological sciences to show machines are deficient compared to biological systems at incorporating intelligence. Machines fall short on two counts: firstly, unlike brains, machines do not self-organize in a recursive manner; secondly, machines are based on classical logic, whereas Nature's intelligence may depend on quantum mechanics.",
        "published": "2006-01-13T19:01:42Z",
        "link": "http://arxiv.org/abs/cs/0601052v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Fast Frequent Querying with Lazy Control Flow Compilation",
        "authors": [
            "Remko Tronçon",
            "Gerda Janssens",
            "Bart Demoen",
            "Henk Vandecasteele"
        ],
        "summary": "Control flow compilation is a hybrid between classical WAM compilation and meta-call, limited to the compilation of non-recursive clause bodies. This approach is used successfully for the execution of dynamically generated queries in an inductive logic programming setting (ILP). Control flow compilation reduces compilation times up to an order of magnitude, without slowing down execution. A lazy variant of control flow compilation is also presented. By compiling code by need, it removes the overhead of compiling unreached code (a frequent phenomenon in practical ILP settings), and thus reduces the size of the compiled code. Both dynamic compilation approaches have been implemented and were combined with query packs, an efficient ILP execution mechanism. It turns out that locality of data and code is important for performance. The experiments reported in the paper show that lazy control flow compilation is superior in both artificial and real life settings.",
        "published": "2006-01-16T13:11:51Z",
        "link": "http://arxiv.org/abs/cs/0601072v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Distributed Kernel Regression: An Algorithm for Training Collaboratively",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "This paper addresses the problem of distributed learning under communication constraints, motivated by distributed signal processing in wireless sensor networks and data mining with distributed databases. After formalizing a general model for distributed learning, an algorithm for collaboratively training regularized kernel least-squares regression estimators is derived. Noting that the algorithm can be viewed as an application of successive orthogonal projection algorithms, its convergence properties are investigated and the statistical behavior of the estimator is discussed in a simplified theoretical setting.",
        "published": "2006-01-20T17:46:45Z",
        "link": "http://arxiv.org/abs/cs/0601089v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Fast Lexically Constrained Viterbi Algorithm (FLCVA): Simultaneous   Optimization of Speed and Memory",
        "authors": [
            "Alain Lifchitz",
            "Frederic Maire",
            "Dominique Revuz"
        ],
        "summary": "Lexical constraints on the input of speech and on-line handwriting systems improve the performance of such systems. A significant gain in speed can be achieved by integrating in a digraph structure the different Hidden Markov Models (HMM) corresponding to the words of the relevant lexicon. This integration avoids redundant computations by sharing intermediate results between HMM's corresponding to different words of the lexicon. In this paper, we introduce a token passing method to perform simultaneously the computation of the a posteriori probabilities of all the words of the lexicon. The coding scheme that we introduce for the tokens is optimal in the information theory sense. The tokens use the minimum possible number of bits. Overall, we optimize simultaneously the execution speed and the memory requirement of the recognition systems.",
        "published": "2006-01-25T17:50:13Z",
        "link": "http://arxiv.org/abs/cs/0601108v4",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.DS",
            "G.2.2; I.5.5; E.2"
        ]
    },
    {
        "title": "Certainty Closure: Reliable Constraint Reasoning with Incomplete or   Erroneous Data",
        "authors": [
            "Neil Yorke-Smith",
            "Carmen Gervet"
        ],
        "summary": "Constraint Programming (CP) has proved an effective paradigm to model and solve difficult combinatorial satisfaction and optimisation problems from disparate domains. Many such problems arising from the commercial world are permeated by data uncertainty. Existing CP approaches that accommodate uncertainty are less suited to uncertainty arising due to incomplete and erroneous data, because they do not build reliable models and solutions guaranteed to address the user's genuine problem as she perceives it. Other fields such as reliable computation offer combinations of models and associated methods to handle these types of uncertain data, but lack an expressive framework characterising the resolution methodology independently of the model.   We present a unifying framework that extends the CP formalism in both model and solutions, to tackle ill-defined combinatorial problems with incomplete or erroneous data. The certainty closure framework brings together modelling and solving methodologies from different fields into the CP paradigm to provide reliable and efficient approches for uncertain constraint problems. We demonstrate the applicability of the framework on a case study in network diagnosis. We define resolution forms that give generic templates, and their associated operational semantics, to derive practical solution methods for reliable solutions.",
        "published": "2006-01-25T20:11:11Z",
        "link": "http://arxiv.org/abs/cs/0601109v3",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "A Multi-Relational Network to Support the Scholarly Communication   Process",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "The general pupose of the scholarly communication process is to support the creation and dissemination of ideas within the scientific community. At a finer granularity, there exists multiple stages which, when confronted by a member of the community, have different requirements and therefore different solutions. In order to take a researcher's idea from an initial inspiration to a community resource, the scholarly communication infrastructure may be required to 1) provide a scientist initial seed ideas; 2) form a team of well suited collaborators; 3) located the most appropriate venue to publish the formalized idea; 4) determine the most appropriate peers to review the manuscript; and 5) disseminate the end product to the most interested members of the community. Through the various delinieations of this process, the requirements of each stage are tied soley to the multi-functional resources of the community: its researchers, its journals, and its manuscritps. It is within the collection of these resources and their inherent relationships that the solutions to scholarly communication are to be found. This paper describes an associative network composed of multiple scholarly artifacts that can be used as a medium for supporting the scholarly communication process.",
        "published": "2006-01-28T22:45:42Z",
        "link": "http://arxiv.org/abs/cs/0601121v2",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Instantaneously Trained Neural Networks",
        "authors": [
            "Abhilash Ponnath"
        ],
        "summary": "This paper presents a review of instantaneously trained neural networks (ITNNs). These networks trade learning time for size and, in the basic model, a new hidden node is created for each training sample. Various versions of the corner-classification family of ITNNs, which have found applications in artificial intelligence (AI), are described. Implementation issues are also considered.",
        "published": "2006-01-30T22:02:47Z",
        "link": "http://arxiv.org/abs/cs/0601129v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Scalable Algorithms for Aggregating Disparate Forecasts of Probability",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "Daniel N. Osherson",
            "H. Vincent Poor"
        ],
        "summary": "In this paper, computational aspects of the panel aggregation problem are addressed. Motivated primarily by applications of risk assessment, an algorithm is developed for aggregating large corpora of internally incoherent probability assessments. The algorithm is characterized by a provable performance guarantee, and is demonstrated to be orders of magnitude faster than existing tools when tested on several real-world data-sets. In addition, unexpected connections between research in risk assessment and wireless sensor networks are exposed, as several key ideas are illustrated to be useful in both fields.",
        "published": "2006-01-31T03:01:35Z",
        "link": "http://arxiv.org/abs/cs/0601131v2",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Study on the Global Convergence Time Complexity of Estimation of   Distribution Algorithms",
        "authors": [
            "R. Rastegar",
            "M. R. Meybodi"
        ],
        "summary": "The Estimation of Distribution Algorithm is a new class of population based search methods in that a probabilistic model of individuals is estimated based on the high quality individuals and used to generate the new individuals. In this paper we compute 1) some upper bounds on the number of iterations required for global convergence of EDA 2) the exact number of iterations needed for EDA to converge to global optima.",
        "published": "2006-01-31T07:10:45Z",
        "link": "http://arxiv.org/abs/cs/0601132v3",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Conjunctive Queries over Trees",
        "authors": [
            "Georg Gottlob",
            "Christoph Koch",
            "Klaus U. Schulz"
        ],
        "summary": "We study the complexity and expressive power of conjunctive queries over unranked labeled trees represented using a variety of structure relations such as ``child'', ``descendant'', and ``following'' as well as unary relations for node labels. We establish a framework for characterizing structures representing trees for which conjunctive queries can be evaluated efficiently. Then we completely chart the tractability frontier of the problem and establish a dichotomy theorem for our axis relations, i.e., we find all subset-maximal sets of axes for which query evaluation is in polynomial time and show that for all other cases, query evaluation is NP-complete. All polynomial-time results are obtained immediately using the proof techniques from our framework. Finally, we study the expressiveness of conjunctive queries over trees and show that for each conjunctive query, there is an equivalent acyclic positive query (i.e., a set of acyclic conjunctive queries), but that in general this query is not of polynomial size.",
        "published": "2006-02-02T23:28:24Z",
        "link": "http://arxiv.org/abs/cs/0602004v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "E.1; F.1.3; F.2.2; H.2.3; H.2.4; I.7.2"
        ]
    },
    {
        "title": "The intuitionistic fragment of computability logic at the propositional   level",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "This paper presents a soundness and completeness proof for propositional intuitionistic calculus with respect to the semantics of computability logic. The latter interprets formulas as interactive computational problems, formalized as games between a machine and its environment. Intuitionistic implication is understood as algorithmic reduction in the weakest possible -- and hence most natural -- sense, disjunction and conjunction as deterministic-choice combinations of problems (disjunction = machine's choice, conjunction = environment's choice), and \"absurd\" as a computational problem of universal strength. See http://www.cis.upenn.edu/~giorgi/cl.html for a comprehensive online source on computability logic.",
        "published": "2006-02-05T12:55:17Z",
        "link": "http://arxiv.org/abs/cs/0602011v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Improving the CSIEC Project and Adapting It to the English Teaching and   Learning in China",
        "authors": [
            "Jiyou Jia",
            "Shufen Hou",
            "Weichao Chen"
        ],
        "summary": "In this paper after short review of the CSIEC project initialized by us in 2003 we present the continuing development and improvement of the CSIEC project in details, including the design of five new Microsoft agent characters representing different virtual chatting partners and the limitation of simulated dialogs in specific practical scenarios like graduate job application interview, then briefly analyze the actual conditions and features of its application field: web-based English education in China. Finally we introduce our efforts to adapt this system to the requirements of English teaching and learning in China and point out the work next to do.",
        "published": "2006-02-06T15:17:34Z",
        "link": "http://arxiv.org/abs/cs/0602018v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.MA",
            "K.3.1; I.2.7; I.2.11"
        ]
    },
    {
        "title": "Using Domain Knowledge in Evolutionary System Identification",
        "authors": [
            "Marc Schoenauer",
            "Michèle Sebag"
        ],
        "summary": "Two example of Evolutionary System Identification are presented to highlight the importance of incorporating Domain Knowledge: the discovery of an analytical indentation law in Structural Mechanics using constrained Genetic Programming, and the identification of the repartition of underground velocities in Seismic Prospection. Critical issues for sucessful ESI are discussed in the light of these results.",
        "published": "2006-02-07T07:45:26Z",
        "link": "http://arxiv.org/abs/cs/0602021v1",
        "categories": [
            "cs.AI",
            "math.AP"
        ]
    },
    {
        "title": "Avoiding the Bloat with Stochastic Grammar-based Genetic Programming",
        "authors": [
            "Alain Ratle",
            "Michèle Sebag"
        ],
        "summary": "The application of Genetic Programming to the discovery of empirical laws is often impaired by the huge size of the search space, and consequently by the computer resources needed. In many cases, the extreme demand for memory and CPU is due to the massive growth of non-coding segments, the introns. The paper presents a new program evolution framework which combines distribution-based evolution in the PBIL spirit, with grammar-based genetic programming; the information is stored as a probability distribution on the gra mmar rules, rather than in a population. Experiments on a real-world like problem show that this approach gives a practical solution to the problem of intron growth.",
        "published": "2006-02-07T07:48:27Z",
        "link": "http://arxiv.org/abs/cs/0602022v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Explaining Constraint Programming",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We discuss here constraint programming (CP) by using a proof-theoretic perspective. To this end we identify three levels of abstraction. Each level sheds light on the essence of CP.   In particular, the highest level allows us to bring CP closer to the computation as deduction paradigm. At the middle level we can explain various constraint propagation algorithms. Finally, at the lowest level we can address the issue of automatic generation and optimization of the constraint propagation algorithms.",
        "published": "2006-02-07T15:19:53Z",
        "link": "http://arxiv.org/abs/cs/0602027v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; F.4.1"
        ]
    },
    {
        "title": "Classifying Signals with Local Classifiers",
        "authors": [
            "Wit Jakuczun"
        ],
        "summary": "This paper deals with the problem of classifying signals. The new method for building so called local classifiers and local features is presented. The method is a combination of the lifting scheme and the support vector machines. Its main aim is to produce effective and yet comprehensible classifiers that would help in understanding processes hidden behind classified signals. To illustrate the method we present the results obtained on an artificial and a real dataset.",
        "published": "2006-02-08T11:38:44Z",
        "link": "http://arxiv.org/abs/cs/0602031v1",
        "categories": [
            "cs.AI",
            "I.5; G.3; G.1.2"
        ]
    },
    {
        "title": "Minimum Cost Homomorphisms to Proper Interval Graphs and Bigraphs",
        "authors": [
            "G. Gutin",
            "P. Hell",
            "A. Rafiey",
            "A. Yeo"
        ],
        "summary": "For graphs $G$ and $H$, a mapping $f: V(G)\\dom V(H)$ is a homomorphism of $G$ to $H$ if $uv\\in E(G)$ implies $f(u)f(v)\\in E(H).$ If, moreover, each vertex $u \\in V(G)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of the homomorphism $f$ is $\\sum_{u\\in V(G)}c_{f(u)}(u)$. For each fixed graph $H$, we have the {\\em minimum cost homomorphism problem}, written as MinHOM($H)$. The problem is to decide, for an input graph $G$ with costs $c_i(u),$ $u \\in V(G), i\\in V(H)$, whether there exists a homomorphism of $G$ to $H$ and, if one exists, to find one of minimum cost. Minimum cost homomorphism problems encompass (or are related to) many well studied optimization problems. We describe a dichotomy of the minimum cost homomorphism problems for graphs $H$, with loops allowed. When each connected component of $H$ is either a reflexive proper interval graph or an irreflexive proper interval bigraph, the problem MinHOM($H)$ is polynomial time solvable. In all other cases the problem MinHOM($H)$ is NP-hard. This solves an open problem from an earlier paper. Along the way, we prove a new characterization of the class of proper interval bigraphs.",
        "published": "2006-02-10T09:39:36Z",
        "link": "http://arxiv.org/abs/cs/0602038v2",
        "categories": [
            "cs.DM",
            "cs.AI"
        ]
    },
    {
        "title": "A third level trigger programmable on FPGA for the gamma/hadron   separation in a Cherenkov telescope using pseudo-Zernike moments and the SVM   classifier",
        "authors": [
            "Marco Frailis",
            "Oriana Mansutti",
            "Praveen Boinee",
            "Giuseppe Cabras",
            "Alessandro De Angelis",
            "Barbara De Lotto",
            "Alberto Forti",
            "Mauro Dell'Orso",
            "Riccardo Paoletti",
            "Angelo Scribano",
            "Nicola Turini",
            "Mose' Mariotti",
            "Luigi Peruzzo",
            "Antonio Saggion"
        ],
        "summary": "We studied the application of the Pseudo-Zernike features as image parameters (instead of the Hillas parameters) for the discrimination between the images produced by atmospheric electromagnetic showers caused by gamma-rays and the ones produced by atmospheric electromagnetic showers caused by hadrons in the MAGIC Experiment. We used a Support Vector Machine as classification algorithm with the computed Pseudo-Zernike features as classification parameters. We implemented on a FPGA board a kernel function of the SVM and the Pseudo-Zernike features to build a third level trigger for the gamma-hadron separation task of the MAGIC Experiment.",
        "published": "2006-02-24T17:29:08Z",
        "link": "http://arxiv.org/abs/cs/0602083v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.5.2; C.3"
        ]
    },
    {
        "title": "Open Answer Set Programming with Guarded Programs",
        "authors": [
            "Stijn Heymans",
            "Davy Van Nieuwenborgh",
            "Dirk Vermeir"
        ],
        "summary": "Open answer set programming (OASP) is an extension of answer set programming where one may ground a program with an arbitrary superset of the program's constants. We define a fixed point logic (FPL) extension of Clark's completion such that open answer sets correspond to models of FPL formulas and identify a syntactic subclass of programs, called (loosely) guarded programs. Whereas reasoning with general programs in OASP is undecidable, the FPL translation of (loosely) guarded programs falls in the decidable (loosely) guarded fixed point logic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP, enabling for the first time, a characterization of an answer set semantics by muLGF formulas. We further extend the open answer set semantics for programs with generalized literals. Such generalized programs (gPs) have interesting properties, e.g., the ability to express infinity axioms. We restrict the syntax of gPs such that both rules and generalized literals are guarded. Via a translation to guarded fixed point logic, we deduce 2-exptime-completeness of satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted GgPs with exptime-complete satisfiability checking, but still sufficiently expressive to optimally simulate computation tree logic (CTL). We translate Datalog lite programs to GgPs, establishing equivalence of GgPs under an open answer set semantics, alternation-free muGF, and Datalog lite.",
        "published": "2006-03-07T17:54:59Z",
        "link": "http://arxiv.org/abs/cs/0603025v2",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Metatheory of actions: beyond consistency",
        "authors": [
            "Andreas Herzig",
            "Ivan Varzinczak"
        ],
        "summary": "Consistency check has been the only criterion for theory evaluation in logic-based approaches to reasoning about actions. This work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domain description in reasoning about actions should have. We state some metatheoretical postulates concerning this sore spot. When all postulates are satisfied together we have a modular action theory. Besides being easier to understand and more elaboration tolerant in McCarthy's sense, modular theories have interesting properties. We point out the problems that arise when the postulates about modularity are violated and propose algorithmic checks that can help the designer of an action theory to overcome them.",
        "published": "2006-03-09T10:07:46Z",
        "link": "http://arxiv.org/abs/cs/0603034v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Estimation of linear, non-gaussian causal models in the presence of   confounding latent variables",
        "authors": [
            "Patrik O. Hoyer",
            "Shohei Shimizu",
            "Antti J. Kerminen"
        ],
        "summary": "The estimation of linear causal models (also known as structural equation models) from data is a well-known problem which has received much attention in the past. Most previous work has, however, made an explicit or implicit assumption of gaussianity, limiting the identifiability of the models. We have recently shown (Shimizu et al, 2005; Hoyer et al, 2006) that for non-gaussian distributions the full causal model can be estimated in the no hidden variables case. In this contribution, we discuss the estimation of the model when confounding latent variables are present. Although in this case uniqueness is no longer guaranteed, there is at most a finite set of models which can fit the data. We develop an algorithm for estimating this set, and describe numerical simulations which confirm the theoretical arguments and demonstrate the practical viability of the approach. Full Matlab code is provided for all simulations.",
        "published": "2006-03-09T14:46:18Z",
        "link": "http://arxiv.org/abs/cs/0603038v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Convergence of Min-Sum Message Passing for Quadratic Optimization",
        "authors": [
            "Ciamac C. Moallemi",
            "Benjamin Van Roy"
        ],
        "summary": "We establish the convergence of the min-sum message passing algorithm for minimization of a broad class of quadratic objective functions: those that admit a convex decomposition. Our results also apply to the equivalent problem of the convergence of Gaussian belief propagation.",
        "published": "2006-03-14T20:32:28Z",
        "link": "http://arxiv.org/abs/cs/0603058v4",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "Consensus Propagation",
        "authors": [
            "Ciamac C. Moallemi",
            "Benjamin Van Roy"
        ],
        "summary": "We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge.",
        "published": "2006-03-19T23:50:28Z",
        "link": "http://arxiv.org/abs/cs/0603078v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "Yet Another Efficient Unification Algorithm",
        "authors": [
            "Alin Suciu"
        ],
        "summary": "The unification algorithm is at the core of the logic programming paradigm, the first unification algorithm being developed by Robinson [5]. More efficient algorithms were developed later [3] and I introduce here yet another efficient unification algorithm centered on a specific data structure, called the Unification Table.",
        "published": "2006-03-20T20:15:38Z",
        "link": "http://arxiv.org/abs/cs/0603080v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Application of Support Vector Regression to Interpolation of Sparse   Shock Physics Data Sets",
        "authors": [
            "Nikita A. Sakhanenko",
            "George F. Luger",
            "Hanna E. Makaruk",
            "David B. Holtkamp"
        ],
        "summary": "Shock physics experiments are often complicated and expensive. As a result, researchers are unable to conduct as many experiments as they would like - leading to sparse data sets. In this paper, Support Vector Machines for regression are applied to velocimetry data sets for shock damaged and melted tin metal. Some success at interpolating between data sets is achieved. Implications for future work are discussed.",
        "published": "2006-03-20T23:43:45Z",
        "link": "http://arxiv.org/abs/cs/0603081v1",
        "categories": [
            "cs.AI",
            "I.2; J.2"
        ]
    },
    {
        "title": "Asymptotic Learnability of Reinforcement Problems with Arbitrary   Dependence",
        "authors": [
            "Daniil Ryabko",
            "Marcus Hutter"
        ],
        "summary": "We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.",
        "published": "2006-03-28T16:22:42Z",
        "link": "http://arxiv.org/abs/cs/0603110v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Approximation Algorithms for K-Modes Clustering",
        "authors": [
            "Zengyou He"
        ],
        "summary": "In this paper, we study clustering with respect to the k-modes objective function, a natural formulation of clustering for categorical data. One of the main contributions of this paper is to establish the connection between k-modes and k-median, i.e., the optimum of k-median is at most twice the optimum of k-modes for the same categorical data clustering problem. Based on this observation, we derive a deterministic algorithm that achieves an approximation factor of 2. Furthermore, we prove that the distance measure in k-modes defines a metric. Hence, we are able to extend existing approximation algorithms for metric k-median to k-modes. Empirical results verify the superiority of our method.",
        "published": "2006-03-30T02:02:37Z",
        "link": "http://arxiv.org/abs/cs/0603120v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Can an Organism Adapt Itself to Unforeseen Circumstances?",
        "authors": [
            "Alexey V. Melkikh"
        ],
        "summary": "A model of an organism as an autonomous intelligent system has been proposed. This model was used to analyze learning of an organism in various environmental conditions. Processes of learning were divided into two types: strong and weak processes taking place in the absence and the presence of aprioristic information about an object respectively. Weak learning is synonymous to adaptation when aprioristic programs already available in a system (an organism) are started. It was shown that strong learning is impossible for both an organism and any autonomous intelligent system. It was shown also that the knowledge base of an organism cannot be updated. Therefore, all behavior programs of an organism are congenital. A model of a conditioned reflex as a series of consecutive measurements of environmental parameters has been advanced. Repeated measurements are necessary in this case to reduce the error during decision making.",
        "published": "2006-04-05T10:29:28Z",
        "link": "http://arxiv.org/abs/cs/0604009v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Nearly optimal exploration-exploitation decision thresholds",
        "authors": [
            "Christos Dimitrakakis"
        ],
        "summary": "While in general trading off exploration and exploitation in reinforcement learning is hard, under some formulations relatively simple solutions exist. In this paper, we first derive upper bounds for the utility of selecting different actions in the multi-armed bandit setting. Unlike the common statistical upper confidence bounds, these explicitly link the planning horizon, uncertainty and the need for exploration explicit. The resulting algorithm can be seen as a generalisation of the classical Thompson sampling algorithm. We experimentally test these algorithms, as well as $\\epsilon$-greedy and the value of perfect information heuristics. Finally, we also introduce the idea of bagging for reinforcement learning. By employing a version of online bootstrapping, we can efficiently sample from an approximate posterior distribution.",
        "published": "2006-04-05T10:29:48Z",
        "link": "http://arxiv.org/abs/cs/0604010v2",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "UniCalc.LIN: a linear constraint solver for the UniCalc system",
        "authors": [
            "E. Petrov",
            "Yu. Kostov",
            "E. Botoeva"
        ],
        "summary": "In this short paper we present a linear constraint solver for the UniCalc system, an environment for reliable solution of mathematical modeling problems.",
        "published": "2006-04-10T06:30:02Z",
        "link": "http://arxiv.org/abs/cs/0604038v1",
        "categories": [
            "cs.MS",
            "cs.AI"
        ]
    },
    {
        "title": "Adaptative combination rule and proportional conflict redistribution   rule for information fusion",
        "authors": [
            "M. C. Florea",
            "J. Dezert",
            "P. Valin",
            "F. Smarandache",
            "Anne-Laure Jousselme"
        ],
        "summary": "This paper presents two new promising rules of combination for the fusion of uncertain and potentially highly conflicting sources of evidences in the framework of the theory of belief functions in order to palliate the well-know limitations of Dempster's rule and to work beyond the limits of applicability of the Dempster-Shafer theory. We present both a new class of adaptive combination rules (ACR) and a new efficient Proportional Conflict Redistribution (PCR) rule allowing to deal with highly conflicting sources for static and dynamic fusion applications.",
        "published": "2006-04-11T14:35:15Z",
        "link": "http://arxiv.org/abs/cs/0604042v1",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "New results on rewrite-based satisfiability procedures",
        "authors": [
            "Alessandro Armando",
            "Maria Paola Bonacina",
            "Silvio Ranise",
            "Stephan Schulz"
        ],
        "summary": "Program analysis and verification require decision procedures to reason on theories of data structures. Many problems can be reduced to the satisfiability of sets of ground literals in theory T. If a sound and complete inference system for first-order logic is guaranteed to terminate on T-satisfiability problems, any theorem-proving strategy with that system and a fair search plan is a T-satisfiability procedure. We prove termination of a rewrite-based first-order engine on the theories of records, integer offsets, integer offsets modulo and lists. We give a modularity theorem stating sufficient conditions for termination on a combinations of theories, given termination on each. The above theories, as well as others, satisfy these conditions. We introduce several sets of benchmarks on these theories and their combinations, including both parametric synthetic benchmarks to test scalability, and real-world problems to test performances on huge sets of literals. We compare the rewrite-based theorem prover E with the validity checkers CVC and CVC Lite. Contrary to the folklore that a general-purpose prover cannot compete with reasoners with built-in theories, the experiments are overall favorable to the theorem prover, showing that not only the rewriting approach is elegant and conceptually simple, but has important practical implications.",
        "published": "2006-04-12T19:53:24Z",
        "link": "http://arxiv.org/abs/cs/0604054v4",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Quantum Fuzzy Sets: Blending Fuzzy Set Theory and Quantum Computation",
        "authors": [
            "Mirco A. Mannucci"
        ],
        "summary": "In this article we investigate a way in which quantum computing can be used to extend the class of fuzzy sets. The core idea is to see states of a quantum register as characteristic functions of quantum fuzzy subsets of a given set. As the real unit interval is embedded in the Bloch sphere, every fuzzy set is automatically a quantum fuzzy set. However, a generic quantum fuzzy set can be seen as a (possibly entangled) superposition of many fuzzy sets at once, offering new opportunities for modeling uncertainty. After introducing the main framework of quantum fuzzy set theory, we analyze the standard operations of fuzzification and defuzzification from our viewpoint. We conclude this preliminary paper with a list of possible applications of quantum fuzzy sets to pattern recognition, as well as future directions of pure research in quantum fuzzy set theory.",
        "published": "2006-04-16T16:23:10Z",
        "link": "http://arxiv.org/abs/cs/0604064v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Retraction and Generalized Extension of Computing with Words",
        "authors": [
            "Yongzhi Cao",
            "Mingsheng Ying",
            "Guoqing Chen"
        ],
        "summary": "Fuzzy automata, whose input alphabet is a set of numbers or symbols, are a formal model of computing with values. Motivated by Zadeh's paradigm of computing with words rather than numbers, Ying proposed a kind of fuzzy automata, whose input alphabet consists of all fuzzy subsets of a set of symbols, as a formal model of computing with all words. In this paper, we introduce a somewhat general formal model of computing with (some special) words. The new features of the model are that the input alphabet only comprises some (not necessarily all) fuzzy subsets of a set of symbols and the fuzzy transition function can be specified arbitrarily. By employing the methodology of fuzzy control, we establish a retraction principle from computing with words to computing with values for handling crisp inputs and a generalized extension principle from computing with words to computing with all words for handling fuzzy inputs. These principles show that computing with values and computing with all words can be respectively implemented by computing with words. Some algebraic properties of retractions and generalized extensions are addressed as well.",
        "published": "2006-04-19T06:28:55Z",
        "link": "http://arxiv.org/abs/cs/0604070v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Naming Games in Spatially-Embedded Random Networks",
        "authors": [
            "Qiming Lu",
            "G. Korniss",
            "Boleslaw K. Szymanski"
        ],
        "summary": "We investigate a prototypical agent-based model, the Naming Game, on random geometric networks. The Naming Game is a minimal model, employing local communications that captures the emergence of shared communication schemes (languages) in a population of autonomous semiotic agents. Implementing the Naming Games on random geometric graphs, local communications being local broadcasts, serves as a model for agreement dynamics in large-scale, autonomously operating wireless sensor networks. Further, it captures essential features of the scaling properties of the agreement process for spatially-embedded autonomous agents. We also present results for the case when a small density of long-range communication links are added on top of the random geometric graph, resulting in a \"small-world\"-like network and yielding a significantly reduced time to reach global agreement.",
        "published": "2006-04-19T17:48:11Z",
        "link": "http://arxiv.org/abs/cs/0604075v3",
        "categories": [
            "cs.MA",
            "cond-mat.stat-mech",
            "cs.AI"
        ]
    },
    {
        "title": "A Knowledge-Based Approach for Selecting Information Sources",
        "authors": [
            "Thomas Eiter",
            "Michael Fink",
            "Hans Tompits"
        ],
        "summary": "Through the Internet and the World-Wide Web, a vast number of information sources has become available, which offer information on various subjects by different providers, often in heterogeneous formats. This calls for tools and methods for building an advanced information-processing infrastructure. One issue in this area is the selection of suitable information sources in query answering. In this paper, we present a knowledge-based approach to this problem, in the setting where one among a set of information sources (prototypically, data repositories) should be selected for evaluating a user query. We use extended logic programs (ELPs) to represent rich descriptions of the information sources, an underlying domain theory, and user queries in a formal query language (here, XML-QL, but other languages can be handled as well). Moreover, we use ELPs for declarative query analysis and generation of a query description. Central to our approach are declarative source-selection programs, for which we define syntax and semantics. Due to the structured nature of the considered data items, the semantics of such programs must carefully respect implicit context information in source-selection rules, and furthermore combine it with possible user preferences. A prototype implementation of our approach has been realized exploiting the DLV KR system and its plp front-end for prioritized ELPs. We describe a representative example involving specific movie databases, and report about experimental results.",
        "published": "2006-04-21T16:53:28Z",
        "link": "http://arxiv.org/abs/cs/0604086v1",
        "categories": [
            "cs.AI",
            "I.2.4; H.3.3"
        ]
    },
    {
        "title": "Probabilistic Automata for Computing with Words",
        "authors": [
            "Yongzhi Cao",
            "Lirong Xia",
            "Mingsheng Ying"
        ],
        "summary": "Usually, probabilistic automata and probabilistic grammars have crisp symbols as inputs, which can be viewed as the formal models of computing with values. In this paper, we first introduce probabilistic automata and probabilistic grammars for computing with (some special) words in a probabilistic framework, where the words are interpreted as probabilistic distributions or possibility distributions over a set of crisp symbols. By probabilistic conditioning, we then establish a retraction principle from computing with words to computing with values for handling crisp inputs and a generalized extension principle from computing with words to computing with all words for handling arbitrary inputs. These principles show that computing with values and computing with all words can be respectively implemented by computing with some special words. To compare the transition probabilities of two near inputs, we also examine some analytical properties of the transition probability functions of generalized extensions. Moreover, the retractions and the generalized extensions are shown to be equivalence-preserving. Finally, we clarify some relationships among the retractions, the generalized extensions, and the extensions studied recently by Qiu and Wang.",
        "published": "2006-04-23T02:58:51Z",
        "link": "http://arxiv.org/abs/cs/0604087v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Perspective alignment in spatial language",
        "authors": [
            "L. Steels",
            "M. Loetzsch"
        ],
        "summary": "It is well known that perspective alignment plays a major role in the planning and interpretation of spatial language. In order to understand the role of perspective alignment and the cognitive processes involved, we have made precise complete cognitive models of situated embodied agents that self-organise a communication system for dialoging about the position and movement of real world objects in their immediate surroundings. We show in a series of robotic experiments which cognitive mechanisms are necessary and sufficient to achieve successful spatial language and why and how perspective alignment can take place, either implicitly or based on explicit marking.",
        "published": "2006-05-04T17:16:02Z",
        "link": "http://arxiv.org/abs/cs/0605012v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Reasoning and Planning with Sensing Actions, Incomplete Information, and   Static Causal Laws using Answer Set Programming",
        "authors": [
            "Phan Huy Tu",
            "Tran Cao Son",
            "Chitta Baral"
        ],
        "summary": "We extend the 0-approximation of sensing actions and incomplete information in [Son and Baral 2000] to action theories with static causal laws and prove its soundness with respect to the possible world semantics. We also show that the conditional planning problem with respect to this approximation is NP-complete. We then present an answer set programming based conditional planner, called ASCP, that is capable of generating both conformant plans and conditional plans in the presence of sensing actions, incomplete information about the initial state, and static causal laws. We prove the correctness of our implementation and argue that our planner is sound and complete with respect to the proposed approximation. Finally, we present experimental results comparing ASCP to other planners.",
        "published": "2006-05-04T22:35:12Z",
        "link": "http://arxiv.org/abs/cs/0605017v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4; I.2.8"
        ]
    },
    {
        "title": "A Formal Measure of Machine Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "summary": "A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this measure formally captures the concept of machine intelligence in the broadest reasonable sense.",
        "published": "2006-05-06T16:56:43Z",
        "link": "http://arxiv.org/abs/cs/0605024v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "On the Design of Agent-Based Systems using UML and Extensions",
        "authors": [
            "Mihaela Dinsoreanu",
            "Ioan Salomie",
            "Kalman Pusztai"
        ],
        "summary": "The Unified Software Development Process (USDP) and UML have been now generally accepted as the standard methodology and modeling language for developing Object-Oriented Systems. Although Agent-based Systems introduces new issues, we consider that USDP and UML can be used in an extended manner for modeling Agent-based Systems. The paper presents a methodology for designing agent-based systems and the specific models expressed in an UML-based notation corresponding to each phase of the software development process. UML was extended using the provided mechanism: stereotypes. Therefore, this approach can be managed with any CASE tool supporting UML. A Case Study, the development of a specific agent-based Student Evaluation System (SAS), is presented.",
        "published": "2006-05-08T12:21:51Z",
        "link": "http://arxiv.org/abs/cs/0605031v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "A framework of reusable structures for mobile agent development",
        "authors": [
            "Tudor Marian",
            "Bogdan Dumitriu",
            "Mihaela Dinsoreanu",
            "Ioan Salomie"
        ],
        "summary": "Mobile agents research is clearly aiming towards imposing agent based development as the next generation of tools for writing software. This paper comes with its own contribution to this global goal by introducing a novel unifying framework meant to bring simplicity and interoperability to and among agent platforms as we know them today. In addition to this, we also introduce a set of agent behaviors which, although tailored for and from the area of virtual learning environments, are none the less generic enough to be used for rapid, simple, useful and reliable agent deployment. The paper also presents an illustrative case study brought forward to prove the feasibility of our design.",
        "published": "2006-05-08T12:27:59Z",
        "link": "http://arxiv.org/abs/cs/0605032v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Mobile Agent Based Solutions for Knowledge Assessment in elearning   Environments",
        "authors": [
            "Mihaela Dinsoreanu",
            "Cristian Godja",
            "Claudiu Anghel",
            "Ioan Salomie",
            "Tom Coffey"
        ],
        "summary": "E-learning is nowadays one of the most interesting of the \"e- \" domains available through the Internet. The main problem to create a Web-based, virtual environment is to model the traditional domain and to implement the model using the most suitable technologies. We analyzed the distance learning domain and investigated the possibility to implement some e-learning services using mobile agent technologies. This paper presents a model of the Student Assessment Service (SAS) and an agent-based framework developed to be used for implementing specific applications. A specific Student Assessment application that relies on the framework was developed.",
        "published": "2006-05-08T12:37:13Z",
        "link": "http://arxiv.org/abs/cs/0605033v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "An Unfolding-Based Semantics for Logic Programming with Aggregates",
        "authors": [
            "Tran Cao Son",
            "Enrico Pontelli",
            "Islam Elkabani"
        ],
        "summary": "The paper presents two equivalent definitions of answer sets for logic programs with aggregates. These definitions build on the notion of unfolding of aggregates, and they are aimed at creating methodologies to translate logic programs with aggregates to normal logic programs or positive programs, whose answer set semantics can be used to defined the semantics of the original programs. The first definition provides an alternative view of the semantics for logic programming with aggregates described by Pelov et al.   The second definition is similar to the traditional answer set definition for normal logic programs, in that, given a logic program with aggregates and an interpretation, the unfolding process produces a positive program. The paper shows how this definition can be extended to consider aggregates in the head of the rules.   The proposed views of logic programming with aggregates are simple and coincide with the ultimate stable model semantics, and with other semantic characterizations for large classes of program (e.g., programs with monotone aggregates and programs that are aggregate-stratified).   Moreover, it can be directly employed to support an implementation using available answer set solvers. The paper describes a system, called ASP^A, that is capable of computing answer sets of programs with arbitrary (e.g., recursively defined) aggregates.",
        "published": "2006-05-09T04:08:24Z",
        "link": "http://arxiv.org/abs/cs/0605038v1",
        "categories": [
            "cs.SE",
            "cs.AI"
        ]
    },
    {
        "title": "Approximate Discrete Probability Distribution Representation using a   Multi-Resolution Binary Tree",
        "authors": [
            "David Bellot",
            "Pierre Bessiere"
        ],
        "summary": "Computing and storing probabilities is a hard problem as soon as one has to deal with complex distributions over multiple random variables. The problem of efficient representation of probability distributions is central in term of computational efficiency in the field of probabilistic reasoning. The main problem arises when dealing with joint probability distributions over a set of random variables: they are always represented using huge probability arrays. In this paper, a new method based on binary-tree representation is introduced in order to store efficiently very large joint distributions. Our approach approximates any multidimensional joint distributions using an adaptive discretization of the space. We make the assumption that the lower is the probability mass of a particular region of feature space, the larger is the discretization step. This assumption leads to a very optimized representation in term of time and memory. The other advantages of our approach are the ability to refine dynamically the distribution every time it is needed leading to a more accurate representation of the probability distribution and to an anytime representation of the distribution.",
        "published": "2006-05-12T13:32:50Z",
        "link": "http://arxiv.org/abs/cs/0605055v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Ontological Representations of Software Patterns",
        "authors": [
            "Jean-Marc Rosengard",
            "Marian Ursu"
        ],
        "summary": "This paper is based on and advocates the trend in software engineering of extending the use of software patterns as means of structuring solutions to software development problems (be they motivated by best practice or by company interests and policies). The paper argues that, on the one hand, this development requires tools for automatic organisation, retrieval and explanation of software patterns. On the other hand, that the existence of such tools itself will facilitate the further development and employment of patterns in the software development process. The paper analyses existing pattern representations and concludes that they are inadequate for the kind of automation intended here. Adopting a standpoint similar to that taken in the semantic web, the paper proposes that feasible solutions can be built on the basis of ontological representations.",
        "published": "2006-05-14T18:31:09Z",
        "link": "http://arxiv.org/abs/cs/0605059v1",
        "categories": [
            "cs.SE",
            "cs.AI"
        ]
    },
    {
        "title": "Modal Logics of Topological Relations",
        "authors": [
            "Carsten Lutz",
            "Frank Wolter"
        ],
        "summary": "Logical formalisms for reasoning about relations between spatial regions play a fundamental role in geographical information systems, spatial and constraint databases, and spatial reasoning in AI. In analogy with Halpern and Shoham's modal logic of time intervals based on the Allen relations, we introduce a family of modal logics equipped with eight modal operators that are interpreted by the Egenhofer-Franzosa (or RCC8) relations between regions in topological spaces such as the real plane. We investigate the expressive power and computational complexity of logics obtained in this way. It turns out that our modal logics have the same expressive power as the two-variable fragment of first-order logic, but are exponentially less succinct. The complexity ranges from (undecidable and) recursively enumerable to highly undecidable, where the recursively enumerable logics are obtained by considering substructures of structures induced by topological spaces. As our undecidability results also capture logics based on the real line, they improve upon undecidability results for interval temporal logics by Halpern and Shoham. We also analyze modal logics based on the five RCC5 relations, with similar results regarding the expressive power, but weaker results regarding the complexity.",
        "published": "2006-05-15T14:42:36Z",
        "link": "http://arxiv.org/abs/cs/0605064v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1; H.2.8; I.2.4"
        ]
    },
    {
        "title": "On the possible Computational Power of the Human Mind",
        "authors": [
            "Hector Zenil",
            "Francisco Hernandez-Quiroz"
        ],
        "summary": "The aim of this paper is to address the question: Can an artificial neural network (ANN) model be used as a possible characterization of the power of the human mind? We will discuss what might be the relationship between such a model and its natural counterpart. A possible characterization of the different power capabilities of the mind is suggested in terms of the information contained (in its computational complexity) or achievable by it. Such characterization takes advantage of recent results based on natural neural networks (NNN) and the computational power of arbitrary artificial neural networks (ANN). The possible acceptance of neural networks as the model of the human mind's operation makes the aforementioned quite relevant.",
        "published": "2006-05-15T17:56:55Z",
        "link": "http://arxiv.org/abs/cs/0605065v4",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Cross-Entropic Learning of a Machine for the Decision in a Partially   Observable Universe",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "Revision of the paper previously entitled \"Learning a Machine for the Decision in a Partially Observable Markov Universe\" In this paper, we are interested in optimal decisions in a partially observable universe. Our approach is to directly approximate an optimal strategic tree depending on the observation. This approximation is made by means of a parameterized probabilistic law. A particular family of hidden Markov models, with input \\emph{and} output, is considered as a model of policy. A method for optimizing the parameters of these HMMs is proposed and applied. This optimization is based on the cross-entropic principle for rare events simulation developed by Rubinstein.",
        "published": "2006-05-18T07:47:58Z",
        "link": "http://arxiv.org/abs/math/0605498v1",
        "categories": [
            "math.OC",
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Supervisory Control of Fuzzy Discrete Event Systems: A Formal Approach",
        "authors": [
            "Daowen Qiu"
        ],
        "summary": "Fuzzy {\\it discrete event systems} (DESs) were proposed recently by Lin and Ying [19], which may better cope with the real-world problems with fuzziness, impreciseness, and subjectivity such as those in biomedicine. As a continuation of [19], in this paper we further develop fuzzy DESs by dealing with supervisory control of fuzzy DESs. More specifically, (i) we reformulate the parallel composition of crisp DESs, and then define the parallel composition of fuzzy DESs that is equivalent to that in [19]; {\\it max-product} and {\\it max-min} automata for modeling fuzzy DESs are considered; (ii) we deal with a number of fundamental problems regarding supervisory control of fuzzy DESs, particularly demonstrate controllability theorem and nonblocking controllability theorem of fuzzy DESs, and thus present the conditions for the existence of supervisors in fuzzy DESs; (iii) we analyze the complexity for presenting a uniform criterion to test the fuzzy controllability condition of fuzzy DESs modeled by max-product automata; in particular, we present in detail a general computing method for checking whether or not the fuzzy controllability condition holds, if max-min automata are used to model fuzzy DESs, and by means of this method we can search for all possible fuzzy states reachable from initial fuzzy state in max-min automata; also, we introduce the fuzzy $n$-controllability condition for some practical problems; (iv) a number of examples serving to illustrate the applications of the derived results and methods are described; some basic properties related to supervisory control of fuzzy DESs are investigated. To conclude, some related issues are raised for further consideration.",
        "published": "2006-05-24T15:23:34Z",
        "link": "http://arxiv.org/abs/cs/0605106v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.1.2; F.4.3; J.7"
        ]
    },
    {
        "title": "Diagnosability of Fuzzy Discrete Event Systems",
        "authors": [
            "Fuchun Liu",
            "Daowen Qiu",
            "Hongyan Xing",
            "Zhujun Fan"
        ],
        "summary": "In order to more effectively cope with the real-world problems of vagueness, {\\it fuzzy discrete event systems} (FDESs) were proposed recently, and the supervisory control theory of FDESs was developed. In view of the importance of failure diagnosis, in this paper, we present an approach of the failure diagnosis in the framework of FDESs. More specifically: (1) We formalize the definition of diagnosability for FDESs, in which the observable set and failure set of events are {\\it fuzzy}, that is, each event has certain degree to be observable and unobservable, and, also, each event may possess different possibility of failure occurring. (2) Through the construction of observability-based diagnosers of FDESs, we investigate its some basic properties. In particular, we present a necessary and sufficient condition for diagnosability of FDESs. (3) Some examples serving to illuminate the applications of the diagnosability of FDESs are described. To conclude, some related issues are raised for further consideration.",
        "published": "2006-05-24T15:49:06Z",
        "link": "http://arxiv.org/abs/cs/0605108v2",
        "categories": [
            "cs.AI",
            "F.1.2; F.4.3; J.7"
        ]
    },
    {
        "title": "An Algorithm to Determine Peer-Reviewers",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen"
        ],
        "summary": "The peer-review process is the most widely accepted certification mechanism for officially accepting the written results of researchers within the scientific community. An essential component of peer-review is the identification of competent referees to review a submitted manuscript. This article presents an algorithm to automatically determine the most appropriate reviewers for a manuscript by way of a co-authorship network data structure and a relative-rank particle-swarm algorithm. This approach is novel in that it is not limited to a pre-selected set of referees, is computationally efficient, requires no human-intervention, and, in some instances, can automatically identify conflict of interest situations. A useful application of this algorithm would be to open commentary peer-review systems because it provides a weighting for each referee with respects to their expertise in the domain of a manuscript. The algorithm is validated using referee bid data from the 2005 Joint Conference on Digital Libraries.",
        "published": "2006-05-24T17:06:32Z",
        "link": "http://arxiv.org/abs/cs/0605112v2",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.DS",
            "H.3.7, H.3.3"
        ]
    },
    {
        "title": "An Internet-enabled technology to support Evolutionary Design",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "K. Ueda"
        ],
        "summary": "This paper discusses the systematic use of product feedback information to support life-cycle design approaches and provides guidelines for developing a design at both the product and the system levels. Design activities are surveyed in the light of the product life cycle, and the design information flow is interpreted from a semiotic perspective. The natural evolution of a design is considered, the notion of design expectations is introduced, and the importance of evaluation of these expectations in dynamic environments is argued. Possible strategies for reconciliation of the expectations and environmental factors are described. An Internet-enabled technology is proposed to monitor product functionality, usage, and operational environment and supply the designer with relevant information. A pilot study of assessing design expectations of a refrigerator is outlined, and conclusions are drawn.",
        "published": "2006-05-25T04:39:11Z",
        "link": "http://arxiv.org/abs/cs/0605119v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.AR",
            "cs.MA",
            "cs.NI"
        ]
    },
    {
        "title": "Understanding Design Fundamentals: How Synthesis and Analysis Drive   Creativity, Resulting in Emergence",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "S. Kitamura"
        ],
        "summary": "This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought - analogical and metaphorical reasoning - is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.",
        "published": "2006-05-25T11:35:39Z",
        "link": "http://arxiv.org/abs/cs/0605120v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "Communication of Social Agents and the Digital City - A Semiotic   Perspective",
        "authors": [
            "Victor V. Kryssanov",
            "Masayuki Okabe",
            "Koh Kakusho",
            "Michihiko Minoh"
        ],
        "summary": "This paper investigates the concept of digital city. First, a functional analysis of a digital city is made in the light of the modern study of urbanism; similarities between the virtual and urban constructions are pointed out. Next, a semiotic perspective on the subject matter is elaborated, and a terminological basis is introduced to treat a digital city as a self-organizing meaning-producing system intended to support social or spatial navigation. An explicit definition of a digital city is formulated. Finally, the proposed approach is discussed, conclusions are given, and future work is outlined.",
        "published": "2006-05-25T11:54:35Z",
        "link": "http://arxiv.org/abs/cs/0605121v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Classification of Ordinal Data",
        "authors": [
            "Jaime S. Cardoso"
        ],
        "summary": "Classification of ordinal data is one of the most important tasks of relation learning. In this thesis a novel framework for ordered classes is proposed. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Compared with a well-known approach using pairwise objects as training samples, the new algorithm has a reduced complexity and training time. A second novel model, the unimodal model, is also introduced and a parametric version is mapped into neural networks. Several case studies are presented to assert the validity of the proposed models.",
        "published": "2006-05-26T09:44:44Z",
        "link": "http://arxiv.org/abs/cs/0605123v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The meaning of manufacturing know-how",
        "authors": [
            "V. V. Kryssanov",
            "V. A. Abramov",
            "Y. Fukuda",
            "K. Konishi"
        ],
        "summary": "This paper investigates the phenomenon of manufacturing know-how. First, the abstract notion of knowledge is discussed, and a terminological basis is introduced to treat know-how as a kind of knowledge. Next, a brief survey of the recently reported works dealt with manufacturing know-how is presented, and an explicit definition of know-how is formulated. Finally, the problem of utilizing know-how with knowledge-based systems is analyzed, and some ideas useful for its solving are given.",
        "published": "2006-05-30T05:12:35Z",
        "link": "http://arxiv.org/abs/cs/0605138v1",
        "categories": [
            "cs.AI",
            "cs.CE"
        ]
    },
    {
        "title": "A Decision-Making Support System Based on Know-How",
        "authors": [
            "V. V. Kryssanov",
            "V. A. Abramov",
            "Y. Fukuda",
            "K. Konishi"
        ],
        "summary": "The research results described are concerned with: - developing a domain modeling method and tools to provide the design and implementation of decision-making support systems for computer integrated manufacturing; - building a decision-making support system based on know-how and its software environment. The research is funded by NEDO, Japan.",
        "published": "2006-06-02T03:06:07Z",
        "link": "http://arxiv.org/abs/cs/0606010v1",
        "categories": [
            "cs.CE",
            "cs.AI"
        ]
    },
    {
        "title": "Imagination as Holographic Processor for Text Animation",
        "authors": [
            "Vadim Astakhov",
            "Tamara Astakhova",
            "Brian Sanders"
        ],
        "summary": "Imagination is the critical point in developing of realistic artificial intelligence (AI) systems. One way to approach imagination would be simulation of its properties and operations. We developed two models: AI-Brain Network Hierarchy of Languages and Semantical Holographic Calculus as well as simulation system ScriptWriter that emulate the process of imagination through an automatic animation of English texts. The purpose of this paper is to demonstrate the model and to present ScriptWriter system http://nvo.sdsc.edu/NVO/JCSG/get_SRB_mime_file2.cgi//home/tamara.sdsc/test/demo.zip?F=/home/tamara.sdsc/test/demo.zip&M=application/x-gtar for simulation of the imagination.",
        "published": "2006-06-05T23:55:37Z",
        "link": "http://arxiv.org/abs/cs/0606020v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A simulation engine to support production scheduling using   genetics-based machine learning",
        "authors": [
            "H. Tamaki",
            "V. V. Kryssanov",
            "S. Kitamura"
        ],
        "summary": "The ever higher complexity of manufacturing systems, continually shortening life cycles of products and their increasing variety, as well as the unstable market situation of the recent years require introducing grater flexibility and responsiveness to manufacturing processes. From this perspective, one of the critical manufacturing tasks, which traditionally attract significant attention in both academia and the industry, but which have no satisfactory universal solution, is production scheduling. This paper proposes an approach based on genetics-based machine learning (GBML) to treat the problem of flow shop scheduling. By the approach, a set of scheduling rules is represented as an individual of genetic algorithms, and the fitness of the individual is estimated based on the makespan of the schedule generated by using the rule-set. A concept of the interactive software environment consisting of a simulator and a GBML simulation engine is introduced to support human decision-making during scheduling. A pilot study is underway to evaluate the performance of the GBML technique in comparison with other methods (such as Johnson's algorithm and simulated annealing) while completing test examples.",
        "published": "2006-06-06T09:30:58Z",
        "link": "http://arxiv.org/abs/cs/0606021v1",
        "categories": [
            "cs.CE",
            "cs.AI"
        ]
    },
    {
        "title": "Consecutive Support: Better Be Close!",
        "authors": [
            "Edgar de Graaf",
            "Jeannette de Graaf",
            "Walter A. Kosters"
        ],
        "summary": "We propose a new measure of support (the number of occur- rences of a pattern), in which instances are more important if they occur with a certain frequency and close after each other in the stream of trans- actions. We will explain this new consecutive support and discuss how patterns can be found faster by pruning the search space, for instance using so-called parent support recalculation. Both consecutiveness and the notion of hypercliques are incorporated into the Eclat algorithm. Synthetic examples show how interesting phenomena can now be discov- ered in the datasets. The new measure can be applied in many areas, ranging from bio-informatics to trade, supermarkets, and even law en- forcement. E.g., in bio-informatics it is important to find patterns con- tained in many individuals, where patterns close together in one chro- mosome are more significant.",
        "published": "2006-06-06T14:28:42Z",
        "link": "http://arxiv.org/abs/cs/0606024v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Building a logical model in the machining domain for CAPP expert systems",
        "authors": [
            "V. V. Kryssanov",
            "A. S. Kleshchev",
            "Y. Fukuda",
            "K. Konishi"
        ],
        "summary": "Recently, extensive efforts have been made on the application of expert system technique to solving the process planning task in the machining domain. This paper introduces a new formal method to design CAPP expert systems. The formal method is applied to provide a contour of the CAPP expert system building technology. Theoretical aspects of the formalism are described and illustrated by an example of know-how analysis. Flexible facilities to utilize multiple knowledge types and multiple planning strategies within one system are provided by the technology.",
        "published": "2006-06-07T03:46:23Z",
        "link": "http://arxiv.org/abs/cs/0606027v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.SE"
        ]
    },
    {
        "title": "Belief Calculus",
        "authors": [
            "Audun Josang"
        ],
        "summary": "In Dempster-Shafer belief theory, general beliefs are expressed as belief mass distribution functions over frames of discernment. In Subjective Logic beliefs are expressed as belief mass distribution functions over binary frames of discernment. Belief representations in Subjective Logic, which are called opinions, also contain a base rate parameter which express the a priori belief in the absence of evidence. Philosophically, beliefs are quantitative representations of evidence as perceived by humans or by other intelligent agents. The basic operators of classical probability calculus, such as addition and multiplication, can be applied to opinions, thereby making belief calculus practical. Through the equivalence between opinions and Beta probability density functions, this also provides a calculus for Beta probability density functions. This article explains the basic elements of belief calculus.",
        "published": "2006-06-07T14:32:55Z",
        "link": "http://arxiv.org/abs/cs/0606029v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Evolutionary Design: Philosophy, Theory, and Application Tactics",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "S. Kitamura"
        ],
        "summary": "Although it has contributed to remarkable improvements in some specific areas, attempts to develop a universal design theory are generally characterized by failure. This paper sketches arguments for a new approach to engineering design based on Semiotics - the science about signs. The approach is to combine different design theories over all the product life cycle stages into one coherent and traceable framework. Besides, it is to bring together the designer's and user's understandings of the notion of 'good product'. Building on the insight from natural sciences that complex systems always exhibit a self-organizing meaning-influential hierarchical dynamics, objective laws controlling product development are found through an examination of design as a semiosis process. These laws are then applied to support evolutionary design of products. An experiment validating some of the theoretical findings is outlined, and concluding remarks are given.",
        "published": "2006-06-09T08:00:37Z",
        "link": "http://arxiv.org/abs/cs/0606039v1",
        "categories": [
            "cs.CE",
            "cs.AI"
        ]
    },
    {
        "title": "The Cumulative Rule for Belief Fusion",
        "authors": [
            "Audun Josang"
        ],
        "summary": "The problem of combining beliefs in the Dempster-Shafer belief theory has attracted considerable attention over the last two decades. The classical Dempster's Rule has often been criticised, and many alternative rules for belief combination have been proposed in the literature. The consensus operator for combining beliefs has nice properties and produces more intuitive results than Dempster's rule, but has the limitation that it can only be applied to belief distribution functions on binary state spaces. In this paper we present a generalisation of the consensus operator that can be applied to Dirichlet belief functions on state spaces of arbitrary size. This rule, called the cumulative rule of belief combination, can be derived from classical statistical theory, and corresponds well with human intuition.",
        "published": "2006-06-14T11:36:06Z",
        "link": "http://arxiv.org/abs/cs/0606066v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Is there an Elegant Universal Theory of Prediction?",
        "authors": [
            "Shane Legg"
        ],
        "summary": "Solomonoff's inductive learning model is a powerful, universal and highly elegant theory of sequence prediction. Its critical flaw is that it is incomputable and thus cannot be used in practice. It is sometimes suggested that it may still be useful to help guide the development of very general and powerful theories of prediction which are computable. In this paper it is shown that although powerful algorithms exist, they are necessarily highly complex. This alone makes their theoretical analysis problematic, however it is further shown that beyond a moderate level of complexity the analysis runs into the deeper problem of Goedel incompleteness. This limits the power of mathematics to analyse and study prediction algorithms, and indeed intelligent systems in general.",
        "published": "2006-06-14T15:22:27Z",
        "link": "http://arxiv.org/abs/cs/0606070v1",
        "categories": [
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "New Millennium AI and the Convergence of History",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "Artificial Intelligence (AI) has recently become a real formal science: the new millennium brought the first mathematically sound, asymptotically optimal, universal problem solvers, providing a new, rigorous foundation for the previously largely heuristic field of General AI and embedded agents. At the same time there has been rapid progress in practical methods for learning true sequence-processing programs, as opposed to traditional methods limited to stationary pattern association. Here we will briefly review some of the new results, and speculate about future developments, pointing out that the time intervals between the most notable events in over 40,000 years or 2^9 lifetimes of human history have sped up exponentially, apparently converging to zero within the next few decades. Or is this impression just a by-product of the way humans allocate memory space to past events?",
        "published": "2006-06-19T09:13:43Z",
        "link": "http://arxiv.org/abs/cs/0606081v3",
        "categories": [
            "cs.AI",
            "I.2"
        ]
    },
    {
        "title": "The Completeness of Propositional Resolution: A Simple and   Constructive<br> Proof",
        "authors": [
            "Jean Gallier"
        ],
        "summary": "It is well known that the resolution method (for propositional logic) is complete. However, completeness proofs found in the literature use an argument by contradiction showing that if a set of clauses is unsatisfiable, then it must have a resolution refutation. As a consequence, none of these proofs actually gives an algorithm for producing a resolution refutation from an unsatisfiable set of clauses. In this note, we give a simple and constructive proof of the completeness of propositional resolution which consists of an algorithm together with a proof of its correctness.",
        "published": "2006-06-19T17:40:42Z",
        "link": "http://arxiv.org/abs/cs/0606084v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2"
        ]
    },
    {
        "title": "Evaluating Variable Length Markov Chain Models for Analysis of User Web   Navigation Sessions",
        "authors": [
            "Jose Borges",
            "Mark Levene"
        ],
        "summary": "Markov models have been widely used to represent and analyse user web navigation data. In previous work we have proposed a method to dynamically extend the order of a Markov chain model and a complimentary method for assessing the predictive power of such a variable length Markov chain. Herein, we review these two methods and propose a novel method for measuring the ability of a variable length Markov model to summarise user web navigation sessions up to a given length. While the summarisation ability of a model is important to enable the identification of user navigation patterns, the ability to make predictions is important in order to foresee the next link choice of a user after following a given trail so as, for example, to personalise a web site. We present an extensive experimental evaluation providing strong evidence that prediction accuracy increases linearly with summarisation ability.",
        "published": "2006-06-28T08:26:45Z",
        "link": "http://arxiv.org/abs/cs/0606115v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "May We Have Your Attention: Analysis of a Selective Attention Task",
        "authors": [
            "Eldan Goldenberg",
            "Jacob R. Garcowski",
            "Randall D. Beer"
        ],
        "summary": "In this paper we present a deeper analysis than has previously been carried out of a selective attention problem, and the evolution of continuous-time recurrent neural networks to solve it. We show that the task has a rich structure, and agents must solve a variety of subproblems to perform well. We consider the relationship between the complexity of an agent and the ease with which it can evolve behavior that generalizes well across subproblems, and demonstrate a shaping protocol that improves generalization.",
        "published": "2006-06-29T22:33:31Z",
        "link": "http://arxiv.org/abs/cs/0606126v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Belief Conditioning Rules (BCRs)",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "In this paper we propose a new family of Belief Conditioning Rules (BCRs) for belief revision. These rules are not directly related with the fusion of several sources of evidence but with the revision of a belief assignment available at a given time according to the new truth (i.e. conditioning constraint) one has about the space of solutions of the problem.",
        "published": "2006-07-02T14:54:54Z",
        "link": "http://arxiv.org/abs/cs/0607005v2",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "Database Querying under Changing Preferences",
        "authors": [
            "Jan Chomicki"
        ],
        "summary": "We present here a formal foundation for an iterative and incremental approach to constructing and evaluating preference queries. Our main focus is on query modification: a query transformation approach which works by revising the preference relation in the query. We provide a detailed analysis of the cases where the order-theoretic properties of the preference relation are preserved by the revision. We consider a number of different revision operators: union, prioritized and Pareto composition. We also formulate algebraic laws that enable incremental evaluation of preference queries. Finally, we consider two variations of the basic framework: finite restrictions of preference relations and weak-order extensions of strict partial order preference relations.",
        "published": "2006-07-05T18:37:12Z",
        "link": "http://arxiv.org/abs/cs/0607013v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "An Analysis of Arithmetic Constraints on Integer Intervals",
        "authors": [
            "Krzysztof R. Apt",
            "Peter Zoeteweij"
        ],
        "summary": "Arithmetic constraints on integer intervals are supported in many constraint programming systems. We study here a number of approaches to implement constraint propagation for these constraints. To describe them we introduce integer interval arithmetic. Each approach is explained using appropriate proof rules that reduce the variable domains. We compare these approaches using a set of benchmarks. For the most promising approach we provide results that characterize the effect of constraint propagation. This is a full version of our earlier paper, cs.PL/0403016.",
        "published": "2006-07-06T10:09:43Z",
        "link": "http://arxiv.org/abs/cs/0607016v2",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.2; D.3.3"
        ]
    },
    {
        "title": "The Minimal Cost Algorithm for Off-Line Diagnosability of Discrete Event   Systems",
        "authors": [
            "Zhujun Fan"
        ],
        "summary": "The failure diagnosis for {\\it discrete event systems} (DESs) has been given considerable attention in recent years. Both on-line and off-line diagnostics in the framework of DESs was first considered by Lin Feng in 1994, and particularly an algorithm for diagnosability of DESs was presented. Motivated by some existing problems to be overcome in previous work, in this paper, we investigate the minimal cost algorithm for diagnosability of DESs.   More specifically: (i) we give a generic method for judging a system's off-line diagnosability, and the complexity of this algorithm is polynomial-time; (ii) and in particular, we present an algorithm of how to search for the minimal set in all observable event sets, whereas the previous algorithm may find {\\it non-minimal} one.",
        "published": "2006-07-09T04:55:03Z",
        "link": "http://arxiv.org/abs/cs/0607037v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "B.1.2; B.2.3; F.1.1; I.2.8"
        ]
    },
    {
        "title": "Dealing with Metonymic Readings of Named Entities",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "The aim of this paper is to propose a method for tagging named entities (NE), using natural language processing techniques. Beyond their literal meaning, named entities are frequently subject to metonymy. We show the limits of current NE type hierarchies and detail a new proposal aiming at dynamically capturing the semantics of entities in context. This model can analyze complex linguistic phenomena like metonymy, which are known to be difficult for natural language processing but crucial for most applications. We present an implementation and some test using the French ESTER corpus and give significant results.",
        "published": "2006-07-11T19:14:49Z",
        "link": "http://arxiv.org/abs/cs/0607052v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Linguistically Grounded Models of Language Change",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "Questions related to the evolution of language have recently known an impressive increase of interest (Briscoe, 2002). This short paper aims at questioning the scientific status of these models and their relations to attested data. We show that one cannot directly model non-linguistic factors (exogenous factors) even if they play a crucial role in language evolution. We then examine the relation between linguistic models and attested language data, as well as their contribution to cognitive linguistics.",
        "published": "2006-07-11T19:17:13Z",
        "link": "http://arxiv.org/abs/cs/0607053v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Reasoning with Intervals on Granules",
        "authors": [
            "Sylviane Schwer"
        ],
        "summary": "The formalizations of periods of time inside a linear model of Time are usually based on the notion of intervals, that may contain or may not their endpoints. This is not enought when the periods are written in terms of coarse granularities with respect to the event taken into account. For instance, how to express the inter-war period in terms of a {\\em years} interval? This paper presents a new type of intervals, neither open, nor closed or open-closed and the extension of operations on intervals of this new type, in order to reduce the gap between the discourse related to temporal relationship and its translation into a discretized model of Time.",
        "published": "2006-07-12T09:16:11Z",
        "link": "http://arxiv.org/abs/cs/0607056v1",
        "categories": [
            "cs.AI",
            "cs.DM",
            "G.2.0"
        ]
    },
    {
        "title": "Decomposable Theories",
        "authors": [
            "Khalil Djelloul"
        ],
        "summary": "We present in this paper a general algorithm for solving first-order formulas in particular theories called \"decomposable theories\". First of all, using special quantifiers, we give a formal characterization of decomposable theories and show some of their properties. Then, we present a general algorithm for solving first-order formulas in any decomposable theory \"T\". The algorithm is given in the form of five rewriting rules. It transforms a first-order formula \"P\", which can possibly contain free variables, into a conjunction \"Q\" of solved formulas easily transformable into a Boolean combination of existentially quantified conjunctions of atomic formulas. In particular, if \"P\" has no free variables then \"Q\" is either the formula \"true\" or \"false\". The correctness of our algorithm proves the completeness of the decomposable theories.   Finally, we show that the theory \"Tr\" of finite or infinite trees is a decomposable theory and give some benchmarks realized by an implementation of our algorithm, solving formulas on two-partner games in \"Tr\" with more than 160 nested alternated quantifiers.",
        "published": "2006-07-13T14:46:44Z",
        "link": "http://arxiv.org/abs/cs/0607065v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Islands for SAT",
        "authors": [
            "H. Fang",
            "Y. Kilani",
            "J. H. M. Lee",
            "P. J. Stuckey"
        ],
        "summary": "In this note we introduce the notion of islands for restricting local search. We show how we can construct islands for CNF SAT problems, and how much search space can be eliminated by restricting search to the island.",
        "published": "2006-07-14T12:44:24Z",
        "link": "http://arxiv.org/abs/cs/0607071v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "`Plausibilities of plausibilities': an approach through circumstances",
        "authors": [
            "P. G. L. Porta Mana",
            "A. Månsson",
            "G. Björk"
        ],
        "summary": "Probability-like parameters appearing in some statistical models, and their prior distributions, are reinterpreted through the notion of `circumstance', a term which stands for any piece of knowledge that is useful in assigning a probability and that satisfies some additional logical properties. The idea, which can be traced to Laplace and Jaynes, is that the usual inferential reasonings about the probability-like parameters of a statistical model can be conceived as reasonings about equivalence classes of `circumstances' - viz., real or hypothetical pieces of knowledge, like e.g. physical hypotheses, that are useful in assigning a probability and satisfy some additional logical properties - that are uniquely indexed by the probability distributions they lead to.",
        "published": "2006-07-17T17:14:19Z",
        "link": "http://arxiv.org/abs/quant-ph/0607111v3",
        "categories": [
            "quant-ph",
            "cs.AI"
        ]
    },
    {
        "title": "Using Answer Set Programming in an Inference-Based approach to Natural   Language Semantics",
        "authors": [
            "Farid Nouioua",
            "Pascal Nicolas"
        ],
        "summary": "Using Answer Set Programming in an Inference-Based approach to Natural Language Semantics",
        "published": "2006-07-18T07:43:07Z",
        "link": "http://arxiv.org/abs/cs/0607088v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "About Norms and Causes",
        "authors": [
            "Daniel Kayser",
            "Farid Nouioua"
        ],
        "summary": "Knowing the norms of a domain is crucial, but there exist no repository of norms. We propose a method to extract them from texts: texts generally do not describe a norm, but rather how a state-of-affairs differs from it. Answers concerning the cause of the state-of-affairs described often reveal the implicit norm. We apply this idea to the domain of driving, and validate it by designing algorithms that identify, in a text, the \"basic\" norms to which it refers implicitly.",
        "published": "2006-07-18T07:46:07Z",
        "link": "http://arxiv.org/abs/cs/0607084v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Representing Knowledge about Norms",
        "authors": [
            "Daniel Kayser",
            "Farid Nouioua"
        ],
        "summary": "Norms are essential to extend inference: inferences based on norms are far richer than those based on logical implications. In the recent decades, much effort has been devoted to reason on a domain, once its norms are represented. How to extract and express those norms has received far less attention. Extraction is difficult: as the readers are supposed to know them, the norms of a domain are seldom made explicit. For one thing, extracting norms requires a language to represent them, and this is the topic of this paper. We apply this language to represent norms in the domain of driving, and show that it is adequate to reason on the causes of accidents, as described by car-crash reports.",
        "published": "2006-07-18T08:15:04Z",
        "link": "http://arxiv.org/abs/cs/0607086v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Expressing Implicit Semantic Relations without Supervision",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations. For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns <P1,...,Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y. For example, given X=ostrich and Y=bird, the two highest ranking output patterns are \"X is the largest Y\" and \"Y such as the X\". The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks. The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X:Y is the expected relational similarity between the given pair and typical pairs for Pi. The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs. On both tasks, the algorithm achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf.",
        "published": "2006-07-27T18:23:45Z",
        "link": "http://arxiv.org/abs/cs/0607120v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "A Foundation to Perception Computing, Logic and Automata",
        "authors": [
            "Mohamed A. Belal"
        ],
        "summary": "In this report, a novel approach to intelligence and learning is introduced, this approach is based on what we call 'perception logic'. Based on this logic, a computing mechanism and automata are introduced. Multi-resolution analysis of perceptual information is given, in which learning is accomplished in at most O(log(N))epochs, where N is the number of samples, and the convergence is guarnteed. This approach combines the favors of computational modeles in the sense that they are structured and mathematically well-defined, and the adaptivity of soft computing approaches, in addition to the continuity and real-time response of dynamical systems.",
        "published": "2006-07-30T10:44:48Z",
        "link": "http://arxiv.org/abs/cs/0607138v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.0; I.2.6"
        ]
    },
    {
        "title": "Target Type Tracking with PCR5 and Dempster's rules: A Comparative   Analysis",
        "authors": [
            "Jean Dezert",
            "Albena Tchamova",
            "Florentin Smarandache",
            "Pavlina Konstantinova"
        ],
        "summary": "In this paper we consider and analyze the behavior of two combinational rules for temporal (sequential) attribute data fusion for target type estimation. Our comparative analysis is based on Dempster's fusion rule proposed in Dempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution rule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We show through very simple scenario and Monte-Carlo simulation, how PCR5 allows a very efficient Target Type Tracking and reduces drastically the latency delay for correct Target Type decision with respect to Demspter's rule. For cases presenting some short Target Type switches, Demspter's rule is proved to be unable to detect the switches and thus to track correctly the Target Type changes. The approach proposed here is totally new, efficient and promising to be incorporated in real-time Generalized Data Association - Multi Target Tracking systems (GDA-MTT) and provides an important result on the behavior of PCR5 with respect to Dempster's rule. The MatLab source code is provided in",
        "published": "2006-07-31T15:32:44Z",
        "link": "http://arxiv.org/abs/cs/0607143v1",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "Fusion of qualitative beliefs using DSmT",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "This paper introduces the notion of qualitative belief assignment to model beliefs of human experts expressed in natural language (with linguistic labels). We show how qualitative beliefs can be efficiently combined using an extension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical quantitative reasoning to qualitative reasoning. We propose a new arithmetic on linguistic labels which allows a direct extension of classical DSm fusion rule or DSm Hybrid rules. An approximate qualitative PCR5 rule is also proposed jointly with a Qualitative Average Operator. We also show how crisp or interval mappings can be used to deal indirectly with linguistic labels. A very simple example is provided to illustrate our qualitative fusion rules.",
        "published": "2006-07-31T17:16:57Z",
        "link": "http://arxiv.org/abs/cs/0607147v2",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "An Introduction to the DSm Theory for the Combination of Paradoxical,   Uncertain, and Imprecise Sources of Information",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been, and still remains today, of primal importance for the development of reliable modern information systems involving artificial reasoning. In this introduction, we present a survey of our recent theory of plausible and paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the literature, developed for dealing with imprecise, uncertain and paradoxical sources of information. We focus our presentation here rather on the foundations of DSmT, and on the two important new rules of combination, than on browsing specific applications of DSmT available in literature. Several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach.",
        "published": "2006-08-01T15:31:13Z",
        "link": "http://arxiv.org/abs/cs/0608002v1",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "Towards \"Propagation = Logic + Control\"",
        "authors": [
            "Sebastian Brand",
            "Roland H. C. Yap"
        ],
        "summary": "Constraint propagation algorithms implement logical inference. For efficiency, it is essential to control whether and in what order basic inference steps are taken. We provide a high-level framework that clearly differentiates between information needed for controlling propagation versus that needed for the logical semantics of complex constraints composed from primitive ones. We argue for the appropriateness of our controlled propagation framework by showing that it captures the underlying principles of manually designed propagation algorithms, such as literal watching for unit clause propagation and the lexicographic ordering constraint. We provide an implementation and benchmark results that demonstrate the practicality and efficiency of our framework.",
        "published": "2006-08-03T02:41:20Z",
        "link": "http://arxiv.org/abs/cs/0608015v1",
        "categories": [
            "cs.PL",
            "cs.AI"
        ]
    },
    {
        "title": "Infinite Qualitative Simulations by Means of Constraint Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Sebastian Brand"
        ],
        "summary": "We introduce a constraint-based framework for studying infinite qualitative simulations concerned with contingencies such as time, space, shape, size, abstracted into a finite set of qualitative relations. To define the simulations, we combine constraints that formalize the background knowledge concerned with qualitative reasoning with appropriate inter-state constraints that are formulated using linear temporal logic. We implemented this approach in a constraint programming system by drawing on ideas from bounded model checking. The resulting system allows us to test and modify the problem specifications in a straightforward way and to combine various knowledge aspects.",
        "published": "2006-08-03T03:08:34Z",
        "link": "http://arxiv.org/abs/cs/0608017v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Relation Variables in Qualitative Spatial Reasoning",
        "authors": [
            "Sebastian Brand"
        ],
        "summary": "We study an alternative to the prevailing approach to modelling qualitative spatial reasoning (QSR) problems as constraint satisfaction problems. In the standard approach, a relation between objects is a constraint whereas in the alternative approach it is a variable. The relation-variable approach greatly simplifies integration and implementation of QSR. To substantiate this point, we discuss several QSR algorithms from the literature which in the relation-variable approach reduce to the customary constraint propagation algorithm enforcing generalised arc-consistency.",
        "published": "2006-08-03T03:24:24Z",
        "link": "http://arxiv.org/abs/cs/0608019v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Using Sets of Probability Measures to Represent Uncertainty",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "I explore the use of sets of probability measures as a representation of uncertainty.",
        "published": "2006-08-04T20:26:25Z",
        "link": "http://arxiv.org/abs/cs/0608028v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Cascade hash tables: a series of multilevel double hashing schemes with   O(1) worst case lookup time",
        "authors": [
            "Shaohua Li"
        ],
        "summary": "In this paper, the author proposes a series of multilevel double hashing schemes called cascade hash tables. They use several levels of hash tables. In each table, we use the common double hashing scheme. Higher level hash tables work as fail-safes of lower level hash tables. By this strategy, it could effectively reduce collisions in hash insertion. Thus it gains a constant worst case lookup time with a relatively high load factor(70%-85%) in random experiments. Different parameters of cascade hash tables are tested.",
        "published": "2006-08-07T15:22:30Z",
        "link": "http://arxiv.org/abs/cs/0608037v3",
        "categories": [
            "cs.DS",
            "cs.AI"
        ]
    },
    {
        "title": "Searching for Globally Optimal Functional Forms for Inter-Atomic   Potentials Using Parallel Tempering and Genetic Programming",
        "authors": [
            "A. Slepoy",
            "A. P. Thompson",
            "M. D. Peters"
        ],
        "summary": "We develop a Genetic Programming-based methodology that enables discovery of novel functional forms for classical inter-atomic force-fields, used in molecular dynamics simulations. Unlike previous efforts in the field, that fit only the parameters to the fixed functional forms, we instead use a novel algorithm to search the space of many possible functional forms. While a follow-on practical procedure will use experimental and {\\it ab inito} data to find an optimal functional form for a forcefield, we first validate the approach using a manufactured solution. This validation has the advantage of a well-defined metric of success. We manufactured a training set of atomic coordinate data with an associated set of global energies using the well-known Lennard-Jones inter-atomic potential. We performed an automatic functional form fitting procedure starting with a population of random functions, using a genetic programming functional formulation, and a parallel tempering Metropolis-based optimization algorithm. Our massively-parallel method independently discovered the Lennard-Jones function after searching for several hours on 100 processors and covering a miniscule portion of the configuration space. We find that the method is suitable for unsupervised discovery of functional forms for inter-atomic potentials/force-fields. We also find that our parallel tempering Metropolis-based approach significantly improves the optimization convergence time, and takes good advantage of the parallel cluster architecture.",
        "published": "2006-08-18T23:17:32Z",
        "link": "http://arxiv.org/abs/cs/0608078v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Intensional Models for the Theory of Types",
        "authors": [
            "Reinhard Muskens"
        ],
        "summary": "In this paper we define intensional models for the classical theory of types, thus arriving at an intensional type logic ITL. Intensional models generalize Henkin's general models and have a natural definition. As a class they do not validate the axiom of Extensionality. We give a cut-free sequent calculus for type theory and show completeness of this calculus with respect to the class of intensional models via a model existence theorem. After this we turn our attention to applications. Firstly, it is argued that, since ITL is truly intensional, it can be used to model ascriptions of propositional attitude without predicting logical omniscience. In order to illustrate this a small fragment of English is defined and provided with an ITL semantics. Secondly, it is shown that ITL models contain certain objects that can be identified with possible worlds. Essential elements of modal logic become available within classical type theory once the axiom of Extensionality is given up.",
        "published": "2006-08-23T10:33:20Z",
        "link": "http://arxiv.org/abs/math/0608571v1",
        "categories": [
            "math.LO",
            "cs.AI",
            "03B15"
        ]
    },
    {
        "title": "Automated verification of weak equivalence within the SMODELS system",
        "authors": [
            "Tomi Janhunen",
            "Emilia Oikarinen"
        ],
        "summary": "In answer set programming (ASP), a problem at hand is solved by (i) writing a logic program whose answer sets correspond to the solutions of the problem, and by (ii) computing the answer sets of the program using an answer set solver as a search engine. Typically, a programmer creates a series of gradually improving logic programs for a particular problem when optimizing program length and execution time on a particular solver. This leads the programmer to a meta-level problem of ensuring that the programs are equivalent, i.e., they give rise to the same answer sets. To ease answer set programming at methodological level, we propose a translation-based method for verifying the equivalence of logic programs. The basic idea is to translate logic programs P and Q under consideration into a single logic program EQT(P,Q) whose answer sets (if such exist) yield counter-examples to the equivalence of P and Q. The method is developed here in a slightly more general setting by taking the visibility of atoms properly into account when comparing answer sets. The translation-based approach presented in the paper has been implemented as a translator called lpeq that enables the verification of weak equivalence within the smodels system using the same search engine as for the search of models. Our experiments with lpeq and smodels suggest that establishing the equivalence of logic programs in this way is in certain cases much faster than naive cross-checking of answer sets.",
        "published": "2006-08-25T13:12:40Z",
        "link": "http://arxiv.org/abs/cs/0608099v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1; F.2.2"
        ]
    },
    {
        "title": "Logic programs with monotone abstract constraint atoms",
        "authors": [
            "V. W. Marek",
            "I. Niemela",
            "M. Truszczynski]"
        ],
        "summary": "We introduce and study logic programs whose clauses are built out of monotone constraint atoms. We show that the operational concept of the one-step provability operator generalizes to programs with monotone constraint atoms, but the generalization involves nondeterminism. Our main results demonstrate that our formalism is a common generalization of (1) normal logic programming with its semantics of models, supported models and stable models, (2) logic programming with weight atoms (lparse programs) with the semantics of stable models, as defined by Niemela, Simons and Soininen, and (3) of disjunctive logic programming with the possible-model semantics of Sakama and Inoue.",
        "published": "2006-08-25T18:24:18Z",
        "link": "http://arxiv.org/abs/cs/0608103v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; D.1.6"
        ]
    },
    {
        "title": "Undecidability of the unification and admissibility problems for modal   and description logics",
        "authors": [
            "Frank Wolter",
            "Michael Zakharyaschev"
        ],
        "summary": "We show that the unification problem `is there a substitution instance of a given formula that is provable in a given logic?' is undecidable for basic modal logics K and K4 extended with the universal modality. It follows that the admissibility problem for inference rules is undecidable for these logics as well. These are the first examples of standard decidable modal logics for which the unification and admissibility problems are undecidable. We also prove undecidability of the unification and admissibility problems for K and K4 with at least two modal operators and nominals (instead of the universal modality), thereby showing that these problems are undecidable for basic hybrid logics. Recently, unification has been introduced as an important reasoning service for description logics. The undecidability proof for K with nominals can be used to show the undecidability of unification for boolean description logics with nominals (such as ALCO and SHIQO). The undecidability proof for K with the universal modality can be used to show that the unification problem relative to role boxes is undecidable for Boolean description logic with transitive roles, inverse roles, and role hierarchies (such as SHI and SHIQ).",
        "published": "2006-09-11T11:41:35Z",
        "link": "http://arxiv.org/abs/cs/0609052v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Matrix Games, Linear Programming, and Linear Approximation",
        "authors": [
            "L. N. Vaserstein"
        ],
        "summary": "The following four classes of computational problems are equivalent: solving matrix games, solving linear programs, best $l^{\\infty}$ linear approximation, best $l^1$ linear approximation.",
        "published": "2006-09-12T00:10:48Z",
        "link": "http://arxiv.org/abs/cs/0609056v1",
        "categories": [
            "cs.GT",
            "cs.AI"
        ]
    },
    {
        "title": "A State-Based Regression Formulation for Domains with Sensing   Actions<br> and Incomplete Information",
        "authors": [
            "Le-Chi Tuan",
            "Chitta Baral",
            "Tran Cao Son"
        ],
        "summary": "We present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions. We consider binary domains and employ a three-valued characterization of domains with sensing actions to define the regression function. We prove the soundness and completeness of our regression formulation with respect to the definition of progression. More specifically, we show that (i) a plan obtained through regression for a planning problem is indeed a progression solution of that planning problem, and that (ii) for each plan found through progression, using regression one obtains that plan or an equivalent one.",
        "published": "2006-09-19T21:33:07Z",
        "link": "http://arxiv.org/abs/cs/0609111v2",
        "categories": [
            "cs.AI",
            "I.2.4; I.2.8"
        ]
    },
    {
        "title": "Verification, Validation and Integrity of Distributed and Interchanged   Rule Based Policies and Contracts in the Semantic Web",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Rule-based policy and contract systems have rarely been studied in terms of their software engineering properties. This is a serious omission, because in rule-based policy or contract representation languages rules are being used as a declarative programming language to formalize real-world decision logic and create IS production systems upon. This paper adopts an SE methodology from extreme programming, namely test driven development, and discusses how it can be adapted to verification, validation and integrity testing (V&V&I) of policy and contract specifications. Since, the test-driven approach focuses on the behavioral aspects and the drawn conclusions instead of the structure of the rule base and the causes of faults, it is independent of the complexity of the rule language and the system under test and thus much easier to use and understand for the rule engineer and the user.",
        "published": "2006-09-21T11:50:24Z",
        "link": "http://arxiv.org/abs/cs/0609119v2",
        "categories": [
            "cs.AI",
            "cs.SE",
            "K.6.3; I.2; I.2.4"
        ]
    },
    {
        "title": "Rule-based Knowledge Representation for Service Level Agreement",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Automated management and monitoring of service contracts like Service Level Agreements (SLAs) or higher-level policies is vital for efficient and reliable distributed service-oriented architectures (SOA) with high quality of ser-vice (QoS) levels. IT service provider need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technol-ogy. I propose a novel rule-based knowledge representation (KR) for SLA rules and a respective rule-based service level management (RBSLM) framework. My rule-based approach based on logic programming provides several advantages including automated rule chaining allowing for compact knowledge representation and high levels of automation as well as flexibility to adapt to rapidly changing business requirements. Therewith, I address an urgent need service-oriented busi-nesses do have nowadays which is to dynamically change their business and contractual logic in order to adapt to rapidly changing business environments and to overcome the restricting nature of slow change cycles.",
        "published": "2006-09-21T12:04:33Z",
        "link": "http://arxiv.org/abs/cs/0609120v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.LO",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "Semantic Description of Parameters in Web Service Annotations",
        "authors": [
            "Jochen Gruber"
        ],
        "summary": "A modification of OWL-S regarding parameter description is proposed. It is strictly based on Description Logic. In addition to class description of parameters it also allows the modelling of relations between parameters and the precise description of the size of data to be supplied to a service. In particular, it solves two major issues identified within current proposals for a Semantic Web Service annotation standard.",
        "published": "2006-09-24T17:29:49Z",
        "link": "http://arxiv.org/abs/cs/0609132v1",
        "categories": [
            "cs.AI",
            "F.3.0"
        ]
    },
    {
        "title": "An application-oriented terminology evaluation: the case of back-of-the   book indexes",
        "authors": [
            "Touria Aït El Mekki",
            "Adeline Nazarenko"
        ],
        "summary": "This paper addresses the problem of computational terminology evaluation not per se but in a specific application context. This paper describes the evaluation procedure that has been used to assess the validity of our overall indexing approach and the quality of the IndDoc indexing tool. Even if user-oriented extended evaluation is irreplaceable, we argue that early evaluations are possible and they are useful for development guidance.",
        "published": "2006-09-24T19:59:20Z",
        "link": "http://arxiv.org/abs/cs/0609133v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Using NLP to build the hypertextuel network of a back-of-the-book index",
        "authors": [
            "Touria Aït El Mekki",
            "Adeline Nazarenko"
        ],
        "summary": "Relying on the idea that back-of-the-book indexes are traditional devices for navigation through large documents, we have developed a method to build a hypertextual network that helps the navigation in a document. Building such an hypertextual network requires selecting a list of descriptors, identifying the relevant text segments to associate with each descriptor and finally ranking the descriptors and reference segments by relevance order. We propose a specific document segmentation method and a relevance measure for information ranking. The algorithms are tested on 4 corpora (of different types and domains) without human intervention or any semantic knowledge.",
        "published": "2006-09-24T20:00:58Z",
        "link": "http://arxiv.org/abs/cs/0609134v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Event-based Information Extraction for the biomedical domain: the   Caderige project",
        "authors": [
            "Erick Alphonse",
            "Sophie Aubin",
            "Philippe Bessières",
            "Gilles Bisson",
            "Thierry Hamon",
            "Sandrine Lagarrigue",
            "Adeline Nazarenko",
            "Alain-Pierre Manine",
            "Claire Nédellec",
            "Mohamed Ould Abdel Vetah",
            "Thierry Poibeau",
            "Davy Weissenbacher"
        ],
        "summary": "This paper gives an overview of the Caderige project. This project involves teams from different areas (biology, machine learning, natural language processing) in order to develop high-level analysis tools for extracting structured information from biological bibliographical databases, especially Medline. The paper gives an overview of the approach and compares it to the state of the art.",
        "published": "2006-09-24T20:03:06Z",
        "link": "http://arxiv.org/abs/cs/0609135v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "The ALVIS Format for Linguistically Annotated Documents",
        "authors": [
            "Adeline Nazarenko",
            "Erick Alphonse",
            "Julien Derivière",
            "Thierry Hamon",
            "Guillaume Vauvert",
            "Davy Weissenbacher"
        ],
        "summary": "The paper describes the ALVIS annotation format designed for the indexing of large collections of documents in topic-specific search engines. This paper is exemplified on the biological domain and on MedLine abstracts, as developing a specialized search engine for biologists is one of the ALVIS case studies. The ALVIS principle for linguistic annotations is based on existing works and standard propositions. We made the choice of stand-off annotations rather than inserted mark-up. Annotations are encoded as XML elements which form the linguistic subsection of the document record.",
        "published": "2006-09-24T20:04:01Z",
        "link": "http://arxiv.org/abs/cs/0609136v1",
        "categories": [
            "cs.AI",
            "H.3.1"
        ]
    },
    {
        "title": "Ontologies and Information Extraction",
        "authors": [
            "Claire Nédellec",
            "Adeline Nazarenko"
        ],
        "summary": "This report argues that, even in the simplest cases, IE is an ontology-driven process. It is not a mere text filtering method based on simple pattern matching and keywords, because the extracted pieces of texts are interpreted with respect to a predefined partial domain model. This report shows that depending on the nature and the depth of the interpretation to be done for extracting the information, more or less knowledge must be involved. This report is mainly illustrated in biology, a domain in which there are critical needs for content-based exploration of the scientific literature and which becomes a major application domain for IE.",
        "published": "2006-09-24T20:10:35Z",
        "link": "http://arxiv.org/abs/cs/0609137v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Modular self-organization",
        "authors": [
            "Bruno Scherrer"
        ],
        "summary": "The aim of this paper is to provide a sound framework for addressing a difficult problem: the automatic construction of an autonomous agent's modular architecture. We combine results from two apparently uncorrelated domains: Autonomous planning through Markov Decision Processes and a General Data Clustering Approach using a kernel-like method. Our fundamental idea is that the former is a good framework for addressing autonomy whereas the latter allows to tackle self-organizing problems.",
        "published": "2006-09-26T07:52:54Z",
        "link": "http://arxiv.org/abs/cs/0609142v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "ECA-LP / ECA-RuleML: A Homogeneous Event-Condition-Action Logic   Programming Language",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Event-driven reactive functionalities are an urgent need in nowadays distributed service-oriented applications and (Semantic) Web-based environments. An important problem to be addressed is how to correctly and efficiently capture and process the event-based behavioral, reactive logic represented as ECA rules in combination with other conditional decision logic which is represented as derivation rules. In this paper we elaborate on a homogeneous integration approach which combines derivation rules, reaction rules (ECA rules) and other rule types such as integrity constraint into the general framework of logic programming. The developed ECA-LP language provides expressive features such as ID-based updates with support for external and self-updates of the intensional and extensional knowledge, transac-tions including integrity testing and an event algebra to define and process complex events and actions based on a novel interval-based Event Calculus variant.",
        "published": "2006-09-26T14:36:47Z",
        "link": "http://arxiv.org/abs/cs/0609143v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SE",
            "I.2"
        ]
    },
    {
        "title": "Sensor Scheduling for Optimal Observability Using Estimation Entropy",
        "authors": [
            "Mohammad Rezaeian"
        ],
        "summary": "We consider sensor scheduling as the optimal observability problem for partially observable Markov decision processes (POMDP). This model fits to the cases where a Markov process is observed by a single sensor which needs to be dynamically adjusted or by a set of sensors which are selected one at a time in a way that maximizes the information acquisition from the process. Similar to conventional POMDP problems, in this model the control action is based on all past measurements; however here this action is not for the control of state process, which is autonomous, but it is for influencing the measurement of that process. This POMDP is a controlled version of the hidden Markov process, and we show that its optimal observability problem can be formulated as an average cost Markov decision process (MDP) scheduling problem. In this problem, a policy is a rule for selecting sensors or adjusting the measuring device based on the measurement history. Given a policy, we can evaluate the estimation entropy for the joint state-measurement processes which inversely measures the observability of state process for that policy. Considering estimation entropy as the cost of a policy, we show that the problem of finding optimal policy is equivalent to an average cost MDP scheduling problem where the cost function is the entropy function over the belief space. This allows the application of the policy iteration algorithm for finding the policy achieving minimum estimation entropy, thus optimum observability.",
        "published": "2006-09-29T11:38:53Z",
        "link": "http://arxiv.org/abs/cs/0609157v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "I.2.8; G.3; H.1.1"
        ]
    },
    {
        "title": "Rapport technique du projet OGRE",
        "authors": [
            "Gérard Bécher",
            "Patrice Enjalbert",
            "Estelle Fievé",
            "Laurent Gosselin",
            "François Lévy",
            "Gérard Ligozat"
        ],
        "summary": "This repport concerns automatic understanding of (french) iterative sentences, i.e. sentences where one single verb has to be interpreted by a more or less regular plurality of events. A linguistic analysis is proposed along an extension of Reichenbach's theory, several formal representations are considered and a corpus of 18000 newspaper extracts is described.",
        "published": "2006-10-01T17:39:44Z",
        "link": "http://arxiv.org/abs/cs/0610004v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "A Typed Hybrid Description Logic Programming Language with Polymorphic   Order-Sorted DL-Typed Unification for Semantic Web Type Systems",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "In this paper we elaborate on a specific application in the context of hybrid description logic programs (hybrid DLPs), namely description logic Semantic Web type systems (DL-types) which are used for term typing of LP rules based on a polymorphic, order-sorted, hybrid DL-typed unification as procedural semantics of hybrid DLPs. Using Semantic Web ontologies as type systems facilitates interchange of domain-independent rules over domain boundaries via dynamically typing and mapping of explicitly defined type ontologies.",
        "published": "2006-10-02T08:57:54Z",
        "link": "http://arxiv.org/abs/cs/0610006v2",
        "categories": [
            "cs.AI",
            "F.3; H.2; I.2; D.2"
        ]
    },
    {
        "title": "Why did the accident happen? A norm-based reasoning approach",
        "authors": [
            "Farid Nouioua"
        ],
        "summary": "In this paper we describe an architecture of a system that answer the question : Why did the accident happen? from the textual description of an accident. We present briefly the different parts of the architecture and then we describe with more detail the semantic part of the system i.e. the part in which the norm-based reasoning is performed on the explicit knowlege extracted from the text.",
        "published": "2006-10-04T11:32:22Z",
        "link": "http://arxiv.org/abs/cs/0610015v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Norm Based Causal Reasoning in Textual Corpus",
        "authors": [
            "Farid Nouioua"
        ],
        "summary": "Truth based entailments are not sufficient for a good comprehension of NL. In fact, it can not deduce implicit information necessary to understand a text. On the other hand, norm based entailments are able to reach this goal. This idea was behind the development of Frames (Minsky 75) and Scripts (Schank 77, Schank 79) in the 70's. But these theories are not formalized enough and their adaptation to new situations is far from being obvious. In this paper, we present a reasoning system which uses norms in a causal reasoning process in order to find the cause of an accident from a text describing it.",
        "published": "2006-10-04T11:33:02Z",
        "link": "http://arxiv.org/abs/cs/0610016v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Raisonnement stratifié à base de normes pour inférer les   causes dans un corpus textuel",
        "authors": [
            "Farid Nouioua"
        ],
        "summary": "To understand texts written in natural language (LN), we use our knowledge about the norms of the domain. Norms allow to infer more implicit information from the text. This kind of information can, in general, be defeasible, but it remains useful and acceptable while the text do not contradict it explicitly. In this paper we describe a non-monotonic reasoning system based on the norms of the car crash domain. The system infers the cause of an accident from its textual description. The cause of an accident is seen as the most specific norm which has been violated. The predicates and the rules of the system are stratified: organized on layers in order to obtain an efficient reasoning.",
        "published": "2006-10-04T13:09:48Z",
        "link": "http://arxiv.org/abs/cs/0610018v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Une expérience de sémantique inférentielle",
        "authors": [
            "Farid Nouioua",
            "Daniel Kayser"
        ],
        "summary": "We develop a system which must be able to perform the same inferences that a human reader of an accident report can do and more particularly to determine the apparent causes of the accident. We describe the general framework in which we are situated, linguistic and semantic levels of the analysis and the inference rules used by the system.",
        "published": "2006-10-05T05:18:03Z",
        "link": "http://arxiv.org/abs/cs/0610023v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The Application of Fuzzy Logic to the Construction of the Ranking   Function of Information Retrieval Systems",
        "authors": [
            "Neil Rubens"
        ],
        "summary": "The quality of the ranking function is an important factor that determines the quality of the Information Retrieval system. Each document is assigned a score by the ranking function; the score indicates the likelihood of relevance of the document given a query. In the vector space model, the ranking function is defined by a mathematic expression. We propose a fuzzy logic (FL) approach to defining the ranking function. FL provides a convenient way of converting knowledge expressed in a natural language into fuzzy logic rules. The resulting ranking function could be easily viewed, extended, and verified: * if (tf is high) and (idf is high) > (relevance is high); * if (overlap is high) > (relevance is high). By using above FL rules, we are able to achieve performance approximately equal to the state of the art search engine Apache Lucene (deltaP10 +0.92%; deltaMAP -0.1%). The fuzzy logic approach allows combining the logic-based model with the vector model. The resulting model possesses simplicity and formalism of the logic based model, and the flexibility and performance of the vector model.",
        "published": "2006-10-08T05:34:12Z",
        "link": "http://arxiv.org/abs/cs/0610039v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Farthest-Point Heuristic based Initialization Methods for K-Modes   Clustering",
        "authors": [
            "Zengyou He"
        ],
        "summary": "The k-modes algorithm has become a popular technique in solving categorical data clustering problems in different application domains. However, the algorithm requires random selection of initial points for the clusters. Different initial points often lead to considerable distinct clustering results. In this paper we present an experimental study on applying a farthest-point heuristic based initialization method to k-modes clustering to improve its performance. Experiments show that new initialization method leads to better clustering accuracy than random selection initialization method for k-modes clustering.",
        "published": "2006-10-09T12:12:45Z",
        "link": "http://arxiv.org/abs/cs/0610043v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Comparing Typical Opening Move Choices Made by Humans and Chess Engines",
        "authors": [
            "Mark Levene",
            "Judit Bar-Ilan"
        ],
        "summary": "The opening book is an important component of a chess engine, and thus computer chess programmers have been developing automated methods to improve the quality of their books. For chess, which has a very rich opening theory, large databases of high-quality games can be used as the basis of an opening book, from which statistics relating to move choices from given positions can be collected. In order to find out whether the opening books used by modern chess engines in machine versus machine competitions are ``comparable'' to those used by chess players in human versus human competitions, we carried out analysis on 26 test positions using statistics from two opening books one compiled from humans' games and the other from machines' games. Our analysis using several nonparametric measures, shows that, overall, there is a strong association between humans' and machines' choices of opening moves when using a book to guide their choices.",
        "published": "2006-10-11T10:26:40Z",
        "link": "http://arxiv.org/abs/cs/0610060v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Language, logic and ontology: uncovering the structure of commonsense   knowledge",
        "authors": [
            "Walid S. Saba"
        ],
        "summary": "The purpose of this paper is twofold: (i) we argue that the structure of commonsense knowledge must be discovered, rather than invented; and (ii) we argue that natural language, which is the best known theory of our (shared) commonsense knowledge, should itself be used as a guide to discovering the structure of commonsense knowledge. In addition to suggesting a systematic method to the discovery of the structure of commonsense knowledge, the method we propose seems to also provide an explanation for a number of phenomena in natural language, such as metaphor, intensionality, and the semantics of nominal compounds. Admittedly, our ultimate goal is quite ambitious, and it is no less than the systematic 'discovery' of a well-typed ontology of commonsense knowledge, and the subsequent formulation of the long-awaited goal of a meaning algebra.",
        "published": "2006-10-11T14:04:28Z",
        "link": "http://arxiv.org/abs/cs/0610067v3",
        "categories": [
            "cs.AI",
            "math.LO"
        ]
    },
    {
        "title": "On Geometric Algebra representation of Binary Spatter Codes",
        "authors": [
            "Diederik Aerts",
            "Marek Czachor",
            "Bart De Moor"
        ],
        "summary": "Kanerva's Binary Spatter Codes are reformulated in terms of geometric algebra. The key ingredient of the construction is the representation of XOR binding in terms of geometric product.",
        "published": "2006-10-12T18:48:22Z",
        "link": "http://arxiv.org/abs/cs/0610075v2",
        "categories": [
            "cs.AI",
            "quant-ph"
        ]
    },
    {
        "title": "Semantic results for ontic and epistemic change",
        "authors": [
            "H. P. van Ditmarsch",
            "B. P. Kooi"
        ],
        "summary": "We give some semantic results for an epistemic logic incorporating dynamic operators to describe information changing events. Such events include epistemic changes, where agents become more informed about the non-changing state of the world, and ontic changes, wherein the world changes. The events are executed in information states that are modeled as pointed Kripke models. Our contribution consists of three semantic results. (i) Given two information states, there is an event transforming one into the other. The linguistic correspondent to this is that every consistent formula can be made true in every information state by the execution of an event. (ii) A more technical result is that: every event corresponds to an event in which the postconditions formalizing ontic change are assignments to `true' and `false' only (instead of assignments to arbitrary formulas in the logical language). `Corresponds' means that execution of either event in a given information state results in bisimilar information states. (iii) The third, also technical, result is that every event corresponds to a sequence of events wherein all postconditions are assignments of a single atom only (instead of simultaneous assignments of more than one atom).",
        "published": "2006-10-15T04:48:48Z",
        "link": "http://arxiv.org/abs/cs/0610093v4",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Solving planning domains with polytree causal graphs is NP-complete",
        "authors": [
            "Omer Giménez"
        ],
        "summary": "We show that solving planning domains on binary variables with polytree causal graph is \\NP-complete. This is in contrast to a polynomial-time algorithm of Domshlak and Brafman that solves these planning domains for polytree causal graphs of bounded indegree.",
        "published": "2006-10-16T06:18:44Z",
        "link": "http://arxiv.org/abs/cs/0610095v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.8"
        ]
    },
    {
        "title": "Self-organizing traffic lights: A realistic simulation",
        "authors": [
            "Seung-Bae Cools",
            "Carlos Gershenson",
            "Bart D'Hooghe"
        ],
        "summary": "We have previously shown in an abstract simulation (Gershenson, 2005) that self-organizing traffic lights can improve greatly traffic flow for any density. In this paper, we extend these results to a realistic setting, implementing self-organizing traffic lights in an advanced traffic simulator using real data from a Brussels avenue. On average, for different traffic densities, travel waiting times are reduced by 50% compared to the current green wave method.",
        "published": "2006-10-18T08:59:58Z",
        "link": "http://arxiv.org/abs/nlin/0610040v2",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.AI",
            "physics.comp-ph",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Local approximate inference algorithms",
        "authors": [
            "Kyomin Jung",
            "Devavrat Shah"
        ],
        "summary": "We present a new local approximation algorithm for computing Maximum a Posteriori (MAP) and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise Markov random field (MRF), say $G$. Our algorithm is based on decomposition of $G$ into {\\em appropriately} chosen small components; then computing estimates locally in each of these components and then producing a {\\em good} global solution. We show that if the underlying graph $G$ either excludes some finite-sized graph as its minor (e.g. Planar graph) or has low doubling dimension (e.g. any graph with {\\em geometry}), then our algorithm will produce solution for both questions within {\\em arbitrary accuracy}. We present a message-passing implementation of our algorithm for MAP computation using self-avoiding walk of graph. In order to evaluate the computational cost of this implementation, we derive novel tight bounds on the size of self-avoiding walk tree for arbitrary graph.   As a consequence of our algorithmic result, we show that the normalized log-partition function (also known as free-energy) for a class of {\\em regular} MRFs will converge to a limit, that is computable to an arbitrary accuracy.",
        "published": "2006-10-18T21:51:44Z",
        "link": "http://arxiv.org/abs/cs/0610111v3",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Constant for associative patterns ensemble",
        "authors": [
            "Leonid Makarov",
            "Peter Komarov"
        ],
        "summary": "Creation procedure of associative patterns ensemble in terms of formal logic with using neural net-work (NN) model is formulated. It is shown that the associative patterns set is created by means of unique procedure of NN work which having individual parameters of entrance stimulus transformation. It is ascer-tained that the quantity of the selected associative patterns possesses is a constant.",
        "published": "2006-10-24T16:27:09Z",
        "link": "http://arxiv.org/abs/cs/0610140v1",
        "categories": [
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Adaptation Knowledge Discovery from a Case Base",
        "authors": [
            "Mathieu D'Aquin",
            "Fadi Badra",
            "Sandrine Lafrogne",
            "Jean Lieber",
            "Amedeo Napoli",
            "Laszlo Szathmary"
        ],
        "summary": "In case-based reasoning, the adaptation step depends in general on domain-dependent knowledge, which motivates studies on adaptation knowledge acquisition (AKA). CABAMAKA is an AKA system based on principles of knowledge discovery from databases. This system explores the variations within the case base to elicit adaptation knowledge. It has been successfully tested in an application of case-based decision support to breast cancer treatment.",
        "published": "2006-10-27T10:08:32Z",
        "link": "http://arxiv.org/abs/cs/0610156v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Decentralized Failure Diagnosis of Stochastic Discrete Event Systems",
        "authors": [
            "Fuchun Liu",
            "Daowen Qiu",
            "Hongyan Xing",
            "Zhujun Fan"
        ],
        "summary": "Recently, the diagnosability of {\\it stochastic discrete event systems} (SDESs) was investigated in the literature, and, the failure diagnosis considered was {\\it centralized}. In this paper, we propose an approach to {\\it decentralized} failure diagnosis of SDESs, where the stochastic system uses multiple local diagnosers to detect failures and each local diagnoser possesses its own information. In a way, the centralized failure diagnosis of SDESs can be viewed as a special case of the decentralized failure diagnosis presented in this paper with only one projection. The main contributions are as follows: (1) We formalize the notion of codiagnosability for stochastic automata, which means that a failure can be detected by at least one local stochastic diagnoser within a finite delay. (2) We construct a codiagnoser from a given stochastic automaton with multiple projections, and the codiagnoser associated with the local diagnosers is used to test codiagnosability condition of SDESs. (3) We deal with a number of basic properties of the codiagnoser. In particular, a necessary and sufficient condition for the codiagnosability of SDESs is presented. (4) We give a computing method in detail to check whether codiagnosability is violated. And (5) some examples are described to illustrate the applications of the codiagnosability and its computing method.",
        "published": "2006-10-30T09:59:31Z",
        "link": "http://arxiv.org/abs/cs/0610165v1",
        "categories": [
            "cs.AI",
            "F.1.2; I.2.8; G.3; I.6.8"
        ]
    },
    {
        "title": "ECA-RuleML: An Approach combining ECA Rules with temporal interval-based   KR Event/Action Logics and Transactional Update Logics",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "An important problem to be addressed within Event-Driven Architecture (EDA) is how to correctly and efficiently capture and process the event/action-based logic. This paper endeavors to bridge the gap between the Knowledge Representation (KR) approaches based on durable events/actions and such formalisms as event calculus, on one hand, and event-condition-action (ECA) reaction rules extending the approach of active databases that view events as instantaneous occurrences and/or sequences of events, on the other. We propose formalism based on reaction rules (ECA rules) and a novel interval-based event logic and present concrete RuleML-based syntax, semantics and implementation. We further evaluate this approach theoretically, experimentally and on an example derived from common industry use cases and illustrate its benefits.",
        "published": "2006-10-30T11:56:08Z",
        "link": "http://arxiv.org/abs/cs/0610167v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.MA",
            "cs.SE",
            "I.2; H.2.4; I.2.5; I.2.4; K.6.3"
        ]
    },
    {
        "title": "Low-complexity modular policies: learning to play Pac-Man and a new   framework beyond MDPs",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "In this paper we propose a method that learns to play Pac-Man. We define a set of high-level observation and action modules. Actions are temporally extended, and multiple action modules may be in effect concurrently. A decision of the agent is represented as a rule-based policy. For learning, we apply the cross-entropy method, a recent global optimization algorithm. The learned policies reached better score than the hand-crafted policy, and neared the score of average human players. We argue that learning is successful mainly because (i) the policy space includes the combination of individual actions and thus it is sufficiently rich, (ii) the search is biased towards low-complexity policies and low complexity solutions can be found quickly if they exist. Based on these principles, we formulate a new theoretical framework, which can be found in the Appendix as supporting material.",
        "published": "2006-10-30T16:44:58Z",
        "link": "http://arxiv.org/abs/cs/0610170v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "DSmT: A new paradigm shift for information fusion",
        "authors": [
            "Jean Dezert",
            "Florentin Smarandache"
        ],
        "summary": "The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been and still remains of primal importance for the development of reliable information fusion systems. In this short survey paper, we present the theory of plausible and paradoxical reasoning, known as DSmT (Dezert-Smarandache Theory) in literature, developed for dealing with imprecise, uncertain and potentially highly conflicting sources of information. DSmT is a new paradigm shift for information fusion and recent publications have shown the interest and the potential ability of DSmT to solve fusion problems where Dempster's rule used in Dempster-Shafer Theory (DST) provides counter-intuitive results or fails to provide useful result at all. This paper is focused on the foundations of DSmT and on its main rules of combination (classic, hybrid and Proportional Conflict Redistribution rules). Shafer's model on which is based DST appears as a particular and specific case of DSm hybrid model which can be easily handled by DSmT as well. Several simple but illustrative examples are given throughout this paper to show the interest and the generality of this new theory.",
        "published": "2006-10-31T14:50:06Z",
        "link": "http://arxiv.org/abs/cs/0610175v1",
        "categories": [
            "cs.AI",
            "I.4.8"
        ]
    },
    {
        "title": "Efficient constraint propagation engines",
        "authors": [
            "Christian Schulte",
            "Peter J. Stuckey"
        ],
        "summary": "This paper presents a model and implementation techniques for speeding up constraint propagation. Three fundamental approaches to improving constraint propagation based on propagators as implementations of constraints are explored: keeping track of which propagators are at fixpoint, choosing which propagator to apply next, and how to combine several propagators for the same constraint. We show how idempotence reasoning and events help track fixpoints more accurately. We improve these methods by using them dynamically (taking into account current domains to improve accuracy). We define priority-based approaches to choosing a next propagator and show that dynamic priorities can improve propagation. We illustrate that the use of multiple propagators for the same constraint can be advantageous with priorities, and introduce staged propagators that combine the effects of multiple propagators with priorities for greater efficiency.",
        "published": "2006-11-02T09:55:07Z",
        "link": "http://arxiv.org/abs/cs/0611009v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.2; D.3.3"
        ]
    },
    {
        "title": "An associative memory for the on-line recognition and prediction of   temporal sequences",
        "authors": [
            "J. Bose",
            "S. B. Furber",
            "J. L. Shapiro"
        ],
        "summary": "This paper presents the design of an associative memory with feedback that is capable of on-line temporal sequence learning. A framework for on-line sequence learning has been proposed, and different sequence learning models have been analysed according to this framework. The network model is an associative memory with a separate store for the sequence context of a symbol. A sparse distributed memory is used to gain scalability. The context store combines the functionality of a neural layer with a shift register. The sensitivity of the machine to the sequence context is controllable, resulting in different characteristic behaviours. The model can store and predict on-line sequences of various types and length. Numerical simulations on the model have been carried out to determine its properties.",
        "published": "2006-11-05T01:15:01Z",
        "link": "http://arxiv.org/abs/cs/0611020v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Logical Approach to Efficient Max-SAT solving",
        "authors": [
            "Javier Larrosa",
            "Federico Heras",
            "Simon de Givry"
        ],
        "summary": "Weighted Max-SAT is the optimization version of SAT and many important problems can be naturally encoded as such. Solving weighted Max-SAT is an important problem from both a theoretical and a practical point of view. In recent years, there has been considerable interest in finding efficient solving techniques. Most of this work focus on the computation of good quality lower bounds to be used within a branch and bound DPLL-like algorithm. Most often, these lower bounds are described in a procedural way. Because of that, it is difficult to realize the {\\em logic} that is behind.   In this paper we introduce an original framework for Max-SAT that stresses the parallelism with classical SAT. Then, we extend the two basic SAT solving techniques: {\\em search} and {\\em inference}. We show that many algorithmic {\\em tricks} used in state-of-the-art Max-SAT solvers are easily expressable in {\\em logic} terms with our framework in a unified manner.   Besides, we introduce an original search algorithm that performs a restricted amount of {\\em weighted resolution} at each visited node. We empirically compare our algorithm with a variety of solving alternatives on several benchmarks. Our experiments, which constitute to the best of our knowledge the most comprehensive Max-sat evaluation ever reported, show that our algorithm is generally orders of magnitude faster than any competitor.",
        "published": "2006-11-06T12:39:05Z",
        "link": "http://arxiv.org/abs/cs/0611025v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Analytic Tableaux Calculi for KLM Logics of Nonmonotonic Reasoning",
        "authors": [
            "Laura Giordano",
            "Valentina Gliozzi",
            "Nicola Olivetti",
            "Gian Luca Pozzato"
        ],
        "summary": "We present tableau calculi for some logics of nonmonotonic reasoning, as defined by Kraus, Lehmann and Magidor. We give a tableau proof procedure for all KLM logics, namely preferential, loop-cumulative, cumulative and rational logics. Our calculi are obtained by introducing suitable modalities to interpret conditional assertions. We provide a decision procedure for the logics considered, and we study their complexity.",
        "published": "2006-11-10T16:49:33Z",
        "link": "http://arxiv.org/abs/cs/0611046v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "The Reaction RuleML Classification of the Event / Action / State   Processing and Reasoning Space",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Reaction RuleML is a general, practical, compact and user-friendly XML-serialized language for the family of reaction rules. In this white paper we give a review of the history of event / action /state processing and reaction rule approaches and systems in different domains, define basic concepts and give a classification of the event, action, state processing and reasoning space as well as a discussion of relevant / related work",
        "published": "2006-11-10T22:03:11Z",
        "link": "http://arxiv.org/abs/cs/0611047v1",
        "categories": [
            "cs.AI",
            "I.2; I.2.11; F.3; H.2.4"
        ]
    },
    {
        "title": "Evolutionary Optimization in an Algorithmic Setting",
        "authors": [
            "Mark Burgin",
            "Eugene Eberbach"
        ],
        "summary": "Evolutionary processes proved very useful for solving optimization problems. In this work, we build a formalization of the notion of cooperation and competition of multiple systems working toward a common optimization goal of the population using evolutionary computation techniques. It is justified that evolutionary algorithms are more expressive than conventional recursive algorithms. Three subclasses of evolutionary algorithms are proposed here: bounded finite, unbounded finite and infinite types. Some results on completeness, optimality and search decidability for the above classes are presented. A natural extension of Evolutionary Turing Machine model developed in this paper allows one to mathematically represent and study properties of cooperation and competition in a population of optimized species.",
        "published": "2006-11-16T03:27:16Z",
        "link": "http://arxiv.org/abs/cs/0611077v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Fuzzy Logic Classification of Imaging Laser Desorption Fourier Transform   Mass Spectrometry Data",
        "authors": [
            "Timothy R. McJunkin",
            "Jill R. Scott"
        ],
        "summary": "A fuzzy logic based classification engine has been developed for classifying mass spectra obtained with an imaging internal source Fourier transform mass spectrometer (I^2LD-FTMS). Traditionally, an operator uses the relative abundance of ions with specific mass-to-charge (m/z) ratios to categorize spectra. An operator does this by comparing the spectrum of m/z versus abundance of an unknown sample against a library of spectra from known samples. Automated positioning and acquisition allow I^2LD-FTMS to acquire data from very large grids, this would require classification of up to 3600 spectrum per hour to keep pace with the acquisition. The tedious job of classifying numerous spectra generated in an I^2LD-FTMS imaging application can be replaced by a fuzzy rule base if the cues an operator uses can be encapsulated. We present the translation of linguistic rules to a fuzzy classifier for mineral phases in basalt. This paper also describes a method for gathering statistics on ions, which are not currently used in the rule base, but which may be candidates for making the rule base more accurate and complete or to form new rule bases based on data obtained from known samples. A spatial method for classifying spectra with low membership values, based on neighboring sample classifications, is also presented.",
        "published": "2006-11-17T19:14:47Z",
        "link": "http://arxiv.org/abs/cs/0611085v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Learning and discrimination through STDP in a top-down modulated   associative memory",
        "authors": [
            "Anthony Mouraud",
            "Hélène Paugam-Moisy"
        ],
        "summary": "This article underlines the learning and discrimination capabilities of a model of associative memory based on artificial networks of spiking neurons. Inspired from neuropsychology and neurobiology, the model implements top-down modulations, as in neocortical layer V pyramidal neurons, with a learning rule based on synaptic plasticity (STDP), for performing a multimodal association learning task. A temporal correlation method of analysis proves the ability of the model to associate specific activity patterns to different samples of stimulation. Even in the absence of initial learning and with continuously varying weights, the activity patterns become stable enough for discrimination.",
        "published": "2006-11-21T12:54:29Z",
        "link": "http://arxiv.org/abs/cs/0611104v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Neutrosophic Description Logic",
        "authors": [
            "Haibin Wang",
            "Andre Rogatko",
            "Florentin Smarandache",
            "Rajshekhar Sunderraman"
        ],
        "summary": "Description Logics (DLs) are appropriate, widely used, logics for managing structured knowledge. They allow reasoning about individuals and concepts, i.e. set of individuals with common properties. Typically, DLs are limited to dealing with crisp, well defined concepts. That is, concepts for which the problem whether an individual is an instance of it is yes/no question. More often than not, the concepts encountered in the real world do not have a precisely defined criteria of membership: we may say that an individual is an instance of a concept only to a certain degree, depending on the individual's properties. The DLs that deal with such fuzzy concepts are called fuzzy DLs. In order to deal with fuzzy, incomplete, indeterminate and inconsistent concepts, we need to extend the fuzzy DLs, combining the neutrosophic logic with a classical DL. In particular, concepts become neutrosophic (here neutrosophic means fuzzy, incomplete, indeterminate, and inconsistent), thus reasoning about neutrosophic concepts is supported. We'll define its syntax, its semantics, and describe its properties.",
        "published": "2006-11-22T20:04:21Z",
        "link": "http://arxiv.org/abs/cs/0611118v2",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Knowledge Representation Concepts for Automated SLA Management",
        "authors": [
            "Adrian Paschke",
            "Martin Bichler"
        ],
        "summary": "Outsourcing of complex IT infrastructure to IT service providers has increased substantially during the past years. IT service providers must be able to fulfil their service-quality commitments based upon predefined Service Level Agreements (SLAs) with the service customer. They need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technology. The complexity of contractual logic in SLAs requires new forms of knowledge representation to automatically draw inferences and execute contractual agreements. A logic-based approach provides several advantages including automated rule chaining allowing for compact knowledge representation as well as flexibility to adapt to rapidly changing business requirements. We suggest adequate logical formalisms for representation and enforcement of SLA rules and describe a proof-of-concept implementation. The article describes selected formalisms of the ContractLog KR and their adequacy for automated SLA management and presents results of experiments to demonstrate flexibility and scalability of the approach.",
        "published": "2006-11-23T13:25:45Z",
        "link": "http://arxiv.org/abs/cs/0611122v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "I.2"
        ]
    },
    {
        "title": "Low-rank matrix factorization with attributes",
        "authors": [
            "Jacob Abernethy",
            "Francis Bach",
            "Theodoros Evgeniou",
            "Jean-Philippe Vert"
        ],
        "summary": "We develop a new collaborative filtering (CF) method that combines both previously known users' preferences, i.e. standard CF, as well as product/user attributes, i.e. classical function approximation, to predict a given user's interest in a particular product. Our method is a generalized low rank matrix completion problem, where we learn a function whose inputs are pairs of vectors -- the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix. We solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties. Benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people, as well as over standard multi-task or single task learning methods.",
        "published": "2006-11-24T08:49:30Z",
        "link": "http://arxiv.org/abs/cs/0611124v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Genetic Programming for Kernel-based Learning with Co-evolving Subsets   Selection",
        "authors": [
            "Christian Gagné",
            "Marc Schoenauer",
            "Michèle Sebag",
            "Marco Tomassini"
        ],
        "summary": "Support Vector Machines (SVMs) are well-established Machine Learning (ML) algorithms. They rely on the fact that i) linear learning can be formalized as a well-posed optimization problem; ii) non-linear learning can be brought into linear learning thanks to the kernel trick and the mapping of the initial search space onto a high dimensional feature space. The kernel is designed by the ML expert and it governs the efficiency of the SVM approach. In this paper, a new approach for the automatic design of kernels by Genetic Programming, called the Evolutionary Kernel Machine (EKM), is presented. EKM combines a well-founded fitness function inspired from the margin criterion, and a co-evolution framework ensuring the computational scalability of the approach. Empirical validation on standard ML benchmark demonstrates that EKM is competitive using state-of-the-art SVMs with tuned hyper-parameters.",
        "published": "2006-11-27T14:38:44Z",
        "link": "http://arxiv.org/abs/cs/0611135v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Functional Brain Imaging with Multi-Objective Multi-Modal Evolutionary   Optimization",
        "authors": [
            "Vojtech Krmicek",
            "Michèle Sebag"
        ],
        "summary": "Functional brain imaging is a source of spatio-temporal data mining problems. A new framework hybridizing multi-objective and multi-modal optimization is proposed to formalize these data mining problems, and addressed through Evolutionary Computation (EC). The merits of EC for spatio-temporal data mining are demonstrated as the approach facilitates the modelling of the experts' requirements, and flexibly accommodates their changing goals.",
        "published": "2006-11-28T00:54:43Z",
        "link": "http://arxiv.org/abs/cs/0611138v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "On the Benefits of Inoculation, an Example in Train Scheduling",
        "authors": [
            "Yann Semet",
            "Marc Schoenauer"
        ],
        "summary": "The local reconstruction of a railway schedule following a small perturbation of the traffic, seeking minimization of the total accumulated delay, is a very difficult and tightly constrained combinatorial problem. Notoriously enough, the railway company's public image degrades proportionally to the amount of daily delays, and the same goes for its profit! This paper describes an inoculation procedure which greatly enhances an evolutionary algorithm for train re-scheduling. The procedure consists in building the initial population around a pre-computed solution based on problem-related information available beforehand. The optimization is performed by adapting times of departure and arrival, as well as allocation of tracks, for each train at each station. This is achieved by a permutation-based evolutionary algorithm that relies on a semi-greedy heuristic scheduler to gradually reconstruct the schedule by inserting trains one after another. Experimental results are presented on various instances of a large real-world case involving around 500 trains and more than 1 million constraints. In terms of competition with commercial math ematical programming tool ILOG CPLEX, it appears that within a large class of instances, excluding trivial instances as well as too difficult ones, and with very few exceptions, a clever initialization turns an encouraging failure into a clear-cut success auguring of substantial financial savings.",
        "published": "2006-11-28T12:44:43Z",
        "link": "http://arxiv.org/abs/cs/0611140v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "A Generic Global Constraint based on MDDs",
        "authors": [
            "Peter Tiedemann",
            "Henrik Reif Andersen",
            "Rasmus Pagh"
        ],
        "summary": "The paper suggests the use of Multi-Valued Decision Diagrams (MDDs) as the supporting data structure for a generic global constraint. We give an algorithm for maintaining generalized arc consistency (GAC) on this constraint that amortizes the cost of the GAC computation over a root-to-terminal path in the search tree. The technique used is an extension of the GAC algorithm for the regular language constraint on finite length input. Our approach adds support for skipped variables, maintains the reduced property of the MDD dynamically and provides domain entailment detection. Finally we also show how to adapt the approach to constraint types that are closely related to MDDs, such as AOMDDs and Case DAGs.",
        "published": "2006-11-28T14:23:23Z",
        "link": "http://arxiv.org/abs/cs/0611141v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Novel Bayesian Classifier using Copula Functions",
        "authors": [
            "Saket Sathe"
        ],
        "summary": "A useful method for representing Bayesian classifiers is through \\emph{discriminant functions}. Here, using copula functions, we propose a new model for discriminants. This model provides a rich and generalized class of decision boundaries. These decision boundaries significantly boost the classification accuracy especially for high dimensional feature spaces. We strengthen our analysis through simulation results.",
        "published": "2006-11-29T13:59:31Z",
        "link": "http://arxiv.org/abs/cs/0611150v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Remarks on Inheritance Systems",
        "authors": [
            "Karl Schlechta"
        ],
        "summary": "We try a conceptual analysis of inheritance diagrams, first in abstract terms, and then compare to \"normality\" and the \"small/big sets\" of preferential and related reasoning. The main ideas are about nodes as truth values and information sources, truth comparison by paths, accessibility or relevance of information by paths, relative normality, and prototypical reasoning.",
        "published": "2006-11-30T09:26:12Z",
        "link": "http://arxiv.org/abs/math/0611937v4",
        "categories": [
            "math.LO",
            "cs.AI",
            "68T27, 68T30"
        ]
    },
    {
        "title": "Lossless fitness inheritance in genetic algorithms for decision trees",
        "authors": [
            "Dimitris Kalles",
            "Athanassios Papagelis"
        ],
        "summary": "When genetic algorithms are used to evolve decision trees, key tree quality parameters can be recursively computed and re-used across generations of partially similar decision trees. Simply storing instance indices at leaves is enough for fitness to be piecewise computed in a lossless fashion. We show the derivation of the (substantial) expected speed-up on two bounding case problems and trace the attractive property of lossless fitness inheritance to the divide-and-conquer nature of decision trees. The theoretical results are supported by experimental evidence.",
        "published": "2006-11-30T15:20:15Z",
        "link": "http://arxiv.org/abs/cs/0611166v2",
        "categories": [
            "cs.AI",
            "cs.DS",
            "cs.NE"
        ]
    },
    {
        "title": "On Measuring the Impact of Human Actions in the Machine Learning of a   Board Game's Playing Policies",
        "authors": [
            "Dimitris Kalles"
        ],
        "summary": "We investigate systematically the impact of human intervention in the training of computer players in a strategy board game. In that game, computer players utilise reinforcement learning with neural networks for evolving their playing strategies and demonstrate a slow learning speed. Human intervention can significantly enhance learning performance, but carry-ing it out systematically seems to be more of a problem of an integrated game development environment as opposed to automatic evolutionary learning.",
        "published": "2006-11-30T15:30:36Z",
        "link": "http://arxiv.org/abs/cs/0611163v1",
        "categories": [
            "cs.AI",
            "cs.GT",
            "cs.NE"
        ]
    },
    {
        "title": "Player co-modelling in a strategy board game: discovering how to play   fast",
        "authors": [
            "Dimitris Kalles"
        ],
        "summary": "In this paper we experiment with a 2-player strategy board game where playing models are evolved using reinforcement learning and neural networks. The models are evolved to speed up automatic game development based on human involvement at varying levels of sophistication and density when compared to fully autonomous playing. The experimental results suggest a clear and measurable association between the ability to win games and the ability to do that fast, while at the same time demonstrating that there is a minimum level of human involvement beyond which no learning really occurs.",
        "published": "2006-11-30T15:36:27Z",
        "link": "http://arxiv.org/abs/cs/0611164v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Reuse of designs: Desperately seeking an interdisciplinary cognitive   approach",
        "authors": [
            "Willemien Visser",
            "Brigitte Trousse"
        ],
        "summary": "This text analyses the papers accepted for the workshop \"Reuse of designs: an interdisciplinary cognitive approach\". Several dimensions and questions considered as important (by the authors and/or by us) are addressed: What about the \"interdisciplinary cognitive\" character of the approaches adopted by the authors? Is design indeed a domain where the use of CBR is particularly suitable? Are there important distinctions between CBR and other approaches? Which types of knowledge -other than cases- is being, or might be, used in CBR systems? With respect to cases: are there different \"types\" of case and different types of case use? which formats are adopted for their representation? do cases have \"components\"? how are cases organised in the case memory? Concerning their retrieval: which types of index are used? on which types of relation is retrieval based? how does one retrieve only a selected number of cases, i.e., how does one retrieve only the \"best\" cases? which processes and strategies are used, by the system and by its user? Finally, some important aspects of CBR system development are shortly discussed: should CBR systems be assistance or autonomous systems? how can case knowledge be \"acquired\"? what about the empirical evaluation of CBR systems? The conclusion points out some lacking points: not much attention is paid to the user, and few papers have indeed adopted an interdisciplinary cognitive approach.",
        "published": "2006-11-30T22:38:49Z",
        "link": "http://arxiv.org/abs/cs/0612002v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "Loop corrections for approximate inference",
        "authors": [
            "Joris Mooij",
            "Bert Kappen"
        ],
        "summary": "We propose a method for improving approximate inference methods that corrects for the influence of loops in the graphical model. The method is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It is an alternative implementation of an idea introduced recently by Montanari and Rizzo (2005). In its simplest form, which amounts to the assumption that no loops are present, the method reduces to the minimal Cluster Variation Method approximation (which uses maximal factors as outer clusters). On the other hand, using estimates of the effect of loops (obtained by some approximate inference algorithm) and applying the Loop Correcting (LC) method usually gives significantly better results than applying the approximate inference algorithm directly without loop corrections. Indeed, we often observe that the loop corrected error is approximately the square of the error of the approximate inference method used to estimate the effect of loops. We compare different variants of the Loop Correcting method with other approximate inference methods on a variety of graphical models, including \"real world\" networks, and conclude that the LC approach generally obtains the most accurate results.",
        "published": "2006-12-05T15:57:44Z",
        "link": "http://arxiv.org/abs/cs/0612030v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Social Networks and Social Information Filtering on Digg",
        "authors": [
            "Kristina Lerman"
        ],
        "summary": "The new social media sites -- blogs, wikis, Flickr and Digg, among others -- underscore the transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. Digg is a social news aggregator which allows users to submit links to, vote on and discuss news stories. Each day Digg selects a handful of stories to feature on its front page. Rather than rely on the opinion of a few editors, Digg aggregates opinions of thousands of its users to decide which stories to promote to the front page.   Digg users can designate other users as ``friends'' and easily track friends' activities: what new stories they submitted, commented on or read. The friends interface acts as a \\emph{social filtering} system, recommending to user stories his or her friends liked or found interesting. By tracking the votes received by newly submitted stories over time, we showed that social filtering is an effective information filtering approach. Specifically, we showed that (a) users tend to like stories submitted by friends and (b) users tend to like stories their friends read and liked. As a byproduct of social filtering, social networks also play a role in promoting stories to Digg's front page, potentially leading to ``tyranny of the minority'' situation where a disproportionate number of front page stories comes from the same small group of interconnected users. Despite this, social filtering is a promising new technology that can be used to personalize and tailor information to individual users: for example, through personal front pages.",
        "published": "2006-12-07T23:38:23Z",
        "link": "http://arxiv.org/abs/cs/0612046v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Social Browsing on Flickr",
        "authors": [
            "Kristina Lerman",
            "Laurie Jones"
        ],
        "summary": "The new social media sites - blogs, wikis, del.icio.us and Flickr, among others - underscore the transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. The photo-sharing site Flickr, for example, allows users to upload photographs, view photos created by others, comment on those photos, etc. As is common to other social media sites, Flickr allows users to designate others as ``contacts'' and to track their activities in real time. The contacts (or friends) lists form the social network backbone of social media sites. We claim that these social networks facilitate new ways of interacting with information, e.g., through what we call social browsing. The contacts interface on Flickr enables users to see latest images submitted by their friends. Through an extensive analysis of Flickr data, we show that social browsing through the contacts' photo streams is one of the primary methods by which users find new images on Flickr. This finding has implications for creating personalized recommendation systems based on the user's declared contacts lists.",
        "published": "2006-12-07T23:41:51Z",
        "link": "http://arxiv.org/abs/cs/0612047v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "Conscious Intelligent Systems - Part 1 : I X I",
        "authors": [
            "U. Gayathree"
        ],
        "summary": "Did natural consciousness and intelligent systems arise out of a path that was co-evolutionary to evolution? Can we explain human self-consciousness as having risen out of such an evolutionary path? If so how could it have been?   In this first part of a two-part paper (titled IXI), we take a learning system perspective to the problem of consciousness and intelligent systems, an approach that may look unseasonable in this age of fMRI's and high tech neuroscience.   We posit conscious intelligent systems in natural environments and wonder how natural factors influence their design paths. Such a perspective allows us to explain seamlessly a variety of natural factors, factors ranging from the rise and presence of the human mind, man's sense of I, his self-consciousness and his looping thought processes to factors like reproduction, incubation, extinction, sleep, the richness of natural behavior, etc. It even allows us to speculate on a possible human evolution scenario and other natural phenomena.",
        "published": "2006-12-09T17:18:20Z",
        "link": "http://arxiv.org/abs/cs/0612056v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Conscious Intelligent Systems - Part II - Mind, Thought, Language and   Understanding",
        "authors": [
            "U. Gayathree"
        ],
        "summary": "This is the second part of a paper on Conscious Intelligent Systems. We use the understanding gained in the first part (Conscious Intelligent Systems Part 1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the presence of mind affects understanding and intelligent systems; we see that the presence of mind necessitates language. The rise of language in turn has important effects on understanding. We discuss the humanoid question and how the question of self-consciousness (and by association mind/thought/language) would affect humanoids too.",
        "published": "2006-12-09T17:28:24Z",
        "link": "http://arxiv.org/abs/cs/0612057v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Interactive Configuration by Regular String Constraints",
        "authors": [
            "Esben Rune Hansen",
            "Henrik Reif Andersen"
        ],
        "summary": "A product configurator which is complete, backtrack free and able to compute the valid domains at any state of the configuration can be constructed by building a Binary Decision Diagram (BDD). Despite the fact that the size of the BDD is exponential in the number of variables in the worst case, BDDs have proved to work very well in practice. Current BDD-based techniques can only handle interactive configuration with small finite domains. In this paper we extend the approach to handle string variables constrained by regular expressions. The user is allowed to change the strings by adding letters at the end of the string. We show how to make a data structure that can perform fast valid domain computations given some assignment on the set of string variables.   We first show how to do this by using one large DFA. Since this approach is too space consuming to be of practical use, we construct a data structure that simulates the large DFA and in most practical cases are much more space efficient. As an example a configuration problem on $n$ string variables with only one solution in which each string variable is assigned to a value of length of $k$ the former structure will use $\\Omega(k^n)$ space whereas the latter only need $O(kn)$. We also show how this framework easily can be combined with the recent BDD techniques to allow both boolean, integer and string variables in the configuration problem.",
        "published": "2006-12-12T16:21:16Z",
        "link": "http://arxiv.org/abs/cs/0612068v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Approximation of the Two-Part MDL Code",
        "authors": [
            "Pieter Adriaans",
            "Paul Vitanyi"
        ],
        "summary": "Approximation of the optimal two-part MDL code for given data, through successive monotonically length-decreasing two-part MDL codes, has the following properties: (i) computation of each step may take arbitrarily long; (ii) we may not know when we reach the optimum, or whether we will reach the optimum at all; (iii) the sequence of models generated may not monotonically improve the goodness of fit; but (iv) the model associated with the optimum has (almost) the best goodness of fit. To express the practically interesting goodness of fit of individual models for individual data sets we have to rely on Kolmogorov complexity.",
        "published": "2006-12-19T16:18:30Z",
        "link": "http://arxiv.org/abs/cs/0612095v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "E.4; I.2.6; I.5"
        ]
    },
    {
        "title": "Sufficient Conditions for Coarse-Graining Evolutionary Dynamics",
        "authors": [
            "Keki Burjorjee"
        ],
        "summary": "It is commonly assumed that the ability to track the frequencies of a set of schemata in the evolving population of an infinite population genetic algorithm (IPGA) under different fitness functions will advance efforts to obtain a theory of adaptation for the simple GA. Unfortunately, for IPGAs with long genomes and non-trivial fitness functions there do not currently exist theoretical results that allow such a study. We develop a simple framework for analyzing the dynamics of an infinite population evolutionary algorithm (IPEA). This framework derives its simplicity from its abstract nature. In particular we make no commitment to the data-structure of the genomes, the kind of variation performed, or the number of parents involved in a variation operation. We use this framework to derive abstract conditions under which the dynamics of an IPEA can be coarse-grained. We then use this result to derive concrete conditions under which it becomes computationally feasible to closely approximate the frequencies of a family of schemata of relatively low order over multiple generations, even when the bitstsrings in the evolving population of the IPGA are long.",
        "published": "2006-12-21T02:18:17Z",
        "link": "http://arxiv.org/abs/cs/0612104v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; F.2"
        ]
    },
    {
        "title": "Truncating the loop series expansion for Belief Propagation",
        "authors": [
            "Vicenc Gomez",
            "J. M. Mooij",
            "H. J. Kappen"
        ],
        "summary": "Recently, M. Chertkov and V.Y. Chernyak derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the Belief Propagation solution. By adding correction terms to the BP free energy, one for each \"generalized loop\" in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce Truncated Loop Series BP (TLSBP), a particular way of truncating the loop series of M. Chertkov and V.Y. Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model, regular random graphs and on Promedas, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to significant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered.",
        "published": "2006-12-21T17:29:28Z",
        "link": "http://arxiv.org/abs/cs/0612109v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Genetic Programming, Validation Sets, and Parsimony Pressure",
        "authors": [
            "Christian Gagné",
            "Marc Schoenauer",
            "Marc Parizeau",
            "Marco Tomassini"
        ],
        "summary": "Fitness functions based on test cases are very common in Genetic Programming (GP). This process can be assimilated to a learning task, with the inference of models from a limited number of samples. This paper is an investigation on two methods to improve generalization in GP-based learning: 1) the selection of the best-of-run individuals using a three data sets methodology, and 2) the application of parsimony pressure in order to reduce the complexity of the solutions. Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced.",
        "published": "2006-01-11T15:39:16Z",
        "link": "http://arxiv.org/abs/cs/0601044v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Joint universal lossy coding and identification of i.i.d. vector sources",
        "authors": [
            "Maxim Raginsky"
        ],
        "summary": "The problem of joint universal source coding and modeling, addressed by Rissanen in the context of lossless codes, is generalized to fixed-rate lossy coding of continuous-alphabet memoryless sources. We show that, for bounded distortion measures, any compactly parametrized family of i.i.d. real vector sources with absolutely continuous marginals (satisfying appropriate smoothness and Vapnik--Chervonenkis learnability conditions) admits a joint scheme for universal lossy block coding and parameter estimation, and give nonasymptotic estimates of convergence rates for distortion redundancies and variational distances between the active source and the estimated source. We also present explicit examples of parametric sources admitting such joint universal compression and modeling schemes.",
        "published": "2006-01-17T00:08:05Z",
        "link": "http://arxiv.org/abs/cs/0601074v2",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Processing of Test Matrices with Guessing Correction",
        "authors": [
            "Kromer Victor"
        ],
        "summary": "It is suggested to insert into test matrix 1s for correct responses, 0s for response refusals, and negative corrective elements for incorrect responses. With the classical test theory approach test scores of examinees and items are calculated traditionally as sums of matrix elements, organized in rows and columns. Correlation coefficients are estimated using correction coefficients. In item response theory approach examinee and item logits are estimated using maximum likelihood method and probabilities of all matrix elements.",
        "published": "2006-01-20T05:40:44Z",
        "link": "http://arxiv.org/abs/cs/0601087v1",
        "categories": [
            "cs.LG",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Distributed Kernel Regression: An Algorithm for Training Collaboratively",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "This paper addresses the problem of distributed learning under communication constraints, motivated by distributed signal processing in wireless sensor networks and data mining with distributed databases. After formalizing a general model for distributed learning, an algorithm for collaboratively training regularized kernel least-squares regression estimators is derived. Noting that the algorithm can be viewed as an application of successive orthogonal projection algorithms, its convergence properties are investigated and the statistical behavior of the estimator is discussed in a simplified theoretical setting.",
        "published": "2006-01-20T17:46:45Z",
        "link": "http://arxiv.org/abs/cs/0601089v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Decision Making with Side Information and Unbounded Loss Functions",
        "authors": [
            "Majid Fozunbal",
            "Ton Kalker"
        ],
        "summary": "We consider the problem of decision-making with side information and unbounded loss functions. Inspired by probably approximately correct learning model, we use a slightly different model that incorporates the notion of side information in a more generic form to make it applicable to a broader class of applications including parameter estimation and system identification. We address sufficient conditions for consistent decision-making with exponential convergence behavior. In this regard, besides a certain condition on the growth function of the class of loss functions, it suffices that the class of loss functions be dominated by a measurable function whose exponential Orlicz expectation is uniformly bounded over the probabilistic model. Decay exponent, decay constant, and sample complexity are discussed. Example applications to method of moments, maximum likelihood estimation, and system identification are illustrated, as well.",
        "published": "2006-01-27T16:52:09Z",
        "link": "http://arxiv.org/abs/cs/0601115v2",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Nonlinear parametric model for Granger causality of time series",
        "authors": [
            "Daniele Marinazzo",
            "Mario Pellicoro",
            "Sebastiano Stramaglia"
        ],
        "summary": "We generalize a previously proposed approach for nonlinear Granger causality of time series, based on radial basis function. The proposed model is not constrained to be additive in variables from the two time series and can approximate any function of these variables, still being suitable to evaluate causality. Usefulness of this measure of causality is shown in a physiological example and in the study of the feed-back loop in a model of excitatory and inhibitory neurons.",
        "published": "2006-02-07T18:29:35Z",
        "link": "http://arxiv.org/abs/cond-mat/0602183v1",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.LG",
            "physics.med-ph",
            "q-bio.QM"
        ]
    },
    {
        "title": "How to Beat the Adaptive Multi-Armed Bandit",
        "authors": [
            "Varsha Dani",
            "Thomas P. Hayes"
        ],
        "summary": "The multi-armed bandit is a concise model for the problem of iterated decision-making under uncertainty. In each round, a gambler must pull one of $K$ arms of a slot machine, without any foreknowledge of their payouts, except that they are uniformly bounded. A standard objective is to minimize the gambler's regret, defined as the gambler's total payout minus the largest payout which would have been achieved by any fixed arm, in hindsight. Note that the gambler is only told the payout for the arm actually chosen, not for the unchosen arms.   Almost all previous work on this problem assumed the payouts to be non-adaptive, in the sense that the distribution of the payout of arm $j$ in round $i$ is completely independent of the choices made by the gambler on rounds $1, \\dots, i-1$. In the more general model of adaptive payouts, the payouts in round $i$ may depend arbitrarily on the history of past choices made by the algorithm.   We present a new algorithm for this problem, and prove nearly optimal guarantees for the regret against both non-adaptive and adaptive adversaries. After $T$ rounds, our algorithm has regret $O(\\sqrt{T})$ with high probability (the tail probability decays exponentially). This dependence on $T$ is best possible, and matches that of the full-information version of the problem, in which the gambler is told the payouts for all $K$ arms after each round.   Previously, even for non-adaptive payouts, the best high-probability bounds known were $O(T^{2/3})$, due to Auer, Cesa-Bianchi, Freund and Schapire. The expected regret of their algorithm is $O(T^{1/2}) for non-adaptive payouts, but as we show, $\\Omega(T^{2/3})$ for adaptive payouts.",
        "published": "2006-02-14T23:57:01Z",
        "link": "http://arxiv.org/abs/cs/0602053v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Learning rational stochastic languages",
        "authors": [
            "François Denis",
            "Yann Esposito",
            "Amaury Habrard"
        ],
        "summary": "Given a finite set of words w1,...,wn independently drawn according to a fixed unknown distribution law P called a stochastic language, an usual goal in Grammatical Inference is to infer an estimate of P in some class of probabilistic models, such as Probabilistic Automata (PA). Here, we study the class of rational stochastic languages, which consists in stochastic languages that can be generated by Multiplicity Automata (MA) and which strictly includes the class of stochastic languages generated by PA. Rational stochastic languages have minimal normal representation which may be very concise, and whose parameters can be efficiently estimated from stochastic samples. We design an efficient inference algorithm DEES which aims at building a minimal normal representation of the target. Despite the fact that no recursively enumerable class of MA computes exactly the set of rational stochastic languages over Q, we show that DEES strongly identifies tis set in the limit. We study the intermediary MA output by DEES and show that they compute rational series which converge absolutely to one and which can be used to provide stochastic languages which closely estimate the target.",
        "published": "2006-02-17T08:57:44Z",
        "link": "http://arxiv.org/abs/cs/0602062v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "MDL Convergence Speed for Bernoulli Sequences",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied. If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence speed. For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures. We show that this is even the case if the model class contains only Bernoulli distributions. We derive a new upper bound on the prediction error for countable Bernoulli classes. This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes. We discuss the application to Machine Learning tasks such as classification and hypothesis testing, and generalization to countable classes of i.i.d. models.",
        "published": "2006-02-22T16:29:05Z",
        "link": "http://arxiv.org/abs/math/0602505v1",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.PR",
            "stat.TH"
        ]
    },
    {
        "title": "Inconsistent parameter estimation in Markov random fields: Benefits in   the computation-limited setting",
        "authors": [
            "Martin J. Wainwright"
        ],
        "summary": "Consider the problem of joint parameter estimation and prediction in a Markov random field: i.e., the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the \\emph{same convex variational relaxation} is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the ``wrong'' model even in the infinite data limit) can be provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product.",
        "published": "2006-02-27T05:22:15Z",
        "link": "http://arxiv.org/abs/cs/0602092v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Rational stochastic languages",
        "authors": [
            "François Denis",
            "Yann Esposito"
        ],
        "summary": "The goal of the present paper is to provide a systematic and comprehensive study of rational stochastic languages over a semiring K \\in {Q, Q +, R, R+}. A rational stochastic language is a probability distribution over a free monoid \\Sigma^* which is rational over K, that is which can be generated by a multiplicity automata with parameters in K. We study the relations between the classes of rational stochastic languages S rat K (\\Sigma). We define the notion of residual of a stochastic language and we use it to investigate properties of several subclasses of rational stochastic languages. Lastly, we study the representation of rational stochastic languages by means of multiplicity automata.",
        "published": "2006-02-27T10:08:26Z",
        "link": "http://arxiv.org/abs/cs/0602093v1",
        "categories": [
            "cs.LG",
            "cs.CL"
        ]
    },
    {
        "title": "Metric State Space Reinforcement Learning for a Vision-Capable Mobile   Robot",
        "authors": [
            "Viktor Zhumatiy",
            "Faustino Gomez",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ],
        "summary": "We address the problem of autonomously learning controllers for vision-capable mobile robots. We extend McCallum's (1995) Nearest-Sequence Memory algorithm to allow for general metrics over state-action trajectories. We demonstrate the feasibility of our approach by successfully running our algorithm on a real mobile robot. The algorithm is novel and unique in that it (a) explores the environment and learns directly on a mobile robot without using a hand-made computer model as an intermediate step, (b) does not require manual discretization of the sensor input space, (c) works in piecewise continuous perceptual spaces, and (d) copes with partial observability. Together this allows learning from much less experience compared to previous methods.",
        "published": "2006-03-07T08:44:29Z",
        "link": "http://arxiv.org/abs/cs/0603023v1",
        "categories": [
            "cs.RO",
            "cs.LG"
        ]
    },
    {
        "title": "Topological Grammars for Data Approximation",
        "authors": [
            "A. N. Gorban",
            "N. R. Sumner",
            "A. Y. Zinovyev"
        ],
        "summary": "A method of {\\it topological grammars} is proposed for multidimensional data approximation. For data with complex topology we define a {\\it principal cubic complex} of low dimension and given complexity that gives the best approximation for the dataset. This complex is a generalization of linear and non-linear principal manifolds and includes them as particular cases. The problem of optimal principal complex construction is transformed into a series of minimization problems for quadratic functionals. These quadratic functionals have a physically transparent interpretation in terms of elastic energy. For the energy computation, the whole complex is represented as a system of nodes and springs. Topologically, the principal complex is a product of one-dimensional continuums (represented by graphs), and the grammars describe how these continuums transform during the process of optimal complex construction. This factorization of the whole process onto one-dimensional transformations using minimization of quadratic energy functionals allow us to construct efficient algorithms.",
        "published": "2006-03-22T22:52:23Z",
        "link": "http://arxiv.org/abs/cs/0603090v2",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Asymptotic Learnability of Reinforcement Problems with Arbitrary   Dependence",
        "authors": [
            "Daniil Ryabko",
            "Marcus Hutter"
        ],
        "summary": "We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions. The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments. We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class. We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions.",
        "published": "2006-03-28T16:22:42Z",
        "link": "http://arxiv.org/abs/cs/0603110v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Nearly optimal exploration-exploitation decision thresholds",
        "authors": [
            "Christos Dimitrakakis"
        ],
        "summary": "While in general trading off exploration and exploitation in reinforcement learning is hard, under some formulations relatively simple solutions exist. In this paper, we first derive upper bounds for the utility of selecting different actions in the multi-armed bandit setting. Unlike the common statistical upper confidence bounds, these explicitly link the planning horizon, uncertainty and the need for exploration explicit. The resulting algorithm can be seen as a generalisation of the classical Thompson sampling algorithm. We experimentally test these algorithms, as well as $\\epsilon$-greedy and the value of perfect information heuristics. Finally, we also introduce the idea of bagging for reinforcement learning. By employing a version of online bootstrapping, we can efficiently sample from an approximate posterior distribution.",
        "published": "2006-04-05T10:29:48Z",
        "link": "http://arxiv.org/abs/cs/0604010v2",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Semi-Supervised Learning -- A Statistical Physics Approach",
        "authors": [
            "Gad Getz",
            "Noam Shental",
            "Eytan Domany"
        ],
        "summary": "We present a novel approach to semi-supervised learning which is based on statistical physics. Most of the former work in the field of semi-supervised learning classifies the points by minimizing a certain energy function, which corresponds to a minimal k-way cut solution. In contrast to these methods, we estimate the distribution of classifications, instead of the sole minimal k-way cut, which yields more accurate and robust results. Our approach may be applied to all energy functions used for semi-supervised learning. The method is based on sampling using a Multicanonical Markov chain Monte-Carlo algorithm, and has a straightforward probabilistic interpretation, which allows for soft assignments of points to classes, and also to cope with yet unseen class types. The suggested approach is demonstrated on a toy data set and on two real-life data sets of gene expression.",
        "published": "2006-04-05T18:07:31Z",
        "link": "http://arxiv.org/abs/cs/0604011v2",
        "categories": [
            "cs.LG",
            "cond-mat.stat-mech",
            "cs.CV"
        ]
    },
    {
        "title": "Revealing the Autonomous System Taxonomy: The Machine Learning Approach",
        "authors": [
            "Xenofontas Dimitropoulos",
            "Dmitri Krioukov",
            "George Riley",
            "kc claffy"
        ],
        "summary": "Although the Internet AS-level topology has been extensively studied over the past few years, little is known about the details of the AS taxonomy. An AS \"node\" can represent a wide variety of organizations, e.g., large ISP, or small private business, university, with vastly different network characteristics, external connectivity patterns, network growth tendencies, and other properties that we can hardly neglect while working on veracious Internet representations in simulation environments. In this paper, we introduce a radically new approach based on machine learning techniques to map all the ASes in the Internet into a natural AS taxonomy. We successfully classify 95.3% of ASes with expected accuracy of 78.1%. We release to the community the AS-level topology dataset augmented with: 1) the AS taxonomy information and 2) the set of AS attributes we used to classify ASes. We believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the Internet.",
        "published": "2006-04-06T00:08:24Z",
        "link": "http://arxiv.org/abs/cs/0604015v1",
        "categories": [
            "cs.NI",
            "cs.LG"
        ]
    },
    {
        "title": "Generalization error bounds in semi-supervised classification under the   cluster assumption",
        "authors": [
            "Philippe Rigollet"
        ],
        "summary": "We consider semi-supervised classification when part of the available data is unlabeled. These unlabeled data can be useful for the classification problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known \"cluster assumption\" as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples.",
        "published": "2006-04-11T05:41:15Z",
        "link": "http://arxiv.org/abs/math/0604233v1",
        "categories": [
            "math.ST",
            "cs.LG",
            "stat.TH"
        ]
    },
    {
        "title": "Concerning the differentiability of the energy function in vector   quantization algorithms",
        "authors": [
            "Dominique Lepetz",
            "Max Nemoz-Gaillard",
            "Michael Aupetit"
        ],
        "summary": "The adaptation rule for Vector Quantization algorithms, and consequently the convergence of the generated sequence, depends on the existence and properties of a function called the energy function, defined on a topological manifold. Our aim is to investigate the conditions of existence of such a function for a class of algorithms examplified by the initial ''K-means'' and Kohonen algorithms. The results presented here supplement previous studies and show that the energy function is not always a potential but at least the uniform limit of a series of potential functions which we call a pseudo-potential. Our work also shows that a large number of existing vector quantization algorithms developped by the Artificial Neural Networks community fall into this category. The framework we define opens the way to study the convergence of all the corresponding adaptation rules at once, and a theorem gives promising insights in that direction. We also demonstrate that the ''K-means'' energy function is a pseudo-potential but not a potential in general. Consequently, the energy function associated to the ''Neural-Gas'' is not a potential in general.",
        "published": "2006-04-11T14:00:22Z",
        "link": "http://arxiv.org/abs/cs/0604046v1",
        "categories": [
            "cs.LG",
            "cs.NE"
        ]
    },
    {
        "title": "HCI and Educational Metrics as Tools for VLE Evaluation",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "The general set of HCI and Educational principles are considered and a classification system constructed. A frequency analysis of principles is used to obtain the most significant set. Metrics are devised to provide objective measures of these principles and a consistent testing regime devised. These principles are used to analyse Blackboard and Moodle.",
        "published": "2006-04-25T19:32:03Z",
        "link": "http://arxiv.org/abs/cs/0604102v1",
        "categories": [
            "cs.HC",
            "cs.LG"
        ]
    },
    {
        "title": "How accurate are the time delay estimates in gravitational lensing?",
        "authors": [
            "Juan C. Cuevas-Tello",
            "Peter Tino",
            "Somak Raychaudhury"
        ],
        "summary": "We present a novel approach to estimate the time delay between light curves of multiple images in a gravitationally lensed system, based on Kernel methods in the context of machine learning. We perform various experiments with artificially generated irregularly-sampled data sets to study the effect of the various levels of noise and the presence of gaps of various size in the monitoring data. We compare the performance of our method with various other popular methods of estimating the time delay and conclude, from experiments with artificial data, that our method is least vulnerable to missing data and irregular sampling, within reasonable bounds of Gaussian noise. Thereafter, we use our method to determine the time delays between the two images of quasar Q0957+561 from radio monitoring data at 4 cm and 6 cm, and conclude that if only the observations at epochs common to both wavelengths are used, the time delay gives consistent estimates, which can be combined to yield 408\\pm 12 days. The full 6 cm dataset, which covers a longer monitoring period, yields a value which is 10% larger, but this can be attributed to differences in sampling and missing data.",
        "published": "2006-05-01T20:42:03Z",
        "link": "http://arxiv.org/abs/astro-ph/0605042v1",
        "categories": [
            "astro-ph",
            "cs.LG"
        ]
    },
    {
        "title": "On the Foundations of Universal Sequence Prediction",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. We discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. We show that Solomonoff's model possesses many desirable properties: Fast convergence and strong bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments.",
        "published": "2006-05-03T07:47:21Z",
        "link": "http://arxiv.org/abs/cs/0605009v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "A Formal Measure of Machine Intelligence",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "summary": "A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this measure formally captures the concept of machine intelligence in the broadest reasonable sense.",
        "published": "2006-05-06T16:56:43Z",
        "link": "http://arxiv.org/abs/cs/0605024v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Query Chains: Learning to Rank from Implicit Feedback",
        "authors": [
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. We observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. Using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. We also implemented a real-world search engine to test our approach, using a modified ranking SVM to learn an improved ranking function from preference data. Our results demonstrate significant improvements in the ranking given by the search engine. The learned rankings outperform both a static ranking function, as well as one trained without considering query chains.",
        "published": "2006-05-08T22:05:24Z",
        "link": "http://arxiv.org/abs/cs/0605035v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "Evaluating the Robustness of Learning from Implicit Feedback",
        "authors": [
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "This paper evaluates the robustness of learning from implicit feedback in web search. In particular, we create a model of user behavior by drawing upon user studies in laboratory and real-world settings. The model is used to understand the effect of user behavior on the performance of a learning algorithm for ranked retrieval. We explore a wide range of possible user behaviors and find that learning from implicit feedback can be surprisingly robust. This complements previous results that demonstrated our algorithm's effectiveness in a real-world search engine application.",
        "published": "2006-05-08T23:38:13Z",
        "link": "http://arxiv.org/abs/cs/0605036v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "Minimally Invasive Randomization for Collecting Unbiased Preferences   from Clickthrough Logs",
        "authors": [
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "Clickthrough data is a particularly inexpensive and plentiful resource to obtain implicit relevance feedback for improving and personalizing search engines. However, it is well known that the probability of a user clicking on a result is strongly biased toward documents presented higher in the result set irrespective of relevance. We introduce a simple method to modify the presentation of search results that provably gives relevance judgments that are unaffected by presentation bias under reasonable assumptions. We validate this property of the training data in interactive real world experiments. Finally, we show that using these unbiased relevance judgments learning methods can be guaranteed to converge to an ideal ranking given sufficient data.",
        "published": "2006-05-09T01:53:22Z",
        "link": "http://arxiv.org/abs/cs/0605037v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.3"
        ]
    },
    {
        "title": "General Discounting versus Average Reward",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Consider an agent interacting with an environment in cycles. In every interaction cycle the agent is rewarded for its performance. We compare the average reward U from cycle 1 to m (average value) with the future discounted reward V from cycle k to infinity (discounted value). We consider essentially arbitrary (non-geometric) discount sequences and arbitrary reward sequences (non-MDP environments). We show that asymptotically U for m->infinity and V for k->infinity are equal, provided both limits exist. Further, if the effective horizon grows linearly with k or faster, then existence of the limit of U implies that the limit of V exists. Conversely, if the effective horizon grows linearly with k or slower, then existence of the limit of V implies that the limit of U exists.",
        "published": "2006-05-09T10:39:03Z",
        "link": "http://arxiv.org/abs/cs/0605040v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "On Learning Thresholds of Parities and Unions of Rectangles in Random   Walk Models",
        "authors": [
            "S. Roch"
        ],
        "summary": "In a recent breakthrough, [Bshouty et al., 2005] obtained the first passive-learning algorithm for DNFs under the uniform distribution. They showed that DNFs are learnable in the Random Walk and Noise Sensitivity models. We extend their results in several directions. We first show that thresholds of parities, a natural class encompassing DNFs, cannot be learned efficiently in the Noise Sensitivity model using only statistical queries. In contrast, we show that a cyclic version of the Random Walk model allows to learn efficiently polynomially weighted thresholds of parities. We also extend the algorithm of Bshouty et al. to the case of Unions of Rectangles, a natural generalization of DNFs to $\\{0,...,b-1\\}^n$.",
        "published": "2006-05-11T03:27:12Z",
        "link": "http://arxiv.org/abs/cs/0605048v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "Cross-Entropic Learning of a Machine for the Decision in a Partially   Observable Universe",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "Revision of the paper previously entitled \"Learning a Machine for the Decision in a Partially Observable Markov Universe\" In this paper, we are interested in optimal decisions in a partially observable universe. Our approach is to directly approximate an optimal strategic tree depending on the observation. This approximation is made by means of a parameterized probabilistic law. A particular family of hidden Markov models, with input \\emph{and} output, is considered as a model of policy. A method for optimizing the parameters of these HMMs is proposed and applied. This optimization is based on the cross-entropic principle for rare events simulation developed by Rubinstein.",
        "published": "2006-05-18T07:47:58Z",
        "link": "http://arxiv.org/abs/math/0605498v1",
        "categories": [
            "math.OC",
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Bayesian Regression of Piecewise Constant Functions",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "We derive an exact and efficient Bayesian regression algorithm for piecewise constant functions of unknown segment number, boundary location, and levels. It works for any noise and segment level prior, e.g. Cauchy which can handle outliers. We derive simple but good estimates for the in-segment variance. We also propose a Bayesian regression curve as a better way of smoothing data without blurring boundaries. The Bayesian approach also allows straightforward determination of the evidence, break probabilities and error estimates, useful for model selection and significance and robustness studies. We discuss the performance on synthetic and real-world examples. Many possible extensions will be discussed.",
        "published": "2006-06-13T17:05:02Z",
        "link": "http://arxiv.org/abs/math/0606315v1",
        "categories": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.TH"
        ]
    },
    {
        "title": "On Sequence Prediction for Arbitrary Measures",
        "authors": [
            "Daniil Ryabko",
            "Marcus Hutter"
        ],
        "summary": "Suppose we are given two probability measures on the set of one-way infinite finite-alphabet sequences and consider the question when one of the measures predicts the other, that is, when conditional probabilities converge (in a certain sense) when one of the measures is chosen to generate the sequence. This question may be considered a refinement of the problem of sequence prediction in its most general formulation: for a given class of probability measures, does there exist a measure which predicts all of the measures in the class? To address this problem, we find some conditions on local absolute continuity which are sufficient for prediction and which generalize several different notions which are known to be sufficient for prediction. We also formulate some open questions to outline a direction for finding the conditions on classes of measures for which prediction is possible.",
        "published": "2006-06-16T16:33:23Z",
        "link": "http://arxiv.org/abs/cs/0606077v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Predictions as statements and decisions",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "Prediction is a complex notion, and different predictors (such as people, computer programs, and probabilistic theories) can pursue very different goals. In this paper I will review some popular kinds of prediction and argue that the theory of competitive on-line learning can benefit from the kinds of prediction that are now foreign to it.",
        "published": "2006-06-22T04:31:51Z",
        "link": "http://arxiv.org/abs/cs/0606093v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "The generating function of the polytope of transport matrices $U(r,c)$   as a positive semidefinite kernel of the marginals $r$ and $c$",
        "authors": [
            "Marco Cuturi"
        ],
        "summary": "This paper has been withdrawn by the author due to a crucial error in the proof of Lemma 5.",
        "published": "2006-06-23T10:19:40Z",
        "link": "http://arxiv.org/abs/cs/0606100v4",
        "categories": [
            "cs.LG",
            "cs.DM"
        ]
    },
    {
        "title": "Entropy And Vision",
        "authors": [
            "Rami Kanhouche"
        ],
        "summary": "In vector quantization the number of vectors used to construct the codebook is always an undefined problem, there is always a compromise between the number of vectors and the quantity of information lost during the compression. In this text we present a minimum of Entropy principle that gives solution to this compromise and represents an Entropy point of view of signal compression in general. Also we present a new adaptive Object Quantization technique that is the same for the compression and the perception.",
        "published": "2006-06-26T13:03:11Z",
        "link": "http://arxiv.org/abs/math/0606643v3",
        "categories": [
            "math.PR",
            "cs.CV",
            "cs.DB",
            "cs.DM",
            "cs.LG",
            "math.CO",
            "I.2.10 Vision and Scene Understanding"
        ]
    },
    {
        "title": "PAC Classification based on PAC Estimates of Label Class Distributions",
        "authors": [
            "Nick Palmer",
            "Paul W. Goldberg"
        ],
        "summary": "A standard approach in pattern classification is to estimate the distributions of the label classes, and then to apply the Bayes classifier to the estimates of the distributions in order to classify unlabeled examples. As one might expect, the better our estimates of the label class distributions, the better the resulting classifier will be. In this paper we make this observation precise by identifying risk bounds of a classifier in terms of the quality of the estimates of the label class distributions. We show how PAC learnability relates to estimates of the distributions that have a PAC guarantee on their $L_1$ distance from the true distribution, and we bound the increase in negative log likelihood risk in terms of PAC bounds on the KL-divergence. We give an inefficient but general-purpose smoothing method for converting an estimated distribution that is good under the $L_1$ metric into a distribution that is good under the KL-divergence.",
        "published": "2006-07-11T13:52:39Z",
        "link": "http://arxiv.org/abs/cs/0607047v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Competing with stationary prediction strategies",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "In this paper we introduce the class of stationary prediction strategies and construct a prediction algorithm that asymptotically performs as well as the best continuous stationary strategy. We make mild compactness assumptions but no stochastic assumptions about the environment. In particular, no assumption of stationarity is made about the environment, and the stationarity of the considered strategies only means that they do not depend explicitly on time; we argue that it is natural to consider only stationary strategies even for highly non-stationary environments.",
        "published": "2006-07-13T15:52:04Z",
        "link": "http://arxiv.org/abs/cs/0607067v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Using Pseudo-Stochastic Rational Languages in Probabilistic Grammatical   Inference",
        "authors": [
            "Amaury Habrard",
            "Francois Denis",
            "Yann Esposito"
        ],
        "summary": "In probabilistic grammatical inference, a usual goal is to infer a good approximation of an unknown distribution P called a stochastic language. The estimate of P stands in some class of probabilistic models such as probabilistic automata (PA). In this paper, we focus on probabilistic models based on multiplicity automata (MA). The stochastic languages generated by MA are called rational stochastic languages; they strictly include stochastic languages generated by PA; they also admit a very concise canonical representation. Despite the fact that this class is not recursively enumerable, it is efficiently identifiable in the limit by using the algorithm DEES, introduced by the authors in a previous paper. However, the identification is not proper and before the convergence of the algorithm, DEES can produce MA that do not define stochastic languages. Nevertheless, it is possible to use these MA to define stochastic languages. We show that they belong to a broader class of rational series, that we call pseudo-stochastic rational languages. The aim of this paper is twofold. First we provide a theoretical study of pseudo-stochastic rational languages, the languages output by DEES, showing for example that this class is decidable within polynomial time. Second, we have carried out a lot of experiments in order to compare DEES to classical inference algorithms such as ALERGIA and MDI. They show that DEES outperforms them in most cases.",
        "published": "2006-07-18T07:21:51Z",
        "link": "http://arxiv.org/abs/cs/0607085v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Logical settings for concept learning from incomplete examples in First   Order Logic",
        "authors": [
            "Dominique Bouthinon",
            "Henry Soldano",
            "Véronique Ventos"
        ],
        "summary": "We investigate here concept learning from incomplete examples. Our first purpose is to discuss to what extent logical learning settings have to be modified in order to cope with data incompleteness. More precisely we are interested in extending the learning from interpretations setting introduced by L. De Raedt that extends to relational representations the classical propositional (or attribute-value) concept learning from examples framework. We are inspired here by ideas presented by H. Hirsh in a work extending the Version space inductive paradigm to incomplete data. H. Hirsh proposes to slightly modify the notion of solution when dealing with incomplete examples: a solution has to be a hypothesis compatible with all pieces of information concerning the examples. We identify two main classes of incompleteness. First, uncertainty deals with our state of knowledge concerning an example. Second, generalization (or abstraction) deals with what part of the description of the example is sufficient for the learning purpose. These two main sources of incompleteness can be mixed up when only part of the useful information is known. We discuss a general learning setting, referred to as \"learning from possibilities\" that formalizes these ideas, then we present a more specific learning setting, referred to as \"assumption-based learning\" that cope with examples which uncertainty can be reduced when considering contextual information outside of the proper description of the examples. Assumption-based learning is illustrated on a recent work concerning the prediction of a consensus secondary structure common to a set of RNA sequences.",
        "published": "2006-07-20T14:52:08Z",
        "link": "http://arxiv.org/abs/cs/0607096v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Theory of Probabilistic Boosting, Decision Trees and Matryoshki",
        "authors": [
            "Etienne Grossmann"
        ],
        "summary": "We present a theory of boosting probabilistic classifiers. We place ourselves in the situation of a user who only provides a stopping parameter and a probabilistic weak learner/classifier and compare three types of boosting algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of trees, which we call matryoshka. \"Nested tree,\" \"embedded tree\" and \"recursive tree\" are also appropriate names for this algorithm, which is one of our contributions. Our other contribution is the theoretical analysis of the algorithms, in which we give training error bounds. This analysis suggests that the matryoshka leverages probabilistic weak classifiers more efficiently than simple decision trees.",
        "published": "2006-07-25T15:57:56Z",
        "link": "http://arxiv.org/abs/cs/0607110v1",
        "categories": [
            "cs.LG",
            "I.5.1; I.2.6; G.3"
        ]
    },
    {
        "title": "Expressing Implicit Semantic Relations without Supervision",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations. For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns <P1,...,Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y. For example, given X=ostrich and Y=bird, the two highest ranking output patterns are \"X is the largest Y\" and \"Y such as the X\". The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks. The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X:Y is the expected relational similarity between the given pair and typical pairs for Pi. The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs. On both tasks, the algorithm achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf.",
        "published": "2006-07-27T18:23:45Z",
        "link": "http://arxiv.org/abs/cs/0607120v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Leading strategies in competitive on-line prediction",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "We start from a simple asymptotic result for the problem of on-line regression with the quadratic loss function: the class of continuous limited-memory prediction strategies admits a \"leading prediction strategy\", which not only asymptotically performs at least as well as any continuous limited-memory strategy but also satisfies the property that the excess loss of any continuous limited-memory strategy is determined by how closely it imitates the leading strategy. More specifically, for any class of prediction strategies constituting a reproducing kernel Hilbert space we construct a leading strategy, in the sense that the loss of any prediction strategy whose norm is not too large is determined by how closely it imitates the leading strategy. This result is extended to the loss functions given by Bregman divergences and by strictly proper scoring rules.",
        "published": "2006-07-27T22:11:07Z",
        "link": "http://arxiv.org/abs/cs/0607134v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Competing with Markov prediction strategies",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "Assuming that the loss function is convex in the prediction, we construct a prediction strategy universal for the class of Markov prediction strategies, not necessarily continuous. Allowing randomization, we remove the requirement of convexity.",
        "published": "2006-07-28T21:45:41Z",
        "link": "http://arxiv.org/abs/cs/0607136v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Foundation to Perception Computing, Logic and Automata",
        "authors": [
            "Mohamed A. Belal"
        ],
        "summary": "In this report, a novel approach to intelligence and learning is introduced, this approach is based on what we call 'perception logic'. Based on this logic, a computing mechanism and automata are introduced. Multi-resolution analysis of perceptual information is given, in which learning is accomplished in at most O(log(N))epochs, where N is the number of samples, and the convergence is guarnteed. This approach combines the favors of computational modeles in the sense that they are structured and mathematically well-defined, and the adaptivity of soft computing approaches, in addition to the continuity and real-time response of dynamical systems.",
        "published": "2006-07-30T10:44:48Z",
        "link": "http://arxiv.org/abs/cs/0607138v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.0; I.2.6"
        ]
    },
    {
        "title": "A Study on Learnability for Rigid Lambek Grammars",
        "authors": [
            "Roberto Bonato"
        ],
        "summary": "We present basic notions of Gold's \"learnability in the limit\" paradigm, first presented in 1967, a formalization of the cognitive process by which a native speaker gets to grasp the underlying grammar of his/her own native language by being exposed to well formed sentences generated by that grammar. Then we present Lambek grammars, a formalism issued from categorial grammars which, although not as expressive as needed for a full formalization of natural languages, is particularly suited to easily implement a natural interface between syntax and semantics. In the last part of this work, we present a learnability result for Rigid Lambek grammars from structured examples.",
        "published": "2006-08-06T16:10:05Z",
        "link": "http://arxiv.org/abs/cs/0608033v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Graph Laplacians and their convergence on random neighborhood graphs",
        "authors": [
            "Matthias Hein",
            "Jean-Yves Audibert",
            "Ulrike von Luxburg"
        ],
        "summary": "Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator.",
        "published": "2006-08-21T18:35:42Z",
        "link": "http://arxiv.org/abs/math/0608522v2",
        "categories": [
            "math.ST",
            "cs.LG",
            "stat.TH",
            "62H12 (Primary) 62H30, 62G99 (Secondary)"
        ]
    },
    {
        "title": "Similarity of Semantic Relations",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.",
        "published": "2006-08-25T14:35:11Z",
        "link": "http://arxiv.org/abs/cs/0608100v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Occam's hammer: a link between randomized learning and multiple testing   FDR control",
        "authors": [
            "Gilles Blanchard",
            "François Fleuret"
        ],
        "summary": "We establish a generic theoretical tool to construct probabilistic bounds for algorithms where the output is a subset of objects from an initial pool of candidates (or more generally, a probability distribution on said pool). This general device, dubbed \"Occam's hammer'', acts as a meta layer when a probabilistic bound is already known on the objects of the pool taken individually, and aims at controlling the proportion of the objects in the set output not satisfying their individual bound. In this regard, it can be seen as a non-trivial generalization of the \"union bound with a prior'' (\"Occam's razor''), a familiar tool in learning theory. We give applications of this principle to randomized classifiers (providing an interesting alternative approach to PAC-Bayes bounds) and multiple testing (where it allows to retrieve exactly and extend the so-called Benjamini-Yekutieli testing procedure).",
        "published": "2006-08-29T12:35:53Z",
        "link": "http://arxiv.org/abs/math/0608713v1",
        "categories": [
            "math.ST",
            "cs.LG",
            "stat.TH"
        ]
    },
    {
        "title": "A Massive Local Rules Search Approach to the Classification Problem",
        "authors": [
            "Vladislav Malyshkin",
            "Ray Bakhramov",
            "Andrey Gorodetsky"
        ],
        "summary": "An approach to the classification problem of machine learning, based on building local classification rules, is developed. The local rules are considered as projections of the global classification rules to the event we want to classify. A massive global optimization algorithm is used for optimization of quality criterion. The algorithm, which has polynomial complexity in typical case, is used to find all high--quality local rules. The other distinctive feature of the algorithm is the integration of attributes levels selection (for ordered attributes) with rules searching and original conflicting rules resolution strategy. The algorithm is practical; it was tested on a number of data sets from UCI repository, and a comparison with the other predicting techniques is presented.",
        "published": "2006-09-03T21:30:03Z",
        "link": "http://arxiv.org/abs/cs/0609007v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Metric entropy in competitive on-line prediction",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "Competitive on-line prediction (also known as universal prediction of individual sequences) is a strand of learning theory avoiding making any stochastic assumptions about the way the observations are generated. The predictor's goal is to compete with a benchmark class of prediction rules, which is often a proper Banach function space. Metric entropy provides a unifying framework for competitive on-line prediction: the numerous known upper bounds on the metric entropy of various compact sets in function spaces readily imply bounds on the performance of on-line prediction strategies. This paper discusses strengths and limitations of the direct approach to competitive on-line prediction via metric entropy, including comparisons to other approaches.",
        "published": "2006-09-09T11:31:01Z",
        "link": "http://arxiv.org/abs/cs/0609045v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Scanning and Sequential Decision Making for Multi-Dimensional Data -   Part I: the Noiseless Case",
        "authors": [
            "Asaf Cohen",
            "Neri Merhav",
            "Tsachy Weissman"
        ],
        "summary": "We investigate the problem of scanning and prediction (\"scandiction\", for short) of multidimensional data arrays. This problem arises in several aspects of image and video processing, such as predictive coding, for example, where an image is compressed by coding the error sequence resulting from scandicting it. Thus, it is natural to ask what is the optimal method to scan and predict a given image, what is the resulting minimum prediction loss, and whether there exist specific scandiction schemes which are universal in some sense.   Specifically, we investigate the following problems: First, modeling the data array as a random field, we wish to examine whether there exists a scandiction scheme which is independent of the field's distribution, yet asymptotically achieves the same performance as if this distribution was known. This question is answered in the affirmative for the set of all spatially stationary random fields and under mild conditions on the loss function. We then discuss the scenario where a non-optimal scanning order is used, yet accompanied by an optimal predictor, and derive bounds on the excess loss compared to optimal scanning and prediction.   This paper is the first part of a two-part paper on sequential decision making for multi-dimensional data. It deals with clean, noiseless data arrays. The second part deals with noisy data arrays, namely, with the case where the decision maker observes only a noisy version of the data, yet it is judged with respect to the original, clean data.",
        "published": "2006-09-11T09:35:57Z",
        "link": "http://arxiv.org/abs/cs/0609049v2",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "H.1.1; I.2.6"
        ]
    },
    {
        "title": "A kernel method for canonical correlation analysis",
        "authors": [
            "Shotaro Akaho"
        ],
        "summary": "Canonical correlation analysis is a technique to extract common features from a pair of multivariate data. In complex situations, however, it does not extract useful features because of its linearity. On the other hand, kernel method used in support vector machine is an efficient approach to improve such a linear method. In this paper, we investigate the effectiveness of applying kernel method to canonical correlation analysis.",
        "published": "2006-09-13T03:44:08Z",
        "link": "http://arxiv.org/abs/cs/0609071v2",
        "categories": [
            "cs.LG",
            "cs.CV"
        ]
    },
    {
        "title": "Cross-Entropy method: convergence issues for extended implementation",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "The cross-entropy method (CE) developed by R. Rubinstein is an elegant practical principle for simulating rare events. The method approximates the probability of the rare event by means of a family of probabilistic models. The method has been extended to optimization, by considering an optimal event as a rare event. CE works rather good when dealing with deterministic function optimization. Now, it appears that two conditions are needed for a good convergence of the method. First, it is necessary to have a family of models sufficiently flexible for discriminating the optimal events. Indirectly, it appears also that the function to be optimized should be deterministic. The purpose of this paper is to consider the case of partially discriminating model family, and of stochastic functions. It will be shown on simple examples that the CE could fail when relaxing these hypotheses. Alternative improvements of the CE method are investigated and compared on random examples in order to handle this issue.",
        "published": "2006-09-16T07:00:36Z",
        "link": "http://arxiv.org/abs/math/0609461v1",
        "categories": [
            "math.OC",
            "cs.LG",
            "cs.NE",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "PAC Learning Mixtures of Axis-Aligned Gaussians with No Separation   Assumption",
        "authors": [
            "Jon Feldman",
            "Ryan O'Donnell",
            "Rocco A. Servedio"
        ],
        "summary": "We propose and analyze a new vantage point for the learning of mixtures of Gaussians: namely, the PAC-style model of learning probability distributions introduced by Kearns et al. Here the task is to construct a hypothesis mixture of Gaussians that is statistically indistinguishable from the actual mixture generating the data; specifically, the KL-divergence should be at most epsilon.   In this scenario, we give a poly(n/epsilon)-time algorithm that learns the class of mixtures of any constant number of axis-aligned Gaussians in n-dimensional Euclidean space. Our algorithm makes no assumptions about the separation between the means of the Gaussians, nor does it have any dependence on the minimum mixing weight. This is in contrast to learning results known in the ``clustering'' model, where such assumptions are unavoidable.   Our algorithm relies on the method of moments, and a subalgorithm developed in previous work by the authors (FOCS 2005) for a discrete mixture-learning problem.",
        "published": "2006-09-16T14:43:27Z",
        "link": "http://arxiv.org/abs/cs/0609093v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Motion Primitives for Robotic Flight Control",
        "authors": [
            "Baris E. Perk",
            "J. J. E. Slotine"
        ],
        "summary": "We introduce a simple framework for learning aggressive maneuvers in flight control of UAVs. Having inspired from biological environment, dynamic movement primitives are analyzed and extended using nonlinear contraction theory. Accordingly, primitives of an observed movement are stably combined and concatenated. We demonstrate our results experimentally on the Quanser Helicopter, in which we first imitate aggressive maneuvers and then use them as primitives to achieve new maneuvers that can fly over an obstacle.",
        "published": "2006-09-25T19:06:59Z",
        "link": "http://arxiv.org/abs/cs/0609140v2",
        "categories": [
            "cs.RO",
            "cs.LG"
        ]
    },
    {
        "title": "Mining Generalized Graph Patterns based on User Examples",
        "authors": [
            "Pavel Dmitriev",
            "Carl Lagoze"
        ],
        "summary": "There has been a lot of recent interest in mining patterns from graphs. Often, the exact structure of the patterns of interest is not known. This happens, for example, when molecular structures are mined to discover fragments useful as features in chemical compound classification task, or when web sites are mined to discover sets of web pages representing logical documents. Such patterns are often generated from a few small subgraphs (cores), according to certain generalization rules (GRs). We call such patterns \"generalized patterns\"(GPs). While being structurally different, GPs often perform the same function in the network. Previously proposed approaches to mining GPs either assumed that the cores and the GRs are given, or that all interesting GPs are frequent. These are strong assumptions, which often do not hold in practical applications. In this paper, we propose an approach to mining GPs that is free from the above assumptions. Given a small number of GPs selected by the user, our algorithm discovers all GPs similar to the user examples. First, a machine learning-style approach is used to find the cores. Second, generalizations of the cores in the graph are computed to identify GPs. Evaluation on synthetic data, generated using real cores and GRs from biological and web domains, demonstrates effectiveness of our approach.",
        "published": "2006-09-27T18:42:44Z",
        "link": "http://arxiv.org/abs/cs/0609153v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "A kernel for time series based on global alignments",
        "authors": [
            "Marco Cuturi",
            "Jean-Philippe Vert",
            "Oystein Birkenes",
            "Tomoko Matsui"
        ],
        "summary": "We propose in this paper a new family of kernels to handle times series, notably speech data, within the framework of kernel methods which includes popular algorithms such as the Support Vector Machine. These kernels elaborate on the well known Dynamic Time Warping (DTW) family of distances by considering the same set of elementary operations, namely substitutions and repetitions of tokens, to map a sequence onto another. Associating to each of these operations a given score, DTW algorithms use dynamic programming techniques to compute an optimal sequence of operations with high overall score. In this paper we consider instead the score spanned by all possible alignments, take a smoothed version of their maximum and derive a kernel out of this formulation. We prove that this kernel is positive definite under favorable conditions and show how it can be tuned effectively for practical applications as we report encouraging results on a speech recognition task.",
        "published": "2006-10-06T04:45:32Z",
        "link": "http://arxiv.org/abs/cs/0610033v1",
        "categories": [
            "cs.CV",
            "cs.LG"
        ]
    },
    {
        "title": "Structural Inference of Hierarchies in Networks",
        "authors": [
            "Aaron Clauset",
            "Cristopher Moore",
            "M. E. J. Newman"
        ],
        "summary": "One property of networks that has received comparatively little attention is hierarchy, i.e., the property of having vertices that cluster together in groups, which then join to form groups of groups, and so forth, up through all levels of organization in the network. Here, we give a precise definition of hierarchical structure, give a generic model for generating arbitrary hierarchical structure in a random graph, and describe a statistically principled way to learn the set of hierarchical features that most plausibly explain a particular real-world network. By applying this approach to two example networks, we demonstrate its advantages for the interpretation of network data, the annotation of graphs with edge, vertex and community properties, and the generation of generic null models for further hypothesis testing.",
        "published": "2006-10-09T18:41:57Z",
        "link": "http://arxiv.org/abs/physics/0610051v1",
        "categories": [
            "physics.soc-ph",
            "cs.LG",
            "physics.data-an"
        ]
    },
    {
        "title": "Fitness Uniform Optimization",
        "authors": [
            "Marcus Hutter",
            "Shane Legg"
        ],
        "summary": "In evolutionary algorithms, the fitness of a population increases with time by mutating and recombining individuals and by a biased selection of more fit individuals. The right selection pressure is critical in ensuring sufficient optimization progress on the one hand and in preserving genetic diversity to be able to escape from local optima on the other hand. Motivated by a universal similarity relation on the individuals, we propose a new selection scheme, which is uniform in the fitness values. It generates selection pressure toward sparsely populated fitness regions, not necessarily toward higher fitness, as is the case for all other selection schemes. We show analytically on a simple example that the new selection scheme can be much more effective than standard selection schemes. We also propose a new deletion scheme which achieves a similar result via deletion and show how such a scheme preserves genetic diversity more effectively than standard approaches. We compare the performance of the new schemes to tournament selection and random deletion on an artificial deceptive problem and a range of NP-hard problems: traveling salesman, set covering and satisfiability.",
        "published": "2006-10-20T16:37:11Z",
        "link": "http://arxiv.org/abs/cs/0610126v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Metric learning pairwise kernel for graph inference",
        "authors": [
            "Jean-Philippe Vert",
            "Jian Qiu",
            "William Stafford Noble"
        ],
        "summary": "Much recent work in bioinformatics has focused on the inference of various types of biological networks, representing gene regulation, metabolic processes, protein-protein interactions, etc. A common setting involves inferring network edges in a supervised fashion from a set of high-confidence edges, possibly characterized by multiple, heterogeneous data sets (protein sequence, gene expression, etc.). Here, we distinguish between two modes of inference in this setting: direct inference based upon similarities between nodes joined by an edge, and indirect inference based upon similarities between one pair of nodes and another pair of nodes. We propose a supervised approach for the direct case by translating it into a distance metric learning problem. A relaxation of the resulting convex optimization problem leads to the support vector machine (SVM) algorithm with a particular kernel for pairs, which we call the metric learning pairwise kernel (MLPK). We demonstrate, using several real biological networks, that this direct approach often improves upon the state-of-the-art SVM for indirect inference with the tensor product pairwise kernel.",
        "published": "2006-10-21T06:33:24Z",
        "link": "http://arxiv.org/abs/q-bio/0610040v1",
        "categories": [
            "q-bio.QM",
            "cs.LG"
        ]
    },
    {
        "title": "Nonlinear Estimators and Tail Bounds for Dimension Reduction in $l_1$   Using Cauchy Random Projections",
        "authors": [
            "Ping Li",
            "Trevor J. Hastie",
            "Kenneth W. Church"
        ],
        "summary": "For dimension reduction in $l_1$, the method of {\\em Cauchy random projections} multiplies the original data matrix $\\mathbf{A} \\in\\mathbb{R}^{n\\times D}$ with a random matrix $\\mathbf{R} \\in \\mathbb{R}^{D\\times k}$ ($k\\ll\\min(n,D)$) whose entries are i.i.d. samples of the standard Cauchy C(0,1). Because of the impossibility results, one can not hope to recover the pairwise $l_1$ distances in $\\mathbf{A}$ from $\\mathbf{B} = \\mathbf{AR} \\in \\mathbb{R}^{n\\times k}$, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computation, information retrieval, learning, and data mining.   We propose three types of nonlinear estimators: the bias-corrected sample median estimator, the bias-corrected geometric mean estimator, and the bias-corrected maximum likelihood estimator. The sample median estimator and the geometric mean estimator are asymptotically (as $k\\to \\infty$) equivalent but the latter is more accurate at small $k$. We derive explicit tail bounds for the geometric mean estimator and establish an analog of the Johnson-Lindenstrauss (JL) lemma for dimension reduction in $l_1$, which is weaker than the classical JL lemma for dimension reduction in $l_2$.   Asymptotically, both the sample median estimator and the geometric mean estimators are about 80% efficient compared to the maximum likelihood estimator (MLE). We analyze the moments of the MLE and propose approximating the distribution of the MLE by an inverse Gaussian.",
        "published": "2006-10-27T07:08:51Z",
        "link": "http://arxiv.org/abs/cs/0610155v1",
        "categories": [
            "cs.DS",
            "cs.IR",
            "cs.LG"
        ]
    },
    {
        "title": "Considering users' behaviours in improving the responses of an   information base",
        "authors": [
            "Babajide Afolabi",
            "Odile Thiery"
        ],
        "summary": "In this paper, our aim is to propose a model that helps in the efficient use of an information system by users, within the organization represented by the IS, in order to resolve their decisional problems. In other words we want to aid the user within an organization in obtaining the information that corresponds to his needs (informational needs that result from his decisional problems). This type of information system is what we refer to as economic intelligence system because of its support for economic intelligence processes of the organisation. Our assumption is that every EI process begins with the identification of the decisional problem which is translated into an informational need. This need is then translated into one or many information search problems (ISP). We also assumed that an ISP is expressed in terms of the user's expectations and that these expectations determine the activities or the behaviors of the user, when he/she uses an IS. The model we are proposing is used for the conception of the IS so that the process of retrieving of solution(s) or the responses given by the system to an ISP is based on these behaviours and correspond to the needs of the user.",
        "published": "2006-10-27T16:02:34Z",
        "link": "http://arxiv.org/abs/cs/0610158v1",
        "categories": [
            "cs.LG",
            "cs.IR"
        ]
    },
    {
        "title": "Low-complexity modular policies: learning to play Pac-Man and a new   framework beyond MDPs",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "In this paper we propose a method that learns to play Pac-Man. We define a set of high-level observation and action modules. Actions are temporally extended, and multiple action modules may be in effect concurrently. A decision of the agent is represented as a rule-based policy. For learning, we apply the cross-entropy method, a recent global optimization algorithm. The learned policies reached better score than the hand-crafted policy, and neared the score of average human players. We argue that learning is successful mainly because (i) the policy space includes the combination of individual actions and thus it is sufficiently rich, (ii) the search is biased towards low-complexity policies and low complexity solutions can be found quickly if they exist. Based on these principles, we formulate a new theoretical framework, which can be found in the Appendix as supporting material.",
        "published": "2006-10-30T16:44:58Z",
        "link": "http://arxiv.org/abs/cs/0610170v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Evolving controllers for simulated car racing",
        "authors": [
            "Julian Togelius",
            "Simon M. Lucas"
        ],
        "summary": "This paper describes the evolution of controllers for racing a simulated radio-controlled car around a track, modelled on a real physical track. Five different controller architectures were compared, based on neural networks, force fields and action sequences. The controllers use either egocentric (first person), Newtonian (third person) or no information about the state of the car (open-loop controller). The only controller that was able to evolve good racing behaviour was based on a neural network acting on egocentric inputs.",
        "published": "2006-11-02T00:47:57Z",
        "link": "http://arxiv.org/abs/cs/0611006v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "cs.RO"
        ]
    },
    {
        "title": "Hedging predictions in machine learning",
        "authors": [
            "Alexander Gammerman",
            "Vladimir Vovk"
        ],
        "summary": "Recent advances in machine learning make it possible to design efficient prediction algorithms for data sets with huge numbers of parameters. This paper describes a new technique for \"hedging\" the predictions output by many such algorithms, including support vector machines, kernel ridge regression, kernel nearest neighbours, and by many other state-of-the-art methods. The hedged predictions for the labels of new objects include quantitative measures of their own accuracy and reliability. These measures are provably valid under the assumption of randomness, traditional in machine learning: the objects and their labels are assumed to be generated independently from the same probability distribution. In particular, it becomes possible to control (up to statistical fluctuations) the number of erroneous predictions by selecting a suitable confidence level. Validity being achieved automatically, the remaining goal of hedged prediction is efficiency: taking full account of the new objects' features and other available information to produce as accurate predictions as possible. This can be done successfully using the powerful machinery of modern machine learning.",
        "published": "2006-11-02T18:44:49Z",
        "link": "http://arxiv.org/abs/cs/0611011v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Relational Approach to Functional Decomposition of Logic Circuits",
        "authors": [
            "Tony T. Lee",
            "Tong Ye"
        ],
        "summary": "Functional decomposition of logic circuits has profound influence on all quality aspects of the cost-effective implementation of modern digital systems. In this paper, a relational approach to the decomposition of logic circuits is proposed. This approach is parallel to the normalization of relational databases, they are governed by the same concepts of functional dependency (FD) and multi-valued dependency (MVD). It is manifest that the functional decomposition of switching function actually exploits the same idea and serves a similar purpose as database normalization. Partitions play an important role in the decomposition. The interdependency of two partitions can be represented by a bipartite graph. We demonstrate that both FD and MVD can be represented by bipartite graphs with specific topological properties, which are delineated by partitions of minterms. It follows that our algorithms are procedures of constructing those specific bipartite graphs of interest to meet the information-lossless criteria of functional decomposition.",
        "published": "2006-11-06T08:43:02Z",
        "link": "http://arxiv.org/abs/cs/0611024v1",
        "categories": [
            "cs.DM",
            "cs.LG"
        ]
    },
    {
        "title": "CSCR:Computer Supported Collaborative Research",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "It is suggested that a new area of CSCR (Computer Supported Collaborative Research) is distinguished from CSCW and CSCL and that the demarcation between the three areas could do with greater clarification and prescription.",
        "published": "2006-11-09T21:10:32Z",
        "link": "http://arxiv.org/abs/cs/0611042v1",
        "categories": [
            "cs.HC",
            "cs.LG"
        ]
    },
    {
        "title": "How Random is a Coin Toss? Bayesian Inference and the Symbolic Dynamics   of Deterministic Chaos",
        "authors": [
            "Christopher C. Strelioff",
            "James P. Crutchfield"
        ],
        "summary": "Symbolic dynamics has proven to be an invaluable tool in analyzing the mechanisms that lead to unpredictability and random behavior in nonlinear dynamical systems. Surprisingly, a discrete partition of continuous state space can produce a coarse-grained description of the behavior that accurately describes the invariant properties of an underlying chaotic attractor. In particular, measures of the rate of information production--the topological and metric entropy rates--can be estimated from the outputs of Markov or generating partitions. Here we develop Bayesian inference for k-th order Markov chains as a method to finding generating partitions and estimating entropy rates from finite samples of discretized data produced by coarse-grained dynamical systems.",
        "published": "2006-11-13T23:28:12Z",
        "link": "http://arxiv.org/abs/cs/0611054v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "nlin.CD"
        ]
    },
    {
        "title": "Very Sparse Stable Random Projections, Estimators and Tail Bounds for   Stable Random Projections",
        "authors": [
            "Ping Li"
        ],
        "summary": "This paper will focus on three different aspects in improving the current practice of stable random projections.   Firstly, we propose {\\em very sparse stable random projections} to significantly reduce the processing and storage cost, by replacing the $\\alpha$-stable distribution with a mixture of a symmetric $\\alpha$-Pareto distribution (with probability $\\beta$, $0<\\beta\\leq1$) and a point mass at the origin (with a probability $1-\\beta$). This leads to a significant $\\frac{1}{\\beta}$-fold speedup for small $\\beta$.   Secondly, we provide an improved estimator for recovering the original $l_\\alpha$ norms from the projected data. The standard estimator is based on the (absolute) sample median, while we suggest using the geometric mean. The geometric mean estimator we propose is strictly unbiased and is easier to study. Moreover, the geometric mean estimator is more accurate, especially non-asymptotically.   Thirdly, we provide an adequate answer to the basic question of how many projections (samples) are needed for achieving some pre-specified level of accuracy. \\cite{Proc:Indyk_FOCS00,Article:Indyk_TKDE03} did not provide a criterion that can be used in practice. The geometric mean estimator we propose allows us to derive sharp tail bounds which can be expressed in exponential forms with constants explicitly given.",
        "published": "2006-11-22T11:38:25Z",
        "link": "http://arxiv.org/abs/cs/0611114v2",
        "categories": [
            "cs.DS",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Functional Bregman Divergence and Bayesian Estimation of Distributions",
        "authors": [
            "B. A. Frigyik",
            "S. Srivastava",
            "M. R. Gupta"
        ],
        "summary": "A class of distortions termed functional Bregman divergences is defined, which includes squared error and relative entropy. A functional Bregman divergence acts on functions or distributions, and generalizes the standard Bregman divergence for vectors and a previous pointwise Bregman divergence that was defined for functions. A recently published result showed that the mean minimizes the expected Bregman divergence. The new functional definition enables the extension of this result to the continuous case to show that the mean minimizes the expected functional Bregman divergence over a set of functions or distributions. It is shown how this theorem applies to the Bayesian estimation of distributions. Estimation of the uniform distribution from independent and identically drawn samples is used as a case study.",
        "published": "2006-11-23T19:12:30Z",
        "link": "http://arxiv.org/abs/cs/0611123v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Low-rank matrix factorization with attributes",
        "authors": [
            "Jacob Abernethy",
            "Francis Bach",
            "Theodoros Evgeniou",
            "Jean-Philippe Vert"
        ],
        "summary": "We develop a new collaborative filtering (CF) method that combines both previously known users' preferences, i.e. standard CF, as well as product/user attributes, i.e. classical function approximation, to predict a given user's interest in a particular product. Our method is a generalized low rank matrix completion problem, where we learn a function whose inputs are pairs of vectors -- the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix. We solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties. Benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people, as well as over standard multi-task or single task learning methods.",
        "published": "2006-11-24T08:49:30Z",
        "link": "http://arxiv.org/abs/cs/0611124v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "A Unified View of TD Algorithms; Introducing Full-Gradient TD and   Equi-Gradient Descent TD",
        "authors": [
            "Manuel Loth",
            "Philippe Preux"
        ],
        "summary": "This paper addresses the issue of policy evaluation in Markov Decision Processes, using linear function approximation. It provides a unified view of algorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is asserted that they all consist in minimizing a gradient function and differ by the form of this function and their means of minimizing it. Two new schemes are introduced in that framework: Full-gradient TD which uses a generalization of the principle introduced in iLSTD, and EGD TD, which reduces the gradient by successive equi-gradient descents. These three algorithms form a new intermediate family with the interesting property of making much better use of the samples than TD while keeping a gradient descent scheme, which is useful for complexity issues and optimistic policy iteration.",
        "published": "2006-11-29T00:00:57Z",
        "link": "http://arxiv.org/abs/cs/0611145v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "A Novel Bayesian Classifier using Copula Functions",
        "authors": [
            "Saket Sathe"
        ],
        "summary": "A useful method for representing Bayesian classifiers is through \\emph{discriminant functions}. Here, using copula functions, we propose a new model for discriminants. This model provides a rich and generalized class of decision boundaries. These decision boundaries significantly boost the classification accuracy especially for high dimensional feature spaces. We strengthen our analysis through simulation results.",
        "published": "2006-11-29T13:59:31Z",
        "link": "http://arxiv.org/abs/cs/0611150v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Player co-modelling in a strategy board game: discovering how to play   fast",
        "authors": [
            "Dimitris Kalles"
        ],
        "summary": "In this paper we experiment with a 2-player strategy board game where playing models are evolved using reinforcement learning and neural networks. The models are evolved to speed up automatic game development based on human involvement at varying levels of sophistication and density when compared to fully autonomous playing. The experimental results suggest a clear and measurable association between the ability to win games and the ability to do that fast, while at the same time demonstrating that there is a minimum level of human involvement beyond which no learning really occurs.",
        "published": "2006-11-30T15:36:27Z",
        "link": "http://arxiv.org/abs/cs/0611164v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Loop corrections for approximate inference",
        "authors": [
            "Joris Mooij",
            "Bert Kappen"
        ],
        "summary": "We propose a method for improving approximate inference methods that corrects for the influence of loops in the graphical model. The method is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It is an alternative implementation of an idea introduced recently by Montanari and Rizzo (2005). In its simplest form, which amounts to the assumption that no loops are present, the method reduces to the minimal Cluster Variation Method approximation (which uses maximal factors as outer clusters). On the other hand, using estimates of the effect of loops (obtained by some approximate inference algorithm) and applying the Loop Correcting (LC) method usually gives significantly better results than applying the approximate inference algorithm directly without loop corrections. Indeed, we often observe that the loop corrected error is approximately the square of the error of the approximate inference method used to estimate the effect of loops. We compare different variants of the Loop Correcting method with other approximate inference methods on a variety of graphical models, including \"real world\" networks, and conclude that the LC approach generally obtains the most accurate results.",
        "published": "2006-12-05T15:57:44Z",
        "link": "http://arxiv.org/abs/cs/0612030v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Approximation of the Two-Part MDL Code",
        "authors": [
            "Pieter Adriaans",
            "Paul Vitanyi"
        ],
        "summary": "Approximation of the optimal two-part MDL code for given data, through successive monotonically length-decreasing two-part MDL codes, has the following properties: (i) computation of each step may take arbitrarily long; (ii) we may not know when we reach the optimum, or whether we will reach the optimum at all; (iii) the sequence of models generated may not monotonically improve the goodness of fit; but (iv) the model associated with the optimum has (almost) the best goodness of fit. To express the practically interesting goodness of fit of individual models for individual data sets we have to rely on Kolmogorov complexity.",
        "published": "2006-12-19T16:18:30Z",
        "link": "http://arxiv.org/abs/cs/0612095v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "E.4; I.2.6; I.5"
        ]
    },
    {
        "title": "Using state space differential geometry for nonlinear blind source   separation",
        "authors": [
            "David N. Levin"
        ],
        "summary": "Given a time series of multicomponent measurements of an evolving stimulus, nonlinear blind source separation (BSS) seeks to find a \"source\" time series, comprised of statistically independent combinations of the measured components. In this paper, we seek a source time series with local velocity cross correlations that vanish everywhere in stimulus state space. However, in an earlier paper the local velocity correlation matrix was shown to constitute a metric on state space. Therefore, nonlinear BSS maps onto a problem of differential geometry: given the metric observed in the measurement coordinate system, find another coordinate system in which the metric is diagonal everywhere. We show how to determine if the observed data are separable in this way, and, if they are, we show how to construct the required transformation to the source coordinate system, which is essentially unique except for an unknown rotation that can be found by applying the methods of linear BSS. Thus, the proposed technique solves nonlinear BSS in many situations or, at least, reduces it to linear BSS, without the use of probabilistic, parametric, or iterative procedures. This paper also describes a generalization of this methodology that performs nonlinear independent subspace separation. In every case, the resulting decomposition of the observed data is an intrinsic property of the stimulus' evolution in the sense that it does not depend on the way the observer chooses to view it (e.g., the choice of the observing machine's sensors). In other words, the decomposition is a property of the evolution of the \"real\" stimulus that is \"out there\" broadcasting energy to the observer. The technique is illustrated with analytic and numerical examples.",
        "published": "2006-12-19T22:32:18Z",
        "link": "http://arxiv.org/abs/cs/0612096v1",
        "categories": [
            "cs.LG",
            "cs.SD",
            "I.2.6; J.2; I.2.7; I.5"
        ]
    },
    {
        "title": "Statistical Mechanics of On-line Learning when a Moving Teacher Goes   around an Unlearnable True Teacher",
        "authors": [
            "Masahiro Urakami",
            "Seiji Miyoshi",
            "Masato Okada"
        ],
        "summary": "In the framework of on-line learning, a learning machine might move around a teacher due to the differences in structures or output functions between the teacher and the learning machine. In this paper we analyze the generalization performance of a new student supervised by a moving machine. A model composed of a fixed true teacher, a moving teacher, and a student is treated theoretically using statistical mechanics, where the true teacher is a nonmonotonic perceptron and the others are simple perceptrons. Calculating the generalization errors numerically, we show that the generalization errors of a student can temporarily become smaller than that of a moving teacher, even if the student only uses examples from the moving teacher. However, the generalization error of the student eventually becomes the same value with that of the moving teacher. This behavior is qualitatively different from that of a linear model.",
        "published": "2006-12-22T00:25:04Z",
        "link": "http://arxiv.org/abs/cs/0612117v1",
        "categories": [
            "cs.LG",
            "cond-mat.dis-nn"
        ]
    },
    {
        "title": "Minimum-weight triangulation is NP-hard",
        "authors": [
            "Wolfgang Mulzer",
            "Guenter Rote"
        ],
        "summary": "A triangulation of a planar point set S is a maximal plane straight-line graph with vertex set S. In the minimum-weight triangulation (MWT) problem, we are looking for a triangulation of a given point set that minimizes the sum of the edge lengths. We prove that the decision version of this problem is NP-hard. We use a reduction from PLANAR-1-IN-3-SAT. The correct working of the gadgets is established with computer assistance, using dynamic programming on polygonal faces, as well as the beta-skeleton heuristic to certify that certain edges belong to the minimum-weight triangulation.",
        "published": "2006-01-02T16:11:29Z",
        "link": "http://arxiv.org/abs/cs/0601002v3",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Fast Enumeration of Combinatorial Objects",
        "authors": [
            "Boris Ryabko"
        ],
        "summary": "The problem of ranking can be described as follows. We have a set of combinatorial objects $S$, such as, say, the k-subsets of n things, and we can imagine that they have been arranged in some list, say lexicographically, and we want to have a fast method for obtaining the rank of a given object in the list. This problem is widely known in Combinatorial Analysis, Computer Science and Information Theory. Ranking is closely connected with the hashing problem, especially with perfect hashing and with generating of random combinatorial objects. In Information Theory the ranking problem is closely connected with so-called enumerative encoding, which may be described as follows: there is a set of words $S$ and an enumerative code has to one-to-one encode every $s \\in S$ by a binary word $code(s)$. The length of the $code(s)$ must be the same for all $s \\in S$. Clearly, $|code (s)|\\geq \\log |S|$. (Here and below $\\log x=\\log_{2}x)$.) The suggested method allows the exponential growth of the speed of encoding and decoding for all combinatorial problems of enumeration which are considered, including the enumeration of permutations, compositions and others.",
        "published": "2006-01-15T13:55:51Z",
        "link": "http://arxiv.org/abs/cs/0601069v1",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "An O(1) Solution to the Prefix Sum Problem on a Specialized Memory   Architecture",
        "authors": [
            "Andrej Brodnik",
            "Johan Karlsson",
            "J. Ian Munro",
            "Andreas Nilsson"
        ],
        "summary": "In this paper we study the Prefix Sum problem introduced by Fredman.   We show that it is possible to perform both update and retrieval in O(1) time simultaneously under a memory model in which individual bits may be shared by several words.   We also show that two variants (generalizations) of the problem can be solved optimally in $\\Theta(\\lg N)$ time under the comparison based model of computation.",
        "published": "2006-01-18T21:20:10Z",
        "link": "http://arxiv.org/abs/cs/0601081v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.IR",
            "E.1; F.1.1"
        ]
    },
    {
        "title": "Comments on Beckmann's Uniform Reducts",
        "authors": [
            "Stephen Cook"
        ],
        "summary": "Arnold Beckmann defined the uniform reduct of a propositional proof system f to be the set of those bounded arithmetical formulas whose propositional translations have polynomial size f-proofs. We prove that the uniform reduct of f + Extended Frege consists of all true bounded arithmetical formulas iff f + Extended Frege simulates every proof system.",
        "published": "2006-01-19T22:56:49Z",
        "link": "http://arxiv.org/abs/cs/0601086v2",
        "categories": [
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Pseudorandomness and Combinatorial Constructions",
        "authors": [
            "Luca Trevisan"
        ],
        "summary": "In combinatorics, the probabilistic method is a very powerful tool to prove the existence of combinatorial objects with interesting and useful properties. Explicit constructions of objects with such properties are often very difficult, or unknown. In computer science, probabilistic algorithms are sometimes simpler and more efficient than the best known deterministic algorithms for the same problem.   Despite this evidence for the power of random choices, the computational theory of pseudorandomness shows that, under certain complexity-theoretic assumptions, every probabilistic algorithm has an efficient deterministic simulation and a large class of applications of the the probabilistic method can be converted into explicit constructions.   In this survey paper we describe connections between the conditional ``derandomization'' results of the computational theory of pseudorandomness and unconditional explicit constructions of certain combinatorial objects such as error-correcting codes and ``randomness extractors.''",
        "published": "2006-01-23T19:24:40Z",
        "link": "http://arxiv.org/abs/cs/0601100v1",
        "categories": [
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Complexity of the Guarded Two-Variable Fragment with Counting   Quantifiers",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "We show that the finite satisfiability problem for the guarded two-variable fragment with counting quantifiers is in EXPTIME. The method employed also yields a simple proof of a result recently obtained by Y. Kazakov, that the satisfiability problem for the guarded two-variable fragment with counting quantifiers is in EXPTIME.",
        "published": "2006-01-26T10:38:11Z",
        "link": "http://arxiv.org/abs/cs/0601112v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Query-Monotonic Turing Reductions",
        "authors": [
            "Lane A. Hemaspaandra",
            "Mayur Thakur"
        ],
        "summary": "We study reductions that limit the extreme adaptivity of Turing reductions. In particular, we study reductions that make a rapid, structured progression through the set to which they are reducing: Each query is strictly longer (shorter) than the previous one. We call these reductions query-increasing (query-decreasing) Turing reductions. We also study query-nonincreasing (query-nondecreasing) Turing reductions. These are Turing reductions in which the sequence of query lengths is nonincreasing (nondecreasing). We ask whether these restrictions in fact limit the power of reductions. We prove that query-increasing and query-decreasing Turing reductions are incomparable with (that is, are neither strictly stronger than nor strictly weaker than) truth-table reductions and are strictly weaker than Turing reductions. In addition, we prove that query-nonincreasing and query-nondecreasing Turing reductions are strictly stronger than truth-table reductions and strictly weaker than Turing reductions. Despite the fact that we prove query-increasing and query-decreasing Turing reductions to in the general case be strictly weaker than Turing reductions, we identify a broad class of sets A for which any set that Turing reduces to A will also reduce to A via both query-increasing and query-decreasing Turing reductions. In particular, this holds for all tight paddable sets, where a set is said to be tight paddable exactly if it is paddable via a function whose output length is bounded tightly both from above and from below in the length of the input. We prove that many natural NP-complete problems such as satisfiability, clique, and vertex cover are tight paddable.",
        "published": "2006-01-31T22:02:05Z",
        "link": "http://arxiv.org/abs/cs/0602001v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "Conjunctive Queries over Trees",
        "authors": [
            "Georg Gottlob",
            "Christoph Koch",
            "Klaus U. Schulz"
        ],
        "summary": "We study the complexity and expressive power of conjunctive queries over unranked labeled trees represented using a variety of structure relations such as ``child'', ``descendant'', and ``following'' as well as unary relations for node labels. We establish a framework for characterizing structures representing trees for which conjunctive queries can be evaluated efficiently. Then we completely chart the tractability frontier of the problem and establish a dichotomy theorem for our axis relations, i.e., we find all subset-maximal sets of axes for which query evaluation is in polynomial time and show that for all other cases, query evaluation is NP-complete. All polynomial-time results are obtained immediately using the proof techniques from our framework. Finally, we study the expressiveness of conjunctive queries over trees and show that for each conjunctive query, there is an equivalent acyclic positive query (i.e., a set of acyclic conjunctive queries), but that in general this query is not of polynomial size.",
        "published": "2006-02-02T23:28:24Z",
        "link": "http://arxiv.org/abs/cs/0602004v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "E.1; F.1.3; F.2.2; H.2.3; H.2.4; I.7.2"
        ]
    },
    {
        "title": "Reducing Tile Complexity for Self-Assembly Through Temperature   Programming",
        "authors": [
            "Ming-Yang Kao",
            "Robert Schweller"
        ],
        "summary": "We consider the tile self-assembly model and how tile complexity can be eliminated by permitting the temperature of the self-assembly system to be adjusted throughout the assembly process. To do this, we propose novel techniques for designing tile sets that permit an arbitrary length $m$ binary number to be encoded into a sequence of $O(m)$ temperature changes such that the tile set uniquely assembles a supertile that precisely encodes the corresponding binary number. As an application, we show how this provides a general tile set of size O(1) that is capable of uniquely assembling essentially any $n\\times n$ square, where the assembled square is determined by a temperature sequence of length $O(\\log n)$ that encodes a binary description of $n$. This yields an important decrease in tile complexity from the required $\\Omega(\\frac{\\log n}{\\log\\log n})$ for almost all $n$ when the temperature of the system is fixed. We further show that for almost all $n$, no tile system can simultaneously achieve both $o(\\log n)$ temperature complexity and $o(\\frac{\\log n}{\\log\\log n})$ tile complexity, showing that both versions of an optimal square building scheme have been discovered. This work suggests that temperature change can constitute a natural, dynamic method for providing input to self-assembly systems that is potentially superior to the current technique of designing large tile sets with specific inputs hardwired into the tileset.",
        "published": "2006-02-05T01:00:32Z",
        "link": "http://arxiv.org/abs/cs/0602010v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Computational complexity of the landscape I",
        "authors": [
            "Frederik Denef",
            "Michael R. Douglas"
        ],
        "summary": "We study the computational complexity of the physical problem of finding vacua of string theory which agree with data, such as the cosmological constant, and show that such problems are typically NP hard. In particular, we prove that in the Bousso-Polchinski model, the problem is NP complete. We discuss the issues this raises and the possibility that, even if we were to find compelling evidence that some vacuum of string theory describes our universe, we might never be able to find that vacuum explicitly.   In a companion paper, we apply this point of view to the question of how early cosmology might select a vacuum.",
        "published": "2006-02-07T19:15:23Z",
        "link": "http://arxiv.org/abs/hep-th/0602072v2",
        "categories": [
            "hep-th",
            "cs.CC"
        ]
    },
    {
        "title": "Finite-State Dimension and Real Arithmetic",
        "authors": [
            "David Doty",
            "Jack H. Lutz",
            "Satyadev Nandakumar"
        ],
        "summary": "We use entropy rates and Schur concavity to prove that, for every integer k >= 2, every nonzero rational number q, and every real number alpha, the base-k expansions of alpha, q+alpha, and q*alpha all have the same finite-state dimension and the same finite-state strong dimension. This extends, and gives a new proof of, Wall's 1949 theorem stating that the sum or product of a nonzero rational number and a Borel normal number is always Borel normal.",
        "published": "2006-02-09T00:20:30Z",
        "link": "http://arxiv.org/abs/cs/0602032v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A quantum secret ballot",
        "authors": [
            "Shahar Dolev",
            "Itamar Pitowsky",
            "Boaz Tamir"
        ],
        "summary": "The paper concerns the protection of the secrecy of ballots, so that the identity of the voters cannot be matched with their vote. To achieve this we use an entangled quantum state to represent the ballots. Each ballot includes the identity of the voter, explicitly marked on the \"envelope\" containing it. Measuring the content of the envelope yields a random number which reveals no information about the vote. However, the outcome of the elections can be unambiguously decided after adding the random numbers from all envelopes. We consider a few versions of the protocol and their complexity of implementation.",
        "published": "2006-02-09T21:03:35Z",
        "link": "http://arxiv.org/abs/quant-ph/0602087v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum Hardcore Functions by Complexity-Theoretical Quantum List   Decoding",
        "authors": [
            "Akinori Kawachi",
            "Tomoyuki Yamakami"
        ],
        "summary": "Hardcore functions have been used as a technical tool to construct secure cryptographic systems; however, little is known on their quantum counterpart, called quantum hardcore functions. With a new insight into fundamental properties of quantum hardcores, we present three new quantum hardcore functions for any (strong) quantum one-way function. We also give a \"quantum\" solution to Damgard's question (CRYPTO'88) on a classical hardcore property of his pseudorandom generator, by proving its quantum hardcore property. Our major technical tool is the new notion of quantum list-decoding of \"classical\" error-correcting codes (rather than \"quantum\" error-correcting codes), which is defined on the platform of computational complexity theory and computational cryptography (rather than information theory). In particular, we give a simple but powerful criterion that makes a polynomial-time computable classical block code (seen as a function) a quantum hardcore for all quantum one-way functions. On their own interest, we construct efficient quantum list-decoding algorithms for classical block codes whose associated quantum states (called codeword states) form a nearly phase-orthogonal basis.",
        "published": "2006-02-10T04:59:41Z",
        "link": "http://arxiv.org/abs/quant-ph/0602088v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Computing Nash Equilibria: Approximation and Smoothed Complexity",
        "authors": [
            "Xi Chen",
            "Xiaotie Deng",
            "Shang-Hua Teng"
        ],
        "summary": "We show that the BIMATRIX game does not have a fully polynomial-time approximation scheme, unless PPAD is in P. In other words, no algorithm with time polynomial in n and 1/\\epsilon can compute an \\epsilon-approximate Nash equilibrium of an n by nbimatrix game, unless PPAD is in P. Instrumental to our proof, we introduce a new discrete fixed-point problem on a high-dimensional cube with a constant side-length, such as on an n-dimensional cube with side-length 7, and show that they are PPAD-complete. Furthermore, we prove, unless PPAD is in RP, that the smoothed complexity of the Lemke-Howson algorithm or any algorithm for computing a Nash equilibrium of a bimatrix game is polynomial in n and 1/\\sigma under perturbations with magnitude \\sigma. Our result answers a major open question in the smoothed analysis of algorithms and the approximation of Nash equilibria.",
        "published": "2006-02-11T22:46:55Z",
        "link": "http://arxiv.org/abs/cs/0602043v2",
        "categories": [
            "cs.CC",
            "cs.GT",
            "F.1.2; F.1.3; F.2; F.2.3"
        ]
    },
    {
        "title": "Approximability of Integer Programming with Generalised Constraints",
        "authors": [
            "Peter Jonsson",
            "Fredrik Kuivinen",
            "Gustav Nordh"
        ],
        "summary": "We study a family of problems, called \\prob{Maximum Solution}, where the objective is to maximise a linear goal function over the feasible integer assignments to a set of variables subject to a set of constraints. When the domain is Boolean (i.e. restricted to $\\{0,1\\}$), the maximum solution problem is identical to the well-studied \\prob{Max Ones} problem, and the approximability is completely understood for all restrictions on the underlying constraints [Khanna et al., SIAM J. Comput., 30 (2001), pp. 1863-1920]. We continue this line of research by considering domains containing more than two elements. We present two main results: a complete classification for the approximability of all maximal constraint languages over domains of cardinality at most 4, and a complete classification of the approximability of the problem when the set of allowed constraints contains all permutation constraints. Under the assumption that a conjecture due to Szczepara holds, we give a complete classification for all maximal constraint languages. These classes of languages are well-studied in universal algebra and computer science; they have, for instance, been considered in connection with machine learning and constraint satisfaction. Our results are proved by using algebraic results from clone theory and the results indicates that this approach is very powerful for classifying the approximability of certain optimisation problems.",
        "published": "2006-02-13T18:48:52Z",
        "link": "http://arxiv.org/abs/cs/0602047v3",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The quantum measurement problem and physical reality: a computation   theoretic perspective",
        "authors": [
            "R. Srikanth"
        ],
        "summary": "Is the universe computable? If yes, is it computationally a polynomial place? In standard quantum mechanics, which permits infinite parallelism and the infinitely precise specification of states, a negative answer to both questions is not ruled out. On the other hand, empirical evidence suggests that NP-complete problems are intractable in the physical world. Likewise, computational problems known to be algorithmically uncomputable do not seem to be computable by any physical means. We suggest that this close correspondence between the efficiency and power of abstract algorithms on the one hand, and physical computers on the other, finds a natural explanation if the universe is assumed to be algorithmic; that is, that physical reality is the product of discrete sub-physical information processing equivalent to the actions of a probabilistic Turing machine. This assumption can be reconciled with the observed exponentiality of quantum systems at microscopic scales, and the consequent possibility of implementing Shor's quantum polynomial time algorithm at that scale, provided the degree of superposition is intrinsically, finitely upper-bounded. If this bound is associated with the quantum-classical divide (the Heisenberg cut), a natural resolution to the quantum measurement problem arises. From this viewpoint, macroscopic classicality is an evidence that the universe is in BPP, and both questions raised above receive affirmative answers. A recently proposed computational model of quantum measurement, which relates the Heisenberg cut to the discreteness of Hilbert space, is briefly discussed. A connection to quantum gravity is noted. Our results are compatible with the philosophy that mathematical truths are independent of the laws of physics.",
        "published": "2006-02-14T17:50:31Z",
        "link": "http://arxiv.org/abs/quant-ph/0602114v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The approximability of MAX CSP with fixed-value constraints",
        "authors": [
            "Vladimir Deineko",
            "Peter Jonsson",
            "Mikael Klasson",
            "Andrei Krokhin"
        ],
        "summary": "In the maximum constraint satisfaction problem (MAX CSP), one is given a finite collection of (possibly weighted) constraints on overlapping sets of variables, and the goal is to assign values from a given finite domain to the variables so as to maximize the number (or the total weight, for the weighted case) of satisfied constraints. This problem is NP-hard in general, and, therefore, it is natural to study how restricting the allowed types of constraints affects the approximability of the problem. In this paper, we show that any MAX CSP problem with a finite set of allowed constraint types, which includes all fixed-value constraints (i.e., constraints of the form x=a), is either solvable exactly in polynomial-time or else is APX-complete, even if the number of occurrences of variables in instances are bounded. Moreover, we present a simple description of all polynomial-time solvable cases of our problem. This description relies on the well-known algebraic combinatorial property of supermodularity.",
        "published": "2006-02-21T14:13:37Z",
        "link": "http://arxiv.org/abs/cs/0602075v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On the Approximation and Smoothed Complexity of Leontief Market   Equilibria",
        "authors": [
            "Li-Sha Huang",
            "Shang-Hua Teng"
        ],
        "summary": "We show that the problem of finding an \\epsilon-approximate Nash equilibrium of an n by n two-person games can be reduced to the computation of an (\\epsilon/n)^2-approximate market equilibrium of a Leontief economy. Together with a recent result of Chen, Deng and Teng, this polynomial reduction implies that the Leontief market exchange problem does not have a fully polynomial-time approximation scheme, that is, there is no algorithm that can compute an \\epsilon-approximate market equilibrium in time polynomial in m, n, and 1/\\epsilon, unless PPAD is not in P, We also extend the analysis of our reduction to show, unless PPAD is not in RP, that the smoothed complexity of the Scarf's general fixed-point approximation algorithm (when applying to solve the approximate Leontief market exchange problem) or of any algorithm for computing an approximate market equilibrium of Leontief economies is not polynomial in n and 1/\\sigma, under Gaussian or uniform perturbations with magnitude \\sigma.",
        "published": "2006-02-26T07:53:31Z",
        "link": "http://arxiv.org/abs/cs/0602090v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "F.1.2; F.1.3; F.2; F.2.3"
        ]
    },
    {
        "title": "A Measure of Space for Computing over the Reals",
        "authors": [
            "Paulin Jacobé De Naurois"
        ],
        "summary": "We propose a new complexity measure of space for the BSS model of computation. We define LOGSPACE\\_W and PSPACE\\_W complexity classes over the reals. We prove that LOGSPACE\\_W is included in NC^2\\_R and in P\\_W, i.e. is small enough for being relevant. We prove that the Real Circuit Decision Problem is P\\_R-complete under LOGSPACE\\_W reductions, i.e. that LOGSPACE\\_W is large enough for containing natural algorithms. We also prove that PSPACE\\_W is included in PAR\\_R.",
        "published": "2006-03-03T13:27:45Z",
        "link": "http://arxiv.org/abs/cs/0603017v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Characterizing the NP-PSPACE Gap in the Satisfiability Problem for Modal   Logic",
        "authors": [
            "Joseph Y. Halpern",
            "Leandro Chaves Rego"
        ],
        "summary": "There has been a great of work on characterizing the complexity of the satisfiability and validity problem for modal logics. In particular, Ladner showed that the validity problem for all logics between K, T, and S4 is {\\sl PSPACE}-complete, while for S5 it is {\\sl NP}-complete. We show that, in a precise sense, it is \\emph{negative introspection}, the axiom $\\neg K p \\rimp K \\neg K p$, that causes the gap. In a precise sense, if we require this axiom, then the satisfiability problem is {\\sl NP}-complete; without it, it is {\\sl PSPACE}-complete.",
        "published": "2006-03-05T23:11:10Z",
        "link": "http://arxiv.org/abs/cs/0603019v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "New upper and lower bounds for randomized and quantum Local Search",
        "authors": [
            "Shengyu Zhang"
        ],
        "summary": "Local Search problem, which finds a local minimum of a black-box function on a given graph, is of both practical and theoretical importance to combinatorial optimization, complexity theory and many other areas in theoretical computer science. In this paper, we study the problem in the randomized and quantum query models and give new lower and upper bound techniques in both models.   The lower bound technique works for any graph that contains a product graph as a subgraph. Applying it to the Boolean hypercube {0,1}^n and the constant dimensional grids [n]^d, two particular product graphs that recently drew much attention, we get the following tight results:   RLS({0,1}^n) = \\Theta(2^{n/2}n^{1/2}), QLS({0,1}^n) = \\Theta(2^{n/3}n^{1/6}),   RLS([n]^d) = \\Theta(n^{d/2}) for d \\geq 4, QLS([n]^d) = \\Theta(n^{d/3}) for d \\geq 6.   Here RLS(G) and QLS(G) are the randomized and quantum query complexities of Local Search on G, respectively. These improve the previous results by Aaronson [STOC'04], Ambainis (unpublished) and Santha and Szegedy [STOC'04].   Our new algorithms work well when the underlying graph expands slowly. As an application to [n]^2, a new quantum algorithm using O(\\sqrt{n}(\\log\\log n)^{1.5}) queries is given. This improves the previous best known upper bound of O(n^{2/3}) (Aaronson, [STOC'04]), and implies that Local Search on grids exhibits different properties in low dimensions.",
        "published": "2006-03-06T00:21:25Z",
        "link": "http://arxiv.org/abs/quant-ph/0603034v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The Snowblower Problem",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Joseph S. B. Mitchell",
            "Valentin Polishchuk"
        ],
        "summary": "We introduce the snowblower problem (SBP), a new optimization problem that is closely related to milling problems and to some material-handling problems. The objective in the SBP is to compute a short tour for the snowblower to follow to remove all the snow from a domain (driveway, sidewalk, etc.). When a snowblower passes over each region along the tour, it displaces snow into a nearby region. The constraint is that if the snow is piled too high, then the snowblower cannot clear the pile.   We give an algorithmic study of the SBP. We show that in general, the problem is NP-complete, and we present polynomial-time approximation algorithms for removing snow under various assumptions about the operation of the snowblower. Most commercially-available snowblowers allow the user to control the direction in which the snow is thrown. We differentiate between the cases in which the snow can be thrown in any direction, in any direction except backwards, and only to the right. For all cases, we give constant-factor approximation algorithms; the constants increase as the throw direction becomes more restricted.   Our results are also applicable to robotic vacuuming (or lawnmowing) with bounded capacity dust bin and to some versions of material-handling problems, in which the goal is to rearrange cartons on the floor of a warehouse.",
        "published": "2006-03-07T20:35:48Z",
        "link": "http://arxiv.org/abs/cs/0603026v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.RO",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Fast matrix multiplication is stable",
        "authors": [
            "James Demmel",
            "Ioana Dumitriu",
            "Olga Holtz",
            "Robert Kleinberg"
        ],
        "summary": "We perform forward error analysis for a large class of recursive matrix multiplication algorithms in the spirit of [D. Bini and G. Lotti, Stability of fast algorithms for matrix multiplication, Numer. Math. 36 (1980), 63--72]. As a consequence of our analysis, we show that the exponent of matrix multiplication (the optimal running time) can be achieved by numerically stable algorithms. We also show that new group-theoretic algorithms proposed in [H. Cohn, and C. Umans, A group-theoretic approach to fast matrix multiplication, FOCS 2003, 438--449] and [H. Cohn, R. Kleinberg, B. Szegedy and C. Umans, Group-theoretic algorithms for matrix multiplication, FOCS 2005, 379--388] are all included in the class of algorithms to which our analysis applies, and are therefore numerically stable. We perform detailed error analysis for three specific fast group-theoretic algorithms.",
        "published": "2006-03-09T04:34:36Z",
        "link": "http://arxiv.org/abs/math/0603207v3",
        "categories": [
            "math.NA",
            "cs.CC",
            "cs.DS",
            "math.GR",
            "65Y20, 65F30, 65G50, 68Q17, 68W40, 20C05, 20K01, 16S34, 43A30, 65T50"
        ]
    },
    {
        "title": "Time-Space Trade-Offs for Predecessor Search",
        "authors": [
            "Mihai Patrascu",
            "Mikkel Thorup"
        ],
        "summary": "We develop a new technique for proving cell-probe lower bounds for static data structures. Previous lower bounds used a reduction to communication games, which was known not to be tight by counting arguments. We give the first lower bound for an explicit problem which breaks this communication complexity barrier. In addition, our bounds give the first separation between polynomial and near linear space. Such a separation is inherently impossible by communication complexity.   Using our lower bound technique and new upper bound constructions, we obtain tight bounds for searching predecessors among a static set of integers. Given a set Y of n integers of l bits each, the goal is to efficiently find predecessor(x) = max{y in Y | y <= x}, by representing Y on a RAM using space S.   In external memory, it follows that the optimal strategy is to use either standard B-trees, or a RAM algorithm ignoring the larger block size. In the important case of l = c*lg n, for c>1 (i.e. polynomial universes), and near linear space (such as S = n*poly(lg n)), the optimal search time is Theta(lg l). Thus, our lower bound implies the surprising conclusion that van Emde Boas' classic data structure from [FOCS'75] is optimal in this case. Note that for space n^{1+eps}, a running time of O(lg l / lglg l) was given by Beame and Fich [STOC'99].",
        "published": "2006-03-10T14:50:20Z",
        "link": "http://arxiv.org/abs/cs/0603043v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.3; E.2"
        ]
    },
    {
        "title": "Computing the Top Betti Numbers of Semi-algebraic Sets Defined by   Quadratic Inequalities in Polynomial Time",
        "authors": [
            "Saugata Basu"
        ],
        "summary": "For any $\\ell > 0$, we present an algorithm which takes as input a semi-algebraic set, $S$, defined by $P_1 \\leq 0,...,P_s \\leq 0$, where each $P_i \\in \\R[X_1,...,X_k]$ has degree $\\leq 2,$ and computes the top $\\ell$ Betti numbers of $S$, $b_{k-1}(S), ..., b_{k-\\ell}(S),$ in polynomial time. The complexity of the algorithm, stated more precisely, is $ \\sum_{i=0}^{\\ell+2} {s \\choose i} k^{2^{O(\\min(\\ell,s))}}. $ For fixed $\\ell$, the complexity of the algorithm can be expressed as $s^{\\ell+2} k^{2^{O(\\ell)}},$ which is polynomial in the input parameters $s$ and $k$. To our knowledge this is the first polynomial time algorithm for computing non-trivial topological invariants of semi-algebraic sets in $\\R^k$ defined by polynomial inequalities, where the number of inequalities is not fixed and the polynomials are allowed to have degree greater than one. For fixed $s$, we obtain by letting $\\ell = k$, an algorithm for computing all the Betti numbers of $S$ whose complexity is $k^{2^{O(s)}}$.",
        "published": "2006-03-10T21:22:58Z",
        "link": "http://arxiv.org/abs/math/0603262v4",
        "categories": [
            "math.AG",
            "cs.CC",
            "math.LO"
        ]
    },
    {
        "title": "The Quantum Separability Problem for Gaussian States",
        "authors": [
            "Stefano Mancini",
            "Simone Severini"
        ],
        "summary": "Determining whether a quantum state is separable or entangled is a problem of fundamental importance in quantum information science. This is a brief review in which we consider the problem for states in infinite dimensional Hilbert spaces. We show how the problem becomes tractable for a class of Gaussian states.",
        "published": "2006-03-12T14:33:41Z",
        "link": "http://arxiv.org/abs/cs/0603047v2",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "The number of matchings in random graphs",
        "authors": [
            "Lenka Zdeborová",
            "Marc Mézard"
        ],
        "summary": "We study matchings on sparse random graphs by means of the cavity method. We first show how the method reproduces several known results about maximum and perfect matchings in regular and Erdos-Renyi random graphs. Our main new result is the computation of the entropy, i.e. the leading order of the logarithm of the number of solutions, of matchings with a given size. We derive both an algorithm to compute this entropy for an arbitrary graph with a girth that diverges in the large size limit, and an analytic result for the entropy in regular and Erdos-Renyi random graph ensembles.",
        "published": "2006-03-13T14:02:05Z",
        "link": "http://arxiv.org/abs/cond-mat/0603350v2",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Testing Graph Isomorphism in Parallel by Playing a Game",
        "authors": [
            "Martin Grohe",
            "Oleg Verbitsky"
        ],
        "summary": "Our starting point is the observation that if graphs in a class C have low descriptive complexity in first order logic, then the isomorphism problem for C is solvable by a fast parallel algorithm (essentially, by a simple combinatorial algorithm known as the multidimensional Weisfeiler-Lehman algorithm). Using this approach, we prove that isomorphism of graphs of bounded treewidth is testable in TC1, answering an open question posed by Chandrasekharan. Furthermore, we obtain an AC1 algorithm for testing isomorphism of rotation systems (combinatorial specifications of graph embeddings). The AC1 upper bound was known before, but the fact that this bound can be achieved by the simple Weisfeiler-Lehman algorithm is new. Combined with other known results, it also yields a new AC1 isomorphism algorithm for planar graphs.",
        "published": "2006-03-14T15:46:58Z",
        "link": "http://arxiv.org/abs/cs/0603054v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Interaction in Quantum Communication",
        "authors": [
            "Hartmut Klauck",
            "Ashwin Nayak",
            "Amnon Ta-Shma",
            "David Zuckerman"
        ],
        "summary": "In some scenarios there are ways of conveying information with many fewer, even exponentially fewer, qubits than possible classically. Moreover, some of these methods have a very simple structure--they involve only few message exchanges between the communicating parties. It is therefore natural to ask whether every classical protocol may be transformed to a ``simpler'' quantum protocol--one that has similar efficiency, but uses fewer message exchanges.   We show that for any constant k, there is a problem such that its k+1 message classical communication complexity is exponentially smaller than its k message quantum communication complexity. This, in particular, proves a round hierarchy theorem for quantum communication complexity, and implies, via a simple reduction, an Omega(N^{1/k}) lower bound for k message quantum protocols for Set Disjointness for constant k.   Enroute, we prove information-theoretic lemmas, and define a related measure of correlation, the informational distance, that we believe may be of significance in other contexts as well.",
        "published": "2006-03-15T12:45:08Z",
        "link": "http://arxiv.org/abs/quant-ph/0603135v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An Improved Exact Algorithm for the Domatic Number Problem",
        "authors": [
            "Tobias Riege",
            "Jörg Rothe",
            "Holger Spakowski",
            "Masaki Yamamoto"
        ],
        "summary": "The 3-domatic number problem asks whether a given graph can be partitioned intothree dominating sets. We prove that this problem can be solved by a deterministic algorithm in time 2.695^n (up to polynomial factors). This result improves the previous bound of 2.8805^n, which is due to Fomin, Grandoni, Pyatkin, and Stepanov. To prove our result, we combine an algorithm by Fomin et al. with Yamamoto's algorithm for the satisfiability problem. In addition, we show that the 3-domatic number problem can be solved for graphs G with bounded maximum degree Delta(G) by a randomized algorithm, whose running time is better than the previous bound due to Riege and Rothe whenever Delta(G) >= 5. Our new randomized algorithm employs Schoening's approach to constraint satisfaction problems.",
        "published": "2006-03-16T16:05:40Z",
        "link": "http://arxiv.org/abs/cs/0603060v1",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Packrat Parsing: Simple, Powerful, Lazy, Linear Time",
        "authors": [
            "Bryan Ford"
        ],
        "summary": "Packrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(k) or LR(k) grammar can be recognized by a packrat parser, in addition to many languages that conventional linear-time algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a linear-time packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives.",
        "published": "2006-03-18T17:49:45Z",
        "link": "http://arxiv.org/abs/cs/0603077v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.PL",
            "D.3.4; D.1.1; F.4.2"
        ]
    },
    {
        "title": "Strengths and Weaknesses of Quantum Fingerprinting",
        "authors": [
            "Dmytro Gavinsky",
            "Julia Kempe",
            "Ronald de Wolf"
        ],
        "summary": "We study the power of quantum fingerprints in the simultaneous message passing (SMP) setting of communication complexity. Yao recently showed how to simulate, with exponential overhead, classical shared-randomness SMP protocols by means of quantum SMP protocols without shared randomness ($Q^\\parallel$-protocols). Our first result is to extend Yao's simulation to the strongest possible model: every many-round quantum protocol with unlimited shared entanglement can be simulated, with exponential overhead, by $Q^\\parallel$-protocols. We apply our technique to obtain an efficient $Q^\\parallel$-protocol for a function which cannot be efficiently solved through more restricted simulations. Second, we tightly characterize the power of the quantum fingerprinting technique by making a connection to arrangements of homogeneous halfspaces with maximal margin. These arrangements have been well studied in computational learning theory, and we use some strong results obtained in this area to exhibit weaknesses of quantum fingerprinting. In particular, this implies that for almost all functions, quantum fingerprinting protocols are exponentially worse than classical deterministic SMP protocols.",
        "published": "2006-03-20T16:13:53Z",
        "link": "http://arxiv.org/abs/quant-ph/0603173v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Random 3CNF formulas elude the Lovasz theta function",
        "authors": [
            "Uriel Feige",
            "Eran Ofek"
        ],
        "summary": "Let $\\phi$ be a 3CNF formula with n variables and m clauses. A simple nonconstructive argument shows that when m is sufficiently large compared to n, most 3CNF formulas are not satisfiable. It is an open question whether there is an efficient refutation algorithm that for most such formulas proves that they are not satisfiable. A possible approach to refute a formula $\\phi$ is: first, translate it into a graph $G_{\\phi}$ using a generic reduction from 3-SAT to max-IS, then bound the maximum independent set of $G_{\\phi}$ using the Lovasz $\\vartheta$ function. If the $\\vartheta$ function returns a value $< m$, this is a certificate for the unsatisfiability of $\\phi$. We show that for random formulas with $m < n^{3/2 -o(1)}$ clauses, the above approach fails, i.e. the $\\vartheta$ function is likely to return a value of m.",
        "published": "2006-03-22T10:30:36Z",
        "link": "http://arxiv.org/abs/cs/0603084v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Computational complexity of the quantum separability problem",
        "authors": [
            "Lawrence M. Ioannou"
        ],
        "summary": "Ever since entanglement was identified as a computational and cryptographic resource, researchers have sought efficient ways to tell whether a given density matrix represents an unentangled, or separable, state. This paper gives the first systematic and comprehensive treatment of this (bipartite) quantum separability problem, focusing on its deterministic (as opposed to randomized) computational complexity. First, I review the one-sided tests for separability, paying particular attention to the semidefinite programming methods. Then, I discuss various ways of formulating the quantum separability problem, from exact to approximate formulations, the latter of which are the paper's main focus. I then give a thorough treatment of the problem's relationship with the complexity classes NP, NP-complete, and co-NP. I also discuss extensions of Gurvits' NP-hardness result to strong NP-hardness of certain related problems. A major open question is whether the NP-contained formulation (QSEP) of the quantum separability problem is Karp-NP-complete; QSEP may be the first natural example of a problem that is Turing-NP-complete but not Karp-NP-complete. Finally, I survey all the proposed (deterministic) algorithms for the quantum separability problem, including the bounded search for symmetric extensions (via semidefinite programming), based on the recent quantum de Finetti theorem; and the entanglement-witness search (via interior-point algorithms and global optimization). These two algorithms have the lowest complexity, with the latter being the best under advice of asymptotically optimal point-coverings of the sphere.",
        "published": "2006-03-22T18:54:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0603199v7",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Complexity of Consistent Query Answering in Databases under   Cardinality-Based and Incremental Repair Semantics",
        "authors": [
            "Andrei Lopatenko",
            "Leopoldo Bertossi"
        ],
        "summary": "Consistent Query Answering (CQA) is the problem of computing from a database the answers to a query that are consistent with respect to certain integrity constraints that the database, as a whole, may fail to satisfy. Consistent answers have been characterized as those that are invariant under certain minimal forms of restoration of the database consistency. We investigate algorithmic and complexity theoretic issues of CQA under database repairs that minimally depart -wrt the cardinality of the symmetric difference- from the original database. We obtain first tight complexity bounds.   We also address the problem of incremental complexity of CQA, that naturally occurs when an originally consistent database becomes inconsistent after the execution of a sequence of update operations. Tight bounds on incremental complexity are provided for various semantics under denial constraints. Fixed parameter tractability is also investigated in this dynamic context, where the size of the update sequence becomes the relevant parameter.",
        "published": "2006-04-02T03:07:09Z",
        "link": "http://arxiv.org/abs/cs/0604002v1",
        "categories": [
            "cs.DB",
            "cs.CC"
        ]
    },
    {
        "title": "Hypercomputing the Mandelbrot Set?",
        "authors": [
            "Petrus H. Potgieter"
        ],
        "summary": "The Mandelbrot set is an extremely well-known mathematical object that can be described in a quite simple way but has very interesting and non-trivial properties. This paper surveys some results that are known concerning the (non-)computability of the set. It considers two models of decidability over the reals (which have been treated much more thoroughly and technically by Hertling (2005), Blum, Shub and Smale, Brattka (2003) and Weihrauch (1999 and 2003) among others), two over the computable reals (the Russian school and hypercomputation) and a model over the rationals.",
        "published": "2006-04-02T15:31:32Z",
        "link": "http://arxiv.org/abs/cs/0604003v1",
        "categories": [
            "cs.CC",
            "F.1.1"
        ]
    },
    {
        "title": "On the Complexity of Limit Sets of Cellular Automata Associated with   Probability Measures",
        "authors": [
            "Laurent Boyer",
            "Victor Poupet",
            "Guillaume Theyssier"
        ],
        "summary": "We study the notion of limit sets of cellular automata associated with probability measures (mu-limit sets). This notion was introduced by P. Kurka and A. Maass. It is a refinement of the classical notion of omega-limit sets dealing with the typical long term behavior of cellular automata. It focuses on the words whose probability of appearance does not tend to 0 as time tends to infinity (the persistent words). In this paper, we give a characterisation of the persistent language for non sensible cellular automata associated with Bernouilli measures. We also study the computational complexity of these languages. We show that the persistent language can be non-recursive. But our main result is that the set of quasi-nilpotent cellular automata (those with a single configuration in their mu-limit set) is neither recursively enumerable nor co-recursively enumerable.",
        "published": "2006-04-04T10:21:36Z",
        "link": "http://arxiv.org/abs/cs/0604007v2",
        "categories": [
            "cs.DM",
            "cs.CC",
            "math.DS"
        ]
    },
    {
        "title": "Towards Analog Reverse Time Computation",
        "authors": [
            "O. Habibi",
            "U. R. Patihnedj",
            "M. O. Dhar"
        ],
        "summary": "We report the consequences of a destabilization process on a simulated General Purpose Analog Computer. This new technology overcomes problems linked with serial ambiguity, and provides an analog bias to encode algorithms whose complexity is over polynomial. We also implicitly demonstrate how countermesures of the Stochastic Aperture Degeneracy could efficiently reach higher computational classes, and would open a road towards Analog Reverse Time Computation.",
        "published": "2006-04-05T23:40:56Z",
        "link": "http://arxiv.org/abs/cs/0604014v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Approximation Algorithms for Restricted Cycle Covers Based on Cycle   Decompositions",
        "authors": [
            "Bodo Manthey"
        ],
        "summary": "A cycle cover of a graph is a set of cycles such that every vertex is part of exactly one cycle. An L-cycle cover is a cycle cover in which the length of every cycle is in the set L. The weight of a cycle cover of an edge-weighted graph is the sum of the weights of its edges.   We come close to settling the complexity and approximability of computing L-cycle covers. On the one hand, we show that for almost all L, computing L-cycle covers of maximum weight in directed and undirected graphs is APX-hard and NP-hard. Most of our hardness results hold even if the edge weights are restricted to zero and one.   On the other hand, we show that the problem of computing L-cycle covers of maximum weight can be approximated within a factor of 2 for undirected graphs and within a factor of 8/3 in the case of directed graphs. This holds for arbitrary sets L.",
        "published": "2006-04-06T13:53:25Z",
        "link": "http://arxiv.org/abs/cs/0604020v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Cohomology in Grothendieck Topologies and Lower Bounds in Boolean   Complexity II: A Simple Example",
        "authors": [
            "Joel Friedman"
        ],
        "summary": "In a previous paper we have suggested a number of ideas to attack circuit size complexity with cohomology. As a simple example, we take circuits that can only compute the AND of two inputs, which essentially reduces to SET COVER. We show a very special case of the cohomological approach (one particular free category, using injective and superskyscraper sheaves) gives the linear programming bound coming from the relaxation of the standard integer programming reformulation of SET COVER.",
        "published": "2006-04-06T20:07:42Z",
        "link": "http://arxiv.org/abs/cs/0604024v1",
        "categories": [
            "cs.CC",
            "math.AG"
        ]
    },
    {
        "title": "On the Role of Shared Entanglement",
        "authors": [
            "Dmytro Gavinsky"
        ],
        "summary": "Despite the apparent similarity between shared randomness and shared entanglement in the context of Communication Complexity, our understanding of the latter is not as good as of the former. In particular, there is no known \"entanglement analogue\" for the famous theorem by Newman, saying that the number of shared random bits required for solving any communication problem can be at most logarithmic in the input length (i.e., using more than O(log n) shared random bits would not reduce the complexity of an optimal solution).   In this paper we prove that the same is not true for entanglement. We establish a wide range of tight (up to a polylogarithmic factor) entanglement vs. communication tradeoffs for relational problems. The low end is: for any t>2, reducing shared entanglement from log^t(n) to o(log^{t-2}(n)) qubits can increase the communication required for solving a problem almost exponentially, from O(log^t(n)) to \\Omega(\\sqrt n). The high end is: for any \\eps>0, reducing shared entanglement from n^{1-\\eps}log(n) to o(n^{1-\\eps}/log(n)) can increase the required communication from O(n^{1-\\eps}log(n)) to \\Omega(n^{1-\\eps/2}/log(n)). The upper bounds are demonstrated via protocols which are exact and work in the \\e{simultaneous message passing model}, while the lower bounds hold for \\e{bounded-error protocols}, even in the more powerful \\e{model of 1-way communication}. Our protocols use shared EPR pairs while the lower bounds apply to any sort of prior entanglement.   We base the lower bounds on a strong direct product theorem for communication complexity of a certain class of relational problems. We believe that the theorem might have applications outside the scope of this work.",
        "published": "2006-04-08T02:44:26Z",
        "link": "http://arxiv.org/abs/quant-ph/0604052v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum Versus Classical Proofs and Advice",
        "authors": [
            "Scott Aaronson",
            "Greg Kuperberg"
        ],
        "summary": "This paper studies whether quantum proofs are more powerful than classical proofs, or in complexity terms, whether QMA=QCMA. We prove three results about this question. First, we give a \"quantum oracle separation\" between QMA and QCMA. More concretely, we show that any quantum algorithm needs $\\Omega(\\sqrt{2^n/(m+1)})$ queries to find an $n$-qubit \"marked state\" $\\lvert\\psi\\rangle$, even if given an $m$-bit classical description of $\\lvert\\psi\\rangle$ together with a quantum black box that recognizes $\\lvert\\psi\\rangle$. Second, we give an explicit QCMA protocol that nearly achieves this lower bound. Third, we show that, in the one previously-known case where quantum proofs seemed to provide an exponential advantage, classical proofs are basically just as powerful. In particular, Watrous gave a QMA protocol for verifying non-membership in finite groups. Under plausible group-theoretic assumptions, we give a QCMA protocol for the same problem. Even with no assumptions, our protocol makes only polynomially many queries to the group oracle. We end with some conjectures about quantum versus classical oracles, and about the possibility of a classical oracle separation between QMA and QCMA.",
        "published": "2006-04-10T02:29:16Z",
        "link": "http://arxiv.org/abs/quant-ph/0604056v4",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Efficient algorithms for deciding the type of growth of products of   integer matrices",
        "authors": [
            "Raphaël Jungers",
            "Vladimir Protasov",
            "Vincent D. Blondel"
        ],
        "summary": "For a given finite set $\\Sigma$ of matrices with nonnegative integer entries we study the growth of $$ \\max_t(\\Sigma) = \\max\\{\\|A_{1}... A_{t}\\|: A_i \\in \\Sigma\\}.$$ We show how to determine in polynomial time whether the growth with $t$ is bounded, polynomial, or exponential, and we characterize precisely all possible behaviors.",
        "published": "2006-04-11T14:10:37Z",
        "link": "http://arxiv.org/abs/cs/0604047v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Structural Alignments of pseudo-knotted RNA-molecules in polynomial time",
        "authors": [
            "Michael Brinkmeier"
        ],
        "summary": "An RNA molecule is structured on several layers. The primary and most obvious structure is its sequence of bases, i.e. a word over the alphabet {A,C,G,U}. The higher structure is a set of one-to-one base-pairings resulting in a two-dimensional folding of the one-dimensional sequence. One speaks of a secondary structure if these pairings do not cross and of a tertiary structure otherwise.   Since the folding of the molecule is important for its function, the search for related RNA molecules should not only be restricted to the primary structure. It seems sensible to incorporate the higher structures in the search. Based on this assumption and certain edit-operations a distance between two arbitrary structures can be defined. It is known that the general calculation of this measure is NP-complete \\cite{zhang02similarity}. But for some special cases polynomial algorithms are known. Using a new formal description of secondary and tertiary structures, we extend the class of structures for which the distance can be calculated in polynomial time. In addition the presented algorithm may be used to approximate the edit-distance between two arbitrary structures with a constant ratio.",
        "published": "2006-04-12T06:46:04Z",
        "link": "http://arxiv.org/abs/cs/0604051v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Solving Classical String Problems on Compressed Texts",
        "authors": [
            "Yury Lifshits"
        ],
        "summary": "Here we study the complexity of string problems as a function of the size of a program that generates input. We consider straight-line programs (SLP), since all algorithms on SLP-generated strings could be applied to processing LZ-compressed texts.   The main result is a new algorithm for pattern matching when both a text T and a pattern P are presented by SLPs (so-called fully compressed pattern matching problem). We show how to find a first occurrence, count all occurrences, check whether any given position is an occurrence or not in time O(n^2m). Here m,n are the sizes of straight-line programs generating correspondingly P and T.   Then we present polynomial algorithms for computing fingerprint table and compressed representation of all covers (for the first time) and for finding periods of a given compressed string (our algorithm is faster than previously known). On the other hand, we show that computing the Hamming distance between two SLP-generated strings is NP- and coNP-hard.",
        "published": "2006-04-13T08:12:39Z",
        "link": "http://arxiv.org/abs/cs/0604058v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "E.4; F.2.2; I.7"
        ]
    },
    {
        "title": "Univariate polynomial real root isolation: Continued Fractions revisited",
        "authors": [
            "Elias P. Tsigaridas",
            "Ioannis Z. Emiris"
        ],
        "summary": "We present algorithmic, complexity and implementation results concerning real root isolation of integer univariate polynomials using the continued fraction expansion of real algebraic numbers. One motivation is to explain the method's good performance in practice. We improve the previously known bound by a factor of $d \\tau$, where $d$ is the polynomial degree and $\\tau$ bounds the coefficient bitsize, thus matching the current record complexity for real root isolation by exact methods. Namely, the complexity bound is $\\sOB(d^4 \\tau^2)$ using the standard bound on the expected bitsize of the integers in the continued fraction expansion. We show how to compute the multiplicities within the same complexity and extend the algorithm to non square-free polynomials. Finally, we present an efficient open-source \\texttt{C++} implementation in the algebraic library \\synaps, and illustrate its efficiency as compared to other available software. We use polynomials with coefficient bitsize up to 8000 and degree up to 1000.",
        "published": "2006-04-17T10:52:35Z",
        "link": "http://arxiv.org/abs/cs/0604066v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "cs.MS"
        ]
    },
    {
        "title": "The Kesten-Stigum Reconstruction Bound Is Tight for Roughly Symmetric   Binary Channels",
        "authors": [
            "Christian Borgs",
            "Jennifer Chayes",
            "Elchanan Mossel",
            "Sebastien Roch"
        ],
        "summary": "We establish the exact threshold for the reconstruction problem for a binary asymmetric channel on the b-ary tree, provided that the asymmetry is sufficiently small. This is the first exact reconstruction threshold obtained in roughly a decade. We discuss the implications of our result for Glauber dynamics, phylogenetic reconstruction, and so-called ``replica symmetry breaking'' in spin glasses and random satisfiability problems.",
        "published": "2006-04-17T10:59:09Z",
        "link": "http://arxiv.org/abs/math/0604366v1",
        "categories": [
            "math.PR",
            "cs.CC",
            "q-bio.PE"
        ]
    },
    {
        "title": "Constructing Non-Computable Julia Sets",
        "authors": [
            "Mark Braverman",
            "Michael Yampolsky"
        ],
        "summary": "We completely characterize the conformal radii of Siegel disks in the family $$P_\\theta(z)=e^{2\\pi i\\theta}z+z^2,$$ corresponding to {\\bf computable} parameters $\\theta$. As a consequence, we constructively produce quadratic polynomials with {\\bf non-computable} Julia sets.",
        "published": "2006-04-17T18:16:05Z",
        "link": "http://arxiv.org/abs/math/0604371v1",
        "categories": [
            "math.DS",
            "cs.CC",
            "37F50"
        ]
    },
    {
        "title": "Complexity and Philosophy",
        "authors": [
            "Francis Heylighen",
            "Paul Cilliers",
            "Carlos Gershenson"
        ],
        "summary": "The science of complexity is based on a new way of thinking that stands in sharp contrast to the philosophy underlying Newtonian science, which is based on reductionism, determinism, and objective knowledge. This paper reviews the historical development of this new world view, focusing on its philosophical foundations. Determinism was challenged by quantum mechanics and chaos theory. Systems theory replaced reductionism by a scientifically based holism. Cybernetics and postmodern social science showed that knowledge is intrinsically subjective. These developments are being integrated under the header of \"complexity science\". Its central paradigm is the multi-agent system. Agents are intrinsically subjective and uncertain about their environment and future, but out of their local interactions, a global organization emerges. Although different philosophers, and in particular the postmodernists, have voiced similar ideas, the paradigm of complexity still needs to be fully assimilated by philosophy. This will throw a new light on old philosophical issues such as relativism, ethics and the role of the subject.",
        "published": "2006-04-19T11:12:38Z",
        "link": "http://arxiv.org/abs/cs/0604072v1",
        "categories": [
            "cs.CC",
            "cond-mat.other"
        ]
    },
    {
        "title": "One-in-Two-Matching Problem is NP-complete",
        "authors": [
            "Sergio Caracciolo",
            "Davide Fichera",
            "Andrea Sportiello"
        ],
        "summary": "2-dimensional Matching Problem, which requires to find a matching of left- to right-vertices in a balanced $2n$-vertex bipartite graph, is a well-known polynomial problem, while various variants, like the 3-dimensional analogoue (3DM, with triangles on a tripartite graph), or the Hamiltonian Circuit Problem (HC, a restriction to ``unicyclic'' matchings) are among the main examples of NP-hard problems, since the first Karp reduction series of 1972. The same holds for the weighted variants of these problems, the Linear Assignment Problem being polynomial, and the Numerical 3-Dimensional Matching and Travelling Salesman Problem being NP-complete.   In this paper we show that a small modification of the 2-dimensional Matching and Assignment Problems in which for each $i \\leq n/2$ it is required that either $\\pi(2i-1)=2i-1$ or $\\pi(2i)=2i$, is a NP-complete problem. The proof is by linear reduction from SAT (or NAE-SAT), with the size $n$ of the Matching Problem being four times the number of edges in the factor graph representation of the boolean problem. As a corollary, in combination with the simple linear reduction of One-in-Two Matching to 3-Dimensional Matching, we show that SAT can be linearly reduced to 3DM, while the original Karp reduction was only cubic.",
        "published": "2006-04-28T16:40:09Z",
        "link": "http://arxiv.org/abs/cs/0604113v1",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "A New Cryptosystem Based On Hidden Order Groups",
        "authors": [
            "Amitabh Saxena",
            "Ben Soh"
        ],
        "summary": "Let $G_1$ be a cyclic multiplicative group of order $n$. It is known that the Diffie-Hellman problem is random self-reducible in $G_1$ with respect to a fixed generator $g$ if $\\phi(n)$ is known. That is, given $g, g^x\\in G_1$ and having oracle access to a `Diffie-Hellman Problem' solver with fixed generator $g$, it is possible to compute $g^{1/x} \\in G_1$ in polynomial time (see theorem 3.2). On the other hand, it is not known if such a reduction exists when $\\phi(n)$ is unknown (see conjuncture 3.1). We exploit this ``gap'' to construct a cryptosystem based on hidden order groups and present a practical implementation of a novel cryptographic primitive called an \\emph{Oracle Strong Associative One-Way Function} (O-SAOWF). O-SAOWFs have applications in multiparty protocols. We demonstrate this by presenting a key agreement protocol for dynamic ad-hoc groups.",
        "published": "2006-04-30T18:13:10Z",
        "link": "http://arxiv.org/abs/cs/0605003v4",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "N-Fold Integer Programming",
        "authors": [
            "Jesús A. De Loera",
            "Raymond Hemmecke",
            "Shmuel Onn",
            "Robert Weismantel"
        ],
        "summary": "In this article we study a broad class of integer programming problems in variable dimension. We show that these so-termed {\\em n-fold integer programming problems} are polynomial time solvable. Our proof involves two heavy ingredients discovered recently: the equivalence of linear optimization and so-called directed augmentation, and the stabilization of certain Graver bases.   We discuss several applications of our algorithm to multiway transportation problems and to packing problems. One important consequence of our results is a polynomial time algorithm for the $d$-dimensional integer transportation problem for long multiway tables. Another interesting application is a new algorithm for the classical cutting stock problem.",
        "published": "2006-05-09T20:33:41Z",
        "link": "http://arxiv.org/abs/math/0605242v1",
        "categories": [
            "math.OC",
            "cs.CC",
            "cs.DM",
            "math.CO",
            "05A; 15A; 51M; 52A; 52B; 52C; 68Q; 68R; 68U; 90B; 90C"
        ]
    },
    {
        "title": "On Learning Thresholds of Parities and Unions of Rectangles in Random   Walk Models",
        "authors": [
            "S. Roch"
        ],
        "summary": "In a recent breakthrough, [Bshouty et al., 2005] obtained the first passive-learning algorithm for DNFs under the uniform distribution. They showed that DNFs are learnable in the Random Walk and Noise Sensitivity models. We extend their results in several directions. We first show that thresholds of parities, a natural class encompassing DNFs, cannot be learned efficiently in the Noise Sensitivity model using only statistical queries. In contrast, we show that a cyclic version of the Random Walk model allows to learn efficiently polynomially weighted thresholds of parities. We also extend the algorithm of Bshouty et al. to the case of Unions of Rectangles, a natural generalization of DNFs to $\\{0,...,b-1\\}^n$.",
        "published": "2006-05-11T03:27:12Z",
        "link": "http://arxiv.org/abs/cs/0605048v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "A Polynomial Time Nilpotence Test for Galois Groups and Related Results",
        "authors": [
            "V. Arvind",
            "Piyush P Kurur"
        ],
        "summary": "We give a deterministic polynomial-time algorithm to check whether the Galois group $\\Gal{f}$ of an input polynomial $f(X) \\in \\Q[X]$ is nilpotent: the running time is polynomial in $\\size{f}$. Also, we generalize the Landau-Miller solvability test to an algorithm that tests if $\\Gal{f}$ is in $\\Gamma_d$: this algorithm runs in time polynomial in $\\size{f}$ and $n^d$ and, moreover, if $\\Gal{f}\\in\\Gamma_d$ it computes all the prime factors of $# \\Gal{f}$.",
        "published": "2006-05-11T08:20:44Z",
        "link": "http://arxiv.org/abs/cs/0605050v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Modal Logics of Topological Relations",
        "authors": [
            "Carsten Lutz",
            "Frank Wolter"
        ],
        "summary": "Logical formalisms for reasoning about relations between spatial regions play a fundamental role in geographical information systems, spatial and constraint databases, and spatial reasoning in AI. In analogy with Halpern and Shoham's modal logic of time intervals based on the Allen relations, we introduce a family of modal logics equipped with eight modal operators that are interpreted by the Egenhofer-Franzosa (or RCC8) relations between regions in topological spaces such as the real plane. We investigate the expressive power and computational complexity of logics obtained in this way. It turns out that our modal logics have the same expressive power as the two-variable fragment of first-order logic, but are exponentially less succinct. The complexity ranges from (undecidable and) recursively enumerable to highly undecidable, where the recursively enumerable logics are obtained by considering substructures of structures induced by topological spaces. As our undecidability results also capture logics based on the real line, they improve upon undecidability results for interval temporal logics by Halpern and Shoham. We also analyze modal logics based on the five RCC5 relations, with similar results regarding the expressive power, but weaker results regarding the complexity.",
        "published": "2006-05-15T14:42:36Z",
        "link": "http://arxiv.org/abs/cs/0605064v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1; H.2.8; I.2.4"
        ]
    },
    {
        "title": "On the possible Computational Power of the Human Mind",
        "authors": [
            "Hector Zenil",
            "Francisco Hernandez-Quiroz"
        ],
        "summary": "The aim of this paper is to address the question: Can an artificial neural network (ANN) model be used as a possible characterization of the power of the human mind? We will discuss what might be the relationship between such a model and its natural counterpart. A possible characterization of the different power capabilities of the mind is suggested in terms of the information contained (in its computational complexity) or achievable by it. Such characterization takes advantage of recent results based on natural neural networks (NNN) and the computational power of arbitrary artificial neural networks (ANN). The possible acceptance of neural networks as the model of the human mind's operation makes the aforementioned quite relevant.",
        "published": "2006-05-15T17:56:55Z",
        "link": "http://arxiv.org/abs/cs/0605065v4",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Some elementary rigorous remark about the replica formalism in the   Statistical Physics' approach to threshold phenomena in Computational   Complexity Theory",
        "authors": [
            "Gavriel Segre"
        ],
        "summary": "Some elementary rigorous remark about the replica formalism in the Statistical Physics' approach to threshold phenomena in Computational Complexity Theory is presented.",
        "published": "2006-05-16T19:09:43Z",
        "link": "http://arxiv.org/abs/math-ph/0605049v3",
        "categories": [
            "math-ph",
            "cond-mat.stat-mech",
            "cs.CC",
            "math.MP"
        ]
    },
    {
        "title": "The BQP-hardness of approximating the Jones Polynomial",
        "authors": [
            "Dorit Aharonov",
            "Itai Arad"
        ],
        "summary": "A celebrated important result due to Freedman, Larsen and Wang states that providing additive approximations of the Jones polynomial at the k'th root of unity, for constant k=5 and k>6, is BQP-hard. Together with the algorithmic results of Freedman et al and Aharonov et al, this gives perhaps the most natural BQP-complete problem known today and motivates further study of the topic. In this paper we focus on the universality proof; we extend the universality result of Freedman et al to k's that grow polynomially with the number of strands and crossings in the link, thus extending the BQP-hardness of Jones polynomial approximations to all values for which the AJL algorithm applies, proving that for all those values, the problems are BQP-complete. As a side benefit, we derive a fairly elementary proof of the Freedman et al density result, without referring to advanced results from Lie algebra representation theory, making this important result accessible to computer science audience. We make use of two general lemmas we prove, the Bridge lemma and the Decoupling lemma, which provide tools for establishing density of subgroups in SU(n). Those tools seem to be of independent interest in more general contexts of proving quantum universality. Our result also implies a completely classical statement, that the_multiplicative_ approximations of the Jones polynomial, at exactly the same values, are #P-hard, via a recent result due to Kuperberg. Since the first publication of those results in their preliminary form (arXiv:quant-ph/0605181v2), the methods we present here were used in several other contexts. This paper is an improved and extended version of the original results, and also includes discussions of the developments since then.",
        "published": "2006-05-21T21:39:38Z",
        "link": "http://arxiv.org/abs/quant-ph/0605181v5",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Proof Search in Hajek's Basic Logic",
        "authors": [
            "S. Bova",
            "F. Montagna"
        ],
        "summary": "We introduce a proof system for Hajek's logic BL based on a relational hypersequents framework. We prove that the rules of our logical calculus, called RHBL, are sound and invertible with respect to any valuation of BL into a suitable algebra, called omega[0,1]. Refining the notion of reduction tree that arises naturally from RHBL, we obtain a decision algorithm for BL provability whose running time upper bound is 2^O(n), where n is the number of connectives of the input formula. Moreover, if a formula is unprovable, we exploit the constructiveness of a polynomial time algorithm for leaves validity for providing a procedure to build countermodels in omega[0,1]. Finally, since the size of the reduction tree branches is O(n^3), we can describe a polynomial time verification algorithm for BL unprovability.",
        "published": "2006-05-22T08:18:56Z",
        "link": "http://arxiv.org/abs/cs/0605094v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Nonlinear Bipartite Matching",
        "authors": [
            "Yael Berstein",
            "Shmuel Onn"
        ],
        "summary": "We study the problem of optimizing nonlinear objective functions over bipartite matchings. While the problem is generally intractable, we provide several efficient algorithms for it, including a deterministic algorithm for maximizing convex objectives, approximative algorithms for norm minimization and maximization, and a randomized algorithm for optimizing arbitrary objectives.",
        "published": "2006-05-23T11:03:05Z",
        "link": "http://arxiv.org/abs/math/0605610v1",
        "categories": [
            "math.OC",
            "cs.CC",
            "cs.DM",
            "math.CO",
            "05A; 15A; 51M; 52A; 52B; 52C; 68Q; 68R; 68U; 90B; 90C"
        ]
    },
    {
        "title": "Inapproximability of the Tutte polynomial",
        "authors": [
            "Leslie Ann Goldberg",
            "Mark Jerrum"
        ],
        "summary": "The Tutte polynomial of a graph G is a two-variable polynomial T(G;x,y) that encodes many interesting properties of the graph. We study the complexity of the following problem, for rationals x and y: take as input a graph G, and output a value which is a good approximation to T(G;x,y). Jaeger, Vertigan and Welsh have completely mapped the complexity of exactly computing the Tutte polynomial. They have shown that this is #P-hard, except along the hyperbola (x-1)(y-1)=1 and at four special points. We are interested in determining for which points (x,y) there is a \"fully polynomial randomised approximation scheme\" (FPRAS) for T(G;x,y). Under the assumption RP is not equal to NP, we prove that there is no FPRAS at (x,y) if (x,y) is in one of the half-planes x<-1 or y<-1 (excluding the easy-to-compute cases mentioned above). Two exceptions to this result are the half-line x<-1, y=1 (which is still open) and the portion of the hyperbola (x-1)(y-1)=2 corresponding to y<-1 which we show to be equivalent in difficulty to approximately counting perfect matchings. We give further intractability results for (x,y) in the vicinity of the origin. A corollary of our results is that, under the assumption RP is not equal to NP, there is no FPRAS at the point (x,y)=(0,1--lambda) when \\lambda>2 is a positive integer. Thus there is no FPRAS for counting nowhere-zero \\lambda flows for \\lambda>2. This is an interesting consequence of our work since the corresponding decision problem is in P for example for \\lambda=6.",
        "published": "2006-05-30T11:48:20Z",
        "link": "http://arxiv.org/abs/cs/0605140v2",
        "categories": [
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "The Consequences of Eliminating NP Solutions",
        "authors": [
            "Piotr Faliszewski",
            "Lane A. Hemaspaandra"
        ],
        "summary": "Given a function based on the computation of an NP machine, can one in general eliminate some solutions? That is, can one in general decrease the ambiguity? This simple question remains, even after extensive study by many researchers over many years, mostly unanswered. However, complexity-theoretic consequences and enabling conditions are known. In this tutorial-style article we look at some of those, focusing on the most natural framings: reducing the number of solutions of NP functions, refining the solutions of NP functions, and subtracting from or otherwise shrinking #P functions. We will see how small advice strings are important here, but we also will see how increasing advice size to achieve robustness is central to the proof of a key ambiguity-reduction result for NP functions.",
        "published": "2006-06-02T01:34:26Z",
        "link": "http://arxiv.org/abs/cs/0606009v4",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2; F.1.1"
        ]
    },
    {
        "title": "On the communication between cells of a cellular automaton on the penta-   and heptagrids of the hyperbolic plane",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "This contribution belongs to a combinatorial approach to hyperbolic geometry and it is aimed at possible applications to computer simulations.   It is based on the splitting method which was introduced by the author and which is reminded in the second section of the paper. Then we sketchily remind the application to the classical case of the pentagrid, i.e. the tiling of the hyperbolic plane which is generated by reflections of the regular rectangular pentagon in its sides and, recursively, of its images in their sides. From this application, we derived a system of coordinates to locate the tiles, allowing an implementation of cellular automata.   At the software level, cells exchange messages thanks to a new representation which improves the speed of contacts between cells. In the new setting, communications are exchanged along actual geodesics and the contribution of the cellular automaton is also linear in the coordinates of the cells.",
        "published": "2006-06-02T07:34:17Z",
        "link": "http://arxiv.org/abs/cs/0606012v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "Simplifying Random Satisfiability Problem by Removing Frustrating   Interactions",
        "authors": [
            "A. Ramezanpour",
            "S. Moghimi-Araghi"
        ],
        "summary": "How can we remove some interactions in a constraint satisfaction problem (CSP) such that it still remains satisfiable? In this paper we study a modified survey propagation algorithm that enables us to address this question for a prototypical CSP, i.e. random K-satisfiability problem. The average number of removed interactions is controlled by a tuning parameter in the algorithm. If the original problem is satisfiable then we are able to construct satisfiable subproblems ranging from the original one to a minimal one with minimum possible number of interactions. The minimal satisfiable subproblems will provide directly the solutions of the original problem.",
        "published": "2006-06-06T15:48:35Z",
        "link": "http://arxiv.org/abs/cond-mat/0606128v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Natural Halting Probabilities, Partial Randomness, and Zeta Functions",
        "authors": [
            "Cristian S. Calude",
            "Michael A. Stay"
        ],
        "summary": "We introduce the zeta number, natural halting probability and natural complexity of a Turing machine and we relate them to Chaitin's Omega number, halting probability, and program-size complexity. A classification of Turing machines according to their zeta numbers is proposed: divergent, convergent and tuatara. We prove the existence of universal convergent and tuatara machines. Various results on (algorithmic) randomness and partial randomness are proved. For example, we show that the zeta number of a universal tuatara machine is c.e. and random. A new type of partial randomness, asymptotic randomness, is introduced. Finally we show that in contrast to classical (algorithmic) randomness--which cannot be naturally characterised in terms of plain complexity--asymptotic randomness admits such a characterisation.",
        "published": "2006-06-07T20:18:13Z",
        "link": "http://arxiv.org/abs/cs/0606033v2",
        "categories": [
            "cs.CC",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Exponential Separation of Quantum and Classical Online Space Complexity",
        "authors": [
            "Francois Le Gall"
        ],
        "summary": "Although quantum algorithms realizing an exponential time speed-up over the best known classical algorithms exist, no quantum algorithm is known performing computation using less space resources than classical algorithms. In this paper, we study, for the first time explicitly, space-bounded quantum algorithms for computational problems where the input is given not as a whole, but bit by bit. We show that there exist such problems that a quantum computer can solve using exponentially less work space than a classical computer. More precisely, we introduce a very natural and simple model of a space-bounded quantum online machine and prove an exponential separation of classical and quantum online space complexity, in the bounded-error setting and for a total language. The language we consider is inspired by a communication problem (the set intersection function) that Buhrman, Cleve and Wigderson used to show an almost quadratic separation of quantum and classical bounded-error communication complexity. We prove that, in the framework of online space complexity, the separation becomes exponential.",
        "published": "2006-06-08T04:27:30Z",
        "link": "http://arxiv.org/abs/quant-ph/0606066v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Average-Case Complexity",
        "authors": [
            "Andrej Bogdanov",
            "Luca Trevisan"
        ],
        "summary": "We survey the average-case complexity of problems in NP.   We discuss various notions of good-on-average algorithms, and present completeness results due to Impagliazzo and Levin. Such completeness results establish the fact that if a certain specific (but somewhat artificial) NP problem is easy-on-average with respect to the uniform distribution, then all problems in NP are easy-on-average with respect to all samplable distributions. Applying the theory to natural distributional problems remain an outstanding open question. We review some natural distributional problems whose average-case complexity is of particular interest and that do not yet fit into this theory.   A major open question whether the existence of hard-on-average problems in NP can be based on the P$\\neq$NP assumption or on related worst-case assumptions. We review negative results showing that certain proof techniques cannot prove such a result. While the relation between worst-case and average-case complexity for general NP problems remains open, there has been progress in understanding the relation between different \"degrees\" of average-case complexity. We discuss some of these \"hardness amplification\" results.",
        "published": "2006-06-08T18:40:21Z",
        "link": "http://arxiv.org/abs/cs/0606037v3",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Tight Bounds on the Complexity of Recognizing Odd-Ranked Elements",
        "authors": [
            "Shripad Thite"
        ],
        "summary": "Let S = <s_1, s_2, s_3, ..., s_n> be a given vector of n real numbers. The rank of a real z with respect to S is defined as the number of elements s_i in S such that s_i is less than or equal to z. We consider the following decision problem: determine whether the odd-numbered elements s_1, s_3, s_5, ... are precisely the elements of S whose rank with respect to S is odd. We prove a bound of Theta(n log n) on the number of operations required to solve this problem in the algebraic computation tree model.",
        "published": "2006-06-08T21:28:09Z",
        "link": "http://arxiv.org/abs/cs/0606038v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Approximation Algorithms for Multi-Criteria Traveling Salesman Problems",
        "authors": [
            "Bodo Manthey",
            "L. Shankar Ram"
        ],
        "summary": "In multi-criteria optimization problems, several objective functions have to be optimized. Since the different objective functions are usually in conflict with each other, one cannot consider only one particular solution as the optimal solution. Instead, the aim is to compute a so-called Pareto curve of solutions. Since Pareto curves cannot be computed efficiently in general, we have to be content with approximations to them.   We design a deterministic polynomial-time algorithm for multi-criteria g-metric STSP that computes (min{1 +g, 2g^2/(2g^2 -2g +1)} + eps)-approximate Pareto curves for all 1/2<=g<=1. In particular, we obtain a (2+eps)-approximation for multi-criteria metric STSP. We also present two randomized approximation algorithms for multi-criteria g-metric STSP that achieve approximation ratios of (2g^3 +2g^2)/(3g^2 -2g +1) + eps and (1 +g)/(1 +3g -4g^2) + eps, respectively.   Moreover, we present randomized approximation algorithms for multi-criteria g-metric ATSP (ratio 1/2 + g^3/(1 -3g^2) + eps) for g < 1/sqrt(3)), STSP with weights 1 and 2 (ratio 4/3) and ATSP with weights 1 and 2 (ratio 3/2). To do this, we design randomized approximation schemes for multi-criteria cycle cover and graph factor problems.",
        "published": "2006-06-09T11:41:53Z",
        "link": "http://arxiv.org/abs/cs/0606040v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Schedule generation schemes for the job-shop problem with   sequence-dependent setup times: dominance properties and computational   analysis",
        "authors": [
            "Christian Artigues",
            "Pierre Lopez",
            "Pierre-Dimitri Ayache"
        ],
        "summary": "We consider the job-shop problem with sequence-dependent setup times. We focus on the formal definition of schedule generation schemes (SGSs) based on the semi-active, active, and non-delay schedule categories. We study dominance properties of the sets of schedules obtainable with each SGS. We show how the proposed SGSs can be used within single-pass and multi-pass priority rule based heuristics. We study several priority rules for the problem and provide a comparative computational analysis of the different SGSs on sets of instances taken from the literature. The proposed SGSs significantly improve previously best-known results on a set of hard benchmark instances.",
        "published": "2006-06-09T16:32:20Z",
        "link": "http://arxiv.org/abs/cs/0606043v1",
        "categories": [
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Syntactic Characterisations of Polynomial-Time Optimisation Classes   (Syntactic Characterizations of Polynomial-Time Optimization Classes)",
        "authors": [
            "Prabhu Manyem"
        ],
        "summary": "In Descriptive Complexity, there is a vast amount of literature on decision problems, and their classes such as \\textbf{P, NP, L and NL}. ~ However, research on the descriptive complexity of optimisation problems has been limited. Optimisation problems corresponding to the \\textbf{NP} class have been characterised in terms of logic expressions by Papadimitriou and Yannakakis, Panconesi and Ranjan, Kolaitis and Thakur, Khanna et al, and by Zimand. Gr\\\"{a}del characterised the polynomial class \\textbf{P} of decision problems. In this paper, we attempt to characterise the optimisation versions of \\textbf{P} via expressions in second order logic, many of them using universal Horn formulae with successor relations. The polynomially bound versions of maximisation (maximization) and minimisation (minimization) problems are treated first, and then the maximisation problems in the \"not necessarily polynomially bound\" class.",
        "published": "2006-06-12T00:42:29Z",
        "link": "http://arxiv.org/abs/cs/0606050v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3"
        ]
    },
    {
        "title": "Fast and Simple Methods For Computing Control Points",
        "authors": [
            "Jean Gallier",
            "Weqing Gu"
        ],
        "summary": "The purpose of this paper is to present simple and fast methods for computing control points for polynomial curves and polynomial surfaces given explicitly in terms of polynomials (written as sums of monomials). We give recurrence formulae w.r.t. arbitrary affine frames. As a corollary, it is amusing that we can also give closed-form expressions in the case of the frame (r, s) for curves, and the frame ((1, 0, 0), (0, 1, 0), (0, 0, 1) for surfaces. Our methods have the same low polynomial (time and space) complexity as the other best known algorithms, and are very easy to implement.",
        "published": "2006-06-13T00:47:34Z",
        "link": "http://arxiv.org/abs/cs/0606056v1",
        "categories": [
            "cs.CC",
            "cs.GR"
        ]
    },
    {
        "title": "Approximability of Bounded Occurrence Max Ones",
        "authors": [
            "Fredrik Kuivinen"
        ],
        "summary": "We study the approximability of Max Ones when the number of variable occurrences is bounded by a constant. For conservative constraint languages (i.e., when the unary relations are included) we give a complete classification when the number of occurrences is three or more and a partial classification when the bound is two.   For the non-conservative case we prove that it is either trivial or equivalent to the corresponding conservative problem under polynomial-time many-one reductions.",
        "published": "2006-06-13T06:44:21Z",
        "link": "http://arxiv.org/abs/cs/0606057v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Lower bounds and complete problems in nondeterministic linear time and   sublinear space complexity classes",
        "authors": [
            "Philippe Chapdelaine",
            "Etienne Grandjean"
        ],
        "summary": "Proving lower bounds remains the most difficult of tasks in computational complexity theory. In this paper, we show that whereas most natural NP-complete problems belong to NLIN (linear time on nondeterministic RAMs), some of them, typically the planar versions of many NP-complete problems are recognized by nondeterministic RAMs in linear time and sublinear space. The main results of this paper are the following: as the second author did for NLIN, we give exact logical characterizations of nondeterministic polynomial time-space complexity classes; we derive from them a class of problems, which are complete in these classes, and as a consequence of such a precise result and of some recent separation theorems using diagonalization, prove time-space lower bounds for these problems.",
        "published": "2006-06-13T08:10:15Z",
        "link": "http://arxiv.org/abs/cs/0606058v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "Improved Exponential Time Lower Bound of Knapsack Problem under BT model",
        "authors": [
            "Xin Li",
            "Tian Liu",
            "Han Peng",
            "Hongtao Sun",
            "Jiaqi Zhu"
        ],
        "summary": "M.Alekhnovich et al. recently have proposed a model of algorithms, called BT model, which covers Greedy, Backtrack and Simple Dynamic Programming methods and can be further divided into fixed, adaptive and fully adaptive three kinds, and have proved exponential time lower bounds of exact and approximation algorithms under adaptive BT model for Knapsack problem which are $\\Omega(2^{n/2}/\\sqrt n)=\\Omega(2^{0.5n}/\\sqrt n)$ and $\\Omega((1/\\epsilon)^{1/3.17})\\approx\\Omega((1/\\epsilon)^{0.315})$(for approximation ratio $1-\\epsilon$) respectively (M. Alekhovich, A. Borodin, J. Buresh-Oppenheim, R. Impagliazzo, A. Magen, and T. Pitassi, Toward a Model for Backtracking and Dynamic Programming, \\emph{Proceedings of Twentieth Annual IEEE Conference on Computational Complexity}, pp308-322, 2005). In this note, we slightly improved their lower bounds to $\\Omega(2^{(2-\\epsilon)n/3}/\\sqrt{n})\\approx \\Omega(2^{0.66n}/\\sqrt{n})$ and $\\Omega((1/\\epsilon)^{1/2.38})\\approx\\Omega((1/\\epsilon)^{0.420})$, and proposed as an open question what is the best achievable lower bounds for knapsack under adaptive BT models.",
        "published": "2006-06-14T07:54:14Z",
        "link": "http://arxiv.org/abs/cs/0606064v1",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Is there an Elegant Universal Theory of Prediction?",
        "authors": [
            "Shane Legg"
        ],
        "summary": "Solomonoff's inductive learning model is a powerful, universal and highly elegant theory of sequence prediction. Its critical flaw is that it is incomputable and thus cannot be used in practice. It is sometimes suggested that it may still be useful to help guide the development of very general and powerful theories of prediction which are computable. In this paper it is shown that although powerful algorithms exist, they are necessarily highly complex. This alone makes their theoretical analysis problematic, however it is further shown that beyond a moderate level of complexity the analysis runs into the deeper problem of Goedel incompleteness. This limits the power of mathematics to analyse and study prediction algorithms, and indeed intelligent systems in general.",
        "published": "2006-06-14T15:22:27Z",
        "link": "http://arxiv.org/abs/cs/0606070v1",
        "categories": [
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Dimension Extractors and Optimal Decompression",
        "authors": [
            "David Doty"
        ],
        "summary": "A *dimension extractor* is an algorithm designed to increase the effective dimension -- i.e., the amount of computational randomness -- of an infinite binary sequence, in order to turn a \"partially random\" sequence into a \"more random\" sequence. Extractors are exhibited for various effective dimensions, including constructive, computable, space-bounded, time-bounded, and finite-state dimension. Using similar techniques, the Kucera-Gacs theorem is examined from the perspective of decompression, by showing that every infinite sequence S is Turing reducible to a Martin-Loef random sequence R such that the asymptotic number of bits of R needed to compute n bits of S, divided by n, is precisely the constructive dimension of S, which is shown to be the optimal ratio of query bits to computed bits achievable with Turing reductions. The extractors and decompressors that are developed lead directly to new characterizations of some effective dimensions in terms of optimal decompression by Turing reductions.",
        "published": "2006-06-19T02:24:34Z",
        "link": "http://arxiv.org/abs/cs/0606078v2",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT",
            "F.1.3; E.4; H.1.1"
        ]
    },
    {
        "title": "On the structure of linear-time reducibility",
        "authors": [
            "Philippe Chapdelaine"
        ],
        "summary": "In 1975, Ladner showed that under the hypothesis that P is not equal to NP, there exists a language which is neither in P, nor NP-complete. This result was latter generalized by Schoning and several authors to various polynomial-time complexity classes. We show here that such results also apply to linear-time reductions on RAMs (resp. Turing machines), and hence allow for separation results in linear-time classes similar to Ladner's ones for polynomial time.",
        "published": "2006-06-19T08:48:58Z",
        "link": "http://arxiv.org/abs/cs/0606080v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "On symmetric sandpiles",
        "authors": [
            "Enrico Formenti",
            "Benoît Masson",
            "Theophilos Pisokas"
        ],
        "summary": "A symmetric version of the well-known SPM model for sandpiles is introduced. We prove that the new model has fixed point dynamics. Although there might be several fixed points, a precise description of the fixed points is given. Moreover, we provide a simple closed formula for counting the number of fixed points originated by initial conditions made of a single column of grains.",
        "published": "2006-06-29T09:53:05Z",
        "link": "http://arxiv.org/abs/cs/0606120v1",
        "categories": [
            "cs.CC",
            "cs.PF",
            "F.1.1; G.2.1"
        ]
    },
    {
        "title": "A Novel Application of Lifting Scheme for Multiresolution Correlation of   Complex Radar Signals",
        "authors": [
            "Chinmoy Bhattacharya",
            "P. R. Mahapatra"
        ],
        "summary": "The lifting scheme of discrete wavelet transform (DWT) is now quite well established as an efficient technique for image compression, and has been incorporated into the JPEG2000 standards. However, the potential of the lifting scheme has not been exploited in the context of correlationbased processing, such as encountered in radar applications. This paper presents a complete and consistent framework for the application of DWT for correlation of complex signals. In particular, lifting scheme factorization of biorthogonal filterbanks is carried out in dual analysis basis spaces for multiresolution correlation of complex radar signals in the DWT domain only. A causal formulation of lifting for orthogonal filterbank is also developed. The resulting parallel algorithms and consequent saving of computational effort are briefly dealt with.",
        "published": "2006-07-01T16:58:49Z",
        "link": "http://arxiv.org/abs/cs/0607001v1",
        "categories": [
            "cs.DC",
            "cs.CC"
        ]
    },
    {
        "title": "NP-completeness of 4-incidence colorability of semi-cubic graphs",
        "authors": [
            "Xueliang Li",
            "Jianhua Tu"
        ],
        "summary": "The incidence coloring conjecture, proposed by Brualdi and Massey in 1993, states that the incidence coloring number of every graph is at most ${\\it \\Delta}+2$, where ${\\it \\Delta}$ is the maximum degree of a graph. The conjecture was shown to be false in general by Guiduli in 1997, following the work of Algor and Alon. However, in 2005 Maydanskiy proved that the conjecture holds for any graph with ${\\it \\Delta}\\leq 3$. It is easily deduced that the incidence coloring number of a semi-cubic graph is 4 or 5. In this paper, we show that it is already NP-complete to determine if a semi-cubic graph is 4-incidence colorable, and therefore it is NP-complete to determine if a general graph is $k$-incidence colorable.",
        "published": "2006-07-04T08:19:26Z",
        "link": "http://arxiv.org/abs/math/0607071v1",
        "categories": [
            "math.CO",
            "cs.CC",
            "05C15; 68Q17"
        ]
    },
    {
        "title": "Planar Graphs: Logical Complexity and Parallel Isomorphism Tests",
        "authors": [
            "Oleg Verbitsky"
        ],
        "summary": "We prove that every triconnected planar graph is definable by a first order sentence that uses at most 15 variables and has quantifier depth at most $11\\log_2 n+43$. As a consequence, a canonic form of such graphs is computable in $AC^1$ by the 14-dimensional Weisfeiler-Lehman algorithm. This provides another way to show that the planar graph isomorphism is solvable in $AC^1$.",
        "published": "2006-07-08T10:37:52Z",
        "link": "http://arxiv.org/abs/cs/0607033v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "The Minimal Cost Algorithm for Off-Line Diagnosability of Discrete Event   Systems",
        "authors": [
            "Zhujun Fan"
        ],
        "summary": "The failure diagnosis for {\\it discrete event systems} (DESs) has been given considerable attention in recent years. Both on-line and off-line diagnostics in the framework of DESs was first considered by Lin Feng in 1994, and particularly an algorithm for diagnosability of DESs was presented. Motivated by some existing problems to be overcome in previous work, in this paper, we investigate the minimal cost algorithm for diagnosability of DESs.   More specifically: (i) we give a generic method for judging a system's off-line diagnosability, and the complexity of this algorithm is polynomial-time; (ii) and in particular, we present an algorithm of how to search for the minimal set in all observable event sets, whereas the previous algorithm may find {\\it non-minimal} one.",
        "published": "2006-07-09T04:55:03Z",
        "link": "http://arxiv.org/abs/cs/0607037v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "B.1.2; B.2.3; F.1.1; I.2.8"
        ]
    },
    {
        "title": "Combinatorial laplacians and positivity under partial transpose",
        "authors": [
            "Roland Hildebrand",
            "Stefano Mancini",
            "Simone Severini"
        ],
        "summary": "Density matrices of graphs are combinatorial laplacians normalized to have trace one (Braunstein \\emph{et al.} \\emph{Phys. Rev. A,} \\textbf{73}:1, 012320 (2006)). If the vertices of a graph are arranged as an array, then its density matrix carries a block structure with respect to which properties such as separability can be considered. We prove that the so-called degree-criterion, which was conjectured to be necessary and sufficient for separability of density matrices of graphs, is equivalent to the PPT-criterion. As such it is not sufficient for testing the separability of density matrices of graphs (we provide an explicit example). Nonetheless, we prove the sufficiency when one of the array dimensions has length two (for an alternative proof see Wu, \\emph{Phys. Lett. A}\\textbf{351} (2006), no. 1-2, 18--22).   Finally we derive a rational upper bound on the concurrence of density matrices of graphs and show that this bound is exact for graphs on four vertices.",
        "published": "2006-07-10T10:38:38Z",
        "link": "http://arxiv.org/abs/cs/0607036v3",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "A rigorous proof of the cavity method for counting matchings",
        "authors": [
            "Mohsen Bayati",
            "Chandra Nair"
        ],
        "summary": "In this paper we rigorously prove the validity of the cavity method for the problem of counting the number of matchings in graphs with large girth. Cavity method is an important heuristic developed by statistical physicists that has lead to the development of faster distributed algorithms for problems in various combinatorial optimization problems. The validity of the approach has been supported mostly by numerical simulations. In this paper we prove the validity of cavity method for the problem of counting matchings using rigorous techniques. We hope that these rigorous approaches will finally help us establish the validity of the cavity method in general.",
        "published": "2006-07-11T18:59:30Z",
        "link": "http://arxiv.org/abs/cond-mat/0607290v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Elementary Proof of a Theorem of Jean Ville",
        "authors": [
            "Elliott H. Lieb",
            "Daniel Osherson",
            "Scott Weinstein"
        ],
        "summary": "Considerable thought has been devoted to an adequate definition of the class of infinite, random binary sequences (the sort of sequence that almost certainly arises from flipping a fair coin indefinitely). The first mathematical exploration of this problem was due to R. Von Mises, and based on his concept of a \"selection function.\" A decisive objection to Von Mises' idea was formulated in a theorem offered by Jean Ville in 1939. It shows that some sequences admitted by Von Mises as \"random\" in fact manifest a certain kind of systematicity. Ville's proof is challenging, and an alternative approach has appeared only in condensed form. We attempt to provide an expanded version of the latter, alternative argument.",
        "published": "2006-07-11T19:52:56Z",
        "link": "http://arxiv.org/abs/cs/0607054v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Length-based cryptanalysis: The case of Thompson's Group",
        "authors": [
            "Dima Ruinskiy",
            "Adi Shamir",
            "Boaz Tsaban"
        ],
        "summary": "The length-based approach is a heuristic for solving randomly generated equations in groups which possess a reasonably behaved length function. We describe several improvements of the previously suggested length-based algorithms, that make them applicable to Thompson's group with significant success rates. In particular, this shows that the Shpilrain-Ushakov public key cryptosystem based on Thompson's group is insecure, and suggests that no practical public key cryptosystem based on this group can be secure.",
        "published": "2006-07-17T14:26:53Z",
        "link": "http://arxiv.org/abs/cs/0607079v4",
        "categories": [
            "cs.CR",
            "cs.CC",
            "math.GR"
        ]
    },
    {
        "title": "An Elegant Argument that P is not NP",
        "authors": [
            "Craig Alan Feinstein"
        ],
        "summary": "In this note, we present an elegant argument that P is not NP by demonstrating that the Meet-in-the-Middle algorithm must have the fastest running-time of all deterministic and exact algorithms which solve the SUBSET-SUM problem on a classical computer.",
        "published": "2006-07-19T19:01:09Z",
        "link": "http://arxiv.org/abs/cs/0607093v21",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of Quantified Constraint Satisfaction: Collapsibility,   Sink Algebras, and the Three-Element Case",
        "authors": [
            "Hubie Chen"
        ],
        "summary": "The constraint satisfaction probem (CSP) is a well-acknowledged framework in which many combinatorial search problems can be naturally formulated. The CSP may be viewed as the problem of deciding the truth of a logical sentence consisting of a conjunction of constraints, in front of which all variables are existentially quantified. The quantified constraint satisfaction problem (QCSP) is the generalization of the CSP where universal quantification is permitted in addition to existential quantification. The general intractability of these problems has motivated research studying the complexity of these problems under a restricted constraint language, which is a set of relations that can be used to express constraints.   This paper introduces collapsibility, a technique for deriving positive complexity results on the QCSP. In particular, this technique allows one to show that, for a particular constraint language, the QCSP reduces to the CSP. We show that collapsibility applies to three known tractable cases of the QCSP that were originally studied using disparate proof techniques in different decades: Quantified 2-SAT (Aspvall, Plass, and Tarjan 1979), Quantified Horn-SAT (Karpinski, Kleine B\\\"{u}ning, and Schmitt 1987), and Quantified Affine-SAT (Creignou, Khanna, and Sudan 2001). This reconciles and reveals common structure among these cases, which are describable by constraint languages over a two-element domain. In addition to unifying these known tractable cases, we study constraint languages over domains of larger size.",
        "published": "2006-07-24T16:03:56Z",
        "link": "http://arxiv.org/abs/cs/0607106v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "The one-way communication complexity of the Boolean Hidden Matching   Problem",
        "authors": [
            "Iordanis Kerenidis",
            "Ran Raz"
        ],
        "summary": "We give a tight lower bound of Omega(\\sqrt{n}) for the randomized one-way communication complexity of the Boolean Hidden Matching Problem [BJK04]. Since there is a quantum one-way communication complexity protocol of O(\\log n) qubits for this problem, we obtain an exponential separation of quantum and classical one-way communication complexity for partial functions. A similar result was independently obtained by Gavinsky, Kempe, de Wolf [GKdW06]. Our lower bound is obtained by Fourier analysis, using the Fourier coefficients inequality of Kahn Kalai and Linial [KKL88].",
        "published": "2006-07-25T10:07:29Z",
        "link": "http://arxiv.org/abs/quant-ph/0607173v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Exponential Separation of Quantum and Classical One-Way Communication   Complexity for a Boolean Function",
        "authors": [
            "Dmytro Gavinsky",
            "Julia Kempe",
            "Ronald de Wolf"
        ],
        "summary": "We give an exponential separation between one-way quantum and classical communication complexity for a Boolean function. Earlier such a separation was known only for a relation. A very similar result was obtained earlier but independently by Kerenidis and Raz [KR06]. Our version of the result gives an example in the bounded storage model of cryptography, where the key is secure if the adversary has a certain amount of classical storage, but is completely insecure if he has a similar amount of quantum storage.",
        "published": "2006-07-25T10:08:07Z",
        "link": "http://arxiv.org/abs/quant-ph/0607174v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Complexity and Applications of Edge-Induced Vertex-Cuts",
        "authors": [
            "Marko Samer",
            "Stefan Szeider"
        ],
        "summary": "Motivated by hypergraph decomposition algorithms, we introduce the notion of edge-induced vertex-cuts and compare it with the well-known notions of edge-cuts and vertex-cuts. We investigate the complexity of computing minimum edge-induced vertex-cuts and demonstrate the usefulness of our notion by applications in network reliability and constraint satisfaction.",
        "published": "2006-07-25T16:17:22Z",
        "link": "http://arxiv.org/abs/cs/0607109v2",
        "categories": [
            "cs.DM",
            "cs.CC",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "A new function algebra of EXPTIME functions by safe nested recursion",
        "authors": [
            "Toshiyasu Arai",
            "Naohi Eguchi"
        ],
        "summary": "Bellantoni and Cook have given a function-algebra characterization of the polynomial-time computable functions via an unbounded recursion scheme which is called safe recursion. Inspired by their work, we characterize the exponential-time computable functions with the use of a safe variant of nested recursion.",
        "published": "2006-07-27T06:34:53Z",
        "link": "http://arxiv.org/abs/cs/0607118v2",
        "categories": [
            "cs.CC",
            "F.4.1; F.1.1; F.1.3"
        ]
    },
    {
        "title": "A polynomial-time approximation algorithm for the number of k-matchings   in bipartite graphs",
        "authors": [
            "Shmuel Friedland",
            "Daniel Levy"
        ],
        "summary": "We show that the number of $k$-matching in a given undirected graph   $G$ is equal to the number of perfect matching of the corresponding graph   $G_k$ on an even number of vertices divided by a suitable factor.   If $G$ is bipartite then one can construct a bipartite $G_k$.   For bipartite graphs this result implies that the number of $k$-matching has a polynomial-time approximation algorithm. The above results are extended to permanents and hafnians of corresponding matrices.",
        "published": "2006-07-28T19:03:07Z",
        "link": "http://arxiv.org/abs/cs/0607135v1",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Parallel repetition: simplifications and the no-signaling case",
        "authors": [
            "Thomas Holenstein"
        ],
        "summary": "Consider a game where a refereed a referee chooses (x,y) according to a publicly known distribution P_XY, sends x to Alice, and y to Bob. Without communicating with each other, Alice responds with a value \"a\" and Bob responds with a value \"b\". Alice and Bob jointly win if a publicly known predicate Q(x,y,a,b) holds.   Let such a game be given and assume that the maximum probability that Alice and Bob can win is v<1. Raz (SIAM J. Comput. 27, 1998) shows that if the game is repeated n times in parallel, then the probability that Alice and Bob win all games simultaneously is at most v'^(n/log(s)), where s is the maximal number of possible responses from Alice and Bob in the initial game, and v' is a constant depending only on v.   In this work, we simplify Raz's proof in various ways and thus shorten it significantly. Further we study the case where Alice and Bob are not restricted to local computations and can use any strategy which does not imply communication among them.",
        "published": "2006-07-31T16:09:37Z",
        "link": "http://arxiv.org/abs/cs/0607139v3",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "Pull-Based Data Broadcast with Dependencies: Be Fair to Users, not to   Items",
        "authors": [
            "Julien Robert",
            "Nicolas Schabanel"
        ],
        "summary": "Broadcasting is known to be an efficient means of disseminating data in wireless communication environments (such as Satellite, mobile phone networks,...). It has been recently observed that the average service time of broadcast systems can be considerably improved by taking into consideration existing correlations between requests. We study a pull-based data broadcast system where users request possibly overlapping sets of items; a request is served when all its requested items are downloaded. We aim at minimizing the average user perceived latency, i.e. the average flow time of the requests. We first show that any algorithm that ignores the dependencies can yield arbitrary bad performances with respect to the optimum even if it is given arbitrary extra resources. We then design a $(4+\\epsilon)$-speed $O(1+1/\\epsilon^2)$-competitive algorithm for this setting that consists in 1) splitting evenly the bandwidth among each requested set and in 2) broadcasting arbitrarily the items still missing in each set into the bandwidth the set has received. Our algorithm presents several interesting features: it is simple to implement, non-clairvoyant, fair to users so that no user may starve for a long period of time, and guarantees good performances in presence of correlations between user requests (without any change in the broadcast protocol). We also present a $ (4+\\epsilon)$-speed $O(1+1/\\epsilon^3)$-competitive algorithm which broadcasts at most one item at any given time and preempts each item broadcast at most once on average. As a side result of our analysis, we design a competitive algorithm for a particular setting of non-clairvoyant job scheduling with dependencies, which might be of independent interest.",
        "published": "2006-08-02T15:00:02Z",
        "link": "http://arxiv.org/abs/cs/0608013v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Search via Quantum Walk",
        "authors": [
            "Frédéric Magniez",
            "Ashwin Nayak",
            "Jérémie Roland",
            "Miklos Santha"
        ],
        "summary": "We propose a new method for designing quantum search algorithms for finding a \"marked\" element in the state space of a classical Markov chain. The algorithm is based on a quantum walk \\'a la Szegedy (2004) that is defined in terms of the Markov chain. The main new idea is to apply quantum phase estimation to the quantum walk in order to implement an approximate reflection operator. This operator is then used in an amplitude amplification scheme. As a result we considerably expand the scope of the previous approaches of Ambainis (2004) and Szegedy (2004). Our algorithm combines the benefits of these approaches in terms of being able to find marked elements, incurring the smaller cost of the two, and being applicable to a larger class of Markov chains. In addition, it is conceptually simple and avoids some technical difficulties in the previous analyses of several algorithms based on quantum walk.",
        "published": "2006-08-02T18:43:09Z",
        "link": "http://arxiv.org/abs/quant-ph/0608026v4",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Quasi-friendly sup-interpretations",
        "authors": [
            "Jean-Yves Marion",
            "Romain Pechoux"
        ],
        "summary": "In a previous paper, the sup-interpretation method was proposed as a new tool to control memory resources of first order functional programs with pattern matching by static analysis. Basically, a sup-interpretation provides an upper bound on the size of function outputs. In this former work, a criterion, which can be applied to terminating as well as non-terminating programs, was developed in order to bound polynomially the stack frame size. In this paper, we suggest a new criterion which captures more algorithms computing values polynomially bounded in the size of the inputs. Since this work is related to quasi-interpretations, we compare the two notions obtaining two main features. The first one is that, given a program, we have heuristics for finding a sup-interpretation when we consider polynomials of bounded degree. The other one consists in the characterizations of the set of function computable in polynomial time and in polynomial space.",
        "published": "2006-08-03T13:05:32Z",
        "link": "http://arxiv.org/abs/cs/0608020v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On Quasi-Interpretations, Blind Abstractions and Implicit Complexity",
        "authors": [
            "Patrick Baillot",
            "Ugo Dal Lago",
            "Jean-Yves Moyen"
        ],
        "summary": "Quasi-interpretations are a technique to guarantee complexity bounds on first-order functional programs: with termination orderings they give in particular a sufficient condition for a program to be executable in polynomial time, called here the P-criterion. We study properties of the programs satisfying the P-criterion, in order to better understand its intensional expressive power. Given a program on binary lists, its blind abstraction is the nondeterministic program obtained by replacing lists by their lengths (natural numbers). A program is blindly polynomial if its blind abstraction terminates in polynomial time. We show that all programs satisfying a variant of the P-criterion are in fact blindly polynomial. Then we give two extensions of the P-criterion: one by relaxing the termination ordering condition, and the other one (the bounded value property) giving a necessary and sufficient condition for a program to be polynomial time executable, with memoisation.",
        "published": "2006-08-06T07:20:45Z",
        "link": "http://arxiv.org/abs/cs/0608030v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Reversal Complexity Revisited",
        "authors": [
            "Andre Hernich",
            "Nicole Schweikardt"
        ],
        "summary": "We study a generalized version of reversal bounded Turing machines where, apart from several tapes on which the number of head reversals is bounded by r(n), there are several further tapes on which head reversals remain unrestricted, but size is bounded by s(n). Recently, such machines were introduced as a formalization of a computation model that restricts random access to external memory and internal memory space. Here, each of the tapes with a restriction on the head reversals corresponds to an external memory device, and the tapes of restricted size model internal memory. We use ST(r(n),s(n),O(1)) to denote the class of all problems that can be solved by deterministic Turing machines that comply to the above resource bounds. Similarly, NST and RST, respectively, are used for the corresponding nondeterministic and randomized classes.   While previous papers focused on lower bounds for particular problems, including sorting, the set equality problem, and several query evaluation problems, the present paper addresses the relations between the (R,N)ST-classes and classical complexity classes and investigates the structural complexity of the (R,N)ST-classes. Our main results are (1) a trade-off between internal memory space and external memory head reversals, (2) correspondences between the (R,N)ST-classes and ``classical'' time-bounded, space-bounded, reversal-bounded, and circuit complexity classes, and (3) hierarchies of (R)ST-classes in terms of increasing numbers of head reversals on external memory tapes.",
        "published": "2006-08-07T11:37:11Z",
        "link": "http://arxiv.org/abs/cs/0608036v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "Renormalization group approach to the P versus NP question",
        "authors": [
            "S. N. Coppersmith"
        ],
        "summary": "This paper argues that the ideas underlying the renormalization group technique used to characterize phase transitions in condensed matter systems could be useful for distinguishing computational complexity classes. The paper presents a renormalization group transformation that maps an arbitrary Boolean function of $N$ Boolean variables to one of $N-1$ variables. When this transformation is applied repeatedly, the behavior of the resulting sequence of functions is different for a generic Boolean function than for Boolean functions that can be written as a polynomial of degree $\\xi$ with $\\xi \\ll N$ as well as for functions that depend on composite variables such as the arithmetic sum of the inputs. Being able to demonstrate that functions are non-generic is of interest because it suggests an avenue for constructing an algorithm capable of demonstrating that a given Boolean function cannot be computed using resources that are bounded by a polynomial of $N$.",
        "published": "2006-08-11T17:48:42Z",
        "link": "http://arxiv.org/abs/cs/0608053v2",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "F.1.3"
        ]
    },
    {
        "title": "Dispersion of Mass and the Complexity of Randomized Geometric Algorithms",
        "authors": [
            "Luis Rademacher",
            "Santosh Vempala"
        ],
        "summary": "How much can randomness help computation? Motivated by this general question and by volume computation, one of the few instances where randomness provably helps, we analyze a notion of dispersion and connect it to asymptotic convex geometry. We obtain a nearly quadratic lower bound on the complexity of randomized volume algorithms for convex bodies in R^n (the current best algorithm has complexity roughly n^4, conjectured to be n^3). Our main tools, dispersion of random determinants and dispersion of the length of a random point from a convex body, are of independent interest and applicable more generally; in particular, the latter is closely related to the variance hypothesis from convex geometry. This geometric dispersion also leads to lower bounds for matrix problems and property testing.",
        "published": "2006-08-12T23:31:07Z",
        "link": "http://arxiv.org/abs/cs/0608054v2",
        "categories": [
            "cs.CC",
            "cs.CG",
            "cs.DS",
            "math.FA"
        ]
    },
    {
        "title": "Hybrid Elections Broaden Complexity-Theoretic Resistance to Control",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "Electoral control refers to attempts by an election's organizer (\"the chair\") to influence the outcome by adding/deleting/partitioning voters or candidates. The groundbreaking work of Bartholdi, Tovey, and Trick [BTT92] on (constructive) control proposes computational complexity as a means of resisting control attempts: Look for election systems where the chair's task in seeking control is itself computationally infeasible.   We introduce and study a method of combining two or more candidate-anonymous election schemes in such a way that the combined scheme possesses all the resistances to control (i.e., all the NP-hardnesses of control) possessed by any of its constituents: It combines their strengths. From this and new resistance constructions, we prove for the first time that there exists an election scheme that is resistant to all twenty standard types of electoral control.",
        "published": "2006-08-14T16:15:25Z",
        "link": "http://arxiv.org/abs/cs/0608057v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "On Polynomial Time Computable Numbers",
        "authors": [
            "Tetsushi Matsui"
        ],
        "summary": "It will be shown that the polynomial time computable numbers form a field, and especially an algebraically closed field.",
        "published": "2006-08-16T15:26:08Z",
        "link": "http://arxiv.org/abs/cs/0608067v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "From Invariants to Canonization in Parallel",
        "authors": [
            "Johannes Koebler",
            "Oleg Verbitsky"
        ],
        "summary": "A function $f$ of a graph is called a complete graph invariant if the isomorphism of graphs $G$ and $H$ is equivalent to the equality $f(G)=f(H)$. If, in addition, $f(G)$ is a graph isomorphic to $G$, then $f$ is called a canonical form for graphs. Gurevich proves that graphs have a polynomial-time computable canonical form exactly when they have a polynomial-time computable complete invariant. We extend this equivalence to the polylogarithmic-time model of parallel computation for classes of graphs with bounded rigidity index and for classes of graphs with small separators. In particular, our results apply to three representative classes of graphs embeddable into a fixed surface, namely, to 5-connected graphs, to 3-connected graphs admitting a polyhedral embedding, and 3-connected graphs admitting a large-edge-width embedding. Another application covers graphs with bounded treewidth. Since in the latter case an NC complete-invariant algorithm is known, we conclude that graphs of bounded treewidth have a canonical form (and even a canonical labeling) computable in NC.",
        "published": "2006-08-18T09:46:09Z",
        "link": "http://arxiv.org/abs/cs/0608074v4",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "How Hard Is Bribery in Elections?",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra"
        ],
        "summary": "We study the complexity of influencing elections through bribery: How computationally complex is it for an external actor to determine whether by a certain amount of bribing voters a specified candidate can be made the election's winner? We study this problem for election systems as varied as scoring protocols and Dodgson voting, and in a variety of settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters, and succinct-vs.-nonsuccinct input specification. We obtain both polynomial-time bribery algorithms and proofs of the intractability of bribery, and indeed our results show that the complexity of bribery is extremely sensitive to the setting. For example, we find settings in which bribery is NP-complete but manipulation (by voters) is in P, and we find settings in which bribing weighted voters is NP-complete but bribing voters with individual bribe thresholds is in P. For the broad class of elections (including plurality, Borda, k-approval, and veto) known as scoring protocols, we prove a dichotomy result for bribery of weighted voters: We find a simple-to-evaluate condition that classifies every case as either NP-complete or in P.",
        "published": "2006-08-19T23:24:03Z",
        "link": "http://arxiv.org/abs/cs/0608081v3",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "The computational power of population protocols",
        "authors": [
            "Dana Angluin",
            "James Aspnes",
            "David Eisenstat",
            "Eric Ruppert"
        ],
        "summary": "We consider the model of population protocols introduced by Angluin et al., in which anonymous finite-state agents stably compute a predicate of the multiset of their inputs via two-way interactions in the all-pairs family of communication networks. We prove that all predicates stably computable in this model (and certain generalizations of it) are semilinear, answering a central open question about the power of the model. Removing the assumption of two-way interaction, we also consider several variants of the model in which agents communicate by anonymous message-passing where the recipient of each message is chosen by an adversary and the sender is not identified to the recipient. These one-way models are distinguished by whether messages are delivered immediately or after a delay, whether a sender can record that it has sent a message, and whether a recipient can queue incoming messages, refusing to accept new messages until it has had a chance to send out messages of its own. We characterize the classes of predicates stably computable in each of these one-way models using natural subclasses of the semilinear predicates.",
        "published": "2006-08-21T23:17:55Z",
        "link": "http://arxiv.org/abs/cs/0608084v1",
        "categories": [
            "cs.CC",
            "cs.DC"
        ]
    },
    {
        "title": "A Quadratic Time-Space Tradeoff for Unrestricted Deterministic Decision   Branching Programs",
        "authors": [
            "Nandakishore Santhi",
            "Alexander Vardy"
        ],
        "summary": "For a decision problem from coding theory, we prove a quadratic expected time-space tradeoff of the form $\\eT\\eS=\\Omega(\\tfrac{n^2}{q})$ for $q$-way deterministic decision branching programs, where $q\\geq 2$. Here $\\eT$ is the expected computation time and $\\eS$ is the expected space, when all inputs are equally likely. This bound is to our knowledge, the first such to show an exponential size requirement whenever $\\eT = O(n^2)$. Previous exponential size tradeoffs for Boolean decision branching programs were valid for time-restricted models with $T=o(n\\log_2{n})$. Proving quadratic time-space tradeoffs for unrestricted time decision branching programs has been a major goal of recent research -- this goal has already been achieved for multiple-output branching programs two decades ago. We also show the first quadratic time-space tradeoffs for Boolean decision branching programs verifying circular convolution, matrix-vector multiplication and discrete Fourier transform. Furthermore, we demonstrate a constructive Boolean decision function which has a quadratic expected time-space tradeoff in the Boolean deterministic decision branching program model. When $q$ is a constant the tradeoff results derived here for decision functions verifying various functions are order-comparable to previously known tradeoff bounds for calculating the corresponding multiple-output functions.",
        "published": "2006-08-22T04:28:27Z",
        "link": "http://arxiv.org/abs/cs/0608085v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.IT",
            "math.IT",
            "F.2.3; E.4"
        ]
    },
    {
        "title": "On Universality in Real Computation",
        "authors": [
            "Hector Zenil"
        ],
        "summary": "Models of computation operating over the real numbers and computing a larger class of functions compared to the class of general recursive functions invariably introduce a non-finite element of infinite information encoded in an arbitrary non-computable number or non-recursive function. In this paper we show that Turing universality is only possible at every Turing degree but not over all, in that sense universality at the first level is elegantly well defined while universality at higher degrees is at least ambiguous. We propose a concept of universal relativity and universal jump between levels in the arithmetical and analytical hierarchy.",
        "published": "2006-08-24T21:21:21Z",
        "link": "http://arxiv.org/abs/cs/0608094v6",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Stationary Algorithmic Probability",
        "authors": [
            "Markus Mueller"
        ],
        "summary": "Kolmogorov complexity and algorithmic probability are defined only up to an additive resp. multiplicative constant, since their actual values depend on the choice of the universal reference computer. In this paper, we analyze a natural approach to eliminate this machine-dependence.   Our method is to assign algorithmic probabilities to the different computers themselves, based on the idea that \"unnatural\" computers should be hard to emulate. Therefore, we study the Markov process of universal computers randomly emulating each other. The corresponding stationary distribution, if it existed, would give a natural and machine-independent probability measure on the computers, and also on the binary strings.   Unfortunately, we show that no stationary distribution exists on the set of all computers; thus, this method cannot eliminate machine-dependence. Moreover, we show that the reason for failure has a clear and interesting physical interpretation, suggesting that every other conceivable attempt to get rid of those additive constants must fail in principle, too.   However, we show that restricting to some subclass of computers might help to get rid of some amount of machine-dependence in some situations, and the resulting stationary computer and string probabilities have beautiful properties.",
        "published": "2006-08-25T00:44:45Z",
        "link": "http://arxiv.org/abs/cs/0608095v5",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "math.PR"
        ]
    },
    {
        "title": "A study of fuzzy and many-valued logics in cellular automata",
        "authors": [
            "Angelo B. Mingarelli"
        ],
        "summary": "In this paper we provide an analytical study of the theory of multi-valued and fuzzy cellular automata where the fuzziness appears as the result of the application of an underlying multi-valued or continuous logic as opposed to standard logic as used conventionally. Using the disjunctive normal form of any one of the 255 ECA's so defined, we modify the underlying logic structure and redefine the ECA within the framework of this new logic. The idea here is to show that the evolution of space-time diagrams of ECA's under even a probabilistic logic can exhibit non-chaotic behavior. This is looked at specifically for Probabilistic Rule 110, in contrast with Boolean Rule 110 which is known to be capable of universal computation.",
        "published": "2006-08-25T09:21:49Z",
        "link": "http://arxiv.org/abs/cs/0608097v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "B.6.1; F.1.1; I.2.3"
        ]
    },
    {
        "title": "Minimum Cost Homomorphisms to Semicomplete Bipartite Digraphs",
        "authors": [
            "G. Gutin",
            "A. Rafiey",
            "A. Yeo"
        ],
        "summary": "For digraphs $D$ and $H$, a mapping $f: V(D)\\dom V(H)$ is a homomorphism of $D$ to $H$ if $uv\\in A(D)$ implies $f(u)f(v)\\in A(H).$ If, moreover, each vertex $u \\in V(D)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of the homomorphism $f$ is $\\sum_{u\\in V(D)}c_{f(u)}(u)$. For each fixed digraph $H$, we have the {\\em minimum cost homomorphism problem for} $H$. The problem is to decide, for an input graph $D$ with costs $c_i(u),$ $u \\in V(D), i\\in V(H)$, whether there exists a homomorphism of $D$ to $H$ and, if one exists, to find one of minimum cost. Minimum cost homomorphism problems encompass (or are related to) many well studied optimization problems. We describe a dichotomy of the minimum cost homomorphism problem for semicomplete multipartite digraphs $H$. This solves an open problem from an earlier paper. To obtain the dichotomy of this paper, we introduce and study a new notion, a $k$-Min-Max ordering of digraphs.",
        "published": "2006-08-25T17:47:34Z",
        "link": "http://arxiv.org/abs/cs/0608101v1",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "Lp Computable Functions and Fourier Series",
        "authors": [
            "Philippe Moser"
        ],
        "summary": "This paper studies how well computable functions can be approximated by their Fourier series. To this end, we equip the space of Lp-computable functions (computable Lebesgue integrable functions) with a size notion, by introducing Lp-computable Baire categories.   We show that Lp-computable Baire categories satisfy the following three basic properties. Singleton sets {f} (where f is Lp-computable) are meager, suitable infinite unions of meager sets are meager, and the whole space of Lp-computable functions is not meager. We give an alternative characterization of meager sets via Banach Mazur games.   We study the convergence of Fourier series for Lp-computable functions and show that whereas for every p>1, the Fourier series of every Lp-computable function f converges to f in the Lp norm, the set of L1-computable functions whose Fourier series does not diverge almost everywhere is meager.",
        "published": "2006-08-28T12:50:36Z",
        "link": "http://arxiv.org/abs/cs/0608106v3",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Equality of complexity classes P and NP: Linear programming formulation   of the quadratic assignment problem",
        "authors": [
            "Moustapha Diaby"
        ],
        "summary": "In this paper, we present a polynomial-sized linear programming formulation of the Quadratic Assignment Problem (QAP). The proposed linear program is a network flow-based model. Hence, it provides for the solution of the QAP in polynomial time. Computational testing and results are discussed.",
        "published": "2006-09-02T09:12:27Z",
        "link": "http://arxiv.org/abs/cs/0609004v9",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "The traveling salesman problem: A Linear programming formulation",
        "authors": [
            "Moustapha Diaby"
        ],
        "summary": "In this paper, we present a polynomial-sized linear programming formulation of the Traveling Salesman Problem (TSP). The proposed linear program is a network flow-based model. Numerical implementation issues and results are discussed. (The exposition and proofs are much more detailed in an edition which I wrote in collaboration with Dr. M.H. Karwan in 2012-2014 . That edition is available at http://users.business.uconn.edu/mdiaby/P=NPProofPapers/tspPaper.pdf)",
        "published": "2006-09-02T09:28:41Z",
        "link": "http://arxiv.org/abs/cs/0609005v7",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "On the freeze quantifier in Constraint LTL: decidability and complexity",
        "authors": [
            "Stéphane Demri",
            "Ranko Lazic",
            "David Nowak"
        ],
        "summary": "Constraint LTL, a generalisation of LTL over Presburger constraints, is often used as a formal language to specify the behavior of operational models with constraints. The freeze quantifier can be part of the language, as in some real-time logics, but this variable-binding mechanism is quite general and ubiquitous in many logical languages (first-order temporal logics, hybrid logics, logics for sequence diagrams, navigation logics, logics with lambda-abstraction etc.). We show that Constraint LTL over the simple domain (N,=) augmented with the freeze quantifier is undecidable which is a surprising result in view of the poor language for constraints (only equality tests). Many versions of freeze-free Constraint LTL are decidable over domains with qualitative predicates and our undecidability result actually establishes Sigma_1^1-completeness. On the positive side, we provide complexity results when the domain is finite (EXPSPACE-completeness) or when the formulae are flat in a sense introduced in the paper. Our undecidability results are sharp (i.e. with restrictions on the number of variables) and all our complexity characterisations ensure completeness with respect to some complexity class (mainly PSPACE and EXPSPACE).",
        "published": "2006-09-04T06:20:03Z",
        "link": "http://arxiv.org/abs/cs/0609008v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Baire Categories on Small Complexity Classes and Meager-Comeager Laws",
        "authors": [
            "Philippe Moser"
        ],
        "summary": "We introduce two resource-bounded Baire category notions on small complexity classes such as P, SUBEXP, and PSPACE and on probabilistic classes such as BPP, which differ on how the corresponding finite extension strategies are computed. We give an alternative characterization of small sets via resource-bounded Banach-Mazur games.   As an application of the first notion, we show that for almost every language A (i.e. all except a meager class) computable in subexponential time, P(A)=BPP(A). We also show that almost all languages in PSPACE do not have small nonuniform complexity.   We then switch to the second Baire category notion (called locally-computable), and show that the class SPARSE is meager in P. We show that in contrast to the resource-bounded measure case, meager-comeager laws can be obtained for many standard complexity classes, relative to locally-computable Baire category on BPP and PSPACE.   Another topic where locally-computable Baire categories differ from resource-bounded measure is regarding weak-completeness: we show that there is no weak-completeness notion in P based on locally-computable Baire categories, i.e. every P-weakly-complete set is complete for P. We also prove that the class of complete sets for P under Turing-logspace reductions is meager in P, if P is not equal to DSPACE(log n), and that the same holds unconditionally for quasi-poly time.   Finally we observe that locally-computable Baire categories are incomparable with all existing resource-bounded measure notions on small complexity classes, which might explain why those two settings seem to differ so fundamentally.",
        "published": "2006-09-05T09:02:11Z",
        "link": "http://arxiv.org/abs/cs/0609012v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Residual Finite Tree Automata",
        "authors": [
            "J. Carme",
            "R. Gilleron",
            "A. Lemay",
            "A. Terlutte",
            "M. Tommasi"
        ],
        "summary": "Tree automata based algorithms are essential in many fields in computer science such as verification, specification, program analysis. They become also essential for databases with the development of standards such as XML. In this paper, we define new classes of non deterministic tree automata, namely residual finite tree automata (RFTA). In the bottom-up case, we obtain a new characterization of regular tree languages. In the top-down case, we obtain a subclass of regular tree languages which contains the class of languages recognized by deterministic top-down tree automata. RFTA also come with the property of existence of canonical non deterministic tree automata.",
        "published": "2006-09-05T16:23:08Z",
        "link": "http://arxiv.org/abs/cs/0609015v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Geometrical organization of solutions to random linear Boolean equations",
        "authors": [
            "Thierry Mora",
            "Marc Mézard"
        ],
        "summary": "The random XORSAT problem deals with large random linear systems of Boolean variables. The difficulty of such problems is controlled by the ratio of number of equations to number of variables. It is known that in some range of values of this parameter, the space of solutions breaks into many disconnected clusters. Here we study precisely the corresponding geometrical organization. In particular, the distribution of distances between these clusters is computed by the cavity method. This allows to study the `x-satisfiability' threshold, the critical density of equations where there exist two solutions at a given distance.",
        "published": "2006-09-05T16:44:18Z",
        "link": "http://arxiv.org/abs/cond-mat/0609099v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Fast algorithms for computing isogenies between elliptic curves",
        "authors": [
            "Alin Bostan",
            "Bruno Salvy",
            "Francois Morain",
            "Eric Schost"
        ],
        "summary": "We survey algorithms for computing isogenies between elliptic curves defined over a field of characteristic either 0 or a large prime. We introduce a new algorithm that computes an isogeny of degree $\\ell$ ($\\ell$ different from the characteristic) in time quasi-linear with respect to $\\ell$. This is based in particular on fast algorithms for power series expansion of the Weierstrass $\\wp$-function and related functions.",
        "published": "2006-09-06T12:10:17Z",
        "link": "http://arxiv.org/abs/cs/0609020v1",
        "categories": [
            "cs.CC",
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "Dichotomies and Duality in First-order Model Checking Problems",
        "authors": [
            "Barnaby Martin"
        ],
        "summary": "We study the complexity of the model checking problem, for fixed model A, over certain fragments L of first-order logic. These are sometimes known as the expression complexities of L. We obtain various complexity classification theorems for these logics L as each ranges over models A, in the spirit of the dichotomy conjecture for the Constraint Satisfaction Problem -- which itself may be seen as the model checking problem for existential conjunctive positive first-order logic.",
        "published": "2006-09-06T13:04:36Z",
        "link": "http://arxiv.org/abs/cs/0609022v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Approximation Algorithms for the Bipartite Multi-cut Problem",
        "authors": [
            "Sreyash Kenkre",
            "Sundar Vishwanathan"
        ],
        "summary": "We introduce the {\\it Bipartite Multi-cut} problem. This is a generalization of the {\\it st-Min-cut} problem, is similar to the {\\it Multi-cut} problem (except for more stringent requirements) and also turns out to be an immediate generalization of the {\\it Min UnCut} problem. We prove that this problem is {\\bf NP}-hard and then present LP and SDP based approximation algorithms. While the LP algorithm is based on the Garg-Vazirani-Yannakakis algorithm for {\\it Multi-cut}, the SDP algorithm uses the {\\it Structure Theorem} of $\\ell_2^2$ Metrics.",
        "published": "2006-09-07T18:10:39Z",
        "link": "http://arxiv.org/abs/cs/0609031v2",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "The Connectivity of Boolean Satisfiability: Computational and Structural   Dichotomies",
        "authors": [
            "Parikshit Gopalan",
            "Phokion G. Kolaitis",
            "Elitza Maneva",
            "Christos H. Papadimitriou"
        ],
        "summary": "Boolean satisfiability problems are an important benchmark for questions about complexity, algorithms, heuristics and threshold phenomena. Recent work on heuristics, and the satisfiability threshold has centered around the structure and connectivity of the solution space. Motivated by this work, we study structural and connectivity-related properties of the space of solutions of Boolean satisfiability problems and establish various dichotomies in Schaefer's framework.   On the structural side, we obtain dichotomies for the kinds of subgraphs of the hypercube that can be induced by the solutions of Boolean formulas, as well as for the diameter of the connected components of the solution space. On the computational side, we establish dichotomy theorems for the complexity of the connectivity and st-connectivity questions for the graph of solutions of Boolean formulas. Our results assert that the intractable side of the computational dichotomies is PSPACE-complete, while the tractable side - which includes but is not limited to all problems with polynomial time algorithms for satisfiability - is in P for the st-connectivity question, and in coNP for the connectivity question. The diameter of components can be exponential for the PSPACE-complete cases, whereas in all other cases it is linear; thus, small diameter and tractability of the connectivity problems are remarkably aligned. The crux of our results is an expressibility theorem showing that in the tractable cases, the subgraphs induced by the solution space possess certain good structural properties, whereas in the intractable cases, the subgraphs can be arbitrary.",
        "published": "2006-09-13T07:25:59Z",
        "link": "http://arxiv.org/abs/cs/0609072v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Classifying extrema using intervals",
        "authors": [
            "Marek W. Gutowski"
        ],
        "summary": "We present a straightforward and verified method of deciding whether the n-dimensional point x (n>=1), such that \\nabla f(x)=0, is the local minimizer, maximizer or just a saddle point of a real-valued function f.   The method scales linearly with dimensionality of the problem and never produces false results.",
        "published": "2006-09-14T18:32:46Z",
        "link": "http://arxiv.org/abs/cs/0609082v1",
        "categories": [
            "cs.MS",
            "cs.CC",
            "cs.NA",
            "F.2.2; G.1.0; G.1.2; J.2"
        ]
    },
    {
        "title": "Finite-State Dimension and Lossy Decompressors",
        "authors": [
            "David Doty",
            "Philippe Moser"
        ],
        "summary": "This paper examines information-theoretic questions regarding the difficulty of compressing data versus the difficulty of decompressing data and the role that information loss plays in this interaction. Finite-state compression and decompression are shown to be of equivalent difficulty, even when the decompressors are allowed to be lossy.   Inspired by Kolmogorov complexity, this paper defines the optimal *decompression *ratio achievable on an infinite sequence by finite-state decompressors (that is, finite-state transducers outputting the sequence in question). It is shown that the optimal compression ratio achievable on a sequence S by any *information lossless* finite state compressor, known as the finite-state dimension of S, is equal to the optimal decompression ratio achievable on S by any finite-state decompressor. This result implies a new decompression characterization of finite-state dimension in terms of lossy finite-state transducers.",
        "published": "2006-09-18T05:00:39Z",
        "link": "http://arxiv.org/abs/cs/0609096v2",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Can rare SAT formulas be easily recognized? On the efficiency of message   passing algorithms for K-SAT at large clause-to-variable ratios",
        "authors": [
            "Fabrizio Altarelli",
            "Remi Monasson",
            "Francesco Zamponi"
        ],
        "summary": "For large clause-to-variable ratio, typical K-SAT instances drawn from the uniform distribution have no solution. We argue, based on statistical mechanics calculations using the replica and cavity methods, that rare satisfiable instances from the uniform distribution are very similar to typical instances drawn from the so-called planted distribution, where instances are chosen uniformly between the ones that admit a given solution. It then follows, from a recent article by Feige, Mossel and Vilenchik, that these rare instances can be easily recognized (in O(log N) time and with probability close to 1) by a simple message-passing algorithm.",
        "published": "2006-09-18T08:38:16Z",
        "link": "http://arxiv.org/abs/cs/0609101v2",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech"
        ]
    },
    {
        "title": "Minimum-weight Cycle Covers and Their Approximability",
        "authors": [
            "Bodo Manthey"
        ],
        "summary": "A cycle cover of a graph is a set of cycles such that every vertex is part of exactly one cycle. An L-cycle cover is a cycle cover in which the length of every cycle is in the set L.   We investigate how well L-cycle covers of minimum weight can be approximated. For undirected graphs, we devise a polynomial-time approximation algorithm that achieves a constant approximation ratio for all sets L. On the other hand, we prove that the problem cannot be approximated within a factor of 2-eps for certain sets L.   For directed graphs, we present a polynomial-time approximation algorithm that achieves an approximation ratio of O(n), where $n$ is the number of vertices. This is asymptotically optimal: We show that the problem cannot be approximated within a factor of o(n).   To contrast the results for cycle covers of minimum weight, we show that the problem of computing L-cycle covers of maximum weight can, at least in principle, be approximated arbitrarily well.",
        "published": "2006-09-18T13:22:39Z",
        "link": "http://arxiv.org/abs/cs/0609103v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "On the Impossibility of a Quantum Sieve Algorithm for Graph Isomorphism",
        "authors": [
            "Cristopher Moore",
            "Alexander Russell"
        ],
        "summary": "It is known that any quantum algorithm for Graph Isomorphism that works within the framework of the hidden subgroup problem (HSP) must perform highly entangled measurements across Omega(n log n) coset states. One of the only known models for how such a measurement could be carried out efficiently is Kuperberg's algorithm for the HSP in the dihedral group, in which quantum states are adaptively combined and measured according to the decomposition of tensor products into irreducible representations. This ``quantum sieve'' starts with coset states, and works its way down towards representations whose probabilities differ depending on, for example, whether the hidden subgroup is trivial or nontrivial.   In this paper we give strong evidence that no such approach can succeed for Graph Isomorphism. Specifically, we consider the natural reduction of Graph Isomorphism to the HSP over the the wreath product S_n \\wr Z_2. We show, modulo a group-theoretic conjecture regarding the asymptotic characters of the symmetric group, that no matter what rule we use to adaptively combine quantum states, there is a constant b > 0 such that no algorithm in this family can solve Graph Isomorphism in e^{b sqrt{n}} time. In particular, such algorithms are essentially no better than the best known classical algorithms, whose running time is e^{O(sqrt{n \\log n})}.",
        "published": "2006-09-18T20:01:41Z",
        "link": "http://arxiv.org/abs/quant-ph/0609138v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "math.RT"
        ]
    },
    {
        "title": "Generalized Majority-Minority Operations are Tractable",
        "authors": [
            "Victor Dalmau"
        ],
        "summary": "Generalized majority-minority (GMM) operations are introduced as a common generalization of near unanimity operations and Mal'tsev operations on finite sets. We show that every instance of the constraint satisfaction problem (CSP), where all constraint relations are invariant under a (fixed) GMM operation, is solvable in polynomial time. This constitutes one of the largest tractable cases of the CSP.",
        "published": "2006-09-19T15:12:46Z",
        "link": "http://arxiv.org/abs/cs/0609108v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A Richer Understanding of the Complexity of Election Systems",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "We provide an overview of some recent progress on the complexity of election systems. The issues studied include the complexity of the winner, manipulation, bribery, and control problems.",
        "published": "2006-09-19T22:57:36Z",
        "link": "http://arxiv.org/abs/cs/0609112v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Max-Cut and Max-Bisection are NP-hard on unit disk graphs",
        "authors": [
            "Josep Diaz",
            "Marcin Kaminski"
        ],
        "summary": "We prove that the Max-Cut and Max-Bisection problems are NP-hard on unit disk graphs. We also show that $\\lambda$-precision graphs are planar for $\\lambda$ > 1 / \\sqrt{2}$.",
        "published": "2006-09-22T18:17:12Z",
        "link": "http://arxiv.org/abs/cs/0609128v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "A Predicative Harmonization of the Time and Provable Hierarchies",
        "authors": [
            "Salvatore Caporaso"
        ],
        "summary": "A decidable transfinite hierarchy is defined by assigning ordinals to the programs of an imperative language. It singles out: the classes TIMEF(n^c) and TIMEF(n_c); the finite Grzegorczyk classes at and above the elementary level, and the \\Sigma_k-IND fragments of PA. Limited operators, diagonalization, and majorization functions are not used.",
        "published": "2006-09-23T14:28:54Z",
        "link": "http://arxiv.org/abs/cs/0609130v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Polygon Convexity: A Minimal O(n) Test",
        "authors": [
            "Iosif Pinelis"
        ],
        "summary": "An O(n) test for polygon convexity is stated and proved. It is also proved that the test is minimal in a certain exact sense.",
        "published": "2006-09-25T19:23:55Z",
        "link": "http://arxiv.org/abs/cs/0609141v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "math.CO",
            "math.MG",
            "I.3.5; F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "VPSPACE and a Transfer Theorem over the Reals",
        "authors": [
            "Pascal Koiran",
            "Sylvain Perifel"
        ],
        "summary": "We introduce a new class VPSPACE of families of polynomials. Roughly speaking, a family of polynomials is in VPSPACE if its coefficients can be computed in polynomial space. Our main theorem is that if (uniform, constant-free) VPSPACE families can be evaluated efficiently then the class PAR of decision problems that can be solved in parallel polynomial time over the real numbers collapses to P. As a result, one must first be able to show that there are VPSPACE families which are hard to evaluate in order to separate over the reals P from NP, or even from PAR.",
        "published": "2006-10-03T13:48:44Z",
        "link": "http://arxiv.org/abs/cs/0610009v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Iterative Decoding of Low-Density Parity Check Codes (A Survey)",
        "authors": [
            "Venkatesan Guruswami"
        ],
        "summary": "Much progress has been made on decoding algorithms for error-correcting codes in the last decade. In this article, we give an introduction to some fundamental results on iterative, message-passing algorithms for low-density parity check codes. For certain important stochastic channels, this line of work has enabled getting very close to Shannon capacity with algorithms that are extremely efficient (both in theory and practice).",
        "published": "2006-10-05T04:31:52Z",
        "link": "http://arxiv.org/abs/cs/0610022v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "LTL with the Freeze Quantifier and Register Automata",
        "authors": [
            "Stephane Demri",
            "Ranko Lazic"
        ],
        "summary": "A data word is a sequence of pairs of a letter from a finite alphabet and an element from an infinite set, where the latter can only be compared for equality. To reason about data words, linear temporal logic is extended by the freeze quantifier, which stores the element at the current word position into a register, for equality comparisons deeper in the formula. By translations from the logic to alternating automata with registers and then to faulty counter automata whose counters may erroneously increase at any time, and from faulty and error-free counter automata to the logic, we obtain a complete complexity table for logical fragments defined by varying the set of temporal operators and the number of registers. In particular, the logic with future-time operators and 1 register is decidable but not primitive recursive over finite data words. Adding past-time operators or 1 more register, or switching to infinite data words, cause undecidability.",
        "published": "2006-10-05T15:47:22Z",
        "link": "http://arxiv.org/abs/cs/0610027v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for The Traveling Salesman Problem",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "The ATSP polytope can be expressed by asymmetric polynomial size linear program.",
        "published": "2006-10-09T13:15:12Z",
        "link": "http://arxiv.org/abs/cs/0610042v3",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Restricted Complexity, General Complexity",
        "authors": [
            "Edgar Morin"
        ],
        "summary": "Why has the problematic of complexity appeared so late? And why would it be justified?",
        "published": "2006-10-10T07:59:18Z",
        "link": "http://arxiv.org/abs/cs/0610049v1",
        "categories": [
            "cs.CC",
            "nlin.AO"
        ]
    },
    {
        "title": "The central nature of the Hidden Subgroup problem",
        "authors": [
            "S. A. Fenner",
            "Y. Zhang"
        ],
        "summary": "We show that several problems that figure prominently in quantum computing, including Hidden Coset, Hidden Shift, and Orbit Coset, are equivalent or reducible to Hidden Subgroup for a large variety of groups. We also show that, over permutation groups, the decision version and search version of Hidden Subgroup are polynomial-time equivalent. For Hidden Subgroup over dihedral groups, such an equivalence can be obtained if the order of the group is smooth. Finally, we give nonadaptive program checkers for Hidden Subgroup and its decision version.",
        "published": "2006-10-13T18:24:18Z",
        "link": "http://arxiv.org/abs/cs/0610086v2",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "Solving planning domains with polytree causal graphs is NP-complete",
        "authors": [
            "Omer Giménez"
        ],
        "summary": "We show that solving planning domains on binary variables with polytree causal graph is \\NP-complete. This is in contrast to a polynomial-time algorithm of Domshlak and Brafman that solves these planning domains for polytree causal graphs of bounded indegree.",
        "published": "2006-10-16T06:18:44Z",
        "link": "http://arxiv.org/abs/cs/0610095v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.8"
        ]
    },
    {
        "title": "Entropy generation in a model of reversible computation",
        "authors": [
            "Diego de Falco",
            "Dario Tamascelli"
        ],
        "summary": "We present a model in which, due to the quantum nature of the signals controlling the implementation time of successive unitary computational steps, \\emph{physical} irreversibility appears in the execution of a \\emph{logically} reversible computation.",
        "published": "2006-10-17T08:51:43Z",
        "link": "http://arxiv.org/abs/cs/0610101v1",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "CHAC. A MOACO Algorithm for Computation of Bi-Criteria Military Unit   Path in the Battlefield",
        "authors": [
            "A. M. Mora",
            "J. J. Merelo",
            "C. Millan",
            "J. Torrecillas",
            "J. L. J. Laredo"
        ],
        "summary": "In this paper we propose a Multi-Objective Ant Colony Optimization (MOACO) algorithm called CHAC, which has been designed to solve the problem of finding the path on a map (corresponding to a simulated battlefield) that minimizes resources while maximizing safety. CHAC has been tested with two different state transition rules: an aggregative function that combines the heuristic and pheromone information of both objectives and a second one that is based on the dominance concept of multiobjective optimization problems. These rules have been evaluated in several different situations (maps with different degree of difficulty), and we have found that they yield better results than a greedy algorithm (taken as baseline) in addition to a military behaviour that is also better in the tactical sense. The aggregative function, in general, yields better results than the one based on dominance.",
        "published": "2006-10-19T10:41:16Z",
        "link": "http://arxiv.org/abs/cs/0610113v1",
        "categories": [
            "cs.MA",
            "cs.CC"
        ]
    },
    {
        "title": "Instant Computing - A New Computation Paradigm",
        "authors": [
            "Hans-Rudolf Thomann"
        ],
        "summary": "Voltage peaks on a conventional computer's power lines allow for the well-known dangerous DPA attacks. We show that measurement of a quantum computer's transient state during a computational step reveals information about a complete computation of arbitrary length, which can be extracted by repeated probing, if the computer is suitably programmed. Instant computing, as we name this mode of operation, recognizes for any total or partial recursive function arguments lying in the domain of definition and yields their function value with arbitrary small error probability in probabilistic linear time. This implies recognition of (not necessarily recursively enumerable) complements of recursively enumerable sets and the solution of the halting problem. Future quantum computers are shown to be likely to allow for instant computing, and some consequences are pointed out.",
        "published": "2006-10-19T14:11:38Z",
        "link": "http://arxiv.org/abs/cs/0610114v3",
        "categories": [
            "cs.CC",
            "cs.CR",
            "quant-ph",
            "F.1.1; F.1.2; F.1.3; F.1.4"
        ]
    },
    {
        "title": "Report on article: P=NP Linear programming formulation of the Traveling   Salesman Problem",
        "authors": [
            "Radoslaw Hofman"
        ],
        "summary": "This article presents counter examples for three articles claiming that P=NP. Articles for which it applies are: Moustapha Diaby \"P = NP: Linear programming formulation of the traveling salesman problem\" and \"Equality of complexity classes P and NP: Linear programming formulation of the quadratic assignment problem\", and also Sergey Gubin \"A Polynomial Time Algorithm for The Traveling Salesman Problem\"",
        "published": "2006-10-20T14:01:22Z",
        "link": "http://arxiv.org/abs/cs/0610125v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "Quantum List Decoding of Classical Block Codes of Polynomially Small   Rate from Quantumly Corrupted Codewords",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "Given a classical error-correcting block code, the task of quantum list decoding is to produce from any quantumly corrupted codeword a short list containing all messages whose codewords exhibit high \"presence\" in the quantumly corrupted codeword. Efficient quantum list decoders have been used to prove a quantum hardcore property of classical codes. However, the code rates of all known families of efficiently quantum list-decodable codes are, unfortunately, too small for other practical applications. To improve those known code rates, we prove that a specific code family of polynomially small code rate over a fixed code alphabet, obtained by concatenating generalized Reed-Solomon codes as outer codes with Hadamard codes as inner codes, has an efficient quantum list-decoding algorithm if its codewords have relatively high codeword presence in a given quantumly corrupted codeword. As an immediate application, we use the quantum list decodability of this code family to solve a certain form of quantum search problems in polynomial time. When the codeword presence becomes smaller, in contrast, we show that the quantum list decodability of generalized Reed-Solomon codes with high confidence is closely related to the efficient solvability of the following two problems: the noisy polynomial interpolation problem and the bounded distance vector problem. Moreover, assuming that NP is not included in BQP, we also prove that no efficient quantum list decoder exists for the generalized Reed-Solomon codes.",
        "published": "2006-10-23T14:57:40Z",
        "link": "http://arxiv.org/abs/quant-ph/0610200v5",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Fixed-Parameter Algorithm for #SAT with Parameter Incidence Treewidth",
        "authors": [
            "Marko Samer",
            "Stefan Szeider"
        ],
        "summary": "We present an efficient fixed-parameter algorithm for #SAT parameterized by the incidence treewidth, i.e., the treewidth of the bipartite graph whose vertices are the variables and clauses of the given CNF formula; a variable and a clause are joined by an edge if and only if the variable occurs in the clause. Our algorithm runs in time O(4^k k l N), where k denotes the incidence treewidth, l denotes the size of a largest clause, and N denotes the number of nodes of the tree-decomposition.",
        "published": "2006-10-31T12:58:36Z",
        "link": "http://arxiv.org/abs/cs/0610174v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "Coupling of quantum angular momenta: an insight into analogic/discrete   and local/global models of computation",
        "authors": [
            "Annalisa Marzuoli",
            "Mario Rasetti"
        ],
        "summary": "In the past few years there has been a tumultuous activity aimed at introducing novel conceptual schemes for quantum computing. The approach proposed in (Marzuoli A and Rasetti M 2002, 2005a) relies on the (re)coupling theory of SU(2) angular momenta and can be viewed as a generalization to arbitrary values of the spin variables of the usual quantum-circuit model based on `qubits' and Boolean gates. Computational states belong to finite-dimensional Hilbert spaces labelled by both discrete and continuous parameters, and unitary gates may depend on quantum numbers ranging over finite sets of values as well as continuous (angular) variables. Such a framework is an ideal playground to discuss discrete (digital) and analogic computational processes, together with their relationships occuring when a consistent semiclassical limit takes place on discrete quantum gates. When working with purely discrete unitary gates, the simulator is naturally modelled as families of quantum finite states--machines which in turn represent discrete versions of topological quantum computation models. We argue that our model embodies a sort of unifying paradigm for computing inspired by Nature and, even more ambitiously, a universal setting in which suitably encoded quantum symbolic manipulations of combinatorial, topological and algebraic problems might find their `natural' computational reference model.",
        "published": "2006-10-31T15:44:08Z",
        "link": "http://arxiv.org/abs/cs/0610171v1",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "Merlin-Arthur Games and Stoquastic Complexity",
        "authors": [
            "Sergey Bravyi",
            "Arvid J. Bessen",
            "Barbara M. Terhal"
        ],
        "summary": "MA is a class of decision problems for which `yes'-instances have a proof that can be efficiently checked by a classical randomized algorithm. We prove that MA has a natural complete problem which we call the stoquastic k-SAT problem. This is a matrix-valued analogue of the satisfiability problem in which clauses are k-qubit projectors with non-negative matrix elements, while a satisfying assignment is a vector that belongs to the space spanned by these projectors. Stoquastic k-SAT is the first non-trivial example of a MA-complete problem. We also study the minimum eigenvalue problem for local stoquastic Hamiltonians that was introduced in quant-ph/0606140, stoquastic LH-MIN. A new complexity class StoqMA is introduced so that stoquastic LH-MIN is StoqMA-complete. Lastly, we consider the average LH-MIN problem for local stoquastic Hamiltonians that depend on a random or `quenched disorder' parameter, stoquastic AV-LH-MIN. We prove that stoquastic AV-LH-MIN is contained in the complexity class \\AM, the class of decision problems for which yes-instances have a randomized interactive proof with two-way communication between prover and verifier.",
        "published": "2006-11-02T01:20:42Z",
        "link": "http://arxiv.org/abs/quant-ph/0611021v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Why Linear Programming cannot solve large instances of NP-complete   problems in polynomial time",
        "authors": [
            "Radoslaw Hofman"
        ],
        "summary": "This article discusses ability of Linear Programming models to be used as solvers of NP-complete problems. Integer Linear Programming is known as NP-complete problem, but non-integer Linear Programming problems can be solved in polynomial time, what places them in P class. During past three years there appeared some articles using LP to solve NP-complete problems. This methods use large number of variables (O(n^9)) solving correctly almost all instances that can be solved in reasonable time. Can they solve infinitively large instances? This article gives answer to this question.",
        "published": "2006-11-02T08:40:53Z",
        "link": "http://arxiv.org/abs/cs/0611008v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.NA",
            "F.1; F.2"
        ]
    },
    {
        "title": "On the Solution-Space Geometry of Random Constraint Satisfaction   Problems",
        "authors": [
            "Dimitris Achlioptas",
            "Federico Ricci-Tersenghi"
        ],
        "summary": "For a large number of random constraint satisfaction problems, such as random k-SAT and random graph and hypergraph coloring, there are very good estimates of the largest constraint density for which solutions exist. Yet, all known polynomial-time algorithms for these problems fail to find solutions even at much lower densities. To understand the origin of this gap we study how the structure of the space of solutions evolves in such problems as constraints are added. In particular, we prove that much before solutions disappear, they organize into an exponential number of clusters, each of which is relatively small and far apart from all other clusters. Moreover, inside each cluster most variables are frozen, i.e., take only one value. The existence of such frozen variables gives a satisfying intuitive explanation for the failure of the polynomial-time algorithms analyzed so far. At the same time, our results establish rigorously one of the two main hypotheses underlying Survey Propagation, a heuristic introduced by physicists in recent years that appears to perform extraordinarily well on random constraint satisfaction problems.",
        "published": "2006-11-13T11:09:49Z",
        "link": "http://arxiv.org/abs/cs/0611052v2",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn"
        ]
    },
    {
        "title": "On \"P = NP: Linear Programming Formulation of the Traveling Salesman   Problem\": A reply to Hofman's Claim of a \"Counter-Example\"",
        "authors": [
            "Moustapha Diaby"
        ],
        "summary": "We show that Hofman's claim of a \"counter-example\" to Diaby's LP formulation of the TSP is invalid.",
        "published": "2006-11-16T03:16:04Z",
        "link": "http://arxiv.org/abs/cs/0611074v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2; F.2.2"
        ]
    },
    {
        "title": "The Importance of the Algorithmic Information Theory to Construct a   Possible Example Where NP # P - II: An Irreducible Sentence",
        "authors": [
            "Rubens Viana Ramos"
        ],
        "summary": "In this short communication it is discussed the relation between disentangled states and algorithmic information theory aiming to construct an irreducible sentence whose length increases in a non-polynomial way when the number of qubits increases.",
        "published": "2006-11-16T17:23:47Z",
        "link": "http://arxiv.org/abs/cs/0611081v2",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The Computational Complexity of the Traveling Salesman Problem",
        "authors": [
            "Craig Alan Feinstein"
        ],
        "summary": "In this note, we show that the Traveling Salesman Problem cannot be solved in polynomial-time on a classical computer.",
        "published": "2006-11-17T18:03:58Z",
        "link": "http://arxiv.org/abs/cs/0611082v6",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Lossy Bulk Synchronous Parallel Processing Model for Very Large Scale   Grids",
        "authors": [
            "Elankovan Sundararajan",
            "Aaron Harwood",
            "Kotagiri Ramamohanarao"
        ],
        "summary": "The performance of a parallel algorithm in a very large scale grid is significantly influenced by the underlying Internet protocols and inter-connectivity. Many grid programming platforms use TCP due to its reliability, usually with some optimizations to reduce its costs. However, TCP does not perform well in a high bandwidth and high delay network environment. On the other hand, UDP is the fastest protocol available because it omits connection setup process, acknowledgments and retransmissions sacrificing reliable transfer. Many new bulk data transfer schemes using UDP for data transmission such as RBUDP, Tsunami, and SABUL have been introduced and shown to have better performance compared to TCP. In this paper, we consider the use of UDP and examine the relationship between packet loss and speedup with respect to the number of grid nodes. Our measurement suggests that packet loss rates between 5%-15% on average are not uncommon between PlanetLab nodes that are widely distributed over the Internet. We show that transmitting multiple copies of same packet produces higher speedup. We show the minimum number of packet duplication required to maximize the possible speedup for a given number of nodes using a BSP based model. Our work demonstrates that by using an appropriate number of packet copies, we can increase performance of parallel program.",
        "published": "2006-11-20T00:20:44Z",
        "link": "http://arxiv.org/abs/cs/0611091v2",
        "categories": [
            "cs.DC",
            "cs.CC",
            "cs.PF"
        ]
    },
    {
        "title": "Barriers and local minima in energy landscapes of stochastic local   search",
        "authors": [
            "Petteri Kaski"
        ],
        "summary": "A local search algorithm operating on an instance of a Boolean constraint satisfaction problem (in particular, k-SAT) can be viewed as a stochastic process traversing successive adjacent states in an ``energy landscape'' defined by the problem instance on the n-dimensional Boolean hypercube. We investigate analytically the worst-case topography of such landscapes in the context of satisfiable k-SAT via a random ensemble of satisfiable ``k-regular'' linear equations modulo 2.   We show that for each fixed k=3,4,..., the typical k-SAT energy landscape induced by an instance drawn from the ensemble has a set of 2^{\\Omega(n)} local energy minima, each separated by an unconditional \\Omega(n) energy barrier from each of the O(1) ground states, that is, solution states with zero energy. The main technical aspect of the analysis is that a random k-regular 0/1 matrix constitutes a strong boundary expander with almost full GF(2)-linear rank, a property which also enables us to prove a 2^{\\Omega(n)} lower bound for the expected number of steps required by the focused random walk heuristic to solve typical instances drawn from the ensemble. These results paint a grim picture of the worst-case topography of k-SAT for local search, and constitute apparently the first rigorous analysis of the growth of energy barriers in a random ensemble of k-SAT landscapes as the number of variables n is increased.",
        "published": "2006-11-21T12:52:57Z",
        "link": "http://arxiv.org/abs/cs/0611103v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "F.2.2; G.2.1; G.3; I.2.8"
        ]
    },
    {
        "title": "On the Complexity of Processing Massive, Unordered, Distributed Data",
        "authors": [
            "Jon Feldman",
            "S. Muthukrishnan",
            "Anastasios Sidiropoulos",
            "Cliff Stein",
            "Zoya Svitkina"
        ],
        "summary": "An existing approach for dealing with massive data sets is to stream over the input in few passes and perform computations with sublinear resources. This method does not work for truly massive data where even making a single pass over the data with a processor is prohibitive. Successful log processing systems in practice such as Google's MapReduce and Apache's Hadoop use multiple machines. They efficiently perform a certain class of highly distributable computations defined by local computations that can be applied in any order to the input.   Motivated by the success of these systems, we introduce a simple algorithmic model for massive, unordered, distributed (mud) computation. We initiate the study of understanding its computational complexity. Our main result is a positive one: any unordered function that can be computed by a streaming algorithm can also be computed with a mud algorithm, with comparable space and communication complexity. We extend this result to some useful classes of approximate and randomized streaming algorithms. We also give negative results, using communication complexity arguments to prove that extensions to private randomness, promise problems and indeterminate functions are impossible.   We believe that the line of research we introduce in this paper has the potential for tremendous impact. The distributed systems that motivate our work successfully process data at an unprecedented scale, distributed over hundreds or even thousands of machines, and perform hundreds of such analyses each day. The mud model (and its generalizations) inspire a set of complexity-theoretic questions that lie at their heart.",
        "published": "2006-11-21T16:11:06Z",
        "link": "http://arxiv.org/abs/cs/0611108v2",
        "categories": [
            "cs.CC",
            "cs.DC"
        ]
    },
    {
        "title": "Toward a general theory of quantum games",
        "authors": [
            "Gus Gutoski",
            "John Watrous"
        ],
        "summary": "We study properties of quantum strategies, which are complete specifications of a given party's actions in any multiple-round interaction involving the exchange of quantum information with one or more other parties. In particular, we focus on a representation of quantum strategies that generalizes the Choi-Jamio{\\l}kowski representation of quantum operations. This new representation associates with each strategy a positive semidefinite operator acting only on the tensor product of its input and output spaces. Various facts about such representations are established, and two applications are discussed: the first is a new and conceptually simple proof of Kitaev's lower bound for strong coin-flipping, and the second is a proof of the exact characterization QRG = EXP of the class of problems having quantum refereed games.",
        "published": "2006-11-22T21:05:24Z",
        "link": "http://arxiv.org/abs/quant-ph/0611234v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.GT"
        ]
    },
    {
        "title": "P is not equal to NP",
        "authors": [
            "Raju Renjit. G"
        ],
        "summary": "This submission has been withdrawn at the request of the author.",
        "published": "2006-11-29T08:27:00Z",
        "link": "http://arxiv.org/abs/cs/0611147v9",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Fast linear algebra is stable",
        "authors": [
            "James Demmel",
            "Ioana Dumitriu",
            "Olga Holtz"
        ],
        "summary": "In an earlier paper, we showed that a large class of fast recursive matrix multiplication algorithms is stable in a normwise sense, and that in fact if multiplication of $n$-by-$n$ matrices can be done by any algorithm in $O(n^{\\omega + \\eta})$ operations for any $\\eta > 0$, then it can be done stably in $O(n^{\\omega + \\eta})$ operations for any $\\eta > 0$. Here we extend this result to show that essentially all standard linear algebra operations, including LU decomposition, QR decomposition, linear equation solving, matrix inversion, solving least squares problems, (generalized) eigenvalue problems and the singular value decomposition can also be done stably (in a normwise sense) in $O(n^{\\omega + \\eta})$ operations.",
        "published": "2006-12-10T20:44:57Z",
        "link": "http://arxiv.org/abs/math/0612264v3",
        "categories": [
            "math.NA",
            "cs.CC",
            "cs.DS",
            "65Y20, 65F30, 65G50, 68Q17, 68Q25"
        ]
    },
    {
        "title": "The Common Prefix Problem On Trees",
        "authors": [
            "Sreyash Kenkre",
            "Sundar Vishwanathan"
        ],
        "summary": "We present a theoretical study of a problem arising in database query optimization, which we call as The Common Prefix Problem. We present a $(1-o(1))$ factor approximation algorithm for this problem, when the underlying graph is a binary tree. We then use a result of Feige and Kogan to show that even on stars, the problem is hard to approximate.",
        "published": "2006-12-11T12:32:02Z",
        "link": "http://arxiv.org/abs/cs/0612060v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "How a Clebsch-Gordan Transform Helps to Solve the Heisenberg Hidden   Subgroup Problem",
        "authors": [
            "Dave Bacon"
        ],
        "summary": "It has recently been shown that quantum computers can efficiently solve the Heisenberg hidden subgroup problem, a problem whose classical query complexity is exponential. This quantum algorithm was discovered within the framework of using pretty-good measurements for obtaining optimal measurements in the hidden subgroup problem. Here we show how to solve the Heisenberg hidden subgroup problem using arguments based instead on the symmetry of certain hidden subgroup states. The symmetry we consider leads naturally to a unitary transform known as the Clebsch-Gordan transform over the Heisenberg group. This gives a new representation theoretic explanation for the pretty-good measurement derived algorithm for efficiently solving the Heisenberg hidden subgroup problem and provides evidence that Clebsch-Gordan transforms over finite groups are a new primitive in quantum algorithm design.",
        "published": "2006-12-13T18:37:08Z",
        "link": "http://arxiv.org/abs/quant-ph/0612107v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Gibbs States and the Set of Solutions of Random Constraint Satisfaction   Problems",
        "authors": [
            "Florent Krzakala",
            "Andrea Montanari",
            "Federico Ricci-Tersenghi",
            "Guilhem Semerjian",
            "Lenka Zdeborova"
        ],
        "summary": "An instance of a random constraint satisfaction problem defines a random subset S (the set of solutions) of a large product space (the set of assignments). We consider two prototypical problem ensembles (random k-satisfiability and q-coloring of random regular graphs), and study the uniform measure with support on S. As the number of constraints per variable increases, this measure first decomposes into an exponential number of pure states (\"clusters\"), and subsequently condensates over the largest such states. Above the condensation point, the mass carried by the n largest states follows a Poisson-Dirichlet process.   For typical large instances, the two transitions are sharp. We determine for the first time their precise location. Further, we provide a formal definition of each phase transition in terms of different notions of correlation between distinct variables in the problem.   The degree of correlation naturally affects the performances of many search/sampling algorithms. Empirical evidence suggests that local Monte Carlo Markov Chain strategies are effective up to the clustering phase transition, and belief propagation up to the condensation point. Finally, refined message passing techniques (such as survey propagation) may beat also this threshold.",
        "published": "2006-12-14T14:27:53Z",
        "link": "http://arxiv.org/abs/cond-mat/0612365v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "On the time complexity of 2-tag systems and small universal Turing   machines",
        "authors": [
            "Damien Woods",
            "Turlough Neary"
        ],
        "summary": "We show that 2-tag systems efficiently simulate Turing machines. As a corollary we find that the small universal Turing machines of Rogozhin, Minsky and others simulate Turing machines in polynomial time. This is an exponential improvement on the previously known simulation time overhead and improves a forty year old result in the area of small universal Turing machines.",
        "published": "2006-12-19T15:59:45Z",
        "link": "http://arxiv.org/abs/cs/0612089v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.1; F.1.3; F.2.3"
        ]
    },
    {
        "title": "Tales of Huffman",
        "authors": [
            "Paul M. B. Vitanyi",
            "Zvi Lotker"
        ],
        "summary": "We study the new problem of Huffman-like codes subject to individual restrictions on the code-word lengths of a subset of the source words. These are prefix codes with minimal expected code-word length for a random source where additionally the code-word lengths of a subset of the source words is prescribed, possibly differently for every such source word. Based on a structural analysis of properties of optimal solutions, we construct an efficient dynamic programming algorithm for this problem, and for an integer programming problem that may be of independent interest.",
        "published": "2006-12-25T15:09:47Z",
        "link": "http://arxiv.org/abs/cs/0612133v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Geometric Complexity Theory II: Towards explicit obstructions for   embeddings among class varieties",
        "authors": [
            "Ketan D Mulmuley",
            "Milind Sohoni"
        ],
        "summary": "In part I we reduced the arithmetic (characteristic zero) version of the P \\not \\subseteq NP conjecture to the problem of showing that a variety associated with the complexity class NP cannot be embedded in the variety associated the complexity class P. We call these class varieties.   In this paper, this approach is developed further, reducing the nonexistence problems, such as the P vs. NP and related lower bound problems, to existence problems: specifically to proving existence of obstructions to such embeddings among class varieties. It gives two results towards explicit construction of such obstructions.   The first result is a generalization of the Borel-Weil theorem to a class of orbit closures, which include class varieties. The recond result is a weaker form of a conjectured analogue of the second fundamental theorem of invariant theory for the class variety associated with the complexity class NC. These results indicate that the fundamental lower bound problems in complexity theory are intimately linked with explicit construction problems in algebraic geometry and representation theory.",
        "published": "2006-12-25T18:51:47Z",
        "link": "http://arxiv.org/abs/cs/0612134v1",
        "categories": [
            "cs.CC",
            "math.AG",
            "math.RT",
            "F.1.3"
        ]
    },
    {
        "title": "On the Computational Complexity of Defining Sets",
        "authors": [
            "Hamed Hatami",
            "Hossein Maserrat"
        ],
        "summary": "Suppose we have a family ${\\cal F}$ of sets. For every $S \\in {\\cal F}$, a set $D \\subseteq S$ is a {\\sf defining set} for $({\\cal F},S)$ if $S$ is the only element of $\\cal{F}$ that contains $D$ as a subset. This concept has been studied in numerous cases, such as vertex colorings, perfect matchings, dominating sets, block designs, geodetics, orientations, and Latin squares.   In this paper, first, we propose the concept of a defining set of a logical formula, and we prove that the computational complexity of such a problem is $\\Sigma_2$-complete.   We also show that the computational complexity of the following problem about the defining set of vertex colorings of graphs is $\\Sigma_2$-complete:   {\\sc Instance:} A graph $G$ with a vertex coloring $c$ and an integer $k$.   {\\sc Question:} If ${\\cal C}(G)$ be the set of all $\\chi(G)$-colorings of $G$, then does $({\\cal C}(G),c)$ have a defining set of size at most $k$?   Moreover, we study the computational complexity of some other variants of this problem.",
        "published": "2006-12-31T05:47:37Z",
        "link": "http://arxiv.org/abs/cs/0701008v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Hierarchical Analysis of Propositional Temporal Logic Based on   Intervals",
        "authors": [
            "Ben Moszkowski"
        ],
        "summary": "We present a hierarchical framework for analysing propositional linear-time temporal logic (PTL) to obtain standard results such as a small model property, decision procedures and axiomatic completeness. Both finite time and infinite time are considered and one consequent benefit of the framework is the ability to systematically reduce infinite-time reasoning to finite-time reasoning. The treatment of PTL with both the operator Until and past time naturally reduces to that for PTL without either one. Our method utilises a low-level normal form for PTL called a \"transition configuration\". In addition, we employ reasoning about intervals of time. Besides being hierarchical and interval-based, the approach differs from other analyses of PTL typically based on sets of formulas and sequences of such sets. Instead we describe models using time intervals represented as finite and infinite sequences of states. The analysis relates larger intervals with smaller ones. Steps involved are expressed in Propositional Interval Temporal Logic (PITL) which is better suited than PTL for sequentially combining and decomposing formulas. Consequently, we can articulate issues in PTL model construction of equal relevance in more conventional analyses but normally only considered at the metalevel. We also describe a decision procedure based on Binary Decision Diagrams.",
        "published": "2006-01-05T18:35:38Z",
        "link": "http://arxiv.org/abs/cs/0601008v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Forward slicing of functional logic programs by partial evaluation",
        "authors": [
            "Josep Silva",
            "Germán Vidal"
        ],
        "summary": "Program slicing has been mainly studied in the context of imperative languages, where it has been applied to a wide variety of software engineering tasks, like program understanding, maintenance, debugging, testing, code reuse, etc. This work introduces the first forward slicing technique for declarative multi-paradigm programs which integrate features from functional and logic programming. Basically, given a program and a slicing criterion (a function call in our setting), the computed forward slice contains those parts of the original program which are reachable from the slicing criterion. Our approach to program slicing is based on an extension of (online) partial evaluation. Therefore, it provides a simple way to develop program slicing tools from existing partial evaluators and helps to clarify the relation between both methodologies. A slicing tool for the multi-paradigm language Curry, which demonstrates the usefulness of our approach, has been implemented in Curry itself.",
        "published": "2006-01-06T00:41:22Z",
        "link": "http://arxiv.org/abs/cs/0601013v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.4; I.2.2"
        ]
    },
    {
        "title": "Probabilistic bisimilarities between quantum processes",
        "authors": [
            "Yuan Feng",
            "Runyao Duan",
            "Zhengfeng Ji",
            "Mingsheng Ying"
        ],
        "summary": "Modeling and reasoning about concurrent quantum systems is very important both for distributed quantum computing and for quantum protocol verification. As a consequence, a general framework describing formally the communication and concurrency in complex quantum systems is necessary. For this purpose, we propose a model qCCS which is a natural quantum extension of classical value-passing CCS with the input and output of quantum states, and unitary transformations and measurements on quantum systems. The operational semantics of qCCS is given based on probabilistic labeled transition system. This semantics has many different features compared with the proposals in literature in order to describe input and output of quantum systems which are possibly correlated with other components. Based on this operational semantics, we introduce the notions of strong probabilistic bisimilarity and weak probabilistic bisimilarity between quantum processes and discuss some properties of them, such as congruence under various combinators.",
        "published": "2006-01-06T02:47:41Z",
        "link": "http://arxiv.org/abs/cs/0601014v3",
        "categories": [
            "cs.LO",
            "quant-ph",
            "F.1.2"
        ]
    },
    {
        "title": "A comparison between two logical formalisms for rewriting",
        "authors": [
            "Miguel Palomino"
        ],
        "summary": "Meseguer's rewriting logic and the rewriting logic CRWL are two well-known approaches to rewriting as logical deduction that, despite some clear similarities, were designed with different objectives. Here we study the relationships between them, both at a syntactic and at a semantic level. Even though it is not possible to establish an entailment system map between them, both can be naturally simulated in each other. Semantically, there is no embedding between the corresponding institutions. Along the way, the notions of entailment and satisfaction in Meseguer's rewriting logic are generalized. We also use the syntactic results to prove reflective properties of CRWL.",
        "published": "2006-01-06T12:01:55Z",
        "link": "http://arxiv.org/abs/cs/0601018v1",
        "categories": [
            "cs.LO",
            "D.1.6; F.3.1"
        ]
    },
    {
        "title": "Efficient Open World Reasoning for Planning",
        "authors": [
            "Tamara Babaian",
            "James G. Schmolze"
        ],
        "summary": "We consider the problem of reasoning and planning with incomplete knowledge and deterministic actions. We introduce a knowledge representation scheme called PSIPLAN that can effectively represent incompleteness of an agent's knowledge while allowing for sound, complete and tractable entailment in domains where the set of all objects is either unknown or infinite. We present a procedure for state update resulting from taking an action in PSIPLAN that is correct, complete and has only polynomial complexity. State update is performed without considering the set of all possible worlds corresponding to the knowledge state. As a result, planning with PSIPLAN is done without direct manipulation of possible worlds. PSIPLAN representation underlies the PSIPOP planning algorithm that handles quantified goals with or without exceptions that no other domain independent planner has been shown to achieve. PSIPLAN has been implemented in Common Lisp and used in an application on planning in a collaborative interface.",
        "published": "2006-01-09T18:07:37Z",
        "link": "http://arxiv.org/abs/cs/0601032v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.2.8; F.4.1; F.2.2"
        ]
    },
    {
        "title": "Using First-Order Logic to Reason about Policies",
        "authors": [
            "Joseph Y. Halpern",
            "Vicky Weissman"
        ],
        "summary": "A policy describes the conditions under which an action is permitted or forbidden. We show that a fragment of (multi-sorted) first-order logic can be used to represent and reason about policies. Because we use first-order logic, policies have a clear syntax and semantics. We show that further restricting the fragment results in a language that is still quite expressive yet is also tractable. More precisely, questions about entailment, such as `May Alice access the file?', can be answered in time that is a low-order polynomial (indeed, almost linear in some cases), as can questions about the consistency of policy sets.",
        "published": "2006-01-10T04:30:49Z",
        "link": "http://arxiv.org/abs/cs/0601034v3",
        "categories": [
            "cs.LO",
            "cs.CR",
            "H.2.7; K.4.4"
        ]
    },
    {
        "title": "Constraint-based automatic verification of abstract models of   multithreaded programs",
        "authors": [
            "Giorgio Delzanno"
        ],
        "summary": "We present a technique for the automated verification of abstract models of multithreaded programs providing fresh name generation, name mobility, and unbounded control.   As high level specification language we adopt here an extension of communication finite-state machines with local variables ranging over an infinite name domain, called TDL programs. Communication machines have been proved very effective for representing communication protocols as well as for representing abstractions of multithreaded software.   The verification method that we propose is based on the encoding of TDL programs into a low level language based on multiset rewriting and constraints that can be viewed as an extension of Petri Nets. By means of this encoding, the symbolic verification procedure developed for the low level language in our previous work can now be applied to TDL programs. Furthermore, the encoding allows us to isolate a decidable class of verification problems for TDL programs that still provide fresh name generation, name mobility, and unbounded control. Our syntactic restrictions are in fact defined on the internal structure of threads: In order to obtain a complete and terminating method, threads are only allowed to have at most one local variable (ranging over an infinite domain of names).",
        "published": "2006-01-10T12:57:55Z",
        "link": "http://arxiv.org/abs/cs/0601038v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "LPAR-05 Workshop: Empirically Successfull Automated Reasoning in   Higher-Order Logic (ESHOL)",
        "authors": [
            "Christoph Benzmueller",
            "John Harrison",
            "Carsten Schuermann"
        ],
        "summary": "This workshop brings together practioners and researchers who are involved in the everyday aspects of logical systems based on higher-order logic. We hope to create a friendly and highly interactive setting for discussions around the following four topics. Implementation and development of proof assistants based on any notion of impredicativity, automated theorem proving tools for higher-order logic reasoning systems, logical framework technology for the representation of proofs in higher-order logic, formal digital libraries for storing, maintaining and querying databases of proofs.   We envision attendees that are interested in fostering the development and visibility of reasoning systems for higher-order logics. We are particularly interested in a discusssion on the development of a higher-order version of the TPTP and in comparisons of the practical strengths of automated higher-order reasoning systems. Additionally, the workshop includes system demonstrations.   ESHOL is the successor of the ESCAR and ESFOR workshops held at CADE 2005 and IJCAR 2004.",
        "published": "2006-01-10T18:43:59Z",
        "link": "http://arxiv.org/abs/cs/0601042v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Combining Relational Algebra, SQL, Constraint Modelling, and Local   Search",
        "authors": [
            "Marco Cadoli",
            "Toni Mancini"
        ],
        "summary": "The goal of this paper is to provide a strong integration between constraint modelling and relational DBMSs. To this end we propose extensions of standard query languages such as relational algebra and SQL, by adding constraint modelling capabilities to them. In particular, we propose non-deterministic extensions of both languages, which are specially suited for combinatorial problems. Non-determinism is introduced by means of a guessing operator, which declares a set of relations to have an arbitrary extension. This new operator results in languages with higher expressive power, able to express all problems in the complexity class NP. Some syntactical restrictions which make data complexity polynomial are shown. The effectiveness of both extensions is demonstrated by means of several examples. The current implementation, written in Java using local search techniques, is described. To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2006-01-11T14:29:44Z",
        "link": "http://arxiv.org/abs/cs/0601043v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "A Constructive Semantic Characterization of Aggregates in ASP",
        "authors": [
            "Tran Cao Son",
            "Enrico Pontelli"
        ],
        "summary": "This technical note describes a monotone and continuous fixpoint operator to compute the answer sets of programs with aggregates. The fixpoint operator relies on the notion of aggregate solution. Under certain conditions, this operator behaves identically to the three-valued immediate consequence operator $\\Phi^{aggr}_P$ for aggregate programs, independently proposed Pelov et al. This operator allows us to closely tie the computational complexity of the answer set checking and answer sets existence problems to the cost of checking a solution of the aggregates in the program. Finally, we relate the semantics described by the operator to other proposals for logic programming with aggregates.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2006-01-13T16:09:36Z",
        "link": "http://arxiv.org/abs/cs/0601051v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.1.6; D.3.1; D.3.2; D.3.3"
        ]
    },
    {
        "title": "A Formal Foundation for ODRL",
        "authors": [
            "Riccardo Pucella",
            "Vicky Weissman"
        ],
        "summary": "ODRL is a popular XML-based language for stating the conditions under which resources can be accessed legitimately. The language is described in English and, as a result, agreements written in ODRL are open to interpretation. To address this problem, we propose a formal semantics for a representative fragment of the language. We use this semantics to determine precisely when a permission is implied by a set of ODRL statements and show that answering such questions is a decidable NP-hard problem. Finally, we define a tractable fragment of ODRL that is also fairly expressive.",
        "published": "2006-01-19T02:41:34Z",
        "link": "http://arxiv.org/abs/cs/0601085v1",
        "categories": [
            "cs.LO",
            "cs.CR",
            "H.2.7; K.4.4"
        ]
    },
    {
        "title": "On timed automata with input-determined guards",
        "authors": [
            "Deepak D'Souza",
            "Nicolas Tabareau"
        ],
        "summary": "We consider a general notion of timed automata with input-determined guards and show that they admit a robust logical framework along the lines of [D 'Souza03], in terms of a monadic second order logic characterisation and an expressively complete timed temporal logic. We then generalize these automata using the notion of recursive operators introduced by Henzinger, Raskin, and Schobbens, and show that they admit a similar logical framework. These results hold in the ``pointwise'' semantics. We finally use this framework to show that the real-time logic MITL of Alur et al is expressively complete with respect to an MSO corresponding to an appropriate input-determined operator.",
        "published": "2006-01-23T10:45:25Z",
        "link": "http://arxiv.org/abs/cs/0601096v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Complexity of the Guarded Two-Variable Fragment with Counting   Quantifiers",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "We show that the finite satisfiability problem for the guarded two-variable fragment with counting quantifiers is in EXPTIME. The method employed also yields a simple proof of a result recently obtained by Y. Kazakov, that the satisfiability problem for the guarded two-variable fragment with counting quantifiers is in EXPTIME.",
        "published": "2006-01-26T10:38:11Z",
        "link": "http://arxiv.org/abs/cs/0601112v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Efficient Query Answering over Conceptual Schemas of Relational   Databases : Technical Report",
        "authors": [
            "Mantas Simkus",
            "Evaldas Taroza",
            "Lina Lubyte",
            "Daniel Trivellato",
            "Zivile Norkunaite"
        ],
        "summary": "We develop a query answering system, where at the core of the work there is an idea of query answering by rewriting. For this purpose we extend the DL DL-Lite with the ability to support n-ary relations, obtaining the DL DLR-Lite, which is still polynomial in the size of the data. We devise a flexible way of mapping the conceptual level to the relational level, which provides the users an SQL-like query language over the conceptual schema. The rewriting technique adds value to conventional query answering techniques, allowing to formulate simpler queries, with the ability to infer additional information that was not stated explicitly in the user query. The formalization of the conceptual schema and the developed reasoning technique allow checking for consistency between the database and the conceptual schema, thus improving the trustiness of the information system.",
        "published": "2006-01-27T16:21:25Z",
        "link": "http://arxiv.org/abs/cs/0601114v2",
        "categories": [
            "cs.DB",
            "cs.LO"
        ]
    },
    {
        "title": "Reducibility of Gene Patterns in Ciliates using the Breakpoint Graph",
        "authors": [
            "Robert Brijder",
            "Hendrik Jan Hoogeboom",
            "Grzegorz Rozenberg"
        ],
        "summary": "Gene assembly in ciliates is one of the most involved DNA processings going on in any organism. This process transforms one nucleus (the micronucleus) into another functionally different nucleus (the macronucleus). We continue the development of the theoretical models of gene assembly, and in particular we demonstrate the use of the concept of the breakpoint graph, known from another branch of DNA transformation research. More specifically: (1) we characterize the intermediate gene patterns that can occur during the transformation of a given micronuclear gene pattern to its macronuclear form; (2) we determine the number of applications of the loop recombination operation (the most basic of the three molecular operations that accomplish gene assembly) needed in this transformation; (3) we generalize previous results (and give elegant alternatives for some proofs) concerning characterizations of the micronuclear gene patterns that can be assembled using a specific subset of the three molecular operations.",
        "published": "2006-01-30T20:20:36Z",
        "link": "http://arxiv.org/abs/cs/0601122v1",
        "categories": [
            "cs.LO",
            "q-bio.GN"
        ]
    },
    {
        "title": "Combining decision procedures for the reals",
        "authors": [
            "Jeremy Avigad",
            "Harvey Friedman"
        ],
        "summary": "<p>We address the general problem of determining the validity of boolean combinations of equalities and inequalities between real-valued expressions. In particular, we consider methods of establishing such assertions using only restricted forms of distributivity. At the same time, we explore ways in which \"local\" decision or heuristic procedures for fragments of the theory of the reals can be amalgamated into global ones. </p> <p>Let <em>Tadd[Q]</em> be the first-order theory of the real numbers in the language of ordered groups, with negation, a constant <em>1</em>, and function symbols for multiplication by rational constants. Let <em>Tmult[Q]</em> be the analogous theory for the multiplicative structure, and let <em>T[Q]</em> be the union of the two. We show that although <em>T[Q]</em> is undecidable, the universal fragment of <em>T[Q]</em> is decidable. We also show that terms of <em>T[Q]</em>can fruitfully be put in a normal form. We prove analogous results for theories in which <em>Q</em> is replaced, more generally, by suitable subfields <em>F</em> of the reals. Finally, we consider practical methods of establishing quantifier-free validities that approximate our (impractical) decidability results.</p>",
        "published": "2006-01-31T14:48:05Z",
        "link": "http://arxiv.org/abs/cs/0601134v4",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Strategies of Loop Recombination in Ciliates",
        "authors": [
            "Robert Brijder",
            "Hendrik Jan Hoogeboom",
            "Michael Muskulus"
        ],
        "summary": "Gene assembly in ciliates is an extremely involved DNA transformation process, which transforms a nucleus, the micronucleus, to another functionally different nucleus, the macronucleus. In this paper we characterize which loop recombination operations (one of the three types of molecular operations that accomplish gene assembly) can possibly be applied in the transformation of a given gene from its micronuclear form to its macronuclear form. We also characterize in which order these loop recombination operations are applicable. This is done in the abstract and more general setting of so-called legal strings.",
        "published": "2006-01-31T17:43:36Z",
        "link": "http://arxiv.org/abs/cs/0601135v1",
        "categories": [
            "cs.LO",
            "q-bio.GN"
        ]
    },
    {
        "title": "Towards a Definition of an Algorithm",
        "authors": [
            "Noson S. Yanofsky"
        ],
        "summary": "We define an algorithm to be the set of programs that implement or express that algorithm. The set of all programs is partitioned into equivalence classes. Two programs are equivalent if they are essentially the same program. The set of equivalence classes forms the category of algorithms. Although the set of programs does not even form a category, the set of algorithms form a category with extra structure. The conditions we give that describe when two programs are essentially the same turn out to be coherence relations that enrich the category of algorithms with extra structure. Universal properties of the category of algorithms are proved.",
        "published": "2006-02-02T20:54:04Z",
        "link": "http://arxiv.org/abs/math/0602053v3",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.CT",
            "68W01, 03D20"
        ]
    },
    {
        "title": "Conjunctive Queries over Trees",
        "authors": [
            "Georg Gottlob",
            "Christoph Koch",
            "Klaus U. Schulz"
        ],
        "summary": "We study the complexity and expressive power of conjunctive queries over unranked labeled trees represented using a variety of structure relations such as ``child'', ``descendant'', and ``following'' as well as unary relations for node labels. We establish a framework for characterizing structures representing trees for which conjunctive queries can be evaluated efficiently. Then we completely chart the tractability frontier of the problem and establish a dichotomy theorem for our axis relations, i.e., we find all subset-maximal sets of axes for which query evaluation is in polynomial time and show that for all other cases, query evaluation is NP-complete. All polynomial-time results are obtained immediately using the proof techniques from our framework. Finally, we study the expressiveness of conjunctive queries over trees and show that for each conjunctive query, there is an equivalent acyclic positive query (i.e., a set of acyclic conjunctive queries), but that in general this query is not of polynomial size.",
        "published": "2006-02-02T23:28:24Z",
        "link": "http://arxiv.org/abs/cs/0602004v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "E.1; F.1.3; F.2.2; H.2.3; H.2.4; I.7.2"
        ]
    },
    {
        "title": "The intuitionistic fragment of computability logic at the propositional   level",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "This paper presents a soundness and completeness proof for propositional intuitionistic calculus with respect to the semantics of computability logic. The latter interprets formulas as interactive computational problems, formalized as games between a machine and its environment. Intuitionistic implication is understood as algorithmic reduction in the weakest possible -- and hence most natural -- sense, disjunction and conjunction as deterministic-choice combinations of problems (disjunction = machine's choice, conjunction = environment's choice), and \"absurd\" as a computational problem of universal strength. See http://www.cis.upenn.edu/~giorgi/cl.html for a comprehensive online source on computability logic.",
        "published": "2006-02-05T12:55:17Z",
        "link": "http://arxiv.org/abs/cs/0602011v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Algorithmic correspondence and completeness in modal logic. I. The core   algorithm SQEMA",
        "authors": [
            "Willem Conradie",
            "Valentin Goranko",
            "Dimiter Vakarelov"
        ],
        "summary": "Modal formulae express monadic second-order properties on Kripke frames, but in many important cases these have first-order equivalents. Computing such equivalents is important for both logical and computational reasons. On the other hand, canonicity of modal formulae is important, too, because it implies frame-completeness of logics axiomatized with canonical formulae.   Computing a first-order equivalent of a modal formula amounts to elimination of second-order quantifiers. Two algorithms have been developed for second-order quantifier elimination: SCAN, based on constraint resolution, and DLS, based on a logical equivalence established by Ackermann.   In this paper we introduce a new algorithm, SQEMA, for computing first-order equivalents (using a modal version of Ackermann's lemma) and, moreover, for proving canonicity of modal formulae. Unlike SCAN and DLS, it works directly on modal formulae, thus avoiding Skolemization and the subsequent problem of unskolemization. We present the core algorithm and illustrate it with some examples. We then prove its correctness and the canonicity of all formulae on which the algorithm succeeds. We show that it succeeds not only on all Sahlqvist formulae, but also on the larger class of inductive formulae, introduced in our earlier papers. Thus, we develop a purely algorithmic approach to proving canonical completeness in modal logic and, in particular, establish one of the most general completeness results in modal logic so far.",
        "published": "2006-02-07T10:10:12Z",
        "link": "http://arxiv.org/abs/cs/0602024v4",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "PLTL Partitioned Model Checking for Reactive Systems under Fairness   Assumptions",
        "authors": [
            "Samir Chouali",
            "Jacques Julliand",
            "Pierre-Alain Masson",
            "Françoise Bellegarde"
        ],
        "summary": "We are interested in verifying dynamic properties of finite state reactive systems under fairness assumptions by model checking. The systems we want to verify are specified through a top-down refinement process. In order to deal with the state explosion problem, we have proposed in previous works to partition the reachability graph, and to perform the verification on each part separately. Moreover, we have defined a class, called Bmod, of dynamic properties that are verifiable by parts, whatever the partition. We decide if a property P belongs to Bmod by looking at the form of the Buchi automaton that accepts the negation of P. However, when a property P belongs to Bmod, the property f => P, where f is a fairness assumption, does not necessarily belong to Bmod. In this paper, we propose to use the refinement process in order to build the parts on which the verification has to be performed. We then show that with such a partition, if a property P is verifiable by parts and if f is the expression of the fairness assumptions on a system, then the property f => P is still verifiable by parts. This approach is illustrated by its application to the chip card protocol T=1 using the B engineering design language.",
        "published": "2006-02-10T14:48:29Z",
        "link": "http://arxiv.org/abs/cs/0602040v1",
        "categories": [
            "cs.LO",
            "D.2.4"
        ]
    },
    {
        "title": "Bisimulations of enrichments",
        "authors": [
            "Vincent Schmitt",
            "Krzysztof Worytkiewicz"
        ],
        "summary": "In this paper we show that classical notions from automata theory such as simulation and bisimulation can be lifted to the context of enriched categories. The usual properties of bisimulation are nearly all preserved in this new context. The class of enriched functors that correspond to functionnal bisimulations surjective on objects is investigated and appears \"nearly\" open in the sense of Joyal and Moerdijk. Seeing the change of base techniques as a convenient means to define process refinement/abstractions, we give sufficient conditions for the change of base categories to preserve bisimularity. We apply these concepts to Betti's generalized automata, categorical transition systems, and other exotic categories.",
        "published": "2006-02-21T17:27:27Z",
        "link": "http://arxiv.org/abs/cs/0602077v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "The dimension of a variety",
        "authors": [
            "Ewa Graczyńska",
            "Dietmar Schweigert"
        ],
        "summary": "We invent the notion of a {\\it dimension of a variety} $V$ as the cardinality of all its proper {\\it derived} subvarieties (of the same type). The dimensions of varieties of lattices, varieties of regular bands and other general algebraic structures are determined.",
        "published": "2006-02-22T12:12:30Z",
        "link": "http://arxiv.org/abs/math/0602493v2",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.GM",
            "08B99, 08A40"
        ]
    },
    {
        "title": "Pivotal and Pivotal-discriminative Consequence Relations",
        "authors": [
            "Jonathan Ben-Naim"
        ],
        "summary": "In the present paper, we investigate consequence relations that are both paraconsistent and plausible (but still monotonic). More precisely, we put the focus on pivotal consequence relations, i.e. those relations that can be defined by a pivot (in the style of e.g. D. Makinson). A pivot is a fixed subset of valuations which are considered to be the important ones in the absolute sense. We worked with a general notion of valuation that covers e.g. the classical valuations as well as certain kinds of many-valued valuations. In the many-valued cases, pivotal consequence relations are paraconsistant (in addition to be plausible), i.e. they are capable of drawing reasonable conclusions which contain contradictions. We will provide in our general framework syntactic characterizations of several families of pivotal relations. In addition, we will provide, again in our general framework, characterizations of several families of pivotal discriminative consequence relations. The latter are defined exactly as the plain version, but contradictory conclusions are rejected. We will also answer negatively a representation problem that was left open by Makinson. Finally, we will put in evidence a connexion with X-logics from Forget, Risch, and Siegel. The motivations and the framework of the present paper are very close to those of a previous paper of the author which is about preferential consequence relations.",
        "published": "2006-03-01T15:15:46Z",
        "link": "http://arxiv.org/abs/cs/0603006v4",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Reasoning About Knowledge of Unawareness",
        "authors": [
            "Joseph Y. halpern",
            "Leandro Chaves Rego"
        ],
        "summary": "Awareness has been shown to be a useful addition to standard epistemic logic for many applications. However, standard propositional logics for knowledge and awareness cannot express the fact that an agent knows that there are facts of which he is unaware without there being an explicit fact that the agent knows he is unaware of. We propose a logic for reasoning about knowledge of unawareness, by extending Fagin and Halpern's \\emph{Logic of General Awareness}. The logic allows quantification over variables, so that there is a formula in the language that can express the fact that ``an agent explicitly knows that there exists a fact of which he is unaware''. Moreover, that formula can be true without the agent explicitly knowing that he is unaware of any particular formula. We provide a sound and complete axiomatization of the logic, using standard axioms from the literature to capture the quantification operator. Finally, we show that the validity problem for the logic is recursively enumerable, but not decidable.",
        "published": "2006-03-05T21:14:59Z",
        "link": "http://arxiv.org/abs/cs/0603020v1",
        "categories": [
            "cs.LO",
            "cs.MA"
        ]
    },
    {
        "title": "Characterizing the NP-PSPACE Gap in the Satisfiability Problem for Modal   Logic",
        "authors": [
            "Joseph Y. Halpern",
            "Leandro Chaves Rego"
        ],
        "summary": "There has been a great of work on characterizing the complexity of the satisfiability and validity problem for modal logics. In particular, Ladner showed that the validity problem for all logics between K, T, and S4 is {\\sl PSPACE}-complete, while for S5 it is {\\sl NP}-complete. We show that, in a precise sense, it is \\emph{negative introspection}, the axiom $\\neg K p \\rimp K \\neg K p$, that causes the gap. In a precise sense, if we require this axiom, then the satisfiability problem is {\\sl NP}-complete; without it, it is {\\sl PSPACE}-complete.",
        "published": "2006-03-05T23:11:10Z",
        "link": "http://arxiv.org/abs/cs/0603019v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Testing Graph Isomorphism in Parallel by Playing a Game",
        "authors": [
            "Martin Grohe",
            "Oleg Verbitsky"
        ],
        "summary": "Our starting point is the observation that if graphs in a class C have low descriptive complexity in first order logic, then the isomorphism problem for C is solvable by a fast parallel algorithm (essentially, by a simple combinatorial algorithm known as the multidimensional Weisfeiler-Lehman algorithm). Using this approach, we prove that isomorphism of graphs of bounded treewidth is testable in TC1, answering an open question posed by Chandrasekharan. Furthermore, we obtain an AC1 algorithm for testing isomorphism of rotation systems (combinatorial specifications of graph embeddings). The AC1 upper bound was known before, but the fact that this bound can be achieved by the simple Weisfeiler-Lehman algorithm is new. Combined with other known results, it also yields a new AC1 isomorphism algorithm for planar graphs.",
        "published": "2006-03-14T15:46:58Z",
        "link": "http://arxiv.org/abs/cs/0603054v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Unary Primitive Recursive Functions",
        "authors": [
            "Daniel E. Severin"
        ],
        "summary": "In this article, we study some new characterizations of primitive recursive functions based on restricted forms of primitive recursion, improving the pioneering work of R. M. Robinson and M. D. Gladstone in this area. We reduce certain recursion schemes (mixed/pure iteration without parameters) and we characterize one-argument primitive recursive functions as the closure under substitution and iteration of certain optimal sets.",
        "published": "2006-03-16T17:20:25Z",
        "link": "http://arxiv.org/abs/cs/0603063v3",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "An Explicit Solution to Post's Problem over the Reals",
        "authors": [
            "Klaus Meer",
            "Martin Ziegler"
        ],
        "summary": "In the BCSS model of real number computations we prove a concrete and explicit semi-decidable language to be undecidable yet not reducible from (and thus strictly easier than) the real Halting Language. This solution to Post's Problem over the reals significantly differs from its classical, discrete variant where advanced diagonalization techniques are only known to yield the existence of such intermediate Turing degrees. Strengthening the above result, we construct (that is, obtain again explicitly) as well an uncountable number of incomparable semi-decidable Turing degrees below the real Halting problem in the BCSS model. Finally we show the same to hold for the linear BCSS model, that is over (R,+,-,<) rather than (R,+,-,*,/,<).",
        "published": "2006-03-17T16:12:23Z",
        "link": "http://arxiv.org/abs/cs/0603071v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Yet Another Efficient Unification Algorithm",
        "authors": [
            "Alin Suciu"
        ],
        "summary": "The unification algorithm is at the core of the logic programming paradigm, the first unification algorithm being developed by Robinson [5]. More efficient algorithms were developed later [3] and I introduce here yet another efficient unification algorithm centered on a specific data structure, called the Unification Table.",
        "published": "2006-03-20T20:15:38Z",
        "link": "http://arxiv.org/abs/cs/0603080v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Random 3CNF formulas elude the Lovasz theta function",
        "authors": [
            "Uriel Feige",
            "Eran Ofek"
        ],
        "summary": "Let $\\phi$ be a 3CNF formula with n variables and m clauses. A simple nonconstructive argument shows that when m is sufficiently large compared to n, most 3CNF formulas are not satisfiable. It is an open question whether there is an efficient refutation algorithm that for most such formulas proves that they are not satisfiable. A possible approach to refute a formula $\\phi$ is: first, translate it into a graph $G_{\\phi}$ using a generic reduction from 3-SAT to max-IS, then bound the maximum independent set of $G_{\\phi}$ using the Lovasz $\\vartheta$ function. If the $\\vartheta$ function returns a value $< m$, this is a certificate for the unsatisfiability of $\\phi$. We show that for random formulas with $m < n^{3/2 -o(1)}$ clauses, the above approach fails, i.e. the $\\vartheta$ function is likely to return a value of m.",
        "published": "2006-03-22T10:30:36Z",
        "link": "http://arxiv.org/abs/cs/0603084v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Verification of Ptime reducibility for system F terms via Dual Light   Affine Logic",
        "authors": [
            "Vincent Atassi",
            "Patrick Baillot",
            "Kazushige Terui"
        ],
        "summary": "In a previous work we introduced Dual Light Affine Logic (DLAL) ([BaillotTerui04]) as a variant of Light Linear Logic suitable for guaranteeing complexity properties on lambda-calculus terms: all typable terms can be evaluated in polynomial time and all Ptime functions can be represented. In the present work we address the problem of typing lambda-terms in second-order DLAL. For that we give a procedure which, starting with a term typed in system F, finds all possible ways to decorate it into a DLAL typed term. We show that our procedure can be run in time polynomial in the size of the original Church typed system F term.",
        "published": "2006-03-27T09:14:01Z",
        "link": "http://arxiv.org/abs/cs/0603104v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Affine functions and series with co-inductive real numbers",
        "authors": [
            "Yves Bertot"
        ],
        "summary": "We extend the work of A. Ciaffaglione and P. Di Gianantonio on mechanical verification of algorithms for exact computation on real numbers, using infinite streams of digits implemented as co-inductive types. Four aspects are studied: the first aspect concerns the proof that digit streams can be related to the axiomatized real numbers that are already axiomatized in the proof system (axiomatized, but with no fixed representation). The second aspect re-visits the definition of an addition function, looking at techniques to let the proof search mechanism perform the effective construction of an algorithm that is correct by construction. The third aspect concerns the definition of a function to compute affine formulas with positive rational coefficients. This should be understood as a testbed to describe a technique to combine co-recursion and recursion to obtain a model for an algorithm that appears at first sight to be outside the expressive power allowed by the proof system. The fourth aspect concerns the definition of a function to compute series, with an application on the series that is used to compute Euler's number e. All these experiments should be reproducible in any proof system that supports co-inductive types, co-recursion and general forms of terminating recursion, but we performed with the Coq system [12, 3, 14].",
        "published": "2006-03-29T19:36:03Z",
        "link": "http://arxiv.org/abs/cs/0603117v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Coq in a Hurry",
        "authors": [
            "Yves Bertot"
        ],
        "summary": "These notes provide a quick introduction to the Coq system and show how it can be used to define logical concepts and functions and reason about them. It is designed as a tutorial, so that readers can quickly start their own experiments, learning only a few of the capabilities of the system. A much more comprehensive study is provided in [1], which also provides an extensive collection of exercises to train on.",
        "published": "2006-03-29T19:38:33Z",
        "link": "http://arxiv.org/abs/cs/0603118v3",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "CoInduction in Coq",
        "authors": [
            "Yves Bertot"
        ],
        "summary": "We describe the basic notions of co-induction as they are available in the coq system. As an application, we describe arithmetic properties for simple representations of real numbers.",
        "published": "2006-03-29T19:39:37Z",
        "link": "http://arxiv.org/abs/cs/0603119v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Real Computational Universality: The Word Problem for a class of groups   with infinite presentation",
        "authors": [
            "Martin Ziegler",
            "Klaus Meer"
        ],
        "summary": "The word problem for discrete groups is well-known to be undecidable by a Turing Machine; more precisely, it is reducible both to and from and thus equivalent to the discrete Halting Problem.   The present work introduces and studies a real extension of the word problem for a certain class of groups which are presented as quotient groups of a free group and a normal subgroup. Most important, the free group will be generated by an uncountable set of generators with index running over certain sets of real numbers. This allows to include many mathematically important groups which are not captured in the framework of the classical word problem.   Our contribution extends computational group theory from the discrete to the Blum-Shub-Smale (BSS) model of real number computation. We believe this to be an interesting step towards applying BSS theory, in addition to semi-algebraic geometry, also to further areas of mathematics.   The main result establishes the word problem for such groups to be not only semi-decidable (and thus reducible FROM) but also reducible TO the Halting Problem for such machines. It thus provides the first non-trivial example of a problem COMPLETE, that is, computationally universal for this model.",
        "published": "2006-04-07T23:57:46Z",
        "link": "http://arxiv.org/abs/cs/0604032v3",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.1.1; F.4.1; F.4.2"
        ]
    },
    {
        "title": "New results on rewrite-based satisfiability procedures",
        "authors": [
            "Alessandro Armando",
            "Maria Paola Bonacina",
            "Silvio Ranise",
            "Stephan Schulz"
        ],
        "summary": "Program analysis and verification require decision procedures to reason on theories of data structures. Many problems can be reduced to the satisfiability of sets of ground literals in theory T. If a sound and complete inference system for first-order logic is guaranteed to terminate on T-satisfiability problems, any theorem-proving strategy with that system and a fair search plan is a T-satisfiability procedure. We prove termination of a rewrite-based first-order engine on the theories of records, integer offsets, integer offsets modulo and lists. We give a modularity theorem stating sufficient conditions for termination on a combinations of theories, given termination on each. The above theories, as well as others, satisfy these conditions. We introduce several sets of benchmarks on these theories and their combinations, including both parametric synthetic benchmarks to test scalability, and real-world problems to test performances on huge sets of literals. We compare the rewrite-based theorem prover E with the validity checkers CVC and CVC Lite. Contrary to the folklore that a general-purpose prover cannot compete with reasoners with built-in theories, the experiments are overall favorable to the theorem prover, showing that not only the rewriting approach is elegant and conceptually simple, but has important practical implications.",
        "published": "2006-04-12T19:53:24Z",
        "link": "http://arxiv.org/abs/cs/0604054v4",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Quantum Fuzzy Sets: Blending Fuzzy Set Theory and Quantum Computation",
        "authors": [
            "Mirco A. Mannucci"
        ],
        "summary": "In this article we investigate a way in which quantum computing can be used to extend the class of fuzzy sets. The core idea is to see states of a quantum register as characteristic functions of quantum fuzzy subsets of a given set. As the real unit interval is embedded in the Bloch sphere, every fuzzy set is automatically a quantum fuzzy set. However, a generic quantum fuzzy set can be seen as a (possibly entangled) superposition of many fuzzy sets at once, offering new opportunities for modeling uncertainty. After introducing the main framework of quantum fuzzy set theory, we analyze the standard operations of fuzzification and defuzzification from our viewpoint. We conclude this preliminary paper with a list of possible applications of quantum fuzzy sets to pattern recognition, as well as future directions of pure research in quantum fuzzy set theory.",
        "published": "2006-04-16T16:23:10Z",
        "link": "http://arxiv.org/abs/cs/0604064v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Event Systems and Access Control",
        "authors": [
            "Dominique Méry",
            "Stephan Merz"
        ],
        "summary": "We consider the interpretations of notions of access control (permissions, interdictions, obligations, and user rights) as run-time properties of information systems specified as event systems with fairness. We give proof rules for verifying that an access control policy is enforced in a system, and consider preservation of access control by refinement of event systems. In particular, refinement of user rights is non-trivial; we propose to combine low-level user rights and system obligations to implement high-level user rights.",
        "published": "2006-04-21T13:21:40Z",
        "link": "http://arxiv.org/abs/cs/0604081v1",
        "categories": [
            "cs.LO",
            "cs.CR"
        ]
    },
    {
        "title": "The complexity of acyclic conjunctive queries revisited",
        "authors": [
            "Arnaud Durand",
            "Etienne Grandjean"
        ],
        "summary": "In this paper, we consider first-order logic over unary functions and study the complexity of the evaluation problem for conjunctive queries described by such kind of formulas. A natural notion of query acyclicity for this language is introduced and we study the complexity of a large number of variants or generalizations of acyclic query problems in that context (Boolean or not Boolean, with or without inequalities, comparisons, etc...). Our main results show that all those problems are \\textit{fixed-parameter linear} i.e. they can be evaluated in time $f(|Q|).|\\textbf{db}|.|Q(\\textbf{db})|$ where $|Q|$ is the size of the query $Q$, $|\\textbf{db}|$ the database size, $|Q(\\textbf{db})|$ is the size of the output and $f$ is some function whose value depends on the specific variant of the query problem (in some cases, $f$ is the identity function). Our results have two kinds of consequences. First, they can be easily translated in the relational (i.e., classical) setting. Previously known bounds for some query problems are improved and new tractable cases are then exhibited. Among others, as an immediate corollary, we improve a result of \\~\\cite{PapadimitriouY-99} by showing that any (relational) acyclic conjunctive query with inequalities can be evaluated in time $f(|Q|).|\\textbf{db}|.|Q(\\textbf{db})|$. A second consequence of our method is that it provides a very natural descriptive approach to the complexity of well-known algorithmic problems. A number of examples (such as acyclic subgraph problems, multidimensional matching, etc...) are considered for which new insights of their complexity are given.",
        "published": "2006-05-02T09:31:58Z",
        "link": "http://arxiv.org/abs/cs/0605008v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Fast and Generalized Polynomial Time Memory Consistency Verification",
        "authors": [
            "Amitabha Roy",
            "Stephan Zeisset",
            "Charles J. Fleckenstein",
            "John C. Huang"
        ],
        "summary": "The problem of verifying multi-threaded execution against the memory consistency model of a processor is known to be an NP hard problem. However polynomial time algorithms exist that detect almost all failures in such execution. These are often used in practice for microprocessor verification. We present a low complexity and fully parallelized algorithm to check program execution against the processor consistency model. In addition our algorithm is general enough to support a number of consistency models without any degradation in performance. An implementation of this algorithm is currently used in practice to verify processors in the post silicon stage for multiple architectures.",
        "published": "2006-05-09T05:45:52Z",
        "link": "http://arxiv.org/abs/cs/0605039v4",
        "categories": [
            "cs.AR",
            "cs.LO",
            "cs.PF"
        ]
    },
    {
        "title": "Continuations, proofs and tests",
        "authors": [
            "Stefano Guerrini",
            "Andrea Masini"
        ],
        "summary": "Continuation Passing Style (CPS) is one of the most important issues in the field of functional programming languages, and the quest for a primitive notion of types for continuation is still open. Starting from the notion of ``test'' proposed by Girard, we develop a notion of test for intuitionistic logic. We give a complete deductive system for tests and we show that it is good to deal with ``continuations''. In particular, in the proposed system it is possible to work with Call by Value and Call by Name translations in a uniform way.",
        "published": "2006-05-09T20:11:27Z",
        "link": "http://arxiv.org/abs/cs/0605043v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "From Proof Nets to the Free *-Autonomous Category",
        "authors": [
            "Francois Lamarche",
            "Lutz Strassburger"
        ],
        "summary": "In the first part of this paper we present a theory of proof nets for full multiplicative linear logic, including the two units. It naturally extends the well-known theory of unit-free multiplicative proof nets. A linking is no longer a set of axiom links but a tree in which the axiom links are subtrees. These trees will be identified according to an equivalence relation based on a simple form of graph rewriting. We show the standard results of sequentialization and strong normalization of cut elimination. In the second part of the paper we show that the identifications enforced on proofs are such that the class of two-conclusion proof nets defines the free *-autonomous category.",
        "published": "2006-05-12T12:03:16Z",
        "link": "http://arxiv.org/abs/cs/0605054v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Modal Logics of Topological Relations",
        "authors": [
            "Carsten Lutz",
            "Frank Wolter"
        ],
        "summary": "Logical formalisms for reasoning about relations between spatial regions play a fundamental role in geographical information systems, spatial and constraint databases, and spatial reasoning in AI. In analogy with Halpern and Shoham's modal logic of time intervals based on the Allen relations, we introduce a family of modal logics equipped with eight modal operators that are interpreted by the Egenhofer-Franzosa (or RCC8) relations between regions in topological spaces such as the real plane. We investigate the expressive power and computational complexity of logics obtained in this way. It turns out that our modal logics have the same expressive power as the two-variable fragment of first-order logic, but are exponentially less succinct. The complexity ranges from (undecidable and) recursively enumerable to highly undecidable, where the recursively enumerable logics are obtained by considering substructures of structures induced by topological spaces. As our undecidability results also capture logics based on the real line, they improve upon undecidability results for interval temporal logics by Halpern and Shoham. We also analyze modal logics based on the five RCC5 relations, with similar results regarding the expressive power, but weaker results regarding the complexity.",
        "published": "2006-05-15T14:42:36Z",
        "link": "http://arxiv.org/abs/cs/0605064v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC",
            "F.4.1; H.2.8; I.2.4"
        ]
    },
    {
        "title": "SAT Solving for Argument Filterings",
        "authors": [
            "Michael Codish",
            "Peter Schneider-Kamp",
            "Vitaly Lagoon",
            "René Thiemann",
            "Jürgen Giesl"
        ],
        "summary": "This paper introduces a propositional encoding for lexicographic path orders in connection with dependency pairs. This facilitates the application of SAT solvers for termination analysis of term rewrite systems based on the dependency pair method. We address two main inter-related issues and encode them as satisfiability problems of propositional formulas that can be efficiently handled by SAT solving: (1) the combined search for a lexicographic path order together with an \\emph{argument filtering} to orient a set of inequalities; and (2) how the choice of the argument filtering influences the set of inequalities that have to be oriented. We have implemented our contributions in the termination prover AProVE. Extensive experiments show that by our encoding and the application of SAT solvers one obtains speedups in orders of magnitude as well as increased termination proving power.",
        "published": "2006-05-17T12:47:18Z",
        "link": "http://arxiv.org/abs/cs/0605074v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Scalable Algorithm for Minimal Unsatisfiable Core Extraction",
        "authors": [
            "Nachum Dershowitz",
            "Ziyad Hanna",
            "Alexander Nadel"
        ],
        "summary": "We propose a new algorithm for minimal unsatisfiable core extraction, based on a deeper exploration of resolution-refutation properties. We provide experimental results on formal verification benchmarks confirming that our algorithm finds smaller cores than suboptimal algorithms; and that it runs faster than those algorithms that guarantee minimality of the core.",
        "published": "2006-05-19T08:19:37Z",
        "link": "http://arxiv.org/abs/cs/0605085v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Proof Search in Hajek's Basic Logic",
        "authors": [
            "S. Bova",
            "F. Montagna"
        ],
        "summary": "We introduce a proof system for Hajek's logic BL based on a relational hypersequents framework. We prove that the rules of our logical calculus, called RHBL, are sound and invertible with respect to any valuation of BL into a suitable algebra, called omega[0,1]. Refining the notion of reduction tree that arises naturally from RHBL, we obtain a decision algorithm for BL provability whose running time upper bound is 2^O(n), where n is the number of connectives of the input formula. Moreover, if a formula is unprovable, we exploit the constructiveness of a polynomial time algorithm for leaves validity for providing a procedure to build countermodels in omega[0,1]. Finally, since the size of the reduction tree branches is O(n^3), we can describe a polynomial time verification algorithm for BL unprovability.",
        "published": "2006-05-22T08:18:56Z",
        "link": "http://arxiv.org/abs/cs/0605094v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Supervisory Control of Fuzzy Discrete Event Systems: A Formal Approach",
        "authors": [
            "Daowen Qiu"
        ],
        "summary": "Fuzzy {\\it discrete event systems} (DESs) were proposed recently by Lin and Ying [19], which may better cope with the real-world problems with fuzziness, impreciseness, and subjectivity such as those in biomedicine. As a continuation of [19], in this paper we further develop fuzzy DESs by dealing with supervisory control of fuzzy DESs. More specifically, (i) we reformulate the parallel composition of crisp DESs, and then define the parallel composition of fuzzy DESs that is equivalent to that in [19]; {\\it max-product} and {\\it max-min} automata for modeling fuzzy DESs are considered; (ii) we deal with a number of fundamental problems regarding supervisory control of fuzzy DESs, particularly demonstrate controllability theorem and nonblocking controllability theorem of fuzzy DESs, and thus present the conditions for the existence of supervisors in fuzzy DESs; (iii) we analyze the complexity for presenting a uniform criterion to test the fuzzy controllability condition of fuzzy DESs modeled by max-product automata; in particular, we present in detail a general computing method for checking whether or not the fuzzy controllability condition holds, if max-min automata are used to model fuzzy DESs, and by means of this method we can search for all possible fuzzy states reachable from initial fuzzy state in max-min automata; also, we introduce the fuzzy $n$-controllability condition for some practical problems; (iv) a number of examples serving to illustrate the applications of the derived results and methods are described; some basic properties related to supervisory control of fuzzy DESs are investigated. To conclude, some related issues are raised for further consideration.",
        "published": "2006-05-24T15:23:34Z",
        "link": "http://arxiv.org/abs/cs/0605106v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.1.2; F.4.3; J.7"
        ]
    },
    {
        "title": "Fuzzy Discrete Event Systems under Fuzzy Observability and a   test-algorithm",
        "authors": [
            "Daowen Qiu",
            "Fuchun Liu"
        ],
        "summary": "In order to more effectively cope with the real-world problems of vagueness, impreciseness, and subjectivity, fuzzy discrete event systems (FDESs) were proposed recently. Notably, FDESs have been applied to biomedical control for HIV/AIDS treatment planning and sensory information processing for robotic control. Qiu, Cao and Ying independently developed supervisory control theory of FDESs. We note that the controllability of events in Qiu's work is fuzzy but the observability of events is crisp, and, the observability of events in Cao and Ying's work is also crisp although the controllability is not completely crisp since the controllable events can be disabled with any degrees. Motivated by the necessity to consider the situation that the events may be observed or controlled with some membership degrees, in this paper, we establish the supervisory control theory of FDESs with partial observations, in which both the observability and controllability of events are fuzzy instead. We formalize the notions of fuzzy controllability condition and fuzzy observability condition. And Controllability and Observability Theorem of FDESs is set up in a more generic framework. In particular, we present a detailed computing flow to verify whether the controllability and observability conditions hold. Thus, this result can decide the existence of supervisors. Also, we use this computing method to check the existence of supervisors in the Controllability and Observability Theorem of classical discrete event systems (DESs), which is a new method and different from classical case. A number of examples are elaborated on to illustrate the presented results.",
        "published": "2006-05-24T15:41:00Z",
        "link": "http://arxiv.org/abs/cs/0605107v3",
        "categories": [
            "cs.LO",
            "F.1.2; F.4.3; J.7"
        ]
    },
    {
        "title": "Logic Column 15: Coalgebras and Their Logics",
        "authors": [
            "Alexander Kurz"
        ],
        "summary": "This article describes recent work on the topic of specifying properties of transition systems. By giving a suitably abstract description of transition systems as coalgebras, it is possible to derive logics for capturing properties of these transition systems in an elegant way.",
        "published": "2006-05-28T19:30:12Z",
        "link": "http://arxiv.org/abs/cs/0605128v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A synchronous pi-calculus",
        "authors": [
            "Roberto Amadio"
        ],
        "summary": "The SL synchronous programming model is a relaxation of the Esterel synchronous model where the reaction to the absence of a signal within an instant can only happen at the next instant. In previous work, we have revisited the SL synchronous programming model. In particular, we have discussed an alternative design of the model including thread spawning and recursive definitions, introduced a CPS translation to a tail recursive form, and proposed a notion of bisimulation equivalence. In the present work, we extend the tail recursive model with first-order data types obtaining a non-deterministic synchronous model whose complexity is comparable to the one of the pi-calculus. We show that our approach to bisimulation equivalence can cope with this extension and in particular that labelled bisimulation can be characterised as a contextual bisimulation.",
        "published": "2006-06-05T16:46:41Z",
        "link": "http://arxiv.org/abs/cs/0606019v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Syntactic Characterisations of Polynomial-Time Optimisation Classes   (Syntactic Characterizations of Polynomial-Time Optimization Classes)",
        "authors": [
            "Prabhu Manyem"
        ],
        "summary": "In Descriptive Complexity, there is a vast amount of literature on decision problems, and their classes such as \\textbf{P, NP, L and NL}. ~ However, research on the descriptive complexity of optimisation problems has been limited. Optimisation problems corresponding to the \\textbf{NP} class have been characterised in terms of logic expressions by Papadimitriou and Yannakakis, Panconesi and Ranjan, Kolaitis and Thakur, Khanna et al, and by Zimand. Gr\\\"{a}del characterised the polynomial class \\textbf{P} of decision problems. In this paper, we attempt to characterise the optimisation versions of \\textbf{P} via expressions in second order logic, many of them using universal Horn formulae with successor relations. The polynomially bound versions of maximisation (maximization) and minimisation (minimization) problems are treated first, and then the maximisation problems in the \"not necessarily polynomially bound\" class.",
        "published": "2006-06-12T00:42:29Z",
        "link": "http://arxiv.org/abs/cs/0606050v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3"
        ]
    },
    {
        "title": "Context-Sensitive Languages, Rational Graphs and Determinism",
        "authors": [
            "Arnaud Carayol",
            "Antoine Meyer"
        ],
        "summary": "We investigate families of infinite automata for context-sensitive languages. An infinite automaton is an infinite labeled graph with two sets of initial and final vertices. Its language is the set of all words labelling a path from an initial vertex to a final vertex. In 2001, Morvan and Stirling proved that rational graphs accept the context-sensitive languages between rational sets of initial and final vertices. This result was later extended to sub-families of rational graphs defined by more restricted classes of transducers. languages.<br><br>   Our contribution is to provide syntactical and self-contained proofs of the above results, when earlier constructions relied on a non-trivial normal form of context-sensitive grammars defined by Penttonen in the 1970's. These new proof techniques enable us to summarize and refine these results by considering several sub-families defined by restrictions on the type of transducers, the degree of the graph or the size of the set of initial vertices.",
        "published": "2006-06-12T09:35:18Z",
        "link": "http://arxiv.org/abs/cs/0606053v2",
        "categories": [
            "cs.LO",
            "F.4.3"
        ]
    },
    {
        "title": "Lower bounds and complete problems in nondeterministic linear time and   sublinear space complexity classes",
        "authors": [
            "Philippe Chapdelaine",
            "Etienne Grandjean"
        ],
        "summary": "Proving lower bounds remains the most difficult of tasks in computational complexity theory. In this paper, we show that whereas most natural NP-complete problems belong to NLIN (linear time on nondeterministic RAMs), some of them, typically the planar versions of many NP-complete problems are recognized by nondeterministic RAMs in linear time and sublinear space. The main results of this paper are the following: as the second author did for NLIN, we give exact logical characterizations of nondeterministic polynomial time-space complexity classes; we derive from them a class of problems, which are complete in these classes, and as a consequence of such a precise result and of some recent separation theorems using diagonalization, prove time-space lower bounds for these problems.",
        "published": "2006-06-13T08:10:15Z",
        "link": "http://arxiv.org/abs/cs/0606058v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "Logics for Unranked Trees: An Overview",
        "authors": [
            "Leonid Libkin"
        ],
        "summary": "Labeled unranked trees are used as a model of XML documents, and logical languages for them have been studied actively over the past several years. Such logics have different purposes: some are better suited for extracting data, some for expressing navigational properties, and some make it easy to relate complex properties of trees to the existence of tree automata for those properties. Furthermore, logics differ significantly in their model-checking properties, their automata models, and their behavior on ordered and unordered trees. In this paper we present a survey of logics for unranked trees.",
        "published": "2006-06-13T16:27:28Z",
        "link": "http://arxiv.org/abs/cs/0606062v2",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.2.3; H.2.1; I.7; F.2.3; F.4.1; F.4.3"
        ]
    },
    {
        "title": "On the complexity of XPath containment in the presence of disjunction,   DTDs, and variables",
        "authors": [
            "Frank Neven",
            "Thomas Schwentick"
        ],
        "summary": "XPath is a simple language for navigating an XML-tree and returning a set of answer nodes. The focus in this paper is on the complexity of the containment problem for various fragments of XPath. We restrict attention to the most common XPath expressions which navigate along the child and/or descendant axis. In addition to basic expressions using only node tests and simple predicates, we also consider disjunction and variables (ranging over nodes). Further, we investigate the containment problem relative to a given DTD. With respect to variables we study two semantics, (1) the original semantics of XPath, where the values of variables are given by an outer context, and (2) an existential semantics introduced by Deutsch and Tannen, in which the values of variables are existentially quantified. In this framework, we establish an exact classification of the complexity of the containment problem for many XPath fragments.",
        "published": "2006-06-14T09:49:17Z",
        "link": "http://arxiv.org/abs/cs/0606065v4",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2; I.7.2; F.4"
        ]
    },
    {
        "title": "Relational Parametricity and Control",
        "authors": [
            "Masahito Hasegawa"
        ],
        "summary": "We study the equational theory of Parigot's second-order &lambda;&mu;-calculus in connection with a call-by-name continuation-passing style (CPS) translation into a fragment of the second-order &lambda;-calculus. It is observed that the relational parametricity on the target calculus induces a natural notion of equivalence on the &lambda;&mu;-terms. On the other hand, the unconstrained relational parametricity on the &lambda;&mu;-calculus turns out to be inconsistent with this CPS semantics. Following these facts, we propose to formulate the relational parametricity on the &lambda;&mu;-calculus in a constrained way, which might be called ``focal parametricity''.",
        "published": "2006-06-15T11:45:01Z",
        "link": "http://arxiv.org/abs/cs/0606072v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Lack of Finite Characterizations for the Distance-based Revision",
        "authors": [
            "Jonathan Ben-Naim"
        ],
        "summary": "Lehmann, Magidor, and Schlechta developed an approach to belief revision based on distances between any two valuations. Suppose we are given such a distance D. This defines an operator |D, called a distance operator, which transforms any two sets of valuations V and W into the set V |D W of all elements of W that are closest to V. This operator |D defines naturally the revision of K by A as the set of all formulas satisfied in M(K) |D M(A) (i.e. those models of A that are closest to the models of K). This constitutes a distance-based revision operator. Lehmann et al. characterized families of them using a loop condition of arbitrarily big size. An interesting question is whether this loop condition can be replaced by a finite one. Extending the results of Schlechta, we will provide elements of negative answer. In fact, we will show that for families of distance operators, there is no \"normal\" characterization. Approximatively, a normal characterization contains only finite and universally quantified conditions. These results have an interest of their own for they help to understand the limits of what is possible in this area. Now, we are quite confident that this work can be continued to show similar impossibility results for distance-based revision operators, which suggests that the big loop condition cannot be simplified.",
        "published": "2006-06-19T09:34:27Z",
        "link": "http://arxiv.org/abs/cs/0606082v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "The Completeness of Propositional Resolution: A Simple and   Constructive<br> Proof",
        "authors": [
            "Jean Gallier"
        ],
        "summary": "It is well known that the resolution method (for propositional logic) is complete. However, completeness proofs found in the literature use an argument by contradiction showing that if a set of clauses is unsatisfiable, then it must have a resolution refutation. As a consequence, none of these proofs actually gives an algorithm for producing a resolution refutation from an unsatisfiable set of clauses. In this note, we give a simple and constructive proof of the completeness of propositional resolution which consists of an algorithm together with a proof of its correctness.",
        "published": "2006-06-19T17:40:42Z",
        "link": "http://arxiv.org/abs/cs/0606084v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2"
        ]
    },
    {
        "title": "Uniform Random Sampling of Traces in Very Large Models",
        "authors": [
            "Alain Denise",
            "Marie-Claude Gaudel",
            "Sandrine-Dominique Gouraud",
            "Richard Lasseigne",
            "Sylvain Peyronnet",
            "the RaST Collaboration"
        ],
        "summary": "This paper presents some first results on how to perform uniform random walks (where every trace has the same probability to occur) in very large models. The models considered here are described in a succinct way as a set of communicating reactive modules. The method relies upon techniques for counting and drawing uniformly at random words in regular languages. Each module is considered as an automaton defining such a language. It is shown how it is possible to combine local uniform drawings of traces, and to obtain some global uniform random sampling, without construction of the global model.",
        "published": "2006-06-20T09:53:41Z",
        "link": "http://arxiv.org/abs/cs/0606086v1",
        "categories": [
            "cs.LO",
            "D.2.4; D.2.5"
        ]
    },
    {
        "title": "Planar Graphs: Logical Complexity and Parallel Isomorphism Tests",
        "authors": [
            "Oleg Verbitsky"
        ],
        "summary": "We prove that every triconnected planar graph is definable by a first order sentence that uses at most 15 variables and has quantifier depth at most $11\\log_2 n+43$. As a consequence, a canonic form of such graphs is computable in $AC^1$ by the 14-dimensional Weisfeiler-Lehman algorithm. This provides another way to show that the planar graph isomorphism is solvable in $AC^1$.",
        "published": "2006-07-08T10:37:52Z",
        "link": "http://arxiv.org/abs/cs/0607033v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Craig's Interpolation Theorem formalised and mechanised in Isabelle/HOL",
        "authors": [
            "Tom Ridge"
        ],
        "summary": "We formalise and mechanise a construtive, proof theoretic proof of Craig's Interpolation Theorem in Isabelle/HOL. We give all the definitions and lemma statements both formally and informally. We also transcribe informally the formal proofs. We detail the main features of our mechanisation, such as the formalisation of binding for first order formulae. We also give some applications of Craig's Interpolation Theorem.",
        "published": "2006-07-12T17:26:29Z",
        "link": "http://arxiv.org/abs/cs/0607058v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Decomposable Theories",
        "authors": [
            "Khalil Djelloul"
        ],
        "summary": "We present in this paper a general algorithm for solving first-order formulas in particular theories called \"decomposable theories\". First of all, using special quantifiers, we give a formal characterization of decomposable theories and show some of their properties. Then, we present a general algorithm for solving first-order formulas in any decomposable theory \"T\". The algorithm is given in the form of five rewriting rules. It transforms a first-order formula \"P\", which can possibly contain free variables, into a conjunction \"Q\" of solved formulas easily transformable into a Boolean combination of existentially quantified conjunctions of atomic formulas. In particular, if \"P\" has no free variables then \"Q\" is either the formula \"true\" or \"false\". The correctness of our algorithm proves the completeness of the decomposable theories.   Finally, we show that the theory \"Tr\" of finite or infinite trees is a decomposable theory and give some benchmarks realized by an implementation of our algorithm, solving formulas on two-partner games in \"Tr\" with more than 160 nested alternated quantifiers.",
        "published": "2006-07-13T14:46:44Z",
        "link": "http://arxiv.org/abs/cs/0607065v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "The Complexity of Quantified Constraint Satisfaction: Collapsibility,   Sink Algebras, and the Three-Element Case",
        "authors": [
            "Hubie Chen"
        ],
        "summary": "The constraint satisfaction probem (CSP) is a well-acknowledged framework in which many combinatorial search problems can be naturally formulated. The CSP may be viewed as the problem of deciding the truth of a logical sentence consisting of a conjunction of constraints, in front of which all variables are existentially quantified. The quantified constraint satisfaction problem (QCSP) is the generalization of the CSP where universal quantification is permitted in addition to existential quantification. The general intractability of these problems has motivated research studying the complexity of these problems under a restricted constraint language, which is a set of relations that can be used to express constraints.   This paper introduces collapsibility, a technique for deriving positive complexity results on the QCSP. In particular, this technique allows one to show that, for a particular constraint language, the QCSP reduces to the CSP. We show that collapsibility applies to three known tractable cases of the QCSP that were originally studied using disparate proof techniques in different decades: Quantified 2-SAT (Aspvall, Plass, and Tarjan 1979), Quantified Horn-SAT (Karpinski, Kleine B\\\"{u}ning, and Schmitt 1987), and Quantified Affine-SAT (Creignou, Khanna, and Sudan 2001). This reconciles and reveals common structure among these cases, which are describable by constraint languages over a two-element domain. In addition to unifying these known tractable cases, we study constraint languages over domains of larger size.",
        "published": "2006-07-24T16:03:56Z",
        "link": "http://arxiv.org/abs/cs/0607106v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Revising Type-2 Computation and Degrees of Discontinuity",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "By the sometimes so-called MAIN THEOREM of Recursive Analysis, every computable real function is necessarily continuous. Weihrauch and Zheng (TCS'2000), Brattka (MLQ'2005), and Ziegler (ToCS'2006) have considered different relaxed notions of computability to cover also discontinuous functions. The present work compares and unifies these approaches. This is based on the concept of the JUMP of a representation: both a TTE-counterpart to the well known recursion-theoretic jump on Kleene's Arithmetical Hierarchy of hypercomputation: and a formalization of revising computation in the sense of Shoenfield.   We also consider Markov and Banach/Mazur oracle-computation of discontinuous fu nctions and characterize the computational power of Type-2 nondeterminism to coincide with the first level of the Analytical Hierarchy.",
        "published": "2006-07-26T06:07:22Z",
        "link": "http://arxiv.org/abs/cs/0607114v2",
        "categories": [
            "cs.LO",
            "math.LO",
            "F.1.1; F.1.2; F.4.1"
        ]
    },
    {
        "title": "Logic Column 16: Higher-Order Abstract Syntax: Setting the Record   Straight",
        "authors": [
            "Karl Crary",
            "Robert Harper"
        ],
        "summary": "This article responds to a critique of higher-order abstract syntax appearing in Logic Column 14, ``Nominal Logic and Abstract Syntax'', cs.LO/0511025.",
        "published": "2006-07-31T14:15:23Z",
        "link": "http://arxiv.org/abs/cs/0607141v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Modeling Adversaries in a Logic for Security Protocol Analysis",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "Logics for security protocol analysis require the formalization of an adversary model that specifies the capabilities of adversaries. A common model is the Dolev-Yao model, which considers only adversaries that can compose and replay messages, and decipher them with known keys. The Dolev-Yao model is a useful abstraction, but it suffers from some drawbacks: it cannot handle the adversary knowing protocol-specific information, and it cannot handle probabilistic notions, such as the adversary attempting to guess the keys. We show how we can analyze security protocols under different adversary models by using a logic with a notion of algorithmic knowledge. Roughly speaking, adversaries are assumed to use algorithms to compute their knowledge; adversary capabilities are captured by suitable restrictions on the algorithms used. We show how we can model the standard Dolev-Yao adversary in this setting, and how we can capture more general capabilities including protocol-specific knowledge and guesses.",
        "published": "2006-07-31T16:44:15Z",
        "link": "http://arxiv.org/abs/cs/0607146v6",
        "categories": [
            "cs.CR",
            "cs.LO",
            "cs.LO"
        ]
    },
    {
        "title": "A Finite Equational Base for CCS with Left Merge and Communication Merge",
        "authors": [
            "Luca Aceto",
            "Wan Fokkink",
            "Anna Ingolfsdottir",
            "Bas Luttik"
        ],
        "summary": "Using the left merge and communication merge from ACP, we present an equational base (i.e., a ground-complete and $\\omega$-complete set of valid equations) for the fragment of CCS without recursion, restriction and relabelling. Our equational base is finite if the set of actions is finite.",
        "published": "2006-08-01T10:45:29Z",
        "link": "http://arxiv.org/abs/cs/0608001v2",
        "categories": [
            "cs.LO",
            "D.3.1; F.1.1; F.1.2; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Infinite Qualitative Simulations by Means of Constraint Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Sebastian Brand"
        ],
        "summary": "We introduce a constraint-based framework for studying infinite qualitative simulations concerned with contingencies such as time, space, shape, size, abstracted into a finite set of qualitative relations. To define the simulations, we combine constraints that formalize the background knowledge concerned with qualitative reasoning with appropriate inter-state constraints that are formulated using linear temporal logic. We implemented this approach in a constraint programming system by drawing on ideas from bounded model checking. The resulting system allows us to test and modify the problem specifications in a straightforward way and to combine various knowledge aspects.",
        "published": "2006-08-03T03:08:34Z",
        "link": "http://arxiv.org/abs/cs/0608017v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Conflict Free Rule for Combining Evidences",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "Recent works have investigated the problem of the conflict redistribution in the fusion rules of evidence theories. As a consequence of these works, many new rules have been proposed. Now, there is not a clear theoretical criterion for a choice of a rule instead another. The present chapter proposes a new theoretically grounded rule, based on a new concept of sensor independence. This new rule avoids the conflict redistribution, by an adaptive combination of the beliefs. Both the logical grounds and the algorithmic implementation are considered.",
        "published": "2006-08-04T04:53:24Z",
        "link": "http://arxiv.org/abs/math/0608116v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "On Quasi-Interpretations, Blind Abstractions and Implicit Complexity",
        "authors": [
            "Patrick Baillot",
            "Ugo Dal Lago",
            "Jean-Yves Moyen"
        ],
        "summary": "Quasi-interpretations are a technique to guarantee complexity bounds on first-order functional programs: with termination orderings they give in particular a sufficient condition for a program to be executable in polynomial time, called here the P-criterion. We study properties of the programs satisfying the P-criterion, in order to better understand its intensional expressive power. Given a program on binary lists, its blind abstraction is the nondeterministic program obtained by replacing lists by their lengths (natural numbers). A program is blindly polynomial if its blind abstraction terminates in polynomial time. We show that all programs satisfying a variant of the P-criterion are in fact blindly polynomial. Then we give two extensions of the P-criterion: one by relaxing the termination ordering condition, and the other one (the bounded value property) giving a necessary and sufficient condition for a program to be polynomial time executable, with memoisation.",
        "published": "2006-08-06T07:20:45Z",
        "link": "http://arxiv.org/abs/cs/0608030v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Satisfying KBO Constraints",
        "authors": [
            "Harald Zankl",
            "Aart Middeldorp"
        ],
        "summary": "This paper presents two new approaches to prove termination of rewrite systems with the Knuth-Bendix order efficiently. The constraints for the weight function and for the precedence are encoded in (pseudo-)propositional logic and the resulting formula is tested for satisfiability. Any satisfying assignment represents a weight function and a precedence such that the induced Knuth-Bendix order orients the rules of the encoded rewrite system from left to right.",
        "published": "2006-08-06T12:42:31Z",
        "link": "http://arxiv.org/abs/cs/0608032v2",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "Resource Usage Analysis for the Pi-Calculus",
        "authors": [
            "Naoki Kobayashi",
            "Kohei Suenaga",
            "Lucian Wischik"
        ],
        "summary": "We propose a type-based resource usage analysis for the &#960;-calculus extended with resource creation/access primitives. The goal of the resource usage analysis is to statically check that a program accesses resources such as files and memory in a valid manner. Our type system is an extension of previous behavioral type systems for the &#960;-calculus, and can guarantee the safety property that no invalid access is performed, as well as the property that necessary accesses (such as the close operation for a file) are eventually performed unless the program diverges. A sound type inference algorithm for the type system is also developed to free the programmer from the burden of writing complex type annotations. Based on the algorithm, we have implemented a prototype resource usage analyzer for the &#960;-calculus. To the authors' knowledge, ours is the first type-based resource usage analysis that deals with an expressive concurrent language like the pi-calculus.",
        "published": "2006-08-07T05:22:42Z",
        "link": "http://arxiv.org/abs/cs/0608035v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.1; D.3.1"
        ]
    },
    {
        "title": "The weak pigeonhole principle for function classes in S^1_2",
        "authors": [
            "Norman Danner",
            "Chris Pollett"
        ],
        "summary": "It is well known that S^1_2 cannot prove the injective weak pigeonhole principle for polynomial time functions unless RSA is insecure. In this note we investigate the provability of the surjective (dual) weak pigeonhole principle in S^1_2 for provably weaker function classes.",
        "published": "2006-08-07T19:44:02Z",
        "link": "http://arxiv.org/abs/cs/0608039v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "An Embedding of the BSS Model of Computation in Light Affine   Lambda-Calculus",
        "authors": [
            "Patrick Baillot",
            "Marco Pedicini"
        ],
        "summary": "This paper brings together two lines of research: implicit characterization of complexity classes by Linear Logic (LL) on the one hand, and computation over an arbitrary ring in the Blum-Shub-Smale (BSS) model on the other. Given a fixed ring structure K we define an extension of Terui's light affine lambda-calculus typed in LAL (Light Affine Logic) with a basic type for K. We show that this calculus captures the polynomial time function class FP(K): every typed term can be evaluated in polynomial time and conversely every polynomial time BSS machine over K can be simulated in this calculus.",
        "published": "2006-08-08T07:13:58Z",
        "link": "http://arxiv.org/abs/cs/0608040v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Modules over Monads and Linearity",
        "authors": [
            "André Hirschowitz",
            "Marco Maggesi"
        ],
        "summary": "Inspired by the classical theory of modules over a monoid, we give a first account of the natural notion of module over a monad. The associated notion of morphism of left modules (\"Linear\" natural transformations) captures an important property of compatibility with substitution, in the heterogeneous case where \"terms\" and variables therein could be of different types as well as in the homogeneous case. In this paper, we present basic constructions of modules and we show examples concerning in particular abstract syntax and lambda-calculus.",
        "published": "2006-08-11T15:05:27Z",
        "link": "http://arxiv.org/abs/cs/0608051v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "A Distribution Law for CCS and a New Congruence Result for the   pi-calculus",
        "authors": [
            "Daniel Hirschkoff",
            "Damien Pous"
        ],
        "summary": "We give an axiomatisation of strong bisimilarity on a small fragment of CCS that does not feature the sum operator. This axiomatisation is then used to derive congruence of strong bisimilarity in the finite pi-calculus in absence of sum. To our knowledge, this is the only nontrivial subcalculus of the pi-calculus that includes the full output prefix and for which strong bisimilarity is a congruence.",
        "published": "2006-08-14T18:55:53Z",
        "link": "http://arxiv.org/abs/cs/0608059v5",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Tarski's influence on computer science",
        "authors": [
            "Solomon Feferman"
        ],
        "summary": "The influence of Alfred Tarski on computer science was indirect but significant in a number of directions and was in certain respects fundamental. Here surveyed is the work of Tarski on the decision procedure for algebra and geometry, the method of elimination of quantifiers, the semantics of formal languages, modeltheoretic preservation theorems, and algebraic logic; various connections of each with computer science are taken up.",
        "published": "2006-08-15T16:40:24Z",
        "link": "http://arxiv.org/abs/cs/0608062v2",
        "categories": [
            "cs.GL",
            "cs.LO",
            "F.4.1; F.4.3"
        ]
    },
    {
        "title": "On Universality in Real Computation",
        "authors": [
            "Hector Zenil"
        ],
        "summary": "Models of computation operating over the real numbers and computing a larger class of functions compared to the class of general recursive functions invariably introduce a non-finite element of infinite information encoded in an arbitrary non-computable number or non-recursive function. In this paper we show that Turing universality is only possible at every Turing degree but not over all, in that sense universality at the first level is elegantly well defined while universality at higher degrees is at least ambiguous. We propose a concept of universal relativity and universal jump between levels in the arithmetical and analytical hierarchy.",
        "published": "2006-08-24T21:21:21Z",
        "link": "http://arxiv.org/abs/cs/0608094v6",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "A study of fuzzy and many-valued logics in cellular automata",
        "authors": [
            "Angelo B. Mingarelli"
        ],
        "summary": "In this paper we provide an analytical study of the theory of multi-valued and fuzzy cellular automata where the fuzziness appears as the result of the application of an underlying multi-valued or continuous logic as opposed to standard logic as used conventionally. Using the disjunctive normal form of any one of the 255 ECA's so defined, we modify the underlying logic structure and redefine the ECA within the framework of this new logic. The idea here is to show that the evolution of space-time diagrams of ECA's under even a probabilistic logic can exhibit non-chaotic behavior. This is looked at specifically for Probabilistic Rule 110, in contrast with Boolean Rule 110 which is known to be capable of universal computation.",
        "published": "2006-08-25T09:21:49Z",
        "link": "http://arxiv.org/abs/cs/0608097v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "B.6.1; F.1.1; I.2.3"
        ]
    },
    {
        "title": "Automated verification of weak equivalence within the SMODELS system",
        "authors": [
            "Tomi Janhunen",
            "Emilia Oikarinen"
        ],
        "summary": "In answer set programming (ASP), a problem at hand is solved by (i) writing a logic program whose answer sets correspond to the solutions of the problem, and by (ii) computing the answer sets of the program using an answer set solver as a search engine. Typically, a programmer creates a series of gradually improving logic programs for a particular problem when optimizing program length and execution time on a particular solver. This leads the programmer to a meta-level problem of ensuring that the programs are equivalent, i.e., they give rise to the same answer sets. To ease answer set programming at methodological level, we propose a translation-based method for verifying the equivalence of logic programs. The basic idea is to translate logic programs P and Q under consideration into a single logic program EQT(P,Q) whose answer sets (if such exist) yield counter-examples to the equivalence of P and Q. The method is developed here in a slightly more general setting by taking the visibility of atoms properly into account when comparing answer sets. The translation-based approach presented in the paper has been implemented as a translator called lpeq that enables the verification of weak equivalence within the smodels system using the same search engine as for the search of models. Our experiments with lpeq and smodels suggest that establishing the equivalence of logic programs in this way is in certain cases much faster than naive cross-checking of answer sets.",
        "published": "2006-08-25T13:12:40Z",
        "link": "http://arxiv.org/abs/cs/0608099v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1; F.2.2"
        ]
    },
    {
        "title": "Logic programs with monotone abstract constraint atoms",
        "authors": [
            "V. W. Marek",
            "I. Niemela",
            "M. Truszczynski]"
        ],
        "summary": "We introduce and study logic programs whose clauses are built out of monotone constraint atoms. We show that the operational concept of the one-step provability operator generalizes to programs with monotone constraint atoms, but the generalization involves nondeterminism. Our main results demonstrate that our formalism is a common generalization of (1) normal logic programming with its semantics of models, supported models and stable models, (2) logic programming with weight atoms (lparse programs) with the semantics of stable models, as defined by Niemela, Simons and Soininen, and (3) of disjunctive logic programming with the possible-model semantics of Sakama and Inoue.",
        "published": "2006-08-25T18:24:18Z",
        "link": "http://arxiv.org/abs/cs/0608103v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; D.1.6"
        ]
    },
    {
        "title": "Calculating modules in contextual logic program refinement",
        "authors": [
            "Robert Colvin",
            "Ian J. Hayes",
            "Paul Strooper"
        ],
        "summary": "The refinement calculus for logic programs is a framework for deriving logic programs from specifications. It is based on a wide-spectrum language that can express both specifications and code, and a refinement relation that models the notion of correct implementation. In this paper we extend and generalise earlier work on contextual refinement. Contextual refinement simplifies the refinement process by abstractly capturing the context of a subcomponent of a program, which typically includes information about the values of the free variables. This paper also extends and generalises module refinement. A module is a collection of procedures that operate on a common data type; module refinement between a specification module A and an implementation module C allows calls to the procedures of A to be systematically replaced with calls to the corresponding procedures of C. Based on the conditions for module refinement, we present a method for calculating an implementation module from a specification module. Both contextual and module refinement within the refinement calculus have been generalised from earlier work and the results are presented in a unified framework.",
        "published": "2006-08-29T05:31:11Z",
        "link": "http://arxiv.org/abs/cs/0608110v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Decidability of Type-checking in the Calculus of Algebraic Constructions   with Size Annotations",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "Since Val Tannen's pioneer work on the combination of simply-typed lambda-calculus and first-order rewriting (LICS'88), many authors have contributed to this subject by extending it to richer typed lambda-calculi and rewriting paradigms, culminating in calculi like the Calculus of Algebraic Constructions. These works provide theoretical foundations for type-theoretic proof assistants where functions and predicates are defined by oriented higher-order equations. This kind of definitions subsumes induction-based definitions, is easier to write and provides more automation. On the other hand, checking that user-defined rewrite rules are strongly normalizing and confluent, and preserve the decidability of type-checking when combined with beta-reduction, is more difficult. Most termination criteria rely on the term structure. In a previous work, we extended to dependent types and higher-order rewriting, the notion of ``sized types'' studied by several authors in the simpler framework of ML-like languages, and proved that it preserves strong normalization. The main contribution of the present paper is twofold. First, we prove that, in the Calculus of Algebraic Constructions with size annotations, the problems of type inference and type-checking are decidable, provided that the sets of constraints generated by size annotations are satisfiable and admit most general solutions. Second, we prove the later properties for a size algebra rich enough for capturing usual induction-based definitions and much more.",
        "published": "2006-08-31T14:26:20Z",
        "link": "http://arxiv.org/abs/cs/0608125v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "On the confluence of lambda-calculus with conditional rewriting",
        "authors": [
            "Frédéric Blanqui",
            "Claude Kirchner",
            "Colin Riba"
        ],
        "summary": "The confluence of untyped lambda-calculus with unconditional rewriting has already been studied in various directions. In this paper, we investigate the confluence of lambda-calculus with conditional rewriting and provide general results in two directions. First, when conditional rules are algebraic. This extends results of Muller and Dougherty for unconditional rewriting. Two cases are considered, whether beta-reduction is allowed or not in the evaluation of conditions. Moreover, Dougherty's result is improved from the assumption of strongly normalizing beta-reduction to weakly normalizing beta-reduction. We also provide examples showing that outside these conditions, modularity of confluence is difficult to achieve. Second, we go beyond the algebraic framework and get new confluence results using an extended notion of orthogonality that takes advantage of the conditional part of rewrite rules.",
        "published": "2006-09-01T12:56:02Z",
        "link": "http://arxiv.org/abs/cs/0609002v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "On the freeze quantifier in Constraint LTL: decidability and complexity",
        "authors": [
            "Stéphane Demri",
            "Ranko Lazic",
            "David Nowak"
        ],
        "summary": "Constraint LTL, a generalisation of LTL over Presburger constraints, is often used as a formal language to specify the behavior of operational models with constraints. The freeze quantifier can be part of the language, as in some real-time logics, but this variable-binding mechanism is quite general and ubiquitous in many logical languages (first-order temporal logics, hybrid logics, logics for sequence diagrams, navigation logics, logics with lambda-abstraction etc.). We show that Constraint LTL over the simple domain (N,=) augmented with the freeze quantifier is undecidable which is a surprising result in view of the poor language for constraints (only equality tests). Many versions of freeze-free Constraint LTL are decidable over domains with qualitative predicates and our undecidability result actually establishes Sigma_1^1-completeness. On the positive side, we provide complexity results when the domain is finite (EXPSPACE-completeness) or when the formulae are flat in a sense introduced in the paper. Our undecidability results are sharp (i.e. with restrictions on the number of variables) and all our complexity characterisations ensure completeness with respect to some complexity class (mainly PSPACE and EXPSPACE).",
        "published": "2006-09-04T06:20:03Z",
        "link": "http://arxiv.org/abs/cs/0609008v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Combining typing and size constraints for checking the termination of   higher-order conditional rewrite systems",
        "authors": [
            "Frédéric Blanqui",
            "Colin Riba"
        ],
        "summary": "In a previous work, the first author extended to higher-order rewriting and dependent types the use of size annotations in types, a termination proof technique called type or size based termination and initially developed for ML-like programs. Here, we go one step further by considering conditional rewriting and explicit quantifications and constraints on size annotations. This allows to describe more precisely how the size of the output of a function depends on the size of its inputs. Hence, we can check the termination of more functions. We first give a general type-checking algorithm based on constraint solving. Then, we give a termination criterion with constraints in Presburger arithmetic. To our knowledge, this is the first termination criterion for higher-order conditional rewriting taking into account the conditions in termination.",
        "published": "2006-09-05T14:23:38Z",
        "link": "http://arxiv.org/abs/cs/0609013v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Residual Finite Tree Automata",
        "authors": [
            "J. Carme",
            "R. Gilleron",
            "A. Lemay",
            "A. Terlutte",
            "M. Tommasi"
        ],
        "summary": "Tree automata based algorithms are essential in many fields in computer science such as verification, specification, program analysis. They become also essential for databases with the development of standards such as XML. In this paper, we define new classes of non deterministic tree automata, namely residual finite tree automata (RFTA). In the bottom-up case, we obtain a new characterization of regular tree languages. In the top-down case, we obtain a subclass of regular tree languages which contains the class of languages recognized by deterministic top-down tree automata. RFTA also come with the property of existence of canonical non deterministic tree automata.",
        "published": "2006-09-05T16:23:08Z",
        "link": "http://arxiv.org/abs/cs/0609015v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Non uniform (hyper/multi)coherence spaces",
        "authors": [
            "Pierre Boudes"
        ],
        "summary": "In (hyper)coherence semantics, proofs/terms are cliques in (hyper)graphs. Intuitively, vertices represent results of computations and the edge relation witnesses the ability of being assembled into a same piece of data or a same (strongly) stable function, at arrow types. In (hyper)coherence semantics, the argument of a (strongly) stable functional is always a (strongly) stable function. As a consequence, comparatively to the relational semantics, where there is no edge relation, some vertices are missing. Recovering these vertices is essential for the purpose of reconstructing proofs/terms from their interpretations. It shall also be useful for the comparison with other semantics, like game semantics. In [BE01], Bucciarelli and Ehrhard introduced a so called non uniform coherence space semantics where no vertex is missing. By constructing the co-free exponential we set a new version of this last semantics, together with non uniform versions of hypercoherences and multicoherences, a new semantics where an edge is a finite multiset. Thanks to the co-free construction, these non uniform semantics are deterministic in the sense that the intersection of a clique and of an anti-clique contains at most one vertex, a result of interaction, and extensionally collapse onto the corresponding uniform semantics.",
        "published": "2006-09-06T12:14:25Z",
        "link": "http://arxiv.org/abs/cs/0609021v1",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Dichotomies and Duality in First-order Model Checking Problems",
        "authors": [
            "Barnaby Martin"
        ],
        "summary": "We study the complexity of the model checking problem, for fixed model A, over certain fragments L of first-order logic. These are sometimes known as the expression complexities of L. We obtain various complexity classification theorems for these logics L as each ranges over models A, in the spirit of the dichotomy conjecture for the Constraint Satisfaction Problem -- which itself may be seen as the model checking problem for existential conjunctive positive first-order logic.",
        "published": "2006-09-06T13:04:36Z",
        "link": "http://arxiv.org/abs/cs/0609022v2",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "(HO)RPO Revisited",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "The notion of computability closure has been introduced for proving the termination of the combination of higher-order rewriting and beta-reduction. It is also used for strengthening the higher-order recursive path ordering. In the present paper, we study in more details the relations between the computability closure and the (higher-order) recursive path ordering. We show that the first-order recursive path ordering is equal to an ordering naturally defined from the computability closure. In the higher-order case, we get an ordering containing the higher-order recursive path ordering whose well-foundedness relies on the correctness of the computability closure. This provides a simple way to extend the higher-order recursive path ordering to richer type systems.",
        "published": "2006-09-08T07:35:35Z",
        "link": "http://arxiv.org/abs/cs/0609037v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Higher-Order Termination: from Kruskal to Computability",
        "authors": [
            "Frédéric Blanqui",
            "Jean-Pierre Jouannaud",
            "Albert Rubio"
        ],
        "summary": "Termination is a major question in both logic and computer science. In logic, termination is at the heart of proof theory where it is usually called strong normalization (of cut elimination). In computer science, termination has always been an important issue for showing programs correct. In the early days of logic, strong normalization was usually shown by assigning ordinals to expressions in such a way that eliminating a cut would yield an expression with a smaller ordinal. In the early days of verification, computer scientists used similar ideas, interpreting the arguments of a program call by a natural number, such as their size. Showing the size of the arguments to decrease for each recursive call gives a termination proof of the program, which is however rather weak since it can only yield quite small ordinals. In the sixties, Tait invented a new method for showing cut elimination of natural deduction, based on a predicate over the set of terms, such that the membership of an expression to the predicate implied the strong normalization property for that expression. The predicate being defined by induction on types, or even as a fixpoint, this method could yield much larger ordinals. Later generalized by Girard under the name of reducibility or computability candidates, it showed very effective in proving the strong normalization property of typed lambda-calculi...",
        "published": "2006-09-08T07:48:08Z",
        "link": "http://arxiv.org/abs/cs/0609039v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Elgot Algebras",
        "authors": [
            "Jiri Adamek",
            "Stefan Milius",
            "Jiri Velebil"
        ],
        "summary": "Denotational semantics can be based on algebras with additional structure (order, metric, etc.) which makes it possible to interpret recursive specifications. It was the idea of Elgot to base denotational semantics on iterative theories instead, i.e., theories in which abstract recursive specifications are required to have unique solutions. Later Bloom and Esik studied iteration theories and iteration algebras in which a specified solution has to obey certain axioms. We propose so-called Elgot algebras as a convenient structure for semantics in the present paper. An Elgot algebra is an algebra with a specified solution for every system of flat recursive equations. That specification satisfies two simple and well motivated axioms: functoriality (stating that solutions are stable under renaming of recursion variables) and compositionality (stating how to perform simultaneous recursion). These two axioms stem canonically from Elgot's iterative theories: We prove that the category of Elgot algebras is the Eilenberg-Moore category of the monad given by a free iterative theory.",
        "published": "2006-09-08T08:51:33Z",
        "link": "http://arxiv.org/abs/cs/0609040v2",
        "categories": [
            "cs.LO",
            "math.CT",
            "F.3.2"
        ]
    },
    {
        "title": "On the logical definability of certain graph and poset languages",
        "authors": [
            "Pascal Weil"
        ],
        "summary": "We show that it is equivalent, for certain sets of finite graphs, to be definable in CMS (counting monadic second-order logic, a natural extension of monadic second-order logic), and to be recognizable in an algebraic framework induced by the notion of modular decomposition of a finite graph. More precisely, we consider the set $F\\_\\infty$ of composition operations on graphs which occur in the modular decomposition of finite graphs. If $F$ is a subset of $F\\_{\\infty}$, we say that a graph is an $\\calF$-graph if it can be decomposed using only operations in $F$. A set of $F$-graphs is recognizable if it is a union of classes in a finite-index equivalence relation which is preserved by the operations in $F$. We show that if $F$ is finite and its elements enjoy only a limited amount of commutativity -- a property which we call weak rigidity, then recognizability is equivalent to CMS-definability. This requirement is weak enough to be satisfied whenever all $F$-graphs are posets, that is, transitive dags. In particular, our result generalizes Kuske's recent result on series-parallel poset languages.",
        "published": "2006-09-11T08:25:10Z",
        "link": "http://arxiv.org/abs/cs/0609048v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Undecidability of the unification and admissibility problems for modal   and description logics",
        "authors": [
            "Frank Wolter",
            "Michael Zakharyaschev"
        ],
        "summary": "We show that the unification problem `is there a substitution instance of a given formula that is provable in a given logic?' is undecidable for basic modal logics K and K4 extended with the universal modality. It follows that the admissibility problem for inference rules is undecidable for these logics as well. These are the first examples of standard decidable modal logics for which the unification and admissibility problems are undecidable. We also prove undecidability of the unification and admissibility problems for K and K4 with at least two modal operators and nominals (instead of the universal modality), thereby showing that these problems are undecidable for basic hybrid logics. Recently, unification has been introduced as an important reasoning service for description logics. The undecidability proof for K with nominals can be used to show the undecidability of unification for boolean description logics with nominals (such as ALCO and SHIQO). The undecidability proof for K with the universal modality can be used to show that the unification problem relative to role boxes is undecidable for Boolean description logic with transitive roles, inverse roles, and role hierarchies (such as SHI and SHIQ).",
        "published": "2006-09-11T11:41:35Z",
        "link": "http://arxiv.org/abs/cs/0609052v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Nominal Logic Programming",
        "authors": [
            "James Cheney",
            "Christian Urban"
        ],
        "summary": "Nominal logic is an extension of first-order logic which provides a simple foundation for formalizing and reasoning about abstract syntax modulo consistent renaming of bound names (that is, alpha-equivalence). This article investigates logic programming based on nominal logic. We describe some typical nominal logic programs, and develop the model-theoretic, proof-theoretic, and operational semantics of such programs. Besides being of interest for ensuring the correct behavior of implementations, these results provide a rigorous foundation for techniques for analysis and reasoning about nominal logic programs, as we illustrate via examples.",
        "published": "2006-09-12T09:32:23Z",
        "link": "http://arxiv.org/abs/cs/0609062v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Solution of a Problem of Barendregt on Sensible lambda-Theories",
        "authors": [
            "Benedetto Intrigila",
            "Richard Statman"
        ],
        "summary": "<i>H</i> is the theory extending &#946;-conversion by identifying all closed unsolvables. <i>H</i>&#969; is the closure of this theory under the &#969;-rule (and &#946;-conversion). A long-standing conjecture of H. Barendregt states that the provable equations of <i>H</i>&#969; form &#928;<sub>1</sub><sup>1</sup>-complete set. Here we prove that conjecture.",
        "published": "2006-09-14T16:01:52Z",
        "link": "http://arxiv.org/abs/cs/0609080v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Free Choice Petri Nets without frozen tokens and Bipolar Synchronization   Systems",
        "authors": [
            "Joachim Wehler"
        ],
        "summary": "Bipolar synchronization systems (BP-systems) constitute a class of coloured Petri nets, well suited for modeling the control flow of discrete, dynamical systems. Every BP-system has an underlying ordinary Petri net, which is a T-system. Moreover, it has a second ordinary net attached, which is a free-choice system. We prove that a BP-system is live and safe if the T-system and the free-choice system are live and safe and if the free-choice system has no frozen tokens. This result is the converse of a theorem of Genrich and Thiagarajan and proves an elder conjecture. The proof compares the different Petri nets by Petri net morphisms and makes use of the classical theory of free-choice systems",
        "published": "2006-09-17T17:42:41Z",
        "link": "http://arxiv.org/abs/cs/0609095v2",
        "categories": [
            "cs.LO",
            "D.2.2"
        ]
    },
    {
        "title": "Using groups for investigating rewrite systems",
        "authors": [
            "Patrick Dehornoy"
        ],
        "summary": "We describe several technical tools that prove to be efficient for investigating the rewrite systems associated with a family of algebraic laws, and might be useful for more general rewrite systems. These tools consist in introducing a monoid of partial operators, listing the monoid relations expressing the possible local confluence of the rewrite system, then introducing the group presented by these relations, and finally replacing the initial rewrite system with a internal process entirely sitting in the latter group. When the approach can be completed, one typically obtains a practical method for constructing algebras satisfying prescribed laws and for solving the associated word problem.",
        "published": "2006-09-18T12:34:47Z",
        "link": "http://arxiv.org/abs/cs/0609102v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On Verifying Complex Properties using Symbolic Shape Analysis",
        "authors": [
            "Thomas Wies",
            "Viktor Kuncak",
            "Karen Zee",
            "Andreas Podelski",
            "Martin Rinard"
        ],
        "summary": "One of the main challenges in the verification of software systems is the analysis of unbounded data structures with dynamic memory allocation, such as linked data structures and arrays. We describe Bohne, a new analysis for verifying data structures. Bohne verifies data structure operations and shows that 1) the operations preserve data structure invariants and 2) the operations satisfy their specifications expressed in terms of changes to the set of objects stored in the data structure. During the analysis, Bohne infers loop invariants in the form of disjunctions of universally quantified Boolean combinations of formulas. To synthesize loop invariants of this form, Bohne uses a combination of decision procedures for Monadic Second-Order Logic over trees, SMT-LIB decision procedures (currently CVC Lite), and an automated reasoner within the Isabelle interactive theorem prover. This architecture shows that synthesized loop invariants can serve as a useful communication mechanism between different decision procedures. Using Bohne, we have verified operations on data structures such as linked lists with iterators and back pointers, trees with and without parent pointers, two-level skip lists, array data structures, and sorted lists. We have deployed Bohne in the Hob and Jahob data structure analysis systems, enabling us to combine Bohne with analyses of data structure clients and apply it in the context of larger programs. This report describes the Bohne algorithm as well as techniques that Bohne uses to reduce the ammount of annotations and the running time of the analysis.",
        "published": "2006-09-18T14:52:16Z",
        "link": "http://arxiv.org/abs/cs/0609104v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "Generalized Majority-Minority Operations are Tractable",
        "authors": [
            "Victor Dalmau"
        ],
        "summary": "Generalized majority-minority (GMM) operations are introduced as a common generalization of near unanimity operations and Mal'tsev operations on finite sets. We show that every instance of the constraint satisfaction problem (CSP), where all constraint relations are invariant under a (fixed) GMM operation, is solvable in polynomial time. This constitutes one of the largest tractable cases of the CSP.",
        "published": "2006-09-19T15:12:46Z",
        "link": "http://arxiv.org/abs/cs/0609108v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "The recognizability of sets of graphs is a robust property",
        "authors": [
            "Bruno Courcelle",
            "Pascal Weil"
        ],
        "summary": "Once the set of finite graphs is equipped with an algebra structure (arising from the definition of operations that generalize the concatenation of words), one can define the notion of a recognizable set of graphs in terms of finite congruences. Applications to the construction of efficient algorithms and to the theory of context-free sets of graphs follow naturally. The class of recognizable sets depends on the signature of graph operations. We consider three signatures related respectively to Hyperedge Replacement (HR) context-free graph grammars, to Vertex Replacement (VR) context-free graph grammars, and to modular decompositions of graphs. We compare the corresponding classes of recognizable sets. We show that they are robust in the sense that many variants of each signature (where in particular operations are defined by quantifier-free formulas, a quite flexible framework) yield the same notions of recognizability. We prove that for graphs without large complete bipartite subgraphs, HR-recognizability and VR-recognizability coincide. The same combinatorial condition equates HR-context-free and VR-context-free sets of graphs. Inasmuch as possible, results are formulated in the more general framework of relational structures.",
        "published": "2006-09-19T15:18:07Z",
        "link": "http://arxiv.org/abs/cs/0609109v1",
        "categories": [
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Algebraic recognizability of languages",
        "authors": [
            "Pascal Weil"
        ],
        "summary": "Recognizable languages of finite words are part of every computer science cursus, and they are routinely described as a cornerstone for applications and for theory. We would like to briefly explore why that is, and how this word-related notion extends to more complex models, such as those developed for modeling distributed or timed behaviors.",
        "published": "2006-09-19T15:21:08Z",
        "link": "http://arxiv.org/abs/cs/0609110v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Rule-based Knowledge Representation for Service Level Agreement",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Automated management and monitoring of service contracts like Service Level Agreements (SLAs) or higher-level policies is vital for efficient and reliable distributed service-oriented architectures (SOA) with high quality of ser-vice (QoS) levels. IT service provider need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technol-ogy. I propose a novel rule-based knowledge representation (KR) for SLA rules and a respective rule-based service level management (RBSLM) framework. My rule-based approach based on logic programming provides several advantages including automated rule chaining allowing for compact knowledge representation and high levels of automation as well as flexibility to adapt to rapidly changing business requirements. Therewith, I address an urgent need service-oriented busi-nesses do have nowadays which is to dynamically change their business and contractual logic in order to adapt to rapidly changing business environments and to overcome the restricting nature of slow change cycles.",
        "published": "2006-09-21T12:04:33Z",
        "link": "http://arxiv.org/abs/cs/0609120v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.LO",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "The Three Gap Theorem (Steinhauss Conjecture)",
        "authors": [
            "Micaela Mayero"
        ],
        "summary": "We deal with the distribution of N points placed consecutively around the circle by a fixed angle of a. From the proof of Tony van Ravenstein, we propose a detailed proof of the Steinhaus conjecture whose result is the following: the N points partition the circle into gaps of at most three different lengths. We study the mathematical notions required for the proof of this theorem revealed during a formal proof carried out in Coq.",
        "published": "2006-09-22T11:30:34Z",
        "link": "http://arxiv.org/abs/cs/0609124v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Predicative Harmonization of the Time and Provable Hierarchies",
        "authors": [
            "Salvatore Caporaso"
        ],
        "summary": "A decidable transfinite hierarchy is defined by assigning ordinals to the programs of an imperative language. It singles out: the classes TIMEF(n^c) and TIMEF(n_c); the finite Grzegorczyk classes at and above the elementary level, and the \\Sigma_k-IND fragments of PA. Limited operators, diagonalization, and majorization functions are not used.",
        "published": "2006-09-23T14:28:54Z",
        "link": "http://arxiv.org/abs/cs/0609130v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "ECA-LP / ECA-RuleML: A Homogeneous Event-Condition-Action Logic   Programming Language",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Event-driven reactive functionalities are an urgent need in nowadays distributed service-oriented applications and (Semantic) Web-based environments. An important problem to be addressed is how to correctly and efficiently capture and process the event-based behavioral, reactive logic represented as ECA rules in combination with other conditional decision logic which is represented as derivation rules. In this paper we elaborate on a homogeneous integration approach which combines derivation rules, reaction rules (ECA rules) and other rule types such as integrity constraint into the general framework of logic programming. The developed ECA-LP language provides expressive features such as ID-based updates with support for external and self-updates of the intensional and extensional knowledge, transac-tions including integrity testing and an event algebra to define and process complex events and actions based on a novel interval-based Event Calculus variant.",
        "published": "2006-09-26T14:36:47Z",
        "link": "http://arxiv.org/abs/cs/0609143v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SE",
            "I.2"
        ]
    },
    {
        "title": "Updates in Answer Set Programming: An Approach Based on Basic Structural   Properties",
        "authors": [
            "Mauricio Osorio",
            "Víctor Cuevas"
        ],
        "summary": "We have studied the update operator defined for update sequences by Eiter et al. without tautologies and we have observed that it satisfies an interesting property This property, which we call Weak Independence of Syntax (WIS), is similar to one of the postulates proposed by Alchourron, Gardenfors, and Makinson (AGM); only that in this case it applies to nonmonotonic logic. In addition, we consider other five additional basic properties about update programs and we show that the operator of Eiter et al. satisfies them. This work continues the analysis of the AGM postulates under a refined view that considers nelson logic as a monotonic logic which allows us to expand our understanding of answer sets. Moreover, nelson logic helped us to derive an alternative definition of the operator defined by Eiter et al. avoiding the use of unnecessary extra atoms.",
        "published": "2006-09-29T17:00:52Z",
        "link": "http://arxiv.org/abs/cs/0609167v1",
        "categories": [
            "cs.LO",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "LTL with the Freeze Quantifier and Register Automata",
        "authors": [
            "Stephane Demri",
            "Ranko Lazic"
        ],
        "summary": "A data word is a sequence of pairs of a letter from a finite alphabet and an element from an infinite set, where the latter can only be compared for equality. To reason about data words, linear temporal logic is extended by the freeze quantifier, which stores the element at the current word position into a register, for equality comparisons deeper in the formula. By translations from the logic to alternating automata with registers and then to faulty counter automata whose counters may erroneously increase at any time, and from faulty and error-free counter automata to the logic, we obtain a complete complexity table for logical fragments defined by varying the set of temporal operators and the number of registers. In particular, the logic with future-time operators and 1 register is decidable but not primitive recursive over finite data words. Adding past-time operators or 1 more register, or switching to infinite data words, cause undecidability.",
        "published": "2006-10-05T15:47:22Z",
        "link": "http://arxiv.org/abs/cs/0610027v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Postinal Determinacy of Games with Infinitely Many Priorities",
        "authors": [
            "Erich Graedel",
            "Igor Walukiewicz"
        ],
        "summary": "We study two-player games of infinite duration that are played on finite or infinite game graphs. A winning strategy for such a game is positional if it only depends on the current position, and not on the history of the play. A game is positionally determined if, from each position, one of the two players has a positional winning strategy.   The theory of such games is well studied for winning conditions that are defined in terms of a mapping that assigns to each position a priority from a finite set. Specifically, in Muller games the winner of a play is determined by the set of those priorities that have been seen infinitely often; an important special case are parity games where the least (or greatest) priority occurring infinitely often determines the winner. It is well-known that parity games are positionally determined whereas Muller games are determined via finite-memory strategies.   In this paper, we extend this theory to the case of games with infinitely many priorities. Such games arise in several application areas, for instance in pushdown games with winning conditions depending on stack contents.   For parity games there are several generalisations to the case of infinitely many priorities. While max-parity games over omega or min-parity games over larger ordinals than omega require strategies with infinite memory, we can prove that min-parity games with priorities in omega are positionally determined. Indeed, it turns out that the min-parity condition over omega is the only infinitary Muller condition that guarantees positional determinacy on all game graphs.",
        "published": "2006-10-06T11:05:55Z",
        "link": "http://arxiv.org/abs/cs/0610034v2",
        "categories": [
            "cs.LO",
            "cs.GT"
        ]
    },
    {
        "title": "Positional Determinacy of Games with Infinitely Many Priorities",
        "authors": [
            "Erich Graedel",
            "Igor Walukiewicz"
        ],
        "summary": "We study two-player games of infinite duration that are played on finite or infinite game graphs. A winning strategy for such a game is positional if it only depends on the current position, and not on the history of the play. A game is positionally determined if, from each position, one of the two players has a positional winning strategy.   The theory of such games is well studied for winning conditions that are defined in terms of a mapping that assigns to each position a priority from a finite set. Specifically, in Muller games the winner of a play is determined by the set of those priorities that have been seen infinitely often; an important special case are parity games where the least (or greatest) priority occurring infinitely often determines the winner. It is well-known that parity games are positionally determined whereas Muller games are determined via finite-memory strategies.   In this paper, we extend this theory to the case of games with infinitely many priorities. Such games arise in several application areas, for instance in pushdown games with winning conditions depending on stack contents.   For parity games there are several generalisations to the case of infinitely many priorities. While max-parity games over omega or min-parity games over larger ordinals than omega require strategies with infinite memory, we can prove that min-parity games with priorities in omega are positionally determined. Indeed, it turns out that the min-parity condition over omega is the only infinitary Muller condition that guarantees positional determinacy on all game graphs.",
        "published": "2006-10-06T11:07:37Z",
        "link": "http://arxiv.org/abs/cs/0610035v2",
        "categories": [
            "cs.LO",
            "cs.GT",
            "F.4.1; G.2"
        ]
    },
    {
        "title": "Church's thesis is questioned by new calculation paradigm",
        "authors": [
            "Hannes Hutzelmeyer"
        ],
        "summary": "Church's thesis claims that all effecticely calculable functions are recursive. A shortcoming of the various definitions of recursive functions lies in the fact that it is not a matter of a syntactical check to find out if an entity gives rise to a function. Eight new ideas for a precise setup of arithmetical logic and its metalanguage give the proper environment for the construction of a special computer, the ARBACUS computer. Computers do not come to a necessary halt; it is requested that calculators are constructed on the basis of computers in a way that they always come to a halt, then all calculations are effective. The ARBATOR is defined as a calculator with two-layer-computation. It allows for the calculation of all primitive recursive functions, but multi-level-arbation also allows for the calculation of other arbative functions that are not primitive recursive. The new paradigm of calculation does not have the above mentioned shortcoming. The defenders of Church's thesis are challenged to show that exotic arbative functions are recursive and to put forward a recursive function that is not arbative. A construction with three-tier-multi-level-arbation that includes a diagonalisation leads to the extravagant yet calculable Snark-function that is not arbative. As long as it is not shown that all exotic arbative functions and particularily the Snark-function are arithmetically representable Goedel's first incompleteness sentence is in limbo.",
        "published": "2006-10-07T12:53:04Z",
        "link": "http://arxiv.org/abs/cs/0610038v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Enumeration Problems Related to Ground Horn Theories",
        "authors": [
            "Nachum Dershowitz",
            "Mitchell A. Harris",
            "Guan-Shieng Huang"
        ],
        "summary": "We investigate the enumeration of varieties of boolean theories related to Horn clauses. We describe a number of combinatorial equivalences among different characterizations and calculate the number of different theories in $n$ variables for slightly different characterizations. The method of counting is via counting models using a satisfiability checker.",
        "published": "2006-10-10T19:28:19Z",
        "link": "http://arxiv.org/abs/cs/0610054v2",
        "categories": [
            "cs.LO",
            "cs.DM",
            "F.4.1"
        ]
    },
    {
        "title": "A type-based termination criterion for dependently-typed higher-order   rewrite systems",
        "authors": [
            "Frederic Blanqui"
        ],
        "summary": "Several authors devised type-based termination criteria for ML-like languages allowing non-structural recursive calls. We extend these works to general rewriting and dependent types, hence providing a powerful termination criterion for the combination of rewriting and beta-reduction in the Calculus of Constructions.",
        "published": "2006-10-11T12:47:46Z",
        "link": "http://arxiv.org/abs/cs/0610062v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Extending the Calculus of Constructions with Tarski's fix-point theorem",
        "authors": [
            "Yves Bertot"
        ],
        "summary": "We propose to use Tarski's least fixpoint theorem as a basis to define recursive functions in the calculus of inductive constructions. This widens the class of functions that can be modeled in type-theory based theorem proving tool to potentially non-terminating functions. This is only possible if we extend the logical framework by adding the axioms that correspond to classical logic. We claim that the extended framework makes it possible to reason about terminating and non-terminating computations and we show that common facilities of the calculus of inductive construction, like program extraction can be extended to also handle the new functions.",
        "published": "2006-10-11T12:50:32Z",
        "link": "http://arxiv.org/abs/cs/0610055v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "The Calculus of Algebraic Constructions",
        "authors": [
            "Frédéric Blanqui",
            "Jean-Pierre Jouannaud",
            "Mitsuhiro Okada"
        ],
        "summary": "This paper is concerned with the foundations of the Calculus of Algebraic Constructions (CAC), an extension of the Calculus of Constructions by inductive data types. CAC generalizes inductive types equipped with higher-order primitive recursion, by providing definitions of functions by pattern-matching which capture recursor definitions for arbitrary non-dependent and non-polymorphic inductive types satisfying a strictly positivity condition. CAC also generalizes the first-order framework of abstract data types by providing dependent types and higher-order rewrite rules.",
        "published": "2006-10-11T13:45:46Z",
        "link": "http://arxiv.org/abs/cs/0610063v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Termination and Confluence of Higher-Order Rewrite Systems",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "In the last twenty years, several approaches to higher-order rewriting have been proposed, among which Klop's Combinatory Rewrite Systems (CRSs), Nipkow's Higher-order Rewrite Systems (HRSs) and Jouannaud and Okada's higher-order algebraic specification languages, of which only the last one considers typed terms. The later approach has been extended by Jouannaud, Okada and the present author into Inductive Data Type Systems (IDTSs). In this paper, we extend IDTSs with the CRS higher-order pattern-matching mechanism, resulting in simply-typed CRSs. Then, we show how the termination criterion developed for IDTSs with first-order pattern-matching, called the General Schema, can be extended so as to prove the strong normalization of IDTSs with higher-order pattern-matching. Next, we compare the unified approach with HRSs. We first prove that the extended General Schema can also be applied to HRSs. Second, we show how Nipkow's higher-order critical pair analysis technique for proving local confluence can be applied to IDTSs.",
        "published": "2006-10-11T13:47:47Z",
        "link": "http://arxiv.org/abs/cs/0610064v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Definitions by Rewriting in the Calculus of Constructions",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "The main novelty of this paper is to consider an extension of the Calculus of Constructions where predicates can be defined with a general form of rewrite rules. We prove the strong normalization of the reduction relation generated by the beta-rule and the user-defined rules under some general syntactic conditions including confluence. As examples, we show that two important systems satisfy these conditions: a sub-system of the Calculus of Inductive Constructions which is the basis of the proof assistant Coq, and the Natural Deduction Modulo a large class of equational theories.",
        "published": "2006-10-11T13:48:36Z",
        "link": "http://arxiv.org/abs/cs/0610065v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Inductive-data-type Systems",
        "authors": [
            "Frédéric Blanqui",
            "Jean-Pierre Jouannaud",
            "Mitsuhiro Okada"
        ],
        "summary": "In a previous work (\"Abstract Data Type Systems\", TCS 173(2), 1997), the last two authors presented a combined language made of a (strongly normalizing) algebraic rewrite system and a typed lambda-calculus enriched by pattern-matching definitions following a certain format, called the \"General Schema\", which generalizes the usual recursor definitions for natural numbers and similar \"basic inductive types\". This combined language was shown to be strongly normalizing. The purpose of this paper is to reformulate and extend the General Schema in order to make it easily extensible, to capture a more general class of inductive types, called \"strictly positive\", and to ease the strong normalization proof of the resulting system. This result provides a computation model for the combination of an algebraic specification language based on abstract data types and of a strongly typed functional language with strictly positive inductive types.",
        "published": "2006-10-11T13:54:42Z",
        "link": "http://arxiv.org/abs/cs/0610066v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Type theory and rewriting",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "We study the properties, in particular termination, of dependent types systems for lambda calculus and rewriting.",
        "published": "2006-10-11T14:09:16Z",
        "link": "http://arxiv.org/abs/cs/0610068v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "An Isabelle formalization of protocol-independent secrecy with an   application to e-commerce",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "A protocol-independent secrecy theorem is established and applied to several non-trivial protocols. In particular, it is applied to protocols proposed for protecting the computation results of free-roaming mobile agents doing comparison shopping. All the results presented here have been formally proved in Isabelle by building on Larry Paulson's inductive approach. This therefore provides a library of general theorems that can be applied to other protocols.",
        "published": "2006-10-11T14:20:28Z",
        "link": "http://arxiv.org/abs/cs/0610069v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Inductive types in the Calculus of Algebraic Constructions",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "In a previous work, we proved that almost all of the Calculus of Inductive Constructions (CIC), which is the basis of the proof assistant Coq, can be seen as a Calculus of Algebraic Constructions (CAC), an extension of the Calculus of Constructions with functions and predicates defined by higher-order rewrite rules. In this paper, we not only prove that CIC as a whole can be seen as a CAC, but also that it can be extended with non-free constructors, pattern-matching on defined symbols, non-strictly positive types and inductive-recursive types.",
        "published": "2006-10-11T15:18:13Z",
        "link": "http://arxiv.org/abs/cs/0610070v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Rewriting modulo in Deduction modulo",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "We study the termination of rewriting modulo a set of equations in the Calculus of Algebraic Constructions, an extension of the Calculus of Constructions with functions and predicates defined by higher-order rewrite rules. In a previous work, we defined general syntactic conditions based on the notion of computable closure for ensuring the termination of the combination of rewriting and beta-reduction. Here, we show that this result is preserved when considering rewriting modulo a set of equations if the equivalence classes generated by these equations are finite, the equations are linear and satisfy general syntactic conditions also based on the notion of computable closure. This includes equations like associativity and commutativity, and provides an original treatment of termination modulo equations.",
        "published": "2006-10-11T15:21:50Z",
        "link": "http://arxiv.org/abs/cs/0610071v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Definitions by rewriting in the Calculus of Constructions",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "This paper presents general syntactic conditions ensuring the strong normalization and the logical consistency of the Calculus of Algebraic Constructions, an extension of the Calculus of Constructions with functions and predicates defined by higher-order rewrite rules. On the one hand, the Calculus of Constructions is a powerful type system in which one can formalize the propositions and natural deduction proofs of higher-order logic. On the other hand, rewriting is a simple and powerful computation paradigm. The combination of both allows, among other things, to develop formal proofs with a reduced size and more automation compared with more traditional proof assistants. The main novelty is to consider a general form of rewriting at the predicate-level which generalizes the strong elimination of the Calculus of Inductive Constructions.",
        "published": "2006-10-12T11:47:54Z",
        "link": "http://arxiv.org/abs/cs/0610072v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Inductive types in the Calculus of Algebraic Constructions",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "In a previous work, we proved that an important part of the Calculus of Inductive Constructions (CIC), the basis of the Coq proof assistant, can be seen as a Calculus of Algebraic Constructions (CAC), an extension of the Calculus of Constructions with functions and predicates defined by higher-order rewrite rules. In this paper, we prove that almost all CIC can be seen as a CAC, and that it can be further extended with non-strictly positive types and inductive-recursive types together with non-free constructors and pattern-matching on defined symbols.",
        "published": "2006-10-12T11:48:39Z",
        "link": "http://arxiv.org/abs/cs/0610073v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Computable Closed Euclidean Subsets with and without Computable Points",
        "authors": [
            "Stéphane Le Roux",
            "Martin Ziegler"
        ],
        "summary": "The empty set of course contains no computable point. On the other hand, surprising results due to Zaslavskii, Tseitin, Kreisel, and Lacombe assert the existence of NON-empty co-r.e. closed sets devoid of computable points: sets which are `large' in the sense of positive Lebesgue measure. We observe that a certain size is in fact necessary: every non-empty co-r.e. closed real set without computable points has continuum cardinality.   This leads us to investigate for various classes of computable real subsets whether they necessarily contain a (not necessarily effectively findable) computable point.",
        "published": "2006-10-13T07:45:05Z",
        "link": "http://arxiv.org/abs/cs/0610080v5",
        "categories": [
            "cs.LO",
            "math.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Semantics of Separation-Logic Typing and Higher-order Frame Rules   for<br> Algol-like Languages",
        "authors": [
            "Lars Birkedal",
            "Noah Torp-Smith",
            "Hongseok Yang"
        ],
        "summary": "We show how to give a coherent semantics to programs that are well-specified in a version of separation logic for a language with higher types: idealized algol extended with heaps (but with immutable stack variables). In particular, we provide simple sound rules for deriving higher-order frame rules, allowing for local reasoning.",
        "published": "2006-10-13T08:21:31Z",
        "link": "http://arxiv.org/abs/cs/0610081v2",
        "categories": [
            "cs.LO",
            "F.3; D.3"
        ]
    },
    {
        "title": "Symbolic Simulation-Checking of Dense-Time Systems",
        "authors": [
            "Farn Wang"
        ],
        "summary": "Intuitively, an (implementation) automata is simulated by a (specification) automata if every externally observable transition by the implementation automata can also be made by the specification automata. In this work, we present a symbolic algorithm for the simulation-checking of timed automatas. We first present a simulation-checking procedure that operates on state spaces, representable with convex polyhedra, of timed automatas. We then present techniques to represent those intermediate result convex polyhedra with zones and make the procedure an algorithm. We then discuss how to handle Zeno states in the implementation automata. Finally, we have endeavored to realize the algorithm and report the performance of our algorithm in the experiment.",
        "published": "2006-10-13T18:02:47Z",
        "link": "http://arxiv.org/abs/cs/0610085v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "D.3.1; F.1.1; F.4.3"
        ]
    },
    {
        "title": "Semantic results for ontic and epistemic change",
        "authors": [
            "H. P. van Ditmarsch",
            "B. P. Kooi"
        ],
        "summary": "We give some semantic results for an epistemic logic incorporating dynamic operators to describe information changing events. Such events include epistemic changes, where agents become more informed about the non-changing state of the world, and ontic changes, wherein the world changes. The events are executed in information states that are modeled as pointed Kripke models. Our contribution consists of three semantic results. (i) Given two information states, there is an event transforming one into the other. The linguistic correspondent to this is that every consistent formula can be made true in every information state by the execution of an event. (ii) A more technical result is that: every event corresponds to an event in which the postconditions formalizing ontic change are assignments to `true' and `false' only (instead of assignments to arbitrary formulas in the logical language). `Corresponds' means that execution of either event in a given information state results in bisimilar information states. (iii) The third, also technical, result is that every event corresponds to a sequence of events wherein all postconditions are assignments of a single atom only (instead of simultaneous assignments of more than one atom).",
        "published": "2006-10-15T04:48:48Z",
        "link": "http://arxiv.org/abs/cs/0610093v4",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Quantifier elimination for the reals with a predicate for the powers of   two",
        "authors": [
            "Jeremy Avigad",
            "Yimu Yin"
        ],
        "summary": "In 1985, van den Dries showed that the theory of the reals with a predicate for the integer powers of two admits quantifier elimination in an expanded language, and is hence decidable. He gave a model-theoretic argument, which provides no apparent bounds on the complexity of a decision procedure. We provide a syntactic argument that yields a procedure that is primitive recursive, although not elementary. In particular, we show that it is possible to eliminate a single block of existential quantifiers in time $2^0_{O(n)}$, where $n$ is the length of the input formula and $2_k^x$ denotes $k$-fold iterated exponentiation.",
        "published": "2006-10-19T17:39:49Z",
        "link": "http://arxiv.org/abs/cs/0610117v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Proof Nets and the Identity of Proofs",
        "authors": [
            "Lutz Strassburger"
        ],
        "summary": "These are the notes for a 5-lecture-course given at ESSLLI 2006 in Malaga, Spain. The URL of the school is http://esslli2006.lcc.uma.es/ . This version slightly differs from the one which has been distributed at the school because typos have been removed and comments and suggestions by students have been worked in. The course is intended to be introductory. That means no prior knowledge of proof nets is required. However, the student should be familiar with the basics of propositional logic, and should have seen formal proofs in some formal deductive system (e.g., sequent calculus, natural deduction, resolution, tableaux, calculus of structures, Frege-Hilbert-systems, ...). It is probably helpful if the student knows already what cut elimination is, but this is not strictly necessary. In these notes, I will introduce the concept of ``proof nets'' from the viewpoint of the problem of the identity of proofs. I will proceed in a rather informal way. The focus will be more on presenting ideas than on presenting technical details. The goal of the course is to give the student an overview of the theory of proof nets and make the vast amount of literature on the topic easier accessible to the beginner. For introducing the basic concepts of the theory, I will in the first part of the course stick to the unit-free multiplicative fragment of linear logic because of its rather simple notion of proof nets. In the second part of the course we will see proof nets for more sophisticated logics. This is a basic introduction into proof nets from the perspective of the identity of proofs. We discuss how deductive proofs can be translated into proof nets and what a correctness criterion is.",
        "published": "2006-10-20T11:35:05Z",
        "link": "http://arxiv.org/abs/cs/0610123v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Concurrent Calculus with Atomic Transactions",
        "authors": [
            "Lucia Acciai",
            "Michele Boreale",
            "Silvano Dal Zilio"
        ],
        "summary": "The Software Transactional Memory (STM) model is an original approach for controlling concurrent accesses to ressources without the need for explicit lock-based synchronization mechanisms. A key feature of STM is to provide a way to group sequences of read and write actions inside atomic blocks, similar to database transactions, whose whole effect should occur atomically. In this paper, we investigate STM from a process algebra perspective and define an extension of asynchronous CCS with atomic blocks of actions. Our goal is not only to set a formal ground for reasoning on STM implementations but also to understand how this model fits with other concurrency control mechanisms. We also view this calculus as a test bed for extending process calculi with atomic transactions. This is an interesting direction for investigation since, for the most part, actual works that mix transactions with process calculi consider compensating transactions, a model that lacks all the well-known ACID properties. We show that the addition of atomic transactions results in a very expressive calculus, enough to easily encode other concurrent primitives such as guarded choice and multiset-synchronization (\\`{a} la join-calculus). The correctness of our encodings is proved using a suitable notion of bisimulation equivalence. The equivalence is then applied to prove interesting ``laws of transactions'' and to obtain a simple normal form for transactions.",
        "published": "2006-10-24T08:26:04Z",
        "link": "http://arxiv.org/abs/cs/0610137v1",
        "categories": [
            "cs.LO",
            "cs.DC"
        ]
    },
    {
        "title": "Canonical decomposition of catenation of factorial languages",
        "authors": [
            "A. Frid"
        ],
        "summary": "According to a previous result by S. V. Avgustinovich and the author, each factorial language admits a unique canonical decomposition to a catenation of factorial languages. In this paper, we analyze the appearance of the canonical decomposition of a catenation of two factorial languages whose canonical decompositions are given.",
        "published": "2006-10-26T06:38:30Z",
        "link": "http://arxiv.org/abs/cs/0610149v3",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "ECA-RuleML: An Approach combining ECA Rules with temporal interval-based   KR Event/Action Logics and Transactional Update Logics",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "An important problem to be addressed within Event-Driven Architecture (EDA) is how to correctly and efficiently capture and process the event/action-based logic. This paper endeavors to bridge the gap between the Knowledge Representation (KR) approaches based on durable events/actions and such formalisms as event calculus, on one hand, and event-condition-action (ECA) reaction rules extending the approach of active databases that view events as instantaneous occurrences and/or sequences of events, on the other. We propose formalism based on reaction rules (ECA rules) and a novel interval-based event logic and present concrete RuleML-based syntax, semantics and implementation. We further evaluate this approach theoretically, experimentally and on an example derived from common industry use cases and illustrate its benefits.",
        "published": "2006-10-30T11:56:08Z",
        "link": "http://arxiv.org/abs/cs/0610167v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.MA",
            "cs.SE",
            "I.2; H.2.4; I.2.5; I.2.4; K.6.3"
        ]
    },
    {
        "title": "A Fixed-Parameter Algorithm for #SAT with Parameter Incidence Treewidth",
        "authors": [
            "Marko Samer",
            "Stefan Szeider"
        ],
        "summary": "We present an efficient fixed-parameter algorithm for #SAT parameterized by the incidence treewidth, i.e., the treewidth of the bipartite graph whose vertices are the variables and clauses of the given CNF formula; a variable and a clause are joined by an edge if and only if the variable occurs in the clause. Our algorithm runs in time O(4^k k l N), where k denotes the incidence treewidth, l denotes the size of a largest clause, and N denotes the number of nodes of the tree-decomposition.",
        "published": "2006-10-31T12:58:36Z",
        "link": "http://arxiv.org/abs/cs/0610174v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "Linear Abadi and Plotkin Logic",
        "authors": [
            "Lars Birkedal",
            "Rasmus E. Møgelberg",
            "Rasmus Lerchedahl Petersen"
        ],
        "summary": "We present a formalization of a version of Abadi and   Plotkin's logic for parametricity for a polymorphic dual intuitionistic/linear type theory with fixed points, and show, following Plotkin's suggestions, that it can be used to define a wide collection of types, including existential types, inductive types, coinductive types and general recursive types. We show that the recursive types satisfy a universal property called dinaturality, and we develop reasoning principles for the constructed types. In the case of recursive types, the reasoning principle is a mixed induction/coinduction principle, with the curious property that coinduction holds for general relations, but induction only for a limited collection of ``admissible'' relations. A similar property was observed in Pitts' 1995 analysis of recursive types in domain theory. In a future paper we will develop a category theoretic notion of models of the logic presented here, and show how the results developed in the logic can be transferred to the models.",
        "published": "2006-11-01T08:50:11Z",
        "link": "http://arxiv.org/abs/cs/0611004v3",
        "categories": [
            "cs.LO",
            "F.4.1; D.3.3"
        ]
    },
    {
        "title": "Logic Column 17: A Rendezvous of Logic, Complexity, and Algebra",
        "authors": [
            "Hubie Chen"
        ],
        "summary": "This article surveys recent advances in applying algebraic techniques to constraint satisfaction problems.",
        "published": "2006-11-03T21:51:16Z",
        "link": "http://arxiv.org/abs/cs/0611018v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Logical Approach to Efficient Max-SAT solving",
        "authors": [
            "Javier Larrosa",
            "Federico Heras",
            "Simon de Givry"
        ],
        "summary": "Weighted Max-SAT is the optimization version of SAT and many important problems can be naturally encoded as such. Solving weighted Max-SAT is an important problem from both a theoretical and a practical point of view. In recent years, there has been considerable interest in finding efficient solving techniques. Most of this work focus on the computation of good quality lower bounds to be used within a branch and bound DPLL-like algorithm. Most often, these lower bounds are described in a procedural way. Because of that, it is difficult to realize the {\\em logic} that is behind.   In this paper we introduce an original framework for Max-SAT that stresses the parallelism with classical SAT. Then, we extend the two basic SAT solving techniques: {\\em search} and {\\em inference}. We show that many algorithmic {\\em tricks} used in state-of-the-art Max-SAT solvers are easily expressable in {\\em logic} terms with our framework in a unified manner.   Besides, we introduce an original search algorithm that performs a restricted amount of {\\em weighted resolution} at each visited node. We empirically compare our algorithm with a variety of solving alternatives on several benchmarks. Our experiments, which constitute to the best of our knowledge the most comprehensive Max-sat evaluation ever reported, show that our algorithm is generally orders of magnitude faster than any competitor.",
        "published": "2006-11-06T12:39:05Z",
        "link": "http://arxiv.org/abs/cs/0611025v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Linear Encodings of Bounded LTL Model Checking",
        "authors": [
            "Armin Biere",
            "Keijo Heljanko",
            "Tommi Junttila",
            "Timo Latvala",
            "Viktor Schuppan"
        ],
        "summary": "We consider the problem of bounded model checking (BMC) for linear temporal logic (LTL). We present several efficient encodings that have size linear in the bound. Furthermore, we show how the encodings can be extended to LTL with past operators (PLTL). The generalised encoding is still of linear size, but cannot detect minimal length counterexamples. By using the virtual unrolling technique minimal length counterexamples can be captured, however, the size of the encoding is quadratic in the specification. We also extend virtual unrolling to Buchi automata, enabling them to accept minimal length counterexamples.   Our BMC encodings can be made incremental in order to benefit from incremental SAT technology. With fairly small modifications the incremental encoding can be further enhanced with a termination check, allowing us to prove properties with BMC. Experiments clearly show that our new encodings improve performance of BMC considerably, particularly in the case of the incremental encoding, and that they are very competitive for finding bugs. An analysis of the liveness-to-safety transformation reveals many similarities to the BMC encodings in this paper. Using the liveness-to-safety translation with BDD-based invariant checking results in an efficient method to find shortest counterexamples that complements the BMC-based approach.",
        "published": "2006-11-07T00:07:26Z",
        "link": "http://arxiv.org/abs/cs/0611029v3",
        "categories": [
            "cs.LO",
            "F.3.1; B.6.3; D.2.4; F.4.1"
        ]
    },
    {
        "title": "The Formal System lambda-delta",
        "authors": [
            "F. Guidi"
        ],
        "summary": "The formal system lambda-delta is a typed lambda calculus that pursues the unification of terms, types, environments and contexts as the main goal. lambda-delta takes some features from the Automath-related lambda calculi and some from the pure type systems, but differs from both in that it does not include the Pi construction while it provides for an abbreviation mechanism at the level of terms. lambda-delta enjoys some important desirable properties such as the confluence of reduction, the correctness of types, the uniqueness of types up to conversion, the subject reduction of the type assignment, the strong normalization of the typed terms and, as a corollary, the decidability of type inference problem.",
        "published": "2006-11-09T11:36:58Z",
        "link": "http://arxiv.org/abs/cs/0611040v10",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Analytic Tableaux Calculi for KLM Logics of Nonmonotonic Reasoning",
        "authors": [
            "Laura Giordano",
            "Valentina Gliozzi",
            "Nicola Olivetti",
            "Gian Luca Pozzato"
        ],
        "summary": "We present tableau calculi for some logics of nonmonotonic reasoning, as defined by Kraus, Lehmann and Magidor. We give a tableau proof procedure for all KLM logics, namely preferential, loop-cumulative, cumulative and rational logics. Our calculi are obtained by introducing suitable modalities to interpret conditional assertions. We provide a decision procedure for the logics considered, and we study their complexity.",
        "published": "2006-11-10T16:49:33Z",
        "link": "http://arxiv.org/abs/cs/0611046v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Dense-Timed Petri Nets: Checking Zenoness, Token liveness and   Boundedness",
        "authors": [
            "Parosh Abdulla",
            "Pritha Mahata",
            "Richard Mayr"
        ],
        "summary": "We consider Dense-Timed Petri Nets (TPN), an extension of Petri nets in which each token is equipped with a real-valued clock and where the semantics is lazy (i.e., enabled transitions need not fire; time can pass and disable transitions). We consider the following verification problems for TPNs. (i) Zenoness: whether there exists a zeno-computation from a given marking, i.e., an infinite computation which takes only a finite amount of time. We show decidability of zenoness for TPNs, thus solving an open problem from [Escrig et al.]. Furthermore, the related question if there exist arbitrarily fast computations from a given marking is also decidable. On the other hand, universal zenoness, i.e., the question if all infinite computations from a given marking are zeno, is undecidable. (ii) Token liveness: whether a token is alive in a marking, i.e., whether there is a computation from the marking which eventually consumes the token. We show decidability of the problem by reducing it to the coverability problem, which is decidable for TPNs. (iii) Boundedness: whether the size of the reachable markings is bounded. We consider two versions of the problem; namely semantic boundedness where only live tokens are taken into consideration in the markings, and syntactic boundedness where also dead tokens are considered. We show undecidability of semantic boundedness, while we prove that syntactic boundedness is decidable through an extension of the Karp-Miller algorithm.",
        "published": "2006-11-11T00:08:46Z",
        "link": "http://arxiv.org/abs/cs/0611048v2",
        "categories": [
            "cs.LO",
            "F.1.1; F.3.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "Numerical Simulation guided Lazy Abstraction Refinement for Nonlinear   Hybrid Automata",
        "authors": [
            "Sumit Kumar Jha"
        ],
        "summary": "This draft suggests a new counterexample guided abstraction refinement (CEGAR) framework that uses the combination of numerical simulation for nonlinear differential equations with linear programming for linear hybrid automata (LHA) to perform reachability analysis on nonlinear hybrid automata. A notion of $\\epsilon-$ structural robustness is also introduced which allows the algorithm to validate counterexamples using numerical simulations.   Keywords: verification, model checking, hybrid systems, hybrid automata, robustness, robust hybrid systems, numerical simulation, cegar, abstraction refinement.",
        "published": "2006-11-13T17:28:24Z",
        "link": "http://arxiv.org/abs/cs/0611051v1",
        "categories": [
            "cs.LO",
            "B.5.2"
        ]
    },
    {
        "title": "Formalising Sylow's theorems in Coq",
        "authors": [
            "Laurent Thery"
        ],
        "summary": "This report presents a formalisation of Sylow's theorems done in {\\sc Coq}. The formalisation has been done in a couple of weeks on top of Georges Gonthier's {\\sc ssreflect} \\cite{ssreflect}. There were two ideas behind formalising Sylow's theorems. The first one was to get familiar with Georges way of doing proofs. The second one was to contribute to the collective effort to formalise a large subset of group theory in {\\sc Coq} with some non-trivial proofs.}",
        "published": "2006-11-14T12:58:27Z",
        "link": "http://arxiv.org/abs/cs/0611057v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Model Theory of Ultrafinitism I: Fuzzy Initial Segments of Arithmetics",
        "authors": [
            "Mirco A. Mannucci",
            "Rose M. Cherubin"
        ],
        "summary": "This article is the first of an intended series of works on the model theory of Ultrafinitism. It is roughly divided into two parts. The first one addresses some of the issues related to ultrafinitistic programs, as well as some of the core ideas proposed thus far. The second part of the paper presents a model of ultrafinitistic arithmetics based on the notion of fuzzy initial segments of the standard natural numbers series. We also introduce a proof theory and a semantics for ultrafinitism through which feasibly consistent theories can be treated on the same footing as their classically consistent counterparts. We conclude with a brief sketch of a foundational program, that aims at reproducing the transfinite within the finite realm.",
        "published": "2006-11-21T03:05:30Z",
        "link": "http://arxiv.org/abs/cs/0611100v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Expressiveness of Metric modalities for continuous time",
        "authors": [
            "Yoram Hirshfeld",
            "Alexander Rabinovich"
        ],
        "summary": "We prove a conjecture by A. Pnueli and strengthen it showing a sequence of \"counting modalities\" none of which is expressible in the temporal logic generated by the previous modalities, over the real line, or over the positive reals. Moreover, there is no finite temporal logic that can express all of them over the real line, so that no finite metric temporal logic is expressively complete.",
        "published": "2006-11-22T21:03:26Z",
        "link": "http://arxiv.org/abs/cs/0611119v2",
        "categories": [
            "cs.LO",
            "F.3.1; F.4; F.4.1"
        ]
    },
    {
        "title": "Knowledge Representation Concepts for Automated SLA Management",
        "authors": [
            "Adrian Paschke",
            "Martin Bichler"
        ],
        "summary": "Outsourcing of complex IT infrastructure to IT service providers has increased substantially during the past years. IT service providers must be able to fulfil their service-quality commitments based upon predefined Service Level Agreements (SLAs) with the service customer. They need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technology. The complexity of contractual logic in SLAs requires new forms of knowledge representation to automatically draw inferences and execute contractual agreements. A logic-based approach provides several advantages including automated rule chaining allowing for compact knowledge representation as well as flexibility to adapt to rapidly changing business requirements. We suggest adequate logical formalisms for representation and enforcement of SLA rules and describe a proof-of-concept implementation. The article describes selected formalisms of the ContractLog KR and their adequacy for automated SLA management and presents results of experiments to demonstrate flexibility and scalability of the approach.",
        "published": "2006-11-23T13:25:45Z",
        "link": "http://arxiv.org/abs/cs/0611122v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "I.2"
        ]
    },
    {
        "title": "Predicate Abstraction via Symbolic Decision Procedures",
        "authors": [
            "Shuvendu K. Lahiri",
            "Thomas Ball",
            "Byron Cook"
        ],
        "summary": "We present a new approach for performing predicate abstraction based on symbolic decision procedures. Intuitively, a symbolic decision procedure for a theory takes a set of predicates in the theory and symbolically executes a decision procedure on all the subsets over the set of predicates. The result of the symbolic decision procedure is a shared expression (represented by a directed acyclic graph) that implicitly represents the answer to a predicate abstraction query.   We present symbolic decision procedures for the logic of Equality and Uninterpreted Functions (EUF) and Difference logic (DIFF) and show that these procedures run in pseudo-polynomial (rather than exponential) time. We then provide a method to construct symbolic decision procedures for simple mixed theories (including the two theories mentioned above) using an extension of the Nelson-Oppen combination method. We present preliminary evaluation of our Procedure on predicate abstraction benchmarks from device driver verification in SLAM.",
        "published": "2006-12-01T19:56:11Z",
        "link": "http://arxiv.org/abs/cs/0612003v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Termination orders for 3-dimensional rewriting",
        "authors": [
            "Yves Guiraud"
        ],
        "summary": "This paper studies 3-polygraphs as a framework for rewriting on two-dimensional words. A translation of term rewriting systems into 3-polygraphs with explicit resource management is given, and the respective computational properties of each system are studied. Finally, a convergent 3-polygraph for the (commutative) theory of Z/2Z-vector spaces is given. In order to prove these results, it is explained how to craft a class of termination orders for 3-polygraphs.",
        "published": "2006-12-04T11:32:52Z",
        "link": "http://arxiv.org/abs/math/0612083v1",
        "categories": [
            "math.CT",
            "cs.LO",
            "08A50; 08A70; 16S15; 18C10; 18D05; 68Q70"
        ]
    },
    {
        "title": "Termination orders for 3-polygraphs",
        "authors": [
            "Yves Guiraud"
        ],
        "summary": "This note presents the first known class of termination orders for 3-polygraphs, together with an application.",
        "published": "2006-12-04T11:45:08Z",
        "link": "http://arxiv.org/abs/math/0612084v1",
        "categories": [
            "math.CT",
            "cs.LO",
            "08A50; 08A70; 16S15; 18C10; 18D05; 68Q70"
        ]
    },
    {
        "title": "Two polygraphic presentations of Petri nets",
        "authors": [
            "Yves Guiraud"
        ],
        "summary": "This document gives an algebraic and two polygraphic translations of Petri nets, all three providing an easier way to describe reductions and to identify some of them. The first one sees places as generators of a commutative monoid and transitions as rewriting rules on it: this setting is totally equivalent to Petri nets, but lacks any graphical intuition. The second one considers places as 1-dimensional cells and transitions as 2-dimensional ones: this translation recovers a graphical meaning but raises many difficulties since it uses explicit permutations. Finally, the third translation sees places as degenerated 2-dimensional cells and transitions as 3-dimensional ones: this is a setting equivalent to Petri nets, equipped with a graphical interpretation.",
        "published": "2006-12-04T12:35:10Z",
        "link": "http://arxiv.org/abs/math/0612088v1",
        "categories": [
            "math.CT",
            "cs.LO"
        ]
    },
    {
        "title": "The three dimensions of proofs",
        "authors": [
            "Yves Guiraud"
        ],
        "summary": "In this document, we study a 3-polygraphic translation for the proofs of SKS, a formal system for classical propositional logic. We prove that the free 3-category generated by this 3-polygraph describes the proofs of classical propositional logic modulo structural bureaucracy. We give a 3-dimensional generalization of Penrose diagrams and use it to provide several pictures of a proof. We sketch how local transformations of proofs yield a non contrived example of 4-dimensional rewriting.",
        "published": "2006-12-04T13:03:09Z",
        "link": "http://arxiv.org/abs/math/0612089v1",
        "categories": [
            "math.CT",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Cores of Countably Categorical Structures",
        "authors": [
            "Manuel Bodirsky"
        ],
        "summary": "A relational structure is a core, if all its endomorphisms are embeddings. This notion is important for computational complexity classification of constraint satisfaction problems. It is a fundamental fact that every finite structure has a core, i.e., has an endomorphism such that the structure induced by its image is a core; moreover, the core is unique up to isomorphism. Weprove that every \\omega -categorical structure has a core. Moreover, every \\omega-categorical structure is homomorphically equivalent to a model-complete core, which is unique up to isomorphism, and which is finite or \\omega -categorical. We discuss consequences for constraint satisfaction with \\omega -categorical templates.",
        "published": "2006-12-13T09:59:56Z",
        "link": "http://arxiv.org/abs/cs/0612069v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "On Completeness of Logical Relations for Monadic Types",
        "authors": [
            "Slawomir Lasota",
            "David Nowak",
            "Yu Zhang"
        ],
        "summary": "Software security can be ensured by specifying and verifying security properties of software using formal methods with strong theoretical bases. In particular, programs can be modeled in the framework of lambda-calculi, and interesting properties can be expressed formally by contextual equivalence (a.k.a. observational equivalence). Furthermore, imperative features, which exist in most real-life software, can be nicely expressed in the so-called computational lambda-calculus. Contextual equivalence is difficult to prove directly, but we can often use logical relations as a tool to establish it in lambda-calculi. We have already defined logical relations for the computational lambda-calculus in previous work. We devote this paper to the study of their completeness w.r.t. contextual equivalence in the computational lambda-calculus.",
        "published": "2006-12-21T09:02:32Z",
        "link": "http://arxiv.org/abs/cs/0612106v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.1; F.3.1"
        ]
    },
    {
        "title": "Adventures in time and space",
        "authors": [
            "Norman Danner",
            "James S. Royer"
        ],
        "summary": "This paper investigates what is essentially a call-by-value version of PCF under a complexity-theoretically motivated type system. The programming formalism, ATR, has its first-order programs characterize the polynomial-time computable functions, and its second-order programs characterize the type-2 basic feasible functionals of Mehlhorn and of Cook and Urquhart. (The ATR-types are confined to levels 0, 1, and 2.) The type system comes in two parts, one that primarily restricts the sizes of values of expressions and a second that primarily restricts the time required to evaluate expressions. The size-restricted part is motivated by Bellantoni and Cook's and Leivant's implicit characterizations of polynomial-time. The time-restricting part is an affine version of Barber and Plotkin's DILL. Two semantics are constructed for ATR. The first is a pruning of the naive denotational semantics for ATR. This pruning removes certain functions that cause otherwise feasible forms of recursion to go wrong. The second semantics is a model for ATR's time complexity relative to a certain abstract machine. This model provides a setting for complexity recurrences arising from ATR recursions, the solutions of which yield second-order polynomial time bounds. The time-complexity semantics is also shown to be sound relative to the costs of interpretation on the abstract machine.",
        "published": "2006-12-21T23:54:20Z",
        "link": "http://arxiv.org/abs/cs/0612116v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.3; F.1.3; F.3.2"
        ]
    },
    {
        "title": "Lineal: A linear-algebraic Lambda-calculus",
        "authors": [
            "Pablo Arrighi",
            "Gilles Dowek"
        ],
        "summary": "We provide a computational definition of the notions of vector space and bilinear functions. We use this result to introduce a minimal language combining higher-order computation and linear algebra. This language extends the Lambda-calculus with the possibility to make arbitrary linear combinations of terms alpha.t + beta.u. We describe how to \"execute\" this language in terms of a few rewrite rules, and justify them through the two fundamental requirements that the language be a language of linear operators, and that it be higher-order. We mention the perspectives of this work in the field of quantum computation, whose circuits we show can be easily encoded in the calculus. Finally, we prove the confluence of the entire calculus.",
        "published": "2006-12-22T17:25:56Z",
        "link": "http://arxiv.org/abs/quant-ph/0612199v6",
        "categories": [
            "quant-ph",
            "cs.LO",
            "cs.PL",
            "03B40, 68N18, 81P68",
            "F.4.1; F.4.2; F.1.1"
        ]
    },
    {
        "title": "Generalizing the Paige-Tarjan Algorithm by Abstract Interpretation",
        "authors": [
            "Francesco Ranzato",
            "Francesco Tapparo"
        ],
        "summary": "The Paige and Tarjan algorithm (PT) for computing the coarsest refinement of a state partition which is a bisimulation on some Kripke structure is well known. It is also well known in model checking that bisimulation is equivalent to strong preservation of CTL, or, equivalently, of Hennessy-Milner logic. Drawing on these observations, we analyze the basic steps of the PT algorithm from an abstract interpretation perspective, which allows us to reason on strong preservation in the context of generic inductively defined (temporal) languages and of possibly non-partitioning abstract models specified by abstract interpretation. This leads us to design a generalized Paige-Tarjan algorithm, called GPT, for computing the minimal refinement of an abstract interpretation-based model that strongly preserves some given language. It turns out that PT is a straight instance of GPT on the domain of state partitions for the case of strong preservation of Hennessy-Milner logic. We provide a number of examples showing that GPT is of general use. We first show how a well-known efficient algorithm for computing stuttering equivalence can be viewed as a simple instance of GPT. We then instantiate GPT in order to design a new efficient algorithm for computing simulation equivalence that is competitive with the best available algorithms. Finally, we show how GPT allows to compute new strongly preserving abstract models by providing an efficient algorithm that computes the coarsest refinement of a given partition that strongly preserves the language generated by the reachability operator.",
        "published": "2006-12-22T19:45:34Z",
        "link": "http://arxiv.org/abs/cs/0612120v2",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.2"
        ]
    },
    {
        "title": "Characterizing correlations of flow oscillations at bottlenecks",
        "authors": [
            "Tobias Kretz",
            "Marko Woelki",
            "Michael Schreckenberg"
        ],
        "summary": "\"Oscillations\" occur in quite different kinds of many-particle-systems when two groups of particles with different directions of motion meet or intersect at a certain spot. We present a model of pedestrian motion that is able to reproduce oscillations with different characteristics. The Wald-Wolfowitz test and Gillis' correlated random walk are shown to hold observables that can be used to characterize different kinds of oscillations.",
        "published": "2006-01-02T10:10:19Z",
        "link": "http://arxiv.org/abs/cond-mat/0601021v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.MA"
        ]
    },
    {
        "title": "Community Structure in the United States House of Representatives",
        "authors": [
            "Mason A. Porter",
            "Peter J. Mucha",
            "M. E. J. Newman",
            "A. J. Friend"
        ],
        "summary": "We investigate the networks of committee and subcommittee assignments in the United States House of Representatives from the 101st--108th Congresses, with the committees connected by ``interlocks'' or common membership. We examine the community structure in these networks using several methods, revealing strong links between certain committees as well as an intrinsic hierarchical structure in the House as a whole. We identify structural changes, including additional hierarchical levels and higher modularity, resulting from the 1994 election, in which the Republican party earned majority status in the House for the first time in more than forty years. We also combine our network approach with analysis of roll call votes using singular value decomposition to uncover correlations between the political and organizational structure of House committees.",
        "published": "2006-02-04T22:41:34Z",
        "link": "http://arxiv.org/abs/physics/0602033v3",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.MA",
            "nlin.AO",
            "physics.data-an"
        ]
    },
    {
        "title": "Improving the CSIEC Project and Adapting It to the English Teaching and   Learning in China",
        "authors": [
            "Jiyou Jia",
            "Shufen Hou",
            "Weichao Chen"
        ],
        "summary": "In this paper after short review of the CSIEC project initialized by us in 2003 we present the continuing development and improvement of the CSIEC project in details, including the design of five new Microsoft agent characters representing different virtual chatting partners and the limitation of simulated dialogs in specific practical scenarios like graduate job application interview, then briefly analyze the actual conditions and features of its application field: web-based English education in China. Finally we introduce our efforts to adapt this system to the requirements of English teaching and learning in China and point out the work next to do.",
        "published": "2006-02-06T15:17:34Z",
        "link": "http://arxiv.org/abs/cs/0602018v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.MA",
            "K.3.1; I.2.7; I.2.11"
        ]
    },
    {
        "title": "Preference fusion when the number of alternatives exceeds two: indirect   scoring procedures",
        "authors": [
            "Pavel Chebotarev",
            "Elena Shamis"
        ],
        "summary": "We consider the problem of aggregation of incomplete preferences represented by arbitrary binary relations or incomplete paired comparison matrices. For a number of indirect scoring procedures we examine whether or not they satisfy the axiom of self-consistent monotonicity. The class of {\\em win-loss combining scoring procedures} is introduced which contains a majority of known scoring procedures. Two main results are established. According to the first one, every win-loss combining scoring procedure breaks self-consistent monotonicity. The second result provides a sufficient condition of satisfying self-consistent monotonicity.",
        "published": "2006-02-08T20:23:37Z",
        "link": "http://arxiv.org/abs/math/0602171v3",
        "categories": [
            "math.OC",
            "cs.MA",
            "math.CO",
            "91B08, 91B10, 91B12, 62J15"
        ]
    },
    {
        "title": "Emergence Explained",
        "authors": [
            "Russ Abbott"
        ],
        "summary": "Emergence (macro-level effects from micro-level causes) is at the heart of the conflict between reductionism and functionalism. How can there be autonomous higher level laws of nature (the functionalist claim) if everything can be reduced to the fundamental forces of physics (the reductionist position)? We cut through this debate by applying a computer science lens to the way we view nature. We conclude (a) that what functionalism calls the special sciences (sciences other than physics) do indeed study autonomous laws and furthermore that those laws pertain to real higher level entities but (b) that interactions among such higher-level entities is epiphenomenal in that they can always be reduced to primitive physical forces. In other words, epiphenomena, which we will identify with emergent phenomena, do real higher-level work. The proposed perspective provides a framework for understanding many thorny issues including the nature of entities, stigmergy, the evolution of complexity, phase transitions, supervenience, and downward entailment. We also discuss some practical considerations pertaining to systems of systems and the limitations of modeling.",
        "published": "2006-02-12T22:11:14Z",
        "link": "http://arxiv.org/abs/cs/0602045v1",
        "categories": [
            "cs.MA",
            "cs.DC",
            "cs.GL"
        ]
    },
    {
        "title": "Building Scenarios for Environmental Management and Planning: An   IT-Based Approach",
        "authors": [
            "Dino Borri",
            "Domenico Camarda"
        ],
        "summary": "Oftentimes, the need to build multidiscipline knowledge bases, oriented to policy scenarios, entails the involvement of stakeholders in manifold domains, with a juxtaposition of different languages whose semantics can hardly allow inter-domain transfers. A useful support for planning is the building up of durable IT based interactive platforms, where it is possible to modify initial positions toward a semantic convergence. The present paper shows an area-based application of these tools, for the integrated distance-management of different forms of knowledge expressed by selected stakeholders about environmental planning issues, in order to build alternative development scenarios.   Keywords: Environmental planning, Scenario building, Multi-source knowledge, IT-based",
        "published": "2006-02-15T14:46:05Z",
        "link": "http://arxiv.org/abs/cs/0602056v1",
        "categories": [
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Characterizations of scoring methods for preference aggregation",
        "authors": [
            "Pavel Chebotarev",
            "Elena Shamis"
        ],
        "summary": "The paper surveys more than forty characterizations of scoring methods for preference aggregation and contains one new result. A general scoring operator is {\\it self-consistent} if alternative $i$ is assigned a greater score than $j$ whenever $i$ gets no worse (better) results of comparisons and its `opponents' are assigned respectively greater (no smaller) scores than those of $j$. We prove that self-consistency is satisfied if and only if the application of a scoring operator reduces to the solution of a homogeneous system of algebraic equations with a monotone function on the left-hand side.",
        "published": "2006-02-23T09:23:57Z",
        "link": "http://arxiv.org/abs/math/0602522v1",
        "categories": [
            "math.OC",
            "cs.MA",
            "math.FA",
            "91B08; 91B10; 91B12; 91B16"
        ]
    },
    {
        "title": "From Incomplete Preferences to Ranking via Optimization",
        "authors": [
            "Pavel Chebotarev",
            "Elena Shamis"
        ],
        "summary": "We consider methods for aggregating preferences that are based on the resolution of discrete optimization problems. The preferences are represented by arbitrary binary relations (possibly weighted) or incomplete paired comparison matrices. This incomplete case remains practically unexplored so far. We examine the properties of several known methods and propose one new method. In particular, we test whether these methods obey a new axiom referred to as {\\it Self-Consistent Monotonicity}. Some results are established that characterize solutions of the related optimization problems.",
        "published": "2006-02-24T16:09:14Z",
        "link": "http://arxiv.org/abs/math/0602552v1",
        "categories": [
            "math.OC",
            "cs.MA",
            "math.CO",
            "91B10; 90C35; 90C27; 91B08; 90B80"
        ]
    },
    {
        "title": "Reasoning About Knowledge of Unawareness",
        "authors": [
            "Joseph Y. halpern",
            "Leandro Chaves Rego"
        ],
        "summary": "Awareness has been shown to be a useful addition to standard epistemic logic for many applications. However, standard propositional logics for knowledge and awareness cannot express the fact that an agent knows that there are facts of which he is unaware without there being an explicit fact that the agent knows he is unaware of. We propose a logic for reasoning about knowledge of unawareness, by extending Fagin and Halpern's \\emph{Logic of General Awareness}. The logic allows quantification over variables, so that there is a formula in the language that can express the fact that ``an agent explicitly knows that there exists a fact of which he is unaware''. Moreover, that formula can be true without the agent explicitly knowing that he is unaware of any particular formula. We provide a sound and complete axiomatization of the logic, using standard axioms from the literature to capture the quantification operator. Finally, we show that the validity problem for the logic is recursively enumerable, but not decidable.",
        "published": "2006-03-05T21:14:59Z",
        "link": "http://arxiv.org/abs/cs/0603020v1",
        "categories": [
            "cs.LO",
            "cs.MA"
        ]
    },
    {
        "title": "If a tree casts a shadow is it telling the time?",
        "authors": [
            "Russ Abbott"
        ],
        "summary": "Physical processes are computations only when we use them to externalize thought. Computation is the performance of one or more fixed processes within a contingent environment. We reformulate the Church-Turing thesis so that it applies to programs rather than to computability. When suitably formulated agent-based computing in an open, multi-scalar environment represents the current consensus view of how we interact with the world. But we don't know how to formulate multi-scalar environments.",
        "published": "2006-03-30T22:31:26Z",
        "link": "http://arxiv.org/abs/cs/0603125v2",
        "categories": [
            "cs.MA",
            "cs.GL"
        ]
    },
    {
        "title": "Open at the Top; Open at the Bottom; and Continually (but Slowly)   Evolving",
        "authors": [
            "Russ Abbott"
        ],
        "summary": "Systems of systems differ from traditional systems in that they are open at the top, open at the bottom, and continually (but slowly) evolving. \"Open at the top\" means that there is no pre-defined top level application. New applications may be created at any time. \"Open at the bottom\" means that the system primitives are defined functionally rather than concretely. This allows the implementation of these primitives to be modified as technology changes. \"Continually (but slowly) evolving\" means that the system's functionality is stable enough to be useful but is understood to be subject to modification. Systems with these properties tend to be environments within which other systems operate--and hence are systems of systems. It is also important to understand the larger environment within which a system of systems exists.",
        "published": "2006-03-30T22:53:15Z",
        "link": "http://arxiv.org/abs/cs/0603126v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Complex Systems + Systems Engineering = Complex Systems Engineeri",
        "authors": [
            "Russ Abbott"
        ],
        "summary": "One may define a complex system as a system in which phenomena emerge as a consequence of multiscale interaction among the system's components and their environments. The field of Complex Systems is the study of such systems--usually naturally occurring, either bio-logical or social. Systems Engineering may be understood to include the conceptualising and building of systems that consist of a large number of concurrently operating and interacting components--usually including both human and non-human elements. It has become increasingly apparent that the kinds of systems that systems engineers build have many of the same multiscale characteristics as those of naturally occurring complex systems. In other words, systems engineering is the engineering of complex systems. This paper and the associated panel will explore some of the connections between the fields of complex systems and systems engineering.",
        "published": "2006-03-30T22:58:12Z",
        "link": "http://arxiv.org/abs/cs/0603127v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Naming Games in Spatially-Embedded Random Networks",
        "authors": [
            "Qiming Lu",
            "G. Korniss",
            "Boleslaw K. Szymanski"
        ],
        "summary": "We investigate a prototypical agent-based model, the Naming Game, on random geometric networks. The Naming Game is a minimal model, employing local communications that captures the emergence of shared communication schemes (languages) in a population of autonomous semiotic agents. Implementing the Naming Games on random geometric graphs, local communications being local broadcasts, serves as a model for agreement dynamics in large-scale, autonomously operating wireless sensor networks. Further, it captures essential features of the scaling properties of the agreement process for spatially-embedded autonomous agents. We also present results for the case when a small density of long-range communication links are added on top of the random geometric graph, resulting in a \"small-world\"-like network and yielding a significantly reduced time to reach global agreement.",
        "published": "2006-04-19T17:48:11Z",
        "link": "http://arxiv.org/abs/cs/0604075v3",
        "categories": [
            "cs.MA",
            "cond-mat.stat-mech",
            "cs.AI"
        ]
    },
    {
        "title": "The emergence of knowledge exchange: an agent-based model of a software   market",
        "authors": [
            "Maria Chli",
            "Philippe De Wilde"
        ],
        "summary": "We investigate knowledge exchange among commercial organisations, the rationale behind it and its effects on the market. Knowledge exchange is known to be beneficial for industry, but in order to explain it, authors have used high level concepts like network effects, reputation and trust. We attempt to formalise a plausible and elegant explanation of how and why companies adopt information exchange and why it benefits the market as a whole when this happens. This explanation is based on a multi-agent model that simulates a market of software providers. Even though the model does not include any high-level concepts, information exchange naturally emerges during simulations as a successful profitable behaviour. The conclusions reached by this agent-based analysis are twofold: (1) A straightforward set of assumptions is enough to give rise to exchange in a software market. (2) Knowledge exchange is shown to increase the efficiency of the market.",
        "published": "2006-04-20T11:20:16Z",
        "link": "http://arxiv.org/abs/cs/0604078v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "Modeling and Mathematical Analysis of Swarms of Microscopic Robots",
        "authors": [
            "Aram Galstyan",
            "Tad Hogg",
            "Kristina Lerman"
        ],
        "summary": "The biologically-inspired swarm paradigm is being used to design self-organizing systems of locally interacting artificial agents. A major difficulty in designing swarms with desired characteristics is understanding the causal relation between individual agent and collective behaviors. Mathematical analysis of swarm dynamics can address this difficulty to gain insight into system design. This paper proposes a framework for mathematical modeling of swarms of microscopic robots that may one day be useful in medical applications. While such devices do not yet exist, the modeling approach can be helpful in identifying various design trade-offs for the robots and be a useful guide for their eventual fabrication. Specifically, we examine microscopic robots that reside in a fluid, for example, a bloodstream, and are able to detect and respond to different chemicals. We present the general mathematical model of a scenario in which robots locate a chemical source. We solve the scenario in one-dimension and show how results can be used to evaluate certain design decisions.",
        "published": "2006-04-27T23:33:25Z",
        "link": "http://arxiv.org/abs/cs/0604110v1",
        "categories": [
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "Analysis of Dynamic Task Allocation in Multi-Robot Systems",
        "authors": [
            "Kristina Lerman",
            "Chris Jones",
            "Aram Galstyan",
            "Maja J Mataric"
        ],
        "summary": "Dynamic task allocation is an essential requirement for multi-robot systems operating in unknown dynamic environments. It allows robots to change their behavior in response to environmental changes or actions of other robots in order to improve overall system performance. Emergent coordination algorithms for task allocation that use only local sensing and no direct communication between robots are attractive because they are robust and scalable. However, a lack of formal analysis tools makes emergent coordination algorithms difficult to design. In this paper we present a mathematical model of a general dynamic task allocation mechanism. Robots using this mechanism have to choose between two types of task, and the goal is to achieve a desired task division in the absence of explicit communication and global knowledge. Robots estimate the state of the environment from repeated local observations and decide which task to choose based on these observations. We model the robots and observations as stochastic processes and study the dynamics of the collective behavior. Specifically, we analyze the effect that the number of observations and the choice of the decision function have on the performance of the system. The mathematical models are validated in a multi-robot multi-foraging scenario. The model's predictions agree very closely with experimental results from sensor-based simulations.",
        "published": "2006-04-27T23:56:10Z",
        "link": "http://arxiv.org/abs/cs/0604111v1",
        "categories": [
            "cs.RO",
            "cs.MA"
        ]
    },
    {
        "title": "On the Design of Agent-Based Systems using UML and Extensions",
        "authors": [
            "Mihaela Dinsoreanu",
            "Ioan Salomie",
            "Kalman Pusztai"
        ],
        "summary": "The Unified Software Development Process (USDP) and UML have been now generally accepted as the standard methodology and modeling language for developing Object-Oriented Systems. Although Agent-based Systems introduces new issues, we consider that USDP and UML can be used in an extended manner for modeling Agent-based Systems. The paper presents a methodology for designing agent-based systems and the specific models expressed in an UML-based notation corresponding to each phase of the software development process. UML was extended using the provided mechanism: stereotypes. Therefore, this approach can be managed with any CASE tool supporting UML. A Case Study, the development of a specific agent-based Student Evaluation System (SAS), is presented.",
        "published": "2006-05-08T12:21:51Z",
        "link": "http://arxiv.org/abs/cs/0605031v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "A framework of reusable structures for mobile agent development",
        "authors": [
            "Tudor Marian",
            "Bogdan Dumitriu",
            "Mihaela Dinsoreanu",
            "Ioan Salomie"
        ],
        "summary": "Mobile agents research is clearly aiming towards imposing agent based development as the next generation of tools for writing software. This paper comes with its own contribution to this global goal by introducing a novel unifying framework meant to bring simplicity and interoperability to and among agent platforms as we know them today. In addition to this, we also introduce a set of agent behaviors which, although tailored for and from the area of virtual learning environments, are none the less generic enough to be used for rapid, simple, useful and reliable agent deployment. The paper also presents an illustrative case study brought forward to prove the feasibility of our design.",
        "published": "2006-05-08T12:27:59Z",
        "link": "http://arxiv.org/abs/cs/0605032v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Mobile Agent Based Solutions for Knowledge Assessment in elearning   Environments",
        "authors": [
            "Mihaela Dinsoreanu",
            "Cristian Godja",
            "Claudiu Anghel",
            "Ioan Salomie",
            "Tom Coffey"
        ],
        "summary": "E-learning is nowadays one of the most interesting of the \"e- \" domains available through the Internet. The main problem to create a Web-based, virtual environment is to model the traditional domain and to implement the model using the most suitable technologies. We analyzed the distance learning domain and investigated the possibility to implement some e-learning services using mobile agent technologies. This paper presents a model of the Student Assessment Service (SAS) and an agent-based framework developed to be used for implementing specific applications. A specific Student Assessment application that relies on the framework was developed.",
        "published": "2006-05-08T12:37:13Z",
        "link": "http://arxiv.org/abs/cs/0605033v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Three Logistic Models for the Ecological and Economic Interactions:   Symbiosis, Predator-Prey and Competition",
        "authors": [
            "Ricardo Lopez-Ruiz",
            "Daniele Fournier-Prunaret"
        ],
        "summary": "If one isolated species (corporation) is supposed to evolve following the logistic mapping, then we are tempted to think that the dynamics of two species (corporations) can be expressed by a coupled system of two discrete logistic equations. As three basic relationships between two species are present in Nature, namely symbiosis, predator-prey and competition, three different models are obtained. Each model is a cubic two-dimensional discrete logistic-type equation with its own dynamical properties: stationary regime, periodicity, quasi-periodicity and chaos. We also propose that these models could be useful for thinking in the different interactions happening in the economic world, as for instance for the competition and the collaboration between corporations. Furthermore, these models could be considered as the basic ingredients to construct more complex interactions in the ecological and economic networks.",
        "published": "2006-05-12T17:57:52Z",
        "link": "http://arxiv.org/abs/nlin/0605029v1",
        "categories": [
            "nlin.AO",
            "cs.MA",
            "math.DS"
        ]
    },
    {
        "title": "Curve Shortening and the Rendezvous Problem for Mobile Autonomous Robots",
        "authors": [
            "Stephen L. Smith",
            "Mireille E. Broucke",
            "Bruce A. Francis"
        ],
        "summary": "If a smooth, closed, and embedded curve is deformed along its normal vector field at a rate proportional to its curvature, it shrinks to a circular point. This curve evolution is called Euclidean curve shortening and the result is known as the Gage-Hamilton-Grayson Theorem. Motivated by the rendezvous problem for mobile autonomous robots, we address the problem of creating a polygon shortening flow. A linear scheme is proposed that exhibits several analogues to Euclidean curve shortening: The polygon shrinks to an elliptical point, convex polygons remain convex, and the perimeter of the polygon is monotonically decreasing.",
        "published": "2006-05-16T22:30:01Z",
        "link": "http://arxiv.org/abs/cs/0605070v1",
        "categories": [
            "cs.RO",
            "cs.MA",
            "I.2.9"
        ]
    },
    {
        "title": "An Internet-enabled technology to support Evolutionary Design",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "K. Ueda"
        ],
        "summary": "This paper discusses the systematic use of product feedback information to support life-cycle design approaches and provides guidelines for developing a design at both the product and the system levels. Design activities are surveyed in the light of the product life cycle, and the design information flow is interpreted from a semiotic perspective. The natural evolution of a design is considered, the notion of design expectations is introduced, and the importance of evaluation of these expectations in dynamic environments is argued. Possible strategies for reconciliation of the expectations and environmental factors are described. An Internet-enabled technology is proposed to monitor product functionality, usage, and operational environment and supply the designer with relevant information. A pilot study of assessing design expectations of a refrigerator is outlined, and conclusions are drawn.",
        "published": "2006-05-25T04:39:11Z",
        "link": "http://arxiv.org/abs/cs/0605119v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.AR",
            "cs.MA",
            "cs.NI"
        ]
    },
    {
        "title": "Stable partitions in coalitional games",
        "authors": [
            "Krzysztof R. Apt",
            "Tadeusz Radzik"
        ],
        "summary": "We propose a notion of a stable partition in a coalitional game that is parametrized by the concept of a defection function. This function assigns to each partition of the grand coalition a set of different coalition arrangements for a group of defecting players. The alternatives are compared using their social welfare. We characterize the stability of a partition for a number of most natural defection functions and investigate whether and how so defined stable partitions can be reached from any initial partition by means of simple transformations. The approach is illustrated by analyzing an example in which a set of stores seeks an optimal transportation arrangement.",
        "published": "2006-05-29T16:03:20Z",
        "link": "http://arxiv.org/abs/cs/0605132v1",
        "categories": [
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "Microscopic activity patterns in the Naming Game",
        "authors": [
            "Luca Dall'Asta",
            "Andrea Baronchelli"
        ],
        "summary": "The models of statistical physics used to study collective phenomena in some interdisciplinary contexts, such as social dynamics and opinion spreading, do not consider the effects of the memory on individual decision processes. On the contrary, in the Naming Game, a recently proposed model of Language formation, each agent chooses a particular state, or opinion, by means of a memory-based negotiation process, during which a variable number of states is collected and kept in memory. In this perspective, the statistical features of the number of states collected by the agents becomes a relevant quantity to understand the dynamics of the model, and the influence of topological properties on memory-based models. By means of a master equation approach, we analyze the internal agent dynamics of Naming Game in populations embedded on networks, finding that it strongly depends on very general topological properties of the system (e.g. average and fluctuations of the degree). However, the influence of topological properties on the microscopic individual dynamics is a general phenomenon that should characterize all those social interactions that can be modeled by memory-based negotiation processes.",
        "published": "2006-06-05T19:57:55Z",
        "link": "http://arxiv.org/abs/cond-mat/0606125v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Self-Replication and Self-Assembly for Manufacturing",
        "authors": [
            "Robert Ewaschuk",
            "Peter D. Turney"
        ],
        "summary": "It has been argued that a central objective of nanotechnology is to make products inexpensively, and that self-replication is an effective approach to very low-cost manufacturing. The research presented here is intended to be a step towards this vision. We describe a computational simulation of nanoscale machines floating in a virtual liquid. The machines can bond together to form strands (chains) that self-replicate and self-assemble into user-specified meshes. There are four types of machines and the sequence of machine types in a strand determines the shape of the mesh they will build. A strand may be in an unfolded state, in which the bonds are straight, or in a folded state, in which the bond angles depend on the types of machines. By choosing the sequence of machine types in a strand, the user can specify a variety of polygonal shapes. A simulation typically begins with an initial unfolded seed strand in a soup of unbonded machines. The seed strand replicates by bonding with free machines in the soup. The child strands fold into the encoded polygonal shape, and then the polygons drift together and bond to form a mesh. We demonstrate that a variety of polygonal meshes can be manufactured in the simulation, by simply changing the sequence of machine types in the seed.",
        "published": "2006-07-27T17:55:16Z",
        "link": "http://arxiv.org/abs/cs/0607133v1",
        "categories": [
            "cs.MA",
            "cs.CE",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "Hybrid Elections Broaden Complexity-Theoretic Resistance to Control",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "Electoral control refers to attempts by an election's organizer (\"the chair\") to influence the outcome by adding/deleting/partitioning voters or candidates. The groundbreaking work of Bartholdi, Tovey, and Trick [BTT92] on (constructive) control proposes computational complexity as a means of resisting control attempts: Look for election systems where the chair's task in seeking control is itself computationally infeasible.   We introduce and study a method of combining two or more candidate-anonymous election schemes in such a way that the combined scheme possesses all the resistances to control (i.e., all the NP-hardnesses of control) possessed by any of its constituents: It combines their strengths. From this and new resistance constructions, we prove for the first time that there exists an election scheme that is resistant to all twenty standard types of electoral control.",
        "published": "2006-08-14T16:15:25Z",
        "link": "http://arxiv.org/abs/cs/0608057v2",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "How Hard Is Bribery in Elections?",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra"
        ],
        "summary": "We study the complexity of influencing elections through bribery: How computationally complex is it for an external actor to determine whether by a certain amount of bribing voters a specified candidate can be made the election's winner? We study this problem for election systems as varied as scoring protocols and Dodgson voting, and in a variety of settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters, and succinct-vs.-nonsuccinct input specification. We obtain both polynomial-time bribery algorithms and proofs of the intractability of bribery, and indeed our results show that the complexity of bribery is extremely sensitive to the setting. For example, we find settings in which bribery is NP-complete but manipulation (by voters) is in P, and we find settings in which bribing weighted voters is NP-complete but bribing voters with individual bribe thresholds is in P. For the broad class of elections (including plurality, Borda, k-approval, and veto) known as scoring protocols, we prove a dichotomy result for bribery of weighted voters: We find a simple-to-evaluate condition that classifies every case as either NP-complete or in P.",
        "published": "2006-08-19T23:24:03Z",
        "link": "http://arxiv.org/abs/cs/0608081v3",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Primitive operations for the construction and reorganization of   minimally persistent formations",
        "authors": [
            "Julien M. Hendrickx",
            "Baris Fidan",
            "Changbin Yu",
            "Brian D. O. Anderson",
            "Vincent D. Blondel"
        ],
        "summary": "In this paper, we study the construction and transformation of two-dimensional persistent graphs. Persistence is a generalization to directed graphs of the undirected notion of rigidity. In the context of moving autonomous agent formations, persistence characterizes the efficacy of a directed structure of unilateral distances constraints seeking to preserve a formation shape. Analogously to the powerful results about Henneberg sequences in minimal rigidity theory, we propose different types of directed graph operations allowing one to sequentially build any minimally persistent graph (i.e. persistent graph with a minimal number of edges for a given number of vertices), each intermediate graph being also minimally persistent. We also consider the more generic problem of obtaining one minimally persistent graph from another, which corresponds to the on-line reorganization of an autonomous agent formation. We prove that we can obtain any minimally persistent formation from any other one by a sequence of elementary local operations such that minimal persistence is preserved throughout the reorganization process.",
        "published": "2006-09-08T10:12:35Z",
        "link": "http://arxiv.org/abs/cs/0609041v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "F.A.S.T. - Floor field- and Agent-based Simulation Tool",
        "authors": [
            "Tobias Kretz",
            "Michael Schreckenberg"
        ],
        "summary": "In this paper a model of pedestrian motion is presented. As application its parameters are fitted to one run in a primary school evacuation exercise. Simulations with these parameters are compared to further runs during the same exercise.",
        "published": "2006-09-12T08:37:20Z",
        "link": "http://arxiv.org/abs/physics/0609097v1",
        "categories": [
            "physics.comp-ph",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Fame Emerges as a Result of Small Memory",
        "authors": [
            "Haluk Bingol"
        ],
        "summary": "A dynamic memory model is proposed in which an agent ``learns'' a new agent by means of recommendation. The agents can also ``remember'' and ``forget''. The memory size is decreased while the population size is kept constant. ``Fame'' emerged as a few agents become very well known in expense of the majority being completely forgotten. The minimum and the maximum of fame change linearly with the relative memory size. The network properties of the who-knows-who graph, which represents the state of the system, are investigated.",
        "published": "2006-09-12T21:47:21Z",
        "link": "http://arxiv.org/abs/nlin/0609033v2",
        "categories": [
            "nlin.AO",
            "cs.CY",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "A Richer Understanding of the Complexity of Election Systems",
        "authors": [
            "Piotr Faliszewski",
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "We provide an overview of some recent progress on the complexity of election systems. The issues studied include the complexity of the winner, manipulation, bribery, and control problems.",
        "published": "2006-09-19T22:57:36Z",
        "link": "http://arxiv.org/abs/cs/0609112v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Rule-based Knowledge Representation for Service Level Agreement",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Automated management and monitoring of service contracts like Service Level Agreements (SLAs) or higher-level policies is vital for efficient and reliable distributed service-oriented architectures (SOA) with high quality of ser-vice (QoS) levels. IT service provider need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technol-ogy. I propose a novel rule-based knowledge representation (KR) for SLA rules and a respective rule-based service level management (RBSLM) framework. My rule-based approach based on logic programming provides several advantages including automated rule chaining allowing for compact knowledge representation and high levels of automation as well as flexibility to adapt to rapidly changing business requirements. Therewith, I address an urgent need service-oriented busi-nesses do have nowadays which is to dynamically change their business and contractual logic in order to adapt to rapidly changing business environments and to overcome the restricting nature of slow change cycles.",
        "published": "2006-09-21T12:04:33Z",
        "link": "http://arxiv.org/abs/cs/0609120v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.LO",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "Semantic results for ontic and epistemic change",
        "authors": [
            "H. P. van Ditmarsch",
            "B. P. Kooi"
        ],
        "summary": "We give some semantic results for an epistemic logic incorporating dynamic operators to describe information changing events. Such events include epistemic changes, where agents become more informed about the non-changing state of the world, and ontic changes, wherein the world changes. The events are executed in information states that are modeled as pointed Kripke models. Our contribution consists of three semantic results. (i) Given two information states, there is an event transforming one into the other. The linguistic correspondent to this is that every consistent formula can be made true in every information state by the execution of an event. (ii) A more technical result is that: every event corresponds to an event in which the postconditions formalizing ontic change are assignments to `true' and `false' only (instead of assignments to arbitrary formulas in the logical language). `Corresponds' means that execution of either event in a given information state results in bisimilar information states. (iii) The third, also technical, result is that every event corresponds to a sequence of events wherein all postconditions are assignments of a single atom only (instead of simultaneous assignments of more than one atom).",
        "published": "2006-10-15T04:48:48Z",
        "link": "http://arxiv.org/abs/cs/0610093v4",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Characterizing Solution Concepts in Games Using Knowledge-Based Programs",
        "authors": [
            "Joseph Y. Halpern",
            "Yoram Moses"
        ],
        "summary": "We show how solution concepts in games such as Nash equilibrium, correlated equilibrium, rationalizability, and sequential equilibrium can be given a uniform definition in terms of \\emph{knowledge-based programs}. Intuitively, all solution concepts are implementations of two knowledge-based programs, one appropriate for games represented in normal form, the other for games represented in extensive form. These knowledge-based programs can be viewed as embodying rationality. The representation works even if (a) information sets do not capture an agent's knowledge, (b) uncertainty is not represented by probability, or (c) the underlying game is not common knowledge.",
        "published": "2006-10-16T13:00:33Z",
        "link": "http://arxiv.org/abs/cs/0610098v1",
        "categories": [
            "cs.GT",
            "cs.DC",
            "cs.MA"
        ]
    },
    {
        "title": "CHAC. A MOACO Algorithm for Computation of Bi-Criteria Military Unit   Path in the Battlefield",
        "authors": [
            "A. M. Mora",
            "J. J. Merelo",
            "C. Millan",
            "J. Torrecillas",
            "J. L. J. Laredo"
        ],
        "summary": "In this paper we propose a Multi-Objective Ant Colony Optimization (MOACO) algorithm called CHAC, which has been designed to solve the problem of finding the path on a map (corresponding to a simulated battlefield) that minimizes resources while maximizing safety. CHAC has been tested with two different state transition rules: an aggregative function that combines the heuristic and pheromone information of both objectives and a second one that is based on the dominance concept of multiobjective optimization problems. These rules have been evaluated in several different situations (maps with different degree of difficulty), and we have found that they yield better results than a greedy algorithm (taken as baseline) in addition to a military behaviour that is also better in the tactical sense. The aggregative function, in general, yields better results than the one based on dominance.",
        "published": "2006-10-19T10:41:16Z",
        "link": "http://arxiv.org/abs/cs/0610113v1",
        "categories": [
            "cs.MA",
            "cs.CC"
        ]
    },
    {
        "title": "Community Detection in Complex Networks Using Agents",
        "authors": [
            "Ismail Gunes",
            "Haluk Bingol"
        ],
        "summary": "Community structure identification has been one of the most popular research areas in recent years due to its applicability to the wide scale of disciplines. To detect communities in varied topics, there have been many algorithms proposed so far. However, most of them still have some drawbacks to be addressed. In this paper, we present an agent-based based community detection algorithm. The algorithm that is a stochastic one makes use of agents by forcing them to perform biased moves in a smart way. Using the information collected by the traverses of these agents in the network, the network structure is revealed. Also, the network modularity is used for determining the number of communities. Our algorithm removes the need for prior knowledge about the network such as number of the communities or any threshold values. Furthermore, the definite community structure is provided as a result instead of giving some structures requiring further processes. Besides, the computational and time costs are optimized because of using thread like working agents. The algorithm is tested on three network data of different types and sizes named Zachary karate club, college football and political books. For all three networks, the real network structures are identified in almost every run.",
        "published": "2006-10-23T06:34:10Z",
        "link": "http://arxiv.org/abs/cs/0610129v1",
        "categories": [
            "cs.MA",
            "cs.CY",
            "I.2.11"
        ]
    },
    {
        "title": "ECA-RuleML: An Approach combining ECA Rules with temporal interval-based   KR Event/Action Logics and Transactional Update Logics",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "An important problem to be addressed within Event-Driven Architecture (EDA) is how to correctly and efficiently capture and process the event/action-based logic. This paper endeavors to bridge the gap between the Knowledge Representation (KR) approaches based on durable events/actions and such formalisms as event calculus, on one hand, and event-condition-action (ECA) reaction rules extending the approach of active databases that view events as instantaneous occurrences and/or sequences of events, on the other. We propose formalism based on reaction rules (ECA rules) and a novel interval-based event logic and present concrete RuleML-based syntax, semantics and implementation. We further evaluate this approach theoretically, experimentally and on an example derived from common industry use cases and illustrate its benefits.",
        "published": "2006-10-30T11:56:08Z",
        "link": "http://arxiv.org/abs/cs/0610167v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.MA",
            "cs.SE",
            "I.2; H.2.4; I.2.5; I.2.4; K.6.3"
        ]
    },
    {
        "title": "Distributed Control of Microscopic Robots in Biomedical Applications",
        "authors": [
            "Tad Hogg"
        ],
        "summary": "Current developments in molecular electronics, motors and chemical sensors could enable constructing large numbers of devices able to sense, compute and act in micron-scale environments. Such microscopic machines, of sizes comparable to bacteria, could simultaneously monitor entire populations of cells individually in vivo. This paper reviews plausible capabilities for microscopic robots and the physical constraints due to operation in fluids at low Reynolds number, diffusion-limited sensing and thermal noise from Brownian motion. Simple distributed controls are then presented in the context of prototypical biomedical tasks, which require control decisions on millisecond time scales. The resulting behaviors illustrate trade-offs among speed, accuracy and resource use. A specific example is monitoring for patterns of chemicals in a flowing fluid released at chemically distinctive sites. Information collected from a large number of such devices allows estimating properties of cell-sized chemical sources in a macroscopic volume. The microscopic devices moving with the fluid flow in small blood vessels can detect chemicals released by tissues in response to localized injury or infection. We find the devices can readily discriminate a single cell-sized chemical source from the background chemical concentration, providing high-resolution sensing in both time and space. By contrast, such a source would be difficult to distinguish from background when diluted throughout the blood volume as obtained with a blood sample.",
        "published": "2006-11-21T23:22:20Z",
        "link": "http://arxiv.org/abs/cs/0611111v1",
        "categories": [
            "cs.RO",
            "cs.MA",
            "I.2.9; I.2.11"
        ]
    },
    {
        "title": "Why the Maxwellian Distribution is the Attractive Fixed Point of the   Boltzmann Equation",
        "authors": [
            "Ricardo Lopez-Ruiz",
            "Xavier Calbet"
        ],
        "summary": "The origin of the Boltzmann factor is revisited. An alternative derivation from the microcanonical picture is given. The Maxwellian distribution in a mono-dimensional ideal gas is obtained by following this insight. Other possible applications, as for instance the obtaining of the wealth distribution in the human society, are suggested in the remarks.",
        "published": "2006-11-23T12:27:14Z",
        "link": "http://arxiv.org/abs/nlin/0611044v1",
        "categories": [
            "nlin.CD",
            "cond-mat.stat-mech",
            "cs.MA",
            "math.ST",
            "physics.class-ph",
            "stat.TH"
        ]
    },
    {
        "title": "Non-equilibrium phase transition in negotiation dynamics",
        "authors": [
            "A. Baronchelli",
            "L. Dall'Asta",
            "A. Barrat",
            "V. Loreto"
        ],
        "summary": "We introduce a model of negotiation dynamics whose aim is that of mimicking the mechanisms leading to opinion and convention formation in a population of individuals. The negotiation process, as opposed to ``herding-like'' or ``bounded confidence'' driven processes, is based on a microscopic dynamics where memory and feedback play a central role. Our model displays a non-equilibrium phase transition from an absorbing state in which all agents reach a consensus to an active stationary state characterized either by polarization or fragmentation in clusters of agents with different opinions. We show the exystence of at least two different universality classes, one for the case with two possible opinions and one for the case with an unlimited number of opinions. The phase transition is studied analytically and numerically for various topologies of the agents' interaction network. In both cases the universality classes do not seem to depend on the specific interaction topology, the only relevant feature being the total number of different opinions ever present in the system.",
        "published": "2006-11-28T17:21:13Z",
        "link": "http://arxiv.org/abs/cond-mat/0611717v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.MA",
            "physics.soc-ph",
            "q-bio.PE"
        ]
    },
    {
        "title": "Geographic Gossip on Geometric Random Graphs via Affine Combinations",
        "authors": [
            "Hariharan Narayanan"
        ],
        "summary": "In recent times, a considerable amount of work has been devoted to the development and analysis of gossip algorithms in Geometric Random Graphs. In a recently introduced model termed \"Geographic Gossip,\" each node is aware of its position but possesses no further information. Traditionally, gossip protocols have always used convex linear combinations to achieve averaging. We develop a new protocol for Geographic Gossip, in which counter-intuitively, we use {\\it non-convex affine combinations} as updates in addition to convex combinations to accelerate the averaging process. The dependence of the number of transmissions used by our algorithm on the number of sensors $n$ is $n \\exp(O(\\log \\log n)^2) = n^{1 + o(1)}$. For the previous algorithm, this dependence was $\\tilde{O}(n^{1.5})$. The exponent 1+ o(1) of our algorithm is asymptotically optimal. Our algorithm involves a hierarchical structure of $\\log \\log n$ depth and is not completely decentralized. However, the extent of control exercised by a sensor on another is restricted to switching the other on or off.",
        "published": "2006-12-04T03:20:25Z",
        "link": "http://arxiv.org/abs/cs/0612012v1",
        "categories": [
            "cs.MA",
            "cs.IT",
            "math.IT",
            "C.2.2"
        ]
    },
    {
        "title": "Going Stupid with EcoLab",
        "authors": [
            "Russell K. Standish"
        ],
        "summary": "In 2005, Railsback et al. proposed a very simple model ({\\em Stupid   Model}) that could be implemented within a couple of hours, and later extended to demonstrate the use of common ABM platform functionality. They provided implementations of the model in several agent based modelling platforms, and compared the platforms for ease of implementation of this simple model, and performance. In this paper, I implement Railsback et al's Stupid Model in the EcoLab simulation platform, a C++ based modelling platform, demonstrating that it is a feasible platform for these sorts of models, and compare the performance of the implementation with Repast, Mason and Swarm versions.",
        "published": "2006-12-04T11:02:28Z",
        "link": "http://arxiv.org/abs/cs/0612014v1",
        "categories": [
            "cs.MA",
            "I.6.7"
        ]
    },
    {
        "title": "Analyzing language development from a network approach",
        "authors": [
            "J-Y Ke",
            "Y. Yao"
        ],
        "summary": "In this paper we propose some new measures of language development using network analyses, which is inspired by the recent surge of interests in network studies of many real-world systems. Children's and care-takers' speech data from a longitudinal study are represented as a series of networks, word forms being taken as nodes and collocation of words as links. Measures on the properties of the networks, such as size, connectivity, hub and authority analyses, etc., allow us to make quantitative comparison so as to reveal different paths of development. For example, the asynchrony of development in network size and average degree suggests that children cannot be simply classified as early talkers or late talkers by one or two measures. Children follow different paths in a multi-dimensional space. They may develop faster in one dimension but slower in another dimension. The network approach requires little preprocessing of words and analyses on sentence structures, and the characteristics of words and their usage emerge from the network and are independent of any grammatical presumptions. We show that the change of the two articles \"the\" and \"a\" in their roles as important nodes in the network reflects the progress of children's syntactic development: the two articles often start in children's networks as hubs and later shift to authorities, while they are authorities constantly in the adult's networks. The network analyses provide a new approach to study language development, and at the same time language development also presents a rich area for network theories to explore.",
        "published": "2006-01-04T18:33:53Z",
        "link": "http://arxiv.org/abs/cs/0601005v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Constraint-based verification of abstract models of multitreaded   programs",
        "authors": [
            "Giorgio Delzanno"
        ],
        "summary": "We present a technique for the automated verification of abstract models of multithreaded programs providing fresh name generation, name mobility, and unbounded control.   As high level specification language we adopt here an extension of communication finite-state machines with local variables ranging over an infinite name domain, called TDL programs. Communication machines have been proved very effective for representing communication protocols as well as for representing abstractions of multithreaded software.   The verification method that we propose is based on the encoding of TDL programs into a low level language based on multiset rewriting and constraints that can be viewed as an extension of Petri Nets. By means of this encoding, the symbolic verification procedure developed for the low level language in our previous work can now be applied to TDL programs. Furthermore, the encoding allows us to isolate a decidable class of verification problems for TDL programs that still provide fresh name generation, name mobility, and unbounded control. Our syntactic restrictions are in fact defined on the internal structure of threads: In order to obtain a complete and terminating method, threads are only allowed to have at most one local variable (ranging over an infinite domain of names).",
        "published": "2006-01-10T12:57:46Z",
        "link": "http://arxiv.org/abs/cs/0601037v1",
        "categories": [
            "cs.CL",
            "cs.PL"
        ]
    },
    {
        "title": "PageRank without hyperlinks: Structural re-ranking using links induced   by language models",
        "authors": [
            "Oren Kurland",
            "Lillian Lee"
        ],
        "summary": "Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural re-ranking approach to ad hoc information retrieval: we reorder the documents in an initially retrieved set by exploiting asymmetric relationships between them. Specifically, we consider generation links, which indicate that the language model induced from one document assigns high probability to the text of another; in doing so, we take care to prevent bias against long documents. We study a number of re-ranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks.",
        "published": "2006-01-11T21:27:28Z",
        "link": "http://arxiv.org/abs/cs/0601045v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Better than the real thing? Iterative pseudo-query processing using   cluster-based language models",
        "authors": [
            "Oren Kurland",
            "Lillian Lee",
            "Carmel Domshlak"
        ],
        "summary": "We present a novel approach to pseudo-feedback-based ad hoc retrieval that uses language models induced from both documents and clusters. First, we treat the pseudo-feedback documents produced in response to the original query as a set of pseudo-queries that themselves can serve as input to the retrieval process. Observing that the documents returned in response to the pseudo-queries can then act as pseudo-queries for subsequent rounds, we arrive at a formulation of pseudo-query-based retrieval as an iterative process. Experiments show that several concrete instantiations of this idea, when applied in conjunction with techniques designed to heighten precision, yield performance results rivaling those of a number of previously-proposed algorithms, including the standard language-modeling approach. The use of cluster-based language models is a key contributing factor to our algorithms' success.",
        "published": "2006-01-11T21:47:44Z",
        "link": "http://arxiv.org/abs/cs/0601046v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Improving the CSIEC Project and Adapting It to the English Teaching and   Learning in China",
        "authors": [
            "Jiyou Jia",
            "Shufen Hou",
            "Weichao Chen"
        ],
        "summary": "In this paper after short review of the CSIEC project initialized by us in 2003 we present the continuing development and improvement of the CSIEC project in details, including the design of five new Microsoft agent characters representing different virtual chatting partners and the limitation of simulated dialogs in specific practical scenarios like graduate job application interview, then briefly analyze the actual conditions and features of its application field: web-based English education in China. Finally we introduce our efforts to adapt this system to the requirements of English teaching and learning in China and point out the work next to do.",
        "published": "2006-02-06T15:17:34Z",
        "link": "http://arxiv.org/abs/cs/0602018v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.MA",
            "K.3.1; I.2.7; I.2.11"
        ]
    },
    {
        "title": "Rational stochastic languages",
        "authors": [
            "François Denis",
            "Yann Esposito"
        ],
        "summary": "The goal of the present paper is to provide a systematic and comprehensive study of rational stochastic languages over a semiring K \\in {Q, Q +, R, R+}. A rational stochastic language is a probability distribution over a free monoid \\Sigma^* which is rational over K, that is which can be generated by a multiplicity automata with parameters in K. We study the relations between the classes of rational stochastic languages S rat K (\\Sigma). We define the notion of residual of a stochastic language and we use it to investigate properties of several subclasses of rational stochastic languages. Lastly, we study the representation of rational stochastic languages by means of multiplicity automata.",
        "published": "2006-02-27T10:08:26Z",
        "link": "http://arxiv.org/abs/cs/0602093v1",
        "categories": [
            "cs.LG",
            "cs.CL"
        ]
    },
    {
        "title": "Unification of multi-lingual scientific terminological resources using   the ISO 16642 standard. The TermSciences initiative",
        "authors": [
            "Majid Khayari",
            "Stéphane Schneider",
            "Isabelle Kramer",
            "Laurent Romary",
            "the termsciences Collaboration"
        ],
        "summary": "This paper presents the TermSciences portal, which deals with the implementation of a conceptual model that uses the recent ISO 16642 standard (Terminological Markup Framework). This standard turns out to be suitable for concept modeling since it allowed for organizing the original resources by concepts and to associate the various terms for a given concept. Additional structuring is produced by sharing conceptual relationships, that is, cross-linking of resource results through the introduction of semantic relations which may have initially be missing.",
        "published": "2006-04-07T13:10:30Z",
        "link": "http://arxiv.org/abs/cs/0604027v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Probabilistic Automata for Computing with Words",
        "authors": [
            "Yongzhi Cao",
            "Lirong Xia",
            "Mingsheng Ying"
        ],
        "summary": "Usually, probabilistic automata and probabilistic grammars have crisp symbols as inputs, which can be viewed as the formal models of computing with values. In this paper, we first introduce probabilistic automata and probabilistic grammars for computing with (some special) words in a probabilistic framework, where the words are interpreted as probabilistic distributions or possibility distributions over a set of crisp symbols. By probabilistic conditioning, we then establish a retraction principle from computing with words to computing with values for handling crisp inputs and a generalized extension principle from computing with words to computing with all words for handling arbitrary inputs. These principles show that computing with values and computing with all words can be respectively implemented by computing with some special words. To compare the transition probabilities of two near inputs, we also examine some analytical properties of the transition probability functions of generalized extensions. Moreover, the retractions and the generalized extensions are shown to be equivalence-preserving. Finally, we clarify some relationships among the retractions, the generalized extensions, and the extensions studied recently by Qiu and Wang.",
        "published": "2006-04-23T02:58:51Z",
        "link": "http://arxiv.org/abs/cs/0604087v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Numeration-automatic sequences",
        "authors": [
            "J. F. J. Laros"
        ],
        "summary": "We present a base class of automata that induce a numeration system and we give an algorithm to give the n-th word in the language of the automaton when the expansion of n in the induced numeration system is feeded to the automaton. Furthermore we give some algorithms for reverse reading of this expansion and a way to combine automata to other automata having the same properties.",
        "published": "2006-05-17T17:12:25Z",
        "link": "http://arxiv.org/abs/cs/0605076v1",
        "categories": [
            "cs.CL",
            "cs.DM"
        ]
    },
    {
        "title": "Modeling the Dynamics of Social Networks",
        "authors": [
            "Victor V. Kryssanov",
            "Frank J. Rinaldo",
            "Evgeny L. Kuleshov",
            "Hitoshi Ogawa"
        ],
        "summary": "Modeling human dynamics responsible for the formation and evolution of the so-called social networks - structures comprised of individuals or organizations and indicating connectivities existing in a community - is a topic recently attracting a significant research interest. It has been claimed that these dynamics are scale-free in many practically important cases, such as impersonal and personal communication, auctioning in a market, accessing sites on the WWW, etc., and that human response times thus conform to the power law. While a certain amount of progress has recently been achieved in predicting the general response rate of a human population, existing formal theories of human behavior can hardly be found satisfactory to accommodate and comprehensively explain the scaling observed in social networks. In the presented study, a novel system-theoretic modeling approach is proposed and successfully applied to determine important characteristics of a communication network and to analyze consumer behavior on the WWW.",
        "published": "2006-05-24T02:13:13Z",
        "link": "http://arxiv.org/abs/cs/0605101v1",
        "categories": [
            "cs.CY",
            "cs.CE",
            "cs.CL",
            "cs.HC",
            "cs.NI",
            "physics.data-an"
        ]
    },
    {
        "title": "Communication of Social Agents and the Digital City - A Semiotic   Perspective",
        "authors": [
            "Victor V. Kryssanov",
            "Masayuki Okabe",
            "Koh Kakusho",
            "Michihiko Minoh"
        ],
        "summary": "This paper investigates the concept of digital city. First, a functional analysis of a digital city is made in the light of the modern study of urbanism; similarities between the virtual and urban constructions are pointed out. Next, a semiotic perspective on the subject matter is elaborated, and a terminological basis is introduced to treat a digital city as a self-organizing meaning-producing system intended to support social or spatial navigation. An explicit definition of a digital city is formulated. Finally, the proposed approach is discussed, conclusions are given, and future work is outlined.",
        "published": "2006-05-25T11:54:35Z",
        "link": "http://arxiv.org/abs/cs/0605121v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Utilisation de la linguistique en reconnaissance de la parole : un   état de l'art",
        "authors": [
            "Stéphane Huet",
            "Pascale Sébillot",
            "Guillaume Gravier"
        ],
        "summary": "To transcribe speech, automatic speech recognition systems use statistical methods, particularly hidden Markov model and N-gram models. Although these techniques perform well and lead to efficient systems, they approach their maximum possibilities. It seems thus necessary, in order to outperform current results, to use additional information, especially bound to language. However, introducing such knowledge must be realized taking into account specificities of spoken language (hesitations for example) and being robust to possible misrecognized words. This document presents a state of the art of these researches, evaluating the impact of the insertion of linguistic information on the quality of the transcription.",
        "published": "2006-05-30T15:32:41Z",
        "link": "http://arxiv.org/abs/cs/0605147v1",
        "categories": [
            "cs.HC",
            "cs.CL"
        ]
    },
    {
        "title": "Foundations of Modern Language Resource Archives",
        "authors": [
            "Peter Wittenburg",
            "Daan Broeder",
            "Wolfgang Klein",
            "Stephen Levinson",
            "Laurent Romary"
        ],
        "summary": "A number of serious reasons will convince an increasing amount of researchers to store their relevant material in centers which we will call \"language resource archives\". They combine the duty of taking care of long-term preservation as well as the task to give access to their material to different user groups. Access here is meant in the sense that an active interaction with the data will be made possible to support the integration of new data, new versions or commentaries of all sort. Modern Language Resource Archives will have to adhere to a number of basic principles to fulfill all requirements and they will have to be involved in federations to create joint language resource domains making it even more simple for the researchers to access the data. This paper makes an attempt to formulate the essential pillars language resource archives have to adhere to.",
        "published": "2006-06-01T09:14:51Z",
        "link": "http://arxiv.org/abs/cs/0606006v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "From semiotics of hypermedia to physics of semiosis: A view from system   theory",
        "authors": [
            "V. V. Kryssanov",
            "K. Kakusho"
        ],
        "summary": "Given that theoretical analysis and empirical validation is fundamental to any model, whether conceptual or formal, it is surprising that these two tools of scientific discovery are so often ignored in the contemporary studies of communication. In this paper, we pursued the ideas of a) correcting and expanding the modeling approaches of linguistics, which are otherwise inapplicable (more precisely, which should not but are widely applied), to the general case of hypermedia-based communication, and b) developing techniques for empirical validation of semiotic models, which are nowadays routinely used to explore (in fact, to conjecture about) internal mechanisms of complex systems, yet on a purely speculative basis. This study thus offers two experimentally tested substantive contributions: the formal representation of communication as the mutually-orienting behavior of coupled autonomous systems, and the mathematical interpretation of the semiosis of communication, which together offer a concrete and parsimonious understanding of diverse communication phenomena.",
        "published": "2006-06-05T03:39:09Z",
        "link": "http://arxiv.org/abs/cs/0606017v1",
        "categories": [
            "cs.HC",
            "cs.CL",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Inference and Evaluation of the Multinomial Mixture Model for Text   Clustering",
        "authors": [
            "Loïs Rigouste",
            "Olivier Cappé",
            "François Yvon"
        ],
        "summary": "In this article, we investigate the use of a probabilistic model for unsupervised clustering in text collections. Unsupervised clustering has become a basic module for many intelligent text processing applications, such as information retrieval, text classification or information extraction. The model considered in this contribution consists of a mixture of multinomial distributions over the word counts, each component corresponding to a different theme. We present and contrast various estimation procedures, which apply both in supervised and unsupervised contexts. In supervised learning, this work suggests a criterion for evaluating the posterior odds of new documents which is more statistically sound than the \"naive Bayes\" approach. In an unsupervised context, we propose measures to set up a systematic evaluation framework and start with examining the Expectation-Maximization (EM) algorithm as the basic tool for inference. We discuss the importance of initialization and the influence of other features such as the smoothing strategy or the size of the vocabulary, thereby illustrating the difficulties incurred by the high dimensionality of the parameter space. We also propose a heuristic algorithm based on iterative EM with vocabulary reduction to solve this problem. Using the fact that the latent variables can be analytically integrated out, we finally show that Gibbs sampling algorithm is tractable and compares favorably to the basic expectation maximization approach.",
        "published": "2006-06-14T14:44:06Z",
        "link": "http://arxiv.org/abs/cs/0606069v1",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "Building a resource for studying translation shifts",
        "authors": [
            "Lea Cyrus"
        ],
        "summary": "This paper describes an interdisciplinary approach which brings together the fields of corpus linguistics and translation studies. It presents ongoing work on the creation of a corpus resource in which translation shifts are explicitly annotated. Translation shifts denote departures from formal correspondence between source and target text, i.e. deviations that have occurred during the translation process. A resource in which such shifts are annotated in a systematic way will make it possible to study those phenomena that need to be addressed if machine translation output is to resemble human translation. The resource described in this paper contains English source texts (parliamentary proceedings) and their German translations. The shift annotation is based on predicate-argument structures and proceeds in two steps: first, predicates and their arguments are annotated monolingually in a straightforward manner. Then, the corresponding English and German predicates and arguments are aligned with each other. Whenever a shift - mainly grammatical or semantic -has occurred, the alignment is tagged accordingly.",
        "published": "2006-06-22T13:26:52Z",
        "link": "http://arxiv.org/abs/cs/0606096v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Adapting a general parser to a sublanguage",
        "authors": [
            "Sophie Aubin",
            "Adeline Nazarenko",
            "Claire Nédellec"
        ],
        "summary": "In this paper, we propose a method to adapt a general parser (Link Parser) to sublanguages, focusing on the parsing of texts in biology. Our main proposal is the use of terminology (identication and analysis of terms) in order to reduce the complexity of the text to be parsed. Several other strategies are explored and finally combined among which text normalization, lexicon and morpho-guessing module extensions and grammar rules adaptation. We compare the parsing results before and after these adaptations.",
        "published": "2006-06-28T14:43:42Z",
        "link": "http://arxiv.org/abs/cs/0606118v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.4"
        ]
    },
    {
        "title": "Lexical Adaptation of Link Grammar to the Biomedical Sublanguage: a   Comparative Evaluation of Three Approaches",
        "authors": [
            "Sampo Pyysalo",
            "Tapio Salakoski",
            "Sophie Aubin",
            "Adeline Nazarenko"
        ],
        "summary": "We study the adaptation of Link Grammar Parser to the biomedical sublanguage with a focus on domain terms not found in a general parser lexicon. Using two biomedical corpora, we implement and evaluate three approaches to addressing unknown words: automatic lexicon expansion, the use of morphological clues, and disambiguation using a part-of-speech tagger. We evaluate each approach separately for its effect on parsing performance and consider combinations of these approaches. In addition to a 45% increase in parsing efficiency, we find that the best approach, incorporating information from a domain part-of-speech tagger, offers a statistically signicant 10% relative decrease in error. The adapted parser is available under an open-source license at http://www.it.utu.fi/biolg.",
        "published": "2006-06-28T14:44:48Z",
        "link": "http://arxiv.org/abs/cs/0606119v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.4"
        ]
    },
    {
        "title": "Raisonner avec des diagrammes : perspectives cognitives et   computationnelles",
        "authors": [
            "Catherine Recanati"
        ],
        "summary": "Diagrammatic, analogical or iconic representations are often contrasted with linguistic or logical representations, in which the shape of the symbols is arbitrary. The aim of this paper is to make a case for the usefulness of diagrams in inferential knowledge representation systems. Although commonly used, diagrams have for a long time suffered from the reputation of being only a heuristic tool or a mere support for intuition. The first part of this paper is an historical background paying tribute to the logicians, psychologists and computer scientists who put an end to this formal prejudice against diagrams. The second part is a discussion of their characteristics as opposed to those of linguistic forms. The last part is aimed at reviving the interest for heterogeneous representation systems including both linguistic and diagrammatic representations.",
        "published": "2006-07-11T19:13:25Z",
        "link": "http://arxiv.org/abs/cs/0607051v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Dealing with Metonymic Readings of Named Entities",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "The aim of this paper is to propose a method for tagging named entities (NE), using natural language processing techniques. Beyond their literal meaning, named entities are frequently subject to metonymy. We show the limits of current NE type hierarchies and detail a new proposal aiming at dynamically capturing the semantics of entities in context. This model can analyze complex linguistic phenomena like metonymy, which are known to be difficult for natural language processing but crucial for most applications. We present an implementation and some test using the French ESTER corpus and give significant results.",
        "published": "2006-07-11T19:14:49Z",
        "link": "http://arxiv.org/abs/cs/0607052v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Linguistically Grounded Models of Language Change",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "Questions related to the evolution of language have recently known an impressive increase of interest (Briscoe, 2002). This short paper aims at questioning the scientific status of these models and their relations to attested data. We show that one cannot directly model non-linguistic factors (exogenous factors) even if they play a crucial role in language evolution. We then examine the relation between linguistic models and attested language data, as well as their contribution to cognitive linguistics.",
        "published": "2006-07-11T19:17:13Z",
        "link": "http://arxiv.org/abs/cs/0607053v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Get out the vote: Determining support or opposition from Congressional   floor-debate transcripts",
        "authors": [
            "Matt Thomas",
            "Bo Pang",
            "Lillian Lee"
        ],
        "summary": "We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation.",
        "published": "2006-07-12T20:03:52Z",
        "link": "http://arxiv.org/abs/cs/0607062v3",
        "categories": [
            "cs.CL",
            "cs.SI",
            "physics.soc-ph",
            "I.2.7"
        ]
    },
    {
        "title": "Using Answer Set Programming in an Inference-Based approach to Natural   Language Semantics",
        "authors": [
            "Farid Nouioua",
            "Pascal Nicolas"
        ],
        "summary": "Using Answer Set Programming in an Inference-Based approach to Natural Language Semantics",
        "published": "2006-07-18T07:43:07Z",
        "link": "http://arxiv.org/abs/cs/0607088v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "Expressing Implicit Semantic Relations without Supervision",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations. For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns <P1,...,Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y. For example, given X=ostrich and Y=bird, the two highest ranking output patterns are \"X is the largest Y\" and \"Y such as the X\". The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks. The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X:Y is the expected relational similarity between the given pair and typical pairs for Pi. The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs. On both tasks, the algorithm achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf.",
        "published": "2006-07-27T18:23:45Z",
        "link": "http://arxiv.org/abs/cs/0607120v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Similarity of Semantic Relations",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.",
        "published": "2006-08-25T14:35:11Z",
        "link": "http://arxiv.org/abs/cs/0608100v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Improving Term Extraction with Terminological Resources",
        "authors": [
            "Sophie Aubin",
            "Thierry Hamon"
        ],
        "summary": "Studies of different term extractors on a corpus of the biomedical domain revealed decreasing performances when applied to highly technical texts. The difficulty or impossibility of customising them to new domains is an additional limitation. In this paper, we propose to use external terminologies to influence generic linguistic data in order to augment the quality of the extraction. The tool we implemented exploits testified terms at different steps of the process: chunking, parsing and extraction of term candidates. Experiments reported here show that, using this method, more term candidates can be acquired with a higher level of reliability. We further describe the extraction process involving endogenous disambiguation implemented in the term extractor YaTeA.",
        "published": "2006-09-06T11:41:27Z",
        "link": "http://arxiv.org/abs/cs/0609019v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Challenging the principle of compositionality in interpreting natural   language texts",
        "authors": [
            "Françoise Gayral",
            "Daniel Kayser",
            "François Lévy"
        ],
        "summary": "The paper aims at emphasizing that, even relaxed, the hypothesis of compositionality has to face many problems when used for interpreting natural language texts. Rather than fixing these problems within the compositional framework, we believe that a more radical change is necessary, and propose another approach.",
        "published": "2006-09-08T14:01:57Z",
        "link": "http://arxiv.org/abs/cs/0609043v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "The role of time in considering collections",
        "authors": [
            "Françoise Gayral",
            "Daniel Kayser",
            "François Lévy"
        ],
        "summary": "The paper concerns the understanding of plurals in the framework of Artificial Intelligence and emphasizes the role of time. The construction of collection(s) and their evolution across time is often crucial and has to be accounted for. The paper contrasts a \"de dicto\" collection where the collection can be considered as persisting over these situations even if its members change with a \"de re\" collection whose composition does not vary through time. It expresses different criteria of choice between the two interpretations (de re and de dicto) depending on the context of enunciation.",
        "published": "2006-09-08T14:11:50Z",
        "link": "http://arxiv.org/abs/cs/0609044v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Multilingual person name recognition and transliteration",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat",
            "Irina Temnikova",
            "Anna Widiger",
            "Wajdi Zaghouani",
            "Jan Zizka"
        ],
        "summary": "We present an exploratory tool that extracts person names from multilingual news collections, matches name variants referring to the same person, and infers relationships between people based on the co-occurrence of their names in related news. A novel feature is the matching of name variants across languages and writing systems, including names written with the Greek, Cyrillic and Arabic writing system. Due to our highly multilingual setting, we use an internal standard representation for name representation and matching, instead of adopting the traditional bilingual approach to transliteration. This work is part of the news analysis system NewsExplorer that clusters an average of 25,000 news articles per day to detect related news within the same and across different languages.",
        "published": "2006-09-11T11:08:23Z",
        "link": "http://arxiv.org/abs/cs/0609051v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.5"
        ]
    },
    {
        "title": "Navigating multilingual news collections using automatically extracted   information",
        "authors": [
            "Ralf Steinberger",
            "Bruno Pouliquen",
            "Camelia Ignat"
        ],
        "summary": "We are presenting a text analysis tool set that allows analysts in various fields to sieve through large collections of multilingual news items quickly and to find information that is of relevance to them. For a given document collection, the tool set automatically clusters the texts into groups of similar articles, extracts names of places, people and organisations, lists the user-defined specialist terms found, links clusters and entities, and generates hyperlinks. Through its daily news analysis operating on thousands of articles per day, the tool also learns relationships between people and other entities. The fully functional prototype system allows users to explore and navigate multilingual document collections across languages and time.",
        "published": "2006-09-11T11:54:41Z",
        "link": "http://arxiv.org/abs/cs/0609053v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.5"
        ]
    },
    {
        "title": "The JRC-Acquis: A multilingual aligned parallel corpus with 20+   languages",
        "authors": [
            "Ralf Steinberger",
            "Bruno Pouliquen",
            "Anna Widiger",
            "Camelia Ignat",
            "Tomaz Erjavec",
            "Dan Tufis",
            "Daniel Varga"
        ],
        "summary": "We present a new, unique and freely available parallel corpus containing European Union (EU) documents of mostly legal nature. It is available in all 20 official EUanguages, with additional documents being available in the languages of the EU candidate countries. The corpus consists of almost 8,000 documents per language, with an average size of nearly 9 million words per language. Pair-wise paragraph alignment information produced by two different aligners (Vanilla and HunAlign) is available for all 190+ language pair combinations. Most texts have been manually classified according to the EUROVOC subject domains so that the collection can also be used to train and test multi-label classification algorithms and keyword-assignment software. The corpus is encoded in XML, according to the Text Encoding Initiative Guidelines. Due to the large number of parallel texts in many languages, the JRC-Acquis is particularly suitable to carry out all types of cross-language research, as well as to test and benchmark text analysis software across different languages (for instance for alignment, sentence splitting and term extraction).",
        "published": "2006-09-12T07:10:15Z",
        "link": "http://arxiv.org/abs/cs/0609058v1",
        "categories": [
            "cs.CL",
            "H.3.1; H.3.6"
        ]
    },
    {
        "title": "Automatic annotation of multilingual text collections with a conceptual   thesaurus",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat"
        ],
        "summary": "Automatic annotation of documents with controlled vocabulary terms (descriptors) from a conceptual thesaurus is not only useful for document indexing and retrieval. The mapping of texts onto the same thesaurus furthermore allows to establish links between similar documents. This is also a substantial requirement of the Semantic Web. This paper presents an almost language-independent system that maps documents written in different languages onto the same multilingual conceptual thesaurus, EUROVOC. Conceptual thesauri differ from Natural Language Thesauri in that they consist of relatively small controlled lists of words or phrases with a rather abstract meaning. To automatically identify which thesaurus descriptors describe the contents of a document best, we developed a statistical, associative system that is trained on texts that have previously been indexed manually. In addition to describing the large number of empirically optimised parameters of the fully functional application, we present the performance of the software according to a human evaluation by professional indexers.",
        "published": "2006-09-12T07:24:01Z",
        "link": "http://arxiv.org/abs/cs/0609059v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.6"
        ]
    },
    {
        "title": "Automatic Identification of Document Translations in Large Multilingual   Document Collections",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat"
        ],
        "summary": "Texts and their translations are a rich linguistic resource that can be used to train and test statistics-based Machine Translation systems and many other applications. In this paper, we present a working system that can identify translations and other very similar documents among a large number of candidates, by representing the document contents with a vector of thesaurus terms from a multilingual thesaurus, and by then measuring the semantic similarity between the vectors. Tests on different text types have shown that the system can detect translations with over 96% precision in a large search space of 820 documents or more. The system was tuned to ignore language-specific similarities and to give similar documents in a second language the same similarity score as equivalent documents in the same language. The application can also be used to detect cross-lingual document plagiarism.",
        "published": "2006-09-12T08:44:53Z",
        "link": "http://arxiv.org/abs/cs/0609060v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.6"
        ]
    },
    {
        "title": "Cross-lingual keyword assignment",
        "authors": [
            "Ralf Steinberger"
        ],
        "summary": "This paper presents a language-independent approach to controlled vocabulary keyword assignment using the EUROVOC thesaurus. Due to the multilingual nature of EUROVOC, the keywords for a document written in one language can be displayed in all eleven official European Union languages. The mapping of documents written in different languages to the same multilingual thesaurus furthermore allows cross-language document comparison. The assignment of the controlled vocabulary thesaurus descriptors is achieved by applying a statistical method that uses a collection of manually indexed documents to identify, for each thesaurus descriptor, a large number of lemmas that are statistically associated to the descriptor. These associated words are then used during the assignment procedure to identify a ranked list of those EUROVOC terms that are most likely to be good keywords for a given document. The paper also describes the challenges of this task and discusses the achieved results of the fully functional prototype.",
        "published": "2006-09-12T09:29:57Z",
        "link": "http://arxiv.org/abs/cs/0609061v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.6"
        ]
    },
    {
        "title": "Extending an Information Extraction tool set to Central and Eastern   European languages",
        "authors": [
            "Camelia Ignat",
            "Bruno Pouliquen",
            "Antonio Ribeiro",
            "Ralf Steinberger"
        ],
        "summary": "In a highly multilingual and multicultural environment such as in the European Commission with soon over twenty official languages, there is an urgent need for text analysis tools that use minimal linguistic knowledge so that they can be adapted to many languages without much human effort. We are presenting two such Information Extraction tools that have already been adapted to various Western and Eastern European languages: one for the recognition of date expressions in text, and one for the detection of geographical place names and the visualisation of the results in geographical maps. An evaluation of the performance has produced very satisfying results.",
        "published": "2006-09-12T12:29:17Z",
        "link": "http://arxiv.org/abs/cs/0609063v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.6; H.3.4"
        ]
    },
    {
        "title": "Exploiting multilingual nomenclatures and language-independent text   features as an interlingua for cross-lingual text analysis applications",
        "authors": [
            "Ralf Steinberger",
            "Bruno Pouliquen",
            "Camelia Ignat"
        ],
        "summary": "We are proposing a simple, but efficient basic approach for a number of multilingual and cross-lingual language technology applications that are not limited to the usual two or three languages, but that can be applied with relatively little effort to larger sets of languages. The approach consists of using existing multilingual linguistic resources such as thesauri, nomenclatures and gazetteers, as well as exploiting the existence of additional more or less language-independent text items such as dates, currency expressions, numbers, names and cognates. Mapping texts onto the multilingual resources and identifying word token links between texts in different languages are basic ingredients for applications such as cross-lingual document similarity calculation, multilingual clustering and categorisation, cross-lingual document retrieval, and tools to provide cross-lingual information access.",
        "published": "2006-09-12T12:43:37Z",
        "link": "http://arxiv.org/abs/cs/0609064v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "Geocoding multilingual texts: Recognition, disambiguation and   visualisation",
        "authors": [
            "Bruno Pouliquen",
            "Marco Kimler",
            "Ralf Steinberger",
            "Camelia Ignat",
            "Tamara Oellinger",
            "Ken Blackler",
            "Flavio Fuart",
            "Wajdi Zaghouani",
            "Anna Widiger",
            "Ann-Charlotte Forslund",
            "Clive Best"
        ],
        "summary": "We are presenting a method to recognise geographical references in free text. Our tool must work on various languages with a minimum of language-dependent resources, except a gazetteer. The main difficulty is to disambiguate these place names by distinguishing places from persons and by selecting the most likely place out of a list of homographic place names world-wide. The system uses a number of language-independent clues and heuristics to disambiguate place name homographs. The final aim is to index texts with the countries and cities they mention and to automatically visualise this information on geographical maps using various tools.",
        "published": "2006-09-12T12:57:38Z",
        "link": "http://arxiv.org/abs/cs/0609065v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "Building and displaying name relations using automatic unsupervised   analysis of newspaper articles",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat",
            "Tamara Oellinger"
        ],
        "summary": "We present a tool that, from automatically recognised names, tries to infer inter-person relations in order to present associated people on maps. Based on an in-house Named Entity Recognition tool, applied on clusters of an average of 15,000 news articles per day, in 15 different languages, we build a knowledge base that allows extracting statistical co-occurrences of persons and visualising them on a per-person page or in various graphs.",
        "published": "2006-09-12T13:09:37Z",
        "link": "http://arxiv.org/abs/cs/0609066v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "A tool set for the quick and efficient exploration of large document   collections",
        "authors": [
            "Camelia Ignat",
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Tomaz Erjavec"
        ],
        "summary": "We are presenting a set of multilingual text analysis tools that can help analysts in any field to explore large document collections quickly in order to determine whether the documents contain information of interest, and to find the relevant text passages. The automatic tool, which currently exists as a fully functional prototype, is expected to be particularly useful when users repeatedly have to sieve through large collections of documents such as those downloaded automatically from the internet. The proposed system takes a whole document collection as input. It first carries out some automatic analysis tasks (named entity recognition, geo-coding, clustering, term extraction), annotates the texts with the generated meta-information and stores the meta-information in a database. The system then generates a zoomable and hyperlinked geographic map enhanced with information on entities and terms found. When the system is used on a regular basis, it builds up a historical database that contains information on which names have been mentioned together with which other names or places, and users can query this database to retrieve information extracted in the past.",
        "published": "2006-09-12T13:30:01Z",
        "link": "http://arxiv.org/abs/cs/0609067v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "Rapport technique du projet OGRE",
        "authors": [
            "Gérard Bécher",
            "Patrice Enjalbert",
            "Estelle Fievé",
            "Laurent Gosselin",
            "François Lévy",
            "Gérard Ligozat"
        ],
        "summary": "This repport concerns automatic understanding of (french) iterative sentences, i.e. sentences where one single verb has to be interpreted by a more or less regular plurality of events. A linguistic analysis is proposed along an extension of Reichenbach's theory, several formal representations are considered and a corpus of 18000 newspaper extracts is described.",
        "published": "2006-10-01T17:39:44Z",
        "link": "http://arxiv.org/abs/cs/0610004v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "One-Pass, One-Hash n-Gram Statistics Estimation",
        "authors": [
            "Daniel Lemire",
            "Owen Kaser"
        ],
        "summary": "In multimedia, text or bioinformatics databases, applications query sequences of n consecutive symbols called n-grams. Estimating the number of distinct n-grams is a view-size estimation problem. While view sizes can be estimated by sampling under statistical assumptions, we desire an unassuming algorithm with universally valid accuracy bounds. Most related work has focused on repeatedly hashing the data, which is prohibitive for large data sources. We prove that a one-pass one-hash algorithm is sufficient for accurate estimates if the hashing is sufficiently independent. To reduce costs further, we investigate recursive random hashing algorithms and show that they are sufficiently independent in practice. We compare our running times with exact counts using suffix arrays and show that, while we use hardly any storage, we are an order of magnitude faster. The approach further is extended to a one-pass/one-hash computation of n-gram entropy and iceberg counts. The experiments use a large collection of English text from the Gutenberg Project as well as synthetic data.",
        "published": "2006-10-03T18:04:22Z",
        "link": "http://arxiv.org/abs/cs/0610010v4",
        "categories": [
            "cs.DB",
            "cs.CL"
        ]
    },
    {
        "title": "Norm Based Causal Reasoning in Textual Corpus",
        "authors": [
            "Farid Nouioua"
        ],
        "summary": "Truth based entailments are not sufficient for a good comprehension of NL. In fact, it can not deduce implicit information necessary to understand a text. On the other hand, norm based entailments are able to reach this goal. This idea was behind the development of Frames (Minsky 75) and Scripts (Schank 77, Schank 79) in the 70's. But these theories are not formalized enough and their adaptation to new situations is far from being obvious. In this paper, we present a reasoning system which uses norms in a causal reasoning process in order to find the cause of an accident from a text describing it.",
        "published": "2006-10-04T11:33:02Z",
        "link": "http://arxiv.org/abs/cs/0610016v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Raisonnement stratifié à base de normes pour inférer les   causes dans un corpus textuel",
        "authors": [
            "Farid Nouioua"
        ],
        "summary": "To understand texts written in natural language (LN), we use our knowledge about the norms of the domain. Norms allow to infer more implicit information from the text. This kind of information can, in general, be defeasible, but it remains useful and acceptable while the text do not contradict it explicitly. In this paper we describe a non-monotonic reasoning system based on the norms of the car crash domain. The system infers the cause of an accident from its textual description. The cause of an accident is seen as the most specific norm which has been violated. The predicates and the rules of the system are stratified: organized on layers in order to obtain an efficient reasoning.",
        "published": "2006-10-04T13:09:48Z",
        "link": "http://arxiv.org/abs/cs/0610018v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "DepAnn - An Annotation Tool for Dependency Treebanks",
        "authors": [
            "Tuomo Kakkonen"
        ],
        "summary": "DepAnn is an interactive annotation tool for dependency treebanks, providing both graphical and text-based annotation interfaces. The tool is aimed for semi-automatic creation of treebanks. It aids the manual inspection and correction of automatically created parses, making the annotation process faster and less error-prone. A novel feature of the tool is that it enables the user to view outputs from several parsers as the basis for creating the final tree to be saved to the treebank. DepAnn uses TIGER-XML, an XML-based general encoding format for both, representing the parser outputs and saving the annotated treebank. The tool includes an automatic consistency checker for sentence structures. In addition, the tool enables users to build structures manually, add comments on the annotations, modify the tagsets, and mark sentences for further revision.",
        "published": "2006-10-19T17:42:57Z",
        "link": "http://arxiv.org/abs/cs/0610116v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Applying Part-of-Seech Enhanced LSA to Automatic Essay Grading",
        "authors": [
            "Tuomo Kakkonen",
            "Niko Myller",
            "Erkki Sutinen"
        ],
        "summary": "Latent Semantic Analysis (LSA) is a widely used Information Retrieval method based on \"bag-of-words\" assumption. However, according to general conception, syntax plays a role in representing meaning of sentences. Thus, enhancing LSA with part-of-speech (POS) information to capture the context of word occurrences appears to be theoretically feasible extension. The approach is tested empirically on a automatic essay grading system using LSA for document similarity comparisons. A comparison on several POS-enhanced LSA models is reported. Our findings show that the addition of contextual information in the form of POS tags can raise the accuracy of the LSA-based scoring models up to 10.77 per cent.",
        "published": "2006-10-19T17:50:57Z",
        "link": "http://arxiv.org/abs/cs/0610118v1",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "Dependency Treebanks: Methods, Annotation Schemes and Tools",
        "authors": [
            "Tuomo Kakkonen"
        ],
        "summary": "In this paper, current dependencybased treebanks are introduced and analyzed. The methods used for building the resources, the annotation schemes applied, and the tools used (such as POS taggers, parsers and annotation software) are discussed.",
        "published": "2006-10-20T11:48:38Z",
        "link": "http://arxiv.org/abs/cs/0610124v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Un modèle générique d'organisation de corpus en ligne: application   à la FReeBank",
        "authors": [
            "Susanne Salmon-Alt",
            "Laurent Romary",
            "Jean-Marie Pierrel"
        ],
        "summary": "The few available French resources for evaluating linguistic models or algorithms on other linguistic levels than morpho-syntax are either insufficient from quantitative as well as qualitative point of view or not freely accessible. Based on this fact, the FREEBANK project intends to create French corpora constructed using manually revised output from a hybrid Constraint Grammar parser and annotated on several linguistic levels (structure, morpho-syntax, syntax, coreference), with the objective to make them available on-line for research purposes. Therefore, we will focus on using standard annotation schemes, integration of existing resources and maintenance allowing for continuous enrichment of the annotations. Prior to the actual presentation of the prototype that has been implemented, this paper describes a generic model for the organization and deployment of a linguistic resource archive, in compliance with the various works currently conducted within international standardization initiatives (TEI and ISO/TC 37/SC 4).",
        "published": "2006-11-06T14:37:27Z",
        "link": "http://arxiv.org/abs/cs/0611026v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Scaling Construction Grammar up to Production Systems: the SCIM",
        "authors": [
            "Guillaume Pitel"
        ],
        "summary": "While a great effort has concerned the development of fully integrated modular understanding systems, few researches have focused on the problem of unifying existing linguistic formalisms with cognitive processing models. The Situated Constructional Interpretation Model is one of these attempts. In this model, the notion of \"construction\" has been adapted in order to be able to mimic the behavior of Production Systems. The Construction Grammar approach establishes a model of the relations between linguistic forms and meaning, by the mean of constructions. The latter can be considered as pairings from a topologically structured space to an unstructured space, in some way a special kind of production rules.",
        "published": "2006-11-15T12:35:45Z",
        "link": "http://arxiv.org/abs/cs/0611069v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "An Anthological Review of Research Utilizing MontyLingua, a Python-Based   End-to-End Text Processor",
        "authors": [
            "Maurice HT Ling"
        ],
        "summary": "MontyLingua, an integral part of ConceptNet which is currently the largest commonsense knowledge base, is an English text processor developed using Python programming language in MIT Media Lab. The main feature of MontyLingua is the coverage for all aspects of English text processing from raw input text to semantic meanings and summary generation, yet each component in MontyLingua is loosely-coupled to each other at the architectural and code level, which enabled individual components to be used independently or substituted. However, there has been no review exploring the role of MontyLingua in recent research work utilizing it. This paper aims to review the use of and roles played by MontyLingua and its components in research work published in 19 articles between October 2004 and August 2006. We had observed a diversified use of MontyLingua in many different areas, both generic and domain-specific. Although the use of text summarizing component had not been observe, we are optimistic that it will have a crucial role in managing the current trend of information overload in future research.",
        "published": "2006-11-22T03:24:54Z",
        "link": "http://arxiv.org/abs/cs/0611113v1",
        "categories": [
            "cs.CL",
            "H.5.2; I.2.7"
        ]
    },
    {
        "title": "Next Generation Language Resources using GRID",
        "authors": [
            "Federico Calzolari",
            "Eva Sassolini",
            "Manuela Sassi",
            "Sebastiana Cucurullo",
            "Eugenio Picchi",
            "Francesca Bertagna",
            "Alessandro Enea",
            "Monica Monachini",
            "Claudia Soria",
            "Nicoletta Calzolari"
        ],
        "summary": "This paper presents a case study concerning the challenges and requirements posed by next generation language resources, realized as an overall model of open, distributed and collaborative language infrastructure. If a sort of \"new paradigm\" is required, we think that the emerging and still evolving technology connected to Grid computing is a very interesting and suitable one for a concrete realization of this vision. Given the current limitations of Grid computing, it is very important to test the new environment on basic language analysis tools, in order to get the feeling of what are the potentialities and possible limitations connected to its use in NLP. For this reason, we have done some experiments on a module of Linguistic Miner, i.e. the extraction of linguistic patterns from restricted domain corpora.",
        "published": "2006-11-29T10:49:23Z",
        "link": "http://arxiv.org/abs/cs/0611148v3",
        "categories": [
            "cs.DC",
            "cs.CL"
        ]
    },
    {
        "title": "Acronym-Meaning Extraction from Corpora Using Multi-Tape Weighted   Finite-State Machines",
        "authors": [
            "André Kempe"
        ],
        "summary": "The automatic extraction of acronyms and their meaning from corpora is an important sub-task of text mining. It can be seen as a special case of string alignment, where a text chunk is aligned with an acronym. Alternative alignments have different cost, and ideally the least costly one should give the correct meaning of the acronym. We show how this approach can be implemented by means of a 3-tape weighted finite-state machine (3-WFSM) which reads a text chunk on tape 1 and an acronym on tape 2, and generates all alternative alignments on tape 3. The 3-WFSM can be automatically generated from a simple regular expression. No additional algorithms are required at any stage. Our 3-WFSM has a size of 27 states and 64 transitions, and finds the best analysis of an acronym in a few milliseconds.",
        "published": "2006-12-06T10:13:12Z",
        "link": "http://arxiv.org/abs/cs/0612033v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "cs.SC",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Viterbi Algorithm Generalized for n-Tape Best-Path Search",
        "authors": [
            "André Kempe"
        ],
        "summary": "We present a generalization of the Viterbi algorithm for identifying the path with minimal (resp. maximal) weight in a n-tape weighted finite-state machine (n-WFSM), that accepts a given n-tuple of input strings (s_1,... s_n). It also allows us to compile the best transduction of a given input n-tuple by a weighted (n+m)-WFSM (transducer) with n input and m output tapes. Our algorithm has a worst-case time complexity of O(|s|^n |E| log (|s|^n |Q|)), where n and |s| are the number and average length of the strings in the n-tuple, and |Q| and |E| the number of states and transitions in the n-WFSM, respectively. A straight forward alternative, consisting in intersection followed by classical shortest-distance search, operates in O(|s|^n (|E|+|Q|) log (|s|^n |Q|)) time.",
        "published": "2006-12-07T08:42:46Z",
        "link": "http://arxiv.org/abs/cs/0612041v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "cs.SC",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "PageRank without hyperlinks: Structural re-ranking using links induced   by language models",
        "authors": [
            "Oren Kurland",
            "Lillian Lee"
        ],
        "summary": "Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural re-ranking approach to ad hoc information retrieval: we reorder the documents in an initially retrieved set by exploiting asymmetric relationships between them. Specifically, we consider generation links, which indicate that the language model induced from one document assigns high probability to the text of another; in doing so, we take care to prevent bias against long documents. We study a number of re-ranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks.",
        "published": "2006-01-11T21:27:28Z",
        "link": "http://arxiv.org/abs/cs/0601045v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Better than the real thing? Iterative pseudo-query processing using   cluster-based language models",
        "authors": [
            "Oren Kurland",
            "Lillian Lee",
            "Carmel Domshlak"
        ],
        "summary": "We present a novel approach to pseudo-feedback-based ad hoc retrieval that uses language models induced from both documents and clusters. First, we treat the pseudo-feedback documents produced in response to the original query as a set of pseudo-queries that themselves can serve as input to the retrieval process. Observing that the documents returned in response to the pseudo-queries can then act as pseudo-queries for subsequent rounds, we arrive at a formulation of pseudo-query-based retrieval as an iterative process. Experiments show that several concrete instantiations of this idea, when applied in conjunction with techniques designed to heighten precision, yield performance results rivaling those of a number of previously-proposed algorithms, including the standard language-modeling approach. The use of cluster-based language models is a key contributing factor to our algorithms' success.",
        "published": "2006-01-11T21:47:44Z",
        "link": "http://arxiv.org/abs/cs/0601046v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Automatic Detection of Trends in Dynamical Text: An Evolutionary   Approach",
        "authors": [
            "Lourdes Araujo",
            "Juan J. Merelo"
        ],
        "summary": "This paper presents an evolutionary algorithm for modeling the arrival dates of document streams, which is any time-stamped collection of documents, such as newscasts, e-mails, IRC conversations, scientific journals archives and weblog postings. This algorithm assigns frequencies (number of document arrivals per time unit) to time intervals so that it produces an optimal fit to the data. The optimization is a trade off between accurately fitting the data and avoiding too many frequency changes; this way the analysis is able to find fits which ignore the noise. Classical dynamic programming algorithms are limited by memory and efficiency requirements, which can be a problem when dealing with long streams. This suggests to explore alternative search methods which allow for some degree of uncertainty to achieve tractability. Experiments have shown that the designed evolutionary algorithm is able to reach the same solution quality as those classical dynamic programming algorithms in a shorter time. We have also explored different probabilistic models to optimize the fitting of the date streams, and applied these algorithms to infer whether a new arrival increases or decreases {\\em interest} in the topic the document stream is about.",
        "published": "2006-01-12T20:23:06Z",
        "link": "http://arxiv.org/abs/cs/0601047v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "An O(1) Solution to the Prefix Sum Problem on a Specialized Memory   Architecture",
        "authors": [
            "Andrej Brodnik",
            "Johan Karlsson",
            "J. Ian Munro",
            "Andreas Nilsson"
        ],
        "summary": "In this paper we study the Prefix Sum problem introduced by Fredman.   We show that it is possible to perform both update and retrieval in O(1) time simultaneously under a memory model in which individual bits may be shared by several words.   We also show that two variants (generalizations) of the problem can be solved optimally in $\\Theta(\\lg N)$ time under the comparison based model of computation.",
        "published": "2006-01-18T21:20:10Z",
        "link": "http://arxiv.org/abs/cs/0601081v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.IR",
            "E.1; F.1.1"
        ]
    },
    {
        "title": "Google Web APIs - an Instrument for Webometric Analyses?",
        "authors": [
            "Philipp Mayr",
            "Fabio Tosques"
        ],
        "summary": "This paper introduces Google Web APIs (Google APIs) as an instrument and playground for webometric studies. Several examples of Google APIs implementations are given. Our examples show that this Google Web Service can be used successfully for informetric Internet based studies albeit with some restrictions.",
        "published": "2006-01-24T10:23:15Z",
        "link": "http://arxiv.org/abs/cs/0601103v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "A Multi-Relational Network to Support the Scholarly Communication   Process",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "The general pupose of the scholarly communication process is to support the creation and dissemination of ideas within the scientific community. At a finer granularity, there exists multiple stages which, when confronted by a member of the community, have different requirements and therefore different solutions. In order to take a researcher's idea from an initial inspiration to a community resource, the scholarly communication infrastructure may be required to 1) provide a scientist initial seed ideas; 2) form a team of well suited collaborators; 3) located the most appropriate venue to publish the formalized idea; 4) determine the most appropriate peers to review the manuscript; and 5) disseminate the end product to the most interested members of the community. Through the various delinieations of this process, the requirements of each stage are tied soley to the multi-functional resources of the community: its researchers, its journals, and its manuscritps. It is within the collection of these resources and their inherent relationships that the solutions to scholarly communication are to be found. This paper describes an associative network composed of multiple scholarly artifacts that can be used as a medium for supporting the scholarly communication process.",
        "published": "2006-01-28T22:45:42Z",
        "link": "http://arxiv.org/abs/cs/0601121v2",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "The Matrix-Forest Theorem and Measuring Relations in Small Social Groups",
        "authors": [
            "Pavel Chebotarev",
            "Elena Shamis"
        ],
        "summary": "We propose a family of graph structural indices related to the Matrix-forest theorem. The properties of the basic index that expresses the mutual connectivity of two vertices are studied in detail. The derivative indices that measure \"dissociation,\" \"solitariness,\" and \"provinciality\" of vertices are also considered. A nonstandard metric on the set of vertices is introduced, which is determined by their connectivity. The application of these indices in sociometry is discussed.",
        "published": "2006-02-04T14:55:27Z",
        "link": "http://arxiv.org/abs/math/0602070v1",
        "categories": [
            "math.CO",
            "cs.IR",
            "math.AG",
            "05C50; 05C05; 15A51"
        ]
    },
    {
        "title": "eJournal interface can influence usage statistics: implications for   libraries, publishers, and Project COUNTER",
        "authors": [
            "Philip M. Davis",
            "Jason S. Price"
        ],
        "summary": "The design of a publisher's electronic interface can have a measurable effect on electronic journal usage statistics. A study of journal usage from six COUNTER-compliant publishers at thirty-two research institutions in the United States, the United Kingdom and Sweden indicates that the ratio of PDF to HTML views is not consistent across publisher interfaces, even after controlling for differences in publisher content. The number of fulltext downloads may be artificially inflated when publishers require users to view HTML versions before accessing PDF versions or when linking mechanisms, such as CrossRef, direct users to the full text, rather than the abstract, of each article. These results suggest that usage reports from COUNTER-compliant publishers are not directly comparable in their current form. One solution may be to modify publisher numbers with adjustment factors deemed to be representative of the benefit or disadvantage due to its interface. Standardization of some interface and linking protocols may obviate these differences and allow for more accurate cross-publisher comparisons.",
        "published": "2006-02-16T20:29:25Z",
        "link": "http://arxiv.org/abs/cs/0602060v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "Similarity of Objects and the Meaning of Words",
        "authors": [
            "Rudi Cilibrasi",
            "Paul Vitanyi"
        ],
        "summary": "We survey the emerging area of compression-based, parameter-free, similarity distance measures useful in data-mining, pattern recognition, learning and automatic semantics extraction. Given a family of distances on a set of objects, a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set, up to the stated precision (we do not require the universal distance to be an element of the family). We consider similarity distances for two types of objects: literal objects that as such contain all of their meaning, like genomes or books, and names for objects. The latter may have literal embodyments like the first type, but may also be abstract like ``red'' or ``christianity.'' For the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular featuresdistances generated by web users corresponding to particular semantic relations between the (names for) the designated objects. For both families we give universal similarity distance measures, incorporating all particular distance measures in the family. In the first case the universal distance is based on compression and in the second case it is based on Google page counts related to search terms. In both cases experiments on a massive scale give evidence of the viability of the approaches. between pairs of literal objects. For the second type we consider similarity",
        "published": "2006-02-17T16:15:07Z",
        "link": "http://arxiv.org/abs/cs/0602065v1",
        "categories": [
            "cs.CV",
            "cs.IR",
            "I.5; E.4; H.5; I.2.6; I.2.7; I.6"
        ]
    },
    {
        "title": "Exploring term-document matrices from matrix models in text mining",
        "authors": [
            "Ioannis Antonellis",
            "Efstratios Gallopoulos"
        ],
        "summary": "We explore a matrix-space model, that is a natural extension to the vector space model for Information Retrieval. Each document can be represented by a matrix that is based on document extracts (e.g. sentences, paragraphs, sections). We focus on the performance of this model for the specific case in which documents are originally represented as term-by-sentence matrices. We use the singular value decomposition to approximate the term-by-sentence matrices and assemble these results to form the pseudo-``term-document'' matrix that forms the basis of a text mining method alternative to traditional VSM and LSI. We investigate the singular values of this matrix and provide experimental evidence suggesting that the method can be particularly effective in terms of accuracy for text collections with multi-topic documents, such as web pages with news.",
        "published": "2006-02-21T16:14:16Z",
        "link": "http://arxiv.org/abs/cs/0602076v1",
        "categories": [
            "cs.IR",
            "cs.DB",
            "cs.DL"
        ]
    },
    {
        "title": "Does the arXiv lead to higher citations and reduced publisher downloads   for mathematics articles?",
        "authors": [
            "Philip M. Davis",
            "Michael J. Fromerth"
        ],
        "summary": "An analysis of 2,765 articles published in four math journals from 1997 to 2005 indicate that articles deposited in the arXiv received 35% more citations on average than non-deposited articles (an advantage of about 1.1 citations per article), and that this difference was most pronounced for highly-cited articles. Open Access, Early View, and Quality Differential were examined as three non-exclusive postulates for explaining the citation advantage. There was little support for a universal Open Access explanation, and no empirical support for Early View. There was some inferential support for a Quality Differential brought about by more highly-citable articles being deposited in the arXiv. In spite of their citation advantage, arXiv-deposited articles received 23% fewer downloads from the publisher's website (about 10 fewer downloads per article) in all but the most recent two years after publication. The data suggest that arXiv and the publisher's website may be fulfilling distinct functional needs of the reader.",
        "published": "2006-03-14T19:36:24Z",
        "link": "http://arxiv.org/abs/cs/0603056v5",
        "categories": [
            "cs.DL",
            "cs.IR",
            "math.HO"
        ]
    },
    {
        "title": "VXA: A Virtual Architecture for Durable Compressed Archives",
        "authors": [
            "Bryan Ford"
        ],
        "summary": "Data compression algorithms change frequently, and obsolete decoders do not always run on new hardware and operating systems, threatening the long-term usability of content archived using those algorithms. Re-encoding content into new formats is cumbersome, and highly undesirable when lossy compression is involved. Processor architectures, in contrast, have remained comparatively stable over recent decades. VXA, an archival storage system designed around this observation, archives executable decoders along with the encoded content it stores. VXA decoders run in a specialized virtual machine that implements an OS-independent execution environment based on the standard x86 architecture. The VXA virtual machine strictly limits access to host system services, making decoders safe to run even if an archive contains malicious code. VXA's adoption of a \"native\" processor architecture instead of type-safe language technology allows reuse of existing \"hand-optimized\" decoders in C and assembly language, and permits decoders access to performance-enhancing architecture features such as vector processing instructions. The performance cost of VXA's virtualization is typically less than 15% compared with the same decoders running natively. The storage cost of archived decoders, typically 30-130KB each, can be amortized across many archived files sharing the same compression method.",
        "published": "2006-03-18T16:31:33Z",
        "link": "http://arxiv.org/abs/cs/0603073v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7; H.1.1; D.4.5; E.5"
        ]
    },
    {
        "title": "On Conditional Branches in Optimal Search Trees",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Algorithms for efficiently finding optimal alphabetic decision trees -- such as the Hu-Tucker algorithm -- are well established and commonly used. However, such algorithms generally assume that the cost per decision is uniform and thus independent of the outcome of the decision. The few algorithms without this assumption instead use one cost if the decision outcome is ``less than'' and another cost otherwise. In practice, neither assumption is accurate for software optimized for today's microprocessors. Such software generally has one cost for the more likely decision outcome and a greater cost -- often far greater -- for the less likely decision outcome. This problem and generalizations thereof are thus applicable to hard coding static decision tree instances in software, e.g., for optimizing program bottlenecks or for compiling switch statements. An O(n^3)-time O(n^2)-space dynamic programming algorithm can solve this optimal binary decision tree problem, and this approach has many generalizations that optimize for the behavior of processors with predictive branch capabilities, both static and dynamic. Solutions to this formulation are often faster in practice than ``optimal'' decision trees as formulated in the literature. Different search paradigms can sometimes yield even better performance.",
        "published": "2006-04-06T00:54:44Z",
        "link": "http://arxiv.org/abs/cs/0604016v2",
        "categories": [
            "cs.PF",
            "cs.DS",
            "cs.IR",
            "B.1.4; C.0; C.1.1; D.3.4; E.1; F.2.2; G.3; H.3.3; I.2.8"
        ]
    },
    {
        "title": "Collaborative thesaurus tagging the Wikipedia way",
        "authors": [
            "Jakob Voss"
        ],
        "summary": "This paper explores the system of categories that is used to classify articles in Wikipedia. It is compared to collaborative tagging systems like del.icio.us and to hierarchical classification like the Dewey Decimal Classification (DDC). Specifics and commonalitiess of these systems of subject indexing are exposed. Analysis of structural and statistical properties (descriptors per record, records per descriptor, descriptor levels) shows that the category system of Wikimedia is a thesaurus that combines collaborative tagging and hierarchical subject indexing in a special way.",
        "published": "2006-04-10T12:04:29Z",
        "link": "http://arxiv.org/abs/cs/0604036v2",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1"
        ]
    },
    {
        "title": "Query Chains: Learning to Rank from Implicit Feedback",
        "authors": [
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "This paper presents a novel approach for using clickthrough data to learn ranked retrieval functions for web search results. We observe that users searching the web often perform a sequence, or chain, of queries with a similar information need. Using query chains, we generate new types of preference judgments from search engine logs, thus taking advantage of user intelligence in reformulating queries. To validate our method we perform a controlled user study comparing generated preference judgments to explicit relevance judgments. We also implemented a real-world search engine to test our approach, using a modified ranking SVM to learn an improved ranking function from preference data. Our results demonstrate significant improvements in the ranking given by the search engine. The learned rankings outperform both a static ranking function, as well as one trained without considering query chains.",
        "published": "2006-05-08T22:05:24Z",
        "link": "http://arxiv.org/abs/cs/0605035v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "Evaluating the Robustness of Learning from Implicit Feedback",
        "authors": [
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "This paper evaluates the robustness of learning from implicit feedback in web search. In particular, we create a model of user behavior by drawing upon user studies in laboratory and real-world settings. The model is used to understand the effect of user behavior on the performance of a learning algorithm for ranked retrieval. We explore a wide range of possible user behaviors and find that learning from implicit feedback can be surprisingly robust. This complements previous results that demonstrated our algorithm's effectiveness in a real-world search engine application.",
        "published": "2006-05-08T23:38:13Z",
        "link": "http://arxiv.org/abs/cs/0605036v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "Minimally Invasive Randomization for Collecting Unbiased Preferences   from Clickthrough Logs",
        "authors": [
            "Filip Radlinski",
            "Thorsten Joachims"
        ],
        "summary": "Clickthrough data is a particularly inexpensive and plentiful resource to obtain implicit relevance feedback for improving and personalizing search engines. However, it is well known that the probability of a user clicking on a result is strongly biased toward documents presented higher in the result set irrespective of relevance. We introduce a simple method to modify the presentation of search results that provably gives relevance judgments that are unaffected by presentation bias under reasonable assumptions. We validate this property of the training data in interactive real world experiments. Finally, we show that using these unbiased relevance judgments learning methods can be guaranteed to converge to an ideal ranking given sufficient data.",
        "published": "2006-05-09T01:53:22Z",
        "link": "http://arxiv.org/abs/cs/0605037v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.3"
        ]
    },
    {
        "title": "Modeling Hypermedia-Based Communication",
        "authors": [
            "V. V. Kryssanov",
            "K. Kakusho",
            "E. L. Kuleshov",
            "M. Minoh"
        ],
        "summary": "In this article, we explore two approaches to modeling hypermedia-based communication. It is argued that the classical conveyor-tube framework is not applicable to the case of computer- and Internet- mediated communication. We then present a simple but very general system-theoretic model of the communication process, propose its mathematical interpretation, and derive several formulas, which qualitatively and quantitatively accord with data obtained on-line. The devised theoretical results generalize and correct the Zipf-Mandelbrot law and can be used in information system design. At the paper's end, we give some conclusions and draw implications for future work.",
        "published": "2006-05-25T12:34:03Z",
        "link": "http://arxiv.org/abs/cs/0605122v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "cs.IR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Inference and Evaluation of the Multinomial Mixture Model for Text   Clustering",
        "authors": [
            "Loïs Rigouste",
            "Olivier Cappé",
            "François Yvon"
        ],
        "summary": "In this article, we investigate the use of a probabilistic model for unsupervised clustering in text collections. Unsupervised clustering has become a basic module for many intelligent text processing applications, such as information retrieval, text classification or information extraction. The model considered in this contribution consists of a mixture of multinomial distributions over the word counts, each component corresponding to a different theme. We present and contrast various estimation procedures, which apply both in supervised and unsupervised contexts. In supervised learning, this work suggests a criterion for evaluating the posterior odds of new documents which is more statistically sound than the \"naive Bayes\" approach. In an unsupervised context, we propose measures to set up a systematic evaluation framework and start with examining the Expectation-Maximization (EM) algorithm as the basic tool for inference. We discuss the importance of initialization and the influence of other features such as the smoothing strategy or the size of the vocabulary, thereby illustrating the difficulties incurred by the high dimensionality of the parameter space. We also propose a heuristic algorithm based on iterative EM with vocabulary reduction to solve this problem. Using the fact that the latent variables can be analytically integrated out, we finally show that Gibbs sampling algorithm is tractable and compares favorably to the basic expectation maximization approach.",
        "published": "2006-06-14T14:44:06Z",
        "link": "http://arxiv.org/abs/cs/0606069v1",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "Comparison of the estimation of the degree of polarization from four or   two intensity images degraded by speckle noise",
        "authors": [
            "Muriel Roche",
            "Philippe Réfrégier"
        ],
        "summary": "Active polarimetric imagery is a powerful tool for accessing the information present in a scene. Indeed, the polarimetric images obtained can reveal polarizing properties of the objects that are not avalaible using conventional imaging systems. However, when coherent light is used to illuminate the scene, the images are degraded by speckle noise. The polarization properties of a scene are characterized by the degree of polarization. In standard polarimetric imagery system, four intensity images are needed to estimate this degree . If we assume the uncorrelation of the measurements, this number can be decreased to two images using the Orthogonal State Contrast Image (OSCI). However, this approach appears too restrictive in some cases. We thus propose in this paper a new statistical parametric method to estimate the degree of polarization assuming correlated measurements with only two intensity images. The estimators obtained from four images, from the OSCI and from the proposed method, are compared using simulated polarimetric data degraded by speckle noise.",
        "published": "2006-06-15T13:13:30Z",
        "link": "http://arxiv.org/abs/cs/0606073v1",
        "categories": [
            "cs.IR",
            "physics.optics"
        ]
    },
    {
        "title": "Synonym search in Wikipedia: Synarcher",
        "authors": [
            "A. Krizhanovsky"
        ],
        "summary": "The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming.",
        "published": "2006-06-22T14:17:26Z",
        "link": "http://arxiv.org/abs/cs/0606097v2",
        "categories": [
            "cs.IR",
            "cs.DM",
            "H.3.1; H.3.3; H.4.3; G.2.2"
        ]
    },
    {
        "title": "Iso9000 Based Advanced Quality Approach for Continuous Improvement of   Manufacturing Processes",
        "authors": [
            "Salah Deeb",
            "Benoît Iung"
        ],
        "summary": "The continuous improvement in TQM is considered as the core value by which organisation could maintain a competitive edge. Several techniques and tools are known to support this core value but most of the time these techniques are informal and without modelling the interdependence between the core value and tools. Thus, technique formalisation is one of TQM challenges for increasing efficiency of quality process implementation. In that way, the paper proposes and experiments an advanced quality modelling approach based on meta-modelling the \"process approach\" as advocated by the standard ISO9000:2000. This meta-model allows formalising the interdependence between technique, tools and core value",
        "published": "2006-06-26T11:51:41Z",
        "link": "http://arxiv.org/abs/cs/0606105v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Evaluating Variable Length Markov Chain Models for Analysis of User Web   Navigation Sessions",
        "authors": [
            "Jose Borges",
            "Mark Levene"
        ],
        "summary": "Markov models have been widely used to represent and analyse user web navigation data. In previous work we have proposed a method to dynamically extend the order of a Markov chain model and a complimentary method for assessing the predictive power of such a variable length Markov chain. Herein, we review these two methods and propose a novel method for measuring the ability of a variable length Markov model to summarise user web navigation sessions up to a given length. While the summarisation ability of a model is important to enable the identification of user navigation patterns, the ability to make predictions is important in order to foresee the next link choice of a user after following a given trail so as, for example, to personalise a web site. We present an extensive experimental evaluation providing strong evidence that prediction accuracy increases linearly with summarisation ability.",
        "published": "2006-06-28T08:26:45Z",
        "link": "http://arxiv.org/abs/cs/0606115v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Adapting a general parser to a sublanguage",
        "authors": [
            "Sophie Aubin",
            "Adeline Nazarenko",
            "Claire Nédellec"
        ],
        "summary": "In this paper, we propose a method to adapt a general parser (Link Parser) to sublanguages, focusing on the parsing of texts in biology. Our main proposal is the use of terminology (identication and analysis of terms) in order to reduce the complexity of the text to be parsed. Several other strategies are explored and finally combined among which text normalization, lexicon and morpho-guessing module extensions and grammar rules adaptation. We compare the parsing results before and after these adaptations.",
        "published": "2006-06-28T14:43:42Z",
        "link": "http://arxiv.org/abs/cs/0606118v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.4"
        ]
    },
    {
        "title": "Lexical Adaptation of Link Grammar to the Biomedical Sublanguage: a   Comparative Evaluation of Three Approaches",
        "authors": [
            "Sampo Pyysalo",
            "Tapio Salakoski",
            "Sophie Aubin",
            "Adeline Nazarenko"
        ],
        "summary": "We study the adaptation of Link Grammar Parser to the biomedical sublanguage with a focus on domain terms not found in a general parser lexicon. Using two biomedical corpora, we implement and evaluate three approaches to addressing unknown words: automatic lexicon expansion, the use of morphological clues, and disambiguation using a part-of-speech tagger. We evaluate each approach separately for its effect on parsing performance and consider combinations of these approaches. In addition to a 45% increase in parsing efficiency, we find that the best approach, incorporating information from a domain part-of-speech tagger, offers a statistically signicant 10% relative decrease in error. The adapted parser is available under an open-source license at http://www.it.utu.fi/biolg.",
        "published": "2006-06-28T14:44:48Z",
        "link": "http://arxiv.org/abs/cs/0606119v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.4"
        ]
    },
    {
        "title": "Automatic forming lists of semantically related terms based on texts   rating in the corpus with hyperlinks and categories (In Russian)",
        "authors": [
            "A. Krizhanovsky"
        ],
        "summary": "HITS adapted algorithm for synonym search, the program architecture, and the program work evaluation with test examples are presented in the paper. Synarcher program for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of search are presented in the form of a graph. It is possible to explore the graph and search graph elements interactively. The proposed algorithm could be applied to the search request extending and for synonym dictionary forming.",
        "published": "2006-06-30T15:17:36Z",
        "link": "http://arxiv.org/abs/cs/0606128v1",
        "categories": [
            "cs.IR",
            "cs.DM",
            "H.3.1; H.3.3; H.4.3; G.2.2"
        ]
    },
    {
        "title": "A Flexible Structured-based Representation for XML Document Mining",
        "authors": [
            "Anne-Marie Vercoustre",
            "Mounir Fegas",
            "Saba Gul",
            "Yves Lechevallier"
        ],
        "summary": "This paper reports on the INRIA group's approach to XML mining while participating in the INEX XML Mining track 2005. We use a flexible representation of XML documents that allows taking into account the structure only or both the structure and content. Our approach consists of representing XML documents by a set of their sub-paths, defined according to some criteria (length, root beginning, leaf ending). By considering those sub-paths as words, we can use standard methods for vocabulary reduction, and simple clustering methods such as K-means that scale well. We actually use an implementation of the clustering algorithm known as \"dynamic clouds\" that can work with distinct groups of independent variables put in separate variables. This is useful in our model since embedded sub-paths are not independent: we split potentially dependant paths into separate variables, resulting in each of them containing independant paths. Experiments with the INEX collections show good results for the structure-only collections, but our approach could not scale well for large structure-and-content collections.",
        "published": "2006-07-05T12:07:16Z",
        "link": "http://arxiv.org/abs/cs/0607012v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "The uncovering of hidden structures by Latent Semantic Analysis",
        "authors": [
            "Juan C. Valle-Lisboa",
            "Eduardo Mizraji"
        ],
        "summary": "Latent Semantic Analysis (LSA) is a well known method for information retrieval. It has also been applied as a model of cognitive processing and word-meaning acquisition. This dual importance of LSA derives from its capacity to modulate the meaning of words by contexts, dealing successfully with polysemy and synonymy. The underlying reasons that make the method work are not clear enough. We propose that the method works because it detects an underlying block structure (the blocks corresponding to topics) in the term by document matrix. In real cases this block structure is hidden because of perturbations. We propose that the correct explanation for LSA must be searched in the structure of singular vectors rather than in the profile of singular values. Using Perron-Frobenius theory we show that the presence of disjoint blocks of documents is marked by sign-homogeneous entries in the vectors corresponding to the documents of one block and zeros elsewhere. In the case of nearly disjoint blocks, perturbation theory shows that if the perturbations are small the zeros in the leading vectors are replaced by small numbers (pseudo-zeros). Since the singular values of each block might be very different in magnitude, their order does not mirror the order of blocks. When the norms of the blocks are similar, LSA works fine, but we propose that when the topics have different sizes, the usual procedure of selecting the first k singular triplets (k being the number of blocks) should be replaced by a method that selects the perturbed Perron vectors for each block.",
        "published": "2006-07-05T22:56:29Z",
        "link": "http://arxiv.org/abs/cs/0607015v1",
        "categories": [
            "cs.IR",
            "H.3.3; H.3.1"
        ]
    },
    {
        "title": "Système de représentation d'aide au besoin dans le domaine   architectural",
        "authors": [
            "Marie-France Ango-Obiang"
        ],
        "summary": "The image is a very important mean of communication in the field of architectural who intervenes in the various phases of the design of a project. It can be regarded as a tool of decision-making aid. The study of our research aims at to see the contribution of the Economic Intelligence in the resolution of a decisional problem of the various partners (Architect, Contractor, Customer) in the architectural field, in order to make strategic decisions within the framework of the realization or design of an architectural work. The economic Intelligence allows the taking into account of the real needs for the user-decision makers, so that their waiting are considered at the first stage of a search for information and not in the final stage of the development of the tool in the evaluation of this last.",
        "published": "2006-07-18T05:04:01Z",
        "link": "http://arxiv.org/abs/cs/0607081v1",
        "categories": [
            "cs.OH",
            "cs.IR"
        ]
    },
    {
        "title": "In-Degree and PageRank of Web pages: Why do they follow similar power   laws?",
        "authors": [
            "N. Litvak",
            "W. R. W. Scheinhardt",
            "Y. Volkovich"
        ],
        "summary": "The PageRank is a popularity measure designed by Google to rank Web pages. Experiments confirm that the PageRank obeys a `power law' with the same exponent as the In-Degree. This paper presents a novel mathematical model that explains this phenomenon. The relation between the PageRank and In-Degree is modelled through a stochastic equation, which is inspired by the original definition of the PageRank, and is analogous to the well-known distributional identity for the busy period in the M/G/1 queue. Further, we employ the theory of regular variation and Tauberian theorems to analytically prove that the tail behavior of the PageRank and the In-Degree differ only by a multiplicative factor, for which we derive a closed-form expression. Our analytical results are in good agreement with experimental data.",
        "published": "2006-07-20T16:07:10Z",
        "link": "http://arxiv.org/abs/math/0607507v3",
        "categories": [
            "math.PR",
            "cs.IR",
            "90B15, 68P10, 40E05"
        ]
    },
    {
        "title": "Singular Values and Eigenvalues of Tensors: A Variational Approach",
        "authors": [
            "Lek-Heng Lim"
        ],
        "summary": "We propose a theory of eigenvalues, eigenvectors, singular values, and singular vectors for tensors based on a constrained variational approach much like the Rayleigh quotient for symmetric matrix eigenvalues. These notions are particularly useful in generalizing certain areas where the spectral theory of matrices has traditionally played an important role. For illustration, we will discuss a multilinear generalization of the Perron-Frobenius theorem.",
        "published": "2006-07-26T03:07:35Z",
        "link": "http://arxiv.org/abs/math/0607648v1",
        "categories": [
            "math.SP",
            "cs.IR",
            "cs.NA",
            "math.NA",
            "math.OC",
            "15A18, 15A48, 15A69, 15A72, 65F15, 65K10"
        ]
    },
    {
        "title": "Expressing Implicit Semantic Relations without Supervision",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations. For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns <P1,...,Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y. For example, given X=ostrich and Y=bird, the two highest ranking output patterns are \"X is the largest Y\" and \"Y such as the X\". The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks. The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X:Y is the expected relational similarity between the given pair and typical pairs for Pi. The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs. On both tasks, the algorithm achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf.",
        "published": "2006-07-27T18:23:45Z",
        "link": "http://arxiv.org/abs/cs/0607120v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Separating the articles of authors with the same name",
        "authors": [
            "Jose M. Soler"
        ],
        "summary": "I describe a method to separate the articles of different authors with the same name. It is based on a distance between any two publications, defined in terms of the probability that they would have as many coincidences if they were drawn at random from all published documents. Articles with a given author name are then clustered according to their distance, so that all articles in a cluster belong very likely to the same author. The method has proven very useful in generating groups of papers that are then selected manually. This simplifies considerably citation analysis when the author publication lists are not available.",
        "published": "2006-08-01T18:23:26Z",
        "link": "http://arxiv.org/abs/cs/0608004v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Using Users' Expectations to Adapt Business Intelligence Systems",
        "authors": [
            "Babajide Afolabi",
            "Odile Thiery"
        ],
        "summary": "This paper takes a look at the general characteristics of business or economic intelligence system. The role of the user within this type of system is emphasized. We propose two models which we consider important in order to adapt this system to the user. The first model is based on the definition of decisional problem and the second on the four cognitive phases of human learning. We also describe the application domain we are using to test these models in this type of system.",
        "published": "2006-08-08T13:19:08Z",
        "link": "http://arxiv.org/abs/cs/0608043v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Solving non-uniqueness in agglomerative hierarchical clustering using   multidendrograms",
        "authors": [
            "Alberto Fernandez",
            "Sergio Gomez"
        ],
        "summary": "In agglomerative hierarchical clustering, pair-group methods suffer from a problem of non-uniqueness when two or more distances between different clusters coincide during the amalgamation process. The traditional approach for solving this drawback has been to take any arbitrary criterion in order to break ties between distances, which results in different hierarchical classifications depending on the criterion followed. In this article we propose a variable-group algorithm that consists in grouping more than two clusters at the same time when ties occur. We give a tree representation for the results of the algorithm, which we call a multidendrogram, as well as a generalization of the Lance and Williams' formula which enables the implementation of the algorithm in a recursive way.",
        "published": "2006-08-08T18:35:52Z",
        "link": "http://arxiv.org/abs/cs/0608049v2",
        "categories": [
            "cs.IR",
            "math.ST",
            "physics.data-an",
            "stat.TH",
            "H.3.3; I.5.3"
        ]
    },
    {
        "title": "Information filtering via Iterative Refinement",
        "authors": [
            "P. Laureti",
            "L. Moret",
            "Y. -C. Zhang",
            "Y. -K. Yu"
        ],
        "summary": "With the explosive growth of accessible information, expecially on the Internet, evaluation-based filtering has become a crucial task. Various systems have been devised aiming to sort through large volumes of information and select what is likely to be more relevant. In this letter we analyse a new ranking method, where the reputation of information providers is determined self-consistently.",
        "published": "2006-08-16T12:27:48Z",
        "link": "http://arxiv.org/abs/physics/0608166v1",
        "categories": [
            "physics.data-an",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Similarity of Semantic Relations",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.",
        "published": "2006-08-25T14:35:11Z",
        "link": "http://arxiv.org/abs/cs/0608100v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "The Haar Wavelet Transform of a Dendrogram",
        "authors": [
            "Fionn Murtagh"
        ],
        "summary": "We describe a new wavelet transform, for use on hierarchies or binary rooted trees. The theoretical framework of this approach to data analysis is described. Case studies are used to further exemplify this approach. A first set of application studies deals with data array smoothing, or filtering. A second set of application studies relates to hierarchical tree condensation. Finally, a third study explores the wavelet decomposition, and the reproducibility of data sets such as text, including a new perspective on the generation or computability of such data objects.",
        "published": "2006-08-28T17:05:07Z",
        "link": "http://arxiv.org/abs/cs/0608107v3",
        "categories": [
            "cs.IR",
            "I.5.3; H.3.1; I.1.m"
        ]
    },
    {
        "title": "In Quest of Image Semantics: Are We Looking for It Under the Right   Lamppost?",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "In the last years we witness a dramatic growth of research focused on semantic image understanding. Indeed, without understanding image content successful accomplishment of any image-processing task is simply incredible. Up to the recent times, the ultimate need for such understanding has been met by the knowledge that a domain expert or a vision system supervisor have contributed to every image-processing application. The advent of the Internet has drastically changed this situation. Internet sources of visual information are diffused and dispersed over the whole Web, so the duty of information content discovery and evaluation must be relegated now to an image understanding agent (a machine or a computer program) capable to perform image content assessment at a remote image location. Development of Content Based Image Retrieval (CBIR) techniques was a right move in a right direction, launched about ten years ago. Unfortunately, very little progress has been made since then. The reason for this can be seen in a rank of long lasting misconceptions that CBIR designers are continuing to adhere to. I hope, my arguments will help them to change their minds.",
        "published": "2006-09-02T11:50:29Z",
        "link": "http://arxiv.org/abs/cs/0609003v1",
        "categories": [
            "cs.CV",
            "cs.IR"
        ]
    },
    {
        "title": "Multilingual person name recognition and transliteration",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat",
            "Irina Temnikova",
            "Anna Widiger",
            "Wajdi Zaghouani",
            "Jan Zizka"
        ],
        "summary": "We present an exploratory tool that extracts person names from multilingual news collections, matches name variants referring to the same person, and infers relationships between people based on the co-occurrence of their names in related news. A novel feature is the matching of name variants across languages and writing systems, including names written with the Greek, Cyrillic and Arabic writing system. Due to our highly multilingual setting, we use an internal standard representation for name representation and matching, instead of adopting the traditional bilingual approach to transliteration. This work is part of the news analysis system NewsExplorer that clusters an average of 25,000 news articles per day to detect related news within the same and across different languages.",
        "published": "2006-09-11T11:08:23Z",
        "link": "http://arxiv.org/abs/cs/0609051v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.5"
        ]
    },
    {
        "title": "Navigating multilingual news collections using automatically extracted   information",
        "authors": [
            "Ralf Steinberger",
            "Bruno Pouliquen",
            "Camelia Ignat"
        ],
        "summary": "We are presenting a text analysis tool set that allows analysts in various fields to sieve through large collections of multilingual news items quickly and to find information that is of relevance to them. For a given document collection, the tool set automatically clusters the texts into groups of similar articles, extracts names of places, people and organisations, lists the user-defined specialist terms found, links clusters and entities, and generates hyperlinks. Through its daily news analysis operating on thousands of articles per day, the tool also learns relationships between people and other entities. The fully functional prototype system allows users to explore and navigate multilingual document collections across languages and time.",
        "published": "2006-09-11T11:54:41Z",
        "link": "http://arxiv.org/abs/cs/0609053v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.5"
        ]
    },
    {
        "title": "Automatic annotation of multilingual text collections with a conceptual   thesaurus",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat"
        ],
        "summary": "Automatic annotation of documents with controlled vocabulary terms (descriptors) from a conceptual thesaurus is not only useful for document indexing and retrieval. The mapping of texts onto the same thesaurus furthermore allows to establish links between similar documents. This is also a substantial requirement of the Semantic Web. This paper presents an almost language-independent system that maps documents written in different languages onto the same multilingual conceptual thesaurus, EUROVOC. Conceptual thesauri differ from Natural Language Thesauri in that they consist of relatively small controlled lists of words or phrases with a rather abstract meaning. To automatically identify which thesaurus descriptors describe the contents of a document best, we developed a statistical, associative system that is trained on texts that have previously been indexed manually. In addition to describing the large number of empirically optimised parameters of the fully functional application, we present the performance of the software according to a human evaluation by professional indexers.",
        "published": "2006-09-12T07:24:01Z",
        "link": "http://arxiv.org/abs/cs/0609059v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.6"
        ]
    },
    {
        "title": "Automatic Identification of Document Translations in Large Multilingual   Document Collections",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat"
        ],
        "summary": "Texts and their translations are a rich linguistic resource that can be used to train and test statistics-based Machine Translation systems and many other applications. In this paper, we present a working system that can identify translations and other very similar documents among a large number of candidates, by representing the document contents with a vector of thesaurus terms from a multilingual thesaurus, and by then measuring the semantic similarity between the vectors. Tests on different text types have shown that the system can detect translations with over 96% precision in a large search space of 820 documents or more. The system was tuned to ignore language-specific similarities and to give similar documents in a second language the same similarity score as equivalent documents in the same language. The application can also be used to detect cross-lingual document plagiarism.",
        "published": "2006-09-12T08:44:53Z",
        "link": "http://arxiv.org/abs/cs/0609060v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.6"
        ]
    },
    {
        "title": "Cross-lingual keyword assignment",
        "authors": [
            "Ralf Steinberger"
        ],
        "summary": "This paper presents a language-independent approach to controlled vocabulary keyword assignment using the EUROVOC thesaurus. Due to the multilingual nature of EUROVOC, the keywords for a document written in one language can be displayed in all eleven official European Union languages. The mapping of documents written in different languages to the same multilingual thesaurus furthermore allows cross-language document comparison. The assignment of the controlled vocabulary thesaurus descriptors is achieved by applying a statistical method that uses a collection of manually indexed documents to identify, for each thesaurus descriptor, a large number of lemmas that are statistically associated to the descriptor. These associated words are then used during the assignment procedure to identify a ranked list of those EUROVOC terms that are most likely to be good keywords for a given document. The paper also describes the challenges of this task and discusses the achieved results of the fully functional prototype.",
        "published": "2006-09-12T09:29:57Z",
        "link": "http://arxiv.org/abs/cs/0609061v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4; H.3.6"
        ]
    },
    {
        "title": "Extending an Information Extraction tool set to Central and Eastern   European languages",
        "authors": [
            "Camelia Ignat",
            "Bruno Pouliquen",
            "Antonio Ribeiro",
            "Ralf Steinberger"
        ],
        "summary": "In a highly multilingual and multicultural environment such as in the European Commission with soon over twenty official languages, there is an urgent need for text analysis tools that use minimal linguistic knowledge so that they can be adapted to many languages without much human effort. We are presenting two such Information Extraction tools that have already been adapted to various Western and Eastern European languages: one for the recognition of date expressions in text, and one for the detection of geographical place names and the visualisation of the results in geographical maps. An evaluation of the performance has produced very satisfying results.",
        "published": "2006-09-12T12:29:17Z",
        "link": "http://arxiv.org/abs/cs/0609063v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.6; H.3.4"
        ]
    },
    {
        "title": "Exploiting multilingual nomenclatures and language-independent text   features as an interlingua for cross-lingual text analysis applications",
        "authors": [
            "Ralf Steinberger",
            "Bruno Pouliquen",
            "Camelia Ignat"
        ],
        "summary": "We are proposing a simple, but efficient basic approach for a number of multilingual and cross-lingual language technology applications that are not limited to the usual two or three languages, but that can be applied with relatively little effort to larger sets of languages. The approach consists of using existing multilingual linguistic resources such as thesauri, nomenclatures and gazetteers, as well as exploiting the existence of additional more or less language-independent text items such as dates, currency expressions, numbers, names and cognates. Mapping texts onto the multilingual resources and identifying word token links between texts in different languages are basic ingredients for applications such as cross-lingual document similarity calculation, multilingual clustering and categorisation, cross-lingual document retrieval, and tools to provide cross-lingual information access.",
        "published": "2006-09-12T12:43:37Z",
        "link": "http://arxiv.org/abs/cs/0609064v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "Geocoding multilingual texts: Recognition, disambiguation and   visualisation",
        "authors": [
            "Bruno Pouliquen",
            "Marco Kimler",
            "Ralf Steinberger",
            "Camelia Ignat",
            "Tamara Oellinger",
            "Ken Blackler",
            "Flavio Fuart",
            "Wajdi Zaghouani",
            "Anna Widiger",
            "Ann-Charlotte Forslund",
            "Clive Best"
        ],
        "summary": "We are presenting a method to recognise geographical references in free text. Our tool must work on various languages with a minimum of language-dependent resources, except a gazetteer. The main difficulty is to disambiguate these place names by distinguishing places from persons and by selecting the most likely place out of a list of homographic place names world-wide. The system uses a number of language-independent clues and heuristics to disambiguate place name homographs. The final aim is to index texts with the countries and cities they mention and to automatically visualise this information on geographical maps using various tools.",
        "published": "2006-09-12T12:57:38Z",
        "link": "http://arxiv.org/abs/cs/0609065v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "Building and displaying name relations using automatic unsupervised   analysis of newspaper articles",
        "authors": [
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Camelia Ignat",
            "Tamara Oellinger"
        ],
        "summary": "We present a tool that, from automatically recognised names, tries to infer inter-person relations in order to present associated people on maps. Based on an in-house Named Entity Recognition tool, applied on clusters of an average of 15,000 news articles per day, in 15 different languages, we build a knowledge base that allows extracting statistical co-occurrences of persons and visualising them on a per-person page or in various graphs.",
        "published": "2006-09-12T13:09:37Z",
        "link": "http://arxiv.org/abs/cs/0609066v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "A tool set for the quick and efficient exploration of large document   collections",
        "authors": [
            "Camelia Ignat",
            "Bruno Pouliquen",
            "Ralf Steinberger",
            "Tomaz Erjavec"
        ],
        "summary": "We are presenting a set of multilingual text analysis tools that can help analysts in any field to explore large document collections quickly in order to determine whether the documents contain information of interest, and to find the relevant text passages. The automatic tool, which currently exists as a fully functional prototype, is expected to be particularly useful when users repeatedly have to sieve through large collections of documents such as those downloaded automatically from the internet. The proposed system takes a whole document collection as input. It first carries out some automatic analysis tasks (named entity recognition, geo-coding, clustering, term extraction), annotates the texts with the generated meta-information and stores the meta-information in a database. The system then generates a zoomable and hyperlinked geographic map enhanced with information on entities and terms found. When the system is used on a regular basis, it builds up a historical database that contains information on which names have been mentioned together with which other names or places, and users can query this database to retrieve information extracted in the past.",
        "published": "2006-09-12T13:30:01Z",
        "link": "http://arxiv.org/abs/cs/0609067v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; H.3.4"
        ]
    },
    {
        "title": "An application-oriented terminology evaluation: the case of back-of-the   book indexes",
        "authors": [
            "Touria Aït El Mekki",
            "Adeline Nazarenko"
        ],
        "summary": "This paper addresses the problem of computational terminology evaluation not per se but in a specific application context. This paper describes the evaluation procedure that has been used to assess the validity of our overall indexing approach and the quality of the IndDoc indexing tool. Even if user-oriented extended evaluation is irreplaceable, we argue that early evaluations are possible and they are useful for development guidance.",
        "published": "2006-09-24T19:59:20Z",
        "link": "http://arxiv.org/abs/cs/0609133v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Using NLP to build the hypertextuel network of a back-of-the-book index",
        "authors": [
            "Touria Aït El Mekki",
            "Adeline Nazarenko"
        ],
        "summary": "Relying on the idea that back-of-the-book indexes are traditional devices for navigation through large documents, we have developed a method to build a hypertextual network that helps the navigation in a document. Building such an hypertextual network requires selecting a list of descriptors, identifying the relevant text segments to associate with each descriptor and finally ranking the descriptors and reference segments by relevance order. We propose a specific document segmentation method and a relevance measure for information ranking. The algorithms are tested on 4 corpora (of different types and domains) without human intervention or any semantic knowledge.",
        "published": "2006-09-24T20:00:58Z",
        "link": "http://arxiv.org/abs/cs/0609134v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Event-based Information Extraction for the biomedical domain: the   Caderige project",
        "authors": [
            "Erick Alphonse",
            "Sophie Aubin",
            "Philippe Bessières",
            "Gilles Bisson",
            "Thierry Hamon",
            "Sandrine Lagarrigue",
            "Adeline Nazarenko",
            "Alain-Pierre Manine",
            "Claire Nédellec",
            "Mohamed Ould Abdel Vetah",
            "Thierry Poibeau",
            "Davy Weissenbacher"
        ],
        "summary": "This paper gives an overview of the Caderige project. This project involves teams from different areas (biology, machine learning, natural language processing) in order to develop high-level analysis tools for extracting structured information from biological bibliographical databases, especially Medline. The paper gives an overview of the approach and compares it to the state of the art.",
        "published": "2006-09-24T20:03:06Z",
        "link": "http://arxiv.org/abs/cs/0609135v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Ontologies and Information Extraction",
        "authors": [
            "Claire Nédellec",
            "Adeline Nazarenko"
        ],
        "summary": "This report argues that, even in the simplest cases, IE is an ontology-driven process. It is not a mere text filtering method based on simple pattern matching and keywords, because the extracted pieces of texts are interpreted with respect to a predefined partial domain model. This report shows that depending on the nature and the depth of the interpretation to be done for extracting the information, more or less knowledge must be involved. This report is mainly illustrated in biology, a domain in which there are critical needs for content-based exploration of the scientific literature and which becomes a major application domain for IE.",
        "published": "2006-09-24T20:10:35Z",
        "link": "http://arxiv.org/abs/cs/0609137v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.1"
        ]
    },
    {
        "title": "Creation and use of Citations in the ADS",
        "authors": [
            "Alberto Accomazzi",
            "Gunther Eichhorn",
            "Michael J. Kurtz",
            "Carolyn S. Grant",
            "Edwin Henneken",
            "Markus Demleitner",
            "Donna Thompson",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "With over 20 million records, the ADS citation database is regularly used by researchers and librarians to measure the scientific impact of individuals, groups, and institutions. In addition to the traditional sources of citations, the ADS has recently added references extracted from the arXiv e-prints on a nightly basis. We review the procedures used to harvest and identify the reference data used in the creation of citations, the policies and procedures that we follow to avoid double-counting and to eliminate contributions which may not be scholarly in nature. Finally, we describe how users and institutions can easily obtain quantitative citation data from the ADS, both interactively and via web-based programming tools.   The ADS is available at http://ads.harvard.edu.",
        "published": "2006-10-03T21:29:03Z",
        "link": "http://arxiv.org/abs/cs/0610011v1",
        "categories": [
            "cs.DL",
            "astro-ph",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "NectaRSS, an RSS feed ranking system that implicitly learns user   preferences",
        "authors": [
            "Juan J. Samper",
            "Pedro A. Castillo",
            "Lourdes Araujo",
            "J. J. Merelo"
        ],
        "summary": "In this paper a new RSS feed ranking method called NectaRSS is introduced. The system recommends information to a user based on his/her past choices. User preferences are automatically acquired, avoiding explicit feedback, and ranking is based on those preferences distilled to a user profile. NectaRSS uses the well-known vector space model for user profiles and new documents, and compares them using information-retrieval techniques, but introduces a novel method for user profile creation and adaptation from users' past choices. The efficiency of the proposed method has been tested by embedding it into an intelligent aggregator (RSS feed reader), which has been used by different and heterogeneous users. Besides, this paper proves that the ranking of newsitems yielded by NectaRSS improves its quality with user's choices, and its superiority over other algorithms that use a different information representation method.",
        "published": "2006-10-04T15:13:55Z",
        "link": "http://arxiv.org/abs/cs/0610019v1",
        "categories": [
            "cs.IR",
            "cs.HC"
        ]
    },
    {
        "title": "The Application of Fuzzy Logic to the Construction of the Ranking   Function of Information Retrieval Systems",
        "authors": [
            "Neil Rubens"
        ],
        "summary": "The quality of the ranking function is an important factor that determines the quality of the Information Retrieval system. Each document is assigned a score by the ranking function; the score indicates the likelihood of relevance of the document given a query. In the vector space model, the ranking function is defined by a mathematic expression. We propose a fuzzy logic (FL) approach to defining the ranking function. FL provides a convenient way of converting knowledge expressed in a natural language into fuzzy logic rules. The resulting ranking function could be easily viewed, extended, and verified: * if (tf is high) and (idf is high) > (relevance is high); * if (overlap is high) > (relevance is high). By using above FL rules, we are able to achieve performance approximately equal to the state of the art search engine Apache Lucene (deltaP10 +0.92%; deltaMAP -0.1%). The fuzzy logic approach allows combining the logic-based model with the vector model. The resulting model possesses simplicity and formalism of the logic based model, and the flexibility and performance of the vector model.",
        "published": "2006-10-08T05:34:12Z",
        "link": "http://arxiv.org/abs/cs/0610039v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Context-sensitive access to e-document corpus",
        "authors": [
            "A. V. Smirnov",
            "T. V. Levashova",
            "M. P. Pashkin",
            "N. G. Shilov",
            "A. A. Krizhanovsky",
            "A. M. Kashevnik",
            "A. S. Komarova"
        ],
        "summary": "The methodology of context-sensitive access to e-documents considers context as a problem model based on the knowledge extracted from the application domain, and presented in the form of application ontology. Efficient access to an information in the text form is needed. Wiki resources as a modern text format provides huge number of text in a semi formalized structure. At the first stage of the methodology, documents are indexed against the ontology representing macro-situation. The indexing method uses a topic tree as a middle layer between documents and the application ontology. At the second stage documents relevant to the current situation (the abstract and operational contexts) are identified and sorted by degree of relevance. Abstract context is a problem-oriented ontology-based model. Operational context is an instantiation of the abstract context with data provided by the information sources. The following parts of the methodology are described: (i) metrics for measuring similarity of e-documents to ontology, (ii) a document index storing results of indexing of e-documents against the ontology; (iii) a method for identification of relevant e-documents based on semantic similarity measures. Wikipedia (wiki resource) is used as a corpus of e-documents for approach evaluation in a case study. Text categorization, the presence of metadata, and an existence of a lot of articles related to different topics characterize the corpus.",
        "published": "2006-10-11T08:02:14Z",
        "link": "http://arxiv.org/abs/cs/0610058v1",
        "categories": [
            "cs.IR",
            "H.3.1; H.3.3; H.4.3; G.2.2"
        ]
    },
    {
        "title": "Peano Count Trees (P-Trees) and Rule Association Mining for Gene   Expression Profiling of Microarray Data",
        "authors": [
            "Willy Valdivia-Granda",
            "William Perrizo",
            "Edward Deckard",
            "Francis Larson"
        ],
        "summary": "The greatest challenge in maximizing the use of gene expression data is to develop new computational tools capable of interconnecting and interpreting the results from different organisms and experimental settings. We propose an integrative and comprehensive approach including a super-chip containing data from microarray experiments collected on different species subjected to hypoxic and anoxic stress. A data mining technology called Peano count tree (P-trees) is used to represent genomic data in multidimensions. Each microarray spot is presented as a pixel with its corresponding red/green intensity feature bands. Each bad is stored separately in a reorganized 8-separate (bSQ) file format. Each bSQ is converted to a quadrant base tree structure (P-tree) from which a superchip is represented as expression P-trees (EP-trees) and repression P-trees (RP-trees). The use of association rule mining is proposed to derived to meanigingfully organize signal transduction pathways taking in consideration evolutionary considerations. We argue that the genetic constitution of an organism (K) can be represented by the total number of genes belonging to two groups. The group X constitutes genes (X1,Xn) and they can be represented as 1 or 0 depending on whether the gene was expressed or not. The second group of Y genes (Y1,Yn) is expressed at different levels. These genes have a very high repression, high expression, very repressed or highly repressed. However, many genes of the group Y are specie specific and modulated by the products and combinations of genes of the group X. In this paper, we introduce the dSQ and P-tree technology; the biological implications of association rule mining using X and Y gene groups and some advances in the integration of this information using the BRAIN architecture.",
        "published": "2006-10-12T19:55:32Z",
        "link": "http://arxiv.org/abs/cs/0610076v2",
        "categories": [
            "cs.DS",
            "cs.IR",
            "q-bio.MN"
        ]
    },
    {
        "title": "On the Behavior of Journal Impact Factor Rank-Order Distribution",
        "authors": [
            "R. Mansilla",
            "E. Köppen",
            "G. Cocho",
            "P. Miramontes"
        ],
        "summary": "An empirical law for the rank-order behavior of journal impact factors is found. Using an extensive data base on impact factors including journals on Education, Agrosciences, Geosciences, Biosciences and Environ- mental, Chemical, Computer, Engineering, Material, Mathematical, Medical and Physical Sciences we have found extremely good fits out- performing other rank-order models. Some extensions to other areas of knowledge are discussed.",
        "published": "2006-10-14T17:03:46Z",
        "link": "http://arxiv.org/abs/cs/0610091v4",
        "categories": [
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Applying Part-of-Seech Enhanced LSA to Automatic Essay Grading",
        "authors": [
            "Tuomo Kakkonen",
            "Niko Myller",
            "Erkki Sutinen"
        ],
        "summary": "Latent Semantic Analysis (LSA) is a widely used Information Retrieval method based on \"bag-of-words\" assumption. However, according to general conception, syntax plays a role in representing meaning of sentences. Thus, enhancing LSA with part-of-speech (POS) information to capture the context of word occurrences appears to be theoretically feasible extension. The approach is tested empirically on a automatic essay grading system using LSA for document similarity comparisons. A comparison on several POS-enhanced LSA models is reported. Our findings show that the addition of contextual information in the form of POS tags can raise the accuracy of the LSA-based scoring models up to 10.77 per cent.",
        "published": "2006-10-19T17:50:57Z",
        "link": "http://arxiv.org/abs/cs/0610118v1",
        "categories": [
            "cs.IR",
            "cs.CL"
        ]
    },
    {
        "title": "Nonlinear Estimators and Tail Bounds for Dimension Reduction in $l_1$   Using Cauchy Random Projections",
        "authors": [
            "Ping Li",
            "Trevor J. Hastie",
            "Kenneth W. Church"
        ],
        "summary": "For dimension reduction in $l_1$, the method of {\\em Cauchy random projections} multiplies the original data matrix $\\mathbf{A} \\in\\mathbb{R}^{n\\times D}$ with a random matrix $\\mathbf{R} \\in \\mathbb{R}^{D\\times k}$ ($k\\ll\\min(n,D)$) whose entries are i.i.d. samples of the standard Cauchy C(0,1). Because of the impossibility results, one can not hope to recover the pairwise $l_1$ distances in $\\mathbf{A}$ from $\\mathbf{B} = \\mathbf{AR} \\in \\mathbb{R}^{n\\times k}$, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computation, information retrieval, learning, and data mining.   We propose three types of nonlinear estimators: the bias-corrected sample median estimator, the bias-corrected geometric mean estimator, and the bias-corrected maximum likelihood estimator. The sample median estimator and the geometric mean estimator are asymptotically (as $k\\to \\infty$) equivalent but the latter is more accurate at small $k$. We derive explicit tail bounds for the geometric mean estimator and establish an analog of the Johnson-Lindenstrauss (JL) lemma for dimension reduction in $l_1$, which is weaker than the classical JL lemma for dimension reduction in $l_2$.   Asymptotically, both the sample median estimator and the geometric mean estimators are about 80% efficient compared to the maximum likelihood estimator (MLE). We analyze the moments of the MLE and propose approximating the distribution of the MLE by an inverse Gaussian.",
        "published": "2006-10-27T07:08:51Z",
        "link": "http://arxiv.org/abs/cs/0610155v1",
        "categories": [
            "cs.DS",
            "cs.IR",
            "cs.LG"
        ]
    },
    {
        "title": "Considering users' behaviours in improving the responses of an   information base",
        "authors": [
            "Babajide Afolabi",
            "Odile Thiery"
        ],
        "summary": "In this paper, our aim is to propose a model that helps in the efficient use of an information system by users, within the organization represented by the IS, in order to resolve their decisional problems. In other words we want to aid the user within an organization in obtaining the information that corresponds to his needs (informational needs that result from his decisional problems). This type of information system is what we refer to as economic intelligence system because of its support for economic intelligence processes of the organisation. Our assumption is that every EI process begins with the identification of the decisional problem which is translated into an informational need. This need is then translated into one or many information search problems (ISP). We also assumed that an ISP is expressed in terms of the user's expectations and that these expectations determine the activities or the behaviors of the user, when he/she uses an IS. The model we are proposing is used for the conception of the IS so that the process of retrieving of solution(s) or the responses given by the system to an ISP is based on these behaviours and correspond to the needs of the user.",
        "published": "2006-10-27T16:02:34Z",
        "link": "http://arxiv.org/abs/cs/0610158v1",
        "categories": [
            "cs.LG",
            "cs.IR"
        ]
    },
    {
        "title": "The effect of 'Open Access' upon citation impact: An analysis of ArXiv's   Condensed Matter Section",
        "authors": [
            "Henk F. Moed"
        ],
        "summary": "This article statistically analyses how the citation impact of articles deposited in the Condensed Matter section of the preprint server ArXiv (hosted by Cornell University), and subsequently published in a scientific journal, compares to that of articles in the same journal that were not deposited in that archive. Its principal aim is to further illustrate and roughly estimate the effect of two factors, 'early view' and 'quality bias', upon differences in citation impact between these two sets of papers, using citation data from Thomson Scientific's Web of Science. It presents estimates for a number of journals in the field of condensed matter physics. In order to discriminate between an 'open access' effect and an early view effect, longitudinal citation data was analysed covering a time period as long as 7 years. Quality bias was measured by calculating ArXiv citation impact differentials at the level of individual authors publishing in a journal, taking into account co-authorship. The analysis provided evidence of a strong quality bias and early view effect. Correcting for these effects, there is in a sample of 6 condensed matter physics journals studied in detail, no sign of a general 'open access advantage' of papers deposited in ArXiv. The study does provide evidence that ArXiv accelerates citation, due to the fact that that ArXiv makes papers earlier available rather than that it makes papers freely available.",
        "published": "2006-11-14T14:48:15Z",
        "link": "http://arxiv.org/abs/cs/0611060v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Low-rank matrix factorization with attributes",
        "authors": [
            "Jacob Abernethy",
            "Francis Bach",
            "Theodoros Evgeniou",
            "Jean-Philippe Vert"
        ],
        "summary": "We develop a new collaborative filtering (CF) method that combines both previously known users' preferences, i.e. standard CF, as well as product/user attributes, i.e. classical function approximation, to predict a given user's interest in a particular product. Our method is a generalized low rank matrix completion problem, where we learn a function whose inputs are pairs of vectors -- the standard low rank matrix completion problem being a special case where the inputs to the function are the row and column indices of the matrix. We solve this generalized matrix completion problem using tensor product kernels for which we also formally generalize standard kernel properties. Benchmark experiments on movie ratings show the advantages of our generalized matrix completion method over the standard matrix completion one with no information about movies or people, as well as over standard multi-task or single task learning methods.",
        "published": "2006-11-24T08:49:30Z",
        "link": "http://arxiv.org/abs/cs/0611124v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Scatter Networks: A New Approach for Analyzing Information Scatter on   the Web",
        "authors": [
            "Lada A. Adamic",
            "Suresh K. Bhavnani",
            "Xiaolin Shi"
        ],
        "summary": "Information on any given topic is often scattered across the web. Previously this scatter has been characterized through the distribution of a set of facts (i.e. pieces of information) across web pages, showing that typically a few pages contain many facts on the topic, while many pages contain just a few. While such approaches have revealed important scatter phenomena, they are lossy in that they conceal how specific facts (e.g. rare facts) occur in specific types of pages (e.g. fact-rich pages). To reveal such regularities, we construct bi-partite networks, consisting of two types of vertices: the facts contained in webpages and the webpages themselves. Such a representation enables the application of a series of network analysis techniques, revealing structural features such as connectivity, robustness, and clustering. We discuss the implications of each of these features to the users' ability to find comprehensive information online. Finally, we compare the bipartite graph structure of webpages and facts with the hyperlink structure between the webpages.",
        "published": "2006-11-26T22:06:23Z",
        "link": "http://arxiv.org/abs/cs/0611131v3",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "A Model of a Trust-based Recommendation System on a Social Network",
        "authors": [
            "Frank E. Walter",
            "Stefano Battiston",
            "Frank Schweitzer"
        ],
        "summary": "In this paper, we present a model of a trust-based recommendation system on a social network. The idea of the model is that agents use their social network to reach information and their trust relationships to filter it. We investigate how the dynamics of trust among agents affect the performance of the system by comparing it to a frequency-based recommendation system. Furthermore, we identify the impact of network density, preference heterogeneity among agents, and knowledge sparseness to be crucial factors for the performance of the system. The system self-organises in a state with performance near to the optimum; the performance on the global level is an emergent property of the system, achieved without explicit coordination from the local interactions of agents.",
        "published": "2006-11-28T16:06:34Z",
        "link": "http://arxiv.org/abs/nlin/0611054v3",
        "categories": [
            "nlin.AO",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "A Novel Bayesian Classifier using Copula Functions",
        "authors": [
            "Saket Sathe"
        ],
        "summary": "A useful method for representing Bayesian classifiers is through \\emph{discriminant functions}. Here, using copula functions, we propose a new model for discriminants. This model provides a rich and generalized class of decision boundaries. These decision boundaries significantly boost the classification accuracy especially for high dimensional feature spaces. We strengthen our analysis through simulation results.",
        "published": "2006-11-29T13:59:31Z",
        "link": "http://arxiv.org/abs/cs/0611150v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Experimental Information and Statistical Modeling of Physical Laws",
        "authors": [
            "Igor Grabec"
        ],
        "summary": "Statistical modeling of physical laws connects experiments with mathematical descriptions of natural phenomena. The modeling is based on the probability density of measured variables expressed by experimental data via a kernel estimator. As an objective kernel the scattering function determined by calibration of the instrument is introduced. This function provides for a new definition of experimental information and redundancy of experimentation in terms of information entropy. The redundancy increases with the number of experiments, while the experimental information converges to a value that describes the complexity of the data. The difference between the redundancy and the experimental information is proposed as the model cost function. From its minimum, a proper number of data in the model is estimated. As an optimal, nonparametric estimator of the relation between measured variables the conditional average extracted from the kernel estimator is proposed. The modeling is demonstrated on noisy chaotic data.",
        "published": "2006-12-05T13:08:24Z",
        "link": "http://arxiv.org/abs/cs/0612027v1",
        "categories": [
            "cs.IT",
            "cs.IR",
            "math.IT"
        ]
    },
    {
        "title": "About the Lifespan of Peer to Peer Networks",
        "authors": [
            "R. Cilibrasi",
            "Z. Lotker",
            "A. Navarra",
            "S. Perennes",
            "P. Vitanyi"
        ],
        "summary": "We analyze the ability of peer to peer networks to deliver a complete file among the peers. Early on we motivate a broad generalization of network behavior organizing it into one of two successive phases. According to this view the network has two main states: first centralized - few sources (roots) hold the complete file, and next distributed - peers hold some parts (chunks) of the file such that the entire network has the whole file, but no individual has it. In the distributed state we study two scenarios, first, when the peers are ``patient'', i.e, do not leave the system until they obtain the complete file; second, peers are ``impatient'' and almost always leave the network before obtaining the complete file.",
        "published": "2006-12-07T16:20:00Z",
        "link": "http://arxiv.org/abs/cs/0612043v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Clustering fetal heart rate tracings by compression",
        "authors": [
            "C. Costa Santos",
            "J. Bernardes",
            "P. Vitanyi",
            "L. Antunes"
        ],
        "summary": "Fetal heart rate (FHR) monitoring, before and during labor, is a very important medical practice in the detection of fetuses in danger. We clustered FHR tracings by compression in order to identify abnormal ones. We use a recently introduced approach based on algorithmic information theory, a theoretical, rigorous and well-studied notion of information content in individual objects. The new method can mine patterns in completely different areas, there are no domain-specific parameters to set, and it does not require specific background knowledge. At the highest level the FHR tracings were clustered according to an unanticipated feature, namely the technology used in signal acquisition. At the lower levels all tracings with abnormal or suspicious patterns were clustered together, independent of the technology used. Moreover, FHR tracings with future poor neonatal outcomes were included in the cluster with other suspicious patterns.",
        "published": "2006-12-07T16:45:15Z",
        "link": "http://arxiv.org/abs/q-bio/0612013v1",
        "categories": [
            "q-bio.TO",
            "cs.CV",
            "cs.IR",
            "q-bio.QM"
        ]
    },
    {
        "title": "Social Networks and Social Information Filtering on Digg",
        "authors": [
            "Kristina Lerman"
        ],
        "summary": "The new social media sites -- blogs, wikis, Flickr and Digg, among others -- underscore the transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. Digg is a social news aggregator which allows users to submit links to, vote on and discuss news stories. Each day Digg selects a handful of stories to feature on its front page. Rather than rely on the opinion of a few editors, Digg aggregates opinions of thousands of its users to decide which stories to promote to the front page.   Digg users can designate other users as ``friends'' and easily track friends' activities: what new stories they submitted, commented on or read. The friends interface acts as a \\emph{social filtering} system, recommending to user stories his or her friends liked or found interesting. By tracking the votes received by newly submitted stories over time, we showed that social filtering is an effective information filtering approach. Specifically, we showed that (a) users tend to like stories submitted by friends and (b) users tend to like stories their friends read and liked. As a byproduct of social filtering, social networks also play a role in promoting stories to Digg's front page, potentially leading to ``tyranny of the minority'' situation where a disproportionate number of front page stories comes from the same small group of interconnected users. Despite this, social filtering is a promising new technology that can be used to personalize and tailor information to individual users: for example, through personal front pages.",
        "published": "2006-12-07T23:38:23Z",
        "link": "http://arxiv.org/abs/cs/0612046v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Unifying Lexicons in view of a Phonological and Morphological Lexical DB",
        "authors": [
            "Federico Calzolari",
            "Michele Mammini",
            "Monica Monachini"
        ],
        "summary": "The present work falls in the line of activities promoted by the European Languguage Resource Association (ELRA) Production Committee (PCom) and raises issues in methods, procedures and tools for the reusability, creation, and management of Language Resources. A two-fold purpose lies behind this experiment. The first aim is to investigate the feasibility, define methods and procedures for combining two Italian lexical resources that have incompatible formats and complementary information into a Unified Lexicon (UL). The adopted strategy and the procedures appointed are described together with the driving criterion of the merging task, where a balance between human and computational efforts is pursued. The coverage of the UL has been maximized, by making use of simple and fast matching procedures. The second aim is to exploit this newly obtained resource for implementing the phonological and morphological layers of the CLIPS lexical database. Implementing these new layers and linking them with the already exisitng syntactic and semantic layers is not a trivial task. The constraints imposed by the model, the impact at the architectural level and the solution adopted in order to make the whole database `speak' efficiently are presented. Advantages vs. disadvantages are discussed.",
        "published": "2006-12-11T14:45:49Z",
        "link": "http://arxiv.org/abs/cs/0612062v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Electronic Laboratory Notebook Assisting Reflectance Spectrometry in   Legal Medicine",
        "authors": [
            "Lioudmila Belenkaia",
            "Michael Bohnert",
            "Andreas W. Liehr"
        ],
        "summary": "Reflectance spectrometry is a fast and reliable method for the characterisation of human skin if the spectra are analysed with respect to a physical model describing the optical properties of human skin. For a field study performed at the Institute of Legal Medicine and the Freiburg Materials Research Center of the University of Freiburg an electronic laboratory notebook has been developed, which assists in the recording, management, and analysis of reflectance spectra. The core of the electronic laboratory notebook is a MySQL database. It is filled with primary data via a web interface programmed in Java, which also enables the user to browse the database and access the results of data analysis. These are carried out by Matlab, Tcl and   Python scripts, which retrieve the primary data from the electronic laboratory notebook, perform the analysis, and store the results in the database for further usage.",
        "published": "2006-12-22T13:29:27Z",
        "link": "http://arxiv.org/abs/cs/0612123v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "cs.IR",
            "H.2.8"
        ]
    },
    {
        "title": "A New Era in Citation and Bibliometric Analyses: Web of Science, Scopus,   and Google Scholar",
        "authors": [
            "Lokman I. Meho",
            "Kiduk Yang"
        ],
        "summary": "Academic institutions, federal agencies, publishers, editors, authors, and librarians increasingly rely on citation analysis for making hiring, promotion, tenure, funding, and/or reviewer and journal evaluation and selection decisions. The Institute for Scientific Information's (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science), however, may no longer be adequate as the only or even the main sources of citations because new databases and tools that allow citation searching are now available. Whether these new databases and tools complement or represent alternatives to Web of Science (WoS) is important to explore. Using a group of 15 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. The paper discusses the strengths and weaknesses of WoS, Scopus, and GS, their overlap and uniqueness, quality and language of the citations, and the implications of the findings for citation analysis. The project involved citation searching for approximately 1,100 scholarly works published by the study group and over 200 works by a test group (an additional 10 faculty members). Overall, more than 10,000 citing and purportedly citing documents were examined. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours.",
        "published": "2006-12-23T14:47:24Z",
        "link": "http://arxiv.org/abs/cs/0612132v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Incremental copying garbage collection for WAM-based Prolog systems",
        "authors": [
            "Ruben Vandeginste",
            "Bart Demoen"
        ],
        "summary": "The design and implementation of an incremental copying heap garbage collector for WAM-based Prolog systems is presented. Its heap layout consists of a number of equal-sized blocks. Other changes to the standard WAM allow these blocks to be garbage collected independently. The independent collection of heap blocks forms the basis of an incremental collecting algorithm which employs copying without marking (contrary to the more frequently used mark&copy or mark&slide algorithms in the context of Prolog). Compared to standard semi-space copying collectors, this approach to heap garbage collection lowers in many cases the memory usage and reduces pause times. The algorithm also allows for a wide variety of garbage collection policies including generational ones. The algorithm is implemented and evaluated in the context of hProlog.",
        "published": "2006-01-02T20:58:56Z",
        "link": "http://arxiv.org/abs/cs/0601003v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Forward slicing of functional logic programs by partial evaluation",
        "authors": [
            "Josep Silva",
            "Germán Vidal"
        ],
        "summary": "Program slicing has been mainly studied in the context of imperative languages, where it has been applied to a wide variety of software engineering tasks, like program understanding, maintenance, debugging, testing, code reuse, etc. This work introduces the first forward slicing technique for declarative multi-paradigm programs which integrate features from functional and logic programming. Basically, given a program and a slicing criterion (a function call in our setting), the computed forward slice contains those parts of the original program which are reachable from the slicing criterion. Our approach to program slicing is based on an extension of (online) partial evaluation. Therefore, it provides a simple way to develop program slicing tools from existing partial evaluators and helps to clarify the relation between both methodologies. A slicing tool for the multi-paradigm language Curry, which demonstrates the usefulness of our approach, has been implemented in Curry itself.",
        "published": "2006-01-06T00:41:22Z",
        "link": "http://arxiv.org/abs/cs/0601013v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.3.4; I.2.2"
        ]
    },
    {
        "title": "Canonical Abstract Syntax Trees",
        "authors": [
            "Antoine Reilles"
        ],
        "summary": "This paper presents Gom, a language for describing abstract syntax trees and generating a Java implementation for those trees. Gom includes features allowing the user to specify and modify the interface of the data structure. These features provide in particular the capability to maintain the internal representation of data in canonical form with respect to a rewrite system. This explicitly guarantees that the client program only manipulates normal forms for this rewrite system, a feature which is only implicitly used in many implementations.",
        "published": "2006-01-06T19:50:29Z",
        "link": "http://arxiv.org/abs/cs/0601019v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Constraint-based verification of abstract models of multitreaded   programs",
        "authors": [
            "Giorgio Delzanno"
        ],
        "summary": "We present a technique for the automated verification of abstract models of multithreaded programs providing fresh name generation, name mobility, and unbounded control.   As high level specification language we adopt here an extension of communication finite-state machines with local variables ranging over an infinite name domain, called TDL programs. Communication machines have been proved very effective for representing communication protocols as well as for representing abstractions of multithreaded software.   The verification method that we propose is based on the encoding of TDL programs into a low level language based on multiset rewriting and constraints that can be viewed as an extension of Petri Nets. By means of this encoding, the symbolic verification procedure developed for the low level language in our previous work can now be applied to TDL programs. Furthermore, the encoding allows us to isolate a decidable class of verification problems for TDL programs that still provide fresh name generation, name mobility, and unbounded control. Our syntactic restrictions are in fact defined on the internal structure of threads: In order to obtain a complete and terminating method, threads are only allowed to have at most one local variable (ranging over an infinite domain of names).",
        "published": "2006-01-10T12:57:46Z",
        "link": "http://arxiv.org/abs/cs/0601037v1",
        "categories": [
            "cs.CL",
            "cs.PL"
        ]
    },
    {
        "title": "Constraint-based automatic verification of abstract models of   multithreaded programs",
        "authors": [
            "Giorgio Delzanno"
        ],
        "summary": "We present a technique for the automated verification of abstract models of multithreaded programs providing fresh name generation, name mobility, and unbounded control.   As high level specification language we adopt here an extension of communication finite-state machines with local variables ranging over an infinite name domain, called TDL programs. Communication machines have been proved very effective for representing communication protocols as well as for representing abstractions of multithreaded software.   The verification method that we propose is based on the encoding of TDL programs into a low level language based on multiset rewriting and constraints that can be viewed as an extension of Petri Nets. By means of this encoding, the symbolic verification procedure developed for the low level language in our previous work can now be applied to TDL programs. Furthermore, the encoding allows us to isolate a decidable class of verification problems for TDL programs that still provide fresh name generation, name mobility, and unbounded control. Our syntactic restrictions are in fact defined on the internal structure of threads: In order to obtain a complete and terminating method, threads are only allowed to have at most one local variable (ranging over an infinite domain of names).",
        "published": "2006-01-10T12:57:55Z",
        "link": "http://arxiv.org/abs/cs/0601038v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Removing Redundant Arguments Automatically",
        "authors": [
            "Maria Alpuente",
            "Santiago Escobar",
            "Salvador Lucas"
        ],
        "summary": "The application of automatic transformation processes during the formal development and optimization of programs can introduce encumbrances in the generated code that programmers usually (or presumably) do not write. An example is the introduction of redundant arguments in the functions defined in the program. Redundancy of a parameter means that replacing it by any expression does not change the result. In this work, we provide methods for the analysis and elimination of redundant arguments in term rewriting systems as a model for the programs that can be written in more sophisticated languages. On the basis of the uselessness of redundant arguments, we also propose an erasure procedure which may avoid wasteful computations while still preserving the semantics (under ascertained conditions). A prototype implementation of these methods has been undertaken, which demonstrates the practicality of our approach.",
        "published": "2006-01-10T13:13:54Z",
        "link": "http://arxiv.org/abs/cs/0601039v1",
        "categories": [
            "cs.PL",
            "D.2.4; F.3.1; F.3.3; I.2.2; I.2.5"
        ]
    },
    {
        "title": "A Constructive Semantic Characterization of Aggregates in ASP",
        "authors": [
            "Tran Cao Son",
            "Enrico Pontelli"
        ],
        "summary": "This technical note describes a monotone and continuous fixpoint operator to compute the answer sets of programs with aggregates. The fixpoint operator relies on the notion of aggregate solution. Under certain conditions, this operator behaves identically to the three-valued immediate consequence operator $\\Phi^{aggr}_P$ for aggregate programs, independently proposed Pelov et al. This operator allows us to closely tie the computational complexity of the answer set checking and answer sets existence problems to the cost of checking a solution of the aggregates in the program. Finally, we relate the semantics described by the operator to other proposals for logic programming with aggregates.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2006-01-13T16:09:36Z",
        "link": "http://arxiv.org/abs/cs/0601051v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.1.6; D.3.1; D.3.2; D.3.3"
        ]
    },
    {
        "title": "Constraint Functional Logic Programming over Finite Domains",
        "authors": [
            "Antonio J. Fernandez",
            "Teresa Hortala-Gonzalez",
            "Fernando Saenz-Perez",
            "Rafael del Vado-Virseda"
        ],
        "summary": "In this paper, we present our proposal to Constraint Functional Logic Programming over Finite Domains (CFLP(FD)) with a lazy functional logic programming language which seamlessly embodies finite domain (FD) constraints. This proposal increases the expressiveness and power of constraint logic programming over finite domains (CLP(FD)) by combining functional and relational notation, curried expressions, higher-order functions, patterns, partial applications, non-determinism, lazy evaluation, logical variables, types, domain variables, constraint composition, and finite domain constraints.   We describe the syntax of the language, its type discipline, and its declarative and operational semantics. We also describe TOY(FD), an implementation for CFLPFD(FD), and a comparison of our approach with respect to CLP(FD) from a programming point of view, showing the new features we introduce. And, finally, we show a performance analysis which demonstrates that our implementation is competitive with respect to existing CLP(FD) systems and that clearly outperforms the closer approach to CFLP(FD).",
        "published": "2006-01-16T11:45:02Z",
        "link": "http://arxiv.org/abs/cs/0601071v1",
        "categories": [
            "cs.PL",
            "D.3.2; D.3.3; F.3.2"
        ]
    },
    {
        "title": "Fast Frequent Querying with Lazy Control Flow Compilation",
        "authors": [
            "Remko Tronçon",
            "Gerda Janssens",
            "Bart Demoen",
            "Henk Vandecasteele"
        ],
        "summary": "Control flow compilation is a hybrid between classical WAM compilation and meta-call, limited to the compilation of non-recursive clause bodies. This approach is used successfully for the execution of dynamically generated queries in an inductive logic programming setting (ILP). Control flow compilation reduces compilation times up to an order of magnitude, without slowing down execution. A lazy variant of control flow compilation is also presented. By compiling code by need, it removes the overhead of compiling unreached code (a frequent phenomenon in practical ILP settings), and thus reduces the size of the compiled code. Both dynamic compilation approaches have been implemented and were combined with query packs, an efficient ILP execution mechanism. It turns out that locality of data and code is important for performance. The experiments reported in the paper show that lazy control flow compilation is superior in both artificial and real life settings.",
        "published": "2006-01-16T13:11:51Z",
        "link": "http://arxiv.org/abs/cs/0601072v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Demand Analysis with Partial Predicates",
        "authors": [
            "Julio Marino",
            "Angel Herranz",
            "Juan Jose Moreno-Navarro"
        ],
        "summary": "In order to alleviate the inefficiencies caused by the interaction of the logic and functional sides, integrated languages may take advantage of \\emph{demand} information -- i.e. knowing in advance which computations are needed and, to which extent, in a particular context. This work studies \\emph{demand analysis} -- which is closely related to \\emph{backwards strictness analysis} -- in a semantic framework of \\emph{partial predicates}, which in turn are constructive realizations of ideals in a domain. This will allow us to give a concise, unified presentation of demand analysis, to relate it to other analyses based on abstract interpretation or strictness logics, some hints for the implementation, and, more important, to prove the soundness of our analysis based on \\emph{demand equations}. There are also some innovative results. One of them is that a set constraint-based analysis has been derived in a stepwise manner using ideas taken from the area of program transformation. The other one is the possibility of using program transformation itself to perform the analysis, specially in those domains of properties where algorithms based on constraint solving are too weak.",
        "published": "2006-02-04T20:22:34Z",
        "link": "http://arxiv.org/abs/cs/0602008v1",
        "categories": [
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "Explaining Constraint Programming",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We discuss here constraint programming (CP) by using a proof-theoretic perspective. To this end we identify three levels of abstraction. Each level sheds light on the essence of CP.   In particular, the highest level allows us to bring CP closer to the computation as deduction paradigm. At the middle level we can explain various constraint propagation algorithms. Finally, at the lowest level we can address the issue of automatic generation and optimization of the constraint propagation algorithms.",
        "published": "2006-02-07T15:19:53Z",
        "link": "http://arxiv.org/abs/cs/0602027v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; F.4.1"
        ]
    },
    {
        "title": "Quantum Predicative Programming",
        "authors": [
            "Anya Tafliovich",
            "E. C. R. Hehner"
        ],
        "summary": "The subject of this work is quantum predicative programming -- the study of developing of programs intended for execution on a quantum computer. We look at programming in the context of formal methods of program development, or programming methodology. Our work is based on probabilistic predicative programming, a recent generalisation of the well-established predicative programming. It supports the style of program development in which each programming step is proven correct as it is made. We inherit the advantages of the theory, such as its generality, simple treatment of recursive programs, time and space complexity, and communication. Our theory of quantum programming provides tools to write both classical and quantum specifications, develop quantum programs that implement these specifications, and reason about their comparative time and space complexity all in the same framework.",
        "published": "2006-02-17T20:43:06Z",
        "link": "http://arxiv.org/abs/quant-ph/0602156v1",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "Compositional Semantics for the Procedural Interpretation of Logic",
        "authors": [
            "M. H. van Emden"
        ],
        "summary": "Semantics of logic programs has been given by proof theory, model theory and by fixpoint of the immediate-consequence operator. If clausal logic is a programming language, then it should also have a compositional semantics. Compositional semantics for programming languages follows the abstract syntax of programs, composing the meaning of a unit by a mathematical operation on the meanings of its constituent units. The procedural interpretation of logic has only yielded an incomplete abstract syntax for logic programs. We complete it and use the result as basis of a compositional semantics. We present for comparison Tarski's algebraization of first-order predicate logic, which is in substance the compositional semantics for his choice of syntax. We characterize our semantics by equivalence with the immediate-consequence operator.",
        "published": "2006-02-28T19:28:31Z",
        "link": "http://arxiv.org/abs/cs/0602098v2",
        "categories": [
            "cs.PL",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "Towards Applicative Relational Programming",
        "authors": [
            "H. Ibrahim",
            "M. H. van Emden"
        ],
        "summary": "Functional programming comes in two flavours: one where ``functions are first-class citizens'' (we call this applicative) and one which is based on equations (we call this declarative). In relational programming clauses play the role of equations. Hence Prolog is declarative. The purpose of this paper is to provide in relational programming a mathematical basis for the relational analog of applicative functional programming. We use the cylindric semantics of first-order logic due to Tarski and provide a new notation for the required cylinders that we call tables. We define the Table/Relation Algebra with operators sufficient to translate Horn clauses into algebraic form. We establish basic mathematical properties of these operators. We show how relations can be first-class citizens, and devise mechanisms for modularity, for local scoping of predicates, and for exporting/importing relations between programs.",
        "published": "2006-02-28T19:53:59Z",
        "link": "http://arxiv.org/abs/cs/0602099v1",
        "categories": [
            "cs.PL",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "A Basic Introduction on Math-Link in Mathematica",
        "authors": [
            "Santanu K. Maiti"
        ],
        "summary": "Starting from the basic ideas of mathematica, we give a detailed description about the way of linking of external programs with mathematica through proper mathlink commands. This article may be quite helpful for the beginners to start with and write programs in mathematica.   In the first part, we illustrate how to use a mathemtica notebook and write a complete program in the notebook. Following with this, we also mention elaborately about the utility of the local and global variables those are very essential for writing a program in mathematica. All the commands needed for doing different mathematical operations can be found with some proper examples in the mathematica book written by Stephen Wolfram \\cite{wolfram}.   In the rest of this article, we concentrate our study on the most significant issue which is the process of linking of {\\em external programs} with mathematica, so-called the mathlink operation. By using proper mathlink commands one can run very tedious jobs efficiently and the operations become extremely fast.",
        "published": "2006-03-01T14:29:19Z",
        "link": "http://arxiv.org/abs/cs/0603005v4",
        "categories": [
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Object-Oriented Modeling of Programming Paradigms",
        "authors": [
            "M. H. van Emden",
            "S. C. Somosan"
        ],
        "summary": "For the right application, the use of programming paradigms such as functional or logic programming can enormously increase productivity in software development. But these powerful paradigms are tied to exotic programming languages, while the management of software development dictates standardization on a single language.   This dilemma can be resolved by using object-oriented programming in a new way. It is conventional to analyze an application by object-oriented modeling. In the new approach, the analysis identifies the paradigm that is ideal for the application; development starts with object-oriented modeling of the paradigm. In this paper we illustrate the new approach by giving examples of object-oriented modeling of dataflow and constraint programming. These examples suggest that it is no longer necessary to embody a programming paradigm in a language dedicated to it.",
        "published": "2006-03-03T05:19:42Z",
        "link": "http://arxiv.org/abs/cs/0603016v2",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.1.5; D.2.2; D.2.3; D.3.3"
        ]
    },
    {
        "title": "Language Support for Optional Functionality",
        "authors": [
            "Joy Mukherjee",
            "Srinidhi Varadarajan"
        ],
        "summary": "We recommend a programming construct - availability check - for programs that need to automatically adjust to presence or absence of segments of code. The idea is to check the existence of a valid definition before a function call is invoked. The syntax is that of a simple 'if' statement. The vision is to enable customization of application functionality through addition or removal of optional components, but without requiring complete re-building. Focus is on C-like compiled procedural languages and UNIX-based systems. Essentially, our approach attempts to combine the flexibility of dynamic libraries with the usability of utility (dependency) libraries. We outline the benefits over prevalent strategies mainly in terms of development complexity, crudely measured as lesser lines of code. We also allude to performance and flexibility facets. A Preliminary implementation and figures from early experimental evaluation are presented.",
        "published": "2006-03-06T00:46:09Z",
        "link": "http://arxiv.org/abs/cs/0603021v1",
        "categories": [
            "cs.PL",
            "cs.OS",
            "cs.SE"
        ]
    },
    {
        "title": "On the tree-transformation power of XSLT",
        "authors": [
            "Wim Janssen",
            "Alexandr Korlyukov",
            "Jan Van den Bussche"
        ],
        "summary": "XSLT is a standard rule-based programming language for expressing transformations of XML data. The language is currently in transition from version 1.0 to 2.0. In order to understand the computational consequences of this transition, we restrict XSLT to its pure tree-transformation capabilities. Under this focus, we observe that XSLT~1.0 was not yet a computationally complete tree-transformation language: every 1.0 program can be implemented in exponential time. A crucial new feature of version~2.0, however, which allows nodesets over temporary trees, yields completeness. We provide a formal operational semantics for XSLT programs, and establish confluence for this semantics.",
        "published": "2006-03-08T13:48:45Z",
        "link": "http://arxiv.org/abs/cs/0603028v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "D.3.1; H.2.3; F.1.1"
        ]
    },
    {
        "title": "Packrat Parsing: Simple, Powerful, Lazy, Linear Time",
        "authors": [
            "Bryan Ford"
        ],
        "summary": "Packrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(k) or LR(k) grammar can be recognized by a packrat parser, in addition to many languages that conventional linear-time algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a linear-time packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives.",
        "published": "2006-03-18T17:49:45Z",
        "link": "http://arxiv.org/abs/cs/0603077v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.PL",
            "D.3.4; D.1.1; F.4.2"
        ]
    },
    {
        "title": "A compositional Semantics for CHR",
        "authors": [
            "Maurizio Gabbrielli",
            "Maria Chiara Meo"
        ],
        "summary": "Constraint Handling Rules (CHR) are a committed-choice declarative language which has been designed for writing constraint solvers. A CHR program consists of multi-headed guarded rules which allow one to rewrite constraints into simpler ones until a solved form is reached.   CHR has received a considerable attention, both from the practical and from the theoretical side. Nevertheless, due the use of multi-headed clauses, there are several aspects of the CHR semantics which have not been clarified yet. In particular, no compositional semantics for CHR has been defined so far.   In this paper we introduce a fix-point semantics which characterizes the input/output behavior of a CHR program and which is and-compositional, that is, which allows to retrieve the semantics of a conjunctive query from the semantics of its components. Such a semantics can be used as a basis to define incremental and modular analysis and verification tools.",
        "published": "2006-03-20T14:17:14Z",
        "link": "http://arxiv.org/abs/cs/0603079v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Efficient Compression of Prolog Programs",
        "authors": [
            "Alin Suciu",
            "Kalman Pusztai"
        ],
        "summary": "We propose a special-purpose class of compression algorithms for efficient compression of Prolog programs. It is a dictionary-based compression method, specially designed for the compression of Prolog code, and therefore we name it PCA (Prolog Compression Algorithm). According to the experimental results this method provides better compression than state-of-the-art general-purpose compression algorithms. Since the algorithm works with Prolog syntactic entities (e.g. atoms, terms, etc.) the implementation of a Prolog prototype is straightforward and very easy to use in any Prolog application that needs compression. Although the algorithm is designed for Prolog programs, the idea can be easily applied for the compression of programs written in other (logic) languages.",
        "published": "2006-03-26T20:27:40Z",
        "link": "http://arxiv.org/abs/cs/0603100v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Prolog Server Pages",
        "authors": [
            "Alin Suciu",
            "Kalman Pusztai",
            "Andrei Vancea"
        ],
        "summary": "Prolog Server Pages (PSP) is a scripting language, based on Prolog, than can be embedded in HTML documents. To run PSP applications one needs a web server, a web browser and a PSP interpreter. The code is executed, by the interpreter, on the server-side (web server) and the output (together with the html code in witch the PSP code is embedded) is sent to the client-side (browser). The current implementation supports Apache Web Server. We implemented an Apache web server module that handles PSP files, and sends the result (an html document) to the client. PSP supports both GET and POST http requests. It also provides methods for working with http cookies.",
        "published": "2006-03-26T20:35:56Z",
        "link": "http://arxiv.org/abs/cs/0603101v1",
        "categories": [
            "cs.NI",
            "cs.PL"
        ]
    },
    {
        "title": "Enhanced Prolog Remote Predicate Call Protocol",
        "authors": [
            "Alin Suciu",
            "Kalman Pusztai",
            "Andrei Diaconu"
        ],
        "summary": "Following the ideas of the Remote Procedure Call model, we have developed a logic programming counterpart, naturally called Prolog Remote Predicate Call (Prolog RPC). The Prolog RPC protocol facilitates the integration of Prolog code in multi-language applications as well as the development of distributed intelligent applications. One use of the protocol's most important uses could be the development of distributed applications that use Prolog at least partially to achieve their goals. Most notably the Distributed Artificial Intelligence (DAI) applications that are suitable for logic programming can profit from the use of the protocol. After proving its usefulness, we went further, developing a new version of the protocol, making it more reliable and extending its functionality. Because it has a new syntax and the new set of commands, we call this version Enhanced Prolog Remote Procedure Call. This paper describes the new features and modifications this second version introduced.",
        "published": "2006-03-26T20:44:39Z",
        "link": "http://arxiv.org/abs/cs/0603102v1",
        "categories": [
            "cs.NI",
            "cs.PL"
        ]
    },
    {
        "title": "Demand-driven Inlining in a Region-based Optimizer for ILP Architectures",
        "authors": [
            "Thomas P. Way",
            "Lori L. Pollock"
        ],
        "summary": "Region-based compilation repartitions a program into more desirable compilation units using profiling information and procedure inlining to enable region formation analysis. Heuristics play a key role in determining when it is most beneficial to inline procedures during region formation. An ILP optimizing compiler using a region-based approach restructures a program to better reflect dynamic behavior and increase interprocedural optimization and scheduling opportunities. This paper presents an interprocedural compilation technique which performs procedure inlining on-demand, rather than as a separate phase, to improve the ability of a region-based optimizer to control code growth, compilation time and memory usage while improving performance. The interprocedural region formation algorithm utilizes a demand-driven, heuristics-guided approach to inlining, restructuring an input program into interprocedural regions. Experimental results are presented to demonstrate the impact of the algorithm and several inlining heuristics upon a number of traditional and novel compilation characteristics within a region-based ILP compiler and simulator.",
        "published": "2006-04-11T04:15:39Z",
        "link": "http://arxiv.org/abs/cs/0604043v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.3.4"
        ]
    },
    {
        "title": "Continuations, proofs and tests",
        "authors": [
            "Stefano Guerrini",
            "Andrea Masini"
        ],
        "summary": "Continuation Passing Style (CPS) is one of the most important issues in the field of functional programming languages, and the quest for a primitive notion of types for continuation is still open. Starting from the notion of ``test'' proposed by Girard, we develop a notion of test for intuitionistic logic. We give a complete deductive system for tests and we show that it is good to deal with ``continuations''. In particular, in the proposed system it is possible to work with Call by Value and Call by Name translations in a uniform way.",
        "published": "2006-05-09T20:11:27Z",
        "link": "http://arxiv.org/abs/cs/0605043v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Mathematica: A System of Computer Programs",
        "authors": [
            "Santanu K. Maiti"
        ],
        "summary": "Starting from the basic level of mathematica here we illustrate how to use a mathematica notebook and write a program in the notebook. Next, we investigate elaborately the way of linking of external programs with mathematica, so-called the mathlink operation. Using this technique we can run very tedious jobs quite efficiently, and the operations become extremely fast. Sometimes it is quite desirable to run jobs in background of a computer which can take considerable amount of time to finish, and this allows us to do work on other tasks, while keeping the jobs running. The way of running jobs, written in a mathematica notebook, in background is quite different from the conventional methods i.e., the techniques for the programs written in other languages like C, C++, F77, F90, F95, etc. To illustrate it, in the present article we study how to create a mathematica batch-file from a mathematica notebook and run it in the background. Finally, we explore the most significant issue of this article. Here we describe the basic ideas for parallelizing a mathematica program by sharing its independent parts into all other remote computers available in the network. Doing the parallelization, we can perform large computational operations within a very short period of time, and therefore, the efficiency of the numerical works can be achieved. Parallel computation supports any version of mathematica and it also works significantly well even if different versions of mathematica are installed in different computers. All the operations studied in this article run under any supported operating system like Unix, Windows, Macintosh, etc. For the sake of our illustrations, here we concentrate all the discussions only for the Unix based operating system.",
        "published": "2006-05-20T05:43:55Z",
        "link": "http://arxiv.org/abs/cs/0605090v4",
        "categories": [
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Parsing Transformative LR(1) Languages",
        "authors": [
            "Blake Hegerle"
        ],
        "summary": "We consider, as a means of making programming languages more flexible and powerful, a parsing algorithm in which the parser may freely modify the grammar while parsing. We are particularly interested in a modification of the canonical LR(1) parsing algorithm in which, after the reduction of certain productions, we examine the source sentence seen so far to determine the grammar to use to continue parsing. A naive modification of the canonical LR(1) parsing algorithm along these lines cannot be guaranteed to halt; as a result, we develop a test which examines the grammar as it changes, stopping the parse if the grammar changes in a way that would invalidate earlier assumptions made by the parser. With this test in hand, we can develop our parsing algorithm and prove that it is correct. That being done, we turn to earlier, related work; the idea of programming languages which can be extended to include new syntactic constructs has existed almost as long as the idea of high-level programming languages. Early efforts to construct such a programming language were hampered by an immature theory of formal languages. More recent efforts to construct transformative languages relied either on an inefficient chain of source-to-source translators; or they have a defect, present in our naive parsing algorithm, in that they cannot be known to halt. The present algorithm does not have these undesirable properties, and as such, it should prove a useful foundation for a new kind of programming language.",
        "published": "2006-05-24T19:09:39Z",
        "link": "http://arxiv.org/abs/cs/0605104v2",
        "categories": [
            "cs.PL",
            "D.3.2; D.3.4; F.4.2; F.4.3"
        ]
    },
    {
        "title": "Modeling Aspect Mechanisms: A Top-Down Approach",
        "authors": [
            "Sergei Kojarski",
            "David H. Lorenz"
        ],
        "summary": "A plethora of diverse aspect mechanisms exist today, all of which integrate concerns into artifacts that exhibit crosscutting structure. What we lack and need is a characterization of the design space that these aspect mechanisms inhabit and a model description of their weaving processes. A good design space representation provides a common framework for understanding and evaluating existing mechanisms. A well-understood model of the weaving process can guide the implementor of new aspect mechanisms. It can guide the designer when mechanisms implementing new kinds of weaving are needed. It can also help teach aspect-oriented programming (AOP). In this paper we present and evaluate such a model of the design space for aspect mechanisms and their weaving processes. We model weaving, at an abstract level, as a concern integration process. We derive a weaving process model (WPM) top-down, differentiating a reactive from a nonreactive process. The model provides an in-depth explanation of the key subpro existing aspect mechanisms.",
        "published": "2006-06-01T02:14:16Z",
        "link": "http://arxiv.org/abs/cs/0606003v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.10; D.1.5; D.3.2"
        ]
    },
    {
        "title": "A synchronous pi-calculus",
        "authors": [
            "Roberto Amadio"
        ],
        "summary": "The SL synchronous programming model is a relaxation of the Esterel synchronous model where the reaction to the absence of a signal within an instant can only happen at the next instant. In previous work, we have revisited the SL synchronous programming model. In particular, we have discussed an alternative design of the model including thread spawning and recursive definitions, introduced a CPS translation to a tail recursive form, and proposed a notion of bisimulation equivalence. In the present work, we extend the tail recursive model with first-order data types obtaining a non-deterministic synchronous model whose complexity is comparable to the one of the pi-calculus. We show that our approach to bisimulation equivalence can cope with this extension and in particular that labelled bisimulation can be characterised as a contextual bisimulation.",
        "published": "2006-06-05T16:46:41Z",
        "link": "http://arxiv.org/abs/cs/0606019v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Parallel Evaluation of Mathematica Programs in Remote Computers   Available in Network",
        "authors": [
            "Santanu K. Maiti"
        ],
        "summary": "Mathematica is a powerful application package for doing mathematics and is used almost in all branches of science. It has widespread applications ranging from quantum computation, statistical analysis, number theory, zoology, astronomy, and many more. Mathematica gives a rich set of programming extensions to its end-user language, and it permits us to write programs in procedural, functional, or logic (rule-based) style, or a mixture of all three. For tasks requiring interfaces to the external environment, mathematica provides mathlink, which allows us to communicate mathematica programs with external programs written in C, C++, F77, F90, F95, Java, or other languages. It has also extensive capabilities for editing graphics, equations, text, etc.   In this article, we explore the basic mechanisms of parallelization of a mathematica program by sharing different parts of the program into all other computers available in the network. Doing the parallelization, we can perform large computational operations within a very short period of time, and therefore, the efficiency of the numerical works can be achieved. Parallel computation supports any version of mathematica and it also works as well even if different versions of mathematica are installed in different computers. The whole operation can run under any supported operating system like Unix, Windows, Macintosh, etc. Here we focus our study only for the Unix based operating system, but this method works as well for all other cases.",
        "published": "2006-06-06T12:11:23Z",
        "link": "http://arxiv.org/abs/cs/0606023v3",
        "categories": [
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Relational Parametricity and Control",
        "authors": [
            "Masahito Hasegawa"
        ],
        "summary": "We study the equational theory of Parigot's second-order &lambda;&mu;-calculus in connection with a call-by-name continuation-passing style (CPS) translation into a fragment of the second-order &lambda;-calculus. It is observed that the relational parametricity on the target calculus induces a natural notion of equivalence on the &lambda;&mu;-terms. On the other hand, the unconstrained relational parametricity on the &lambda;&mu;-calculus turns out to be inconsistent with this CPS semantics. Following these facts, we propose to formulate the relational parametricity on the &lambda;&mu;-calculus in a constrained way, which might be called ``focal parametricity''.",
        "published": "2006-06-15T11:45:01Z",
        "link": "http://arxiv.org/abs/cs/0606072v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "On Typechecking Top-Down XML Tranformations: Fixed Input or Output   Schemas",
        "authors": [
            "Wim Martens",
            "Frank Neven",
            "Marc Gyssens"
        ],
        "summary": "Typechecking consists of statically verifying whether the output of an XML transformation always conforms to an output type for documents satisfying a given input type. In this general setting, both the input and output schema as well as the transformation are part of the input for the problem. However, scenarios where the input or output schema can be considered to be fixed, are quite common in practice. In the present work, we investigate the computational complexity of the typechecking problem in the latter setting.",
        "published": "2006-06-22T09:46:48Z",
        "link": "http://arxiv.org/abs/cs/0606094v1",
        "categories": [
            "cs.DB",
            "cs.PL"
        ]
    },
    {
        "title": "Toward Functionality Oriented Programming",
        "authors": [
            "Chengpu Wang"
        ],
        "summary": "The concept of functionality oriented programming is proposed, and some of its aspects are discussed, such as: (1) implementation independent basic types and generic collection types; (2) syntax requirements and recommendations for implementation independence; (3) unified documentation and code; (4) cross-module interface; and (5) cross-language program making scheme. A prototype example is given to demonstrate functionality oriented programming.",
        "published": "2006-06-24T18:51:54Z",
        "link": "http://arxiv.org/abs/cs/0606102v3",
        "categories": [
            "cs.PL",
            "cs.HC"
        ]
    },
    {
        "title": "A common framework for aspect mining based on crosscutting concern sorts",
        "authors": [
            "Marius Marin",
            "Leon Moonen",
            "Arie van Deursen"
        ],
        "summary": "The increasing number of aspect mining techniques proposed in literature calls for a methodological way of comparing and combining them in order to assess, and improve on, their quality. This paper addresses this situation by proposing a common framework based on crosscutting concern sorts which allows for consistent assessment, comparison and combination of aspect mining techniques. The framework identifies a set of requirements that ensure homogeneity in formulating the mining goals, presenting the results and assessing their quality.   We demonstrate feasibility of the approach by retrofitting an existing aspect mining technique to the framework, and by using it to design and implement two new mining techniques. We apply the three techniques to a known aspect mining benchmark and show how they can be consistently assessed and combined to increase the quality of the results. The techniques and combinations are implemented in FINT, our publicly available free aspect mining tool.",
        "published": "2006-06-27T12:40:04Z",
        "link": "http://arxiv.org/abs/cs/0606113v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "Formalizing typical crosscutting concerns",
        "authors": [
            "Marius Marin"
        ],
        "summary": "We present a consistent system for referring crosscutting functionality, relating crosscutting concerns to specific implementation idioms, and formalizing their underlying relations through queries. The system is based on generic crosscutting concerns that we organize and describe in a catalog.   We have designed and implemented a tool support for querying source code for instances of the proposed generic concerns and organizing them in composite concern models. The composite concern model adds a new dimension to the dominant decomposition of the system for describing and making explicit source code relations specific to crosscutting concerns implementations.   We use the proposed approach to describe crosscutting concerns in design patterns and apply the tool to an opensource system (JHotDraw).",
        "published": "2006-06-29T20:04:25Z",
        "link": "http://arxiv.org/abs/cs/0606125v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "Applying and Combining Three Different Aspect Mining Techniques",
        "authors": [
            "Mariano Ceccato",
            "Marius Marin",
            "Kim Mens",
            "Leon Moonen",
            "Paolo Tonella",
            "Tom Tourwe"
        ],
        "summary": "Understanding a software system at source-code level requires understanding the different concerns that it addresses, which in turn requires a way to identify these concerns in the source code. Whereas some concerns are explicitly represented by program entities (like classes, methods and variables) and thus are easy to identify, crosscutting concerns are not captured by a single program entity but are scattered over many program entities and are tangled with the other concerns. Because of their crosscutting nature, such crosscutting concerns are difficult to identify, and reduce the understandability of the system as a whole.   In this paper, we report on a combined experiment in which we try to identify crosscutting concerns in the JHotDraw framework automatically. We first apply three independently developed aspect mining techniques to JHotDraw and evaluate and compare their results. Based on this analysis, we present three interesting combinations of these three techniques, and show how these combinations provide a more complete coverage of the detected concerns as compared to the original techniques individually. Our results are a first step towards improving the understandability of a system that contains crosscutting concerns, and can be used as a basis for refactoring the identified crosscutting concerns into aspects.",
        "published": "2006-07-02T17:16:37Z",
        "link": "http://arxiv.org/abs/cs/0607006v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "An Analysis of Arithmetic Constraints on Integer Intervals",
        "authors": [
            "Krzysztof R. Apt",
            "Peter Zoeteweij"
        ],
        "summary": "Arithmetic constraints on integer intervals are supported in many constraint programming systems. We study here a number of approaches to implement constraint propagation for these constraints. To describe them we introduce integer interval arithmetic. Each approach is explained using appropriate proof rules that reduce the variable domains. We compare these approaches using a set of benchmarks. For the most promising approach we provide results that characterize the effect of constraint propagation. This is a full version of our earlier paper, cs.PL/0403016.",
        "published": "2006-07-06T10:09:43Z",
        "link": "http://arxiv.org/abs/cs/0607016v2",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.2; D.3.3"
        ]
    },
    {
        "title": "PALS: Efficient Or-Parallelism on Beowulf Clusters",
        "authors": [
            "Enrico Pontelli",
            "Karen Villaverde",
            "Hai-Feng Guo",
            "Gopal Gupta"
        ],
        "summary": "This paper describes the development of the PALS system, an implementation of Prolog capable of efficiently exploiting or-parallelism on distributed-memory platforms--specifically Beowulf clusters. PALS makes use of a novel technique, called incremental stack-splitting. The technique proposed builds on the stack-splitting approach, previously described by the authors and experimentally validated on shared-memory systems, which in turn is an evolution of the stack-copying method used in a variety of parallel logic and constraint systems--e.g., MUSE, YAP, and Penny. The PALS system is the first distributed or-parallel implementation of Prolog based on the stack-splitting method ever realized. The results presented confirm the superiority of this method as a simple yet effective technique to transition from shared-memory to distributed-memory systems. PALS extends stack-splitting by combining it with incremental copying; the paper provides a description of the implementation of PALS, including details of how distributed scheduling is handled. We also investigate methodologies to effectively support order-sensitive predicates (e.g., side-effects) in the context of the stack-splitting scheme. Experimental results obtained from running PALS on both Shared Memory and Beowulf systems are presented and analyzed.",
        "published": "2006-07-09T19:56:05Z",
        "link": "http://arxiv.org/abs/cs/0607040v1",
        "categories": [
            "cs.DC",
            "cs.PL"
        ]
    },
    {
        "title": "De l'oprateur de trace dans les jeux de Conway",
        "authors": [
            "Nicolas Tabareau"
        ],
        "summary": "In this report, we propose a game semantics model of intuitionistic linear logic with a notion of brackets and a trace operator. This model is a revised version of Conway games augmented with an algebraicly defined gain which enable to describe well bracketed strategies. We then show the existence of a free cocommutative comonoid in the category of Conway. To conclude, we propose a new model of an Algol-like language with higher-order using the presence of a trace operator in our model to describe the memorial aspect of the language.",
        "published": "2006-07-19T12:43:02Z",
        "link": "http://arxiv.org/abs/math/0607462v1",
        "categories": [
            "math.CT",
            "cs.PL"
        ]
    },
    {
        "title": "Deriving Escape Analysis by Abstract Interpretation: Proofs of results",
        "authors": [
            "Patricia M. Hill",
            "Fausto Spoto"
        ],
        "summary": "Escape analysis of object-oriented languages approximates the set of objects which do not escape from a given context. If we take a method as context, the non-escaping objects can be allocated on its activation stack; if we take a thread, Java synchronisation locks on such objects are not needed. In this paper, we formalise a basic escape domain e as an abstract interpretation of concrete states, which we then refine into an abstract domain er which is more concrete than e and, hence, leads to a more precise escape analysis than e. We provide optimality results for both e and er, in the form of Galois insertions from the concrete to the abstract domains and of optimal abstract operations. The Galois insertion property is obtained by restricting the abstract domains to those elements which do not contain garbage, by using an abstract garbage collector. Our implementation of er is hence an implementation of a formally correct escape analyser, able to detect the stack allocatable creation points of Java (bytecode) applications.   This report contains the proofs of results of a paper with the same title and authors and to be published in the Journal \"Higher-Order Symbolic Computation\".",
        "published": "2006-07-24T09:24:07Z",
        "link": "http://arxiv.org/abs/cs/0607101v2",
        "categories": [
            "cs.PL",
            "F.2"
        ]
    },
    {
        "title": "Towards \"Propagation = Logic + Control\"",
        "authors": [
            "Sebastian Brand",
            "Roland H. C. Yap"
        ],
        "summary": "Constraint propagation algorithms implement logical inference. For efficiency, it is essential to control whether and in what order basic inference steps are taken. We provide a high-level framework that clearly differentiates between information needed for controlling propagation versus that needed for the logical semantics of complex constraints composed from primitive ones. We argue for the appropriateness of our controlled propagation framework by showing that it captures the underlying principles of manually designed propagation algorithms, such as literal watching for unit clause propagation and the lexicographic ordering constraint. We provide an implementation and benchmark results that demonstrate the practicality and efficiency of our framework.",
        "published": "2006-08-03T02:41:20Z",
        "link": "http://arxiv.org/abs/cs/0608015v1",
        "categories": [
            "cs.PL",
            "cs.AI"
        ]
    },
    {
        "title": "ACD Term Rewriting",
        "authors": [
            "Gregory J. Duck",
            "Peter J. Stuckey",
            "Sebastian Brand"
        ],
        "summary": "We introduce Associative Commutative Distributive Term Rewriting (ACDTR), a rewriting language for rewriting logical formulae. ACDTR extends AC term rewriting by adding distribution of conjunction over other operators. Conjunction is vital for expressive term rewriting systems since it allows us to require that multiple conditions hold for a term rewriting rule to be used. ACDTR uses the notion of a \"conjunctive context\", which is the conjunction of constraints that must hold in the context of a term, to enable the programmer to write very expressive and targeted rewriting rules. ACDTR can be seen as a general logic programming language that extends Constraint Handling Rules and AC term rewriting. In this paper we define the semantics of ACDTR and describe our prototype implementation.",
        "published": "2006-08-03T02:55:16Z",
        "link": "http://arxiv.org/abs/cs/0608016v1",
        "categories": [
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "On Quasi-Interpretations, Blind Abstractions and Implicit Complexity",
        "authors": [
            "Patrick Baillot",
            "Ugo Dal Lago",
            "Jean-Yves Moyen"
        ],
        "summary": "Quasi-interpretations are a technique to guarantee complexity bounds on first-order functional programs: with termination orderings they give in particular a sufficient condition for a program to be executable in polynomial time, called here the P-criterion. We study properties of the programs satisfying the P-criterion, in order to better understand its intensional expressive power. Given a program on binary lists, its blind abstraction is the nondeterministic program obtained by replacing lists by their lengths (natural numbers). A program is blindly polynomial if its blind abstraction terminates in polynomial time. We show that all programs satisfying a variant of the P-criterion are in fact blindly polynomial. Then we give two extensions of the P-criterion: one by relaxing the termination ordering condition, and the other one (the bounded value property) giving a necessary and sufficient condition for a program to be polynomial time executable, with memoisation.",
        "published": "2006-08-06T07:20:45Z",
        "link": "http://arxiv.org/abs/cs/0608030v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Resource Usage Analysis for the Pi-Calculus",
        "authors": [
            "Naoki Kobayashi",
            "Kohei Suenaga",
            "Lucian Wischik"
        ],
        "summary": "We propose a type-based resource usage analysis for the &#960;-calculus extended with resource creation/access primitives. The goal of the resource usage analysis is to statically check that a program accesses resources such as files and memory in a valid manner. Our type system is an extension of previous behavioral type systems for the &#960;-calculus, and can guarantee the safety property that no invalid access is performed, as well as the property that necessary accesses (such as the close operation for a file) are eventually performed unless the program diverges. A sound type inference algorithm for the type system is also developed to free the programmer from the burden of writing complex type annotations. Based on the algorithm, we have implemented a prototype resource usage analyzer for the &#960;-calculus. To the authors' knowledge, ours is the first type-based resource usage analysis that deals with an expressive concurrent language like the pi-calculus.",
        "published": "2006-08-07T05:22:42Z",
        "link": "http://arxiv.org/abs/cs/0608035v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.1; D.3.1"
        ]
    },
    {
        "title": "Modules over Monads and Linearity",
        "authors": [
            "André Hirschowitz",
            "Marco Maggesi"
        ],
        "summary": "Inspired by the classical theory of modules over a monoid, we give a first account of the natural notion of module over a monad. The associated notion of morphism of left modules (\"Linear\" natural transformations) captures an important property of compatibility with substitution, in the heterogeneous case where \"terms\" and variables therein could be of different types as well as in the homogeneous case. In this paper, we present basic constructions of modules and we show examples concerning in particular abstract syntax and lambda-calculus.",
        "published": "2006-08-11T15:05:27Z",
        "link": "http://arxiv.org/abs/cs/0608051v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Heap Reference Analysis Using Access Graphs",
        "authors": [
            "Uday Khedker",
            "Amitabha Sanyal",
            "Amey Karkare"
        ],
        "summary": "Despite significant progress in the theory and practice of program analysis, analysing properties of heap data has not reached the same level of maturity as the analysis of static and stack data. The spatial and temporal structure of stack and static data is well understood while that of heap data seems arbitrary and is unbounded. We devise bounded representations which summarize properties of the heap data. This summarization is based on the structure of the program which manipulates the heap. The resulting summary representations are certain kinds of graphs called access graphs. The boundedness of these representations and the monotonicity of the operations to manipulate them make it possible to compute them through data flow analysis.   An important application which benefits from heap reference analysis is garbage collection, where currently liveness is conservatively approximated by reachability from program variables. As a consequence, current garbage collectors leave a lot of garbage uncollected, a fact which has been confirmed by several empirical studies. We propose the first ever end-to-end static analysis to distinguish live objects from reachable objects. We use this information to make dead objects unreachable by modifying the program. This application is interesting because it requires discovering data flow information representing complex semantics. In particular, we discover four properties of heap data: liveness, aliasing, availability, and anticipability. Together, they cover all combinations of directions of analysis (i.e. forward and backward) and confluence of information (i.e. union and intersection). Our analysis can also be used for plugging memory leaks in C/C++ languages.",
        "published": "2006-08-28T11:15:00Z",
        "link": "http://arxiv.org/abs/cs/0608104v3",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.4; F.3.2"
        ]
    },
    {
        "title": "FOSS-Based Grid Computing",
        "authors": [
            "A. Mani"
        ],
        "summary": "In this expository paper we will be primarily concerned with core aspects of Grids and Grid computing using free and open-source software with some emphasis on utility computing. It is based on a technical report entitled 'Grid-Computing Using GNU/Linux' by the present author. This article was written in 2006 and should be of historical interest.",
        "published": "2006-08-30T18:59:33Z",
        "link": "http://arxiv.org/abs/cs/0608122v4",
        "categories": [
            "cs.DC",
            "cs.PL",
            "C.2.4; D.1.3"
        ]
    },
    {
        "title": "Decidability of Type-checking in the Calculus of Algebraic Constructions   with Size Annotations",
        "authors": [
            "Frédéric Blanqui"
        ],
        "summary": "Since Val Tannen's pioneer work on the combination of simply-typed lambda-calculus and first-order rewriting (LICS'88), many authors have contributed to this subject by extending it to richer typed lambda-calculi and rewriting paradigms, culminating in calculi like the Calculus of Algebraic Constructions. These works provide theoretical foundations for type-theoretic proof assistants where functions and predicates are defined by oriented higher-order equations. This kind of definitions subsumes induction-based definitions, is easier to write and provides more automation. On the other hand, checking that user-defined rewrite rules are strongly normalizing and confluent, and preserve the decidability of type-checking when combined with beta-reduction, is more difficult. Most termination criteria rely on the term structure. In a previous work, we extended to dependent types and higher-order rewriting, the notion of ``sized types'' studied by several authors in the simpler framework of ML-like languages, and proved that it preserves strong normalization. The main contribution of the present paper is twofold. First, we prove that, in the Calculus of Algebraic Constructions with size annotations, the problems of type inference and type-checking are decidable, provided that the sets of constraints generated by size annotations are satisfiable and admit most general solutions. Second, we prove the later properties for a size algebra rich enough for capturing usual induction-based definitions and much more.",
        "published": "2006-08-31T14:26:20Z",
        "link": "http://arxiv.org/abs/cs/0608125v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "On the confluence of lambda-calculus with conditional rewriting",
        "authors": [
            "Frédéric Blanqui",
            "Claude Kirchner",
            "Colin Riba"
        ],
        "summary": "The confluence of untyped lambda-calculus with unconditional rewriting has already been studied in various directions. In this paper, we investigate the confluence of lambda-calculus with conditional rewriting and provide general results in two directions. First, when conditional rules are algebraic. This extends results of Muller and Dougherty for unconditional rewriting. Two cases are considered, whether beta-reduction is allowed or not in the evaluation of conditions. Moreover, Dougherty's result is improved from the assumption of strongly normalizing beta-reduction to weakly normalizing beta-reduction. We also provide examples showing that outside these conditions, modularity of confluence is difficult to achieve. Second, we go beyond the algebraic framework and get new confluence results using an extended notion of orthogonality that takes advantage of the conditional part of rewrite rules.",
        "published": "2006-09-01T12:56:02Z",
        "link": "http://arxiv.org/abs/cs/0609002v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Nominal Logic Programming",
        "authors": [
            "James Cheney",
            "Christian Urban"
        ],
        "summary": "Nominal logic is an extension of first-order logic which provides a simple foundation for formalizing and reasoning about abstract syntax modulo consistent renaming of bound names (that is, alpha-equivalence). This article investigates logic programming based on nominal logic. We describe some typical nominal logic programs, and develop the model-theoretic, proof-theoretic, and operational semantics of such programs. Besides being of interest for ensuring the correct behavior of implementations, these results provide a rigorous foundation for techniques for analysis and reasoning about nominal logic programs, as we illustrate via examples.",
        "published": "2006-09-12T09:32:23Z",
        "link": "http://arxiv.org/abs/cs/0609062v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Analysis of Equality Relationships for Imperative Programs",
        "authors": [
            "P. Emelyanov"
        ],
        "summary": "In this article, we discuss a flow--sensitive analysis of equality relationships for imperative programs. We describe its semantic domains, general purpose operations over abstract computational states (term evaluation and identification, semantic completion, widening operator, etc.) and semantic transformers corresponding to program constructs. We summarize our experiences from the last few years concerning this analysis and give attention to applications of analysis of automatically generated code. Among other illustrating examples, we consider a program for which the analysis diverges without a widening operator and results of analyzing residual programs produced by some automatic partial evaluator. An example of analysis of a program generated by this evaluator is given.",
        "published": "2006-09-16T12:22:15Z",
        "link": "http://arxiv.org/abs/cs/0609092v1",
        "categories": [
            "cs.PL",
            "D.3.1; F.3.2"
        ]
    },
    {
        "title": "On Verifying Complex Properties using Symbolic Shape Analysis",
        "authors": [
            "Thomas Wies",
            "Viktor Kuncak",
            "Karen Zee",
            "Andreas Podelski",
            "Martin Rinard"
        ],
        "summary": "One of the main challenges in the verification of software systems is the analysis of unbounded data structures with dynamic memory allocation, such as linked data structures and arrays. We describe Bohne, a new analysis for verifying data structures. Bohne verifies data structure operations and shows that 1) the operations preserve data structure invariants and 2) the operations satisfy their specifications expressed in terms of changes to the set of objects stored in the data structure. During the analysis, Bohne infers loop invariants in the form of disjunctions of universally quantified Boolean combinations of formulas. To synthesize loop invariants of this form, Bohne uses a combination of decision procedures for Monadic Second-Order Logic over trees, SMT-LIB decision procedures (currently CVC Lite), and an automated reasoner within the Isabelle interactive theorem prover. This architecture shows that synthesized loop invariants can serve as a useful communication mechanism between different decision procedures. Using Bohne, we have verified operations on data structures such as linked lists with iterators and back pointers, trees with and without parent pointers, two-level skip lists, array data structures, and sorted lists. We have deployed Bohne in the Hob and Jahob data structure analysis systems, enabling us to combine Bohne with analyses of data structure clients and apply it in the context of larger programs. This report describes the Bohne algorithm as well as techniques that Bohne uses to reduce the ammount of annotations and the running time of the analysis.",
        "published": "2006-09-18T14:52:16Z",
        "link": "http://arxiv.org/abs/cs/0609104v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "Memory and compiler optimizations for low-power and -energy",
        "authors": [
            "Olivier Zendra"
        ],
        "summary": "Embedded systems become more and more widespread, especially autonomous ones, and clearly tend to be ubiquitous. In such systems, low-power and low-energy usage get ever more crucial. Furthermore, these issues also become paramount in (massively) multi-processors systems, either in one machine or more widely in a grid. The various problems faced pertain to autonomy, power supply possibilities, thermal dissipation, or even sheer energy cost. Although it has since long been studied in harware, energy optimization is more recent in software. In this paper, we thus aim at raising awareness to low-power and low-energy issues in the language and compilation community. We thus broadly but briefly survey techniques and solutions to this energy issue, focusing on a few specific aspects in the context of compiler optimizations and memory management.",
        "published": "2006-10-05T17:58:36Z",
        "link": "http://arxiv.org/abs/cs/0610028v1",
        "categories": [
            "cs.PL",
            "cs.PF"
        ]
    },
    {
        "title": "A type-based termination criterion for dependently-typed higher-order   rewrite systems",
        "authors": [
            "Frederic Blanqui"
        ],
        "summary": "Several authors devised type-based termination criteria for ML-like languages allowing non-structural recursive calls. We extend these works to general rewriting and dependent types, hence providing a powerful termination criterion for the combination of rewriting and beta-reduction in the Calculus of Constructions.",
        "published": "2006-10-11T12:47:46Z",
        "link": "http://arxiv.org/abs/cs/0610062v1",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Complexity of Data Flow Analysis for Non-Separable Frameworks",
        "authors": [
            "Bageshri Karkare",
            "Uday Khedker"
        ],
        "summary": "The complexity of round robin method of intraprocedural data flow analysis is measured in number of iterations over the control flow graph. Existing complexity bounds realistically explain the complexity of only Bit-vector frameworks which are separable. In this paper we define the complexity bounds for non-separable frameworks by quantifying the interdependences among the data flow information of program entities using an Entity Dependence Graph.",
        "published": "2006-10-30T08:39:04Z",
        "link": "http://arxiv.org/abs/cs/0610164v2",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Efficient constraint propagation engines",
        "authors": [
            "Christian Schulte",
            "Peter J. Stuckey"
        ],
        "summary": "This paper presents a model and implementation techniques for speeding up constraint propagation. Three fundamental approaches to improving constraint propagation based on propagators as implementations of constraints are explored: keeping track of which propagators are at fixpoint, choosing which propagator to apply next, and how to combine several propagators for the same constraint. We show how idempotence reasoning and events help track fixpoints more accurately. We improve these methods by using them dynamically (taking into account current domains to improve accuracy). We define priority-based approaches to choosing a next propagator and show that dynamic priorities can improve propagation. We illustrate that the use of multiple propagators for the same constraint can be advantageous with priorities, and introduce staged propagators that combine the effects of multiple propagators with priorities for greater efficiency.",
        "published": "2006-11-02T09:55:07Z",
        "link": "http://arxiv.org/abs/cs/0611009v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.2; D.3.3"
        ]
    },
    {
        "title": "Interactive Problem Solving in Prolog",
        "authors": [
            "Erik Braun",
            "Rainer Luetticke",
            "Ingo Gloeckner",
            "Hermann Helbig"
        ],
        "summary": "This paper presents an environment for solving Prolog problems which has been implemented as a module for the virtual laboratory VILAB. During the problem solving processes the learners get fast adaptive feedback. As a result analysing the learner's actions the system suggests the use of suitable auxiliary predicates which will also be checked for proper implementation. The focus of the environment has been set on robustness and the integration in VILAB.",
        "published": "2006-11-03T14:34:57Z",
        "link": "http://arxiv.org/abs/cs/0611014v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "cs.PL",
            "K.3.1; K.3.2; D.1.6; I.2.6; H.5.2"
        ]
    },
    {
        "title": "Effectiveness of Garbage Collection in MIT/GNU Scheme",
        "authors": [
            "Amey Karkare",
            "Amitabha Sanyal",
            "Uday Khedker"
        ],
        "summary": "Scheme uses garbage collection for heap memory management. Ideally, garbage collectors should be able to reclaim all dead objects, i.e. objects that will not be used in future. However, garbage collectors collect only those dead objects that are not reachable from any program variable. Dead objects that are reachable from program variables are not reclaimed.   In this paper we describe our experiments to measure the effectiveness of garbage collection in MIT/GNU Scheme. We compute the drag time of objects, i.e. the time for which an object remains in heap memory after its last use. The number of dead objects and the drag time together indicate opportunities for improving garbage collection. Our experiments reveal that up to 26% of dead objects remain in memory. The average drag time is up to 37% of execution time. Overall, we observe memory saving potential ranging from 9% to 65%.",
        "published": "2006-11-20T09:08:40Z",
        "link": "http://arxiv.org/abs/cs/0611093v2",
        "categories": [
            "cs.PL",
            "cs.PF",
            "D.3.2; D.3.4; D.4.2"
        ]
    },
    {
        "title": "Knowledge Representation Concepts for Automated SLA Management",
        "authors": [
            "Adrian Paschke",
            "Martin Bichler"
        ],
        "summary": "Outsourcing of complex IT infrastructure to IT service providers has increased substantially during the past years. IT service providers must be able to fulfil their service-quality commitments based upon predefined Service Level Agreements (SLAs) with the service customer. They need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technology. The complexity of contractual logic in SLAs requires new forms of knowledge representation to automatically draw inferences and execute contractual agreements. A logic-based approach provides several advantages including automated rule chaining allowing for compact knowledge representation as well as flexibility to adapt to rapidly changing business requirements. We suggest adequate logical formalisms for representation and enforcement of SLA rules and describe a proof-of-concept implementation. The article describes selected formalisms of the ContractLog KR and their adequacy for automated SLA management and presents results of experiments to demonstrate flexibility and scalability of the approach.",
        "published": "2006-11-23T13:25:45Z",
        "link": "http://arxiv.org/abs/cs/0611122v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "I.2"
        ]
    },
    {
        "title": "Predicate Abstraction via Symbolic Decision Procedures",
        "authors": [
            "Shuvendu K. Lahiri",
            "Thomas Ball",
            "Byron Cook"
        ],
        "summary": "We present a new approach for performing predicate abstraction based on symbolic decision procedures. Intuitively, a symbolic decision procedure for a theory takes a set of predicates in the theory and symbolically executes a decision procedure on all the subsets over the set of predicates. The result of the symbolic decision procedure is a shared expression (represented by a directed acyclic graph) that implicitly represents the answer to a predicate abstraction query.   We present symbolic decision procedures for the logic of Equality and Uninterpreted Functions (EUF) and Difference logic (DIFF) and show that these procedures run in pseudo-polynomial (rather than exponential) time. We then provide a method to construct symbolic decision procedures for simple mixed theories (including the two theories mentioned above) using an extension of the Nelson-Oppen combination method. We present preliminary evaluation of our Procedure on predicate abstraction benchmarks from device driver verification in SLAM.",
        "published": "2006-12-01T19:56:11Z",
        "link": "http://arxiv.org/abs/cs/0612003v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Improving Precision of Type Analysis Using Non-Discriminative Union",
        "authors": [
            "Lunjin Lu"
        ],
        "summary": "This paper presents a new type analysis for logic programs. The analysis is performed with a priori type definitions; and type expressions are formed from a fixed alphabet of type constructors. Non-discriminative union is used to join type information from different sources without loss of precision. An operation that is performed repeatedly during an analysis is to detect if a fixpoint has been reached. This is reduced to checking the emptiness of types. Due to the use of non-discriminative union, the fundamental problem of checking the emptiness of types is more complex in the proposed type analysis than in other type analyses with a priori type definitions. The experimental results, however, show that use of tabling reduces the effect to a small fraction of analysis time on a set of benchmarks.   Keywords: Type analysis, Non-discriminative union, Abstract interpretation, Tabling",
        "published": "2006-12-12T03:09:44Z",
        "link": "http://arxiv.org/abs/cs/0612063v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "The Parma Polyhedra Library: Toward a Complete Set of Numerical   Abstractions for the Analysis and Verification of Hardware and Software   Systems",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Since its inception as a student project in 2001, initially just for the handling (as the name implies) of convex polyhedra, the Parma Polyhedra Library has been continuously improved and extended by joining scrupulous research on the theoretical foundations of (possibly non-convex) numerical abstractions to a total adherence to the best available practices in software development. Even though it is still not fully mature and functionally complete, the Parma Polyhedra Library already offers a combination of functionality, reliability, usability and performance that is not matched by similar, freely available libraries. In this paper, we present the main features of the current version of the library, emphasizing those that distinguish it from other similar libraries and those that are important for applications in the field of analysis and verification of hardware and software systems.",
        "published": "2006-12-18T10:15:38Z",
        "link": "http://arxiv.org/abs/cs/0612085v1",
        "categories": [
            "cs.MS",
            "cs.PL",
            "G.4; D.2.4"
        ]
    },
    {
        "title": "A Calculus for Sensor Networks",
        "authors": [
            "Miguel S. Silva",
            "Francisco Martins",
            "Luis Lopes",
            "Joao Barros"
        ],
        "summary": "We consider the problem of providing a rigorous model for programming wireless sensor networks. Assuming that collisions, packet losses, and errors are dealt with at the lower layers of the protocol stack, we propose a Calculus for Sensor Networks (CSN) that captures the main abstractions for programming applications for this class of devices. Besides providing the syntax and semantics for the calculus, we show its expressiveness by providing implementations for several examples of typical operations on sensor networks. Also included is a detailed discussion of possible extensions to CSN that enable the modeling of other important features of these networks such as sensor state, sampling strategies, and network security.",
        "published": "2006-12-19T14:45:00Z",
        "link": "http://arxiv.org/abs/cs/0612093v1",
        "categories": [
            "cs.DC",
            "cs.PL"
        ]
    },
    {
        "title": "On Completeness of Logical Relations for Monadic Types",
        "authors": [
            "Slawomir Lasota",
            "David Nowak",
            "Yu Zhang"
        ],
        "summary": "Software security can be ensured by specifying and verifying security properties of software using formal methods with strong theoretical bases. In particular, programs can be modeled in the framework of lambda-calculi, and interesting properties can be expressed formally by contextual equivalence (a.k.a. observational equivalence). Furthermore, imperative features, which exist in most real-life software, can be nicely expressed in the so-called computational lambda-calculus. Contextual equivalence is difficult to prove directly, but we can often use logical relations as a tool to establish it in lambda-calculi. We have already defined logical relations for the computational lambda-calculus in previous work. We devote this paper to the study of their completeness w.r.t. contextual equivalence in the computational lambda-calculus.",
        "published": "2006-12-21T09:02:32Z",
        "link": "http://arxiv.org/abs/cs/0612106v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.1; F.3.1"
        ]
    },
    {
        "title": "Adventures in time and space",
        "authors": [
            "Norman Danner",
            "James S. Royer"
        ],
        "summary": "This paper investigates what is essentially a call-by-value version of PCF under a complexity-theoretically motivated type system. The programming formalism, ATR, has its first-order programs characterize the polynomial-time computable functions, and its second-order programs characterize the type-2 basic feasible functionals of Mehlhorn and of Cook and Urquhart. (The ATR-types are confined to levels 0, 1, and 2.) The type system comes in two parts, one that primarily restricts the sizes of values of expressions and a second that primarily restricts the time required to evaluate expressions. The size-restricted part is motivated by Bellantoni and Cook's and Leivant's implicit characterizations of polynomial-time. The time-restricting part is an affine version of Barber and Plotkin's DILL. Two semantics are constructed for ATR. The first is a pruning of the naive denotational semantics for ATR. This pruning removes certain functions that cause otherwise feasible forms of recursion to go wrong. The second semantics is a model for ATR's time complexity relative to a certain abstract machine. This model provides a setting for complexity recurrences arising from ATR recursions, the solutions of which yield second-order polynomial time bounds. The time-complexity semantics is also shown to be sound relative to the costs of interpretation on the abstract machine.",
        "published": "2006-12-21T23:54:20Z",
        "link": "http://arxiv.org/abs/cs/0612116v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.3; F.1.3; F.3.2"
        ]
    },
    {
        "title": "Lineal: A linear-algebraic Lambda-calculus",
        "authors": [
            "Pablo Arrighi",
            "Gilles Dowek"
        ],
        "summary": "We provide a computational definition of the notions of vector space and bilinear functions. We use this result to introduce a minimal language combining higher-order computation and linear algebra. This language extends the Lambda-calculus with the possibility to make arbitrary linear combinations of terms alpha.t + beta.u. We describe how to \"execute\" this language in terms of a few rewrite rules, and justify them through the two fundamental requirements that the language be a language of linear operators, and that it be higher-order. We mention the perspectives of this work in the field of quantum computation, whose circuits we show can be easily encoded in the calculus. Finally, we prove the confluence of the entire calculus.",
        "published": "2006-12-22T17:25:56Z",
        "link": "http://arxiv.org/abs/quant-ph/0612199v6",
        "categories": [
            "quant-ph",
            "cs.LO",
            "cs.PL",
            "03B40, 68N18, 81P68",
            "F.4.1; F.4.2; F.1.1"
        ]
    },
    {
        "title": "Minimum-weight triangulation is NP-hard",
        "authors": [
            "Wolfgang Mulzer",
            "Guenter Rote"
        ],
        "summary": "A triangulation of a planar point set S is a maximal plane straight-line graph with vertex set S. In the minimum-weight triangulation (MWT) problem, we are looking for a triangulation of a given point set that minimizes the sum of the edge lengths. We prove that the decision version of this problem is NP-hard. We use a reduction from PLANAR-1-IN-3-SAT. The correct working of the gadgets is established with computer assistance, using dynamic programming on polygonal faces, as well as the beta-skeleton heuristic to certify that certain edges belong to the minimum-weight triangulation.",
        "published": "2006-01-02T16:11:29Z",
        "link": "http://arxiv.org/abs/cs/0601002v3",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "The density of iterated crossing points and a gap result for   triangulations of finite point sets",
        "authors": [
            "Rolf Klein",
            "Martin Kutz"
        ],
        "summary": "Consider a plane graph G, drawn with straight lines. For every pair a,b of vertices of G, we compare the shortest-path distance between a and b in G (with Euclidean edge lengths) to their actual distance in the plane. The worst-case ratio of these two values, for all pairs of points, is called the dilation of G. All finite plane graphs of dilation 1 have been classified. They are closely related to the following iterative procedure. For a given point set P in R^2, we connect every pair of points in P by a line segment and then add to P all those points where two such line segments cross. Repeating this process infinitely often, yields a limit point set P*.   The main result of this paper is the following gap theorem: For any finite point set P in the plane for which the above P* is infinite, there exists a threshold t > 1 such that P is not contained in the vertex set of any finite plane graph of dilation at most t. We construct a concrete point set Q such that any planar graph that contains this set amongst its vertices must have a dilation larger than 1.0000047.",
        "published": "2006-01-10T01:13:27Z",
        "link": "http://arxiv.org/abs/cs/0601033v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "On the 3-distortion of a path",
        "authors": [
            "Pierre Dehornoy"
        ],
        "summary": "We prove that, when a path of length n is embedded in R^2, the 3-distortion is an Omega(n^{1/2}), and that, when embedded in R^d, the 3-distortion is an O(n^{1/d-1}).",
        "published": "2006-01-30T20:58:48Z",
        "link": "http://arxiv.org/abs/cs/0601128v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Approximate Weighted Farthest Neighbors and Minimum Dilation Stars",
        "authors": [
            "John Augustine",
            "David Eppstein",
            "Kevin A. Wortman"
        ],
        "summary": "We provide an efficient reduction from the problem of querying approximate multiplicatively weighted farthest neighbors in a metric space to the unweighted problem. Combining our techniques with core-sets for approximate unweighted farthest neighbors, we show how to find (1+epsilon)-approximate farthest neighbors in time O(log n) per query in D-dimensional Euclidean space for any constants D and epsilon. As an application, we find an O(n log n) expected time algorithm for choosing the center of a star topology network connecting a given set of points, so as to approximately minimize the maximum dilation between any pair of points.",
        "published": "2006-02-07T21:09:11Z",
        "link": "http://arxiv.org/abs/cs/0602029v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Pants Decomposition of the Punctured Plane",
        "authors": [
            "Sheung-Hung Poon",
            "Shripad Thite"
        ],
        "summary": "A pants decomposition of an orientable surface S is a collection of simple cycles that partition S into pants, i.e., surfaces of genus zero with three boundary cycles. Given a set P of n points in the plane, we consider the problem of computing a pants decomposition of the surface S which is the plane minus P, of minimum total length. We give a polynomial-time approximation scheme using Mitchell's guillotine rectilinear subdivisions. We give a quartic-time algorithm to compute the shortest pants decomposition of S when the cycles are restricted to be axis-aligned boxes, and a quadratic-time algorithm when all the points lie on a line; both exact algorithms use dynamic programming with Yao's speedup.",
        "published": "2006-02-22T19:08:48Z",
        "link": "http://arxiv.org/abs/cs/0602080v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Epsilon-Unfolding Orthogonal Polyhedra",
        "authors": [
            "Mirela Damian",
            "Robin Flatland",
            "Joseph O'Rourke"
        ],
        "summary": "An unfolding of a polyhedron is produced by cutting the surface and flattening to a single, connected, planar piece without overlap (except possibly at boundary points). It is a long unsolved problem to determine whether every polyhedron may be unfolded. Here we prove, via an algorithm, that every orthogonal polyhedron (one whose faces meet at right angles) of genus zero may be unfolded. Our cuts are not necessarily along edges of the polyhedron, but they are always parallel to polyhedron edges. For a polyhedron of n vertices, portions of the unfolding will be rectangular strips which, in the worst case, may need to be as thin as epsilon = 1/2^{Omega(n)}.",
        "published": "2006-02-27T17:03:19Z",
        "link": "http://arxiv.org/abs/cs/0602095v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "On comparing sums of square roots of small integers",
        "authors": [
            "Qi Cheng"
        ],
        "summary": "Let $k$ and $n$ be positive integers, $n>k$. Define $r(n,k)$ to be the minimum positive value of $$ |\\sqrt{a_1} + ... + \\sqrt{a_k} - \\sqrt{b_1} - >... -\\sqrt{b_k} | $$ where $ a_1, a_2, ..., a_k, b_1, b_2, ..., b_k $ are positive integers no larger than $n$. It is an important problem in computational geometry to determine a good upper bound of $-\\log r(n,k)$. In this paper we prove an upper bound of $ 2^{O(n/\\log n)} \\log n$, which is better than the best known result $O(2^{2k} \\log n)$ whenever $ n \\leq ck\\log k$ for some constant $c$. In particular, our result implies a {\\em subexponential} algorithm to compare two sums of square roots of integers of size $o(k\\log k)$.",
        "published": "2006-02-28T22:35:37Z",
        "link": "http://arxiv.org/abs/cs/0603002v1",
        "categories": [
            "cs.CG",
            "F.2.1"
        ]
    },
    {
        "title": "An asymptotically tight bound on the number of semi-algebraically   connected components of realizable sign conditions",
        "authors": [
            "Saugata Basu",
            "Richard Pollack",
            "Marie-Francoise Roy"
        ],
        "summary": "We prove an asymptotically tight bound (asymptotic with respect to the number of polynomials for fixed degrees and number of variables) on the number of semi-algebraically connected components of the realizations of all realizable sign conditions of a family of real polynomials. More precisely, we prove that the number of semi-algebraically connected components of the realizations of all realizable sign conditions of a family of $s$ polynomials in $\\R[X_1,...,X_k]$ whose degrees are at most $d$ is bounded by \\[ \\frac{(2d)^k}{k!}s^k + O(s^{k-1}). \\] This improves the best upper bound known previously which was \\[ {1/2}\\frac{(8d)^k}{k!}s^k + O(s^{k-1}). \\] The new bound matches asymptotically the lower bound obtained for families of polynomials each of which is a product of generic polynomials of degree one.",
        "published": "2006-03-10T18:41:15Z",
        "link": "http://arxiv.org/abs/math/0603256v3",
        "categories": [
            "math.CO",
            "cs.CG",
            "math.AG",
            "14P10; 14P25"
        ]
    },
    {
        "title": "Guard Placement For Wireless Localization",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Nodari Sitchinava"
        ],
        "summary": "Motivated by secure wireless networking, we consider the problem of placing fixed localizers that enable mobile communication devices to prove they belong to a secure region that is defined by the interior of a polygon. Each localizer views an infinite wedge of the plane, and a device can prove membership in the secure region if it is inside the wedges for a set of localizers whose common intersection contains no points outside the polygon. This model leads to a broad class of new art gallery type problems, for which we provide upper and lower bounds.",
        "published": "2006-03-14T20:19:36Z",
        "link": "http://arxiv.org/abs/cs/0603057v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "About the domino problem in the hyperbolic plane from an algorithmic   point of view",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "In this paper, we prove that the general problem of tiling the hyperbolic plane with \\`a la Wang tiles is undecidable.",
        "published": "2006-03-23T09:36:52Z",
        "link": "http://arxiv.org/abs/cs/0603093v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.1.1; F.2.2"
        ]
    },
    {
        "title": "Minimum-Cost Coverage of Point Sets by Disks",
        "authors": [
            "Esther M. Arkin",
            "Herve Broennimann",
            "Jeff Erickson",
            "Sandor P. Fekete",
            "Christian Knauer",
            "Jonathan Lenchner",
            "Joseph S. B. Mitchell",
            "Kim Whittlesey"
        ],
        "summary": "We consider a class of geometric facility location problems in which the goal is to determine a set X of disks given by their centers (t_j) and radii (r_j) that cover a given set of demand points Y in the plane at the smallest possible cost. We consider cost functions of the form sum_j f(r_j), where f(r)=r^alpha is the cost of transmission to radius r. Special cases arise for alpha=1 (sum of radii) and alpha=2 (total area); power consumption models in wireless network design often use an exponent alpha>2. Different scenarios arise according to possible restrictions on the transmission centers t_j, which may be constrained to belong to a given discrete set or to lie on a line, etc. We obtain several new results, including (a) exact and approximation algorithms for selecting transmission points t_j on a given line in order to cover demand points Y in the plane; (b) approximation algorithms (and an algebraic intractability result) for selecting an optimal line on which to place transmission points to cover Y; (c) a proof of NP-hardness for a discrete set of transmission points in the plane and any fixed alpha>1; and (d) a polynomial-time approximation scheme for the problem of computing a minimum cost covering tour (MCCT), in which the total cost is a linear combination of the transmission cost for the set of disks and the length of a tour/path that connects the centers of the disks.",
        "published": "2006-04-04T17:24:09Z",
        "link": "http://arxiv.org/abs/cs/0604008v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Locked and Unlocked Chains of Planar Shapes",
        "authors": [
            "Robert Connelly",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Sandor P. Fekete",
            "Stefan Langerman",
            "Joseph S. B. Mitchell",
            "Ares Ribo",
            "Guenter Rote"
        ],
        "summary": "We extend linkage unfolding results from the well-studied case of polygonal linkages to the more general case of linkages of polygons. More precisely, we consider chains of nonoverlapping rigid planar shapes (Jordan regions) that are hinged together sequentially at rotatable joints. Our goal is to characterize the families of planar shapes that admit locked chains, where some configurations cannot be reached by continuous reconfiguration without self-intersection, and which families of planar shapes guarantee universal foldability, where every chain is guaranteed to have a connected configuration space. Previously, only obtuse triangles were known to admit locked shapes, and only line segments were known to guarantee universal foldability. We show that a surprisingly general family of planar shapes, called slender adornments, guarantees universal foldability: roughly, the distance from each edge along the path along the boundary of the slender adornment to each hinge should be monotone. In contrast, we show that isosceles triangles with any desired apex angle less than 90 degrees admit locked chains, which is precisely the threshold beyond which the inward-normal property no longer holds.",
        "published": "2006-04-06T14:46:27Z",
        "link": "http://arxiv.org/abs/cs/0604022v4",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Squarepants in a Tree: Sum of Subtree Clustering and Hyperbolic Pants   Decomposition",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We provide efficient constant factor approximation algorithms for the problems of finding a hierarchical clustering of a point set in any metric space, minimizing the sum of minimimum spanning tree lengths within each cluster, and in the hyperbolic or Euclidean planes, minimizing the sum of cluster perimeters. Our algorithms for the hyperbolic and Euclidean planes can also be used to provide a pants decomposition, that is, a set of disjoint simple closed curves partitioning the plane minus the input points into subsets with exactly three boundary components, with approximately minimum total length. In the Euclidean case, these curves are squares; in the hyperbolic case, they combine our Euclidean square pants decomposition with our tree clustering method for general metric spaces.",
        "published": "2006-04-09T01:18:29Z",
        "link": "http://arxiv.org/abs/cs/0604034v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Inner and Outer Rounding of Boolean Operations on Lattice Polygonal   Regions",
        "authors": [
            "Olivier Devillers",
            "Philippe Guigue"
        ],
        "summary": "Robustness problems due to the substitution of the exact computation on real numbers by the rounded floating point arithmetic are often an obstacle to obtain practical implementation of geometric algorithms. If the adoption of the --exact computation paradigm--[Yap et Dube] gives a satisfactory solution to this kind of problems for purely combinatorial algorithms, this solution does not allow to solve in practice the case of algorithms that cascade the construction of new geometric objects. In this report, we consider the problem of rounding the intersection of two polygonal regions onto the integer lattice with inclusion properties. Namely, given two polygonal regions A and B having their vertices on the integer lattice, the inner and outer rounding modes construct two polygonal regions with integer vertices which respectively is included and contains the true intersection. We also prove interesting results on the Hausdorff distance, the size and the convexity of these polygonal regions.",
        "published": "2006-04-13T14:33:15Z",
        "link": "http://arxiv.org/abs/cs/0604059v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "A Fast and Accurate Nonlinear Spectral Method for Image Recognition and   Registration",
        "authors": [
            "Luciano da Fontoura Costa",
            "Erik Bollt"
        ],
        "summary": "This article addresses the problem of two- and higher dimensional pattern matching, i.e. the identification of instances of a template within a larger signal space, which is a form of registration. Unlike traditional correlation, we aim at obtaining more selective matchings by considering more strict comparisons of gray-level intensity. In order to achieve fast matching, a nonlinear thresholded version of the fast Fourier transform is applied to a gray-level decomposition of the original 2D image. The potential of the method is substantiated with respect to real data involving the selective identification of neuronal cell bodies in gray-level images.",
        "published": "2006-04-24T16:38:01Z",
        "link": "http://arxiv.org/abs/cs/0604094v1",
        "categories": [
            "cs.DC",
            "cond-mat.stat-mech",
            "cs.CG",
            "cs.CV"
        ]
    },
    {
        "title": "Enumeration of non-orientable 3-manifolds using face pairing graphs and   union-find",
        "authors": [
            "Benjamin A. Burton"
        ],
        "summary": "Drawing together techniques from combinatorics and computer science, we improve the census algorithm for enumerating closed minimal P^2-irreducible 3-manifold triangulations. In particular, new constraints are proven for face pairing graphs, and pruning techniques are improved using a modification of the union-find algorithm. Using these results we catalogue all 136 closed non-orientable P^2-irreducible 3-manifolds that can be formed from at most ten tetrahedra.",
        "published": "2006-04-27T06:19:44Z",
        "link": "http://arxiv.org/abs/math/0604584v1",
        "categories": [
            "math.GT",
            "cs.CG",
            "math.CO",
            "57N10, 68W05 (Primary) 57M04, 57M15 (Secondary)"
        ]
    },
    {
        "title": "Spanners for Geometric Intersection Graphs",
        "authors": [
            "Martin Furer",
            "Shiva Prasad Kasiviswanathan"
        ],
        "summary": "Efficient algorithms are presented for constructing spanners in geometric intersection graphs. For a unit ball graph in R^k, a (1+\\epsilon)-spanner is obtained using efficient partitioning of the space into hypercubes and solving bichromatic closest pair problems. The spanner construction has almost equivalent complexity to the construction of Euclidean minimum spanning trees. The results are extended to arbitrary ball graphs with a sub-quadratic running time.   For unit ball graphs, the spanners have a small separator decomposition which can be used to obtain efficient algorithms for approximating proximity problems like diameter and distance queries. The results on compressed quadtrees, geometric graph separators, and diameter approximation might be of independent interest.",
        "published": "2006-05-07T23:38:00Z",
        "link": "http://arxiv.org/abs/cs/0605029v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Efficient algorithm for computing the Euler-Poincaré characteristic of   a semi-algebraic set defined by few quadratic inequalities",
        "authors": [
            "Saugata Basu"
        ],
        "summary": "We present an algorithm which takes as input a closed semi-algebraic set, $S \\subset \\R^k$, defined by \\[ P_1 \\leq 0, ..., P_\\ell \\leq 0, P_i \\in \\R[X_1,...,X_k], \\deg(P_i) \\leq 2, \\] and computes the Euler-Poincar\\'e characteristic of $S$. The complexity of the algorithm is $k^{O(\\ell)}$.",
        "published": "2006-05-18T18:03:29Z",
        "link": "http://arxiv.org/abs/cs/0605082v1",
        "categories": [
            "cs.SC",
            "cs.CG"
        ]
    },
    {
        "title": "Restricted Strip Covering and the Sensor Cover Problem",
        "authors": [
            "Adam L. Buchsbaum",
            "Alon Efrat",
            "Shaili Jain",
            "Suresh Venkatasubramanian",
            "Ke Yi"
        ],
        "summary": "Given a set of objects with durations (jobs) that cover a base region, can we schedule the jobs to maximize the duration the original region remains covered? We call this problem the sensor cover problem. This problem arises in the context of covering a region with sensors. For example, suppose you wish to monitor activity along a fence by sensors placed at various fixed locations. Each sensor has a range and limited battery life. The problem is to schedule when to turn on the sensors so that the fence is fully monitored for as long as possible. This one dimensional problem involves intervals on the real line. Associating a duration to each yields a set of rectangles in space and time, each specified by a pair of fixed horizontal endpoints and a height. The objective is to assign a position to each rectangle to maximize the height at which the spanning interval is fully covered. We call this one dimensional problem restricted strip covering. If we replace the covering constraint by a packing constraint, the problem is identical to dynamic storage allocation, a scheduling problem that is a restricted case of the strip packing problem. We show that the restricted strip covering problem is NP-hard and present an O(log log n)-approximation algorithm. We present better approximations or exact algorithms for some special cases. For the uniform-duration case of restricted strip covering we give a polynomial-time, exact algorithm but prove that the uniform-duration case for higher-dimensional regions is NP-hard. Finally, we consider regions that are arbitrary sets, and we present an O(log n)-approximation algorithm.",
        "published": "2006-05-24T03:27:07Z",
        "link": "http://arxiv.org/abs/cs/0605102v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "A parent-centered radial layout algorithm for interactive graph   visualization and animation",
        "authors": [
            "Andrew Pavlo",
            "Christopher Homan",
            "Jonathan Schull"
        ],
        "summary": "We have developed (1) a graph visualization system that allows users to explore graphs by viewing them as a succession of spanning trees selected interactively, (2) a radial graph layout algorithm, and (3) an animation algorithm that generates meaningful visualizations and smooth transitions between graphs while minimizing edge crossings during transitions and in static layouts.   Our system is similar to the radial layout system of Yee et al. (2001), but differs primarily in that each node is positioned on a coordinate system centered on its own parent rather than on a single coordinate system for all nodes. Our system is thus easy to define recursively and lends itself to parallelization. It also guarantees that layouts have many nice properties, such as: it guarantees certain edges never cross during an animation.   We compared the layouts and transitions produced by our algorithms to those produced by Yee et al. Results from several experiments indicate that our system produces fewer edge crossings during transitions between graph drawings, and that the transitions more often involve changes in local scaling rather than structure.   These findings suggest the system has promise as an interactive graph exploration tool in a variety of settings.",
        "published": "2006-06-01T16:56:55Z",
        "link": "http://arxiv.org/abs/cs/0606007v1",
        "categories": [
            "cs.HC",
            "cs.CG",
            "cs.GR",
            "I.3.3; H.5.0"
        ]
    },
    {
        "title": "On the communication between cells of a cellular automaton on the penta-   and heptagrids of the hyperbolic plane",
        "authors": [
            "Maurice Margenstern"
        ],
        "summary": "This contribution belongs to a combinatorial approach to hyperbolic geometry and it is aimed at possible applications to computer simulations.   It is based on the splitting method which was introduced by the author and which is reminded in the second section of the paper. Then we sketchily remind the application to the classical case of the pentagrid, i.e. the tiling of the hyperbolic plane which is generated by reflections of the regular rectangular pentagon in its sides and, recursively, of its images in their sides. From this application, we derived a system of coordinates to locate the tiles, allowing an implementation of cellular automata.   At the software level, cells exchange messages thanks to a new representation which improves the speed of contacts between cells. In the new setting, communications are exchanged along actual geodesics and the contribution of the cellular automaton is also linear in the coordinates of the cells.",
        "published": "2006-06-02T07:34:17Z",
        "link": "http://arxiv.org/abs/cs/0606012v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "Good Illumination of Minimum Range",
        "authors": [
            "M. Abellanas",
            "A. Bajuelos",
            "G. Hernández",
            "F. Hurtado",
            "I. Matos",
            "B. Palop"
        ],
        "summary": "A point p is 1-well illuminated by a set F of n point lights if p lies in the interior of the convex hull of F. This concept corresponds to triangle-guarding or well-covering. In this paper we consider the illumination range of the light sources as a parameter to be optimized. First, we solve the problem of minimizing the light sources' illumination range to 1-well illuminate a given point p. We also compute a minimal set of light sources that 1-well illuminates p with minimum illumination range. Second, we solve the problem of minimizing the light sources' illumination range to 1-well illuminate all the points of a line segment with an O(n^2) algorithm. Finally, we give an O(n^2 log n) algorithm for preprocessing the data so that one can obtain the illumination range needed to 1-well illuminate a point of a line segment in O(log n) time. These results can be applied to solve problems of 1-well illuminating a trajectory by approaching it to a polygonal path.",
        "published": "2006-06-02T11:24:27Z",
        "link": "http://arxiv.org/abs/cs/0606013v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "Computational Euclid",
        "authors": [
            "M. H. van Emden",
            "B. Moa"
        ],
        "summary": "We analyse the axioms of Euclidean geometry according to standard object-oriented software development methodology. We find a perfect match: the main undefined concepts of the axioms translate to object classes. The result is a suite of C++ classes that efficiently supports the construction of complex geometric configurations. Although all computations are performed in floating-point arithmetic, they correctly implement as semi-decision algorithms the tests for equality of points, a point being on a line or in a plane, a line being in a plane, parallelness of lines, of a line and a plane, and of planes. That is, in accordance to the fundamental limitations to computability requiring that only negative outcomes are given with certainty, while positive outcomes only imply possibility of these conditions being true.",
        "published": "2006-06-08T15:43:09Z",
        "link": "http://arxiv.org/abs/cs/0606036v1",
        "categories": [
            "cs.CG",
            "I.3.5; G.1.0"
        ]
    },
    {
        "title": "Characterization of Pentagons Determined by Two X-rays",
        "authors": [
            "Ming-Zhe Chen"
        ],
        "summary": "This paper contains some results of pentagons which can be determined by two X-rays. The results reveal this problem is more complicated.",
        "published": "2006-06-09T14:13:15Z",
        "link": "http://arxiv.org/abs/cs/0606041v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Simple Methods For Drawing Rational Surfaces as Four or Six Bezier   Patches",
        "authors": [
            "Jean Gallier"
        ],
        "summary": "In this paper, we give several simple methods for drawing a whole rational surface (without base points) as several Bezier patches. The first two methods apply to surfaces specified by triangular control nets and partition the real projective plane RP2 into four and six triangles respectively. The third method applies to surfaces specified by rectangular control nets and partitions the torus RP1 X RP1 into four rectangular regions. In all cases, the new control nets are obtained by sign flipping and permutation of indices from the original control net. The proofs that these formulae are correct involve very little computations and instead exploit the geometry of the parameter space (RP2 or RP1 X RP1). We illustrate our method on some classical examples. We also propose a new method for resolving base points using a simple ``blowing up'' technique involving the computation of ``resolved'' control nets.",
        "published": "2006-06-12T22:02:41Z",
        "link": "http://arxiv.org/abs/cs/0606055v1",
        "categories": [
            "cs.CG",
            "cs.GR"
        ]
    },
    {
        "title": "On the Efficiency of Strategies for Subdividing Polynomial Triangular   Surface Patches",
        "authors": [
            "Jean Gallier"
        ],
        "summary": "In this paper, we investigate the efficiency of various strategies for subdividing polynomial triangular surface patches. We give a simple algorithm performing a regular subdivision in four calls to the standard de Casteljau algorithm (in its subdivision version). A naive version uses twelve calls. We also show that any method for obtaining a regular subdivision using the standard de Casteljau algorithm requires at least 4 calls. Thus, our method is optimal. We give another subdivision algorithm using only three calls to the de Casteljau algorithm. Instead of being regular, the subdivision pattern is diamond-like. Finally, we present a ``spider-like'' subdivision scheme producing six subtriangles in four calls to the de Casteljau algorithm.",
        "published": "2006-06-13T15:09:24Z",
        "link": "http://arxiv.org/abs/cs/0606061v1",
        "categories": [
            "cs.CG",
            "cs.GR"
        ]
    },
    {
        "title": "Outlier Robust ICP for Minimizing Fractional RMSD",
        "authors": [
            "Jeff M. Phillips",
            "Ran Liu",
            "Carlo Tomasi"
        ],
        "summary": "We describe a variation of the iterative closest point (ICP) algorithm for aligning two point sets under a set of transformations. Our algorithm is superior to previous algorithms because (1) in determining the optimal alignment, it identifies and discards likely outliers in a statistically robust manner, and (2) it is guaranteed to converge to a locally optimal solution. To this end, we formalize a new distance measure, fractional root mean squared distance (frmsd), which incorporates the fraction of inliers into the distance function. We lay out a specific implementation, but our framework can easily incorporate most techniques and heuristics from modern registration algorithms. We experimentally validate our algorithm against previous techniques on 2 and 3 dimensional data exposed to a variety of outlier types.",
        "published": "2006-06-22T15:35:56Z",
        "link": "http://arxiv.org/abs/cs/0606098v1",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "Upright-Quad Drawing of st-Planar Learning Spaces",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We consider graph drawing algorithms for learning spaces, a type of st-oriented partial cube derived from antimatroids and used to model states of knowledge of students. We show how to draw any st-planar learning space so all internal faces are convex quadrilaterals with the bottom side horizontal and the left side vertical, with one minimal and one maximal vertex. Conversely, every such drawing represents an st-planar learning space. We also describe connections between these graphs and arrangements of translates of a quadrant.",
        "published": "2006-07-20T04:52:14Z",
        "link": "http://arxiv.org/abs/cs/0607094v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Trees with Convex Faces and Optimal Angles",
        "authors": [
            "Josiah Carlson",
            "David Eppstein"
        ],
        "summary": "We consider drawings of trees in which all edges incident to leaves can be extended to infinite rays without crossing, partitioning the plane into infinite convex polygons. Among all such drawings we seek the one maximizing the angular resolution of the drawing. We find linear time algorithms for solving this problem, both for plane trees and for trees without a fixed embedding. In any such drawing, the edge lengths may be set independently of the angles, without crossing; we describe multiple strategies for setting these lengths.",
        "published": "2006-07-26T00:44:20Z",
        "link": "http://arxiv.org/abs/cs/0607113v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Geometric definition of a new skeletonization concept",
        "authors": [
            "Yannis Bakopoulos",
            "Theophanis Raptis",
            "Doxaras Ioannis"
        ],
        "summary": "The Divider set, as an innovative alternative concept to maximal disks, Voronoi sets and cut loci, is presented with a formal definition based on topology and differential geometry. The relevant mathematical theory by previous authors and a comparison with other medial axis definitions is presented. Appropriate applications are proposed and examined.",
        "published": "2006-07-31T18:17:30Z",
        "link": "http://arxiv.org/abs/cs/0607145v3",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Stability in multidimensional Size Theory",
        "authors": [
            "Andrea Cerri",
            "Patrizio Frosini",
            "Claudia Landi"
        ],
        "summary": "This paper proves that in Size Theory the comparison of multidimensional size functions can be reduced to the 1-dimensional case by a suitable change of variables. Indeed, we show that a foliation in half-planes can be given, such that the restriction of a multidimensional size function to each of these half-planes turns out to be a classical size function in two scalar variables. This leads to the definition of a new distance between multidimensional size functions, and to the proof of their stability with respect to that distance.",
        "published": "2006-08-02T08:49:57Z",
        "link": "http://arxiv.org/abs/cs/0608009v1",
        "categories": [
            "cs.CG",
            "cs.CV",
            "I.3.5; I.5.1"
        ]
    },
    {
        "title": "Localization for Anchoritic Sensor Networks",
        "authors": [
            "Yuliy Baryshnikov",
            "Jian Tan"
        ],
        "summary": "We introduce a class of anchoritic sensor networks, where communications between sensor nodes is undesirable or infeasible, e.g., due to harsh environment, energy constraints, or security considerations.",
        "published": "2006-08-02T22:10:51Z",
        "link": "http://arxiv.org/abs/cs/0608014v1",
        "categories": [
            "cs.NI",
            "cs.CG"
        ]
    },
    {
        "title": "Dispersion of Mass and the Complexity of Randomized Geometric Algorithms",
        "authors": [
            "Luis Rademacher",
            "Santosh Vempala"
        ],
        "summary": "How much can randomness help computation? Motivated by this general question and by volume computation, one of the few instances where randomness provably helps, we analyze a notion of dispersion and connect it to asymptotic convex geometry. We obtain a nearly quadratic lower bound on the complexity of randomized volume algorithms for convex bodies in R^n (the current best algorithm has complexity roughly n^4, conjectured to be n^3). Our main tools, dispersion of random determinants and dispersion of the length of a random point from a convex body, are of independent interest and applicable more generally; in particular, the latter is closely related to the variance hypothesis from convex geometry. This geometric dispersion also leads to lower bounds for matrix problems and property testing.",
        "published": "2006-08-12T23:31:07Z",
        "link": "http://arxiv.org/abs/cs/0608054v2",
        "categories": [
            "cs.CC",
            "cs.CG",
            "cs.DS",
            "math.FA"
        ]
    },
    {
        "title": "A Generic Lazy Evaluation Scheme for Exact Geometric Computations",
        "authors": [
            "Sylvain Pion",
            "Andreas Fabri"
        ],
        "summary": "We present a generic C++ design to perform efficient and exact geometric computations using lazy evaluations. Exact geometric computations are critical for the robustness of geometric algorithms. Their efficiency is also critical for most applications, hence the need for delaying the exact computations at run time until they are actually needed. Our approach is generic and extensible in the sense that it is possible to make it a library which users can extend to their own geometric objects or primitives. It involves techniques such as generic functor adaptors, dynamic polymorphism, reference counting for the management of directed acyclic graphs and exception handling for detecting cases where exact computations are needed. It also relies on multiple precision arithmetic as well as interval arithmetic. We apply our approach to the whole geometric kernel of CGAL.",
        "published": "2006-08-15T19:24:02Z",
        "link": "http://arxiv.org/abs/cs/0608063v1",
        "categories": [
            "cs.CG",
            "cs.PF"
        ]
    },
    {
        "title": "A Condition Number Analysis of a Line-Surface Intersection Algorithm",
        "authors": [
            "Gun Srijuntongsiri",
            "Stephen A. Vavasis"
        ],
        "summary": "We propose an algorithm based on Newton's method and subdivision for finding all zeros of a polynomial system in a bounded region of the plane. This algorithm can be used to find the intersections between a line and a surface, which has applications in graphics and computer-aided geometric design. The algorithm can operate on polynomials represented in any basis that satisfies a few conditions. The power basis, the Bernstein basis, and the first-kind Chebyshev basis are among those compatible with the algorithm. The main novelty of our algorithm is an analysis showing that its running is bounded only in terms of the condition number of the polynomial's zeros and a constant depending on the polynomial basis.",
        "published": "2006-08-23T03:47:07Z",
        "link": "http://arxiv.org/abs/cs/0608090v2",
        "categories": [
            "cs.NA",
            "cs.CG"
        ]
    },
    {
        "title": "Spherical Indexing for Neighborhood Queries",
        "authors": [
            "Nicolas Brodu"
        ],
        "summary": "This is an algorithm for finding neighbors when the objects can freely move and have no predefined position. The query consists in finding neighbors for a center location and a given radius. Space is discretized in cubic cells. This algorithm introduces a direct spherical indexing that gives the list of all cells making up the query sphere, for any radius and any center location. It can additionally take in account both cyclic and non-cyclic regions of interest. Finding only the K nearest neighbors naturally benefits from the spherical indexing by minimally running through the sphere from center to edge, and reducing the maximum distance when K neighbors have been found.",
        "published": "2006-08-29T00:12:55Z",
        "link": "http://arxiv.org/abs/cs/0608108v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Choosing Colors for Geometric Graphs via Color Space Embeddings",
        "authors": [
            "Michael B. Dillencourt",
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "Graph drawing research traditionally focuses on producing geometric embeddings of graphs satisfying various aesthetic constraints. After the geometric embedding is specified, there is an additional step that is often overlooked or ignored: assigning display colors to the graph's vertices. We study the additional aesthetic criterion of assigning distinct colors to vertices of a geometric graph so that the colors assigned to adjacent vertices are as different from one another as possible. We formulate this as a problem involving perceptual metrics in color space and we develop algorithms for solving this problem by embedding the graph in color space. We also present an application of this work to a distributed load-balancing visualization problem.",
        "published": "2006-09-07T20:39:19Z",
        "link": "http://arxiv.org/abs/cs/0609033v1",
        "categories": [
            "cs.CG",
            "G.1.6"
        ]
    },
    {
        "title": "Topology Control and Network Lifetime in Three-Dimensional Wireless   Sensor Networks",
        "authors": [
            "S. M. Nazrul Alam",
            "Zygmunt J. Haas"
        ],
        "summary": "Coverage and connectivity issues of three-dimensional (3D) networks are addressed in [2], but that work assumes that a node can be placed at any arbitrary location. In this work, we drop that assumption and rather assume that nodes are uniformly and densely deployed in a 3D space. We want to devise a mechanism that keeps some nodes active and puts other nodes into sleep so that the number of active nodes at a time is minimized (and thus network life time is maximized), while maintaining full coverage and connectivity. One simple way to do that is to partition the 3D space into cells, and only one node in each cell remains active at a time. Our results show that the number of active nodes can be minimized if the shape of each cell is a truncated octahedron. It requires the sensing range to be at least 0.542326 times the transmission radius. This value is 0.5, 0.53452 and 0.5 for cube, hexagonal prism, and rhombic dodecahedron, respectively. However, at a time the number of active nodes for cube, hexagonal prism and rhombic dodecahedron model is respectively 2.372239, 1.82615 and 1.49468 times of that of truncated octahedron model. So clearly truncated octahedron model has the highest network lifetime. We also provide a distributed topology control algorithm that can be used by each sensor node to determine its cell id using a constant number of local arithmetic operations provided that the sensor node knows its location. We also validate our results by simulation.",
        "published": "2006-09-11T02:01:47Z",
        "link": "http://arxiv.org/abs/cs/0609047v1",
        "categories": [
            "cs.NI",
            "cs.CG",
            "C.2.1"
        ]
    },
    {
        "title": "A Continuum Theory for Unstructured Mesh Generation in Two Dimensions",
        "authors": [
            "Guy Bunin"
        ],
        "summary": "A continuum description of unstructured meshes in two dimensions, both for planar and curved surface domains, is proposed. The meshes described are those which, in the limit of an increasingly finer mesh (smaller cells), and away from irregular vertices, have ideally-shaped cells (squares or equilateral triangles), and can therefore be completely described by two local properties: local cell size and local edge directions. The connection between the two properties is derived by defining a Riemannian manifold whose geodesics trace the edges of the mesh. A function $\\phi$, proportional to the logarithm of the cell size, is shown to obey the Poisson equation, with localized charges corresponding to irregular vertices. The problem of finding a suitable manifold for a given domain is thus shown to exactly reduce to an Inverse Poisson problem on $\\phi$, of finding a distribution of localized charges adhering to the conditions derived for boundary alignment. Possible applications to mesh generation are discussed.",
        "published": "2006-09-14T00:07:39Z",
        "link": "http://arxiv.org/abs/cs/0609078v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "On Bus Graph Realizability",
        "authors": [
            "Anil Ada",
            "Melanie Coggan",
            "Paul Di Marco",
            "Alain Doyon",
            "Liam Flookes",
            "Samuli Heilala",
            "Ethan Kim",
            "Jonathan Li On Wing",
            "Louis-Francois Preville-Ratelle",
            "Sue Whitesides",
            "Nuo Yu"
        ],
        "summary": "In this paper, we consider the following graph embedding problem: Given a bipartite graph G = (V1; V2;E), where the maximum degree of vertices in V2 is 4, can G be embedded on a two dimensional grid such that each vertex in V1 is drawn as a line segment along a grid line, each vertex in V2 is drawn as a point at a grid point, and each edge e = (u; v) for some u 2 V1 and v 2 V2 is drawn as a line segment connecting u and v, perpendicular to the line segment for u? We show that this problem is NP-complete, and sketch how our proof techniques can be used to show the hardness of several other related problems.",
        "published": "2006-09-22T18:01:17Z",
        "link": "http://arxiv.org/abs/cs/0609127v1",
        "categories": [
            "cs.CG",
            "cs.DM"
        ]
    },
    {
        "title": "Polygon Convexity: A Minimal O(n) Test",
        "authors": [
            "Iosif Pinelis"
        ],
        "summary": "An O(n) test for polygon convexity is stated and proved. It is also proved that the test is minimal in a certain exact sense.",
        "published": "2006-09-25T19:23:55Z",
        "link": "http://arxiv.org/abs/cs/0609141v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "math.CO",
            "math.MG",
            "I.3.5; F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Number sequence representation of protein structures based on the second   derivative of a folded tetrahedron sequence",
        "authors": [
            "Naoto Morikawa"
        ],
        "summary": "This paper proposes a new mathematical approach to characterize native protein structures based on the discrete differential geometry of tetrahedron tiles. In the approach, local structure of proteins is classified into finite types according to shape. And one would obtain a number sequence representation of protein structures automatically. As a result, it would become possible to quantify structural preference of amino-acids objectively. And one could use the wide variety of sequence alignment programs to study protein structures since the number sequence has no internal structure.   The programs and this paper with clear figures are available from http://www.genocript.com.",
        "published": "2006-10-09T06:02:34Z",
        "link": "http://arxiv.org/abs/q-bio/0610017v1",
        "categories": [
            "q-bio.BM",
            "cs.CG",
            "cs.DM",
            "math.MG"
        ]
    },
    {
        "title": "Happy endings for flip graphs",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show that the triangulations of a finite point set form a flip graph that can be embedded isometrically into a hypercube, if and only if the point set has no empty convex pentagon. Point sets of this type include convex subsets of lattices, points on two lines, and several other infinite families. As a consequence, flip distance in such point sets can be computed efficiently.",
        "published": "2006-10-15T01:30:48Z",
        "link": "http://arxiv.org/abs/cs/0610092v2",
        "categories": [
            "cs.CG",
            "math.CO",
            "math.MG",
            "F.2.2"
        ]
    },
    {
        "title": "Substitutions for tilings $\\{p,q\\}$",
        "authors": [
            "Maurice Margenstern",
            "Guentcho Skordev"
        ],
        "summary": "In this paper we consider tiling $\\{p, q \\}$ of the Euclidean space and of the hyperbolic space, and its dual graph $\\Gamma_{q, p}$ from a combinatorial point of view. A substitution $\\sigma_{q, p}$ on an appropriate finite alphabet is constructed. The homogeneity of graph $\\Gamma_{q, p}$ and its generation function are the basic tools for the construction. The tree associated with substitution $\\sigma_{q, p}$ is a spanning tree of graph $\\Gamma_{q, p}$. Let $u_n$ be the number of tiles of tiling $\\{p, q \\}$ of generation $n$. The characteristic polynomial of the transition matrix of substitution $\\sigma_{q, p}$ is a characteristic polynomial of a linear recurrence. The sequence $(u_n)_{n \\geq 0}$ is a solution of this recurrence. The growth of sequence $(u_n)_{n \\geq 0}$ is given by the dominant root of the characteristic polynomial.",
        "published": "2006-11-09T07:53:36Z",
        "link": "http://arxiv.org/abs/cs/0611039v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2"
        ]
    },
    {
        "title": "A disk-covering problem with application in optical interferometry",
        "authors": [
            "Trung Nguyen",
            "Jean-Daniel Boissonnat",
            "Frederic Falzon",
            "Christian Knauer"
        ],
        "summary": "Given a disk O in the plane called the objective, we want to find n small disks P_1,...,P_n called the pupils such that $\\bigcup_{i,j=1}^n P_i \\ominus P_j \\supseteq O$, where $\\ominus$ denotes the Minkowski difference operator, while minimizing the number of pupils, the sum of the radii or the total area of the pupils. This problem is motivated by the construction of very large telescopes from several smaller ones by so-called Optical Aperture Synthesis. In this paper, we provide exact, approximate and heuristic solutions to several variations of the problem.",
        "published": "2006-12-05T15:36:21Z",
        "link": "http://arxiv.org/abs/cs/0612026v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "Power Assignment Problems in Wireless Communication",
        "authors": [
            "Stefan Funke",
            "Soeren Laue",
            "Zvi Lotker",
            "Rouven Naujoks"
        ],
        "summary": "A fundamental class of problems in wireless communication is concerned with the assignment of suitable transmission powers to wireless devices/stations such that the resulting communication graph satisfies certain desired properties and the overall energy consumed is minimized. Many concrete communication tasks in a wireless network like broadcast, multicast, point-to-point routing, creation of a communication backbone, etc. can be regarded as such a power assignment problem.   This paper considers several problems of that kind; for example one problem studied before in \\cite{Carrots, Bilo} aims to select and assign powers to $k$ of the stations such that all other stations are within reach of at least one of the selected stations. We improve the running time for obtaining a $(1+\\epsilon)$-approximate solution for this problem from $n^{((\\alpha/\\epsilon)^{O(d)})}$ as reported by Bilo et al. (\\cite{Bilo}) to $O(n+ {(\\frac{k^{2d+1}}{\\epsilon^d})}^{\\min{\\{2k, (\\alpha/\\epsilon)^{O(d)} \\}}})$ that is, we obtain a running time that is \\emph{linear} in the network size. Further results include a constant approximation algorithm for the TSP problem under squared (non-metric!) edge costs, which can be employed to implement a novel data aggregation protocol, as well as efficient schemes to perform $k$-hop multicasts.",
        "published": "2006-12-22T12:53:15Z",
        "link": "http://arxiv.org/abs/cs/0612121v1",
        "categories": [
            "cs.CG",
            "cs.AR",
            "cs.NI"
        ]
    },
    {
        "title": "On the Complexity of the Circular Chromatic Number",
        "authors": [
            "Hamed Hatami",
            "Ruzbeh Tusserkani"
        ],
        "summary": "Circular chromatic number, $\\chi_c$ is a natural generalization of chromatic number. It is known that it is \\NP-hard to determine whether or not an arbitrary graph $G$ satisfies $\\chi(G) = \\chi_c(G)$. In this paper we prove that this problem is \\NP-hard even if the chromatic number of the graph is known. This answers a question of Xuding Zhu. Also we prove that for all positive integers $k \\ge 2$ and $n \\ge 3$, for a given graph $G$ with $\\chi(G)=n$, it is \\NP-complete to verify if $\\chi_c(G) \\le n- \\frac{1}{k}$.",
        "published": "2006-12-31T04:48:59Z",
        "link": "http://arxiv.org/abs/cs/0701007v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Approximation and Inapproximability Results for Maximum Clique of Disc   Graphs in High Dimensions",
        "authors": [
            "Peyman Afshani",
            "Hamed Hatami"
        ],
        "summary": "We prove algorithmic and hardness results for the problem of finding the largest set of a fixed diameter in the Euclidean space. In particular, we prove that if $A^*$ is the largest subset of diameter $r$ of $n$ points in the Euclidean space, then for every $\\epsilon>0$ there exists a polynomial time algorithm that outputs a set $B$ of size at least $|A^*|$ and of diameter at most $r(\\sqrt{2}+\\epsilon)$. On the hardness side, roughly speaking, we show that unless $P=NP$ for every $\\epsilon>0$ it is not possible to guarantee the diameter $r(\\sqrt{4/3}-\\epsilon)$ for $B$ even if the algorithm is allowed to output a set of size $({95\\over 94}-\\epsilon)^{-1}|A^*|$.",
        "published": "2006-12-31T06:14:03Z",
        "link": "http://arxiv.org/abs/cs/0701009v2",
        "categories": [
            "cs.CG",
            "math.MG"
        ]
    },
    {
        "title": "Topological Quantum Error Correction with Optimal Encoding Rate",
        "authors": [
            "H. Bombin",
            "M. A. Martin-Delgado"
        ],
        "summary": "We prove the existence of topological quantum error correcting codes with encoding rates $k/n$ asymptotically approaching the maximum possible value. Explicit constructions of these topological codes are presented using surfaces of arbitrary genus. We find a class of regular toric codes that are optimal. For physical implementations, we present planar topological codes.",
        "published": "2006-02-06T10:43:50Z",
        "link": "http://arxiv.org/abs/quant-ph/0602063v2",
        "categories": [
            "quant-ph",
            "cond-mat.str-el",
            "cs.GR",
            "hep-th",
            "math-ph",
            "math.AT",
            "math.CO",
            "math.MP"
        ]
    },
    {
        "title": "Implementation of float-float operators on graphics hardware",
        "authors": [
            "Guillaume Da Graçca",
            "David Defour"
        ],
        "summary": "The Graphic Processing Unit (GPU) has evolved into a powerful and flexible processor. The latest graphic processors provide fully programmable vertex and pixel processing units that support vector operations up to single floating-point precision. This computational power is now being used for general-purpose computations. However, some applications require higher precision than single precision. This paper describes the emulation of a 44-bit floating-point number format and its corresponding operations. An implementation is presented along with performance and accuracy results.",
        "published": "2006-03-29T11:48:29Z",
        "link": "http://arxiv.org/abs/cs/0603115v1",
        "categories": [
            "cs.AR",
            "cs.GR"
        ]
    },
    {
        "title": "Graphics Turing Test",
        "authors": [
            "Michael McGuigan"
        ],
        "summary": "We define a Graphics Turing Test to measure graphics performance in a similar manner to the definition of the traditional Turing Test. To pass the test one needs to reach a computational scale, the Graphics Turing Scale, for which Computer Generated Imagery becomes comparatively indistinguishable from real images while also being interactive. We derive an estimate for this computational scale which, although large, is within reach of todays supercomputers. We consider advantages and disadvantages of various computer systems designed to pass the Graphics Turing Test. Finally we discuss commercial applications from the creation of such a system, in particular Interactive Cinema.",
        "published": "2006-03-31T19:58:30Z",
        "link": "http://arxiv.org/abs/cs/0603132v1",
        "categories": [
            "cs.GR",
            "I.3.7"
        ]
    },
    {
        "title": "A parent-centered radial layout algorithm for interactive graph   visualization and animation",
        "authors": [
            "Andrew Pavlo",
            "Christopher Homan",
            "Jonathan Schull"
        ],
        "summary": "We have developed (1) a graph visualization system that allows users to explore graphs by viewing them as a succession of spanning trees selected interactively, (2) a radial graph layout algorithm, and (3) an animation algorithm that generates meaningful visualizations and smooth transitions between graphs while minimizing edge crossings during transitions and in static layouts.   Our system is similar to the radial layout system of Yee et al. (2001), but differs primarily in that each node is positioned on a coordinate system centered on its own parent rather than on a single coordinate system for all nodes. Our system is thus easy to define recursively and lends itself to parallelization. It also guarantees that layouts have many nice properties, such as: it guarantees certain edges never cross during an animation.   We compared the layouts and transitions produced by our algorithms to those produced by Yee et al. Results from several experiments indicate that our system produces fewer edge crossings during transitions between graph drawings, and that the transitions more often involve changes in local scaling rather than structure.   These findings suggest the system has promise as an interactive graph exploration tool in a variety of settings.",
        "published": "2006-06-01T16:56:55Z",
        "link": "http://arxiv.org/abs/cs/0606007v1",
        "categories": [
            "cs.HC",
            "cs.CG",
            "cs.GR",
            "I.3.3; H.5.0"
        ]
    },
    {
        "title": "Simple Methods For Drawing Rational Surfaces as Four or Six Bezier   Patches",
        "authors": [
            "Jean Gallier"
        ],
        "summary": "In this paper, we give several simple methods for drawing a whole rational surface (without base points) as several Bezier patches. The first two methods apply to surfaces specified by triangular control nets and partition the real projective plane RP2 into four and six triangles respectively. The third method applies to surfaces specified by rectangular control nets and partitions the torus RP1 X RP1 into four rectangular regions. In all cases, the new control nets are obtained by sign flipping and permutation of indices from the original control net. The proofs that these formulae are correct involve very little computations and instead exploit the geometry of the parameter space (RP2 or RP1 X RP1). We illustrate our method on some classical examples. We also propose a new method for resolving base points using a simple ``blowing up'' technique involving the computation of ``resolved'' control nets.",
        "published": "2006-06-12T22:02:41Z",
        "link": "http://arxiv.org/abs/cs/0606055v1",
        "categories": [
            "cs.CG",
            "cs.GR"
        ]
    },
    {
        "title": "Fast and Simple Methods For Computing Control Points",
        "authors": [
            "Jean Gallier",
            "Weqing Gu"
        ],
        "summary": "The purpose of this paper is to present simple and fast methods for computing control points for polynomial curves and polynomial surfaces given explicitly in terms of polynomials (written as sums of monomials). We give recurrence formulae w.r.t. arbitrary affine frames. As a corollary, it is amusing that we can also give closed-form expressions in the case of the frame (r, s) for curves, and the frame ((1, 0, 0), (0, 1, 0), (0, 0, 1) for surfaces. Our methods have the same low polynomial (time and space) complexity as the other best known algorithms, and are very easy to implement.",
        "published": "2006-06-13T00:47:34Z",
        "link": "http://arxiv.org/abs/cs/0606056v1",
        "categories": [
            "cs.CC",
            "cs.GR"
        ]
    },
    {
        "title": "On the Efficiency of Strategies for Subdividing Polynomial Triangular   Surface Patches",
        "authors": [
            "Jean Gallier"
        ],
        "summary": "In this paper, we investigate the efficiency of various strategies for subdividing polynomial triangular surface patches. We give a simple algorithm performing a regular subdivision in four calls to the standard de Casteljau algorithm (in its subdivision version). A naive version uses twelve calls. We also show that any method for obtaining a regular subdivision using the standard de Casteljau algorithm requires at least 4 calls. Thus, our method is optimal. We give another subdivision algorithm using only three calls to the de Casteljau algorithm. Instead of being regular, the subdivision pattern is diamond-like. Finally, we present a ``spider-like'' subdivision scheme producing six subtriangles in four calls to the de Casteljau algorithm.",
        "published": "2006-06-13T15:09:24Z",
        "link": "http://arxiv.org/abs/cs/0606061v1",
        "categories": [
            "cs.CG",
            "cs.GR"
        ]
    },
    {
        "title": "Outlier Robust ICP for Minimizing Fractional RMSD",
        "authors": [
            "Jeff M. Phillips",
            "Ran Liu",
            "Carlo Tomasi"
        ],
        "summary": "We describe a variation of the iterative closest point (ICP) algorithm for aligning two point sets under a set of transformations. Our algorithm is superior to previous algorithms because (1) in determining the optimal alignment, it identifies and discards likely outliers in a statistically robust manner, and (2) it is guaranteed to converge to a locally optimal solution. To this end, we formalize a new distance measure, fractional root mean squared distance (frmsd), which incorporates the fraction of inliers into the distance function. We lay out a specific implementation, but our framework can easily incorporate most techniques and heuristics from modern registration algorithms. We experimentally validate our algorithm against previous techniques on 2 and 3 dimensional data exposed to a variety of outlier types.",
        "published": "2006-06-22T15:35:56Z",
        "link": "http://arxiv.org/abs/cs/0606098v1",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "Interactive Hatching and Stippling by Example",
        "authors": [
            "Pascal Barla",
            "Simon Breslav",
            "Lee Markosian",
            "Joëlle Thollot"
        ],
        "summary": "We describe a system that lets a designer interactively draw patterns of strokes in the picture plane, then guide the synthesis of similar patterns over new picture regions. Synthesis is based on an initial user-assisted analysis phase in which the system recognizes distinct types of strokes (hatching and stippling) and organizes them according to perceptual grouping criteria. The synthesized strokes are produced by combining properties (eg. length, orientation, parallelism, proximity) of the stroke groups extracted from the input examples. We illustrate our technique with a drawing application that allows the control of attributes and scale-dependent reproduction of the synthesized patterns.",
        "published": "2006-07-11T19:01:41Z",
        "link": "http://arxiv.org/abs/cs/0607050v2",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "A Vortex Method for Bi-phasic Fluids Interacting with Rigid Bodies",
        "authors": [
            "Mathieu Coquerelle",
            "Jérémie Allard",
            "Georges-Henri Cottet",
            "Marie-Paule Cani"
        ],
        "summary": "We present an accurate Lagrangian method based on vortex particles, level-sets, and immersed boundary methods, for animating the interplay between two fluids and rigid solids. We show that a vortex method is a good choice for simulating bi-phase flow, such as liquid and gas, with a good level of realism. Vortex particles are localized at the interfaces between the two fluids and within the regions of high turbulence. We gain local precision and efficiency from the stable advection permitted by the vorticity formulation. Moreover, our numerical method straightforwardly solves the two-way coupling problem between the fluids and animated rigid solids. This new approach is validated through numerical comparisons with reference experiments from the computational fluid community. We also show that the visually appealing results obtained in the CG community can be reproduced with increased efficiency and an easier implementation.",
        "published": "2006-07-24T12:25:59Z",
        "link": "http://arxiv.org/abs/math/0607597v1",
        "categories": [
            "math.NA",
            "cs.GR",
            "ACM I.3.7 ACM I.3.5"
        ]
    },
    {
        "title": "On a solution to display non-filled-in quaternionic Julia sets",
        "authors": [
            "Alessandro Rosa"
        ],
        "summary": "During early 1980s, the so-called `escape time' method, developed to display the Julia sets for complex dynamical systems, was exported to quaternions in order to draw analogous pictures in this wider numerical field. Despite of the fine results in the complex plane, where all topological configurations of Julia sets have been successfully displayed, the `escape time' method fails to render properly the non-filled-in variety of quaternionic Julia sets. So their digital visualisation remained an open problem for several years. Both the solution for extending this old method to non-filled-in quaternionic Julia sets and its implementation into a program are explained here.",
        "published": "2006-08-01T19:25:17Z",
        "link": "http://arxiv.org/abs/cs/0608003v2",
        "categories": [
            "cs.GR",
            "cs.MS",
            "math.DS"
        ]
    },
    {
        "title": "One method for proving inequalities by computer",
        "authors": [
            "Branko J. Malesevic"
        ],
        "summary": "In this article we consider a method for proving a class of analytical inequalities via minimax rational approximations. All numerical calculations in this paper are given by Maple computer program.",
        "published": "2006-08-31T14:59:20Z",
        "link": "http://arxiv.org/abs/math/0608789v7",
        "categories": [
            "math.CA",
            "cs.GR",
            "cs.MS",
            "cs.NA",
            "math.GM",
            "math.NA",
            "26Dxx, 33F05, 41A20"
        ]
    },
    {
        "title": "Non-photorealistic image rendering with a labyrinthine tiling",
        "authors": [
            "A. Sparavigna",
            "B. Montrucchio"
        ],
        "summary": "The paper describes a new image processing for a non-photorealistic rendering. The algorithm is based on a random generation of gray tones and competing statistical requirements. The gray tone value of each pixel in the starting image is replaced selecting among randomly generated tone values, according to the statistics of nearest-neighbor and next-nearest-neighbor pixels. Two competing conditions for replacing the tone values - one position on the local mean value the other on the local variance - produce a peculiar pattern on the image. This pattern has a labyrinthine tiling aspect. For certain subjects, the pattern enhances the look of the image.",
        "published": "2006-09-15T07:21:11Z",
        "link": "http://arxiv.org/abs/cs/0609084v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "Vector field visualization with streamlines",
        "authors": [
            "A. Sparavigna",
            "B. Montrucchio"
        ],
        "summary": "We have recently developed an algorithm for vector field visualization with oriented streamlines, able to depict the flow directions everywhere in a dense vector field and the sense of the local orientations. The algorithm has useful applications in the visualization of the director field in nematic liquid crystals. Here we propose an improvement of the algorithm able to enhance the visualization of the local magnitude of the field. This new approach of the algorithm is compared with the same procedure applied to the Line Integral Convolution (LIC) visualization.",
        "published": "2006-10-14T09:25:52Z",
        "link": "http://arxiv.org/abs/cs/0610088v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "Integration of navigation and action selection functionalities in a   computational model of cortico-basal ganglia-thalamo-cortical loops",
        "authors": [
            "Benoît Girard",
            "David Filliat",
            "Jean-Arcady Meyer",
            "Alain Berthoz",
            "Agnès Guillot"
        ],
        "summary": "This article describes a biomimetic control architecture affording an animat both action selection and navigation functionalities. It satisfies the survival constraint of an artificial metabolism and supports several complementary navigation strategies. It builds upon an action selection model based on the basal ganglia of the vertebrate brain, using two interconnected cortico-basal ganglia-thalamo-cortical loops: a ventral one concerned with appetitive actions and a dorsal one dedicated to consummatory actions. The performances of the resulting model are evaluated in simulation. The experiments assess the prolonged survival permitted by the use of high level navigation strategies and the complementarity of navigation strategies in dynamic environments. The correctness of the behavioral choices in situations of antagonistic or synergetic internal states are also tested. Finally, the modelling choices are discussed with regard to their biomimetic plausibility, while the experimental results are estimated in terms of animat adaptivity.",
        "published": "2006-01-03T10:39:24Z",
        "link": "http://arxiv.org/abs/cs/0601004v1",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "New Technologies for Sustainable Urban Transport in Europe",
        "authors": [
            "Michel Parent"
        ],
        "summary": "In the past few years, the European Commission has financed several projects to examine how new technologies could improve the sustainability of European cities. These technologies concern new public transportation modes such as guided buses to form high capacity networks similar to light rail but at a lower cost and better flexibility, PRT (Personal Rapid Transit) and cybercars (small urban vehicles with fully automatic driving capabilities to be used in carsharing mode, mostly as a complement to mass transport). They also concern private vehicles with technologies which could improve the efficiency of the vehicles as well as their safety (Intelligent Speed Adaptation, Adaptive Cruise >.Control, Stop&Go, Lane Keeping,...) and how these new vehicles can complement mass transport in the form of car-sharing services.",
        "published": "2006-01-10T13:57:28Z",
        "link": "http://arxiv.org/abs/cs/0601040v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Wavefront Propagation and Fuzzy Based Autonomous Navigation",
        "authors": [
            "Adel Al-Jumaily",
            "Cindy Leung"
        ],
        "summary": "Path planning and obstacle avoidance are the two major issues in any navigation system. Wavefront propagation algorithm, as a good path planner, can be used to determine an optimal path. Obstacle avoidance can be achieved using possibility theory. Combining these two functions enable a robot to autonomously navigate to its destination. This paper presents the approach and results in implementing an autonomous navigation system for an indoor mobile robot. The system developed is based on a laser sensor used to retrieve data to update a two dimensional world model of therobot environment. Waypoints in the path are incorporated into the obstacle avoidance. Features such as ageing of objects and smooth motion planning are implemented to enhance efficiency and also to cater for dynamic environments.",
        "published": "2006-01-14T08:09:23Z",
        "link": "http://arxiv.org/abs/cs/0601053v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Control of a Lightweight Flexible Robotic Arm Using Sliding Modes",
        "authors": [
            "Victor Etxebarria",
            "Arantza Sanz",
            "Ibone Lizarraga"
        ],
        "summary": "This paper presents a robust control scheme for flexible link robotic manipulators, which is based on considering the flexible mechanical structure as a system with slow (rigid) and fast (flexible) modes that can be controlled separately. The rigid dynamics is controlled by means of a robust sliding-mode approach with wellestablished stability properties while an LQR optimal design is adopted for the flexible dynamics. Experimental results show that this composite approach achieves good closed loop tracking properties both for the rigid and the flexible dynamics.",
        "published": "2006-01-14T08:10:49Z",
        "link": "http://arxiv.org/abs/cs/0601054v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Hybrid Three Layer Architecture for Fire Agent Management in Rescue   Simulation Environment",
        "authors": [
            "Alborz Geramifard",
            "Peyman Nayeri",
            "Reza Zamani-Nasab",
            "Jafar Habibi"
        ],
        "summary": "This paper presents a new architecture called FAIS for imple- menting intelligent agents cooperating in a special Multi Agent environ- ment, namely the RoboCup Rescue Simulation System. This is a layered architecture which is customized for solving fire extinguishing problem. Structural decision making algorithms are combined with heuristic ones in this model, so it's a hybrid architecture.",
        "published": "2006-01-14T08:12:02Z",
        "link": "http://arxiv.org/abs/cs/0601055v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Dynamic Balance Control of Multi-arm Free-Floating Space Robots",
        "authors": [
            "Panfeng Huang",
            "Yangsheng Xu",
            "Bin Liang"
        ],
        "summary": "This paper investigates the problem of the dynamic balance control of multi-arm free-floating space robot during capturing an active object in close proximity. The position and orientation of space base will be affected during the operation of space manipulator because of the dynamics coupling between the manipulator and space base. This dynamics coupling is unique characteristics of space robot system. Such a disturbance will produce a serious impact between the manipulator hand and the object. To ensure reliable and precise operation, we propose to develop a space robot system consisting of two arms, with one arm (mission arm) for accomplishing the capture mission, and the other one (balance arm) compensating for the disturbance of the base. We present the coordinated control concept for balance of the attitude of the base using the balance arm. The mission arm can move along the given trajectory to approach and capture the target with no considering the disturbance from the coupling of the base. We establish a relationship between the motion of two arm that can realize the zeros reaction to the base. The simulation studies verified the validity and efficiency of the proposed control method.",
        "published": "2006-01-14T08:13:06Z",
        "link": "http://arxiv.org/abs/cs/0601056v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Robust Motion Control for Mobile Manipulator Using Resolved Acceleration   and Proportional-Integral Active Force Control",
        "authors": [
            "Musa Mailah",
            "Endra Pitowarno",
            "Hishamuddin Jamaluddin"
        ],
        "summary": "A resolved acceleration control (RAC) and proportional-integral active force control (PIAFC) is proposed as an approach for the robust motion control of a mobile manipulator (MM) comprising a differentially driven wheeled mobile platform with a two-link planar arm mounted on top of the platform. The study emphasizes on the integrated kinematic and dynamic control strategy in which the RAC is used to manipulate the kinematic component while the PIAFC is implemented to compensate the dynamic effects including the bounded known/unknown disturbances and uncertainties. The effectivenss and robustness of the proposed scheme are investigated through a rigorous simulation study and later complemented with experimental results obtained through a number of experiments performed on a fully developed working prototype in a laboratory environment. A number of disturbances in the form of vibratory and impact forces are deliberately introduced into the system to evaluate the system performances. The investigation clearly demonstrates the extreme robustness feature of the proposed control scheme compared to other systems considered in the study.",
        "published": "2006-01-14T08:14:10Z",
        "link": "http://arxiv.org/abs/cs/0601057v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "CAGD - Computer Aided Gripper Design for a Flexible Gripping System",
        "authors": [
            "Michael Sdahl",
            "Bernd Kuhlenkoetter"
        ],
        "summary": "This paper is a summary of the recently accomplished research work on flexible gripping systems. The goal is to develop a gripper which can be used for a great amount of geometrically variant workpieces. The economic aspect is of particular importance during the whole development. The high flexibility of the gripper is obtained by three parallel used principles. These are human and computer based analysis of the gripping object as well as mechanical adaptation of the gripper to the object with the help of servo motors. The focus is on the gripping of free-form surfaces with suction cup.",
        "published": "2006-01-14T08:15:47Z",
        "link": "http://arxiv.org/abs/cs/0601058v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Descriptive Model of Robot Team and the Dynamic Evolution of Robot   Team Cooperation",
        "authors": [
            "Shu-qin Li",
            "Lan Shuai",
            "Xian-yi Cheng",
            "Zhen-min Tang",
            "Jing-yu Yang"
        ],
        "summary": "At present, the research on robot team cooperation is still in qualitative analysis phase and lacks the description model that can quantitatively describe the dynamical evolution of team cooperative relationships with constantly changeable task demand in Multi-robot field. First this paper whole and static describes organization model HWROM of robot team, then uses Markov course and Bayesian theorem for reference, dynamical describes the team cooperative relationships building. Finally from cooperative entity layer, ability layer and relative layer we research team formation and cooperative mechanism, and discuss how to optimize relative action sets during the evolution. The dynamic evolution model of robot team and cooperative relationships between robot teams proposed and described in this paper can not only generalize the robot team as a whole, but also depict the dynamic evolving process quantitatively. Users can also make the prediction of the cooperative relationship and the action of the robot team encountering new demands based on this model. Journal web page & a lot of robotic related papers www.ars-journal.com",
        "published": "2006-01-14T08:18:15Z",
        "link": "http://arxiv.org/abs/cs/0601059v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Robot Swarms in an Uncertain World: Controllable Adaptability",
        "authors": [
            "Olga Bogatyreva",
            "Alexandr Shillerov"
        ],
        "summary": "There is a belief that complexity and chaos are essential for adaptability. But life deals with complexity every moment, without the chaos that engineers fear so, by invoking goal-directed behaviour. Goals can be programmed. That is why living organisms give us hope to achieve adaptability in robots. In this paper a method for the description of a goal-directed, or programmed, behaviour, interacting with uncertainty of environment, is described. We suggest reducing the structural (goals, intentions) and stochastic components (probability to realise the goal) of individual behaviour to random variables with nominal values to apply probabilistic approach. This allowed us to use a Normalized Entropy Index to detect the system state by estimating the contribution of each agent to the group behaviour. The number of possible group states is 27. We argue that adaptation has a limited number of possible paths between these 27 states. Paths and states can be programmed so that after adjustment to any particular case of task and conditions, adaptability will never involve chaos. We suggest the application of the model to operation of robots or other devices in remote and/or dangerous places.",
        "published": "2006-01-14T08:20:26Z",
        "link": "http://arxiv.org/abs/cs/0601060v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Modular Adaptive System Based on a Multi-Stage Neural Structure for   Recognition of 2D Objects of Discontinuous Production",
        "authors": [
            "I. Topalova"
        ],
        "summary": "This is a presentation of a new system for invariant recognition of 2D objects with overlapping classes, that can not be effectively recognized with the traditional methods. The translation, scale and partial rotation invariant contour object description is transformed in a DCT spectrum space. The obtained frequency spectrums are decomposed into frequency bands in order to feed different BPG neural nets (NNs). The NNs are structured in three stages - filtering and full rotation invariance; partial recognition; general classification. The designed multi-stage BPG Neural Structure shows very good accuracy and flexibility when tested with 2D objects used in the discontinuous production. The reached speed and the opportunuty for an easy restructuring and reprogramming of the system makes it suitable for application in different applied systems for real time work.",
        "published": "2006-01-14T08:25:25Z",
        "link": "http://arxiv.org/abs/cs/0601061v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Study of Self-Organization Model of Multiple Mobile Robot",
        "authors": [
            "Ceng Xian-yi",
            "Li Shu-qin",
            "Xia De-shen"
        ],
        "summary": "A good organization model of multiple mobile robot should be able to improve the efficiency of the system, reduce the complication of robot interactions, and detract the difficulty of computation. From the sociology aspect of topology, structure and organization, this paper studies the multiple mobile robot organization formation and running mechanism in the dynamic, complicated and unknown environment. It presents and describes in detail a Hierarchical- Web Recursive Organization Model (HWROM) and forming algorithm. It defines the robot society leader; robotic team leader and individual robot as the same structure by the united framework and describes the organization model by the recursive structure. The model uses task-oriented and top-down method to dynamically build and maintain structures and organization. It uses market-based techniques to assign task, form teams and allocate resources in dynamic environment. The model holds several characteristics of self-organization, dynamic, conciseness, commonness and robustness.",
        "published": "2006-01-14T08:27:04Z",
        "link": "http://arxiv.org/abs/cs/0601062v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Optimal Point-to-Point Trajectory Tracking of Redundant Manipulators   using Generalized Pattern Search",
        "authors": [
            "Atef A. Ata",
            "Thi Rein Myo"
        ],
        "summary": "Optimal point-to-point trajectory planning for planar redundant manipulator is considered in this study. The main objective is to minimize the sum of the position error of the end-effector at each intermediate point along the trajectory so that the end-effector can track the prescribed trajectory accurately. An algorithm combining Genetic Algorithm and Pattern Search as a Generalized Pattern Search GPS is introduced to design the optimal trajectory. To verify the proposed algorithm, simulations for a 3-D-O-F planar manipulator with different end-effector trajectories have been carried out. A comparison between the Genetic Algorithm and the Generalized Pattern Search shows that The GPS gives excellent tracking performance.",
        "published": "2006-01-14T08:29:07Z",
        "link": "http://arxiv.org/abs/cs/0601063v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Robotics Vision-based Heuristic Reasoning for Underwater Target Tracking   and Navigation",
        "authors": [
            "Chua Kia",
            "Mohd Rizal Arshad"
        ],
        "summary": "This paper presents a robotics vision-based heuristic reasoning system for underwater target tracking and navigation. This system is introduced to improve the level of automation of underwater Remote Operated Vehicles (ROVs) operations. A prototype which combines computer vision with an underwater robotics system is successfully designed and developed to perform target tracking and intelligent navigation. ...",
        "published": "2006-01-14T08:34:52Z",
        "link": "http://arxiv.org/abs/cs/0601064v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "New Intelligent Transmission Concept for Hybrid Mobile Robot Speed   Control",
        "authors": [
            "Nazim Mir-Nasiri",
            "Sulaiman Hussaini"
        ],
        "summary": "This paper presents a new concept of a mobile robot speed control by using two degree of freedom gear transmission. The developed intelligent speed controller utilizes a gear box which comprises of epicyclic gear train with two inputs, one coupled with the engine shaft and another with the shaft of a variable speed dc motor. The net output speed is a combination of the two input speeds and is governed by the transmission ratio of the planetary gear train. This new approach eliminates the use of a torque converter which is otherwise an indispensable part of all available automatic transmissions, thereby reducing the power loss that occurs in the box during the fluid coupling. By gradually varying the speed of the dc motor a stepless transmission has been achieved. The other advantages of the developed controller are pulling over and reversing the vehicle, implemented by intelligent mixing of the dc motor and engine speeds. This approach eliminates traditional braking system in entire vehicle design. The use of two power sources, IC engine and battery driven DC motor, utilizes the modern idea of hybrid vehicles. The new mobile robot speed controller is capable of driving the vehicle even in extreme case of IC engine failure, for example, due to gas depletion.",
        "published": "2006-01-14T08:35:52Z",
        "link": "http://arxiv.org/abs/cs/0601065v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Asymptotic constant-factor approximation algorithm for the Traveling   Salesperson Problem for Dubins' vehicle",
        "authors": [
            "Ketan Savla",
            "Emilio Frazzoli",
            "Francesco Bullo"
        ],
        "summary": "This article proposes the first known algorithm that achieves a constant-factor approximation of the minimum length tour for a Dubins' vehicle through $n$ points on the plane. By Dubins' vehicle, we mean a vehicle constrained to move at constant speed along paths with bounded curvature without reversing direction. For this version of the classic Traveling Salesperson Problem, our algorithm closes the gap between previously established lower and upper bounds; the achievable performance is of order $n^{2/3}$.",
        "published": "2006-03-02T07:07:55Z",
        "link": "http://arxiv.org/abs/cs/0603010v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Vers une commande multivariable sans modèle",
        "authors": [
            "Michel Fliess",
            "Cédric Join",
            "Mamadou Mboup",
            "Hebertt Sira-Ramirez"
        ],
        "summary": "A control strategy without any precise mathematical model is derived for linear or nonlinear systems which are assumed to be finite-dimensional. Two convincing numerical simulations are provided.",
        "published": "2006-03-07T08:15:15Z",
        "link": "http://arxiv.org/abs/math/0603155v3",
        "categories": [
            "math.OC",
            "cs.CE",
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "Metric State Space Reinforcement Learning for a Vision-Capable Mobile   Robot",
        "authors": [
            "Viktor Zhumatiy",
            "Faustino Gomez",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ],
        "summary": "We address the problem of autonomously learning controllers for vision-capable mobile robots. We extend McCallum's (1995) Nearest-Sequence Memory algorithm to allow for general metrics over state-action trajectories. We demonstrate the feasibility of our approach by successfully running our algorithm on a real mobile robot. The algorithm is novel and unique in that it (a) explores the environment and learns directly on a mobile robot without using a hand-made computer model as an intermediate step, (b) does not require manual discretization of the sensor input space, (c) works in piecewise continuous perceptual spaces, and (d) copes with partial observability. Together this allows learning from much less experience compared to previous methods.",
        "published": "2006-03-07T08:44:29Z",
        "link": "http://arxiv.org/abs/cs/0603023v1",
        "categories": [
            "cs.RO",
            "cs.LG"
        ]
    },
    {
        "title": "The Snowblower Problem",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Joseph S. B. Mitchell",
            "Valentin Polishchuk"
        ],
        "summary": "We introduce the snowblower problem (SBP), a new optimization problem that is closely related to milling problems and to some material-handling problems. The objective in the SBP is to compute a short tour for the snowblower to follow to remove all the snow from a domain (driveway, sidewalk, etc.). When a snowblower passes over each region along the tour, it displaces snow into a nearby region. The constraint is that if the snow is piled too high, then the snowblower cannot clear the pile.   We give an algorithmic study of the SBP. We show that in general, the problem is NP-complete, and we present polynomial-time approximation algorithms for removing snow under various assumptions about the operation of the snowblower. Most commercially-available snowblowers allow the user to control the direction in which the snow is thrown. We differentiate between the cases in which the snow can be thrown in any direction, in any direction except backwards, and only to the right. For all cases, we give constant-factor approximation algorithms; the constants increase as the throw direction becomes more restricted.   Our results are also applicable to robotic vacuuming (or lawnmowing) with bounded capacity dust bin and to some versions of material-handling problems, in which the goal is to rearrange cartons on the floor of a warehouse.",
        "published": "2006-03-07T20:35:48Z",
        "link": "http://arxiv.org/abs/cs/0603026v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.RO",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Predicting the Path of an Open System",
        "authors": [
            "S. Z. Stefanov"
        ],
        "summary": "The expected path of an open system,which is a big Poincare system,has been found in this paper.This path has been obtained from the actual and from the expected droop of the open system.The actual droop has been reconstructed from the variations in the power and in the frequency of the open system.The expected droop has been found as a function of rotation from the expected potential energy of the open system under synchronization of that system.",
        "published": "2006-03-17T09:27:01Z",
        "link": "http://arxiv.org/abs/cs/0603070v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Modeling and Mathematical Analysis of Swarms of Microscopic Robots",
        "authors": [
            "Aram Galstyan",
            "Tad Hogg",
            "Kristina Lerman"
        ],
        "summary": "The biologically-inspired swarm paradigm is being used to design self-organizing systems of locally interacting artificial agents. A major difficulty in designing swarms with desired characteristics is understanding the causal relation between individual agent and collective behaviors. Mathematical analysis of swarm dynamics can address this difficulty to gain insight into system design. This paper proposes a framework for mathematical modeling of swarms of microscopic robots that may one day be useful in medical applications. While such devices do not yet exist, the modeling approach can be helpful in identifying various design trade-offs for the robots and be a useful guide for their eventual fabrication. Specifically, we examine microscopic robots that reside in a fluid, for example, a bloodstream, and are able to detect and respond to different chemicals. We present the general mathematical model of a scenario in which robots locate a chemical source. We solve the scenario in one-dimension and show how results can be used to evaluate certain design decisions.",
        "published": "2006-04-27T23:33:25Z",
        "link": "http://arxiv.org/abs/cs/0604110v1",
        "categories": [
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "Analysis of Dynamic Task Allocation in Multi-Robot Systems",
        "authors": [
            "Kristina Lerman",
            "Chris Jones",
            "Aram Galstyan",
            "Maja J Mataric"
        ],
        "summary": "Dynamic task allocation is an essential requirement for multi-robot systems operating in unknown dynamic environments. It allows robots to change their behavior in response to environmental changes or actions of other robots in order to improve overall system performance. Emergent coordination algorithms for task allocation that use only local sensing and no direct communication between robots are attractive because they are robust and scalable. However, a lack of formal analysis tools makes emergent coordination algorithms difficult to design. In this paper we present a mathematical model of a general dynamic task allocation mechanism. Robots using this mechanism have to choose between two types of task, and the goal is to achieve a desired task division in the absence of explicit communication and global knowledge. Robots estimate the state of the environment from repeated local observations and decide which task to choose based on these observations. We model the robots and observations as stochastic processes and study the dynamics of the collective behavior. Specifically, we analyze the effect that the number of observations and the choice of the decision function have on the performance of the system. The mathematical models are validated in a multi-robot multi-foraging scenario. The model's predictions agree very closely with experimental results from sensor-based simulations.",
        "published": "2006-04-27T23:56:10Z",
        "link": "http://arxiv.org/abs/cs/0604111v1",
        "categories": [
            "cs.RO",
            "cs.MA"
        ]
    },
    {
        "title": "Curve Shortening and the Rendezvous Problem for Mobile Autonomous Robots",
        "authors": [
            "Stephen L. Smith",
            "Mireille E. Broucke",
            "Bruce A. Francis"
        ],
        "summary": "If a smooth, closed, and embedded curve is deformed along its normal vector field at a rate proportional to its curvature, it shrinks to a circular point. This curve evolution is called Euclidean curve shortening and the result is known as the Gage-Hamilton-Grayson Theorem. Motivated by the rendezvous problem for mobile autonomous robots, we address the problem of creating a polygon shortening flow. A linear scheme is proposed that exhibits several analogues to Euclidean curve shortening: The polygon shrinks to an elliptical point, convex polygons remain convex, and the perimeter of the polygon is monotonically decreasing.",
        "published": "2006-05-16T22:30:01Z",
        "link": "http://arxiv.org/abs/cs/0605070v1",
        "categories": [
            "cs.RO",
            "cs.MA",
            "I.2.9"
        ]
    },
    {
        "title": "Cross-Entropic Learning of a Machine for the Decision in a Partially   Observable Universe",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "Revision of the paper previously entitled \"Learning a Machine for the Decision in a Partially Observable Markov Universe\" In this paper, we are interested in optimal decisions in a partially observable universe. Our approach is to directly approximate an optimal strategic tree depending on the observation. This approximation is made by means of a parameterized probabilistic law. A particular family of hidden Markov models, with input \\emph{and} output, is considered as a model of policy. A method for optimizing the parameters of these HMMs is proposed and applied. This optimization is based on the cross-entropic principle for rare events simulation developed by Rubinstein.",
        "published": "2006-05-18T07:47:58Z",
        "link": "http://arxiv.org/abs/math/0605498v1",
        "categories": [
            "math.OC",
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Circle Formation of Weak Robots and Lyndon Words",
        "authors": [
            "Yoann Dieudonné",
            "Franck Petit"
        ],
        "summary": "A Lyndon word is a non-empty word strictly smaller in the lexicographic order than any of its suffixes, except itself and the empty word. In this paper, we show how Lyndon words can be used in the distributed control of a set of n weak mobile robots. By weak, we mean that the robots are anonymous, memoryless, without any common sense of direction, and unable to communicate in an other way than observation. An efficient and simple deterministic protocol to form a regular n-gon is presented and proven for n prime.",
        "published": "2006-05-22T12:09:33Z",
        "link": "http://arxiv.org/abs/cs/0605096v1",
        "categories": [
            "cs.DC",
            "cs.RO",
            "C.2.4"
        ]
    },
    {
        "title": "Circle Formation of Weak Mobile Robots",
        "authors": [
            "Yoann Dieudonne",
            "Ouiddad Labbani-Igbida",
            "Franck Petit"
        ],
        "summary": "In this paper we prove the conjecture of D\\'{e}fago & Konagaya. Furthermore, we describe a deterministic protocol for forming a regular n-gon in finite time.",
        "published": "2006-07-12T12:57:28Z",
        "link": "http://arxiv.org/abs/cs/0607060v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Utilisation de la substitution sensorielle par électro-stimulation   linguale pour la prévention des escarres chez les paraplégiques.   Etude préliminaire",
        "authors": [
            "Alexandre Moreau-Gaudry",
            "Fabien Robineau",
            "Pierre-Frédéric André",
            "Anne Prince",
            "Pierre Pauget",
            "Jacques Demongeot",
            "Yohan Payan"
        ],
        "summary": "Pressure ulcers are recognized as a major health issue in individuals with spinal cord injuries and new approaches to prevent this pathology are necessary. An innovative health strategy is being developed through the use of computer and sensory substitution via the tongue in order to compensate for the sensory loss in the buttock area for individuals with paraplegia. This sensory compensation will enable individuals with spinal cord injuries to be aware of a localized excess of pressure at the skin/seat interface and, consequently, will enable them to prevent the formation of pressure ulcers by relieving the cutaneous area of suffering. This work reports an initial evaluation of this approach and the feasibility of creating an adapted behavior, with a change in pressure as a response to electro-stimulated information on the tongue. Obtained during a clinical study in 10 healthy seated subjects, the first results are encouraging, with 92% success in 100 performed tests. These results, which have to be completed and validated in the paraplegic population, may lead to a new approach to education in health to prevent the formation of pressure ulcers within this population. Keywords: Spinal Cord Injuries, Pressure Ulcer, Sensory Substitution, Health Education, Biomedical Informatics.",
        "published": "2006-07-12T13:57:42Z",
        "link": "http://arxiv.org/abs/physics/0607116v1",
        "categories": [
            "physics.med-ph",
            "cs.RO",
            "q-bio.NC"
        ]
    },
    {
        "title": "Application Layer Definition and Analyses of Controller Area Network Bus   for Wire Harness Assembly Machine",
        "authors": [
            "Hui Guo",
            "Ying Jiang"
        ],
        "summary": "With the feature of multi-master bus access, nondestructive contention-based arbitration and flexible configuration, Controller Area Network (CAN) bus is applied into the control system of Wire Harness Assembly Machine (WHAM). To accomplish desired goal, the specific features of the CAN bus is analyzed by compared with other field buses and the functional performances in the CAN bus system of WHAM is discussed. Then the application layer planning of CAN bus for dynamic priority is presented. The critical issue for the use of CAN bus system in WHAM is the data transfer rate between different nodes. So processing efficient model is introduced to assist analyzing data transfer procedure. Through the model, it is convenient to verify the real time feature of the CAN bus system in WHAM.",
        "published": "2006-08-28T12:42:38Z",
        "link": "http://arxiv.org/abs/cs/0608105v2",
        "categories": [
            "cs.RO",
            "cs.NI"
        ]
    },
    {
        "title": "Traveing Salesperson Problems for a double integrator",
        "authors": [
            "Ketan Savla",
            "Francesco Bullo",
            "Emilio Frazzoli"
        ],
        "summary": "In this paper we propose some novel path planning strategies for a double integrator with bounded velocity and bounded control inputs. First, we study the following version of the Traveling Salesperson Problem (TSP): given a set of points in $\\real^d$, find the fastest tour over the point set for a double integrator. We first give asymptotic bounds on the time taken to complete such a tour in the worst-case. Then, we study a stochastic version of the TSP for double integrator where the points are randomly sampled from a uniform distribution in a compact environment in $\\real^2$ and $\\real^3$. We propose novel algorithms that perform within a constant factor of the optimal strategy with high probability. Lastly, we study a dynamic TSP: given a stochastic process that generates targets, is there a policy which guarantees that the number of unvisited targets does not diverge over time? If such stable policies exist, what is the minimum wait for a target? We propose novel stabilizing receding-horizon algorithms whose performances are within a constant factor from the optimum with high probability, in $\\real^2$ as well as $\\real^3$. We also argue that these algorithms give identical performances for a particular nonholonomic vehicle, Dubins vehicle.",
        "published": "2006-09-17T23:38:32Z",
        "link": "http://arxiv.org/abs/cs/0609097v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Motion Primitives for Robotic Flight Control",
        "authors": [
            "Baris E. Perk",
            "J. J. E. Slotine"
        ],
        "summary": "We introduce a simple framework for learning aggressive maneuvers in flight control of UAVs. Having inspired from biological environment, dynamic movement primitives are analyzed and extended using nonlinear contraction theory. Accordingly, primitives of an observed movement are stably combined and concatenated. We demonstrate our results experimentally on the Quanser Helicopter, in which we first imitate aggressive maneuvers and then use them as primitives to achieve new maneuvers that can fly over an obstacle.",
        "published": "2006-09-25T19:06:59Z",
        "link": "http://arxiv.org/abs/cs/0609140v2",
        "categories": [
            "cs.RO",
            "cs.LG"
        ]
    },
    {
        "title": "Evolving controllers for simulated car racing",
        "authors": [
            "Julian Togelius",
            "Simon M. Lucas"
        ],
        "summary": "This paper describes the evolution of controllers for racing a simulated radio-controlled car around a track, modelled on a real physical track. Five different controller architectures were compared, based on neural networks, force fields and action sequences. The controllers use either egocentric (first person), Newtonian (third person) or no information about the state of the car (open-loop controller). The only controller that was able to evolve good racing behaviour was based on a neural network acting on egocentric inputs.",
        "published": "2006-11-02T00:47:57Z",
        "link": "http://arxiv.org/abs/cs/0611006v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "cs.RO"
        ]
    },
    {
        "title": "Multirobot rendezvous with visibility sensors in nonconvex environments",
        "authors": [
            "Anurag Ganguli",
            "Jorge Cortes",
            "Francesco Bullo"
        ],
        "summary": "This paper presents a coordination algorithm for mobile autonomous robots. Relying upon distributed sensing the robots achieve rendezvous, that is, they move to a common location. Each robot is a point mass moving in a nonconvex environment according to an omnidirectional kinematic model. Each robot is equipped with line-of-sight limited-range sensors, i.e., a robot can measure the relative position of any object (robots or environment boundary) if and only if the object is within a given distance and there are no obstacles in-between. The algorithm is designed using the notions of robust visibility, connectivity-preserving constraint sets, and proximity graphs. Simulations illustrate the theoretical results on the correctness of the proposed algorithm, and its performance in asynchronous setups and with sensor measurement and control errors.",
        "published": "2006-11-06T02:50:42Z",
        "link": "http://arxiv.org/abs/cs/0611022v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Distributed Control of Microscopic Robots in Biomedical Applications",
        "authors": [
            "Tad Hogg"
        ],
        "summary": "Current developments in molecular electronics, motors and chemical sensors could enable constructing large numbers of devices able to sense, compute and act in micron-scale environments. Such microscopic machines, of sizes comparable to bacteria, could simultaneously monitor entire populations of cells individually in vivo. This paper reviews plausible capabilities for microscopic robots and the physical constraints due to operation in fluids at low Reynolds number, diffusion-limited sensing and thermal noise from Brownian motion. Simple distributed controls are then presented in the context of prototypical biomedical tasks, which require control decisions on millisecond time scales. The resulting behaviors illustrate trade-offs among speed, accuracy and resource use. A specific example is monitoring for patterns of chemicals in a flowing fluid released at chemically distinctive sites. Information collected from a large number of such devices allows estimating properties of cell-sized chemical sources in a macroscopic volume. The microscopic devices moving with the fluid flow in small blood vessels can detect chemicals released by tissues in response to localized injury or infection. We find the devices can readily discriminate a single cell-sized chemical source from the background chemical concentration, providing high-resolution sensing in both time and space. By contrast, such a source would be difficult to distinguish from background when diluted throughout the blood volume as obtained with a blood sample.",
        "published": "2006-11-21T23:22:20Z",
        "link": "http://arxiv.org/abs/cs/0611111v1",
        "categories": [
            "cs.RO",
            "cs.MA",
            "I.2.9; I.2.11"
        ]
    },
    {
        "title": "Neural Computation with Rings of Quasiperiodic Oscillators",
        "authors": [
            "E. A. Rietman",
            "R. W. Hillis"
        ],
        "summary": "We describe the use of quasiperiodic oscillators for computation and control of robots. We also describe their relationship to central pattern generators in simple organisms and develop a group theory for describing the dynamics of these systems.",
        "published": "2006-11-27T15:28:57Z",
        "link": "http://arxiv.org/abs/cs/0611136v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "A Classification of 6R Manipulators",
        "authors": [
            "Ming-Zhe Chen"
        ],
        "summary": "This paper presents a classification of generic 6-revolute jointed (6R) manipulators using homotopy class of their critical point manifold. A part of classification is listed in this paper because of the complexity of homotopy class of 4-torus. The results of this classification will serve future research of the classification and topological properties of maniplators joint space and workspace.",
        "published": "2006-12-05T16:04:30Z",
        "link": "http://arxiv.org/abs/cs/0612029v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Efficient Query Answering over Conceptual Schemas of Relational   Databases : Technical Report",
        "authors": [
            "Mantas Simkus",
            "Evaldas Taroza",
            "Lina Lubyte",
            "Daniel Trivellato",
            "Zivile Norkunaite"
        ],
        "summary": "We develop a query answering system, where at the core of the work there is an idea of query answering by rewriting. For this purpose we extend the DL DL-Lite with the ability to support n-ary relations, obtaining the DL DLR-Lite, which is still polynomial in the size of the data. We devise a flexible way of mapping the conceptual level to the relational level, which provides the users an SQL-like query language over the conceptual schema. The rewriting technique adds value to conventional query answering techniques, allowing to formulate simpler queries, with the ability to infer additional information that was not stated explicitly in the user query. The formalization of the conceptual schema and the developed reasoning technique allow checking for consistency between the database and the conceptual schema, thus improving the trustiness of the information system.",
        "published": "2006-01-27T16:21:25Z",
        "link": "http://arxiv.org/abs/cs/0601114v2",
        "categories": [
            "cs.DB",
            "cs.LO"
        ]
    },
    {
        "title": "Conjunctive Queries over Trees",
        "authors": [
            "Georg Gottlob",
            "Christoph Koch",
            "Klaus U. Schulz"
        ],
        "summary": "We study the complexity and expressive power of conjunctive queries over unranked labeled trees represented using a variety of structure relations such as ``child'', ``descendant'', and ``following'' as well as unary relations for node labels. We establish a framework for characterizing structures representing trees for which conjunctive queries can be evaluated efficiently. Then we completely chart the tractability frontier of the problem and establish a dichotomy theorem for our axis relations, i.e., we find all subset-maximal sets of axes for which query evaluation is in polynomial time and show that for all other cases, query evaluation is NP-complete. All polynomial-time results are obtained immediately using the proof techniques from our framework. Finally, we study the expressiveness of conjunctive queries over trees and show that for each conjunctive query, there is an equivalent acyclic positive query (i.e., a set of acyclic conjunctive queries), but that in general this query is not of polynomial size.",
        "published": "2006-02-02T23:28:24Z",
        "link": "http://arxiv.org/abs/cs/0602004v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "E.1; F.1.3; F.2.2; H.2.3; H.2.4; I.7.2"
        ]
    },
    {
        "title": "A Visual Query Language for Complex-Value Databases",
        "authors": [
            "Christoph Koch"
        ],
        "summary": "In this paper, a visual language, VCP, for queries on complex-value databases is proposed. The main strength of the new language is that it is purely visual: (i) It has no notion of variable, quantification, partiality, join, pattern matching, regular expression, recursion, or any other construct proper to logical, functional, or other database query languages and (ii) has a very natural, strong, and intuitive design metaphor. The main operation is that of copying and pasting in a schema tree.   We show that despite its simplicity, VCP precisely captures complex-value algebra without powerset, or equivalently, monad algebra with union and difference. Thus, its expressive power is precisely that of the language that is usually considered to play the role of relational algebra for complex-value databases.",
        "published": "2006-02-03T19:22:28Z",
        "link": "http://arxiv.org/abs/cs/0602006v1",
        "categories": [
            "cs.DB",
            "cs.HC"
        ]
    },
    {
        "title": "Path Summaries and Path Partitioning in Modern XML Databases",
        "authors": [
            "Andrei Arion",
            "Angela Bonifati",
            "Ioana Manolescu",
            "Andrea Pugliese"
        ],
        "summary": "We study the applicability of XML path summaries in the context of current-day XML databases. We find that summaries provide an excellent basis for optimizing data access methods, which furthermore mixes very well with path-partitioned stores. We provide practical algorithms for building and exploiting summaries, and prove its benefits through extensive experiments.",
        "published": "2006-02-10T12:21:42Z",
        "link": "http://arxiv.org/abs/cs/0602039v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "The OverRelational Manifesto",
        "authors": [
            "Evgeniy Grigoriev"
        ],
        "summary": "The OverRelational Manifesto (below ORM) proposes a possible approach to creation of data storage systems of the next generation. ORM starts from the requirement that information in a relational database is represented by a set of relation values. Accordingly, it is assumed that the information about any entity of an enterprise must also be represented as a set of relation values (the ORM main requirement). A system of types is introduced, which allows one to fulfill the main requirement. The data are represented in the form of complex objects, and the state of any object is described as a set of relation values. Emphasize that the types describing the objects are encapsulated, inherited, and polymorphic. Then, it is shown that the data represented as a set of such objects may also be represented as a set of relational values defined on the set of scalar domains (dual data representation). In the general case, any class is associated with a set of relation variables (R-variables) each one containing some data about all objects of this class existing in the system. One of the key points is the fact that the usage of complex (from the user's viewpoint) refined names of R-variables and their attributes makes it possible to preserve the semantics of complex data structures represented in the form of a set of relation values. The most important part of the data storage system created on the approach proposed is an object-oriented translator operating over a relational DBMS. The expressiveness of such a system is comparable with that of OO programming languages.",
        "published": "2006-02-14T12:19:08Z",
        "link": "http://arxiv.org/abs/cs/0602052v3",
        "categories": [
            "cs.DB",
            "cs.DS",
            "D.3.3; E.2; F.3.3; F.4.1; H.2.1; H.2.3; H.2.4; H.3.3"
        ]
    },
    {
        "title": "Exploring term-document matrices from matrix models in text mining",
        "authors": [
            "Ioannis Antonellis",
            "Efstratios Gallopoulos"
        ],
        "summary": "We explore a matrix-space model, that is a natural extension to the vector space model for Information Retrieval. Each document can be represented by a matrix that is based on document extracts (e.g. sentences, paragraphs, sections). We focus on the performance of this model for the specific case in which documents are originally represented as term-by-sentence matrices. We use the singular value decomposition to approximate the term-by-sentence matrices and assemble these results to form the pseudo-``term-document'' matrix that forms the basis of a text mining method alternative to traditional VSM and LSI. We investigate the singular values of this matrix and provide experimental evidence suggesting that the method can be particularly effective in terms of accuracy for text collections with multi-topic documents, such as web pages with news.",
        "published": "2006-02-21T16:14:16Z",
        "link": "http://arxiv.org/abs/cs/0602076v1",
        "categories": [
            "cs.IR",
            "cs.DB",
            "cs.DL"
        ]
    },
    {
        "title": "On the tree-transformation power of XSLT",
        "authors": [
            "Wim Janssen",
            "Alexandr Korlyukov",
            "Jan Van den Bussche"
        ],
        "summary": "XSLT is a standard rule-based programming language for expressing transformations of XML data. The language is currently in transition from version 1.0 to 2.0. In order to understand the computational consequences of this transition, we restrict XSLT to its pure tree-transformation capabilities. Under this focus, we observe that XSLT~1.0 was not yet a computationally complete tree-transformation language: every 1.0 program can be implemented in exponential time. A crucial new feature of version~2.0, however, which allows nodesets over temporary trees, yields completeness. We provide a formal operational semantics for XSLT programs, and establish confluence for this semantics.",
        "published": "2006-03-08T13:48:45Z",
        "link": "http://arxiv.org/abs/cs/0603028v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "D.3.1; H.2.3; F.1.1"
        ]
    },
    {
        "title": "First Steps in Relational Lattice",
        "authors": [
            "Marshall Spight",
            "Vadim Tropashko"
        ],
        "summary": "Relational lattice reduces the set of six classic relational algebra operators to two binary lattice operations: natural join and inner union. We give an introduction to this theory with emphasis on formal algebraic laws. New results include Spight distributivity criteria and its applications to query transformations.",
        "published": "2006-03-10T18:11:49Z",
        "link": "http://arxiv.org/abs/cs/0603044v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Automatic generation of simplified weakest preconditions for integrity   constraint verification",
        "authors": [
            "A. Ai T -Bouziad",
            "Irene Guessarian",
            "L. Vieille"
        ],
        "summary": "Given a constraint $c$ assumed to hold on a database $B$ and an update $u$ to be performed on $B$, we address the following question: will $c$ still hold after $u$ is performed? When $B$ is a relational database, we define a confluent terminating rewriting system which, starting from $c$ and $u$, automatically derives a simplified weakest precondition $wp(c,u)$ such that, whenever $B$ satisfies $wp(c,u)$, then the updated database $u(B)$ will satisfy $c$, and moreover $wp(c,u)$ is simplified in the sense that its computation depends only upon the instances of $c$ that may be modified by the update. We then extend the definition of a simplified $wp(c,u)$ to the case of deductive databases; we prove it using fixpoint induction.",
        "published": "2006-03-14T14:30:10Z",
        "link": "http://arxiv.org/abs/cs/0603053v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Complexity of Consistent Query Answering in Databases under   Cardinality-Based and Incremental Repair Semantics",
        "authors": [
            "Andrei Lopatenko",
            "Leopoldo Bertossi"
        ],
        "summary": "Consistent Query Answering (CQA) is the problem of computing from a database the answers to a query that are consistent with respect to certain integrity constraints that the database, as a whole, may fail to satisfy. Consistent answers have been characterized as those that are invariant under certain minimal forms of restoration of the database consistency. We investigate algorithmic and complexity theoretic issues of CQA under database repairs that minimally depart -wrt the cardinality of the symmetric difference- from the original database. We obtain first tight complexity bounds.   We also address the problem of incremental complexity of CQA, that naturally occurs when an originally consistent database becomes inconsistent after the execution of a sequence of update operations. Tight bounds on incremental complexity are provided for various semantics under denial constraints. Fixed parameter tractability is also investigated in this dynamic context, where the size of the update sequence becomes the relevant parameter.",
        "published": "2006-04-02T03:07:09Z",
        "link": "http://arxiv.org/abs/cs/0604002v1",
        "categories": [
            "cs.DB",
            "cs.CC"
        ]
    },
    {
        "title": "Distributed Metadata with the AMGA Metadata Catalog",
        "authors": [
            "Nuno Santos",
            "Birger Koblitz"
        ],
        "summary": "Catalog Services play a vital role on Data Grids by allowing users and applications to discover and locate the data needed. On large Data Grids, with hundreds of geographically distributed sites, centralized Catalog Services do not provide the required scalability, performance or fault-tolerance. In this article, we start by presenting and discussing the general requirements on Grid Catalogs of applications being developed by the EGEE user community. This provides the motivation for the second part of the article, where we present the replication and distribution mechanisms we have designed and implemented into the AMGA Metadata Catalog, which is part of the gLite software stack being developed for the EGEE project. Implementing these mechanisms in the catalog itself has the advantages of not requiring any special support from the relational database back-end, of being database independent, and of allowing tailoring the mechanisms to the specific requirements and characteristics of Metadata Catalogs.",
        "published": "2006-04-19T10:01:26Z",
        "link": "http://arxiv.org/abs/cs/0604071v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4"
        ]
    },
    {
        "title": "Semantically Correct Query Answers in the Presence of Null Values",
        "authors": [
            "Loreto Bravo",
            "Leopoldo Bertossi"
        ],
        "summary": "For several reasons a database may not satisfy a given set of integrity constraints(ICs), but most likely most of the information in it is still consistent with those ICs; and could be retrieved when queries are answered. Consistent answers to queries wrt a set of ICs have been characterized as answers that can be obtained from every possible minimally repaired consistent version of the original database. In this paper we consider databases that contain null values and are also repaired, if necessary, using null values. For this purpose, we propose first a precise semantics for IC satisfaction in a database with null values that is compatible with the way null values are treated in commercial database management systems. Next, a precise notion of repair is introduced that privileges the introduction of null values when repairing foreign key constraints, in such a way that these new values do not create an infinite cycle of new inconsistencies. Finally, we analyze how to specify this kind of repairs of a database that contains null values using disjunctive logic programs with stable model semantics.",
        "published": "2006-04-19T22:33:33Z",
        "link": "http://arxiv.org/abs/cs/0604076v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Designing a Multi-petabyte Database for LSST",
        "authors": [
            "Jacek Becla",
            "Andrew Hanushevsky",
            "Sergei Nikolaev",
            "Ghaleb Abdulla",
            "Alex Szalay",
            "Maria Nieto-Santisteban",
            "Ani Thakar",
            "Jim Gray"
        ],
        "summary": "The 3.2 giga-pixel LSST camera will produce approximately half a petabyte of archive images every month. These data need to be reduced in under a minute to produce real-time transient alerts, and then added to the cumulative catalog for further analysis. The catalog is expected to grow about three hundred terabytes per year. The data volume, the real-time transient alerting requirements of the LSST, and its spatio-temporal aspects require innovative techniques to build an efficient data access system at reasonable cost. As currently envisioned, the system will rely on a database for catalogs and metadata. Several database systems are being evaluated to understand how they perform at these data rates, data volumes, and access patterns. This paper describes the LSST requirements, the challenges they impose, the data access philosophy, results to date from evaluating available database technologies against LSST requirements, and the proposed database architecture to meet the data challenges.",
        "published": "2006-04-28T03:04:00Z",
        "link": "http://arxiv.org/abs/cs/0604112v1",
        "categories": [
            "cs.DB",
            "cs.DL"
        ]
    },
    {
        "title": "A Better Alternative to Piecewise Linear Time Series Segmentation",
        "authors": [
            "Daniel Lemire"
        ],
        "summary": "Time series are difficult to monitor, summarize and predict. Segmentation organizes time series into few intervals having uniform characteristics (flatness, linearity, modality, monotonicity and so on). For scalability, we require fast linear time algorithms. The popular piecewise linear model can determine where the data goes up or down and at what rate. Unfortunately, when the data does not follow a linear model, the computation of the local slope creates overfitting. We propose an adaptive time series model where the polynomial degree of each interval vary (constant, linear and so on). Given a number of regressors, the cost of each interval is its polynomial degree: constant intervals cost 1 regressor, linear intervals cost 2 regressors, and so on. Our goal is to minimize the Euclidean (l_2) error for a given model complexity. Experimentally, we investigate the model where intervals can be either constant or linear. Over synthetic random walks, historical stock market prices, and electrocardiograms, the adaptive model provides a more accurate segmentation than the piecewise linear model without increasing the cross-validation error or the running time, while providing a richer vocabulary to applications. Implementation issues, such as numerical stability and real-world performance, are discussed.",
        "published": "2006-05-24T04:42:53Z",
        "link": "http://arxiv.org/abs/cs/0605103v8",
        "categories": [
            "cs.DB",
            "cs.CV",
            "H.2.8"
        ]
    },
    {
        "title": "Semantics and Complexity of SPARQL",
        "authors": [
            "Jorge Perez",
            "Marcelo Arenas",
            "Claudio Gutierrez"
        ],
        "summary": "SPARQL is the W3C candidate recommendation query language for RDF. In this paper we address systematically the formal study of SPARQL, concentrating in its graph pattern facility. We consider for this study a fragment without literals and a simple version of filters which encompasses all the main issues yet is simple to formalize. We provide a compositional semantics, prove there are normal forms, prove complexity bounds, among others that the evaluation of SPARQL patterns is PSPACE-complete, compare our semantics to an alternative operational semantics, give simple and natural conditions when both semantics coincide and discuss optimizations procedures.",
        "published": "2006-05-26T16:41:15Z",
        "link": "http://arxiv.org/abs/cs/0605124v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Analyzing Large Collections of Electronic Text Using OLAP",
        "authors": [
            "Steven Keith",
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "Computer-assisted reading and analysis of text has various applications in the humanities and social sciences. The increasing size of many electronic text archives has the advantage of a more complete analysis but the disadvantage of taking longer to obtain results. On-Line Analytical Processing is a method used to store and quickly analyze multidimensional data. By storing text analysis information in an OLAP system, a user can obtain solutions to inquiries in a matter of seconds as opposed to minutes, hours, or even days. This analysis is user-driven allowing various users the freedom to pursue their own direction of research.",
        "published": "2006-05-27T00:51:46Z",
        "link": "http://arxiv.org/abs/cs/0605127v1",
        "categories": [
            "cs.DB",
            "cs.DL"
        ]
    },
    {
        "title": "Consecutive Support: Better Be Close!",
        "authors": [
            "Edgar de Graaf",
            "Jeannette de Graaf",
            "Walter A. Kosters"
        ],
        "summary": "We propose a new measure of support (the number of occur- rences of a pattern), in which instances are more important if they occur with a certain frequency and close after each other in the stream of trans- actions. We will explain this new consecutive support and discuss how patterns can be found faster by pruning the search space, for instance using so-called parent support recalculation. Both consecutiveness and the notion of hypercliques are incorporated into the Eclat algorithm. Synthetic examples show how interesting phenomena can now be discov- ered in the datasets. The new measure can be applied in many areas, ranging from bio-informatics to trade, supermarkets, and even law en- forcement. E.g., in bio-informatics it is important to find patterns con- tained in many individuals, where patterns close together in one chro- mosome are more significant.",
        "published": "2006-06-06T14:28:42Z",
        "link": "http://arxiv.org/abs/cs/0606024v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Logics for Unranked Trees: An Overview",
        "authors": [
            "Leonid Libkin"
        ],
        "summary": "Labeled unranked trees are used as a model of XML documents, and logical languages for them have been studied actively over the past several years. Such logics have different purposes: some are better suited for extracting data, some for expressing navigational properties, and some make it easy to relate complex properties of trees to the existence of tree automata for those properties. Furthermore, logics differ significantly in their model-checking properties, their automata models, and their behavior on ordered and unordered trees. In this paper we present a survey of logics for unranked trees.",
        "published": "2006-06-13T16:27:28Z",
        "link": "http://arxiv.org/abs/cs/0606062v2",
        "categories": [
            "cs.LO",
            "cs.DB",
            "H.2.3; H.2.1; I.7; F.2.3; F.4.1; F.4.3"
        ]
    },
    {
        "title": "On the complexity of XPath containment in the presence of disjunction,   DTDs, and variables",
        "authors": [
            "Frank Neven",
            "Thomas Schwentick"
        ],
        "summary": "XPath is a simple language for navigating an XML-tree and returning a set of answer nodes. The focus in this paper is on the complexity of the containment problem for various fragments of XPath. We restrict attention to the most common XPath expressions which navigate along the child and/or descendant axis. In addition to basic expressions using only node tests and simple predicates, we also consider disjunction and variables (ranging over nodes). Further, we investigate the containment problem relative to a given DTD. With respect to variables we study two semantics, (1) the original semantics of XPath, where the values of variables are given by an outer context, and (2) an existential semantics introduced by Deutsch and Tannen, in which the values of variables are existentially quantified. In this framework, we establish an exact classification of the complexity of the containment problem for many XPath fragments.",
        "published": "2006-06-14T09:49:17Z",
        "link": "http://arxiv.org/abs/cs/0606065v4",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2; I.7.2; F.4"
        ]
    },
    {
        "title": "10^(10^6) Worlds and Beyond: Efficient Representation and Processing of   Incomplete Information",
        "authors": [
            "Lyublena Antova",
            "Christoph Koch",
            "Dan Olteanu"
        ],
        "summary": "Current systems and formalisms for representing incomplete information generally suffer from at least one of two weaknesses. Either they are not strong enough for representing results of simple queries, or the handling and processing of the data, e.g. for query evaluation, is intractable.   In this paper, we present a decomposition-based approach to addressing this problem. We introduce world-set decompositions (WSDs), a space-efficient formalism for representing any finite set of possible worlds over relational databases. WSDs are therefore a strong representation system for any relational query language. We study the problem of efficiently evaluating relational algebra queries on sets of worlds represented by WSDs. We also evaluate our technique experimentally in a large census data scenario and show that it is both scalable and efficient.",
        "published": "2006-06-16T14:24:34Z",
        "link": "http://arxiv.org/abs/cs/0606075v2",
        "categories": [
            "cs.DB",
            "H.2.1; H.2.4"
        ]
    },
    {
        "title": "On Typechecking Top-Down XML Tranformations: Fixed Input or Output   Schemas",
        "authors": [
            "Wim Martens",
            "Frank Neven",
            "Marc Gyssens"
        ],
        "summary": "Typechecking consists of statically verifying whether the output of an XML transformation always conforms to an output type for documents satisfying a given input type. In this general setting, both the input and output schema as well as the transformation are part of the input for the problem. However, scenarios where the input or output schema can be considered to be fixed, are quite common in practice. In the present work, we investigate the computational complexity of the typechecking problem in the latter setting.",
        "published": "2006-06-22T09:46:48Z",
        "link": "http://arxiv.org/abs/cs/0606094v1",
        "categories": [
            "cs.DB",
            "cs.PL"
        ]
    },
    {
        "title": "Entropy And Vision",
        "authors": [
            "Rami Kanhouche"
        ],
        "summary": "In vector quantization the number of vectors used to construct the codebook is always an undefined problem, there is always a compromise between the number of vectors and the quantity of information lost during the compression. In this text we present a minimum of Entropy principle that gives solution to this compromise and represents an Entropy point of view of signal compression in general. Also we present a new adaptive Object Quantization technique that is the same for the compression and the perception.",
        "published": "2006-06-26T13:03:11Z",
        "link": "http://arxiv.org/abs/math/0606643v3",
        "categories": [
            "math.PR",
            "cs.CV",
            "cs.DB",
            "cs.DM",
            "cs.LG",
            "math.CO",
            "I.2.10 Vision and Scene Understanding"
        ]
    },
    {
        "title": "Database Querying under Changing Preferences",
        "authors": [
            "Jan Chomicki"
        ],
        "summary": "We present here a formal foundation for an iterative and incremental approach to constructing and evaluating preference queries. Our main focus is on query modification: a query transformation approach which works by revising the preference relation in the query. We provide a detailed analysis of the cases where the order-theoretic properties of the preference relation are preserved by the revision. We consider a number of different revision operators: union, prioritized and Pareto composition. We also formulate algebraic laws that enable incremental evaluation of preference queries. Finally, we consider two variations of the basic framework: finite restrictions of preference relations and weak-order extensions of strict partial order preference relations.",
        "published": "2006-07-05T18:37:12Z",
        "link": "http://arxiv.org/abs/cs/0607013v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Set-Theoretic Preliminaries for Computer Scientists",
        "authors": [
            "M. H. van Emden"
        ],
        "summary": "The basics of set theory are usually copied, directly or indirectly, by computer scientists from introductions to mathematical texts. Often mathematicians are content with special cases when the general case is of no mathematical interest. But sometimes what is of no mathematical interest is of great practical interest in computer science. For example, non-binary relations in mathematics tend to have numerical indexes and tend to be unsorted. In the theory and practice of relational databases both these simplifications are unwarranted. In response to this situation we present here an alternative to the ``set-theoretic preliminaries'' usually found in computer science texts. This paper separates binary relations from the kind of relations that are needed in relational databases. Its treatment of functions supports both computer science in general and the kind of relations needed in databases. As a sample application this paper shows how the mathematical theory of relations naturally leads to the relational data model and how the operations on relations are by themselves already a powerful vehicle for queries.",
        "published": "2006-07-09T14:27:43Z",
        "link": "http://arxiv.org/abs/cs/0607039v1",
        "categories": [
            "cs.DM",
            "cs.DB"
        ]
    },
    {
        "title": "On-line topological simplification of weighted graphs",
        "authors": [
            "Floris Geerts",
            "Peter Revesz",
            "Jan Van den Bussche"
        ],
        "summary": "We describe two efficient on-line algorithms to simplify weighted graphs by eliminating degree-two vertices. Our algorithms are on-line in that they react to updates on the data, keeping the simplification up-to-date. The supported updates are insertions of vertices and edges; hence, our algorithms are partially dynamic. We provide both analytical and empirical evaluations of the efficiency of our approaches. Specifically, we prove an O(log n) upper bound on the amortized time complexity of our maintenance algorithms, with n the number of insertions.",
        "published": "2006-08-23T20:08:53Z",
        "link": "http://arxiv.org/abs/cs/0608091v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Rule-based Knowledge Representation for Service Level Agreement",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Automated management and monitoring of service contracts like Service Level Agreements (SLAs) or higher-level policies is vital for efficient and reliable distributed service-oriented architectures (SOA) with high quality of ser-vice (QoS) levels. IT service provider need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technol-ogy. I propose a novel rule-based knowledge representation (KR) for SLA rules and a respective rule-based service level management (RBSLM) framework. My rule-based approach based on logic programming provides several advantages including automated rule chaining allowing for compact knowledge representation and high levels of automation as well as flexibility to adapt to rapidly changing business requirements. Therewith, I address an urgent need service-oriented busi-nesses do have nowadays which is to dynamically change their business and contractual logic in order to adapt to rapidly changing business environments and to overcome the restricting nature of slow change cycles.",
        "published": "2006-09-21T12:04:33Z",
        "link": "http://arxiv.org/abs/cs/0609120v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.LO",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "The Management and Integration of Biomedical Knowledge: Application in   the Health-e-Child Project (Position Paper)",
        "authors": [
            "E. Jimenez-Ruiz",
            "R. Berlanga",
            "I. Sanz",
            "R. McClatchey",
            "R. Danger",
            "D. Manset",
            "J. Paraire",
            "A. Rios"
        ],
        "summary": "The Health-e-Child project aims to develop an integrated healthcare platform for European paediatrics. In order to achieve a comprehensive view of childrens health, a complex integration of biomedical data, information, and knowledge is necessary. Ontologies will be used to formally define this domain knowledge and will form the basis for the medical knowledge management system. This paper introduces an innovative methodology for the vertical integration of biomedical knowledge. This approach will be largely clinician-centered and will enable the definition of ontology fragments, connections between them (semantic bridges) and enriched ontology fragments (views). The strategy for the specification and capture of fragments, bridges and views is outlined with preliminary examples demonstrated in the collection of biomedical information from hospital databases, biomedical ontologies, and biomedical public databases.",
        "published": "2006-09-26T15:21:40Z",
        "link": "http://arxiv.org/abs/cs/0609144v1",
        "categories": [
            "cs.DB",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Full Text Searching in the Astrophysics Data System",
        "authors": [
            "Günther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn S. Grant",
            "Edwin A. Henneken",
            "Donna M. Thompson",
            "Michael J. Kurtz",
            "Stephen S. Murray"
        ],
        "summary": "The Smithsonian/NASA Astrophysics Data System (ADS) provides a search system for the astronomy and physics scholarly literature. All major and many smaller astronomy journals that were published on paper have been scanned back to volume 1 and are available through the ADS free of charge. All scanned pages have been converted to text and can be searched through the ADS Full Text Search System. In addition, searches can be fanned out to several external search systems to include the literature published in electronic form. Results from the different search systems are combined into one results list.   The ADS Full Text Search System is available at: http://adsabs.harvard.edu/fulltext_service.html",
        "published": "2006-10-02T14:51:45Z",
        "link": "http://arxiv.org/abs/cs/0610007v2",
        "categories": [
            "cs.DL",
            "astro-ph",
            "cs.DB"
        ]
    },
    {
        "title": "Connectivity in the Astronomy Digital Library",
        "authors": [
            "Günther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn S. Grant",
            "Edwin A. Henneken",
            "Donna M. Thompson",
            "Michael J. Kurtz",
            "Stephen S. Murray"
        ],
        "summary": "The Astrophysics Data System (ADS) provides an extensive system of links between the literature and other on-line information. Recently, the journals of the American Astronomical Society (AAS) and a group of NASA data centers have collaborated to provide more links between on-line data obtained by space missions and the on-line journals. Authors can now specify which data sets they have used in their article. This information is used by the participants to provide the links between the literature and the data.   The ADS is available at: http://ads.harvard.edu",
        "published": "2006-10-02T15:06:35Z",
        "link": "http://arxiv.org/abs/cs/0610008v1",
        "categories": [
            "cs.DL",
            "astro-ph",
            "cs.DB"
        ]
    },
    {
        "title": "One-Pass, One-Hash n-Gram Statistics Estimation",
        "authors": [
            "Daniel Lemire",
            "Owen Kaser"
        ],
        "summary": "In multimedia, text or bioinformatics databases, applications query sequences of n consecutive symbols called n-grams. Estimating the number of distinct n-grams is a view-size estimation problem. While view sizes can be estimated by sampling under statistical assumptions, we desire an unassuming algorithm with universally valid accuracy bounds. Most related work has focused on repeatedly hashing the data, which is prohibitive for large data sources. We prove that a one-pass one-hash algorithm is sufficient for accurate estimates if the hashing is sufficiently independent. To reduce costs further, we investigate recursive random hashing algorithms and show that they are sufficiently independent in practice. We compare our running times with exact counts using suffix arrays and show that, while we use hardly any storage, we are an order of magnitude faster. The approach further is extended to a one-pass/one-hash computation of n-gram entropy and iceberg counts. The experiments use a large collection of English text from the Gutenberg Project as well as synthetic data.",
        "published": "2006-10-03T18:04:22Z",
        "link": "http://arxiv.org/abs/cs/0610010v4",
        "categories": [
            "cs.DB",
            "cs.CL"
        ]
    },
    {
        "title": "Creation and use of Citations in the ADS",
        "authors": [
            "Alberto Accomazzi",
            "Gunther Eichhorn",
            "Michael J. Kurtz",
            "Carolyn S. Grant",
            "Edwin Henneken",
            "Markus Demleitner",
            "Donna Thompson",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "With over 20 million records, the ADS citation database is regularly used by researchers and librarians to measure the scientific impact of individuals, groups, and institutions. In addition to the traditional sources of citations, the ADS has recently added references extracted from the arXiv e-prints on a nightly basis. We review the procedures used to harvest and identify the reference data used in the creation of citations, the policies and procedures that we follow to avoid double-counting and to eliminate contributions which may not be scholarly in nature. Finally, we describe how users and institutions can easily obtain quantitative citation data from the ADS, both interactively and via web-based programming tools.   The ADS is available at http://ads.harvard.edu.",
        "published": "2006-10-03T21:29:03Z",
        "link": "http://arxiv.org/abs/cs/0610011v1",
        "categories": [
            "cs.DL",
            "astro-ph",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "XString: XML as a String",
        "authors": [
            "William F. Gilreath"
        ],
        "summary": "Extensible markup language (XML) is a technology that has been much hyped, so that XML has become an industry buzzword. Behind the hype is a powerful technology for data representation in a platform independent manner. As a text document, however, XML suffers from being too bloated, and requires an XML parser to access and manipulate it. XString is an encoding method for XML, in essence, a markup language's markup language. XString gives the benefit of compressing XML, and allows for easy manipulation and processing of XML source as a very long string.",
        "published": "2006-10-04T23:21:11Z",
        "link": "http://arxiv.org/abs/cs/0610020v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Data in the ADS -- Understanding How to Use it Better",
        "authors": [
            "Carolyn S. Grant",
            "Alberto Accomazzi",
            "Donna Thompson",
            "Edwin Henneken",
            "Guenther Eichhorn",
            "Michael J. Kurtz",
            "Stephen S. Murray"
        ],
        "summary": "The Smithsonian/NASA ADS Abstract Service contains a wealth of data for astronomers and librarians alike, yet the vast majority of usage consists of rudimentary searches. Hints on how to obtain more focused search results by using more of the various capabilities of the ADS are presented, including searching by affiliation. We also discuss the classification of articles by content and by referee status.   The ADS is funded by NASA Grant NNG06GG68G-16613687.",
        "published": "2006-10-05T18:51:54Z",
        "link": "http://arxiv.org/abs/cs/0610029v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "How To Break Anonymity of the Netflix Prize Dataset",
        "authors": [
            "Arvind Narayanan",
            "Vitaly Shmatikov"
        ],
        "summary": "We present a new class of statistical de-anonymization attacks against high-dimensional micro-data, such as individual preferences, recommendations, transaction records and so on. Our techniques are robust to perturbation in the data and tolerate some mistakes in the adversary's background knowledge.   We apply our de-anonymization methodology to the Netflix Prize dataset, which contains anonymous movie ratings of 500,000 subscribers of Netflix, the world's largest online movie rental service. We demonstrate that an adversary who knows only a little bit about an individual subscriber can easily identify this subscriber's record in the dataset. Using the Internet Movie Database as the source of background knowledge, we successfully identified the Netflix records of known users, uncovering their apparent political preferences and other potentially sensitive information.",
        "published": "2006-10-18T06:03:41Z",
        "link": "http://arxiv.org/abs/cs/0610105v2",
        "categories": [
            "cs.CR",
            "cs.DB"
        ]
    },
    {
        "title": "Hierarchical Bin Buffering: Online Local Moments for Dynamic External   Memory Arrays",
        "authors": [
            "Daniel Lemire",
            "Owen Kaser"
        ],
        "summary": "Local moments are used for local regression, to compute statistical measures such as sums, averages, and standard deviations, and to approximate probability distributions. We consider the case where the data source is a very large I/O array of size n and we want to compute the first N local moments, for some constant N. Without precomputation, this requires O(n) time. We develop a sequence of algorithms of increasing sophistication that use precomputation and additional buffer space to speed up queries. The simpler algorithms partition the I/O array into consecutive ranges called bins, and they are applicable not only to local-moment queries, but also to algebraic queries (MAX, AVERAGE, SUM, etc.). With N buffers of size sqrt{n}, time complexity drops to O(sqrt n). A more sophisticated approach uses hierarchical buffering and has a logarithmic time complexity (O(b log_b n)), when using N hierarchical buffers of size n/b. Using Overlapped Bin Buffering, we show that only a single buffer is needed, as with wavelet-based algorithms, but using much less storage. Applications exist in multidimensional and statistical databases over massive data sets, interactive image processing, and visualization.",
        "published": "2006-10-21T00:30:57Z",
        "link": "http://arxiv.org/abs/cs/0610128v3",
        "categories": [
            "cs.DS",
            "cs.DB",
            "H.3.5; G.1.1"
        ]
    },
    {
        "title": "Efficient Threshold Aggregation of Moving Objects",
        "authors": [
            "Scot Anderson",
            "Peter Revesz"
        ],
        "summary": "Calculating aggregation operators of moving point objects, using time as a continuous variable, presents unique problems when querying for congestion in a moving and changing (or dynamic) query space. We present a set of congestion query operators, based on a threshold value, that estimate the following 5 aggregation operations in d-dimensions. 1) We call the count of point objects that intersect the dynamic query space during the query time interval, the CountRange. 2) We call the Maximum (or Minimum) congestion in the dynamic query space at any time during the query time interval, the MaxCount (or MinCount). 3) We call the sum of time that the dynamic query space is congested, the ThresholdSum. 4) We call the number of times that the dynamic query space is congested, the ThresholdCount. And 5) we call the average length of time of all the time intervals when the dynamic query space is congested, the ThresholdAverage. These operators rely on a novel approach to transforming the problem of selection based on position to a problem of selection based on a threshold. These operators can be used to predict concentrations of migrating birds that may carry disease such as Bird Flu and hence the information may be used to predict high risk areas. On a smaller scale, those operators are also applicable to maintaining safety in airplane operations. We present the theory of our estimation operators and provide algorithms for exact operators. The implementations of those operators, and experiments, which include data from more than 7500 queries, indicate that our estimation operators produce fast, efficient results with error under 5%.",
        "published": "2006-11-07T18:23:34Z",
        "link": "http://arxiv.org/abs/cs/0611031v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "The Role of Quasi-identifiers in k-Anonymity Revisited",
        "authors": [
            "Claudio Bettini",
            "X. Sean Wang",
            "Sushil Jajodia"
        ],
        "summary": "The concept of k-anonymity, used in the recent literature to formally evaluate the privacy preservation of published tables, was introduced based on the notion of quasi-identifiers (or QI for short). The process of obtaining k-anonymity for a given private table is first to recognize the QIs in the table, and then to anonymize the QI values, the latter being called k-anonymization. While k-anonymization is usually rigorously validated by the authors, the definition of QI remains mostly informal, and different authors seem to have different interpretations of the concept of QI. The purpose of this paper is to provide a formal underpinning of QI and examine the correctness and incorrectness of various interpretations of QI in our formal framework. We observe that in cases where the concept has been used correctly, its application has been conservative; this note provides a formal understanding of the conservative nature in such cases.",
        "published": "2006-11-08T15:03:49Z",
        "link": "http://arxiv.org/abs/cs/0611035v1",
        "categories": [
            "cs.DB",
            "cs.CR"
        ]
    },
    {
        "title": "Reducing Order Enforcement Cost in Complex Query Plans",
        "authors": [
            "Ravindra Guravannavar",
            "S Sudarshan",
            "Ajit A Diwan",
            "Ch. Sobhan Babu"
        ],
        "summary": "Algorithms that exploit sort orders are widely used to implement joins, grouping, duplicate elimination and other set operations. Query optimizers traditionally deal with sort orders by using the notion of interesting orders. The number of interesting orders is unfortunately factorial in the number of participating attributes. Optimizer implementations use heuristics to prune the number of interesting orders, but the quality of the heuristics is unclear. Increasingly complex decision support queries and increasing use of covering indices, which provide multiple alternative sort orders for relations, motivate us to better address the problem of optimization with interesting orders.   We show that even a simplified version of optimization with sort orders is NP-hard and provide principled heuristics for choosing interesting orders. We have implemented the proposed techniques in a Volcano-style cost-based optimizer, and our performance study shows significant improvements in estimated cost. We also executed our plans on a widely used commercial database system, and on PostgreSQL, and found that actual execution times for our plans were significantly better than for plans generated by those systems in several cases.",
        "published": "2006-11-20T10:12:46Z",
        "link": "http://arxiv.org/abs/cs/0611094v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Estimating Aggregate Properties on Probabilistic Streams",
        "authors": [
            "Andrew McGregor",
            "S. Muthukrishnan"
        ],
        "summary": "The probabilistic-stream model was introduced by Jayram et al. \\cite{JKV07}. It is a generalization of the data stream model that is suited to handling ``probabilistic'' data where each item of the stream represents a probability distribution over a set of possible events. Therefore, a probabilistic stream determines a distribution over potentially a very large number of classical \"deterministic\" streams where each item is deterministically one of the domain values. The probabilistic model is applicable for not only analyzing streams where the input has uncertainties (such as sensor data streams that measure physical processes) but also where the streams are derived from the input data by post-processing, such as tagging or reconciling inconsistent and poor quality data.   We present streaming algorithms for computing commonly used aggregates on a probabilistic stream. We present the first known, one pass streaming algorithm for estimating the \\AVG, improving results in \\cite{JKV07}. We present the first known streaming algorithms for estimating the number of \\DISTINCT items on probabilistic streams. Further, we present extensions to other aggregates such as the repeat rate, quantiles, etc. In all cases, our algorithms work with provable accuracy guarantees and within the space constraints of the data stream model.",
        "published": "2006-12-05T23:34:52Z",
        "link": "http://arxiv.org/abs/cs/0612031v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Linear Probing with Constant Independence",
        "authors": [
            "Anna Pagh",
            "Rasmus Pagh",
            "Milan Ruzic"
        ],
        "summary": "Hashing with linear probing dates back to the 1950s, and is among the most studied algorithms. In recent years it has become one of the most important hash table organizations since it uses the cache of modern computers very well. Unfortunately, previous analysis rely either on complicated and space consuming hash functions, or on the unrealistic assumption of free access to a truly random hash function. Already Carter and Wegman, in their seminal paper on universal hashing, raised the question of extending their analysis to linear probing. However, we show in this paper that linear probing using a pairwise independent family may have expected {\\em logarithmic} cost per operation. On the positive side, we show that 5-wise independence is enough to ensure constant expected time per operation. This resolves the question of finding a space and time efficient hash function that provably ensures good performance for linear probing.",
        "published": "2006-12-08T22:50:24Z",
        "link": "http://arxiv.org/abs/cs/0612055v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "A Byzantine Fault Tolerant Distributed Commit Protocol",
        "authors": [
            "Wenbing Zhao"
        ],
        "summary": "In this paper, we present a Byzantine fault tolerant distributed commit protocol for transactions running over untrusted networks. The traditional two-phase commit protocol is enhanced by replicating the coordinator and by running a Byzantine agreement algorithm among the coordinator replicas. Our protocol can tolerate Byzantine faults at the coordinator replicas and a subset of malicious faults at the participants. A decision certificate, which includes a set of registration records and a set of votes from participants, is used to facilitate the coordinator replicas to reach a Byzantine agreement on the outcome of each transaction. The certificate also limits the ways a faulty replica can use towards non-atomic termination of transactions, or semantically incorrect transaction outcomes.",
        "published": "2006-12-18T09:31:43Z",
        "link": "http://arxiv.org/abs/cs/0612083v3",
        "categories": [
            "cs.DC",
            "cs.DB"
        ]
    },
    {
        "title": "An asynchronous, decentralised commitment protocol for semantic   optimistic replication",
        "authors": [
            "Pierre Sutra",
            "Marc Shapiro",
            "João Pedro Barreto"
        ],
        "summary": "We study large-scale distributed cooperative systems that use optimistic replication. We represent a system as a graph of actions (operations) connected by edges that reify semantic constraints between actions. Constraint types include conflict, execution order, dependence, and atomicity. The local state is some schedule that conforms to the constraints; because of conflicts, client state is only tentative. For consistency, site schedules should converge; we designed a decentralised, asynchronous commitment protocol. Each client makes a proposal, reflecting its tentative and{\\slash}or preferred schedules. Our protocol distributes the proposals, which it decomposes into semantically-meaningful units called candidates, and runs an election between comparable candidates. A candidate wins when it receives a majority or a plurality. The protocol is fully asynchronous: each site executes its tentative schedule independently, and determines locally when a candidate has won an election. The committed schedule is as close as possible to the preferences expressed by clients.",
        "published": "2006-12-18T16:22:12Z",
        "link": "http://arxiv.org/abs/cs/0612086v2",
        "categories": [
            "cs.DB",
            "cs.NI"
        ]
    },
    {
        "title": "The Dichotomy of Conjunctive Queries on Probabilistic Structures",
        "authors": [
            "Nilesh Dalvi",
            "Dan Suciu"
        ],
        "summary": "We show that for every conjunctive query, the complexity of evaluating it on a probabilistic database is either \\PTIME or #\\P-complete, and we give an algorithm for deciding whether a given conjunctive query is \\PTIME or #\\P-complete. The dichotomy property is a fundamental result on query evaluation on probabilistic databases and it gives a complete classification of the complexity of conjunctive queries.",
        "published": "2006-12-20T21:11:05Z",
        "link": "http://arxiv.org/abs/cs/0612102v2",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "The Boundary Between Privacy and Utility in Data Anonymization",
        "authors": [
            "Vibhor Rastogi",
            "Dan Suciu",
            "Sungho Hong"
        ],
        "summary": "We consider the privacy problem in data publishing: given a relation I containing sensitive information 'anonymize' it to obtain a view V such that, on one hand attackers cannot learn any sensitive information from V, and on the other hand legitimate users can use V to compute useful statistics on I. These are conflicting goals. We use a definition of privacy that is derived from existing ones in the literature, which relates the a priori probability of a given tuple t, Pr(t), with the a posteriori probability, Pr(t | V), and propose a novel and quite practical definition for utility. Our main result is the following. Denoting n the size of I and m the size of the domain from which I was drawn (i.e. n < m) then: when the a priori probability is Pr(t) = Omega(n/sqrt(m)) for some t, there exists no useful anonymization algorithm, while when Pr(t) = O(n/m) for all tuples t, then we give a concrete anonymization algorithm that is both private and useful. Our algorithm is quite different from the k-anonymization algorithm studied intensively in the literature, and is based on random deletions and insertions to I.",
        "published": "2006-12-21T00:21:45Z",
        "link": "http://arxiv.org/abs/cs/0612103v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Architecture for Modular Data Centers",
        "authors": [
            "James Hamilton"
        ],
        "summary": "Several factors are driving high-scale deployments of large data centers built upon commodity components. These commodity clusters are far cheaper than mainframe systems of the past but they bring serious heat and power density issues. Also the high failure rate of the individual components drives significant administrative costs. This proposal outlines an architecture for data center design based upon 20'x8'x8' modules that substantially changes how these systems are acquired, administered, and then later recycled.",
        "published": "2006-12-21T19:36:38Z",
        "link": "http://arxiv.org/abs/cs/0612110v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Fragmentation in Large Object Repositories",
        "authors": [
            "Russell Sears",
            "Catharine van Ingen"
        ],
        "summary": "Fragmentation leads to unpredictable and degraded application performance. While these problems have been studied in detail for desktop filesystem workloads, this study examines newer systems such as scalable object stores and multimedia repositories. Such systems use a get/put interface to store objects. In principle, databases and filesystems can support such applications efficiently, allowing system designers to focus on complexity, deployment cost and manageability. Although theoretical work proves that certain storage policies behave optimally for some workloads, these policies often behave poorly in practice. Most storage benchmarks focus on short-term behavior or do not measure fragmentation. We compare SQL Server to NTFS and find that fragmentation dominates performance when object sizes exceed 256KB-1MB. NTFS handles fragmentation better than SQL Server. Although the performance curves will vary with other systems and workloads, we expect the same interactions between fragmentation and free space to apply. It is well-known that fragmentation is related to the percentage free space. We found that the ratio of free space to object size also impacts performance. Surprisingly, in both systems, storing objects of a single size causes fragmentation, and changing the size of write requests affects fragmentation. These problems could be addressed with simple changes to the filesystem and database interfaces. It is our hope that an improved understanding of fragmentation will lead to predictable storage systems that require less maintenance after deployment.",
        "published": "2006-12-21T21:51:15Z",
        "link": "http://arxiv.org/abs/cs/0612111v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Managing Query Compilation Memory Consumption to Improve DBMS Throughput",
        "authors": [
            "Boris Baryshnikov",
            "Cipri Clinciu",
            "Conor Cunningham",
            "Leo Giakoumakis",
            "Slava Oks",
            "Stefano Stefani"
        ],
        "summary": "While there are known performance trade-offs between database page buffer pool and query execution memory allocation policies, little has been written on the impact of query compilation memory use on overall throughput of the database management system (DBMS). We present a new aspect of the query optimization problem and offer a solution implemented in Microsoft SQL Server 2005. The solution provides stable throughput for a range of workloads even when memory requests outstrip the ability of the hardware to service those requests.",
        "published": "2006-12-21T21:51:55Z",
        "link": "http://arxiv.org/abs/cs/0612112v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Isolation Support for Service-based Applications: A Position Paper",
        "authors": [
            "Paul Greenfield",
            "Alan Fekete",
            "Julian Jang",
            "Dean Kuo",
            "Surya Nepal"
        ],
        "summary": "In this paper, we propose an approach to providing the benefits of isolation in service-oriented applications where it is not feasible to hold traditional locks for ACID transactions. Our technique, called \"Promises\", provides an uniform view for clients which covers a wide range of implementation techniques on the service side, all allowing the client to check a condition and then later rely on that condition still holding.",
        "published": "2006-12-21T22:08:17Z",
        "link": "http://arxiv.org/abs/cs/0612113v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Demaq: A Foundation for Declarative XML Message Processing",
        "authors": [
            "Alexander Böhm",
            "Carl-Christian Kanne",
            "Guido Moerkotte"
        ],
        "summary": "This paper gives an overview of Demaq, an XML message processing system operating on the foundation of transactional XML message queues. We focus on the syntax and semantics of its fully declarative, rule-based application language and demonstrate our message-based programming paradigm in the context of a case study. Further, we discuss optimization opportunities for executing Demaq programs.",
        "published": "2006-12-21T22:29:07Z",
        "link": "http://arxiv.org/abs/cs/0612114v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Consistent Streaming Through Time: A Vision for Event Stream Processing",
        "authors": [
            "Roger S. Barga",
            "Jonathan Goldstein",
            "Mohamed Ali",
            "Mingsheng Hong"
        ],
        "summary": "Event processing will play an increasingly important role in constructing enterprise applications that can immediately react to business critical events. Various technologies have been proposed in recent years, such as event processing, data streams and asynchronous messaging (e.g. pub/sub). We believe these technologies share a common processing model and differ only in target workload, including query language features and consistency requirements. We argue that integrating these technologies is the next step in a natural progression. In this paper, we present an overview and discuss the foundations of CEDR, an event streaming system that embraces a temporal stream model to unify and further enrich query language features, handle imperfections in event delivery and define correctness guarantees. We describe specific contributions made so far and outline next steps in developing the CEDR system.",
        "published": "2006-12-21T22:40:33Z",
        "link": "http://arxiv.org/abs/cs/0612115v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Electronic Laboratory Notebook Assisting Reflectance Spectrometry in   Legal Medicine",
        "authors": [
            "Lioudmila Belenkaia",
            "Michael Bohnert",
            "Andreas W. Liehr"
        ],
        "summary": "Reflectance spectrometry is a fast and reliable method for the characterisation of human skin if the spectra are analysed with respect to a physical model describing the optical properties of human skin. For a field study performed at the Institute of Legal Medicine and the Freiburg Materials Research Center of the University of Freiburg an electronic laboratory notebook has been developed, which assists in the recording, management, and analysis of reflectance spectra. The core of the electronic laboratory notebook is a MySQL database. It is filled with primary data via a web interface programmed in Java, which also enables the user to browse the database and access the results of data analysis. These are carried out by Matlab, Tcl and   Python scripts, which retrieve the primary data from the electronic laboratory notebook, perform the analysis, and store the results in the database for further usage.",
        "published": "2006-12-22T13:29:27Z",
        "link": "http://arxiv.org/abs/cs/0612123v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "cs.IR",
            "H.2.8"
        ]
    },
    {
        "title": "bdbms -- A Database Management System for Biological Data",
        "authors": [
            "Mohamed Y. Eltabakh",
            "Mourad Ouzzani",
            "Walid G. Aref"
        ],
        "summary": "Biologists are increasingly using databases for storing and managing their data. Biological databases typically consist of a mixture of raw data, metadata, sequences, annotations, and related data obtained from various sources. Current database technology lacks several functionalities that are needed by biological databases. In this paper, we introduce bdbms, an extensible prototype database management system for supporting biological data. bdbms extends the functionalities of current DBMSs to include: (1) Annotation and provenance management including storage, indexing, manipulation, and querying of annotation and provenance as first class objects in bdbms, (2) Local dependency tracking to track the dependencies and derivations among data items, (3) Update authorization to support data curation via content-based authorization, in contrast to identity-based authorization, and (4) New access methods and their supporting operators that support pattern matching on various types of compressed biological data types. This paper presents the design of bdbms along with the techniques proposed to support these functionalities including an extension to SQL. We also outline some open issues in building bdbms.",
        "published": "2006-12-22T20:32:00Z",
        "link": "http://arxiv.org/abs/cs/0612127v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "SASE: Complex Event Processing over Streams",
        "authors": [
            "Daniel Gyllstrom",
            "Eugene Wu",
            "Hee-Jin Chae",
            "Yanlei Diao",
            "Patrick Stahlberg",
            "Gordon Anderson"
        ],
        "summary": "RFID technology is gaining adoption on an increasing scale for tracking and monitoring purposes. Wide deployments of RFID devices will soon generate an unprecedented volume of data. Emerging applications require the RFID data to be filtered and correlated for complex pattern detection and transformed to events that provide meaningful, actionable information to end applications. In this work, we design and develop SASE, a com-plex event processing system that performs such data-information transformation over real-time streams. We design a complex event language for specifying application logic for such transformation, devise new query processing techniques to effi-ciently implement the language, and develop a comprehensive system that collects, cleans, and processes RFID data for deliv-ery of relevant, timely information as well as storing necessary data for future querying. We demonstrate an initial prototype of SASE through a real-world retail management scenario.",
        "published": "2006-12-22T20:38:58Z",
        "link": "http://arxiv.org/abs/cs/0612128v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Impliance: A Next Generation Information Management Appliance",
        "authors": [
            "Bishwaranjan Bhattacharjee",
            "Vuk Ercegovac",
            "Joseph Glider",
            "Richard Golding",
            "Guy Lohman",
            "Volke Markl",
            "Hamid Pirahesh",
            "Jun Rao",
            "Robert Rees",
            "Frederick Reiss",
            "Eugene Shekita",
            "Garret Swart"
        ],
        "summary": "ably successful in building a large market and adapting to the changes of the last three decades, its impact on the broader market of information management is surprisingly limited. If we were to design an information management system from scratch, based upon today's requirements and hardware capabilities, would it look anything like today's database systems?\" In this paper, we introduce Impliance, a next-generation information management system consisting of hardware and software components integrated to form an easy-to-administer appliance that can store, retrieve, and analyze all types of structured, semi-structured, and unstructured information. We first summarize the trends that will shape information management for the foreseeable future. Those trends imply three major requirements for Impliance: (1) to be able to store, manage, and uniformly query all data, not just structured records; (2) to be able to scale out as the volume of this data grows; and (3) to be simple and robust in operation. We then describe four key ideas that are uniquely combined in Impliance to address these requirements, namely the ideas of: (a) integrating software and off-the-shelf hardware into a generic information appliance; (b) automatically discovering, organizing, and managing all data - unstructured as well as structured - in a uniform way; (c) achieving scale-out by exploiting simple, massive parallel processing, and (d) virtualizing compute and storage resources to unify, simplify, and streamline the management of Impliance. Impliance is an ambitious, long-term effort to define simpler, more robust, and more scalable information systems for tomorrow's enterprises.",
        "published": "2006-12-22T20:49:29Z",
        "link": "http://arxiv.org/abs/cs/0612129v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Turning Cluster Management into Data Management: A System Overview",
        "authors": [
            "Eric Robinson",
            "David DeWitt"
        ],
        "summary": "This paper introduces the CondorJ2 cluster management system. Traditionally, cluster management systems such as Condor employ a process-oriented approach with little or no use of modern database system technology. In contrast, CondorJ2 employs a data-centric, 3-tier web-application architecture for all system functions (e.g., job submission, monitoring and scheduling; node configuration, monitoring and management, etc.) except for job execution. Employing a data-oriented approach allows the core challenge (i.e., managing and coordinating a large set of distributed computing resources) to be transformed from a relatively low-level systems problem into a more abstract, higher-level data management problem. Preliminary results suggest that CondorJ2's use of standard 3-tier software represents a significant step forward to the design and implementation of large clusters (1,000 to 10,000 nodes).",
        "published": "2006-12-27T22:21:57Z",
        "link": "http://arxiv.org/abs/cs/0612137v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Automatic Detection of Trends in Dynamical Text: An Evolutionary   Approach",
        "authors": [
            "Lourdes Araujo",
            "Juan J. Merelo"
        ],
        "summary": "This paper presents an evolutionary algorithm for modeling the arrival dates of document streams, which is any time-stamped collection of documents, such as newscasts, e-mails, IRC conversations, scientific journals archives and weblog postings. This algorithm assigns frequencies (number of document arrivals per time unit) to time intervals so that it produces an optimal fit to the data. The optimization is a trade off between accurately fitting the data and avoiding too many frequency changes; this way the analysis is able to find fits which ignore the noise. Classical dynamic programming algorithms are limited by memory and efficiency requirements, which can be a problem when dealing with long streams. This suggests to explore alternative search methods which allow for some degree of uncertainty to achieve tractability. Experiments have shown that the designed evolutionary algorithm is able to reach the same solution quality as those classical dynamic programming algorithms in a shorter time. We have also explored different probabilistic models to optimize the fitting of the date streams, and applied these algorithms to infer whether a new arrival increases or decreases {\\em interest} in the topic the document stream is about.",
        "published": "2006-01-12T20:23:06Z",
        "link": "http://arxiv.org/abs/cs/0601047v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Learning about knowledge: A complex network approach",
        "authors": [
            "Luciano da Fontoura Costa"
        ],
        "summary": "This article describes an approach to modeling knowledge acquisition in terms of walks along complex networks. Each subset of knowledge is represented as a node, and relations between such knowledge are expressed as edges. Two types of edges are considered, corresponding to free and conditional transitions. The latter case implies that a node can only be reached after visiting previously a set of nodes (the required conditions). The process of knowledge acquisition can then be simulated by considering the number of nodes visited as a single agent moves along the network, starting from its lowest layer. It is shown that hierarchical networks, i.e. networks composed of successive interconnected layers, arise naturally as a consequence of compositions of the prerequisite relationships between the nodes. In order to avoid deadlocks, i.e. unreachable nodes, the subnetwork in each layer is assumed to be a connected component. Several configurations of such hierarchical knowledge networks are simulated and the performance of the moving agent quantified in terms of the percentage of visited nodes after each movement. The Barab\\'asi-Albert and random models are considered for the layer and interconnecting subnetworks. Although all subnetworks in each realization have the same number of nodes, several interconnectivities, defined by the average node degree of the interconnection networks, have been considered. Two visiting strategies are investigated: random choice among the existing edges and preferential choice to so far untracked edges. A series of interesting results are obtained, including the identification of a series of plateaux of knowledge stagnation in the case of the preferential movements strategy in presence of conditional edges.",
        "published": "2006-01-17T17:13:20Z",
        "link": "http://arxiv.org/abs/physics/0601118v4",
        "categories": [
            "physics.soc-ph",
            "cond-mat.dis-nn",
            "cs.NE",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Instantaneously Trained Neural Networks",
        "authors": [
            "Abhilash Ponnath"
        ],
        "summary": "This paper presents a review of instantaneously trained neural networks (ITNNs). These networks trade learning time for size and, in the basic model, a new hidden node is created for each training sample. Various versions of the corner-classification family of ITNNs, which have found applications in artificial intelligence (AI), are described. Implementation issues are also considered.",
        "published": "2006-01-30T22:02:47Z",
        "link": "http://arxiv.org/abs/cs/0601129v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Study on the Global Convergence Time Complexity of Estimation of   Distribution Algorithms",
        "authors": [
            "R. Rastegar",
            "M. R. Meybodi"
        ],
        "summary": "The Estimation of Distribution Algorithm is a new class of population based search methods in that a probabilistic model of individuals is estimated based on the high quality individuals and used to generate the new individuals. In this paper we compute 1) some upper bounds on the number of iterations required for global convergence of EDA 2) the exact number of iterations needed for EDA to converge to global optima.",
        "published": "2006-01-31T07:10:45Z",
        "link": "http://arxiv.org/abs/cs/0601132v3",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Réseaux d'Automates de Caianiello Revisité",
        "authors": [
            "René Ndoundam",
            "Maurice Tchuente"
        ],
        "summary": "We exhibit a family of neural networks of McCulloch and Pitts of size $2nk+2$ which can be simulated by a neural networks of Caianiello of size $2n+2$ and memory length $k$. This simulation allows us to find again one of the result of the following article: [Cycles exponentiels des r\\'{e}seaux de Caianiello et compteurs en arithm\\'{e}tique redondante, Technique et Science Informatiques Vol. 19, pages 985-1008] on the existence of neural networks of Caianiello of size $2n+2$ and memory length $k$ which describes a cycle of length $k \\times 2^{nk}$.",
        "published": "2006-02-10T06:32:29Z",
        "link": "http://arxiv.org/abs/cs/0602036v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "On the utility of the multimodal problem generator for assessing the   performance of Evolutionary Algorithms",
        "authors": [
            "Fernando G. Lobo",
            "Claudio F. Lima"
        ],
        "summary": "This paper looks in detail at how an evolutionary algorithm attempts to solve instances from the multimodal problem generator. The paper shows that in order to consistently reach the global optimum, an evolutionary algorithm requires a population size that should grow at least linearly with the number of peaks. It is also shown a close relationship between the supply and decision making issues that have been identified previously in the context of population sizing models for additively decomposable problems.   The most important result of the paper, however, is that solving an instance of the multimodal problem generator is like solving a peak-in-a-haystack, and it is argued that evolutionary algorithms are not the best algorithms for such a task. Finally, and as opposed to what several researchers have been doing, it is our strong belief that the multimodal problem generator is not adequate for assessing the performance of evolutionary algorithms.",
        "published": "2006-02-14T10:30:36Z",
        "link": "http://arxiv.org/abs/cs/0602051v1",
        "categories": [
            "cs.NE",
            "I.2.8"
        ]
    },
    {
        "title": "Numerical Modeling of Coexistence, Competition and Collapse of Rotating   Spiral Waves in Three-Level Excitable Media with Discrete Active Centers and   Absorbing Boundaries",
        "authors": [
            "S. D. Makovetskiy"
        ],
        "summary": "Spatio-temporal dynamics of excitable media with discrete three-level active centers (ACs) and absorbing boundaries is studied numerically by means of a deterministic three-level model (see S. D. Makovetskiy and D. N. Makovetskii, on-line preprint cond-mat/0410460 ), which is a generalization of Zykov- Mikhailov model (see Sov. Phys. -- Doklady, 1986, Vol.31, No.1, P.51) for the case of two-channel diffusion of excitations. In particular, we revealed some qualitatively new features of coexistence, competition and collapse of rotating spiral waves (RSWs) in three-level excitable media under conditions of strong influence of the second channel of diffusion. Part of these features are caused by unusual mechanism of RSWs evolution when RSW's cores get into the surface layer of an active medium (i.~e. the layer of ACs resided at the absorbing boundary). Instead of well known scenario of RSW collapse, which takes place after collision of RSW's core with absorbing boundary, we observed complicated transformations of the core leading to nonlinear ''reflection'' of the RSW from the boundary or even to birth of several new RSWs in the surface layer. To our knowledge, such nonlinear ''reflections'' of RSWs and resulting die hard vorticity in excitable media with absorbing boundaries were unknown earlier. ACM classes: F.1.1, I.6, J.2; PACS numbers: 05.65.+b, 07.05.Tp, 82.20.Wt",
        "published": "2006-02-15T04:49:22Z",
        "link": "http://arxiv.org/abs/cond-mat/0602345v1",
        "categories": [
            "cond-mat.other",
            "cs.NE",
            "nlin.CG"
        ]
    },
    {
        "title": "Revisiting Evolutionary Algorithms with On-the-Fly Population Size   Adjustment",
        "authors": [
            "Fernando G. Lobo",
            "Claudio F. Lima"
        ],
        "summary": "In an evolutionary algorithm, the population has a very important role as its size has direct implications regarding solution quality, speed, and reliability. Theoretical studies have been done in the past to investigate the role of population sizing in evolutionary algorithms. In addition to those studies, several self-adjusting population sizing mechanisms have been proposed in the literature. This paper revisits the latter topic and pays special attention to the genetic algorithm with adaptive population size (APGA), for which several researchers have claimed to be very effective at autonomously (re)sizing the population.   As opposed to those previous claims, this paper suggests a complete opposite view. Specifically, it shows that APGA is not capable of adapting the population size at all. This claim is supported on theoretical grounds and confirmed by computer simulations.",
        "published": "2006-02-15T13:48:13Z",
        "link": "http://arxiv.org/abs/cs/0602055v1",
        "categories": [
            "cs.NE",
            "I.2.8"
        ]
    },
    {
        "title": "Lamarckian Evolution and the Baldwin Effect in Evolutionary Neural   Networks",
        "authors": [
            "P. A. Castillo",
            "M. G. Arenas",
            "J. G. Castellano",
            "J. J. Merelo",
            "A. Prieto",
            "V. Rivas",
            "G. Romero"
        ],
        "summary": "Hybrid neuro-evolutionary algorithms may be inspired on Darwinian or Lamarckian evolu- tion. In the case of Darwinian evolution, the Baldwin effect, that is, the progressive incorporation of learned characteristics to the genotypes, can be observed and leveraged to improve the search. The purpose of this paper is to carry out an exper- imental study into how learning can improve G-Prop genetic search. Two ways of combining learning and genetic search are explored: one exploits the Baldwin effect, while the other uses a Lamarckian strategy. Our experiments show that using a Lamarckian op- erator makes the algorithm find networks with a low error rate, and the smallest size, while using the Bald- win effect obtains MLPs with the smallest error rate, and a larger size, taking longer to reach a solution. Both approaches obtain a lower average error than other BP-based algorithms like RPROP, other evolu- tionary methods and fuzzy logic based methods",
        "published": "2006-03-01T12:26:09Z",
        "link": "http://arxiv.org/abs/cs/0603004v1",
        "categories": [
            "cs.NE",
            "C.1.3"
        ]
    },
    {
        "title": "The Basic Kak Neural Network with Complex Inputs",
        "authors": [
            "Pritam Rajagopal"
        ],
        "summary": "The Kak family of neural networks is able to learn patterns quickly, and this speed of learning can be a decisive advantage over other competing models in many applications. Amongst the implementations of these networks are those using reconfigurable networks, FPGAs and optical networks. In some applications, it is useful to use complex data, and it is with that in mind that this introduction to the basic Kak network with complex inputs is being presented. The training algorithm is prescriptive and the network weights are assigned simply upon examining the inputs. The input is mapped using quaternary encoding for purpose of efficienty. This network family is part of a larger hierarchy of learning schemes that include quantum models.",
        "published": "2006-03-02T23:59:19Z",
        "link": "http://arxiv.org/abs/cs/0603015v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "The NoN Approach to Autonomic Face Recognition",
        "authors": [
            "Willie L. Scott II"
        ],
        "summary": "A method of autonomic face recognition based on the biologically plausible network of networks (NoN) model of information processing is presented. The NoN model is based on locally parallel and globally coordinated transformations in which the neurons or computational units form distributed networks, which themselves link to form larger networks. This models the structures in the cerebral cortex described by Mountcastle and the architecture based on that proposed for information processing by Sutton. In the proposed implementation, face images are processed by a nested family of locally operating networks along with a hierarchically superior network that classifies the information from each of the local networks. The results of the experiments yielded a maximum of 98.5% recognition accuracy and an average of 97.4% recognition accuracy on a benchmark database.",
        "published": "2006-03-09T17:35:31Z",
        "link": "http://arxiv.org/abs/cs/0603042v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Topological Grammars for Data Approximation",
        "authors": [
            "A. N. Gorban",
            "N. R. Sumner",
            "A. Y. Zinovyev"
        ],
        "summary": "A method of {\\it topological grammars} is proposed for multidimensional data approximation. For data with complex topology we define a {\\it principal cubic complex} of low dimension and given complexity that gives the best approximation for the dataset. This complex is a generalization of linear and non-linear principal manifolds and includes them as particular cases. The problem of optimal principal complex construction is transformed into a series of minimization problems for quadratic functionals. These quadratic functionals have a physically transparent interpretation in terms of elastic energy. For the energy computation, the whole complex is represented as a system of nodes and springs. Topologically, the principal complex is a product of one-dimensional continuums (represented by graphs), and the grammars describe how these continuums transform during the process of optimal complex construction. This factorization of the whole process onto one-dimensional transformations using minimization of quadratic energy functionals allow us to construct efficient algorithms.",
        "published": "2006-03-22T22:52:23Z",
        "link": "http://arxiv.org/abs/cs/0603090v2",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Theoretical Properties of Projection Based Multilayer Perceptrons with   Functional Inputs",
        "authors": [
            "Fabrice Rossi",
            "Brieuc Conan-Guez"
        ],
        "summary": "Many real world data are sampled functions. As shown by Functional Data Analysis (FDA) methods, spectra, time series, images, gesture recognition data, etc. can be processed more efficiently if their functional nature is taken into account during the data analysis process. This is done by extending standard data analysis methods so that they can apply to functional inputs. A general way to achieve this goal is to compute projections of the functional data onto a finite dimensional sub-space of the functional space. The coordinates of the data on a basis of this sub-space provide standard vector representations of the functions. The obtained vectors can be processed by any standard method. In our previous work, this general approach has been used to define projection based Multilayer Perceptrons (MLPs) with functional inputs. We study in this paper important theoretical properties of the proposed model. We show in particular that MLPs with functional inputs are universal approximators: they can approximate to arbitrary accuracy any continuous mapping from a compact sub-space of a functional space to R. Moreover, we provide a consistency result that shows that any mapping from a functional space to R can be learned thanks to examples by a projection based MLP: the generalization mean square error of the MLP decreases to the smallest possible mean square error on the data when the number of examples goes to infinity.",
        "published": "2006-04-01T20:16:39Z",
        "link": "http://arxiv.org/abs/cs/0604001v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Concerning the differentiability of the energy function in vector   quantization algorithms",
        "authors": [
            "Dominique Lepetz",
            "Max Nemoz-Gaillard",
            "Michael Aupetit"
        ],
        "summary": "The adaptation rule for Vector Quantization algorithms, and consequently the convergence of the generated sequence, depends on the existence and properties of a function called the energy function, defined on a topological manifold. Our aim is to investigate the conditions of existence of such a function for a class of algorithms examplified by the initial ''K-means'' and Kohonen algorithms. The results presented here supplement previous studies and show that the energy function is not always a potential but at least the uniform limit of a series of potential functions which we call a pseudo-potential. Our work also shows that a large number of existing vector quantization algorithms developped by the Artificial Neural Networks community fall into this category. The framework we define opens the way to study the convergence of all the corresponding adaptation rules at once, and a theorem gives promising insights in that direction. We also demonstrate that the ''K-means'' energy function is a pseudo-potential but not a potential in general. Consequently, the energy function associated to the ''Neural-Gas'' is not a potential in general.",
        "published": "2006-04-11T14:00:22Z",
        "link": "http://arxiv.org/abs/cs/0604046v1",
        "categories": [
            "cs.LG",
            "cs.NE"
        ]
    },
    {
        "title": "Laws in Darwinian Evolutionary Theory",
        "authors": [
            "P Ao"
        ],
        "summary": "In the present article the recent works to formulate laws in Darwinian evolutionary dynamics are discussed. Although there is a strong consensus that general laws in biology may exist, opinions opposing such suggestion are abundant. Based on recent progress in both mathematics and biology, another attempt to address this issue is made in the present article. Specifically, three laws which form a mathematical framework for the evolutionary dynamics in biology are postulated. The second law is most quantitative and is explicitly expressed in the unique form of a stochastic differential equation. Salient features of Darwinian evolutionary dynamics are captured by this law: the probabilistic nature of evolution, ascendancy, and the adaptive landscape. Four dynamical elements are introduced in this formulation: the ascendant matrix, the transverse matrix, the Wright evolutionary potential, and the stochastic drive. The first law may be regarded as a special case of the second law. It gives the reference point to discuss the evolutionary dynamics. The third law describes the relationship between the focused level of description to its lower and higher ones, and defines the dichotomy of deterministic and stochastic drives. It is an acknowledgement of the hierarchical structure in biology. A new interpretation of Fisher's fundamental theorem of natural selection is provided in terms of the F-Theorem. The proposed laws are based on continuous representation in both time and population. Their generic nature is demonstrated through their equivalence to classical formulations. The present three laws appear to provide a coherent framework for the further development of the subject.",
        "published": "2006-05-14T20:01:09Z",
        "link": "http://arxiv.org/abs/q-bio/0605020v1",
        "categories": [
            "q-bio.PE",
            "cond-mat.stat-mech",
            "cs.NE",
            "math.OC",
            "nlin.AO",
            "physics.bio-ph",
            "q-bio.QM"
        ]
    },
    {
        "title": "On the possible Computational Power of the Human Mind",
        "authors": [
            "Hector Zenil",
            "Francisco Hernandez-Quiroz"
        ],
        "summary": "The aim of this paper is to address the question: Can an artificial neural network (ANN) model be used as a possible characterization of the power of the human mind? We will discuss what might be the relationship between such a model and its natural counterpart. A possible characterization of the different power capabilities of the mind is suggested in terms of the information contained (in its computational complexity) or achievable by it. Such characterization takes advantage of recent results based on natural neural networks (NNN) and the computational power of arbitrary artificial neural networks (ANN). The possible acceptance of neural networks as the model of the human mind's operation makes the aforementioned quite relevant.",
        "published": "2006-05-15T17:56:55Z",
        "link": "http://arxiv.org/abs/cs/0605065v4",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Cross-Entropic Learning of a Machine for the Decision in a Partially   Observable Universe",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "Revision of the paper previously entitled \"Learning a Machine for the Decision in a Partially Observable Markov Universe\" In this paper, we are interested in optimal decisions in a partially observable universe. Our approach is to directly approximate an optimal strategic tree depending on the observation. This approximation is made by means of a parameterized probabilistic law. A particular family of hidden Markov models, with input \\emph{and} output, is considered as a model of policy. A method for optimizing the parameters of these HMMs is proposed and applied. This optimization is based on the cross-entropic principle for rare events simulation developed by Rubinstein.",
        "published": "2006-05-18T07:47:58Z",
        "link": "http://arxiv.org/abs/math/0605498v1",
        "categories": [
            "math.OC",
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Optimal estimation for Large-Eddy Simulation of turbulence and   application to the analysis of subgrid models",
        "authors": [
            "Antoine Moreau",
            "Olivier Teytaud",
            "Jean-Pierre Bertoglio"
        ],
        "summary": "The tools of optimal estimation are applied to the study of subgrid models for Large-Eddy Simulation of turbulence. The concept of optimal estimator is introduced and its properties are analyzed in the context of applications to a priori tests of subgrid models. Attention is focused on the Cook and Riley model in the case of a scalar field in isotropic turbulence. Using DNS data, the relevance of the beta assumption is estimated by computing (i) generalized optimal estimators and (ii) the error brought by this assumption alone. Optimal estimators are computed for the subgrid variance using various sets of variables and various techniques (histograms and neural networks). It is shown that optimal estimators allow a thorough exploration of models. Neural networks are proved to be relevant and very efficient in this framework, and further usages are suggested.",
        "published": "2006-06-06T11:36:51Z",
        "link": "http://arxiv.org/abs/physics/0606053v2",
        "categories": [
            "physics.class-ph",
            "cs.NE"
        ]
    },
    {
        "title": "May We Have Your Attention: Analysis of a Selective Attention Task",
        "authors": [
            "Eldan Goldenberg",
            "Jacob R. Garcowski",
            "Randall D. Beer"
        ],
        "summary": "In this paper we present a deeper analysis than has previously been carried out of a selective attention problem, and the evolution of continuous-time recurrent neural networks to solve it. We show that the task has a rich structure, and agents must solve a variety of subproblems to perform well. We consider the relationship between the complexity of an agent and the ease with which it can evolve behavior that generalizes well across subproblems, and demonstrate a shaping protocol that improves generalization.",
        "published": "2006-06-29T22:33:31Z",
        "link": "http://arxiv.org/abs/cs/0606126v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Theory of sexes by Geodakian as it is advanced by Iskrin",
        "authors": [
            "Boris D. Lubachevsky"
        ],
        "summary": "In 1960s V.Geodakian proposed a theory that explains sexes as a mechanism for evolutionary adaptation of the species to changing environmental conditions. In 2001 V.Iskrin refined and augmented the concepts of Geodakian and gave a new and interesting explanation to several phenomena which involve sex, and sex ratio, including the war-years phenomena. He also introduced a new concept of the \"catastrophic sex ratio.\" This note is an attempt to digest technical aspects of the new ideas by Iskrin.",
        "published": "2006-07-03T04:03:23Z",
        "link": "http://arxiv.org/abs/cs/0607007v4",
        "categories": [
            "cs.NE",
            "cs.GL",
            "F.1.1; J.3"
        ]
    },
    {
        "title": "Modelling the Probability Density of Markov Sources",
        "authors": [
            "Stephen Luttrell"
        ],
        "summary": "This paper introduces an objective function that seeks to minimise the average total number of bits required to encode the joint state of all of the layers of a Markov source. This type of encoder may be applied to the problem of optimising the bottom-up (recognition model) and top-down (generative model) connections in a multilayer neural network, and it unifies several previous results on the optimisation of multilayer neural networks.",
        "published": "2006-07-06T18:49:20Z",
        "link": "http://arxiv.org/abs/cs/0607019v1",
        "categories": [
            "cs.NE",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Evaluation de Techniques de Traitement des Refusés pour l'Octroi de   Crédit",
        "authors": [
            "Emmanuel Viennet",
            "Françoise Fogelman Soulié",
            "Benoit Rognier"
        ],
        "summary": "We present the problem of \"Reject Inference\" for credit acceptance. Because of the current legal framework (Basel II), credit institutions need to industrialize their processes for credit acceptance, including Reject Inference. We present here a methodology to compare various techniques of Reject Inference and show that it is necessary, in the absence of real theoretical results, to be able to produce and compare models adapted to available data (selection of \"best\" model conditionnaly on data). We describe some simulations run on a small data set to illustrate the approach and some strategies for choosing the control group, which is the only valid approach to Reject Inference.",
        "published": "2006-07-11T15:25:56Z",
        "link": "http://arxiv.org/abs/cs/0607048v1",
        "categories": [
            "cs.NE",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Neural Networks with Complex and Quaternion Inputs",
        "authors": [
            "Adityan Rishiyur"
        ],
        "summary": "This article investigates Kak neural networks, which can be instantaneously trained, for complex and quaternion inputs. The performance of the basic algorithm has been analyzed and shown how it provides a plausible model of human perception and understanding of images. The motivation for studying quaternion inputs is their use in representing spatial rotations that find applications in computer graphics, robotics, global navigation, computer vision and the spatial orientation of instruments. The problem of efficient mapping of data in quaternion neural networks is examined. Some problems that need to be addressed before quaternion neural networks find applications are identified.",
        "published": "2006-07-18T21:01:43Z",
        "link": "http://arxiv.org/abs/cs/0607090v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Ideas by Statistical Mechanics (ISM)",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Ideas by Statistical Mechanics (ISM) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions. The program is based on the author's work in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the author's Adaptive Simulated Annealing (ASA) code for optimizations of training sets, as well as for importance-sampling to apply the author's copula financial risk-management codes, Trading in Risk Dimensions (TRD), for assessments of risk and uncertainty. This product can be used for decision support for projects ranging from diplomatic, information, military, and economic (DIME) factors of propagation/evolution of ideas, to commercial sales, trading indicators across sectors of financial markets, advertising and political campaigns, etc. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, is the proposed model. Parameters with a given subset of macrocolumns will be fit using ASA to patterns representing ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Tools of financial risk management, developed by the author to process correlated multivariate systems with differing non-Gaussian distributions using modern copula analysis, importance-sampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes.",
        "published": "2006-07-23T16:12:47Z",
        "link": "http://arxiv.org/abs/cs/0607103v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "cs.NE"
        ]
    },
    {
        "title": "Parametrical Neural Networks and Some Other Similar Architectures",
        "authors": [
            "Leonid B. Litinskii"
        ],
        "summary": "A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated.",
        "published": "2006-08-18T08:28:23Z",
        "link": "http://arxiv.org/abs/cs/0608073v1",
        "categories": [
            "cs.CV",
            "cs.NE"
        ]
    },
    {
        "title": "Searching for Globally Optimal Functional Forms for Inter-Atomic   Potentials Using Parallel Tempering and Genetic Programming",
        "authors": [
            "A. Slepoy",
            "A. P. Thompson",
            "M. D. Peters"
        ],
        "summary": "We develop a Genetic Programming-based methodology that enables discovery of novel functional forms for classical inter-atomic force-fields, used in molecular dynamics simulations. Unlike previous efforts in the field, that fit only the parameters to the fixed functional forms, we instead use a novel algorithm to search the space of many possible functional forms. While a follow-on practical procedure will use experimental and {\\it ab inito} data to find an optimal functional form for a forcefield, we first validate the approach using a manufactured solution. This validation has the advantage of a well-defined metric of success. We manufactured a training set of atomic coordinate data with an associated set of global energies using the well-known Lennard-Jones inter-atomic potential. We performed an automatic functional form fitting procedure starting with a population of random functions, using a genetic programming functional formulation, and a parallel tempering Metropolis-based optimization algorithm. Our massively-parallel method independently discovered the Lennard-Jones function after searching for several hours on 100 processors and covering a miniscule portion of the configuration space. We find that the method is suitable for unsupervised discovery of functional forms for inter-atomic potentials/force-fields. We also find that our parallel tempering Metropolis-based approach significantly improves the optimization convergence time, and takes good advantage of the parallel cluster architecture.",
        "published": "2006-08-18T23:17:32Z",
        "link": "http://arxiv.org/abs/cs/0608078v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Neural Network Clustering Based on Distances Between Objects",
        "authors": [
            "Leonid B. Litinskii",
            "Dmitry E. Romanov"
        ],
        "summary": "We present an algorithm of clustering of many-dimensional objects, where only the distances between objects are used. Centers of classes are found with the aid of neuron-like procedure with lateral inhibition. The result of clustering does not depend on starting conditions. Our algorithm makes it possible to give an idea about classes that really exist in the empirical data. The results of computer simulations are presented.",
        "published": "2006-08-29T13:24:37Z",
        "link": "http://arxiv.org/abs/cs/0608115v1",
        "categories": [
            "cs.CV",
            "cs.NE"
        ]
    },
    {
        "title": "From Neuron to Neural Networks dynamics",
        "authors": [
            "B. Cessac",
            "M. Samuelides"
        ],
        "summary": "This paper presents an overview of some techniques and concepts coming from dynamical system theory and used for the analysis of dynamical neural networks models. In a first section, we describe the dynamics of the neuron, starting from the Hodgkin-Huxley description, which is somehow the canonical description for the ``biological neuron''. We discuss some models reducing the Hodgkin-Huxley model to a two dimensional dynamical system, keeping one of the main feature of the neuron: its excitability. We present then examples of phase diagram and bifurcation analysis for the Hodgin-Huxley equations. Finally, we end this section by a dynamical system analysis for the nervous flux propagation along the axon. We then consider neuron couplings, with a brief description of synapses, synaptic plasticiy and learning, in a second section. We also briefly discuss the delicate issue of causal action from one neuron to another when complex feedback effects and non linear dynamics are involved. The third section presents the limit of weak coupling and the use of normal forms technics to handle this situation. We consider then several examples of recurrent models with different type of synaptic interactions (symmetric, cooperative, random). We introduce various techniques coming from statistical physics and dynamical systems theory. A last section is devoted to a detailed example of recurrent model where we go in deep in the analysis of the dynamics and discuss the effect of learning on the neuron dynamics. We also present recent methods allowing the analysis of the non linear effects of the neural dynamics on signal propagation and causal action. An appendix, presenting the main notions of dynamical systems theory useful for the comprehension of the chapter, has been added for the convenience of the reader.",
        "published": "2006-09-15T06:59:44Z",
        "link": "http://arxiv.org/abs/nlin/0609038v1",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cs.NE"
        ]
    },
    {
        "title": "Quantum Pattern Retrieval by Qubit Networks with Hebb Interactions",
        "authors": [
            "M. Cristina Diamantini",
            "Carlo A. Trugenberger"
        ],
        "summary": "Qubit networks with long-range interactions inspired by the Hebb rule can be used as quantum associative memories. Starting from a uniform superposition, the unitary evolution generated by these interactions drives the network through a quantum phase transition at a critical computation time, after which ferromagnetic order guarantees that a measurement retrieves the stored memory. The maximum memory capacity p of these qubit networks is reached at a memory density p/n=1.",
        "published": "2006-09-15T15:19:31Z",
        "link": "http://arxiv.org/abs/quant-ph/0609117v1",
        "categories": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cs.NE"
        ]
    },
    {
        "title": "Cross-Entropy method: convergence issues for extended implementation",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "The cross-entropy method (CE) developed by R. Rubinstein is an elegant practical principle for simulating rare events. The method approximates the probability of the rare event by means of a family of probabilistic models. The method has been extended to optimization, by considering an optimal event as a rare event. CE works rather good when dealing with deterministic function optimization. Now, it appears that two conditions are needed for a good convergence of the method. First, it is necessary to have a family of models sufficiently flexible for discriminating the optimal events. Indirectly, it appears also that the function to be optimized should be deterministic. The purpose of this paper is to consider the case of partially discriminating model family, and of stochastic functions. It will be shown on simple examples that the CE could fail when relaxing these hypotheses. Alternative improvements of the CE method are investigated and compared on random examples in order to handle this issue.",
        "published": "2006-09-16T07:00:36Z",
        "link": "http://arxiv.org/abs/math/0609461v1",
        "categories": [
            "math.OC",
            "cs.LG",
            "cs.NE",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Problem Evolution: A new approach to problem solving systems",
        "authors": [
            "Goren Gordon",
            "Uri Einziger-Lowicz"
        ],
        "summary": "In this paper we present a novel tool to evaluate problem solving systems. Instead of using a system to solve a problem, we suggest using the problem to evaluate the system. By finding a numerical representation of a problem's complexity, one can implement genetic algorithm to search for the most complex problem the given system can solve. This allows a comparison between different systems that solve the same set of problems. In this paper we implement this approach on pattern recognition neural networks to try and find the most complex pattern a given configuration can solve. The complexity of the pattern is calculated using linguistic complexity. The results demonstrate the power of the problem evolution approach in ranking different neural network configurations according to their pattern recognition abilities. Future research and implementations of this technique are also discussed.",
        "published": "2006-09-22T12:40:07Z",
        "link": "http://arxiv.org/abs/cs/0609125v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "A Computational Model of Spatial Memory Anticipation during Visual   Search",
        "authors": [
            "Jérémy Fix",
            "Julien Vitay",
            "Nicolas Rougier"
        ],
        "summary": "Some visual search tasks require to memorize the location of stimuli that have been previously scanned. Considerations about the eye movements raise the question of how we are able to maintain a coherent memory, despite the frequent drastically changes in the perception. In this article, we present a computational model that is able to anticipate the consequences of the eye movements on the visual perception in order to update a spatial memory",
        "published": "2006-10-09T11:42:27Z",
        "link": "http://arxiv.org/abs/cs/0610041v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Fitness Uniform Optimization",
        "authors": [
            "Marcus Hutter",
            "Shane Legg"
        ],
        "summary": "In evolutionary algorithms, the fitness of a population increases with time by mutating and recombining individuals and by a biased selection of more fit individuals. The right selection pressure is critical in ensuring sufficient optimization progress on the one hand and in preserving genetic diversity to be able to escape from local optima on the other hand. Motivated by a universal similarity relation on the individuals, we propose a new selection scheme, which is uniform in the fitness values. It generates selection pressure toward sparsely populated fitness regions, not necessarily toward higher fitness, as is the case for all other selection schemes. We show analytically on a simple example that the new selection scheme can be much more effective than standard selection schemes. We also propose a new deletion scheme which achieves a similar result via deletion and show how such a scheme preserves genetic diversity more effectively than standard approaches. We compare the performance of the new schemes to tournament selection and random deletion on an artificial deceptive problem and a range of NP-hard problems: traveling salesman, set covering and satisfiability.",
        "published": "2006-10-20T16:37:11Z",
        "link": "http://arxiv.org/abs/cs/0610126v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Evolving controllers for simulated car racing",
        "authors": [
            "Julian Togelius",
            "Simon M. Lucas"
        ],
        "summary": "This paper describes the evolution of controllers for racing a simulated radio-controlled car around a track, modelled on a real physical track. Five different controller architectures were compared, based on neural networks, force fields and action sequences. The controllers use either egocentric (first person), Newtonian (third person) or no information about the state of the car (open-loop controller). The only controller that was able to evolve good racing behaviour was based on a neural network acting on egocentric inputs.",
        "published": "2006-11-02T00:47:57Z",
        "link": "http://arxiv.org/abs/cs/0611006v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "cs.RO"
        ]
    },
    {
        "title": "An associative memory for the on-line recognition and prediction of   temporal sequences",
        "authors": [
            "J. Bose",
            "S. B. Furber",
            "J. L. Shapiro"
        ],
        "summary": "This paper presents the design of an associative memory with feedback that is capable of on-line temporal sequence learning. A framework for on-line sequence learning has been proposed, and different sequence learning models have been analysed according to this framework. The network model is an associative memory with a separate store for the sequence context of a symbol. A sparse distributed memory is used to gain scalability. The context store combines the functionality of a neural layer with a shift register. The sensitivity of the machine to the sequence context is controllable, resulting in different characteristic behaviours. The model can store and predict on-line sequences of various types and length. Numerical simulations on the model have been carried out to determine its properties.",
        "published": "2006-11-05T01:15:01Z",
        "link": "http://arxiv.org/abs/cs/0611020v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "V-like formations in flocks of artificial birds",
        "authors": [
            "Andre Nathan",
            "Valmir C. Barbosa"
        ],
        "summary": "We consider flocks of artificial birds and study the emergence of V-like formations during flight. We introduce a small set of fully distributed positioning rules to guide the birds' movements and demonstrate, by means of simulations, that they tend to lead to stabilization into several of the well-known V-like formations that have been observed in nature. We also provide quantitative indicators that we believe are closely related to achieving V-like formations, and study their behavior over a large set of independent simulations.",
        "published": "2006-11-07T20:25:28Z",
        "link": "http://arxiv.org/abs/cs/0611032v2",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Cartes auto-organisées pour l'analyse exploratoire de données et   la visualisation",
        "authors": [
            "Marie Cottrell",
            "SmaÏl Ibbou",
            "Patrick Letrémy",
            "Patrick Rousset"
        ],
        "summary": "This paper shows how to use the Kohonen algorithm to represent multidimensional data, by exploiting the self-organizing property. It is possible to get such maps as well for quantitative variables as for qualitative ones, or for a mixing of both. The contents of the paper come from various works by SAMOS-MATISSE members, in particular by E. de Bodt, B. Girard, P. Letr\\'{e}my, S. Ibbou, P. Rousset. Most of the examples have been studied with the computation routines written by Patrick Letr\\'{e}my, with the language IML-SAS, which are available on the WEB page http://samos.univ-paris1.fr.",
        "published": "2006-11-14T12:52:21Z",
        "link": "http://arxiv.org/abs/math/0611422v1",
        "categories": [
            "math.ST",
            "cs.NE",
            "nlin.AO",
            "stat.TH"
        ]
    },
    {
        "title": "Advances in Self Organising Maps",
        "authors": [
            "Marie Cottrell",
            "Michel Verleysen"
        ],
        "summary": "The Self-Organizing Map (SOM) with its related extensions is the most popular artificial neural algorithm for use in unsupervised learning, clustering, classification and data visualization. Over 5,000 publications have been reported in the open literature, and many commercial projects employ the SOM as a tool for solving hard real-world problems. Each two years, the \"Workshop on Self-Organizing Maps\" (WSOM) covers the new developments in the field. The WSOM series of conferences was initiated in 1997 by Prof. Teuvo Kohonen, and has been successfully organized in 1997 and 1999 by the Helsinki University of Technology, in 2001 by the University of Lincolnshire and Humberside, and in 2003 by the Kyushu Institute of Technology. The Universit\\'{e} Paris I Panth\\'{e}on Sorbonne (SAMOS-MATISSE research centre) organized WSOM 2005 in Paris on September 5-8, 2005.",
        "published": "2006-11-14T13:19:46Z",
        "link": "http://arxiv.org/abs/cs/0611058v1",
        "categories": [
            "cs.NE",
            "math.ST",
            "nlin.AO",
            "stat.TH"
        ]
    },
    {
        "title": "Working times in atypical forms of employment: the special case of   part-time work",
        "authors": [
            "Patrick Letrémy",
            "Marie Cottrell"
        ],
        "summary": "In the present article, we attempt to devise a typology of forms of part-time employment by applying a widely used neuronal methodology called Kohonen maps. Starting out with data that we describe using category-specific variables, we show how it is possible to represent observations and the modalities of the variables that define them simultaneously, on a single map. This allows us to ascertain, and to try to describe, the main categories of part-time employment.",
        "published": "2006-11-14T16:03:38Z",
        "link": "http://arxiv.org/abs/math/0611433v1",
        "categories": [
            "math.ST",
            "cs.NE",
            "stat.TH"
        ]
    },
    {
        "title": "Evolutionary Optimization in an Algorithmic Setting",
        "authors": [
            "Mark Burgin",
            "Eugene Eberbach"
        ],
        "summary": "Evolutionary processes proved very useful for solving optimization problems. In this work, we build a formalization of the notion of cooperation and competition of multiple systems working toward a common optimization goal of the population using evolutionary computation techniques. It is justified that evolutionary algorithms are more expressive than conventional recursive algorithms. Three subclasses of evolutionary algorithms are proposed here: bounded finite, unbounded finite and infinite types. Some results on completeness, optimality and search decidability for the above classes are presented. A natural extension of Evolutionary Turing Machine model developed in this paper allows one to mathematically represent and study properties of cooperation and competition in a population of optimized species.",
        "published": "2006-11-16T03:27:16Z",
        "link": "http://arxiv.org/abs/cs/0611077v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Managing network congestion with a Kohonen-based RED queue",
        "authors": [
            "Emmanuel Lochin",
            "Bruno Talavera"
        ],
        "summary": "The behaviour of the TCP AIMD algorithm is known to cause queue length oscillations when congestion occurs at a router output link. Indeed, due to these queueing variations, end-to-end applications experience large delay jitter. Many studies have proposed efficient Active Queue Management (AQM) mechanisms in order to reduce queue oscillations and stabilize the queue length. These AQM are mostly improvements of the Random Early Detection (RED) model. Unfortunately, these enhancements do not react in a similar manner for various network conditions and are strongly sensitive to their initial setting parameters. Although this paper proposes a solution to overcome the difficulties of setting these parameters by using a Kohonen neural network model, another goal of this study is to investigate whether cognitive intelligence could be placed in the core network to solve such stability problem. In our context, we use results from the neural network area to demonstrate that our proposal, named Kohonen-RED (KRED), enables a stable queue length without complex parameters setting and passive measurements.",
        "published": "2006-11-16T11:26:22Z",
        "link": "http://arxiv.org/abs/cs/0611079v4",
        "categories": [
            "cs.NI",
            "cs.NE"
        ]
    },
    {
        "title": "Learning and discrimination through STDP in a top-down modulated   associative memory",
        "authors": [
            "Anthony Mouraud",
            "Hélène Paugam-Moisy"
        ],
        "summary": "This article underlines the learning and discrimination capabilities of a model of associative memory based on artificial networks of spiking neurons. Inspired from neuropsychology and neurobiology, the model implements top-down modulations, as in neocortical layer V pyramidal neurons, with a learning rule based on synaptic plasticity (STDP), for performing a multimodal association learning task. A temporal correlation method of analysis proves the ability of the model to associate specific activity patterns to different samples of stimulation. Even in the absence of initial learning and with continuously varying weights, the activity patterns become stable enough for discrimination.",
        "published": "2006-11-21T12:54:29Z",
        "link": "http://arxiv.org/abs/cs/0611104v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "On the Benefits of Inoculation, an Example in Train Scheduling",
        "authors": [
            "Yann Semet",
            "Marc Schoenauer"
        ],
        "summary": "The local reconstruction of a railway schedule following a small perturbation of the traffic, seeking minimization of the total accumulated delay, is a very difficult and tightly constrained combinatorial problem. Notoriously enough, the railway company's public image degrades proportionally to the amount of daily delays, and the same goes for its profit! This paper describes an inoculation procedure which greatly enhances an evolutionary algorithm for train re-scheduling. The procedure consists in building the initial population around a pre-computed solution based on problem-related information available beforehand. The optimization is performed by adapting times of departure and arrival, as well as allocation of tracks, for each train at each station. This is achieved by a permutation-based evolutionary algorithm that relies on a semi-greedy heuristic scheduler to gradually reconstruct the schedule by inserting trains one after another. Experimental results are presented on various instances of a large real-world case involving around 500 trains and more than 1 million constraints. In terms of competition with commercial math ematical programming tool ILOG CPLEX, it appears that within a large class of instances, excluding trivial instances as well as too difficult ones, and with very few exceptions, a clever initialization turns an encouraging failure into a clear-cut success auguring of substantial financial savings.",
        "published": "2006-11-28T12:44:43Z",
        "link": "http://arxiv.org/abs/cs/0611140v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Lossless fitness inheritance in genetic algorithms for decision trees",
        "authors": [
            "Dimitris Kalles",
            "Athanassios Papagelis"
        ],
        "summary": "When genetic algorithms are used to evolve decision trees, key tree quality parameters can be recursively computed and re-used across generations of partially similar decision trees. Simply storing instance indices at leaves is enough for fitness to be piecewise computed in a lossless fashion. We show the derivation of the (substantial) expected speed-up on two bounding case problems and trace the attractive property of lossless fitness inheritance to the divide-and-conquer nature of decision trees. The theoretical results are supported by experimental evidence.",
        "published": "2006-11-30T15:20:15Z",
        "link": "http://arxiv.org/abs/cs/0611166v2",
        "categories": [
            "cs.AI",
            "cs.DS",
            "cs.NE"
        ]
    },
    {
        "title": "On Measuring the Impact of Human Actions in the Machine Learning of a   Board Game's Playing Policies",
        "authors": [
            "Dimitris Kalles"
        ],
        "summary": "We investigate systematically the impact of human intervention in the training of computer players in a strategy board game. In that game, computer players utilise reinforcement learning with neural networks for evolving their playing strategies and demonstrate a slow learning speed. Human intervention can significantly enhance learning performance, but carry-ing it out systematically seems to be more of a problem of an integrated game development environment as opposed to automatic evolutionary learning.",
        "published": "2006-11-30T15:30:36Z",
        "link": "http://arxiv.org/abs/cs/0611163v1",
        "categories": [
            "cs.AI",
            "cs.GT",
            "cs.NE"
        ]
    },
    {
        "title": "Statistical mechanics of neocortical interactions: Portfolio of   Physiological Indicators",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "There are several kinds of non-invasive imaging methods that are used to collect data from the brain, e.g., EEG, MEG, PET, SPECT, fMRI, etc. It is difficult to get resolution of information processing using any one of these methods. Approaches to integrate data sources may help to get better resolution of data and better correlations to behavioral phenomena ranging from attention to diagnoses of disease. The approach taken here is to use algorithms developed for the author's Trading in Risk Dimensions (TRD) code using modern methods of copula portfolio risk management, with joint probability distributions derived from the author's model of statistical mechanics of neocortical interactions (SMNI). The author's Adaptive Simulated Annealing (ASA) code is for optimizations of training sets, as well as for importance-sampling. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes.",
        "published": "2006-12-18T21:02:03Z",
        "link": "http://arxiv.org/abs/cs/0612087v1",
        "categories": [
            "cs.CE",
            "cs.IT",
            "cs.NE",
            "math.IT",
            "q-bio.QM"
        ]
    },
    {
        "title": "Sufficient Conditions for Coarse-Graining Evolutionary Dynamics",
        "authors": [
            "Keki Burjorjee"
        ],
        "summary": "It is commonly assumed that the ability to track the frequencies of a set of schemata in the evolving population of an infinite population genetic algorithm (IPGA) under different fitness functions will advance efforts to obtain a theory of adaptation for the simple GA. Unfortunately, for IPGAs with long genomes and non-trivial fitness functions there do not currently exist theoretical results that allow such a study. We develop a simple framework for analyzing the dynamics of an infinite population evolutionary algorithm (IPEA). This framework derives its simplicity from its abstract nature. In particular we make no commitment to the data-structure of the genomes, the kind of variation performed, or the number of parents involved in a variation operation. We use this framework to derive abstract conditions under which the dynamics of an IPEA can be coarse-grained. We then use this result to derive concrete conditions under which it becomes computationally feasible to closely approximate the frequencies of a family of schemata of relatively low order over multiple generations, even when the bitstsrings in the evolving population of the IPGA are long.",
        "published": "2006-12-21T02:18:17Z",
        "link": "http://arxiv.org/abs/cs/0612104v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; F.2"
        ]
    },
    {
        "title": "Magnification Laws of Winner-Relaxing and Winner-Enhancing Kohonen   Feature Maps",
        "authors": [
            "Jens Christian Claussen"
        ],
        "summary": "Self-Organizing Maps are models for unsupervised representation formation of cortical receptor fields by stimuli-driven self-organization in laterally coupled winner-take-all feedforward structures. This paper discusses modifications of the original Kohonen model that were motivated by a potential function, in their ability to set up a neural mapping of maximal mutual information. Enhancing the winner update, instead of relaxing it, results in an algorithm that generates an infomax map corresponding to magnification exponent of one. Despite there may be more than one algorithm showing the same magnification exponent, the magnification law is an experimentally accessible quantity and therefore suitable for quantitative description of neural optimization principles.",
        "published": "2006-12-30T11:48:32Z",
        "link": "http://arxiv.org/abs/cs/0701003v1",
        "categories": [
            "cs.NE",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Integrality gaps of semidefinite programs for Vertex Cover and relations   to $\\ell_1$ embeddability of Negative Type metrics",
        "authors": [
            "Hamed Hatami",
            "Avner Magen",
            "Vangelis Markakis"
        ],
        "summary": "We study various SDP formulations for {\\sc Vertex Cover} by adding different constraints to the standard formulation. We show that {\\sc Vertex Cover} cannot be approximated better than $2-o(1)$ even when we add the so called pentagonal inequality constraints to the standard SDP formulation, en route answering an open question of Karakostas~\\cite{Karakostas}. We further show the surprising fact that by strengthening the SDP with the (intractable) requirement that the metric interpretation of the solution is an $\\ell_1$ metric, we get an exact relaxation (integrality gap is 1), and on the other hand if the solution is arbitrarily close to being $\\ell_1$ embeddable, the integrality gap may be as big as $2-o(1)$. Finally, inspired by the above findings, we use ideas from the integrality gap construction of Charikar \\cite{Char02} to provide a family of simple examples for negative type metrics that cannot be embedded into $\\ell_1$ with distortion better than $8/7-\\eps$. To this end we prove a new isoperimetric inequality for the hypercube.",
        "published": "2006-01-05T23:10:58Z",
        "link": "http://arxiv.org/abs/cs/0601011v3",
        "categories": [
            "cs.DS",
            "cs.DM",
            "math.MG"
        ]
    },
    {
        "title": "Algebraic Structures and Algorithms for Matching and Matroid Problems   (Preliminary Version)",
        "authors": [
            "Nicholas J. A. Harvey"
        ],
        "summary": "Basic path-matchings, introduced by Cunningham and Geelen (FOCS 1996), are a common generalization of matroid intersection and non-bipartite matching. The main results of this paper are a new algebraic characterization of basic path-matching problems and an algorithm for constructing basic path-matchings in O(n^w) time, where n is the number of vertices and w is the exponent for matrix multiplication. Our algorithms are randomized, and our approach assumes that the given matroids are linear and can be represented over the same field.   Our main results have interesting consequences for several special cases of path-matching problems. For matroid intersection, we obtain an algorithm with running time O(nr^(w-1))=O(nr^1.38), where the matroids have n elements and rank r. This improves the long-standing bound of O(nr^1.62) due to Gabow and Xu (FOCS 1989). Also, we obtain a simple, purely algebraic algorithm for non-bipartite matching with running time O(n^w). This resolves the central open problem of Mucha and Sankowski (FOCS 2004).",
        "published": "2006-01-09T13:54:41Z",
        "link": "http://arxiv.org/abs/cs/0601026v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "An O(1) Solution to the Prefix Sum Problem on a Specialized Memory   Architecture",
        "authors": [
            "Andrej Brodnik",
            "Johan Karlsson",
            "J. Ian Munro",
            "Andreas Nilsson"
        ],
        "summary": "In this paper we study the Prefix Sum problem introduced by Fredman.   We show that it is possible to perform both update and retrieval in O(1) time simultaneously under a memory model in which individual bits may be shared by several words.   We also show that two variants (generalizations) of the problem can be solved optimally in $\\Theta(\\lg N)$ time under the comparison based model of computation.",
        "published": "2006-01-18T21:20:10Z",
        "link": "http://arxiv.org/abs/cs/0601081v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.IR",
            "E.1; F.1.1"
        ]
    },
    {
        "title": "Randomized Fast Design of Short DNA Words",
        "authors": [
            "Ming-Yang Kao",
            "Manan Sanghi",
            "Robert Schweller"
        ],
        "summary": "We consider the problem of efficiently designing sets (codes) of equal-length DNA strings (words) that satisfy certain combinatorial constraints. This problem has numerous motivations including DNA computing and DNA self-assembly. Previous work has extended results from coding theory to obtain bounds on code size for new biologically motivated constraints and has applied heuristic local search and genetic algorithm techniques for code design. This paper proposes a natural optimization formulation of the DNA code design problem in which the goal is to design n strings that satisfy a given set of constraints while minimizing the length of the strings. For multiple sets of constraints, we provide high-probability algorithms that run in time polynomial in n and any given constraint parameters, and output strings of length within a constant factor of the optimal. To the best of our knowledge, this work is the first to consider this type of optimization problem in the context of DNA code design.",
        "published": "2006-01-19T00:22:56Z",
        "link": "http://arxiv.org/abs/cs/0601084v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Fast Lexically Constrained Viterbi Algorithm (FLCVA): Simultaneous   Optimization of Speed and Memory",
        "authors": [
            "Alain Lifchitz",
            "Frederic Maire",
            "Dominique Revuz"
        ],
        "summary": "Lexical constraints on the input of speech and on-line handwriting systems improve the performance of such systems. A significant gain in speed can be achieved by integrating in a digraph structure the different Hidden Markov Models (HMM) corresponding to the words of the relevant lexicon. This integration avoids redundant computations by sharing intermediate results between HMM's corresponding to different words of the lexicon. In this paper, we introduce a token passing method to perform simultaneously the computation of the a posteriori probabilities of all the words of the lexicon. The coding scheme that we introduce for the tokens is optimal in the information theory sense. The tokens use the minimum possible number of bits. Overall, we optimize simultaneously the execution speed and the memory requirement of the recognition systems.",
        "published": "2006-01-25T17:50:13Z",
        "link": "http://arxiv.org/abs/cs/0601108v4",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.DS",
            "G.2.2; I.5.5; E.2"
        ]
    },
    {
        "title": "A unifying framework for seed sensitivity and its application to subset   seeds",
        "authors": [
            "Gregory Kucherov",
            "Laurent Noé",
            "Mihkail Roytberg"
        ],
        "summary": "We propose a general approach to compute the seed sensitivity, that can be applied to different definitions of seeds. It treats separately three components of the seed sensitivity problem -- a set of target alignments, an associated probability distribution, and a seed model -- that are specified by distinct finite automata. The approach is then applied to a new concept of subset seeds for which we propose an efficient automaton construction. Experimental results confirm that sensitive subset seeds can be efficiently designed using our approach, and can then be used in similarity search producing better results than ordinary spaced seeds.",
        "published": "2006-01-27T18:53:01Z",
        "link": "http://arxiv.org/abs/cs/0601116v2",
        "categories": [
            "cs.DS",
            "q-bio.QM"
        ]
    },
    {
        "title": "Finding Cliques of a Graph using Prime Numbers",
        "authors": [
            "Dhananjay D. Kulkarni",
            "Shekhar Verma",
            "Prashant"
        ],
        "summary": "This paper proposes a new algorithm for solving maximal cliques for simple undirected graphs using the theory of prime numbers. A novel approach using prime numbers is used to find cliques and ends with a discussion of the algorithm.",
        "published": "2006-01-27T20:11:14Z",
        "link": "http://arxiv.org/abs/cs/0601117v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Truly Online Paging with Locality of Reference",
        "authors": [
            "Amos Fiat",
            "Manor Mendel"
        ],
        "summary": "The competitive analysis fails to model locality of reference in the online paging problem. To deal with it, Borodin et. al. introduced the access graph model, which attempts to capture the locality of reference. However, the access graph model has a number of troubling aspects. The access graph has to be known in advance to the paging algorithm and the memory required to represent the access graph itself may be very large.   In this paper we present truly online strongly competitive paging algorithms in the access graph model that do not have any prior information on the access sequence. We present both deterministic and randomized algorithms. The algorithms need only O(k log n) bits of memory, where k is the number of page slots available and n is the size of the virtual address space. I.e., asymptotically no more memory than needed to store the virtual address translation table.   We also observe that our algorithms adapt themselves to temporal changes in the locality of reference. We model temporal changes in the locality of reference by extending the access graph model to the so called extended access graph model, in which many vertices of the graph can correspond to the same virtual page. We define a measure for the rate of change in the locality of reference in G denoted by Delta(G). We then show our algorithms remain strongly competitive as long as Delta(G) >= (1+ epsilon)k, and no truly online algorithm can be strongly competitive on a class of extended access graphs that includes all graphs G with Delta(G) >= k- o(k).",
        "published": "2006-01-30T20:58:23Z",
        "link": "http://arxiv.org/abs/cs/0601127v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Simulating Network Influence Algorithms Using Particle-Swarms: PageRank   and PageRank-Priors",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen"
        ],
        "summary": "A particle-swarm is a set of indivisible processing elements that traverse a network in order to perform a distributed function. This paper will describe a particular implementation of a particle-swarm that can simulate the behavior of the popular PageRank algorithm in both its {\\it global-rank} and {\\it relative-rank} incarnations. PageRank is compared against the particle-swarm method on artificially generated scale-free networks of 1,000 nodes constructed using a common gamma value, $\\gamma = 2.5$. The running time of the particle-swarm algorithm is $O(|P|+|P|t)$ where $|P|$ is the size of the particle population and $t$ is the number of particle propagation iterations. The particle-swarm method is shown to be useful due to its ease of extension and running time.",
        "published": "2006-01-31T23:24:42Z",
        "link": "http://arxiv.org/abs/cs/0602002v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The Matrix of Maximum Out Forests of a Digraph and Its Applications",
        "authors": [
            "Rafig Agaev",
            "Pavel Chebotarev"
        ],
        "summary": "We study the maximum out forests of a (weighted) digraph and the matrix of maximum out forests. A maximum out forest of a digraph G is a spanning subgraph of G that consists of disjoint diverging trees and has the maximum possible number of arcs. If a digraph contains any out arborescences, then maximum out forests coincide with them. We provide a new proof to the Markov chain tree theorem saying that the matrix of Ces`aro limiting probabilities of an arbitrary stationary finite Markov chain coincides with the normalized matrix of maximum out forests of the weighted digraph that corresponds to the Markov chain. We discuss the applications of the matrix of maximum out forests and its transposition, the matrix of limiting accessibilities of a digraph, to the problems of preference aggregation, measuring the vertex proximity, and uncovering the structure of a digraph.",
        "published": "2006-02-03T13:28:50Z",
        "link": "http://arxiv.org/abs/math/0602059v2",
        "categories": [
            "math.CO",
            "cs.DS",
            "math.AG",
            "05C50; 05C05; 15A51"
        ]
    },
    {
        "title": "On Proximity Measures for Graph Vertices",
        "authors": [
            "Pavel Chebotarev",
            "Elena Shamis"
        ],
        "summary": "We study the properties of several proximity measures for the vertices of weighted multigraphs and multidigraphs. Unlike the classical distance for the vertices of connected graphs, these proximity measures are applicable to weighted structures and take into account not only the shortest, but also all other connections, which is desirable in many applications. To apply these proximity measures to unweighted structures, every edge should be assigned the same weight which determines the proportion of taking account of two routes, from which one is one edge longer than the other. Among the proximity measures we consider path accessibility, route accessibility, relative forest accessibility along with its components, accessibility via dense forests, and connection reliability. A number of characteristic conditions is introduced and employed to characterize the proximity measures. A topological interpretation is obtained for the Moore-Penrose generalized inverse of the Laplacian matrix of a weighted multigraph.",
        "published": "2006-02-05T15:55:52Z",
        "link": "http://arxiv.org/abs/math/0602073v1",
        "categories": [
            "math.CO",
            "cs.DS",
            "cs.NI",
            "math.MG",
            "05C50; 05C05; 15A09; 15A51"
        ]
    },
    {
        "title": "Finding total unimodularity in optimization problems solved by linear   programs",
        "authors": [
            "Christoph Durr",
            "Mathilde Hurand"
        ],
        "summary": "A popular approach in combinatorial optimization is to model problems as integer linear programs. Ideally, the relaxed linear program would have only integer solutions, which happens for instance when the constraint matrix is totally unimodular. Still, sometimes it is possible to build an integer solution with the same cost from the fractional solution. Examples are two scheduling problems and the single disk prefetching/caching problem. We show that problems such as the three previously mentioned can be separated into two subproblems: (1) finding an optimal feasible set of slots, and (2) assigning the jobs or pages to the slots. It is straigthforward to show that the latter can be solved greedily. We are able to solve the former with a totally unimodular linear program, from which we obtain simple combinatorial algorithms with improved worst case running time.",
        "published": "2006-02-06T09:09:03Z",
        "link": "http://arxiv.org/abs/cs/0602016v3",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2"
        ]
    },
    {
        "title": "Approximate Weighted Farthest Neighbors and Minimum Dilation Stars",
        "authors": [
            "John Augustine",
            "David Eppstein",
            "Kevin A. Wortman"
        ],
        "summary": "We provide an efficient reduction from the problem of querying approximate multiplicatively weighted farthest neighbors in a metric space to the unweighted problem. Combining our techniques with core-sets for approximate unweighted farthest neighbors, we show how to find (1+epsilon)-approximate farthest neighbors in time O(log n) per query in D-dimensional Euclidean space for any constants D and epsilon. As an application, we find an O(n log n) expected time algorithm for choosing the center of a star topology network connecting a given set of points, so as to approximately minimize the maximum dilation between any pair of points.",
        "published": "2006-02-07T21:09:11Z",
        "link": "http://arxiv.org/abs/cs/0602029v1",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Why neighbor-joining works",
        "authors": [
            "Radu Mihaescu",
            "Dan Levy",
            "Lior Pachter"
        ],
        "summary": "We show that the neighbor-joining algorithm is a robust quartet method for constructing trees from distances. This leads to a new performance guarantee that contains Atteson's optimal radius bound as a special case and explains many cases where neighbor-joining is successful even when Atteson's criterion is not satisfied. We also provide a proof for Atteson's conjecture on the optimal edge radius of the neighbor-joining algorithm. The strong performance guarantees we provide also hold for the quadratic time fast neighbor-joining algorithm, thus providing a theoretical basis for inferring very large phylogenies with neighbor-joining.",
        "published": "2006-02-10T20:22:59Z",
        "link": "http://arxiv.org/abs/cs/0602041v3",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.0"
        ]
    },
    {
        "title": "The OverRelational Manifesto",
        "authors": [
            "Evgeniy Grigoriev"
        ],
        "summary": "The OverRelational Manifesto (below ORM) proposes a possible approach to creation of data storage systems of the next generation. ORM starts from the requirement that information in a relational database is represented by a set of relation values. Accordingly, it is assumed that the information about any entity of an enterprise must also be represented as a set of relation values (the ORM main requirement). A system of types is introduced, which allows one to fulfill the main requirement. The data are represented in the form of complex objects, and the state of any object is described as a set of relation values. Emphasize that the types describing the objects are encapsulated, inherited, and polymorphic. Then, it is shown that the data represented as a set of such objects may also be represented as a set of relational values defined on the set of scalar domains (dual data representation). In the general case, any class is associated with a set of relation variables (R-variables) each one containing some data about all objects of this class existing in the system. One of the key points is the fact that the usage of complex (from the user's viewpoint) refined names of R-variables and their attributes makes it possible to preserve the semantics of complex data structures represented in the form of a set of relation values. The most important part of the data storage system created on the approach proposed is an object-oriented translator operating over a relational DBMS. The expressiveness of such a system is comparable with that of OO programming languages.",
        "published": "2006-02-14T12:19:08Z",
        "link": "http://arxiv.org/abs/cs/0602052v3",
        "categories": [
            "cs.DB",
            "cs.DS",
            "D.3.3; E.2; F.3.3; F.4.1; H.2.1; H.2.3; H.2.4; H.3.3"
        ]
    },
    {
        "title": "How to Beat the Adaptive Multi-Armed Bandit",
        "authors": [
            "Varsha Dani",
            "Thomas P. Hayes"
        ],
        "summary": "The multi-armed bandit is a concise model for the problem of iterated decision-making under uncertainty. In each round, a gambler must pull one of $K$ arms of a slot machine, without any foreknowledge of their payouts, except that they are uniformly bounded. A standard objective is to minimize the gambler's regret, defined as the gambler's total payout minus the largest payout which would have been achieved by any fixed arm, in hindsight. Note that the gambler is only told the payout for the arm actually chosen, not for the unchosen arms.   Almost all previous work on this problem assumed the payouts to be non-adaptive, in the sense that the distribution of the payout of arm $j$ in round $i$ is completely independent of the choices made by the gambler on rounds $1, \\dots, i-1$. In the more general model of adaptive payouts, the payouts in round $i$ may depend arbitrarily on the history of past choices made by the algorithm.   We present a new algorithm for this problem, and prove nearly optimal guarantees for the regret against both non-adaptive and adaptive adversaries. After $T$ rounds, our algorithm has regret $O(\\sqrt{T})$ with high probability (the tail probability decays exponentially). This dependence on $T$ is best possible, and matches that of the full-information version of the problem, in which the gambler is told the payouts for all $K$ arms after each round.   Previously, even for non-adaptive payouts, the best high-probability bounds known were $O(T^{2/3})$, due to Auer, Cesa-Bianchi, Freund and Schapire. The expected regret of their algorithm is $O(T^{1/2}) for non-adaptive payouts, but as we show, $\\Omega(T^{2/3})$ for adaptive payouts.",
        "published": "2006-02-14T23:57:01Z",
        "link": "http://arxiv.org/abs/cs/0602053v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Plane Decompositions as Tools for Approximation",
        "authors": [
            "Melanie J. Agnew",
            "Christopher M. Homan"
        ],
        "summary": "Tree decompositions were developed by Robertson and Seymour. Since then algorithms have been developed to solve intractable problems efficiently for graphs of bounded treewidth. In this paper we extend tree decompositions to allow cycles to exist in the decomposition graph; we call these new decompositions plane decompositions because we require that the decomposition graph be planar. First, we give some background material about tree decompositions and an overview of algorithms both for decompositions and for approximations of planar graphs. Then, we give our plane decomposition definition and an algorithm that uses this decomposition to approximate the size of the maximum independent set of the underlying graph in polynomial time.",
        "published": "2006-02-15T19:09:39Z",
        "link": "http://arxiv.org/abs/cs/0602057v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Renyi to Renyi -- Source Coding under Siege",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "A novel lossless source coding paradigm applies to problems of unreliable lossless channels with low bit rates, in which a vital message needs to be transmitted prior to termination of communications. This paradigm can be applied to Alfred Renyi's secondhand account of an ancient siege in which a spy was sent to scout the enemy but was captured. After escaping, the spy returned to his base in no condition to speak and unable to write. His commander asked him questions that he could answer by nodding or shaking his head, and the fortress was defended with this information. Renyi told this story with reference to prefix coding, but maximizing probability of survival in the siege scenario is distinct from yet related to the traditional source coding objective of minimizing expected codeword length. Rather than finding a code minimizing expected codeword length $\\sum_{i=1}^n p(i) l(i)$, the siege problem involves maximizing $\\sum_{i=1}^n p(i) \\theta^{l(i)}$ for a known $\\theta \\in (0,1)$. When there are no restrictions on codewords, this problem can be solve using a known generalization of Huffman coding. The optimal solution has coding bounds which are functions of Renyi entropy; in addition to known bounds, new bounds are derived here. The alphabetically constrained version of this problem has applications in search trees and diagnostic testing. A novel dynamic programming algorithm -- based upon the oldest known algorithm for the traditional alphabetic problem -- optimizes this problem in $O(n^3)$ time and $O(n^2)$ space, whereas two novel approximation algorithms can find a suboptimal solution faster: one in linear time, the other in $O(n \\log n)$. Coding bounds for the alphabetic version of this problem are also presented.",
        "published": "2006-02-17T23:40:26Z",
        "link": "http://arxiv.org/abs/cs/0602067v2",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Faster Algorithms for Constructing a Concept (Galois) Lattice",
        "authors": [
            "Vicky Choi"
        ],
        "summary": "In this paper, we present a fast algorithm for constructing a concept (Galois) lattice of a binary relation, including computing all concepts and their lattice order. We also present two efficient variants of the algorithm, one for computing all concepts only, and one for constructing a frequent closed itemset lattice. The running time of our algorithms depends on the lattice structure and is faster than all other existing algorithms for these problems.",
        "published": "2006-02-19T19:47:56Z",
        "link": "http://arxiv.org/abs/cs/0602069v2",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "An O(n^{2.75}) algorithm for online topological ordering",
        "authors": [
            "Deepak Ajwani",
            "Tobias Friedrich",
            "Ulrich Meyer"
        ],
        "summary": "We present a simple algorithm which maintains the topological order of a directed acyclic graph with n nodes under an online edge insertion sequence in O(n^{2.75}) time, independent of the number of edges m inserted. For dense DAGs, this is an improvement over the previous best result of O(min(m^{3/2} log(n), m^{3/2} + n^2 log(n)) by Katriel and Bodlaender. We also provide an empirical comparison of our algorithm with other algorithms for online topological sorting. Our implementation outperforms them on certain hard instances while it is still competitive on random edge insertion sequences leading to complete DAGs.",
        "published": "2006-02-21T10:32:15Z",
        "link": "http://arxiv.org/abs/cs/0602073v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "SISO APP Searches in Lattices with Tanner Graphs",
        "authors": [
            "Dumitru Mihai Ionescu",
            "Haidong Zhu"
        ],
        "summary": "An efficient, low-complexity, soft-output detector for general lattices is presented, based on their Tanner graph (TG) representations. Closest-point searches in lattices can be performed as non-binary belief propagation on associated TGs; soft-information output is naturally generated in the process; the algorithm requires no backtrack (cf. classic sphere decoding), and extracts extrinsic information. A lattice's coding gain enables equivalence relations between lattice points, which can be thereby partitioned in cosets. Total and extrinsic a posteriori probabilities at the detector's output further enable the use of soft detection information in iterative schemes. The algorithm is illustrated via two scenarios that transmit a 32-point, uncoded super-orthogonal (SO) constellation for multiple-input multiple-output (MIMO) channels, carved from an 8-dimensional non-orthogonal lattice (a direct sum of two 4-dimensional checkerboard lattice): it achieves maximum likelihood performance in quasistatic fading; and, performs close to interference-free transmission, and identically to list sphere decoding, in independent fading with coordinate interleaving and iterative equalization and detection. Latter scenario outperforms former despite the absence of forward error correction coding---because the inherent lattice coding gain allows for the refining of extrinsic information. The lattice constellation is the same as the one employed in the SO space-time trellis codes first introduced for 2-by-2 MIMO by Ionescu et al., then independently by Jafarkhani and Seshadri. Complexity is log-linear in lattice dimensionality, vs. cubic in sphere decoders.",
        "published": "2006-02-22T03:28:46Z",
        "link": "http://arxiv.org/abs/cs/0602079v3",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Twenty (or so) Questions: $D$-ary Length-Bounded Prefix Coding",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Efficient optimal prefix coding has long been accomplished via the Huffman algorithm. However, there is still room for improvement and exploration regarding variants of the Huffman problem. Length-limited Huffman coding, useful for many practical applications, is one such variant, for which codes are restricted to the set of codes in which none of the $n$ codewords is longer than a given length, $l_{\\max}$. Binary length-limited coding can be done in $O(n l_{\\max})$ time and O(n) space via the widely used Package-Merge algorithm and with even smaller asymptotic complexity using a lesser-known algorithm. In this paper these algorithms are generalized without increasing complexity in order to introduce a minimum codeword length constraint $l_{\\min}$, to allow for objective functions other than the minimization of expected codeword length, and to be applicable to both binary and nonbinary codes; nonbinary codes were previously addressed using a slower dynamic programming approach. These extensions have various applications -- including fast decompression and a modified version of the game ``Twenty Questions'' -- and can be used to solve the problem of finding an optimal code with limited fringe, that is, finding the best code among codes with a maximum difference between the longest and shortest codewords. The previously proposed method for solving this problem was nonpolynomial time, whereas solving this using the novel linear-space algorithm requires only $O(n (l_{\\max}- l_{\\min})^2)$ time, or even less if $l_{\\max}- l_{\\min}$ is not $O(\\log n)$.",
        "published": "2006-02-25T19:09:11Z",
        "link": "http://arxiv.org/abs/cs/0602085v4",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "G.2.2; F.2; E.4; H.1.1"
        ]
    },
    {
        "title": "Improved Bounds and Schemes for the Declustering Problem",
        "authors": [
            "Benjamin Doerr",
            "Nils Hebbinghaus",
            "Sören Werth"
        ],
        "summary": "The declustering problem is to allocate given data on parallel working storage devices in such a manner that typical requests find their data evenly distributed on the devices. Using deep results from discrepancy theory, we improve previous work of several authors concerning range queries to higher-dimensional data. We give a declustering scheme with an additive error of $O_d(\\log^{d-1} M)$ independent of the data size, where $d$ is the dimension, $M$ the number of storage devices and $d-1$ does not exceed the smallest prime power in the canonical decomposition of $M$ into prime powers. In particular, our schemes work for arbitrary $M$ in dimensions two and three. For general $d$, they work for all $M\\geq d-1$ that are powers of two. Concerning lower bounds, we show that a recent proof of a $\\Omega_d(\\log^{\\frac{d-1}{2}} M)$ bound contains an error. We close the gap in the proof and thus establish the bound.",
        "published": "2006-03-02T15:51:09Z",
        "link": "http://arxiv.org/abs/cs/0603012v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2; E.1"
        ]
    },
    {
        "title": "The Snowblower Problem",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Joseph S. B. Mitchell",
            "Valentin Polishchuk"
        ],
        "summary": "We introduce the snowblower problem (SBP), a new optimization problem that is closely related to milling problems and to some material-handling problems. The objective in the SBP is to compute a short tour for the snowblower to follow to remove all the snow from a domain (driveway, sidewalk, etc.). When a snowblower passes over each region along the tour, it displaces snow into a nearby region. The constraint is that if the snow is piled too high, then the snowblower cannot clear the pile.   We give an algorithmic study of the SBP. We show that in general, the problem is NP-complete, and we present polynomial-time approximation algorithms for removing snow under various assumptions about the operation of the snowblower. Most commercially-available snowblowers allow the user to control the direction in which the snow is thrown. We differentiate between the cases in which the snow can be thrown in any direction, in any direction except backwards, and only to the right. For all cases, we give constant-factor approximation algorithms; the constants increase as the throw direction becomes more restricted.   Our results are also applicable to robotic vacuuming (or lawnmowing) with bounded capacity dust bin and to some versions of material-handling problems, in which the goal is to rearrange cartons on the floor of a warehouse.",
        "published": "2006-03-07T20:35:48Z",
        "link": "http://arxiv.org/abs/cs/0603026v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.RO",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Fast matrix multiplication is stable",
        "authors": [
            "James Demmel",
            "Ioana Dumitriu",
            "Olga Holtz",
            "Robert Kleinberg"
        ],
        "summary": "We perform forward error analysis for a large class of recursive matrix multiplication algorithms in the spirit of [D. Bini and G. Lotti, Stability of fast algorithms for matrix multiplication, Numer. Math. 36 (1980), 63--72]. As a consequence of our analysis, we show that the exponent of matrix multiplication (the optimal running time) can be achieved by numerically stable algorithms. We also show that new group-theoretic algorithms proposed in [H. Cohn, and C. Umans, A group-theoretic approach to fast matrix multiplication, FOCS 2003, 438--449] and [H. Cohn, R. Kleinberg, B. Szegedy and C. Umans, Group-theoretic algorithms for matrix multiplication, FOCS 2005, 379--388] are all included in the class of algorithms to which our analysis applies, and are therefore numerically stable. We perform detailed error analysis for three specific fast group-theoretic algorithms.",
        "published": "2006-03-09T04:34:36Z",
        "link": "http://arxiv.org/abs/math/0603207v3",
        "categories": [
            "math.NA",
            "cs.CC",
            "cs.DS",
            "math.GR",
            "65Y20, 65F30, 65G50, 68Q17, 68W40, 20C05, 20K01, 16S34, 43A30, 65T50"
        ]
    },
    {
        "title": "Time-Space Trade-Offs for Predecessor Search",
        "authors": [
            "Mihai Patrascu",
            "Mikkel Thorup"
        ],
        "summary": "We develop a new technique for proving cell-probe lower bounds for static data structures. Previous lower bounds used a reduction to communication games, which was known not to be tight by counting arguments. We give the first lower bound for an explicit problem which breaks this communication complexity barrier. In addition, our bounds give the first separation between polynomial and near linear space. Such a separation is inherently impossible by communication complexity.   Using our lower bound technique and new upper bound constructions, we obtain tight bounds for searching predecessors among a static set of integers. Given a set Y of n integers of l bits each, the goal is to efficiently find predecessor(x) = max{y in Y | y <= x}, by representing Y on a RAM using space S.   In external memory, it follows that the optimal strategy is to use either standard B-trees, or a RAM algorithm ignoring the larger block size. In the important case of l = c*lg n, for c>1 (i.e. polynomial universes), and near linear space (such as S = n*poly(lg n)), the optimal search time is Theta(lg l). Thus, our lower bound implies the surprising conclusion that van Emde Boas' classic data structure from [FOCS'75] is optimal in this case. Note that for space n^{1+eps}, a running time of O(lg l / lglg l) was given by Beame and Fich [STOC'99].",
        "published": "2006-03-10T14:50:20Z",
        "link": "http://arxiv.org/abs/cs/0603043v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.3; E.2"
        ]
    },
    {
        "title": "Homogeneity vs. Adjacency: generalising some graph decomposition   algorithms",
        "authors": [
            "Binh Minh Bui Xuan",
            "Michel Habib",
            "Vincent Limouzy",
            "Fabien De Montgolfier"
        ],
        "summary": "In this paper, a new general decomposition theory inspired from modular graph decomposition is presented. Our main result shows that, within this general theory, most of the nice algorithmic tools developed for modular decomposition are still efficient. This theory not only unifies the usual modular decomposition generalisations such as modular decomposition of directed graphs or decomposition of 2-structures, but also star cutsets and bimodular decomposition. Our general framework provides a decomposition algorithm which improves the best known algorithms for bimodular decomposition.",
        "published": "2006-03-13T09:48:49Z",
        "link": "http://arxiv.org/abs/cs/0603048v1",
        "categories": [
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "Multiple serial episode matching",
        "authors": [
            "Patrick Cegielski",
            "Irene Guessarian",
            "Yuri Matiyasevich"
        ],
        "summary": "In a previous paper we generalized the Knuth-Morris-Pratt (KMP) pattern matching algorithm and defined a non-conventional kind of RAM, the MP--RAMs (RAMS equipped with extra operations), and designed an O(n) on-line algorithm for solving the serial episode matching problem on MP--RAMs when there is only one single episode. We here give two extensions of this algorithm to the case when we search for several patterns simultaneously and compare them. More preciseley, given $q+1$ strings (a text $t$ of length $n$ and $q$ patterns $m\\_1,...,m\\_q$) and a natural number $w$, the {\\em multiple serial episode matching problem} consists in finding the number of size $w$ windows of text $t$ which contain patterns $m\\_1,...,m\\_q$ as subsequences, i.e. for each $m\\_i$, if $m\\_i=p\\_1,..., p\\_k$, the letters $p\\_1,..., p\\_k$ occur in the window, in the same order as in $m\\_i$, but not necessarily consecutively (they may be interleaved with other letters).} The main contribution is an algorithm solving this problem on-line in time $O(nq)$.",
        "published": "2006-03-13T11:03:34Z",
        "link": "http://arxiv.org/abs/cs/0603050v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Automatic generation of simplified weakest preconditions for integrity   constraint verification",
        "authors": [
            "A. Ai T -Bouziad",
            "Irene Guessarian",
            "L. Vieille"
        ],
        "summary": "Given a constraint $c$ assumed to hold on a database $B$ and an update $u$ to be performed on $B$, we address the following question: will $c$ still hold after $u$ is performed? When $B$ is a relational database, we define a confluent terminating rewriting system which, starting from $c$ and $u$, automatically derives a simplified weakest precondition $wp(c,u)$ such that, whenever $B$ satisfies $wp(c,u)$, then the updated database $u(B)$ will satisfy $c$, and moreover $wp(c,u)$ is simplified in the sense that its computation depends only upon the instances of $c$ that may be modified by the update. We then extend the definition of a simplified $wp(c,u)$ to the case of deductive databases; we prove it using fixpoint induction.",
        "published": "2006-03-14T14:30:10Z",
        "link": "http://arxiv.org/abs/cs/0603053v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Packrat Parsing: Simple, Powerful, Lazy, Linear Time",
        "authors": [
            "Bryan Ford"
        ],
        "summary": "Packrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(k) or LR(k) grammar can be recognized by a packrat parser, in addition to many languages that conventional linear-time algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a linear-time packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives.",
        "published": "2006-03-18T17:49:45Z",
        "link": "http://arxiv.org/abs/cs/0603077v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.PL",
            "D.3.4; D.1.1; F.4.2"
        ]
    },
    {
        "title": "Random 3CNF formulas elude the Lovasz theta function",
        "authors": [
            "Uriel Feige",
            "Eran Ofek"
        ],
        "summary": "Let $\\phi$ be a 3CNF formula with n variables and m clauses. A simple nonconstructive argument shows that when m is sufficiently large compared to n, most 3CNF formulas are not satisfiable. It is an open question whether there is an efficient refutation algorithm that for most such formulas proves that they are not satisfiable. A possible approach to refute a formula $\\phi$ is: first, translate it into a graph $G_{\\phi}$ using a generic reduction from 3-SAT to max-IS, then bound the maximum independent set of $G_{\\phi}$ using the Lovasz $\\vartheta$ function. If the $\\vartheta$ function returns a value $< m$, this is a certificate for the unsatisfiability of $\\phi$. We show that for random formulas with $m < n^{3/2 -o(1)}$ clauses, the above approach fails, i.e. the $\\vartheta$ function is likely to return a value of m.",
        "published": "2006-03-22T10:30:36Z",
        "link": "http://arxiv.org/abs/cs/0603084v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "cs.LO"
        ]
    },
    {
        "title": "Convex Separation from Optimization via Heuristics",
        "authors": [
            "Lawrence M. Ioannou",
            "Benjamin C. Travaglione",
            "Donny Cheung"
        ],
        "summary": "Let $K$ be a full-dimensional convex subset of $\\mathbb{R}^n$. We describe a new polynomial-time Turing reduction from the weak separation problem for $K$ to the weak optimization problem for $K$ that is based on a geometric heuristic. We compare our reduction, which relies on analytic centers, with the standard, more general reduction.",
        "published": "2006-03-22T19:46:58Z",
        "link": "http://arxiv.org/abs/cs/0603089v1",
        "categories": [
            "cs.DS",
            "math.OC"
        ]
    },
    {
        "title": "Complexity of Monadic inf-datalog. Application to temporal logic",
        "authors": [
            "Eugénie Foustoucos",
            "Irene Guessarian"
        ],
        "summary": "In [11] we defined Inf-Datalog and characterized the fragments of Monadic inf-Datalog that have the same expressive power as Modal Logic (resp. $CTL$, alternation-free Modal $\\mu$-calculus and Modal $\\mu$-calculus). We study here the time and space complexity of evaluation of Monadic inf-Datalog programs on finite models. We deduce a new unified proof that model checking has 1. linear data and program complexities (both in time and space) for $CTL$ and alternation-free Modal $\\mu$-calculus, and 2. linear-space (data and program) complexities, linear-time program complexity and polynomial-time data complexity for $L\\mu\\_k$ (Modal $\\mu$-calculus with fixed alternation-depth at most $k$).}",
        "published": "2006-03-30T15:25:11Z",
        "link": "http://arxiv.org/abs/cs/0603122v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Minimum-Cost Coverage of Point Sets by Disks",
        "authors": [
            "Esther M. Arkin",
            "Herve Broennimann",
            "Jeff Erickson",
            "Sandor P. Fekete",
            "Christian Knauer",
            "Jonathan Lenchner",
            "Joseph S. B. Mitchell",
            "Kim Whittlesey"
        ],
        "summary": "We consider a class of geometric facility location problems in which the goal is to determine a set X of disks given by their centers (t_j) and radii (r_j) that cover a given set of demand points Y in the plane at the smallest possible cost. We consider cost functions of the form sum_j f(r_j), where f(r)=r^alpha is the cost of transmission to radius r. Special cases arise for alpha=1 (sum of radii) and alpha=2 (total area); power consumption models in wireless network design often use an exponent alpha>2. Different scenarios arise according to possible restrictions on the transmission centers t_j, which may be constrained to belong to a given discrete set or to lie on a line, etc. We obtain several new results, including (a) exact and approximation algorithms for selecting transmission points t_j on a given line in order to cover demand points Y in the plane; (b) approximation algorithms (and an algebraic intractability result) for selecting an optimal line on which to place transmission points to cover Y; (c) a proof of NP-hardness for a discrete set of transmission points in the plane and any fixed alpha>1; and (d) a polynomial-time approximation scheme for the problem of computing a minimum cost covering tour (MCCT), in which the total cost is a linear combination of the transmission cost for the set of disks and the length of a tour/path that connects the centers of the disks.",
        "published": "2006-04-04T17:24:09Z",
        "link": "http://arxiv.org/abs/cs/0604008v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "On Conditional Branches in Optimal Search Trees",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Algorithms for efficiently finding optimal alphabetic decision trees -- such as the Hu-Tucker algorithm -- are well established and commonly used. However, such algorithms generally assume that the cost per decision is uniform and thus independent of the outcome of the decision. The few algorithms without this assumption instead use one cost if the decision outcome is ``less than'' and another cost otherwise. In practice, neither assumption is accurate for software optimized for today's microprocessors. Such software generally has one cost for the more likely decision outcome and a greater cost -- often far greater -- for the less likely decision outcome. This problem and generalizations thereof are thus applicable to hard coding static decision tree instances in software, e.g., for optimizing program bottlenecks or for compiling switch statements. An O(n^3)-time O(n^2)-space dynamic programming algorithm can solve this optimal binary decision tree problem, and this approach has many generalizations that optimize for the behavior of processors with predictive branch capabilities, both static and dynamic. Solutions to this formulation are often faster in practice than ``optimal'' decision trees as formulated in the literature. Different search paradigms can sometimes yield even better performance.",
        "published": "2006-04-06T00:54:44Z",
        "link": "http://arxiv.org/abs/cs/0604016v2",
        "categories": [
            "cs.PF",
            "cs.DS",
            "cs.IR",
            "B.1.4; C.0; C.1.1; D.3.4; E.1; F.2.2; G.3; H.3.3; I.2.8"
        ]
    },
    {
        "title": "Approximation Algorithms for Restricted Cycle Covers Based on Cycle   Decompositions",
        "authors": [
            "Bodo Manthey"
        ],
        "summary": "A cycle cover of a graph is a set of cycles such that every vertex is part of exactly one cycle. An L-cycle cover is a cycle cover in which the length of every cycle is in the set L. The weight of a cycle cover of an edge-weighted graph is the sum of the weights of its edges.   We come close to settling the complexity and approximability of computing L-cycle covers. On the one hand, we show that for almost all L, computing L-cycle covers of maximum weight in directed and undirected graphs is APX-hard and NP-hard. Most of our hardness results hold even if the edge weights are restricted to zero and one.   On the other hand, we show that the problem of computing L-cycle covers of maximum weight can be approximated within a factor of 2 for undirected graphs and within a factor of 8/3 in the case of directed graphs. This holds for arbitrary sets L.",
        "published": "2006-04-06T13:53:25Z",
        "link": "http://arxiv.org/abs/cs/0604020v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "An O(n^3)-Time Algorithm for Tree Edit Distance",
        "authors": [
            "Erik D. Demaine",
            "Shay Mozes",
            "Benjamin Rossman",
            "Oren Weimann"
        ],
        "summary": "The {\\em edit distance} between two ordered trees with vertex labels is the minimum cost of transforming one tree into the other by a sequence of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. In this paper, we present a worst-case $O(n^3)$-time algorithm for this problem, improving the previous best $O(n^3\\log n)$-time algorithm~\\cite{Klein}. Our result requires a novel adaptive strategy for deciding how a dynamic program divides into subproblems (which is interesting in its own right), together with a deeper understanding of the previous algorithms for the problem. We also prove the optimality of our algorithm among the family of \\emph{decomposition strategy} algorithms--which also includes the previous fastest algorithms--by tightening the known lower bound of $\\Omega(n^2\\log^2 n)$~\\cite{Touzet} to $\\Omega(n^3)$, matching our algorithm's running time. Furthermore, we obtain matching upper and lower bounds of $\\Theta(n m^2 (1 + \\log \\frac{n}{m}))$ when the two trees have different sizes $m$ and~$n$, where $m < n$.",
        "published": "2006-04-10T00:39:11Z",
        "link": "http://arxiv.org/abs/cs/0604037v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "An exact algorithm for higher-dimensional orthogonal packing",
        "authors": [
            "Sandor P. Fekete",
            "Joerg Schepers",
            "Jan C. van der Veen"
        ],
        "summary": "Higher-dimensional orthogonal packing problems have a wide range of practical applications, including packing, cutting, and scheduling. Combining the use of our data structure for characterizing feasible packings with our new classes of lower bounds, and other heuristics, we develop a two-level tree search algorithm for solving higher-dimensional packing problems to optimality. Computational results are reported, including optimal solutions for all two--dimensional test problems from recent literature.   This is the third in a series of articles describing new approaches to higher-dimensional packing; see cs.DS/0310032 and cs.DS/0402044.",
        "published": "2006-04-11T13:55:03Z",
        "link": "http://arxiv.org/abs/cs/0604045v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Structural Alignments of pseudo-knotted RNA-molecules in polynomial time",
        "authors": [
            "Michael Brinkmeier"
        ],
        "summary": "An RNA molecule is structured on several layers. The primary and most obvious structure is its sequence of bases, i.e. a word over the alphabet {A,C,G,U}. The higher structure is a set of one-to-one base-pairings resulting in a two-dimensional folding of the one-dimensional sequence. One speaks of a secondary structure if these pairings do not cross and of a tertiary structure otherwise.   Since the folding of the molecule is important for its function, the search for related RNA molecules should not only be restricted to the primary structure. It seems sensible to incorporate the higher structures in the search. Based on this assumption and certain edit-operations a distance between two arbitrary structures can be defined. It is known that the general calculation of this measure is NP-complete \\cite{zhang02similarity}. But for some special cases polynomial algorithms are known. Using a new formal description of secondary and tertiary structures, we extend the class of structures for which the distance can be calculated in polynomial time. In addition the presented algorithm may be used to approximate the edit-distance between two arbitrary structures with a constant ratio.",
        "published": "2006-04-12T06:46:04Z",
        "link": "http://arxiv.org/abs/cs/0604051v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Beyond Hirsch Conjecture: walks on random polytopes and smoothed   complexity of the simplex method",
        "authors": [
            "Roman Vershynin"
        ],
        "summary": "The smoothed analysis of algorithms is concerned with the expected running time of an algorithm under slight random perturbations of arbitrary inputs. Spielman and Teng proved that the shadow-vertex simplex method has polynomial smoothed complexity. On a slight random perturbation of an arbitrary linear program, the simplex method finds the solution after a walk on polytope(s) with expected length polynomial in the number of constraints n, the number of variables d and the inverse standard deviation of the perturbation 1/sigma.   We show that the length of walk in the simplex method is actually polylogarithmic in the number of constraints n. Spielman-Teng's bound on the walk was O(n^{86} d^{55} sigma^{-30}), up to logarithmic factors. We improve this to O(log^7 n (d^9 + d^3 \\s^{-4})). This shows that the tight Hirsch conjecture n-d on the length of walk on polytopes is not a limitation for the smoothed Linear Programming. Random perturbations create short paths between vertices.   We propose a randomized phase-I for solving arbitrary linear programs, which is of independent interest. Instead of finding a vertex of a feasible set, we add a vertex at random to the feasible set. This does not affect the solution of the linear program with constant probability. This overcomes one of the major difficulties of smoothed analysis of the simplex method -- one can now statistically decouple the walk from the smoothed linear program. This yields a much better reduction of the smoothed complexity to a geometric quantity -- the size of planar sections of random polytopes. We also improve upon the known estimates for that size, showing that it is polylogarithmic in the number of vertices.",
        "published": "2006-04-12T22:36:59Z",
        "link": "http://arxiv.org/abs/cs/0604055v3",
        "categories": [
            "cs.DS",
            "math.FA",
            "G.1.6"
        ]
    },
    {
        "title": "Solving Classical String Problems on Compressed Texts",
        "authors": [
            "Yury Lifshits"
        ],
        "summary": "Here we study the complexity of string problems as a function of the size of a program that generates input. We consider straight-line programs (SLP), since all algorithms on SLP-generated strings could be applied to processing LZ-compressed texts.   The main result is a new algorithm for pattern matching when both a text T and a pattern P are presented by SLPs (so-called fully compressed pattern matching problem). We show how to find a first occurrence, count all occurrences, check whether any given position is an occurrence or not in time O(n^2m). Here m,n are the sizes of straight-line programs generating correspondingly P and T.   Then we present polynomial algorithms for computing fingerprint table and compressed representation of all covers (for the first time) and for finding periods of a given compressed string (our algorithm is faster than previously known). On the other hand, we show that computing the Hamming distance between two SLP-generated strings is NP- and coNP-hard.",
        "published": "2006-04-13T08:12:39Z",
        "link": "http://arxiv.org/abs/cs/0604058v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "E.4; F.2.2; I.7"
        ]
    },
    {
        "title": "On the reduction of a random basis",
        "authors": [
            "Ali Akhavi",
            "Jean-François Marckert",
            "Alain Rouault"
        ],
        "summary": "For $g < n$, let $b\\_1,...,b\\_{n-g}$ be $n - g$ independent vectors in $\\mathbb{R}^n$ with a common distribution invariant by rotation. Considering these vectors as a basis for the Euclidean lattice they generate, the aim of this paper is to provide asymptotic results when $n\\to +\\infty$ concerning the property that such a random basis is reduced in the sense of {\\sc Lenstra, Lenstra & Lov\\'asz}. The proof passes by the study of the process $(r\\_{g+1}^{(n)},r\\_{g+2}^{(n)},...,r\\_{n-1}^{(n)})$ where $r\\_j^{(n)}$ is the ratio of lengths of two consecutive vectors $b^*\\_{n-j+1}$ and $b^*\\_{n-j}$ built from $(b\\_1,...,b\\_{n-g})$ by the Gram--Schmidt orthogonalization procedure, which we believe to be interesting in its own. We show that, as $n\\to+\\infty$, the process $(r\\_j^{(n)}-1)\\_j$ tends in distribution in some sense to an explicit process $({\\mathcal R}\\_j -1)\\_j$; some properties of this latter are provided.",
        "published": "2006-04-14T07:57:50Z",
        "link": "http://arxiv.org/abs/math/0604331v1",
        "categories": [
            "math.PR",
            "cs.DS"
        ]
    },
    {
        "title": "Unifying two Graph Decompositions with Modular Decomposition",
        "authors": [
            "Binh-Minh Bui-Xuan",
            "Michel Habib",
            "Vincent Limouzy",
            "Fabien De Montgolfier"
        ],
        "summary": "We introduces the umodules, a generalisation of the notion of graph module. The theory we develop captures among others undirected graphs, tournaments, digraphs, and $2-$structures. We show that, under some axioms, a unique decomposition tree exists for umodules. Polynomial-time algorithms are provided for: non-trivial umodule test, maximal umodule computation, and decomposition tree computation when the tree exists. Our results unify many known decomposition like modular and bi-join decomposition of graphs, and a new decomposition of tournaments.",
        "published": "2006-04-16T19:41:38Z",
        "link": "http://arxiv.org/abs/cs/0604065v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Network Delay Inference from Additive Metrics",
        "authors": [
            "Shankar Bhamidi",
            "Ram Rajagopal",
            "Sebastien Roch"
        ],
        "summary": "We demonstrate the use of computational phylogenetic techniques to solve a central problem in inferential network monitoring. More precisely, we design a novel algorithm for multicast-based delay inference, i.e. the problem of reconstructing the topology and delay characteristics of a network from end-to-end delay measurements on network paths. Our inference algorithm is based on additive metric techniques widely used in phylogenetics. It runs in polynomial time and requires a sample of size only $\\poly(\\log n)$.",
        "published": "2006-04-17T11:12:31Z",
        "link": "http://arxiv.org/abs/math/0604367v2",
        "categories": [
            "math.PR",
            "cs.DS",
            "cs.NI",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Unbiased Matrix Rounding",
        "authors": [
            "Benjamin Doerr",
            "Tobias Friedrich",
            "Christian Klein",
            "Ralf Osbild"
        ],
        "summary": "We show several ways to round a real matrix to an integer one such that the rounding errors in all rows and columns as well as the whole matrix are less than one. This is a classical problem with applications in many fields, in particular, statistics.   We improve earlier solutions of different authors in two ways. For rounding matrices of size $m \\times n$, we reduce the runtime from $O((m n)^2 Second, our roundings also have a rounding error of less than one in all initial intervals of rows and columns. Consequently, arbitrary intervals have an error of at most two. This is particularly useful in the statistics application of controlled rounding.   The same result can be obtained via (dependent) randomized rounding. This has the additional advantage that the rounding is unbiased, that is, for all entries $y_{ij}$ of our rounding, we have $E(y_{ij}) = x_{ij}$, where $x_{ij}$ is the corresponding entry of the input matrix.",
        "published": "2006-04-18T15:08:37Z",
        "link": "http://arxiv.org/abs/cs/0604068v2",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Fixed-Parameter Complexity of Minimum Profile Problems",
        "authors": [
            "Gregory Gutin",
            "Stefan Szeider",
            "Anders Yeo"
        ],
        "summary": "Let $G=(V,E)$ be a graph. An ordering of $G$ is a bijection $\\alpha: V\\dom \\{1,2,..., |V|\\}.$ For a vertex $v$ in $G$, its closed neighborhood is $N[v]=\\{u\\in V: uv\\in E\\}\\cup \\{v\\}.$ The profile of an ordering $\\alpha$ of $G$ is $\\prf_{\\alpha}(G)=\\sum_{v\\in V}(\\alpha(v)-\\min\\{\\alpha(u): u\\in N[v]\\}).$ The profile $\\prf(G)$ of $G$ is the minimum of $\\prf_{\\alpha}(G)$ over all orderings $\\alpha$ of $G$. It is well-known that $\\prf(G)$ is the minimum number of edges in an interval graph $H$ that contains $G$ is a subgraph. Since $|V|-1$ is a tight lower bound for the profile of connected graphs $G=(V,E)$, the parametrization above the guaranteed value $|V|-1$ is of particular interest. We show that deciding whether the profile of a connected graph $G=(V,E)$ is at most $|V|-1+k$ is fixed-parameter tractable with respect to the parameter $k$. We achieve this result by reduction to a problem kernel of linear size.",
        "published": "2006-04-24T17:30:16Z",
        "link": "http://arxiv.org/abs/cs/0604095v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Approximation algorithms for wavelet transform coding of data streams",
        "authors": [
            "Sudipto Guha",
            "Boulos Harb"
        ],
        "summary": "This paper addresses the problem of finding a B-term wavelet representation of a given discrete function $f \\in \\real^n$ whose distance from f is minimized. The problem is well understood when we seek to minimize the Euclidean distance between f and its representation. The first known algorithms for finding provably approximate representations minimizing general $\\ell_p$ distances (including $\\ell_\\infty$) under a wide variety of compactly supported wavelet bases are presented in this paper. For the Haar basis, a polynomial time approximation scheme is demonstrated. These algorithms are applicable in the one-pass sublinear-space data stream model of computation. They generalize naturally to multiple dimensions and weighted norms. A universal representation that provides a provable approximation guarantee under all p-norms simultaneously; and the first approximation algorithms for bit-budget versions of the problem, known as adaptive quantization, are also presented. Further, it is shown that the algorithms presented here can be used to select a basis from a tree-structured dictionary of bases and find a B-term representation of the given function that provably approximates its best dictionary-basis representation.",
        "published": "2006-04-25T01:27:37Z",
        "link": "http://arxiv.org/abs/cs/0604097v4",
        "categories": [
            "cs.DS",
            "G.1.2"
        ]
    },
    {
        "title": "An Algebraic View of the Relation between Largest Common Subtrees and   Smallest Common Supertrees",
        "authors": [
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "The relationship between two important problems in tree pattern matching, the largest common subtree and the smallest common supertree problems, is established by means of simple constructions, which allow one to obtain a largest common subtree of two trees from a smallest common supertree of them, and vice versa. These constructions are the same for isomorphic, homeomorphic, topological, and minor embeddings, they take only time linear in the size of the trees, and they turn out to have a clear algebraic meaning.",
        "published": "2006-04-27T10:32:43Z",
        "link": "http://arxiv.org/abs/cs/0604108v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "math.CT",
            "G.2.3"
        ]
    },
    {
        "title": "A Hybrid Quantum Encoding Algorithm of Vector Quantization for Image   Compression",
        "authors": [
            "Chao-Yang Pang",
            "Zheng-Wei Zhou",
            "Guang-Can Guo"
        ],
        "summary": "Many classical encoding algorithms of Vector Quantization (VQ) of image compression that can obtain global optimal solution have computational complexity O(N). A pure quantum VQ encoding algorithm with probability of success near 100% has been proposed, that performs operations 45sqrt(N) times approximately. In this paper, a hybrid quantum VQ encoding algorithm between classical method and quantum algorithm is presented. The number of its operations is less than sqrt(N) for most images, and it is more efficient than the pure quantum algorithm.   Key Words: Vector Quantization, Grover's Algorithm, Image Compression, Quantum Algorithm",
        "published": "2006-04-30T13:35:54Z",
        "link": "http://arxiv.org/abs/cs/0605002v3",
        "categories": [
            "cs.MM",
            "cs.DS",
            "H.5.1; F.2.1; F.2.2; F.1.2"
        ]
    },
    {
        "title": "Geometric representation of graphs in low dimension",
        "authors": [
            "L. Sunil Chandran",
            "Mathew C Francis",
            "Naveen Sivadasan"
        ],
        "summary": "We give an efficient randomized algorithm to construct a box representation of any graph G on n vertices in $1.5 (\\Delta + 2) \\ln n$ dimensions, where $\\Delta$ is the maximum degree of G. We also show that $\\boxi(G) \\le (\\Delta + 2) \\ln n$ for any graph G. Our bound is tight up to a factor of $\\ln n$. We also show that our randomized algorithm can be derandomized to get a polynomial time deterministic algorithm. Though our general upper bound is in terms of maximum degree $\\Delta$, we show that for almost all graphs on n vertices, its boxicity is upper bound by $c\\cdot(d_{av} + 1) \\ln n$ where d_{av} is the average degree and c is a small constant. Also, we show that for any graph G, $\\boxi(G) \\le \\sqrt{8 n d_{av} \\ln n}$, which is tight up to a factor of $b \\sqrt{\\ln n}$ for a constant b.",
        "published": "2006-05-04T16:53:29Z",
        "link": "http://arxiv.org/abs/cs/0605013v2",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Message passing for vertex covers",
        "authors": [
            "Martin Weigt",
            "Haijun Zhou"
        ],
        "summary": "Constructing a minimal vertex cover of a graph can be seen as a prototype for a combinatorial optimization problem under hard constraints. In this paper, we develop and analyze message passing techniques, namely warning and survey propagation, which serve as efficient heuristic algorithms for solving these computational hard problems. We show also, how previously obtained results on the typical-case behavior of vertex covers of random graphs can be recovered starting from the message passing equations, and how they can be extended.",
        "published": "2006-05-08T12:30:35Z",
        "link": "http://arxiv.org/abs/cond-mat/0605190v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS"
        ]
    },
    {
        "title": "A Polynomial Time Nilpotence Test for Galois Groups and Related Results",
        "authors": [
            "V. Arvind",
            "Piyush P Kurur"
        ],
        "summary": "We give a deterministic polynomial-time algorithm to check whether the Galois group $\\Gal{f}$ of an input polynomial $f(X) \\in \\Q[X]$ is nilpotent: the running time is polynomial in $\\size{f}$. Also, we generalize the Landau-Miller solvability test to an algorithm that tests if $\\Gal{f}$ is in $\\Gamma_d$: this algorithm runs in time polynomial in $\\size{f}$ and $n^d$ and, moreover, if $\\Gal{f}\\in\\Gamma_d$ it computes all the prime factors of $# \\Gal{f}$.",
        "published": "2006-05-11T08:20:44Z",
        "link": "http://arxiv.org/abs/cs/0605050v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "An algebraic approach to Polya processes",
        "authors": [
            "Nicolas Pouyanne"
        ],
        "summary": "P\\'olya processes are natural generalization of P\\'olya-Eggenberger urn models. This article presents a new approach of their asymptotic behaviour {\\it via} moments, based on the spectral decomposition of a suitable finite difference operator on polynomial functions. Especially, it provides new results for {\\it large} processes (a P\\'olya process is called {\\it small} when 1 is simple eigenvalue of its replacement matrix and when any other eigenvalue has a real part $\\leq 1/2$; otherwise, it is called large).",
        "published": "2006-05-17T11:52:11Z",
        "link": "http://arxiv.org/abs/math/0605472v2",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "math.PR",
            "05D40; 60F15; 60F25; 60J05"
        ]
    },
    {
        "title": "The Complexity of Mean Flow Time Scheduling Problems with Release Times",
        "authors": [
            "Philippe Baptiste",
            "Peter Brucker",
            "Marek Chrobak",
            "Christoph Durr",
            "Svetlana A. Kravchenko",
            "Francis Sourd"
        ],
        "summary": "We study the problem of preemptive scheduling n jobs with given release times on m identical parallel machines. The objective is to minimize the average flow time. We show that when all jobs have equal processing times then the problem can be solved in polynomial time using linear programming. Our algorithm can also be applied to the open-shop problem with release times and unit processing times. For the general case (when processing times are arbitrary), we show that the problem is unary NP-hard.",
        "published": "2006-05-17T22:07:17Z",
        "link": "http://arxiv.org/abs/cs/0605078v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Alphabetic Coding with Exponential Costs",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "An alphabetic binary tree formulation applies to problems in which an outcome needs to be determined via alphabetically ordered search prior to the termination of some window of opportunity. Rather than finding a decision tree minimizing $\\sum_{i=1}^n w(i) l(i)$, this variant involves minimizing $\\log_a \\sum_{i=1}^n w(i) a^{l(i)}$ for a given $a \\in (0,1)$. This note introduces a dynamic programming algorithm that finds the optimal solution in polynomial time and space, and shows that methods traditionally used to improve the speed of optimizations in related problems, such as the Hu-Tucker procedure, fail for this problem. This note thus also introduces two approximation algorithms which can find a suboptimal solution in linear time (for one) or $\\order(n \\log n)$ time (for the other), with associated coding redundancy bounds.",
        "published": "2006-05-23T19:55:47Z",
        "link": "http://arxiv.org/abs/cs/0605099v3",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1; I.2.8"
        ]
    },
    {
        "title": "Restricted Strip Covering and the Sensor Cover Problem",
        "authors": [
            "Adam L. Buchsbaum",
            "Alon Efrat",
            "Shaili Jain",
            "Suresh Venkatasubramanian",
            "Ke Yi"
        ],
        "summary": "Given a set of objects with durations (jobs) that cover a base region, can we schedule the jobs to maximize the duration the original region remains covered? We call this problem the sensor cover problem. This problem arises in the context of covering a region with sensors. For example, suppose you wish to monitor activity along a fence by sensors placed at various fixed locations. Each sensor has a range and limited battery life. The problem is to schedule when to turn on the sensors so that the fence is fully monitored for as long as possible. This one dimensional problem involves intervals on the real line. Associating a duration to each yields a set of rectangles in space and time, each specified by a pair of fixed horizontal endpoints and a height. The objective is to assign a position to each rectangle to maximize the height at which the spanning interval is fully covered. We call this one dimensional problem restricted strip covering. If we replace the covering constraint by a packing constraint, the problem is identical to dynamic storage allocation, a scheduling problem that is a restricted case of the strip packing problem. We show that the restricted strip covering problem is NP-hard and present an O(log log n)-approximation algorithm. We present better approximations or exact algorithms for some special cases. For the uniform-duration case of restricted strip covering we give a polynomial-time, exact algorithm but prove that the uniform-duration case for higher-dimensional regions is NP-hard. Finally, we consider regions that are arbitrary sets, and we present an O(log n)-approximation algorithm.",
        "published": "2006-05-24T03:27:07Z",
        "link": "http://arxiv.org/abs/cs/0605102v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "An Algorithm to Determine Peer-Reviewers",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen"
        ],
        "summary": "The peer-review process is the most widely accepted certification mechanism for officially accepting the written results of researchers within the scientific community. An essential component of peer-review is the identification of competent referees to review a submitted manuscript. This article presents an algorithm to automatically determine the most appropriate reviewers for a manuscript by way of a co-authorship network data structure and a relative-rank particle-swarm algorithm. This approach is novel in that it is not limited to a pre-selected set of referees, is computationally efficient, requires no human-intervention, and, in some instances, can automatically identify conflict of interest situations. A useful application of this algorithm would be to open commentary peer-review systems because it provides a weighting for each referee with respects to their expertise in the domain of a manuscript. The algorithm is validated using referee bid data from the 2005 Joint Conference on Digital Libraries.",
        "published": "2006-05-24T17:06:32Z",
        "link": "http://arxiv.org/abs/cs/0605112v2",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.DS",
            "H.3.7, H.3.3"
        ]
    },
    {
        "title": "Power-aware scheduling for makespan and flow",
        "authors": [
            "David P. Bunde"
        ],
        "summary": "We consider offline scheduling algorithms that incorporate speed scaling to address the bicriteria problem of minimizing energy consumption and a scheduling metric. For makespan, we give linear-time algorithms to compute all non-dominated solutions for the general uniprocessor problem and for the multiprocessor problem when every job requires the same amount of work. We also show that the multiprocessor problem becomes NP-hard when jobs can require different amounts of work.   For total flow, we show that the optimal flow corresponding to a particular energy budget cannot be exactly computed on a machine supporting arithmetic and the extraction of roots. This hardness result holds even when scheduling equal-work jobs on a uniprocessor. We do, however, extend previous work by Pruhs et al. to give an arbitrarily-good approximation for scheduling equal-work jobs on a multiprocessor.",
        "published": "2006-05-26T21:57:35Z",
        "link": "http://arxiv.org/abs/cs/0605126v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Tight Bounds for the Min-Max Boundary Decomposition Cost of Weighted   Graphs",
        "authors": [
            "David Steurer"
        ],
        "summary": "Many load balancing problems that arise in scientific computing applications ask to partition a graph with weights on the vertices and costs on the edges into a given number of almost equally-weighted parts such that the maximum boundary cost over all parts is small.   Here, this partitioning problem is considered for bounded-degree graphs G=(V,E) with edge costs c: E->R+ that have a p-separator theorem for some p>1, i.e., any (arbitrarily weighted) subgraph of G can be separated into two parts of roughly the same weight by removing a vertex set S such that the edges incident to S in the subgraph have total cost at most proportional to (SUM_e c^p_e)^(1/p), where the sum is over all edges e in the subgraph.   We show for all positive integers k and weights w that the vertices of G can be partitioned into k parts such that the weight of each part differs from the average weight by less than MAX{w_v; v in V}, and the boundary edges of each part have cost at most proportional to (SUM_e c_e^p/k)^(1/p) + MAX_e c_e. The partition can be computed in time nearly proportional to the time for computing a separator S of G.   Our upper bound on the boundary costs is shown to be tight up to a constant factor for infinitely many instances with a broad range of parameters. Previous results achieved this bound only if one has c=1, w=1, and one allows parts with weight exceeding the average by a constant fraction.",
        "published": "2006-06-01T01:47:38Z",
        "link": "http://arxiv.org/abs/cs/0606001v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Diagonal Peg Solitaire",
        "authors": [
            "George I. Bell"
        ],
        "summary": "We study the classical game of peg solitaire when diagonal jumps are allowed. We prove that on many boards, one can begin from a full board with one peg missing, and finish with one peg anywhere on the board. We then consider the problem of finding solutions that minimize the number of moves (where a move is one or more jumps by the same peg), and find the shortest solution to the \"central game\", which begins and ends at the center. In some cases we can prove analytically that our solutions are the shortest possible, in other cases we apply A* or bidirectional search heuristics.",
        "published": "2006-06-06T03:07:27Z",
        "link": "http://arxiv.org/abs/math/0606122v2",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "00A08; 97A20"
        ]
    },
    {
        "title": "Tight Bounds on the Complexity of Recognizing Odd-Ranked Elements",
        "authors": [
            "Shripad Thite"
        ],
        "summary": "Let S = <s_1, s_2, s_3, ..., s_n> be a given vector of n real numbers. The rank of a real z with respect to S is defined as the number of elements s_i in S such that s_i is less than or equal to z. We consider the following decision problem: determine whether the odd-numbered elements s_1, s_3, s_5, ... are precisely the elements of S whose rank with respect to S is odd. We prove a bound of Theta(n log n) on the number of operations required to solve this problem in the algebraic computation tree model.",
        "published": "2006-06-08T21:28:09Z",
        "link": "http://arxiv.org/abs/cs/0606038v1",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Approximation Algorithms for Multi-Criteria Traveling Salesman Problems",
        "authors": [
            "Bodo Manthey",
            "L. Shankar Ram"
        ],
        "summary": "In multi-criteria optimization problems, several objective functions have to be optimized. Since the different objective functions are usually in conflict with each other, one cannot consider only one particular solution as the optimal solution. Instead, the aim is to compute a so-called Pareto curve of solutions. Since Pareto curves cannot be computed efficiently in general, we have to be content with approximations to them.   We design a deterministic polynomial-time algorithm for multi-criteria g-metric STSP that computes (min{1 +g, 2g^2/(2g^2 -2g +1)} + eps)-approximate Pareto curves for all 1/2<=g<=1. In particular, we obtain a (2+eps)-approximation for multi-criteria metric STSP. We also present two randomized approximation algorithms for multi-criteria g-metric STSP that achieve approximation ratios of (2g^3 +2g^2)/(3g^2 -2g +1) + eps and (1 +g)/(1 +3g -4g^2) + eps, respectively.   Moreover, we present randomized approximation algorithms for multi-criteria g-metric ATSP (ratio 1/2 + g^3/(1 -3g^2) + eps) for g < 1/sqrt(3)), STSP with weights 1 and 2 (ratio 4/3) and ATSP with weights 1 and 2 (ratio 3/2). To do this, we design randomized approximation schemes for multi-criteria cycle cover and graph factor problems.",
        "published": "2006-06-09T11:41:53Z",
        "link": "http://arxiv.org/abs/cs/0606040v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Enabling user-driven Checkpointing strategies in Reverse-mode Automatic   Differentiation",
        "authors": [
            "Laurent Hascoet",
            "Mauricio Araya-Polo"
        ],
        "summary": "This paper presents a new functionality of the Automatic Differentiation (AD) tool Tapenade. Tapenade generates adjoint codes which are widely used for optimization or inverse problems. Unfortunately, for large applications the adjoint code demands a great deal of memory, because it needs to store a large set of intermediates values. To cope with that problem, Tapenade implements a sub-optimal version of a technique called checkpointing, which is a trade-off between storage and recomputation. Our long-term goal is to provide an optimal checkpointing strategy for every code, not yet achieved by any AD tool. Towards that goal, we first introduce modifications in Tapenade in order to give the user the choice to select the checkpointing strategy most suitable for their code. Second, we conduct experiments in real-size scientific codes in order to gather hints that help us to deduce an optimal checkpointing strategy. Some of the experimental results show memory savings up to 35% and execution time up to 90%.",
        "published": "2006-06-09T16:01:46Z",
        "link": "http://arxiv.org/abs/cs/0606042v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A New Quartet Tree Heuristic for Hierarchical Clustering",
        "authors": [
            "Rudi Cilibrasi",
            "Paul M. B. Vitanyi"
        ],
        "summary": "We consider the problem of constructing an an optimal-weight tree from the 3*(n choose 4) weighted quartet topologies on n objects, where optimality means that the summed weight of the embedded quartet topologiesis optimal (so it can be the case that the optimal tree embeds all quartets as non-optimal topologies). We present a heuristic for reconstructing the optimal-weight tree, and a canonical manner to derive the quartet-topology weights from a given distance matrix. The method repeatedly transforms a bifurcating tree, with all objects involved as leaves, achieving a monotonic approximation to the exact single globally optimal tree. This contrasts to other heuristic search methods from biological phylogeny, like DNAML or quartet puzzling, which, repeatedly, incrementally construct a solution from a random order of objects, and subsequently add agreement values.",
        "published": "2006-06-11T16:05:51Z",
        "link": "http://arxiv.org/abs/cs/0606048v1",
        "categories": [
            "cs.DS",
            "cs.CV",
            "cs.DM",
            "math.ST",
            "physics.data-an",
            "q-bio.QM",
            "stat.TH",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Scheduling Algorithms for Procrastinators",
        "authors": [
            "Michael A. Bender",
            "Raphael Clifford",
            "Kostas Tsichlas"
        ],
        "summary": "This paper presents scheduling algorithms for procrastinators, where the speed that a procrastinator executes a job increases as the due date approaches. We give optimal off-line scheduling policies for linearly increasing speed functions. We then explain the computational/numerical issues involved in implementing this policy. We next explore the online setting, showing that there exist adversaries that force any online scheduling policy to miss due dates. This impossibility result motivates the problem of minimizing the maximum interval stretch of any job; the interval stretch of a job is the job's flow time divided by the job's due date minus release time. We show that several common scheduling strategies, including the \"hit-the-highest-nail\" strategy beloved by procrastinators, have arbitrarily large maximum interval stretch. Then we give the \"thrashing\" scheduling policy and show that it is a \\Theta(1) approximation algorithm for the maximum interval stretch.",
        "published": "2006-06-14T16:55:44Z",
        "link": "http://arxiv.org/abs/cs/0606067v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Precision Arithmetic: A New Floating-Point Arithmetic",
        "authors": [
            "Chengpu Wang"
        ],
        "summary": "A new deterministic floating-point arithmetic called precision arithmetic is developed to track precision for arithmetic calculations. It uses a novel rounding scheme to avoid excessive rounding error propagation of conventional floating-point arithmetic. Unlike interval arithmetic, its uncertainty tracking is based on statistics and the central limit theorem, with a much tighter bounding range. Its stable rounding error distribution is approximated by a truncated normal distribution. Generic standards and systematic methods for validating uncertainty-bearing arithmetics are discussed. The precision arithmetic is found to be better than interval arithmetic in both uncertainty-tracking and uncertainty-bounding for normal usages.   The precision arithmetic is available publicly at http://precisionarithm.sourceforge.net.",
        "published": "2006-06-25T18:56:28Z",
        "link": "http://arxiv.org/abs/cs/0606103v22",
        "categories": [
            "cs.DM",
            "cs.DS",
            "cs.NA",
            "65Y04, 65T50",
            "G.1.0"
        ]
    },
    {
        "title": "Maximum gradient embeddings and monotone clustering",
        "authors": [
            "Manor Mendel",
            "Assaf Naor"
        ],
        "summary": "Let (X,d_X) be an n-point metric space. We show that there exists a distribution D over non-contractive embeddings into trees f:X-->T such that for every x in X, the expectation with respect to D of the maximum over y in X of the ratio d_T(f(x),f(y)) / d_X(x,y) is at most C (log n)^2, where C is a universal constant. Conversely we show that the above quadratic dependence on log n cannot be improved in general. Such embeddings, which we call maximum gradient embeddings, yield a framework for the design of approximation algorithms for a wide range of clustering problems with monotone costs, including fault-tolerant versions of k-median and facility location.",
        "published": "2006-06-26T19:32:29Z",
        "link": "http://arxiv.org/abs/cs/0606109v4",
        "categories": [
            "cs.DS",
            "30L05, 68W25"
        ]
    },
    {
        "title": "Optimal Scheduling of Peer-to-Peer File Dissemination",
        "authors": [
            "Jochen Mundinger",
            "Richard R. Weber",
            "Gideon Weiss"
        ],
        "summary": "Peer-to-peer (P2P) overlay networks such as BitTorrent and Avalanche are increasingly used for disseminating potentially large files from a server to many end users via the Internet. The key idea is to divide the file into many equally-sized parts and then let users download each part (or, for network coding based systems such as Avalanche, linear combinations of the parts) either from the server or from another user who has already downloaded it. However, their performance evaluation has typically been limited to comparing one system relative to another and typically been realized by means of simulation and measurements. In contrast, we provide an analytic performance analysis that is based on a new uplink-sharing version of the well-known broadcasting problem. Assuming equal upload capacities, we show that the minimal time to disseminate the file is the same as for the simultaneous send/receive version of the broadcasting problem. For general upload capacities, we provide a mixed integer linear program (MILP) solution and a complementary fluid limit solution. We thus provide a lower bound which can be used as a performance benchmark for any P2P file dissemination system. We also investigate the performance of a decentralized strategy, providing evidence that the performance of necessarily decentralized P2P file dissemination systems should be close to this bound and therefore that it is useful in practice.",
        "published": "2006-06-27T08:11:57Z",
        "link": "http://arxiv.org/abs/cs/0606110v2",
        "categories": [
            "cs.NI",
            "cs.DS",
            "math.OC"
        ]
    },
    {
        "title": "New Algorithms for Regular Expression Matching",
        "authors": [
            "Philip Bille"
        ],
        "summary": "In this paper we revisit the classical regular expression matching problem, namely, given a regular expression $R$ and a string $Q$, decide if $Q$ matches one of the strings specified by $R$. Let $m$ and $n$ be the length of $R$ and $Q$, respectively. On a standard unit-cost RAM with word length $w \\geq \\log n$, we show that the problem can be solved in $O(m)$ space with the following running times: \\begin{equation*} \\begin{cases}   O(n\\frac{m \\log w}{w} + m \\log w) & \\text{if $m > w$} \\\\   O(n\\log m + m\\log m) & \\text{if $\\sqrt{w} < m \\leq w$} \\\\   O(\\min(n+ m^2, n\\log m + m\\log m)) & \\text{if $m \\leq \\sqrt{w}$.} \\end{cases} \\end{equation*} This improves the best known time bound among algorithms using $O(m)$ space. Whenever $w \\geq \\log^2 n$ it improves all known time bounds regardless of how much space is used.",
        "published": "2006-06-28T10:51:39Z",
        "link": "http://arxiv.org/abs/cs/0606116v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Weighted hierarchical alignment of directed acyclic graph",
        "authors": [
            "Sean M. Falconer",
            "Dmitri Maslov"
        ],
        "summary": "In some applications of matching, the structural or hierarchical properties of the two graphs being aligned must be maintained. The hierarchical properties are induced by the direction of the edges in the two directed graphs. These structural relationships defined by the hierarchy in the graphs act as a constraint on the alignment. In this paper, we formalize the above problem as the weighted alignment between two directed acyclic graphs. We prove that this problem is NP-complete, show several upper bounds for approximating the solution, and finally introduce polynomial time algorithms for sub-classes of directed acyclic graphs.",
        "published": "2006-06-29T18:07:49Z",
        "link": "http://arxiv.org/abs/cs/0606124v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The evolution of navigable small-world networks",
        "authors": [
            "Oskar Sandberg",
            "Ian Clarke"
        ],
        "summary": "Small-world networks, which combine randomized and structured elements, are seen as prevalent in nature. Several random graph models have been given for small-world networks, with one of the most fruitful, introduced by Jon Kleinberg, showing in which type of graphs it is possible to route, or navigate, between vertices with very little knowledge of the graph itself. Kleinberg's model is static, with random edges added to a fixed grid. In this paper we introduce, analyze and test a randomized algorithm which successively rewires a graph with every application. The resulting process gives a model for the evolution of small-world networks with properties similar to those studied by Kleinberg.",
        "published": "2006-07-07T13:21:09Z",
        "link": "http://arxiv.org/abs/cs/0607025v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "Path-independent load balancing with unreliable machines",
        "authors": [
            "James Aspnes",
            "Yang Richard Yang",
            "Yitong Yin"
        ],
        "summary": "We consider algorithms for load balancing on unreliable machines. The objective is to optimize the two criteria of minimizing the makespan and minimizing job reassignments in response to machine failures. We assume that the set of jobs is known in advance but that the pattern of machine failures is unpredictable. Motivated by the requirements of BGP routing, we consider path-independent algorithms, with the property that the job assignment is completely determined by the subset of available machines and not the previous history of the assignments. We examine first the question of performance measurement of path-independent load-balancing algorithms, giving the measure of makespan and the normalized measure of reassignments cost. We then describe two classes of algorithms for optimizing these measures against an oblivious adversary for identical machines. The first, based on independent random assignments, gives expected reassignment costs within a factor of 2 of optimal and gives a makespan within a factor of O(log m/log log m) of optimal with high probability, for unknown job sizes. The second, in which jobs are first grouped into bins and at most one bin is assigned to each machine, gives constant-factor ratios on both reassignment cost and makespan, for known job sizes. Several open problems are discussed.",
        "published": "2006-07-07T14:01:15Z",
        "link": "http://arxiv.org/abs/cs/0607026v1",
        "categories": [
            "cs.DS",
            "cs.NI",
            "F.2.2"
        ]
    },
    {
        "title": "Improved online hypercube packing",
        "authors": [
            "Xin Han",
            "Deshi Ye",
            "Yong Zhou"
        ],
        "summary": "In this paper, we study online multidimensional bin packing problem when all items are hypercubes.   Based on the techniques in one dimensional bin packing algorithm Super Harmonic by Seiden, we give a framework for online hypercube packing problem and obtain new upper bounds of asymptotic competitive ratios.   For square packing, we get an upper bound of 2.1439, which is better than 2.24437.   For cube packing, we also give a new upper bound 2.6852 which is better than 2.9421 by Epstein and van Stee.",
        "published": "2006-07-11T09:43:45Z",
        "link": "http://arxiv.org/abs/cs/0607045v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Strip Packing vs. Bin Packing",
        "authors": [
            "Xin Han",
            "Kazuo Iwama",
            "Deshi Ye",
            "Guochuan Zhang"
        ],
        "summary": "In this paper we establish a general algorithmic framework between bin packing and strip packing, with which we achieve the same asymptotic bounds by applying bin packing algorithms to strip packing. More precisely we obtain the following results: (1) Any offline bin packing algorithm can be applied to strip packing maintaining the same asymptotic worst-case ratio. Thus using FFD (MFFD) as a subroutine, we get a practical (simple and fast) algorithm for strip packing with an upper bound 11/9 (71/60). A simple AFPTAS for strip packing immediately follows. (2) A class of Harmonic-based algorithms for bin packing can be applied to online strip packing maintaining the same asymptotic competitive ratio. It implies online strip packing admits an upper bound of 1.58889 on the asymptotic competitive ratio, which is very close to the lower bound 1.5401 and significantly improves the previously best bound of 1.6910 and affirmatively answers an open question posed by Csirik et. al.",
        "published": "2006-07-11T09:58:34Z",
        "link": "http://arxiv.org/abs/cs/0607046v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "On Some Peculiarities of Dynamic Switch between Component   Implementations in an Autonomic Computing System",
        "authors": [
            "Igor Mackarov"
        ],
        "summary": "Behavior of the delta algorithm of autonomic switch between two component implementations is considered on several examples of a client-server systems involving, in particular, periodic change of intensities of requests for the component. It is shown that in the cases of some specific combinations of elementary requests costs, the number of clients in the system, the number of requests per unit of time, and the cost of switch between the implementations, the algorithm may reveal behavior that is rather far from the desired. A sufficient criterion of a success of the algorithm is proposed based on the analysis of the accumulated implementations costs difference as a function of time. Suggestions are pointed out of practical evaluation of the algorithm functioning regarding the observations made in this paper.",
        "published": "2006-07-12T11:09:52Z",
        "link": "http://arxiv.org/abs/cs/0607061v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.NA"
        ]
    },
    {
        "title": "Complex Lattice Reduction Algorithm for Low-Complexity MIMO Detection",
        "authors": [
            "Ying Hung Gan",
            "Cong Ling",
            "Wai Ho Mow"
        ],
        "summary": "Recently, lattice-reduction-aided detectors have been proposed for multiple-input multiple-output (MIMO) systems to give performance with full diversity like maximum likelihood receiver, and yet with complexity similar to linear receivers. However, these lattice-reduction-aided detectors are based on the traditional LLL reduction algorithm that was originally introduced for reducing real lattice bases, in spite of the fact that the channel matrices are inherently complex-valued. In this paper, we introduce the complex LLL algorithm for direct application to reduce the basis of a complex lattice which is naturally defined by a complex-valued channel matrix. We prove that complex LLL reduction-aided detection can also achieve full diversity. Our analysis reveals that the new complex LLL algorithm can achieve a reduction in complexity of nearly 50% over the traditional LLL algorithm, and this is confirmed by simulation. It is noteworthy that the complex LLL algorithm aforementioned has nearly the same bit-error-rate performance as the traditional LLL algorithm.",
        "published": "2006-07-17T09:01:33Z",
        "link": "http://arxiv.org/abs/cs/0607078v1",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Extending the scalars of minimizations",
        "authors": [
            "Gérard Duchamp",
            "Eric Laugerotte",
            "Jean-Gabriel Luque"
        ],
        "summary": "In the classical theory of formal languages, finite state automata allow to recognize the words of a rational subset of $\\Sigma^*$ where $\\Sigma$ is a set of symbols (or the alphabet). Now, given a semiring $(\\K,+,.)$, one can construct $\\K$-subsets of $\\Sigma^*$ in the sense of Eilenberg, that are alternatively called noncommutative formal power series for which a framework very similar to language theory has been constructed Particular noncommutative formal power series, which are called rational series, are the behaviour of a family of weighted automata (or $\\K$-automata). In order to get an efficient encoding, it may be interesting to point out one of them with the smallest number of states. Minimization processes of $\\K$-automata already exist for $\\K$ being: {\\bf a)} a field, {\\bf b)} a noncommutative field, {\\bf c)} a PID . When $\\K$ is the bolean semiring, such a minimization process (with isomorphisms of minimal objects) is known within the category of deterministic automata. Minimal automata have been proved to be isomorphic in cases {\\bf (a)} and {\\bf (b)}. But the proof given for (b) is not constructive. In fact, it lays on the existence of a basis for a submodule of $\\K^n$. Here we give an independent algorithm which reproves this fact and an example of a pair of nonisomorphic minimal automata. Moreover, we examine the possibility of extending {\\bf (c)}. To this end, we provide an {\\em Effective Minimization Process} (or {\\em EMP}) which can be used for more general sets of coefficients.",
        "published": "2006-07-18T07:06:59Z",
        "link": "http://arxiv.org/abs/math/0607411v1",
        "categories": [
            "math.CO",
            "cs.DS",
            "cs.SC"
        ]
    },
    {
        "title": "List decoding of noisy Reed-Muller-like codes",
        "authors": [
            "A. R. Calderbank",
            "Anna C. Gilbert",
            "Martin J. Strauss"
        ],
        "summary": "First- and second-order Reed-Muller (RM(1) and RM(2), respectively) codes are two fundamental error-correcting codes which arise in communication as well as in probabilistically-checkable proofs and learning. In this paper, we take the first steps toward extending the quick randomized decoding tools of RM(1) into the realm of quadratic binary and, equivalently, Z_4 codes. Our main algorithmic result is an extension of the RM(1) techniques from Goldreich-Levin and Kushilevitz-Mansour algorithms to the Hankel code, a code between RM(1) and RM(2). That is, given signal s of length N, we find a list that is a superset of all Hankel codewords phi with dot product to s at least (1/sqrt(k)) times the norm of s, in time polynomial in k and log(N). We also give a new and simple formulation of a known Kerdock code as a subcode of the Hankel code. As a corollary, we can list-decode Kerdock, too. Also, we get a quick algorithm for finding a sparse Kerdock approximation. That is, for k small compared with 1/sqrt{N} and for epsilon > 0, we find, in time polynomial in (k log(N)/epsilon), a k-Kerdock-term approximation s~ to s with Euclidean error at most the factor (1+epsilon+O(k^2/sqrt{N})) times that of the best such approximation.",
        "published": "2006-07-20T21:02:29Z",
        "link": "http://arxiv.org/abs/cs/0607098v2",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT",
            "E.4; F.2.1"
        ]
    },
    {
        "title": "New Upper Bounds on The Approximability of 3D Strip Packing",
        "authors": [
            "Xin Han",
            "Kazuo Iwama",
            "Guochuan Zhang"
        ],
        "summary": "In this paper, we study the 3D strip packing problem in which we are given a list of 3-dimensional boxes and required to pack all of them into a 3-dimensional strip with length 1 and width 1 and unlimited height to minimize the height used. Our results are below: i) we give an approximation algorithm with asymptotic worst-case ratio 1.69103, which improves the previous best bound of $2+\\epsilon$ by Jansen and Solis-Oba of SODA 2006; ii) we also present an asymptotic PTAS for the case in which all items have {\\em square} bases.",
        "published": "2006-07-22T02:06:26Z",
        "link": "http://arxiv.org/abs/cs/0607100v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric,   Diagonally Dominant Linear Systems",
        "authors": [
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We present a randomized algorithm that, on input a symmetric, weakly diagonally dominant n-by-n matrix A with m nonzero entries and an n-vector b, produces a y such that $\\norm{y - \\pinv{A} b}_{A} \\leq \\epsilon \\norm{\\pinv{A} b}_{A}$ in expected time $O (m \\log^{c}n \\log (1/\\epsilon)),$ for some constant c. By applying this algorithm inside the inverse power method, we compute approximate Fiedler vectors in a similar amount of time. The algorithm applies subgraph preconditioners in a recursive fashion. These preconditioners improve upon the subgraph preconditioners first introduced by Vaidya (1990).   For any symmetric, weakly diagonally-dominant matrix A with non-positive off-diagonal entries and $k \\geq 1$, we construct in time $O (m \\log^{c} n)$ a preconditioner B of A with at most $2 (n - 1) + O ((m/k) \\log^{39} n)$ nonzero off-diagonal entries such that the finite generalized condition number $\\kappa_{f} (A,B)$ is at most k, for some other constant c.   In the special case when the nonzero structure of the matrix is planar the corresponding linear system solver runs in expected time $ O (n \\log^{2} n + n \\log n \\ \\log \\log n \\ \\log (1/\\epsilon))$.   We hope that our introduction of algorithms of low asymptotic complexity will lead to the development of algorithms that are also fast in practice.",
        "published": "2006-07-24T04:02:24Z",
        "link": "http://arxiv.org/abs/cs/0607105v5",
        "categories": [
            "cs.NA",
            "cs.DS",
            "F.2.1; G.1.3"
        ]
    },
    {
        "title": "Polynomial-time algorithm for vertex k-colorability of P_5-free graphs",
        "authors": [
            "Marcin Kaminski",
            "Vadim Lozin"
        ],
        "summary": "We give the first polynomial-time algorithm for coloring vertices of P_5-free graphs with k colors. This settles an open problem and generalizes several previously known results.",
        "published": "2006-07-26T09:20:20Z",
        "link": "http://arxiv.org/abs/cs/0607115v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "The minimum linear arrangement problem on proper interval graphs",
        "authors": [
            "Ilya Safro"
        ],
        "summary": "We present a linear time algorithm for the minimum linear arrangement problem on proper interval graphs. The obtained ordering is a 4-approximation for general interval graphs",
        "published": "2006-08-02T07:46:54Z",
        "link": "http://arxiv.org/abs/cs/0608008v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Pull-Based Data Broadcast with Dependencies: Be Fair to Users, not to   Items",
        "authors": [
            "Julien Robert",
            "Nicolas Schabanel"
        ],
        "summary": "Broadcasting is known to be an efficient means of disseminating data in wireless communication environments (such as Satellite, mobile phone networks,...). It has been recently observed that the average service time of broadcast systems can be considerably improved by taking into consideration existing correlations between requests. We study a pull-based data broadcast system where users request possibly overlapping sets of items; a request is served when all its requested items are downloaded. We aim at minimizing the average user perceived latency, i.e. the average flow time of the requests. We first show that any algorithm that ignores the dependencies can yield arbitrary bad performances with respect to the optimum even if it is given arbitrary extra resources. We then design a $(4+\\epsilon)$-speed $O(1+1/\\epsilon^2)$-competitive algorithm for this setting that consists in 1) splitting evenly the bandwidth among each requested set and in 2) broadcasting arbitrarily the items still missing in each set into the bandwidth the set has received. Our algorithm presents several interesting features: it is simple to implement, non-clairvoyant, fair to users so that no user may starve for a long period of time, and guarantees good performances in presence of correlations between user requests (without any change in the broadcast protocol). We also present a $ (4+\\epsilon)$-speed $O(1+1/\\epsilon^3)$-competitive algorithm which broadcasts at most one item at any given time and preempts each item broadcast at most once on average. As a side result of our analysis, we design a competitive algorithm for a particular setting of non-clairvoyant job scheduling with dependencies, which might be of independent interest.",
        "published": "2006-08-02T15:00:02Z",
        "link": "http://arxiv.org/abs/cs/0608013v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Search via Quantum Walk",
        "authors": [
            "Frédéric Magniez",
            "Ashwin Nayak",
            "Jérémie Roland",
            "Miklos Santha"
        ],
        "summary": "We propose a new method for designing quantum search algorithms for finding a \"marked\" element in the state space of a classical Markov chain. The algorithm is based on a quantum walk \\'a la Szegedy (2004) that is defined in terms of the Markov chain. The main new idea is to apply quantum phase estimation to the quantum walk in order to implement an approximate reflection operator. This operator is then used in an amplitude amplification scheme. As a result we considerably expand the scope of the previous approaches of Ambainis (2004) and Szegedy (2004). Our algorithm combines the benefits of these approaches in terms of being able to find marked elements, incurring the smaller cost of the two, and being applicable to a larger class of Markov chains. In addition, it is conceptually simple and avoids some technical difficulties in the previous analyses of several algorithms based on quantum walk.",
        "published": "2006-08-02T18:43:09Z",
        "link": "http://arxiv.org/abs/quant-ph/0608026v4",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Cascade hash tables: a series of multilevel double hashing schemes with   O(1) worst case lookup time",
        "authors": [
            "Shaohua Li"
        ],
        "summary": "In this paper, the author proposes a series of multilevel double hashing schemes called cascade hash tables. They use several levels of hash tables. In each table, we use the common double hashing scheme. Higher level hash tables work as fail-safes of lower level hash tables. By this strategy, it could effectively reduce collisions in hash insertion. Thus it gains a constant worst case lookup time with a relatively high load factor(70%-85%) in random experiments. Different parameters of cascade hash tables are tested.",
        "published": "2006-08-07T15:22:30Z",
        "link": "http://arxiv.org/abs/cs/0608037v3",
        "categories": [
            "cs.DS",
            "cs.AI"
        ]
    },
    {
        "title": "Recognising the Suzuki groups in their natural representations",
        "authors": [
            "Henrik Bäärnhielm"
        ],
        "summary": "Under the assumption of a certain conjecture, for which there exists strong experimental evidence, we produce an efficient algorithm for constructive membership testing in the Suzuki groups Sz(q), where q = 2^{2m + 1} for some m > 0, in their natural representations of degree 4. It is a Las Vegas algorithm with running time O{log(q)} field operations, and a preprocessing step with running time O{log(q) loglog(q)} field operations. The latter step needs an oracle for the discrete logarithm problem in GF(q).   We also produce a recognition algorithm for Sz(q) = <X>. This is a Las Vegas algorithm with running time O{|X|^2} field operations.   Finally, we give a Las Vegas algorithm that, given <X>^h = Sz(q) for some h in GL(4, q), finds some g such that <X>^g = Sz(q). The running time is O{log(q) loglog(q) + |X|} field operations.   Implementations of the algorithms are available for the computer system MAGMA.",
        "published": "2006-08-09T02:43:35Z",
        "link": "http://arxiv.org/abs/math/0608210v1",
        "categories": [
            "math.GR",
            "cs.DS",
            "20H30, 68Q25, 68W20 (Primary) 20P05, 20C33, 20C40 (Secondary)"
        ]
    },
    {
        "title": "Post-Processing Hierarchical Community Structures: Quality Improvements   and Multi-scale View",
        "authors": [
            "Pascal Pons",
            "Matthieu Latapy"
        ],
        "summary": "Dense sub-graphs of sparse graphs (communities), which appear in most real-world complex networks, play an important role in many contexts. Most existing community detection algorithms produce a hierarchical structure of community and seek a partition into communities that optimizes a given quality function. We propose new methods to improve the results of any of these algorithms. First we show how to optimize a general class of additive quality functions (containing the modularity, the performance, and a new similarity based quality function we propose) over a larger set of partitions than the classical methods. Moreover, we define new multi-scale quality functions which make it possible to detect the different scales at which meaningful community structures appear, while classical approaches find only one partition.",
        "published": "2006-08-09T09:23:06Z",
        "link": "http://arxiv.org/abs/cs/0608050v2",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Dispersion of Mass and the Complexity of Randomized Geometric Algorithms",
        "authors": [
            "Luis Rademacher",
            "Santosh Vempala"
        ],
        "summary": "How much can randomness help computation? Motivated by this general question and by volume computation, one of the few instances where randomness provably helps, we analyze a notion of dispersion and connect it to asymptotic convex geometry. We obtain a nearly quadratic lower bound on the complexity of randomized volume algorithms for convex bodies in R^n (the current best algorithm has complexity roughly n^4, conjectured to be n^3). Our main tools, dispersion of random determinants and dispersion of the length of a random point from a convex body, are of independent interest and applicable more generally; in particular, the latter is closely related to the variance hypothesis from convex geometry. This geometric dispersion also leads to lower bounds for matrix problems and property testing.",
        "published": "2006-08-12T23:31:07Z",
        "link": "http://arxiv.org/abs/cs/0608054v2",
        "categories": [
            "cs.CC",
            "cs.CG",
            "cs.DS",
            "math.FA"
        ]
    },
    {
        "title": "k-Connectivity in the Semi-Streaming Model",
        "authors": [
            "Mariano Zelke"
        ],
        "summary": "We present the first semi-streaming algorithms to determine k-connectivity of an undirected graph with k being any constant. The semi-streaming model for graph algorithms was introduced by Muthukrishnan in 2003 and turns out to be useful when dealing with massive graphs streamed in from an external storage device.   Our two semi-streaming algorithms each compute a sparse subgraph of an input graph G and can use this subgraph in a postprocessing step to decide k-connectivity of G. To this end the first algorithm reads the input stream only once and uses time O(k^2*n) to process each input edge. The second algorithm reads the input k+1 times and needs time O(k+alpha(n)) per input edge. Using its constructed subgraph the second algorithm can also generate all l-separators of the input graph for all l<k.",
        "published": "2006-08-16T10:37:07Z",
        "link": "http://arxiv.org/abs/cs/0608066v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "Algorithmic linear dimension reduction in the l_1 norm for sparse   vectors",
        "authors": [
            "A. C. Gilbert",
            "M. J. Strauss",
            "J. A. Tropp",
            "R. Vershynin"
        ],
        "summary": "This paper develops a new method for recovering m-sparse signals that is simultaneously uniform and quick. We present a reconstruction algorithm whose run time, O(m log^2(m) log^2(d)), is sublinear in the length d of the signal. The reconstruction error is within a logarithmic factor (in m) of the optimal m-term approximation error in l_1. In particular, the algorithm recovers m-sparse signals perfectly and noisy signals are recovered with polylogarithmic distortion. Our algorithm makes O(m log^2 (d)) measurements, which is within a logarithmic factor of optimal. We also present a small-space implementation of the algorithm. These sketching techniques and the corresponding reconstruction algorithms provide an algorithmic dimension reduction in the l_1 norm. In particular, vectors of support m in dimension d can be linearly embedded into O(m log^2 d) dimensions with polylogarithmic distortion. We can reconstruct a vector from its low-dimensional sketch in time O(m log^2(m) log^2(d)). Furthermore, this reconstruction is stable and robust under small perturbations.",
        "published": "2006-08-19T01:55:14Z",
        "link": "http://arxiv.org/abs/cs/0608079v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "On-line topological simplification of weighted graphs",
        "authors": [
            "Floris Geerts",
            "Peter Revesz",
            "Jan Van den Bussche"
        ],
        "summary": "We describe two efficient on-line algorithms to simplify weighted graphs by eliminating degree-two vertices. Our algorithms are on-line in that they react to updates on the data, keeping the simplification up-to-date. The supported updates are insertions of vertices and edges; hence, our algorithms are partially dynamic. We provide both analytical and empirical evaluations of the efficiency of our approaches. Specifically, we prove an O(log n) upper bound on the amortized time complexity of our maintenance algorithms, with n the number of insertions.",
        "published": "2006-08-23T20:08:53Z",
        "link": "http://arxiv.org/abs/cs/0608091v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Spherical Indexing for Neighborhood Queries",
        "authors": [
            "Nicolas Brodu"
        ],
        "summary": "This is an algorithm for finding neighbors when the objects can freely move and have no predefined position. The query consists in finding neighbors for a center location and a given radius. Space is discretized in cubic cells. This algorithm introduces a direct spherical indexing that gives the list of all cells making up the query sphere, for any radius and any center location. It can additionally take in account both cyclic and non-cyclic regions of interest. Finding only the K nearest neighbors naturally benefits from the spherical indexing by minimally running through the sphere from center to edge, and reducing the maximum distance when K neighbors have been found.",
        "published": "2006-08-29T00:12:55Z",
        "link": "http://arxiv.org/abs/cs/0608108v1",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "The Tree Inclusion Problem: In Linear Space and Faster",
        "authors": [
            "Philip Bille",
            "Inge Li Goertz"
        ],
        "summary": "Given two rooted, ordered, and labeled trees $P$ and $T$ the tree inclusion problem is to determine if $P$ can be obtained from $T$ by deleting nodes in $T$. This problem has recently been recognized as an important query primitive in XML databases. Kilpel\\\"ainen and Mannila [\\emph{SIAM J. Comput. 1995}] presented the first polynomial time algorithm using quadratic time and space. Since then several improved results have been obtained for special cases when $P$ and $T$ have a small number of leaves or small depth. However, in the worst case these algorithms still use quadratic time and space. Let $n_S$, $l_S$, and $d_S$ denote the number of nodes, the number of leaves, and the %maximum depth of a tree $S \\in \\{P, T\\}$. In this paper we show that the tree inclusion problem can be solved in space $O(n_T)$ and time: O(\\min(l_Pn_T, l_Pl_T\\log \\log n_T + n_T, \\frac{n_Pn_T}{\\log n_T} + n_{T}\\log n_{T})). This improves or matches the best known time complexities while using only linear space instead of quadratic. This is particularly important in practical applications, such as XML databases, where the space is likely to be a bottleneck.",
        "published": "2006-08-31T12:23:37Z",
        "link": "http://arxiv.org/abs/cs/0608124v5",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Finding heaviest H-subgraphs in real weighted graphs, with applications",
        "authors": [
            "Virginia Vassilevska",
            "Ryan Williams",
            "Raphael Yuster"
        ],
        "summary": "For a graph G with real weights assigned to the vertices (edges), the MAX H-SUBGRAPH problem is to find an H-subgraph of G with maximum total weight, if one exists. The all-pairs MAX H-SUBGRAPH problem is to find for every pair of vertices u,v, a maximum H-subgraph containing both u and v, if one exists. Our main results are new strongly polynomial algorithms for the all-pairs MAX H-SUBGRAPH problem for vertex weighted graphs. We also give improved algorithms for the MAX-H SUBGRAPH problem for edge weighted graphs, and various related problems, including computing the first k most significant bits of the distance product of two matrices. Some of our algorithms are based, in part, on fast matrix multiplication.",
        "published": "2006-09-04T08:08:00Z",
        "link": "http://arxiv.org/abs/cs/0609009v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Approximation Algorithms for the Bipartite Multi-cut Problem",
        "authors": [
            "Sreyash Kenkre",
            "Sundar Vishwanathan"
        ],
        "summary": "We introduce the {\\it Bipartite Multi-cut} problem. This is a generalization of the {\\it st-Min-cut} problem, is similar to the {\\it Multi-cut} problem (except for more stringent requirements) and also turns out to be an immediate generalization of the {\\it Min UnCut} problem. We prove that this problem is {\\bf NP}-hard and then present LP and SDP based approximation algorithms. While the LP algorithm is based on the Garg-Vazirani-Yannakakis algorithm for {\\it Multi-cut}, the SDP algorithm uses the {\\it Structure Theorem} of $\\ell_2^2$ Metrics.",
        "published": "2006-09-07T18:10:39Z",
        "link": "http://arxiv.org/abs/cs/0609031v2",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "CR-precis: A deterministic summary structure for update data streams",
        "authors": [
            "Sumit Ganguly",
            "Anirban Majumder"
        ],
        "summary": "We present the \\crprecis structure, that is a general-purpose, deterministic and sub-linear data structure for summarizing \\emph{update} data streams. The \\crprecis structure yields the \\emph{first deterministic sub-linear space/time algorithms for update streams} for answering a variety of fundamental stream queries, such as, (a) point queries, (b) range queries, (c) finding approximate frequent items, (d) finding approximate quantiles, (e) finding approximate hierarchical heavy hitters, (f) estimating inner-products, (g) near-optimal $B$-bucket histograms, etc..",
        "published": "2006-09-07T19:21:01Z",
        "link": "http://arxiv.org/abs/cs/0609032v3",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Exhausting Error-Prone Patterns in LDPC Codes",
        "authors": [
            "Chih-Chun Wang",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "It is proved in this work that exhaustively determining bad patterns in arbitrary, finite low-density parity-check (LDPC) codes, including stopping sets for binary erasure channels (BECs) and trapping sets (also known as near-codewords) for general memoryless symmetric channels, is an NP-complete problem, and efficient algorithms are provided for codes of practical short lengths n~=500. By exploiting the sparse connectivity of LDPC codes, the stopping sets of size <=13 and the trapping sets of size <=11 can be efficiently exhaustively determined for the first time, and the resulting exhaustive list is of great importance for code analysis and finite code optimization. The featured tree-based narrowing search distinguishes this algorithm from existing ones for which inexhaustive methods are employed. One important byproduct is a pair of upper bounds on the bit-error rate (BER) & frame-error rate (FER) iterative decoding performance of arbitrary codes over BECs that can be evaluated for any value of the erasure probability, including both the waterfall and the error floor regions. The tightness of these upper bounds and the exhaustion capability of the proposed algorithm are proved when combining an optimal leaf-finding module with the tree-based search. These upper bounds also provide a worst-case-performance guarantee which is crucial to optimizing LDPC codes for extremely low error rate applications, e.g., optical/satellite communications. Extensive numerical experiments are conducted that include both randomly and algebraically constructed LDPC codes, the results of which demonstrate the superior efficiency of the exhaustion algorithm and its significant value for finite length code optimization.",
        "published": "2006-09-11T00:50:16Z",
        "link": "http://arxiv.org/abs/cs/0609046v1",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT"
        ]
    },
    {
        "title": "k-Colorability of P5-free graphs",
        "authors": [
            "C. T. Hoang",
            "J. Sawada",
            "X. Shu"
        ],
        "summary": "A polynomial time algorithm that determines for a fixed integer k whether or not a P5-free graph can be k-colored is presented in this paper. If such a coloring exists, the algorithm will produce a valid k-coloring.",
        "published": "2006-09-14T21:30:23Z",
        "link": "http://arxiv.org/abs/cs/0609083v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "Improved Approximate String Matching and Regular Expression Matching on   Ziv-Lempel Compressed Texts",
        "authors": [
            "Philip Bille",
            "Rolf Fagerberg",
            "Inge Li Goertz"
        ],
        "summary": "We study the approximate string matching and regular expression matching problem for the case when the text to be searched is compressed with the Ziv-Lempel adaptive dictionary compression schemes. We present a time-space trade-off that leads to algorithms improving the previously known complexities for both problems. In particular, we significantly improve the space bounds, which in practical applications are likely to be a bottleneck.",
        "published": "2006-09-15T07:36:25Z",
        "link": "http://arxiv.org/abs/cs/0609085v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Minimum-weight Cycle Covers and Their Approximability",
        "authors": [
            "Bodo Manthey"
        ],
        "summary": "A cycle cover of a graph is a set of cycles such that every vertex is part of exactly one cycle. An L-cycle cover is a cycle cover in which the length of every cycle is in the set L.   We investigate how well L-cycle covers of minimum weight can be approximated. For undirected graphs, we devise a polynomial-time approximation algorithm that achieves a constant approximation ratio for all sets L. On the other hand, we prove that the problem cannot be approximated within a factor of 2-eps for certain sets L.   For directed graphs, we present a polynomial-time approximation algorithm that achieves an approximation ratio of O(n), where $n$ is the number of vertices. This is asymptotically optimal: We show that the problem cannot be approximated within a factor of o(n).   To contrast the results for cycle covers of minimum weight, we show that the problem of computing L-cycle covers of maximum weight can, at least in principle, be approximated arbitrarily well.",
        "published": "2006-09-18T13:22:39Z",
        "link": "http://arxiv.org/abs/cs/0609103v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Measuring Fundamental Properties of Real-World Complex Networks",
        "authors": [
            "Matthieu Latapy",
            "Clemence Magnien"
        ],
        "summary": "Complex networks, modeled as large graphs, received much attention during these last years. However, data on such networks is only available through intricate measurement procedures. Until recently, most studies assumed that these procedures eventually lead to samples large enough to be representative of the whole, at least concerning some key properties. This has crucial impact on network modeling and simulation, which rely on these properties.   Recent contributions proved that this approach may be misleading, but no solution has been proposed. We provide here the first practical way to distinguish between cases where it is indeed misleading, and cases where the observed properties may be trusted. It consists in studying how the properties of interest evolve when the sample grows, and in particular whether they reach a steady state or not.   In order to illustrate this method and to demonstrate its relevance, we apply it to data-sets on complex network measurements that are representative of the ones commonly used. The obtained results show that the method fulfills its goals very well. We moreover identify some properties which seem easier to evaluate in practice, thus opening interesting perspectives.",
        "published": "2006-09-20T13:38:41Z",
        "link": "http://arxiv.org/abs/cs/0609115v2",
        "categories": [
            "cs.NI",
            "cond-mat.stat-mech",
            "cs.DS"
        ]
    },
    {
        "title": "Theory and Practice of Triangle Problems in Very Large (Sparse   (Power-Law)) Graphs",
        "authors": [
            "Matthieu Latapy"
        ],
        "summary": "Finding, counting and/or listing triangles (three vertices with three edges) in large graphs are natural fundamental problems, which received recently much attention because of their importance in complex network analysis. We provide here a detailed state of the art on these problems, in a unified way. We note that, until now, authors paid surprisingly little attention to space complexity, despite its both fundamental and practical interest. We give the space complexities of known algorithms and discuss their implications. Then we propose improvements of a known algorithm, as well as a new algorithm, which are time optimal for triangle listing and beats previous algorithms concerning space complexity. They have the additional advantage of performing better on power-law graphs, which we also study. We finally show with an experimental study that these two algorithms perform very well in practice, allowing to handle cases that were previously out of reach.",
        "published": "2006-09-20T14:17:34Z",
        "link": "http://arxiv.org/abs/cs/0609116v1",
        "categories": [
            "cs.DS",
            "cond-mat.stat-mech",
            "cs.NI"
        ]
    },
    {
        "title": "Duality of Fix-Points for Distributive Lattices",
        "authors": [
            "Prahladavaradan Sampath"
        ],
        "summary": "We present a novel algorithm for calculating fix-points. The algorithm calculates fix-points of an endo-function f on a distributive lattice, by performing reachability computation a graph derived from the dual of f; this is in comparison to traditional algorithms that are based on iterated application of f until a fix-point is reached.",
        "published": "2006-09-21T17:34:25Z",
        "link": "http://arxiv.org/abs/cs/0609118v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Max-Cut and Max-Bisection are NP-hard on unit disk graphs",
        "authors": [
            "Josep Diaz",
            "Marcin Kaminski"
        ],
        "summary": "We prove that the Max-Cut and Max-Bisection problems are NP-hard on unit disk graphs. We also show that $\\lambda$-precision graphs are planar for $\\lambda$ > 1 / \\sqrt{2}$.",
        "published": "2006-09-22T18:17:12Z",
        "link": "http://arxiv.org/abs/cs/0609128v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Group Theoretical Formulation of Quantum Partial Search Algorithm",
        "authors": [
            "Vladimir E. Korepin",
            "Brenno C. Vallilo"
        ],
        "summary": "Searching and sorting used as a subroutine in many important algorithms. Quantum algorithm can find a target item in a database faster than any classical algorithm. One can trade accuracy for speed and find a part of the database (a block) containing the target item even faster, this is partial search. An example is the following: exact address of the target item is given by a sequence of many bits, but we need to know only some of them. More generally partial search considers the following problem: a database is separated into several blocks. We want to find a block with the target item, not the target item itself. In this paper we reformulate quantum partial search algorithm in terms of group theory.",
        "published": "2006-09-27T00:32:33Z",
        "link": "http://arxiv.org/abs/quant-ph/0609205v1",
        "categories": [
            "quant-ph",
            "cs.DS",
            "math.GR"
        ]
    },
    {
        "title": "Mining Generalized Graph Patterns based on User Examples",
        "authors": [
            "Pavel Dmitriev",
            "Carl Lagoze"
        ],
        "summary": "There has been a lot of recent interest in mining patterns from graphs. Often, the exact structure of the patterns of interest is not known. This happens, for example, when molecular structures are mined to discover fragments useful as features in chemical compound classification task, or when web sites are mined to discover sets of web pages representing logical documents. Such patterns are often generated from a few small subgraphs (cores), according to certain generalization rules (GRs). We call such patterns \"generalized patterns\"(GPs). While being structurally different, GPs often perform the same function in the network. Previously proposed approaches to mining GPs either assumed that the cores and the GRs are given, or that all interesting GPs are frequent. These are strong assumptions, which often do not hold in practical applications. In this paper, we propose an approach to mining GPs that is free from the above assumptions. Given a small number of GPs selected by the user, our algorithm discovers all GPs similar to the user examples. First, a machine learning-style approach is used to find the cores. Second, generalizations of the cores in the graph are computed to identify GPs. Evaluation on synthetic data, generated using real cores and GRs from biological and web domains, demonstrates effectiveness of our approach.",
        "published": "2006-09-27T18:42:44Z",
        "link": "http://arxiv.org/abs/cs/0609153v1",
        "categories": [
            "cs.DS",
            "cs.LG"
        ]
    },
    {
        "title": "Practical Entropy-Compressed Rank/Select Dictionary",
        "authors": [
            "Daisuke Okanohara",
            "Kunihiko Sadakane"
        ],
        "summary": "Rank/Select dictionaries are data structures for an ordered set $S \\subset \\{0,1,...,n-1\\}$ to compute $\\rank(x,S)$ (the number of elements in $S$ which are no greater than $x$), and $\\select(i,S)$ (the $i$-th smallest element in $S$), which are the fundamental components of \\emph{succinct data structures} of strings, trees, graphs, etc. In those data structures, however, only asymptotic behavior has been considered and their performance for real data is not satisfactory. In this paper, we propose novel four Rank/Select dictionaries, esp, recrank, vcode and sdarray, each of which is small if the number of elements in $S$ is small, and indeed close to $nH_0(S)$ ($H_0(S) \\leq 1$ is the zero-th order \\textit{empirical entropy} of $S$) in practice, and its query time is superior to the previous ones. Experimental results reveal the characteristics of our data structures and also show that these data structures are superior to existing implementations in both size and query time.",
        "published": "2006-09-29T23:52:09Z",
        "link": "http://arxiv.org/abs/cs/0610001v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Polynomial Time Algorithm for The Traveling Salesman Problem",
        "authors": [
            "Sergey Gubin"
        ],
        "summary": "The ATSP polytope can be expressed by asymmetric polynomial size linear program.",
        "published": "2006-10-09T13:15:12Z",
        "link": "http://arxiv.org/abs/cs/0610042v3",
        "categories": [
            "cs.DM",
            "cs.CC",
            "cs.DS",
            "F.2.0; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Streaming Maximum-Minimum Filter Using No More than Three Comparisons   per Element",
        "authors": [
            "Daniel Lemire"
        ],
        "summary": "The running maximum-minimum (max-min) filter computes the maxima and minima over running windows of size w. This filter has numerous applications in signal processing and time series analysis. We present an easy-to-implement online algorithm requiring no more than 3 comparisons per element, in the worst case. Comparatively, no algorithm is known to compute the running maximum (or minimum) filter in 1.5 comparisons per element, in the worst case. Our algorithm has reduced latency and memory usage.",
        "published": "2006-10-09T22:09:42Z",
        "link": "http://arxiv.org/abs/cs/0610046v5",
        "categories": [
            "cs.DS",
            "F.2.1"
        ]
    },
    {
        "title": "Peano Count Trees (P-Trees) and Rule Association Mining for Gene   Expression Profiling of Microarray Data",
        "authors": [
            "Willy Valdivia-Granda",
            "William Perrizo",
            "Edward Deckard",
            "Francis Larson"
        ],
        "summary": "The greatest challenge in maximizing the use of gene expression data is to develop new computational tools capable of interconnecting and interpreting the results from different organisms and experimental settings. We propose an integrative and comprehensive approach including a super-chip containing data from microarray experiments collected on different species subjected to hypoxic and anoxic stress. A data mining technology called Peano count tree (P-trees) is used to represent genomic data in multidimensions. Each microarray spot is presented as a pixel with its corresponding red/green intensity feature bands. Each bad is stored separately in a reorganized 8-separate (bSQ) file format. Each bSQ is converted to a quadrant base tree structure (P-tree) from which a superchip is represented as expression P-trees (EP-trees) and repression P-trees (RP-trees). The use of association rule mining is proposed to derived to meanigingfully organize signal transduction pathways taking in consideration evolutionary considerations. We argue that the genetic constitution of an organism (K) can be represented by the total number of genes belonging to two groups. The group X constitutes genes (X1,Xn) and they can be represented as 1 or 0 depending on whether the gene was expressed or not. The second group of Y genes (Y1,Yn) is expressed at different levels. These genes have a very high repression, high expression, very repressed or highly repressed. However, many genes of the group Y are specie specific and modulated by the products and combinations of genes of the group X. In this paper, we introduce the dSQ and P-tree technology; the biological implications of association rule mining using X and Y gene groups and some advances in the integration of this information using the BRAIN architecture.",
        "published": "2006-10-12T19:55:32Z",
        "link": "http://arxiv.org/abs/cs/0610076v2",
        "categories": [
            "cs.DS",
            "cs.IR",
            "q-bio.MN"
        ]
    },
    {
        "title": "Approximate Convex Optimization by Online Game Playing",
        "authors": [
            "Elad Hazan"
        ],
        "summary": "Lagrangian relaxation and approximate optimization algorithms have received much attention in the last two decades. Typically, the running time of these methods to obtain a $\\epsilon$ approximate solution is proportional to $\\frac{1}{\\epsilon^2}$. Recently, Bienstock and Iyengar, following Nesterov, gave an algorithm for fractional packing linear programs which runs in $\\frac{1}{\\epsilon}$ iterations. The latter algorithm requires to solve a convex quadratic program every iteration - an optimization subroutine which dominates the theoretical running time.   We give an algorithm for convex programs with strictly convex constraints which runs in time proportional to $\\frac{1}{\\epsilon}$. The algorithm does NOT require to solve any quadratic program, but uses gradient steps and elementary operations only. Problems which have strictly convex constraints include maximum entropy frequency estimation, portfolio optimization with loss risk constraints, and various computational problems in signal processing.   As a side product, we also obtain a simpler version of Bienstock and Iyengar's result for general linear programming, with similar running time.   We derive these algorithms using a new framework for deriving convex optimization algorithms from online game playing algorithms, which may be of independent interest.",
        "published": "2006-10-19T22:10:32Z",
        "link": "http://arxiv.org/abs/cs/0610119v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Report on article: P=NP Linear programming formulation of the Traveling   Salesman Problem",
        "authors": [
            "Radoslaw Hofman"
        ],
        "summary": "This article presents counter examples for three articles claiming that P=NP. Articles for which it applies are: Moustapha Diaby \"P = NP: Linear programming formulation of the traveling salesman problem\" and \"Equality of complexity classes P and NP: Linear programming formulation of the quadratic assignment problem\", and also Sergey Gubin \"A Polynomial Time Algorithm for The Traveling Salesman Problem\"",
        "published": "2006-10-20T14:01:22Z",
        "link": "http://arxiv.org/abs/cs/0610125v4",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "Hierarchical Bin Buffering: Online Local Moments for Dynamic External   Memory Arrays",
        "authors": [
            "Daniel Lemire",
            "Owen Kaser"
        ],
        "summary": "Local moments are used for local regression, to compute statistical measures such as sums, averages, and standard deviations, and to approximate probability distributions. We consider the case where the data source is a very large I/O array of size n and we want to compute the first N local moments, for some constant N. Without precomputation, this requires O(n) time. We develop a sequence of algorithms of increasing sophistication that use precomputation and additional buffer space to speed up queries. The simpler algorithms partition the I/O array into consecutive ranges called bins, and they are applicable not only to local-moment queries, but also to algebraic queries (MAX, AVERAGE, SUM, etc.). With N buffers of size sqrt{n}, time complexity drops to O(sqrt n). A more sophisticated approach uses hierarchical buffering and has a logarithmic time complexity (O(b log_b n)), when using N hierarchical buffers of size n/b. Using Overlapped Bin Buffering, we show that only a single buffer is needed, as with wavelet-based algorithms, but using much less storage. Applications exist in multidimensional and statistical databases over massive data sets, interactive image processing, and visualization.",
        "published": "2006-10-21T00:30:57Z",
        "link": "http://arxiv.org/abs/cs/0610128v3",
        "categories": [
            "cs.DS",
            "cs.DB",
            "H.3.5; G.1.1"
        ]
    },
    {
        "title": "Nonlinear Estimators and Tail Bounds for Dimension Reduction in $l_1$   Using Cauchy Random Projections",
        "authors": [
            "Ping Li",
            "Trevor J. Hastie",
            "Kenneth W. Church"
        ],
        "summary": "For dimension reduction in $l_1$, the method of {\\em Cauchy random projections} multiplies the original data matrix $\\mathbf{A} \\in\\mathbb{R}^{n\\times D}$ with a random matrix $\\mathbf{R} \\in \\mathbb{R}^{D\\times k}$ ($k\\ll\\min(n,D)$) whose entries are i.i.d. samples of the standard Cauchy C(0,1). Because of the impossibility results, one can not hope to recover the pairwise $l_1$ distances in $\\mathbf{A}$ from $\\mathbf{B} = \\mathbf{AR} \\in \\mathbb{R}^{n\\times k}$, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computation, information retrieval, learning, and data mining.   We propose three types of nonlinear estimators: the bias-corrected sample median estimator, the bias-corrected geometric mean estimator, and the bias-corrected maximum likelihood estimator. The sample median estimator and the geometric mean estimator are asymptotically (as $k\\to \\infty$) equivalent but the latter is more accurate at small $k$. We derive explicit tail bounds for the geometric mean estimator and establish an analog of the Johnson-Lindenstrauss (JL) lemma for dimension reduction in $l_1$, which is weaker than the classical JL lemma for dimension reduction in $l_2$.   Asymptotically, both the sample median estimator and the geometric mean estimators are about 80% efficient compared to the maximum likelihood estimator (MLE). We analyze the moments of the MLE and propose approximating the distribution of the MLE by an inverse Gaussian.",
        "published": "2006-10-27T07:08:51Z",
        "link": "http://arxiv.org/abs/cs/0610155v1",
        "categories": [
            "cs.DS",
            "cs.IR",
            "cs.LG"
        ]
    },
    {
        "title": "A Taxonomy of Peer-to-Peer Based Complex Queries: a Grid perspective",
        "authors": [
            "Rajiv Ranjan",
            "Aaron Harwood",
            "Rajkumar Buyya"
        ],
        "summary": "Grid superscheduling requires support for efficient and scalable discovery of resources. Resource discovery activities involve searching for the appropriate resource types that match the user's job requirements. To accomplish this goal, a resource discovery system that supports the desired look-up operation is mandatory. Various kinds of solutions to this problem have been suggested, including the centralised and hierarchical information server approach. However, both of these approaches have serious limitations in regards to scalability, fault-tolerance and network congestion. To overcome these limitations, organising resource information using Peer-to-Peer (P2P) network model has been proposed. Existing approaches advocate an extension to structured P2P protocols, to support the Grid resource information system (GRIS). In this paper, we identify issues related to the design of such an efficient, scalable, fault-tolerant, consistent and practical GRIS system using a P2P network model. We compile these issues into various taxonomies in sections III and IV. Further, we look into existing works that apply P2P based network protocols to GRIS. We think that this taxonomy and its mapping to relevant systems would be useful for academic and industry based researchers who are engaged in the design of scalable Grid systems.",
        "published": "2006-10-30T08:30:17Z",
        "link": "http://arxiv.org/abs/cs/0610163v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "A Fixed-Parameter Algorithm for #SAT with Parameter Incidence Treewidth",
        "authors": [
            "Marko Samer",
            "Stefan Szeider"
        ],
        "summary": "We present an efficient fixed-parameter algorithm for #SAT parameterized by the incidence treewidth, i.e., the treewidth of the bipartite graph whose vertices are the variables and clauses of the given CNF formula; a variable and a clause are joined by an edge if and only if the variable occurs in the clause. Our algorithm runs in time O(4^k k l N), where k denotes the incidence treewidth, l denotes the size of a largest clause, and N denotes the number of nodes of the tree-decomposition.",
        "published": "2006-10-31T12:58:36Z",
        "link": "http://arxiv.org/abs/cs/0610174v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.LO",
            "F.2.2; F.4.1"
        ]
    },
    {
        "title": "A near-optimal fully dynamic distributed algorithm for maintaining   sparse spanners",
        "authors": [
            "Michael Elkin"
        ],
        "summary": "In this paper we devise an extremely efficient fully dynamic distributed algorithm for maintaining sparse spanners. Our resuls also include the first fully dynamic centralized algorithm for the problem with non-trivial bounds for both incremental and decremental update. Finally, we devise a very efficient streaming algorithm for the problem.",
        "published": "2006-11-01T09:36:20Z",
        "link": "http://arxiv.org/abs/cs/0611001v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Why Linear Programming cannot solve large instances of NP-complete   problems in polynomial time",
        "authors": [
            "Radoslaw Hofman"
        ],
        "summary": "This article discusses ability of Linear Programming models to be used as solvers of NP-complete problems. Integer Linear Programming is known as NP-complete problem, but non-integer Linear Programming problems can be solved in polynomial time, what places them in P class. During past three years there appeared some articles using LP to solve NP-complete problems. This methods use large number of variables (O(n^9)) solving correctly almost all instances that can be solved in reasonable time. Can they solve infinitively large instances? This article gives answer to this question.",
        "published": "2006-11-02T08:40:53Z",
        "link": "http://arxiv.org/abs/cs/0611008v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "cs.NA",
            "F.1; F.2"
        ]
    },
    {
        "title": "Algorithmic Aspects of a General Modular Decomposition Theory",
        "authors": [
            "Binh-Minh Bui-Xuan",
            "Michel Habib",
            "Vincent Limouzy",
            "Fabien De Montgolfier"
        ],
        "summary": "A new general decomposition theory inspired from modular graph decomposition is presented. This helps unifying modular decomposition on different structures, including (but not restricted to) graphs. Moreover, even in the case of graphs, the terminology ``module'' not only captures the classical graph modules but also allows to handle 2-connected components, star-cutsets, and other vertex subsets. The main result is that most of the nice algorithmic tools developed for modular decomposition of graphs still apply efficiently on our generalisation of modules. Besides, when an essential axiom is satisfied, almost all the important properties can be retrieved. For this case, an algorithm given by Ehrenfeucht, Gabow, McConnell and Sullivan 1994 is generalised and yields a very efficient solution to the associated decomposition problem.",
        "published": "2006-11-04T18:32:23Z",
        "link": "http://arxiv.org/abs/cs/0611019v2",
        "categories": [
            "cs.DS",
            "E.1; G.2; G.2.2"
        ]
    },
    {
        "title": "Faster Streaming algorithms for graph spanners",
        "authors": [
            "Surender Baswana"
        ],
        "summary": "Given an undirected graph $G=(V,E)$ on $n$ vertices, $m$ edges, and an integer $t\\ge 1$, a subgraph $(V,E_S)$, $E_S\\subseteq E$ is called a $t$-spanner if for any pair of vertices $u,v \\in V$, the distance between them in the subgraph is at most $t$ times the actual distance. We present streaming algorithms for computing a $t$-spanner of essentially optimal size-stretch trade offs for any undirected graph.   Our first algorithm is for the classical streaming model and works for unweighted graphs only. The algorithm performs a single pass on the stream of edges and requires $O(m)$ time to process the entire stream of edges. This drastically improves the previous best single pass streaming algorithm for computing a $t$-spanner which requires $\\theta(mn^{\\frac{2}{t}})$ time to process the stream and computes spanner with size slightly larger than the optimal.   Our second algorithm is for {\\em StreamSort} model introduced by Aggarwal et al. [FOCS 2004], which is the streaming model augmented with a sorting primitive. The {\\em StreamSort} model has been shown to be a more powerful and still very realistic model than the streaming model for massive data sets applications. Our algorithm, which works of weighted graphs as well, performs $O(t)$ passes using $O(\\log n)$ bits of working memory only.   Our both the algorithms require elementary data structures.",
        "published": "2006-11-06T03:09:05Z",
        "link": "http://arxiv.org/abs/cs/0611023v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "T-Theory Applications to Online Algorithms for the Server Problem",
        "authors": [
            "Lawrence L. Larmore",
            "James A. Oravec"
        ],
        "summary": "Although largely unnoticed by the online algorithms community, T-theory, a field of discrete mathematics, has contributed to the development of several online algorithms for the k-server problem. A brief summary of the k-server problem, and some important application concepts of T-theory, are given. Additionally, a number of known k-server results are restated using the established terminology of T-theory. Lastly, a previously unpublished 3-competitiveness proof, using T-theory, for the Harmonic algorithm for two servers is presented.",
        "published": "2006-11-18T19:50:57Z",
        "link": "http://arxiv.org/abs/cs/0611088v1",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Analysis of an Efficient Distributed Algorithm for Mutual Exclusion   (Average-Case Analysis of Path Reversal)",
        "authors": [
            "Christian Lavault"
        ],
        "summary": "The algorithm analysed by Na\\\"{i}mi, Trehe and Arnold was the very first distributed algorithm to solve the mutual exclusion problem in complete networks by using a dynamic logical tree structure as its basic distributed data structure, viz. a path reversal transformation in rooted n-node trees; besides, it was also the first one to achieve a logarithmic average-case message complexity. The present paper proposes a direct and general approach to compute the moments of the cost of path reversal. It basically uses one-one correspondences between combinatorial structures and the associated probability generating functions: the expected cost of path reversal is thus proved to be exactly $H_{n-1}$. Moreover, time and message complexity of the algorithm as well as randomized bounds on its worst-case message complexity in arbitrary networks are also given. The average-case analysis of path reversal and the analysis of this distributed algorithm for mutual exclusion are thus fully completed in the paper. The general techniques used should also prove available and fruitful when adapted to the most efficient recent tree-based distributed algorithms for mutual exclusion which require powerful tools, particularly for average-case analyses.",
        "published": "2006-11-20T22:02:29Z",
        "link": "http://arxiv.org/abs/cs/0611098v1",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Fourier meets Möbius: fast subset convolution",
        "authors": [
            "Andreas Björklund",
            "Thore Husfeldt",
            "Petteri Kaski",
            "Mikko Koivisto"
        ],
        "summary": "We present a fast algorithm for the subset convolution problem: given functions f and g defined on the lattice of subsets of an n-element set N, compute their subset convolution f*g, defined for all S\\subseteq N by (f * g)(S) = \\sum_{T \\subseteq S}f(T) g(S\\setminus T), where addition and multiplication is carried out in an arbitrary ring. Via M\\\"{o}bius transform and inversion, our algorithm evaluates the subset convolution in O(n^2 2^n) additions and multiplications, substantially improving upon the straightforward O(3^n) algorithm. Specifically, if the input functions have an integer range {-M,-M+1,...,M}, their subset convolution over the ordinary sum-product ring can be computed in O^*(2^n log M) time; the notation O^* suppresses polylogarithmic factors. Furthermore, using a standard embedding technique we can compute the subset convolution over the max-sum or min-sum semiring in O^*(2^n M) time. To demonstrate the applicability of fast subset convolution, we present the first O^*(2^k n^2 + n m) algorithm for the minimum Steiner tree problem in graphs with n vertices, k terminals, and m edges with bounded integer weights, improving upon the O^*(3^k n + 2^k n^2 + n m) time bound of the classical Dreyfus-Wagner algorithm. We also discuss extensions to recent O^*(2^n)-time algorithms for covering and partitioning problems (Bj\\\"{o}rklund and Husfeldt, FOCS 2006; Koivisto, FOCS 2006).",
        "published": "2006-11-21T08:34:30Z",
        "link": "http://arxiv.org/abs/cs/0611101v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "math.CO",
            "F.2.1; F.2.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Rectangular Layouts and Contact Graphs",
        "authors": [
            "Adam L. Buchsbaum",
            "Emden R. Gansner",
            "Cecilia M. Procopiuc",
            "Suresh Venkatasubramanian"
        ],
        "summary": "Contact graphs of isothetic rectangles unify many concepts from applications including VLSI and architectural design, computational geometry, and GIS. Minimizing the area of their corresponding {\\em rectangular layouts} is a key problem. We study the area-optimization problem and show that it is NP-hard to find a minimum-area rectangular layout of a given contact graph. We present O(n)-time algorithms that construct $O(n^2)$-area rectangular layouts for general contact graphs and $O(n\\log n)$-area rectangular layouts for trees. (For trees, this is an $O(\\log n)$-approximation algorithm.) We also present an infinite family of graphs (rsp., trees) that require $\\Omega(n^2)$ (rsp., $\\Omega(n\\log n)$) area.   We derive these results by presenting a new characterization of graphs that admit rectangular layouts using the related concept of {\\em rectangular duals}. A corollary to our results relates the class of graphs that admit rectangular layouts to {\\em rectangle of influence drawings}.",
        "published": "2006-11-21T15:03:37Z",
        "link": "http://arxiv.org/abs/cs/0611107v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Longest Common Pattern between two Permutations",
        "authors": [
            "Dominique Rossin",
            "Mathilde Bouvel"
        ],
        "summary": "In this paper, we give a polynomial (O(n^8)) algorithm for finding a longest common pattern between two permutations of size n given that one is separable. We also give an algorithm for general permutations whose complexity depends on the length of the longest simple permutation involved in one of our permutations.",
        "published": "2006-11-22T10:47:05Z",
        "link": "http://arxiv.org/abs/math/0611679v1",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "05A05; 05C12; 05C85; 05C05; 90C39"
        ]
    },
    {
        "title": "Very Sparse Stable Random Projections, Estimators and Tail Bounds for   Stable Random Projections",
        "authors": [
            "Ping Li"
        ],
        "summary": "This paper will focus on three different aspects in improving the current practice of stable random projections.   Firstly, we propose {\\em very sparse stable random projections} to significantly reduce the processing and storage cost, by replacing the $\\alpha$-stable distribution with a mixture of a symmetric $\\alpha$-Pareto distribution (with probability $\\beta$, $0<\\beta\\leq1$) and a point mass at the origin (with a probability $1-\\beta$). This leads to a significant $\\frac{1}{\\beta}$-fold speedup for small $\\beta$.   Secondly, we provide an improved estimator for recovering the original $l_\\alpha$ norms from the projected data. The standard estimator is based on the (absolute) sample median, while we suggest using the geometric mean. The geometric mean estimator we propose is strictly unbiased and is easier to study. Moreover, the geometric mean estimator is more accurate, especially non-asymptotically.   Thirdly, we provide an adequate answer to the basic question of how many projections (samples) are needed for achieving some pre-specified level of accuracy. \\cite{Proc:Indyk_FOCS00,Article:Indyk_TKDE03} did not provide a criterion that can be used in practice. The geometric mean estimator we propose allows us to derive sharp tail bounds which can be expressed in exponential forms with constants explicitly given.",
        "published": "2006-11-22T11:38:25Z",
        "link": "http://arxiv.org/abs/cs/0611114v2",
        "categories": [
            "cs.DS",
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Discovering Network Topology in the Presence of Byzantine Faults",
        "authors": [
            "Mikhail Nesterenko",
            "Sébastien Tixeuil"
        ],
        "summary": "We study the problem of Byzantine-robust topology discovery in an arbitrary asynchronous network. We formally state the weak and strong versions of the problem. The weak version requires that either each node discovers the topology of the network or at least one node detects the presence of a faulty node. The strong version requires that each node discovers the topology regardless of faults. We focus on non-cryptographic solutions to these problems. We explore their bounds. We prove that the weak topology discovery problem is solvable only if the connectivity of the network exceeds the number of faults in the system. Similarly, we show that the strong version of the problem is solvable only if the network connectivity is more than twice the number of faults. We present solutions to both versions of the problem. The presented algorithms match the established graph connectivity bounds. The algorithms do not require the individual nodes to know either the diameter or the size of the network. The message complexity of both programs is low polynomial with respect to the network size. We describe how our solutions can be extended to add the property of termination, handle topology changes and perform neighborhood discovery.",
        "published": "2006-11-22T18:25:43Z",
        "link": "http://arxiv.org/abs/cs/0611116v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.OS"
        ]
    },
    {
        "title": "2FACE: Bi-Directional Face Traversal for Efficient Geometric Routing",
        "authors": [
            "Mark Miyashita",
            "Mikhail Nesterenko"
        ],
        "summary": "We propose bi-directional face traversal algorithm $2FACE$ to shorten the path the message takes to reach the destination in geometric routing. Our algorithm combines the practicality of the best single-direction traversal algorithms with the worst case message complexity of $O(|E|)$, where $E$ is the number of network edges. We apply $2FACE$ to a variety of geometric routing algorithms. Our simulation results indicate that bi-directional face traversal decreases the latency of message delivery two to three times compared to single direction face traversal. The thus selected path approaches the shortest possible route. This gain in speed comes with a similar message overhead increase. We describe an algorithm which compensates for this message overhead by recording the preferable face traversal direction. Thus, if a source has several messages to send to the destination, the subsequent messages follow the shortest route. Our simulation results show that with most geometric routing algorithms the message overhead of finding the short route by bi-directional face traversal is compensated within two to four repeat messages.",
        "published": "2006-11-22T19:28:31Z",
        "link": "http://arxiv.org/abs/cs/0611117v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.OS"
        ]
    },
    {
        "title": "Lossless fitness inheritance in genetic algorithms for decision trees",
        "authors": [
            "Dimitris Kalles",
            "Athanassios Papagelis"
        ],
        "summary": "When genetic algorithms are used to evolve decision trees, key tree quality parameters can be recursively computed and re-used across generations of partially similar decision trees. Simply storing instance indices at leaves is enough for fitness to be piecewise computed in a lossless fashion. We show the derivation of the (substantial) expected speed-up on two bounding case problems and trace the attractive property of lossless fitness inheritance to the divide-and-conquer nature of decision trees. The theoretical results are supported by experimental evidence.",
        "published": "2006-11-30T15:20:15Z",
        "link": "http://arxiv.org/abs/cs/0611166v2",
        "categories": [
            "cs.AI",
            "cs.DS",
            "cs.NE"
        ]
    },
    {
        "title": "Using Combinatorics to Prune Search Trees: Independent and Dominating   Set",
        "authors": [
            "Fedor V. Fomin",
            "Serge Gaspers",
            "Saket Saurabh",
            "Alexey A. Stepanov"
        ],
        "summary": "This paper has been withdrawn by the author.",
        "published": "2006-12-05T14:42:56Z",
        "link": "http://arxiv.org/abs/cs/0612028v2",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Estimating Aggregate Properties on Probabilistic Streams",
        "authors": [
            "Andrew McGregor",
            "S. Muthukrishnan"
        ],
        "summary": "The probabilistic-stream model was introduced by Jayram et al. \\cite{JKV07}. It is a generalization of the data stream model that is suited to handling ``probabilistic'' data where each item of the stream represents a probability distribution over a set of possible events. Therefore, a probabilistic stream determines a distribution over potentially a very large number of classical \"deterministic\" streams where each item is deterministically one of the domain values. The probabilistic model is applicable for not only analyzing streams where the input has uncertainties (such as sensor data streams that measure physical processes) but also where the streams are derived from the input data by post-processing, such as tagging or reconciling inconsistent and poor quality data.   We present streaming algorithms for computing commonly used aggregates on a probabilistic stream. We present the first known, one pass streaming algorithm for estimating the \\AVG, improving results in \\cite{JKV07}. We present the first known streaming algorithms for estimating the number of \\DISTINCT items on probabilistic streams. Further, we present extensions to other aggregates such as the repeat rate, quantiles, etc. In all cases, our algorithms work with provable accuracy guarantees and within the space constraints of the data stream model.",
        "published": "2006-12-05T23:34:52Z",
        "link": "http://arxiv.org/abs/cs/0612031v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Acronym-Meaning Extraction from Corpora Using Multi-Tape Weighted   Finite-State Machines",
        "authors": [
            "André Kempe"
        ],
        "summary": "The automatic extraction of acronyms and their meaning from corpora is an important sub-task of text mining. It can be seen as a special case of string alignment, where a text chunk is aligned with an acronym. Alternative alignments have different cost, and ideally the least costly one should give the correct meaning of the acronym. We show how this approach can be implemented by means of a 3-tape weighted finite-state machine (3-WFSM) which reads a text chunk on tape 1 and an acronym on tape 2, and generates all alternative alignments on tape 3. The 3-WFSM can be automatically generated from a simple regular expression. No additional algorithms are required at any stage. Our 3-WFSM has a size of 27 states and 64 transitions, and finds the best analysis of an acronym in a few milliseconds.",
        "published": "2006-12-06T10:13:12Z",
        "link": "http://arxiv.org/abs/cs/0612033v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "cs.SC",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Least Significant Digit First Presburger Automata",
        "authors": [
            "Jérôme Leroux"
        ],
        "summary": "Since 1969 \\cite{C-MST69,S-SMJ77}, we know that any Presburger-definable set \\cite{P-PCM29} (a set of integer vectors satisfying a formula in the first-order additive theory of the integers) can be represented by a state-based symmbolic representation, called in this paper Finite Digit Vector Automata (FDVA). Efficient algorithms for manipulating these sets have been recently developed. However, the problem of deciding if a FDVA represents such a set, is a well-known hard problem first solved by Muchnik in 1991 with a quadruply-exponential time algorithm. In this paper, we show how to determine in polynomial time whether a FDVA represents a Presburger-definable set, and we provide in this positive case a polynomial time algorithm that constructs a Presburger-formula that defines the same set.",
        "published": "2006-12-06T14:55:36Z",
        "link": "http://arxiv.org/abs/cs/0612037v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Viterbi Algorithm Generalized for n-Tape Best-Path Search",
        "authors": [
            "André Kempe"
        ],
        "summary": "We present a generalization of the Viterbi algorithm for identifying the path with minimal (resp. maximal) weight in a n-tape weighted finite-state machine (n-WFSM), that accepts a given n-tuple of input strings (s_1,... s_n). It also allows us to compile the best transduction of a given input n-tuple by a weighted (n+m)-WFSM (transducer) with n input and m output tapes. Our algorithm has a worst-case time complexity of O(|s|^n |E| log (|s|^n |Q|)), where n and |s| are the number and average length of the strings in the n-tuple, and |Q| and |E| the number of states and transitions in the n-WFSM, respectively. A straight forward alternative, consisting in intersection followed by classical shortest-distance search, operates in O(|s|^n (|E|+|Q|) log (|s|^n |Q|)) time.",
        "published": "2006-12-07T08:42:46Z",
        "link": "http://arxiv.org/abs/cs/0612041v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "cs.SC",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Budget Optimization in Search-Based Advertising Auctions",
        "authors": [
            "Jon Feldman",
            "S. Muthukrishnan",
            "Martin Pal",
            "Cliff Stein"
        ],
        "summary": "Internet search companies sell advertisement slots based on users' search queries via an auction. While there has been a lot of attention on the auction process and its game-theoretic aspects, our focus is on the advertisers. In particular, the advertisers have to solve a complex optimization problem of how to place bids on the keywords of their interest so that they can maximize their return (the number of user clicks on their ads) for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywords works well. More precisely, this strategy gets at least 1-1/e fraction of the maximum clicks possible. Such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem.",
        "published": "2006-12-08T17:33:54Z",
        "link": "http://arxiv.org/abs/cs/0612052v1",
        "categories": [
            "cs.DS",
            "cs.CE",
            "cs.GT"
        ]
    },
    {
        "title": "Linear Probing with Constant Independence",
        "authors": [
            "Anna Pagh",
            "Rasmus Pagh",
            "Milan Ruzic"
        ],
        "summary": "Hashing with linear probing dates back to the 1950s, and is among the most studied algorithms. In recent years it has become one of the most important hash table organizations since it uses the cache of modern computers very well. Unfortunately, previous analysis rely either on complicated and space consuming hash functions, or on the unrealistic assumption of free access to a truly random hash function. Already Carter and Wegman, in their seminal paper on universal hashing, raised the question of extending their analysis to linear probing. However, we show in this paper that linear probing using a pairwise independent family may have expected {\\em logarithmic} cost per operation. On the positive side, we show that 5-wise independence is enough to ensure constant expected time per operation. This resolves the question of finding a space and time efficient hash function that provably ensures good performance for linear probing.",
        "published": "2006-12-08T22:50:24Z",
        "link": "http://arxiv.org/abs/cs/0612055v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Adaptive Simulated Annealing: A Near-optimal Connection between Sampling   and Counting",
        "authors": [
            "Daniel Stefankovic",
            "Santosh Vempala",
            "Eric Vigoda"
        ],
        "summary": "We present a near-optimal reduction from approximately counting the cardinality of a discrete set to approximately sampling elements of the set. An important application of our work is to approximating the partition function $Z$ of a discrete system, such as the Ising model, matchings or colorings of a graph. The typical approach to estimating the partition function $Z(\\beta^*)$ at some desired inverse temperature $\\beta^*$ is to define a sequence, which we call a {\\em cooling schedule}, $\\beta_0=0<\\beta_1<...<\\beta_\\ell=\\beta^*$ where Z(0) is trivial to compute and the ratios $Z(\\beta_{i+1})/Z(\\beta_i)$ are easy to estimate by sampling from the distribution corresponding to $Z(\\beta_i)$. Previous approaches required a cooling schedule of length $O^*(\\ln{A})$ where $A=Z(0)$, thereby ensuring that each ratio $Z(\\beta_{i+1})/Z(\\beta_i)$ is bounded. We present a cooling schedule of length $\\ell=O^*(\\sqrt{\\ln{A}})$.   For well-studied problems such as estimating the partition function of the Ising model, or approximating the number of colorings or matchings of a graph, our cooling schedule is of length $O^*(\\sqrt{n})$, which implies an overall savings of $O^*(n)$ in the running time of the approximate counting algorithm (since roughly $\\ell$ samples are needed to estimate each ratio).",
        "published": "2006-12-10T20:00:38Z",
        "link": "http://arxiv.org/abs/cs/0612058v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.3"
        ]
    },
    {
        "title": "Fast linear algebra is stable",
        "authors": [
            "James Demmel",
            "Ioana Dumitriu",
            "Olga Holtz"
        ],
        "summary": "In an earlier paper, we showed that a large class of fast recursive matrix multiplication algorithms is stable in a normwise sense, and that in fact if multiplication of $n$-by-$n$ matrices can be done by any algorithm in $O(n^{\\omega + \\eta})$ operations for any $\\eta > 0$, then it can be done stably in $O(n^{\\omega + \\eta})$ operations for any $\\eta > 0$. Here we extend this result to show that essentially all standard linear algebra operations, including LU decomposition, QR decomposition, linear equation solving, matrix inversion, solving least squares problems, (generalized) eigenvalue problems and the singular value decomposition can also be done stably (in a normwise sense) in $O(n^{\\omega + \\eta})$ operations.",
        "published": "2006-12-10T20:44:57Z",
        "link": "http://arxiv.org/abs/math/0612264v3",
        "categories": [
            "math.NA",
            "cs.CC",
            "cs.DS",
            "65Y20, 65F30, 65G50, 68Q17, 68Q25"
        ]
    },
    {
        "title": "The Common Prefix Problem On Trees",
        "authors": [
            "Sreyash Kenkre",
            "Sundar Vishwanathan"
        ],
        "summary": "We present a theoretical study of a problem arising in database query optimization, which we call as The Common Prefix Problem. We present a $(1-o(1))$ factor approximation algorithm for this problem, when the underlying graph is a binary tree. We then use a result of Feige and Kogan to show that even on stars, the problem is hard to approximate.",
        "published": "2006-12-11T12:32:02Z",
        "link": "http://arxiv.org/abs/cs/0612060v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Stochastic Models for Budget Optimization in Search-Based Advertising",
        "authors": [
            "S. Muthukrishnan",
            "Martin Pal",
            "Zoya Svitkina"
        ],
        "summary": "Internet search companies sell advertisement slots based on users' search queries via an auction. Advertisers have to determine how to place bids on the keywords of their interest in order to maximize their return for a given budget: this is the budget optimization problem. The solution depends on the distribution of future queries.   In this paper, we formulate stochastic versions of the budget optimization problem based on natural probabilistic models of distribution over future queries, and address two questions that arise.   [Evaluation] Given a solution, can we evaluate the expected value of the objective function?   [Optimization] Can we find a solution that maximizes the objective function in expectation?   Our main results are approximation and complexity results for these two problems in our three stochastic models. In particular, our algorithmic results show that simple prefix strategies that bid on all cheap keywords up to some level are either optimal or good approximations for many cases; we show other cases to be NP-hard.",
        "published": "2006-12-14T21:13:57Z",
        "link": "http://arxiv.org/abs/cs/0612072v2",
        "categories": [
            "cs.DS",
            "cs.GT"
        ]
    },
    {
        "title": "Energy Efficient Randomized Communication in Unknown AdHoc Networks",
        "authors": [
            "Petra Berenbrink",
            "Colin Cooper",
            "Zengjian Hu"
        ],
        "summary": "This paper studies broadcasting and gossiping algorithms in random and general AdHoc networks. Our goal is not only to minimise the broadcasting and gossiping time, but also to minimise the energy consumption, which is measured in terms of the total number of messages (or transmissions) sent. We assume that the nodes of the network do not know the network, and that they can only send with a fixed power, meaning they can not adjust the areas sizes that their messages cover. We believe that under these circumstances the number of transmissions is a very good measure for the overall energy consumption.   For random networks, we present a broadcasting algorithm where every node transmits at most once. We show that our algorithm broadcasts in $O(\\log n)$ steps, w.h.p, where $n$ is the number of nodes. We then present a $O(d \\log n)$ ($d$ is the expected degree) gossiping algorithm using $O(\\log n)$ messages per node.   For general networks with known diameter $D$, we present a randomised broadcasting algorithm with optimal broadcasting time $O(D \\log (n/D) + \\log^2 n)$ that uses an expected number of $O(\\log^2 n / \\log (n/D))$ transmissions per node. We also show a tradeoff result between the broadcasting time and the number of transmissions: we construct a network such that any oblivious algorithmusing a time-invariant distribution requires $\\Omega(\\log^2 n / \\log (n/D))$ messages per node in order to finish broadcasting in optimal time. This demonstrates the tightness of our upper bound. We also show that no oblivious algorithm can complete broadcasting w.h.p. using $o(\\log n)$ messages per node.",
        "published": "2006-12-15T03:43:39Z",
        "link": "http://arxiv.org/abs/cs/0612074v1",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Non-Clairvoyant Batch Sets Scheduling: Fairness is Fair enough",
        "authors": [
            "Julien Robert",
            "Nicolas Schabanel"
        ],
        "summary": "Scheduling questions arise naturally in many different areas among which operating system design, compiling,... In real life systems, the characteristics of the jobs (such as release time and processing time) are usually unknown and unpredictable beforehand. The system is typically unaware of the remaining work in each job or of the ability of the job to take advantage of more resources. Following these observations, we adopt the job model by Edmonds et al (2000, 2003) in which the jobs go through a sequence of different phases. Each phase consists of a certain quantity of work and a speed-up function that models how it takes advantage of the number of processors it receives. We consider the non-clairvoyant online setting where a collection of jobs arrives at time 0. We consider the metrics setflowtime introduced by Robert et al (2007). The goal is to minimize the sum of the completion time of the sets, where a set is completed when all of its jobs are done. If the input consists of a single set of jobs, this is simply the makespan of the jobs; and if the input consists of a collection of singleton sets, it is simply the flowtime of the jobs. We show that the non-clairvoyant strategy EQUIoEQUI that evenly splits the available processors among the still unserved sets and then evenly splits these processors among the still uncompleted jobs of each unserved set, achieves a competitive ratio (2+\\sqrt3+o(1))\\frac{ln n}{lnln n} for the setflowtime minimization and that this is asymptotically optimal (up to a constant factor), where n is the size of the largest set. For makespan minimization, we show that the non-clairvoyant strategy EQUI achieves a competitive ratio of (1+o(1))\\frac{ln n}{lnln n}, which is again asymptotically optimal.",
        "published": "2006-12-19T15:19:59Z",
        "link": "http://arxiv.org/abs/cs/0612088v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "On the time complexity of 2-tag systems and small universal Turing   machines",
        "authors": [
            "Damien Woods",
            "Turlough Neary"
        ],
        "summary": "We show that 2-tag systems efficiently simulate Turing machines. As a corollary we find that the small universal Turing machines of Rogozhin, Minsky and others simulate Turing machines in polynomial time. This is an exponential improvement on the previously known simulation time overhead and improves a forty year old result in the area of small universal Turing machines.",
        "published": "2006-12-19T15:59:45Z",
        "link": "http://arxiv.org/abs/cs/0612089v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.1; F.1.3; F.2.3"
        ]
    },
    {
        "title": "Improved results for a memory allocation problem",
        "authors": [
            "Leah Epstein",
            "Rob van Stee"
        ],
        "summary": "We consider a memory allocation problem that can be modeled as a version of bin packing where items may be split, but each bin may contain at most two (parts of) items. A 3/2-approximation algorithm and an NP-hardness proof for this problem was given by Chung et al. We give a simpler 3/2-approximation algorithm for it which is in fact an online algorithm. This algorithm also has good performance for the more general case where each bin may contain at most k parts of items. We show that this general case is also strongly NP-hard. Additionally, we give an efficient 7/5-approximation algorithm.",
        "published": "2006-12-20T13:39:18Z",
        "link": "http://arxiv.org/abs/cs/0612100v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Journal Status",
        "authors": [
            "Johan Bollen",
            "Marko A. Rodriguez",
            "Herbert Van de Sompel"
        ],
        "summary": "The status of an actor in a social context is commonly defined in terms of two factors: the total number of endorsements the actor receives from other actors and the prestige of the endorsing actors. These two factors indicate the distinction between popularity and expert appreciation of the actor, respectively. We refer to the former as popularity and to the latter as prestige. These notions of popularity and prestige also apply to the domain of scholarly assessment. The ISI Impact Factor (ISI IF) is defined as the mean number of citations a journal receives over a 2 year period. By merely counting the amount of citations and disregarding the prestige of the citing journals, the ISI IF is a metric of popularity, not of prestige. We demonstrate how a weighted version of the popular PageRank algorithm can be used to obtain a metric that reflects prestige. We contrast the rankings of journals according to their ISI IF and their weighted PageRank, and we provide an analysis that reveals both significant overlaps and differences. Furthermore, we introduce the Y-factor which is a simple combination of both the ISI IF and the weighted PageRank, and find that the resulting journal rankings correspond well to a general understanding of journal status.",
        "published": "2006-01-09T16:56:01Z",
        "link": "http://arxiv.org/abs/cs/0601030v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "H.3.7"
        ]
    },
    {
        "title": "A Multi-Relational Network to Support the Scholarly Communication   Process",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "The general pupose of the scholarly communication process is to support the creation and dissemination of ideas within the scientific community. At a finer granularity, there exists multiple stages which, when confronted by a member of the community, have different requirements and therefore different solutions. In order to take a researcher's idea from an initial inspiration to a community resource, the scholarly communication infrastructure may be required to 1) provide a scientist initial seed ideas; 2) form a team of well suited collaborators; 3) located the most appropriate venue to publish the formalized idea; 4) determine the most appropriate peers to review the manuscript; and 5) disseminate the end product to the most interested members of the community. Through the various delinieations of this process, the requirements of each stage are tied soley to the multi-functional resources of the community: its researchers, its journals, and its manuscritps. It is within the collection of these resources and their inherent relationships that the solutions to scholarly communication are to be found. This paper describes an associative network composed of multiple scholarly artifacts that can be used as a medium for supporting the scholarly communication process.",
        "published": "2006-01-28T22:45:42Z",
        "link": "http://arxiv.org/abs/cs/0601121v2",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Metadata aggregation and \"automated digital libraries\": A retrospective   on the NSDL experience",
        "authors": [
            "Carl Lagoze",
            "Dean Krafft",
            "Tim Cornwell",
            "Naomi Dushay",
            "Dean Eckstrom",
            "John Saylor"
        ],
        "summary": "Over three years ago, the Core Integration team of the National Science Digital Library (NSDL) implemented a digital library based on metadata aggregation using Dublin Core and OAI-PMH. The initial expectation was that such low-barrier technologies would be relatively easy to automate and administer. While this architectural choice permitted rapid deployment of a production NSDL, our three years of experience have contradicted our original expectations of easy automation and low people cost. We have learned that alleged \"low-barrier\" standards are often harder to deploy than expected. In this paper we report on this experience and comment on the general cost, the functionality, and the ultimate effectiveness of this architecture.",
        "published": "2006-01-30T12:26:53Z",
        "link": "http://arxiv.org/abs/cs/0601125v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "D2D: Digital Archive to MPEG-21 DIDL",
        "authors": [
            "Suchitra Manepalli",
            "Giridhar Manepalli",
            "Michael L. Nelson"
        ],
        "summary": "Digital Archive to MPEG-21 DIDL (D2D) analyzes the contents of the digital archive and produces an MPEG-21 Digital Item Declaration Language (DIDL) encapsulating the analysis results. DIDL is an extensible XML-based language that aggregates resources and the metadata. We provide a brief report on several analysis techniques applied on the digital archive by the D2D and provide an evaluation of its run-time performance.",
        "published": "2006-02-15T22:18:38Z",
        "link": "http://arxiv.org/abs/cs/0602059v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "eJournal interface can influence usage statistics: implications for   libraries, publishers, and Project COUNTER",
        "authors": [
            "Philip M. Davis",
            "Jason S. Price"
        ],
        "summary": "The design of a publisher's electronic interface can have a measurable effect on electronic journal usage statistics. A study of journal usage from six COUNTER-compliant publishers at thirty-two research institutions in the United States, the United Kingdom and Sweden indicates that the ratio of PDF to HTML views is not consistent across publisher interfaces, even after controlling for differences in publisher content. The number of fulltext downloads may be artificially inflated when publishers require users to view HTML versions before accessing PDF versions or when linking mechanisms, such as CrossRef, direct users to the full text, rather than the abstract, of each article. These results suggest that usage reports from COUNTER-compliant publishers are not directly comparable in their current form. One solution may be to modify publisher numbers with adjustment factors deemed to be representative of the benefit or disadvantage due to its interface. Standardization of some interface and linking protocols may obviate these differences and allow for more accurate cross-publisher comparisons.",
        "published": "2006-02-16T20:29:25Z",
        "link": "http://arxiv.org/abs/cs/0602060v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "Exploring term-document matrices from matrix models in text mining",
        "authors": [
            "Ioannis Antonellis",
            "Efstratios Gallopoulos"
        ],
        "summary": "We explore a matrix-space model, that is a natural extension to the vector space model for Information Retrieval. Each document can be represented by a matrix that is based on document extracts (e.g. sentences, paragraphs, sections). We focus on the performance of this model for the specific case in which documents are originally represented as term-by-sentence matrices. We use the singular value decomposition to approximate the term-by-sentence matrices and assemble these results to form the pseudo-``term-document'' matrix that forms the basis of a text mining method alternative to traditional VSM and LSI. We investigate the singular values of this matrix and provide experimental evidence suggesting that the method can be particularly effective in terms of accuracy for text collections with multi-topic documents, such as web pages with news.",
        "published": "2006-02-21T16:14:16Z",
        "link": "http://arxiv.org/abs/cs/0602076v1",
        "categories": [
            "cs.IR",
            "cs.DB",
            "cs.DL"
        ]
    },
    {
        "title": "Digital Libraries: From Process Modelling to Grid-based Service Oriented   Architecture",
        "authors": [
            "Zaheer Abbas Khan",
            "Mohammed Odeh",
            "Richard McClatchey"
        ],
        "summary": "Graphical Business Process Modelling Languages (BPML) like Role Activity Diagrams (RAD) provide ease and flexibility for modelling business behaviour. However, these languages show limited applicability in terms of enactment over distributed systems paradigms like Service Oriented Architecture (SOA) based grid computing. This paper investigates RAD modelling of a Scientific Publishing Process (SPP) for Digital Libraries (DL) and tries to determine the suitability of Pi-Calculus based formal approaches to enact SOA based grid computing. In order to achieve this purpose, the Pi-Calculus based formal transformation from a RAD model of SPP for DL draws attention towards a number of challenging issues including issues that require particular design considerations for appropriate enactment in a SOA based grid system.",
        "published": "2006-02-23T23:12:39Z",
        "link": "http://arxiv.org/abs/cs/0602082v1",
        "categories": [
            "cs.DL",
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "Representing Contextualized Information in the NSDL",
        "authors": [
            "Carl Lagoze",
            "Dean Krafft",
            "Tim Cornwell",
            "Dean Eckstrom",
            "Susan Jesuroga",
            "Chris Wilper"
        ],
        "summary": "The NSDL (National Science Digital Library) is funded by the National Science Foundation to advance science and match education. The inital product was a metadata-based digital library providing search and access to distributed resources. Our recent work recognizes the importance of context - relations, metadata, annotations - for the pedagogical value of a digital library. This new architecture uses Fedora, a tool for representing complex content, data, metadata, web-based services, and semantic relationships, as the basis of an information network overlay (INO). The INO provides an extensible knowl-edge base for an expanding suite of digital library services.",
        "published": "2006-03-07T15:50:22Z",
        "link": "http://arxiv.org/abs/cs/0603024v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Does the arXiv lead to higher citations and reduced publisher downloads   for mathematics articles?",
        "authors": [
            "Philip M. Davis",
            "Michael J. Fromerth"
        ],
        "summary": "An analysis of 2,765 articles published in four math journals from 1997 to 2005 indicate that articles deposited in the arXiv received 35% more citations on average than non-deposited articles (an advantage of about 1.1 citations per article), and that this difference was most pronounced for highly-cited articles. Open Access, Early View, and Quality Differential were examined as three non-exclusive postulates for explaining the citation advantage. There was little support for a universal Open Access explanation, and no empirical support for Early View. There was some inferential support for a Quality Differential brought about by more highly-citable articles being deposited in the arXiv. In spite of their citation advantage, arXiv-deposited articles received 23% fewer downloads from the publisher's website (about 10 fewer downloads per article) in all but the most recent two years after publication. The data suggest that arXiv and the publisher's website may be fulfilling distinct functional needs of the reader.",
        "published": "2006-03-14T19:36:24Z",
        "link": "http://arxiv.org/abs/cs/0603056v5",
        "categories": [
            "cs.DL",
            "cs.IR",
            "math.HO"
        ]
    },
    {
        "title": "VXA: A Virtual Architecture for Durable Compressed Archives",
        "authors": [
            "Bryan Ford"
        ],
        "summary": "Data compression algorithms change frequently, and obsolete decoders do not always run on new hardware and operating systems, threatening the long-term usability of content archived using those algorithms. Re-encoding content into new formats is cumbersome, and highly undesirable when lossy compression is involved. Processor architectures, in contrast, have remained comparatively stable over recent decades. VXA, an archival storage system designed around this observation, archives executable decoders along with the encoded content it stores. VXA decoders run in a specialized virtual machine that implements an OS-independent execution environment based on the standard x86 architecture. The VXA virtual machine strictly limits access to host system services, making decoders safe to run even if an archive contains malicious code. VXA's adoption of a \"native\" processor architecture instead of type-safe language technology allows reuse of existing \"hand-optimized\" decoders in C and assembly language, and permits decoders access to performance-enhancing architecture features such as vector processing instructions. The performance cost of VXA's virtualization is typically less than 15% compared with the same decoders running natively. The storage cost of archived decoders, typically 30-130KB each, can be amortized across many archived files sharing the same compression method.",
        "published": "2006-03-18T16:31:33Z",
        "link": "http://arxiv.org/abs/cs/0603073v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.7; H.1.1; D.4.5; E.5"
        ]
    },
    {
        "title": "Collaborative thesaurus tagging the Wikipedia way",
        "authors": [
            "Jakob Voss"
        ],
        "summary": "This paper explores the system of categories that is used to classify articles in Wikipedia. It is compared to collaborative tagging systems like del.icio.us and to hierarchical classification like the Dewey Decimal Classification (DDC). Specifics and commonalitiess of these systems of subject indexing are exposed. Analysis of structural and statistical properties (descriptors per record, records per descriptor, descriptor levels) shows that the category system of Wikimedia is a thesaurus that combines collaborative tagging and hierarchical subject indexing in a special way.",
        "published": "2006-04-10T12:04:29Z",
        "link": "http://arxiv.org/abs/cs/0604036v2",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1"
        ]
    },
    {
        "title": "Effect of E-printing on Citation Rates in Astronomy and Physics",
        "authors": [
            "Edwin A. Henneken",
            "Michael J. Kurtz",
            "Guenther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn Grant",
            "Donna Thompson",
            "Stephen S. Murray"
        ],
        "summary": "In this report we examine the change in citation behavior since the introduction of the arXiv e-print repository (Ginsparg, 2001). It has been observed that papers that initially appear as arXiv e-prints get cited more than papers that do not (Lawrence, 2001; Brody et al., 2004; Schwarz & Kennicutt, 2004; Kurtz et al., 2005a, Metcalfe, 2005). Using the citation statistics from the NASA-Smithsonian Astrophysics Data System (ADS; Kurtz et al., 1993, 2000), we confirm the findings from other studies, we examine the average citation rate to e-printed papers in the Astrophysical Journal, and we show that for a number of major astronomy and physics journals the most important papers are submitted to the arXiv e-print repository first.",
        "published": "2006-04-13T22:02:05Z",
        "link": "http://arxiv.org/abs/cs/0604061v2",
        "categories": [
            "cs.DL",
            "astro-ph"
        ]
    },
    {
        "title": "Designing a Multi-petabyte Database for LSST",
        "authors": [
            "Jacek Becla",
            "Andrew Hanushevsky",
            "Sergei Nikolaev",
            "Ghaleb Abdulla",
            "Alex Szalay",
            "Maria Nieto-Santisteban",
            "Ani Thakar",
            "Jim Gray"
        ],
        "summary": "The 3.2 giga-pixel LSST camera will produce approximately half a petabyte of archive images every month. These data need to be reduced in under a minute to produce real-time transient alerts, and then added to the cumulative catalog for further analysis. The catalog is expected to grow about three hundred terabytes per year. The data volume, the real-time transient alerting requirements of the LSST, and its spatio-temporal aspects require innovative techniques to build an efficient data access system at reasonable cost. As currently envisioned, the system will rely on a database for catalogs and metadata. Several database systems are being evaluated to understand how they perform at these data rates, data volumes, and access patterns. This paper describes the LSST requirements, the challenges they impose, the data access philosophy, results to date from evaluating available database technologies against LSST requirements, and the proposed database architecture to meet the data challenges.",
        "published": "2006-04-28T03:04:00Z",
        "link": "http://arxiv.org/abs/cs/0604112v1",
        "categories": [
            "cs.DB",
            "cs.DL"
        ]
    },
    {
        "title": "Collaborative Tagging and Semiotic Dynamics",
        "authors": [
            "Ciro Cattuto",
            "Vittorio Loreto",
            "Luciano Pietronero"
        ],
        "summary": "Collaborative tagging has been quickly gaining ground because of its ability to recruit the activity of web users into effectively organizing and sharing vast amounts of information. Here we collect data from a popular system and investigate the statistical properties of tag co-occurrence. We introduce a stochastic model of user behavior embodying two main aspects of collaborative tagging: (i) a frequency-bias mechanism related to the idea that users are exposed to each other's tagging activity; (ii) a notion of memory - or aging of resources - in the form of a heavy-tailed access to the past state of the system. Remarkably, our simple modeling is able to account quantitatively for the observed experimental features, with a surprisingly high accuracy. This points in the direction of a universal behavior of users, who - despite the complexity of their own cognitive processes and the uncoordinated and selfish nature of their tagging activity - appear to follow simple activity patterns.",
        "published": "2006-05-04T18:53:10Z",
        "link": "http://arxiv.org/abs/cs/0605015v1",
        "categories": [
            "cs.CY",
            "cs.DL",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Toward a Collection-based Metadata Maintenance Model",
        "authors": [
            "Martin Kurth",
            "Jim LeBlanc"
        ],
        "summary": "In this paper, the authors identify key entities and relationships in the operational management of metadata catalogs that describe digital collections, and they draft a data model to support the administration of metadata maintenance for collections. Further, they consider this proposed model in light of other data schemes to which it relates and discuss the implications of the model for library metadata maintenance operations.",
        "published": "2006-05-05T18:16:22Z",
        "link": "http://arxiv.org/abs/cs/0605022v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Mapping the Bid Behavior of Conference Referees",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "The peer-review process, in its present form, has been repeatedly criticized. Of the many critiques ranging from publication delays to referee bias, this paper will focus specifically on the issue of how submitted manuscripts are distributed to qualified referees. Unqualified referees, without the proper knowledge of a manuscript's domain, may reject a perfectly valid study or potentially more damaging, unknowingly accept a faulty or fraudulent result. In this paper, referee competence is analyzed with respect to referee bid data collected from the 2005 Joint Conference on Digital Libraries (JCDL). The analysis of the referee bid behavior provides a validation of the intuition that referees are bidding on conference submissions with regards to the subject domain of the submission. Unfortunately, this relationship is not strong and therefore suggests that there exists other factors beyond subject domain that may be influencing referees to bid for particular submissions.",
        "published": "2006-05-24T16:45:08Z",
        "link": "http://arxiv.org/abs/cs/0605110v1",
        "categories": [
            "cs.DL",
            "cs.CY",
            "H.3.7"
        ]
    },
    {
        "title": "A Metadata Registry from Vocabularies UP: The NSDL Registry Project",
        "authors": [
            "Diane I. Hillmann",
            "Stuart A. Sutton",
            "Jon Phipps",
            "Ryan Laundry"
        ],
        "summary": "The NSDL Metadata Registry is designed to provide humans and machines with the means to discover, create, access and manage metadata schemes, schemas, application profiles, crosswalks and concept mappings. This paper describes the general goals and architecture of the NSDL Metadata Registry as well as issues encountered during the first year of the project's implementation.",
        "published": "2006-05-24T16:51:43Z",
        "link": "http://arxiv.org/abs/cs/0605111v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "An Algorithm to Determine Peer-Reviewers",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen"
        ],
        "summary": "The peer-review process is the most widely accepted certification mechanism for officially accepting the written results of researchers within the scientific community. An essential component of peer-review is the identification of competent referees to review a submitted manuscript. This article presents an algorithm to automatically determine the most appropriate reviewers for a manuscript by way of a co-authorship network data structure and a relative-rank particle-swarm algorithm. This approach is novel in that it is not limited to a pre-selected set of referees, is computationally efficient, requires no human-intervention, and, in some instances, can automatically identify conflict of interest situations. A useful application of this algorithm would be to open commentary peer-review systems because it provides a weighting for each referee with respects to their expertise in the domain of a manuscript. The algorithm is validated using referee bid data from the 2005 Joint Conference on Digital Libraries.",
        "published": "2006-05-24T17:06:32Z",
        "link": "http://arxiv.org/abs/cs/0605112v2",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.DS",
            "H.3.7, H.3.3"
        ]
    },
    {
        "title": "An Architecture for the Aggregation and Analysis of Scholarly Usage Data",
        "authors": [
            "Johan Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "Although recording of usage data is common in scholarly information services, its exploitation for the creation of value-added services remains limited due to concerns regarding, among others, user privacy, data validity, and the lack of accepted standards for the representation, sharing and aggregation of usage data. This paper presents a technical, standards-based architecture for sharing usage information, which we have designed and implemented. In this architecture, OpenURL-compliant linking servers aggregate usage information of a specific user community as it navigates the distributed information environment that it has access to. This usage information is made OAI-PMH harvestable so that usage information exposed by many linking servers can be aggregated to facilitate the creation of value-added services with a reach beyond that of a single community or a single information service. This paper also discusses issues that were encountered when implementing the proposed approach, and it presents preliminary results obtained from analyzing a usage data set containing about 3,500,000 requests aggregated by a federation of linking servers at the California State University system over a 20 month period.",
        "published": "2006-05-24T18:06:49Z",
        "link": "http://arxiv.org/abs/cs/0605113v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Analyzing Large Collections of Electronic Text Using OLAP",
        "authors": [
            "Steven Keith",
            "Owen Kaser",
            "Daniel Lemire"
        ],
        "summary": "Computer-assisted reading and analysis of text has various applications in the humanities and social sciences. The increasing size of many electronic text archives has the advantage of a more complete analysis but the disadvantage of taking longer to obtain results. On-Line Analytical Processing is a method used to store and quickly analyze multidimensional data. By storing text analysis information in an OLAP system, a user can obtain solutions to inquiries in a matter of seconds as opposed to minutes, hours, or even days. This analysis is user-driven allowing various users the freedom to pursue their own direction of research.",
        "published": "2006-05-27T00:51:46Z",
        "link": "http://arxiv.org/abs/cs/0605127v1",
        "categories": [
            "cs.DB",
            "cs.DL"
        ]
    },
    {
        "title": "Repository Replication Using NNTP and SMTP",
        "authors": [
            "Joan A. Smith",
            "Martin Klein",
            "Michael L. Nelson"
        ],
        "summary": "We present the results of a feasibility study using shared, existing, network-accessible infrastructure for repository replication. We investigate how dissemination of repository contents can be ``piggybacked'' on top of existing email and Usenet traffic. Long-term persistence of the replicated repository may be achieved thanks to current policies and procedures which ensure that mail messages and news posts are retrievable for evidentiary and other legal purposes for many years after the creation date. While the preservation issues of migration and emulation are not addressed with this approach, it does provide a simple method of refreshing content with unknown partners.",
        "published": "2006-06-01T17:45:36Z",
        "link": "http://arxiv.org/abs/cs/0606008v2",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Ten-Year Cross-Disciplinary Comparison of the Growth of Open Access and   How it Increases Research Citation Impact",
        "authors": [
            "C. Hajjem",
            "S. Harnad",
            "Y. Gingras"
        ],
        "summary": "Lawrence (2001)found computer science articles that were openly accessible (OA) on the Web were cited more. We replicated this in physics. We tested 1,307,038 articles published across 12 years (1992-2003) in 10 disciplines (Biology, Psychology, Sociology, Health, Political Science, Economics, Education, Law, Business, Management). A robot trawls the Web for full-texts using reference metadata ISI citation data (signal detectability d'=2.45; bias = 0.52). Percentage OA (relative to total OA + NOA) articles varies from 5%-16% (depending on discipline, year and country) and is slowly climbing annually (correlation r=.76, sample size N=12, probability p < 0.005). Comparing OA and NOA articles in the same journal/year, OA articles have consistently more citations, the advantage varying from 36%-172% by discipline and year. Comparing articles within six citation ranges (0, 1, 2-3, 4-7, 8-15, 16+ citations), the annual percentage of OA articles is growing significantly faster than NOA within every citation range (r > .90, N=12, p < .0005) and the effect is greater with the more highly cited articles (r = .98, N=6, p < .005). Causality cannot be determined from these data, but our prior finding of a similar pattern in physics, where percent OA is much higher (and even approaches 100% in some subfields), makes it unlikely that the OA citation advantage is merely or mostly a self-selection bias (for making only one's better articles OA). Further research will analyze the effect's timing, causal components and relation to other variables.",
        "published": "2006-06-18T19:51:18Z",
        "link": "http://arxiv.org/abs/cs/0606079v2",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Generalized h-index for Disclosing Latent Facts in Citation Networks",
        "authors": [
            "Antonis Sidiropoulos",
            "Dimitrios Katsaros",
            "Yannis Manolopoulos"
        ],
        "summary": "What is the value of a scientist and its impact upon the scientific thinking? How can we measure the prestige of a journal or of a conference? The evaluation of the scientific work of a scientist and the estimation of the quality of a journal or conference has long attracted significant interest, due to the benefits from obtaining an unbiased and fair criterion. Although it appears to be simple, defining a quality metric is not an easy task. To overcome the disadvantages of the present metrics used for ranking scientists and journals, J.E. Hirsch proposed a pioneering metric, the now famous h-index. In this article, we demonstrate several inefficiencies of this index and develop a pair of generalizations and effective variants of it to deal with scientist ranking and with publication forum ranking. The new citation indices are able to disclose trendsetters in scientific research, as well as researchers that constantly shape their field with their influential work, no matter how old they are. We exhibit the effectiveness and the benefits of the new indices to unfold the full potential of the h-index, with extensive experimental results obtained from DBLP, a widely known on-line digital library.",
        "published": "2006-07-13T15:47:25Z",
        "link": "http://arxiv.org/abs/cs/0607066v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Citation as a Representation Process",
        "authors": [
            "V. V. Kryssanov",
            "F. J. Rinaldo",
            "H. Ogawa",
            "E. Kuleshov"
        ],
        "summary": "The presented work proposes a novel approach to model the citation rate. The paper begins with a brief introduction into informetrics studies and highlights drawbacks of the contemporary approaches to modeling the citation process as a product of social interactions. An alternative modeling framework based on results obtained in cognitive psychology is then introduced and applied in an experiment to investigate properties of the citation process, as they are revealed by a large collection of citation statistics. Major research findings are discussed, and a summary is given.",
        "published": "2006-07-14T10:38:11Z",
        "link": "http://arxiv.org/abs/cs/0607070v2",
        "categories": [
            "cs.DL",
            "cs.CY",
            "physics.data-an"
        ]
    },
    {
        "title": "Separating the articles of authors with the same name",
        "authors": [
            "Jose M. Soler"
        ],
        "summary": "I describe a method to separate the articles of different authors with the same name. It is based on a distance between any two publications, defined in terms of the probability that they would have as many coincidences if they were drawn at random from all published documents. Articles with a given author name are then clustered according to their distance, so that all articles in a cluster belong very likely to the same author. The method has proven very useful in generating groups of papers that are then selected manually. This simplifies considerably citation analysis when the author publication lists are not available.",
        "published": "2006-08-01T18:23:26Z",
        "link": "http://arxiv.org/abs/cs/0608004v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "myADS-arXiv - a Tailor-Made, Open Access, Virtual Journal",
        "authors": [
            "E. Henneken",
            "M. J. Kurtz",
            "G. Eichhorn",
            "A. Accomazzi",
            "C. S. Grant",
            "D. Thompson",
            "E. Bohlen",
            "S. S. Murray"
        ],
        "summary": "The myADS-arXiv service provides the scientific community with a one stop shop for staying up-to-date with a researcher's field of interest. The service provides a powerful and unique filter on the enormous amount of bibliographic information added to the ADS on a daily basis. It also provides a complete view with the most relevant papers available in the subscriber's field of interest. With this service, the subscriber will get to know the lastest developments, popular trends and the most important papers. This makes the service not only unique from a technical point of view, but also from a content point of view. On this poster we will argue why myADS-arXiv is a tailor-made, open access, virtual journal and we will illustrate its unique character.",
        "published": "2006-08-04T15:23:32Z",
        "link": "http://arxiv.org/abs/cs/0608027v1",
        "categories": [
            "cs.DL",
            "astro-ph"
        ]
    },
    {
        "title": "GDF - A general dataformat for biosignals",
        "authors": [
            "Alois Schlögl"
        ],
        "summary": "Biomedical signals are stored in many different data formats. Most formats have been developed for a specific purpose of a specialized community for ECG research, EEG analysis, sleep research, etc. So far none of the existing formats can be considered a general purpose data format for biomedical signals. In order to solve this problem and to unify the various needs of the various biomedical signal processing fields, the so-called \"General Data Format for biomedical signals\" (GDF) is developed. This GDF format is fully described and specified. Software for reading and writing GDF data is implemented in Octave/Matlab and C/C++ and provided through BioSig - an free and open source software library for biomedical signal processing. BioSig privides also converters from various data formats to GDF, and a viewing and scoring software.",
        "published": "2006-08-11T15:13:34Z",
        "link": "http://arxiv.org/abs/cs/0608052v10",
        "categories": [
            "cs.DL",
            "J.3"
        ]
    },
    {
        "title": "E-prints and Journal Articles in Astronomy: a Productive Co-existence",
        "authors": [
            "Edwin A. Henneken",
            "Michael J. Kurtz",
            "Simeon Warner",
            "Paul Ginsparg",
            "Guenther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn S. Grant",
            "Donna Thompson",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "Are the e-prints (electronic preprints) from the arXiv repository being used instead of the journal articles? In this paper we show that the e-prints have not undermined the usage of journal papers in the astrophysics community. As soon as the journal article is published, the astronomical community prefers to read the journal article and the use of e-prints through the NASA Astrophysics Data System drops to zero. This suggests that the majority of astronomers have access to institutional subscriptions and that they choose to read the journal article when given the choice. Within the NASA Astrophysics Data System they are given this choice, because the e-print and the journal article are treated equally, since both are just one click away. In other words, the e-prints have not undermined journal use in the astrophysics community and thus currently do not pose a financial threat to the publishers. We present readership data for the arXiv category \"astro-ph\" and the 4 core journals in astronomy (Astrophysical Journal, Astronomical Journal, Monthly Notices of the Royal Astronomical Society and Astronomy & Astrophysics). Furthermore, we show that the half-life (the point where the use of an article drops to half the use of a newly published article) for an e-print is shorter than for a journal paper.   The ADS is funded by NASA Grant NNG06GG68G. arXiv receives funding from NSF award #0404553",
        "published": "2006-09-22T14:16:00Z",
        "link": "http://arxiv.org/abs/cs/0609126v1",
        "categories": [
            "cs.DL",
            "astro-ph"
        ]
    },
    {
        "title": "The Future of Technical Libraries",
        "authors": [
            "Michael J. Kurtz",
            "Guenther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn Grant",
            "Edwin Henneken",
            "Donna Thompson",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "Technical libraries are currently experiencing very rapid change. In the near future their mission will change, their physical nature will change, and the skills of their employees will change. While some will not be able to make these changes, and will fail, others will lead us into a new era.",
        "published": "2006-09-28T20:49:46Z",
        "link": "http://arxiv.org/abs/astro-ph/0609794v1",
        "categories": [
            "astro-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Full Text Searching in the Astrophysics Data System",
        "authors": [
            "Günther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn S. Grant",
            "Edwin A. Henneken",
            "Donna M. Thompson",
            "Michael J. Kurtz",
            "Stephen S. Murray"
        ],
        "summary": "The Smithsonian/NASA Astrophysics Data System (ADS) provides a search system for the astronomy and physics scholarly literature. All major and many smaller astronomy journals that were published on paper have been scanned back to volume 1 and are available through the ADS free of charge. All scanned pages have been converted to text and can be searched through the ADS Full Text Search System. In addition, searches can be fanned out to several external search systems to include the literature published in electronic form. Results from the different search systems are combined into one results list.   The ADS Full Text Search System is available at: http://adsabs.harvard.edu/fulltext_service.html",
        "published": "2006-10-02T14:51:45Z",
        "link": "http://arxiv.org/abs/cs/0610007v2",
        "categories": [
            "cs.DL",
            "astro-ph",
            "cs.DB"
        ]
    },
    {
        "title": "Connectivity in the Astronomy Digital Library",
        "authors": [
            "Günther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn S. Grant",
            "Edwin A. Henneken",
            "Donna M. Thompson",
            "Michael J. Kurtz",
            "Stephen S. Murray"
        ],
        "summary": "The Astrophysics Data System (ADS) provides an extensive system of links between the literature and other on-line information. Recently, the journals of the American Astronomical Society (AAS) and a group of NASA data centers have collaborated to provide more links between on-line data obtained by space missions and the on-line journals. Authors can now specify which data sets they have used in their article. This information is used by the participants to provide the links between the literature and the data.   The ADS is available at: http://ads.harvard.edu",
        "published": "2006-10-02T15:06:35Z",
        "link": "http://arxiv.org/abs/cs/0610008v1",
        "categories": [
            "cs.DL",
            "astro-ph",
            "cs.DB"
        ]
    },
    {
        "title": "Creation and use of Citations in the ADS",
        "authors": [
            "Alberto Accomazzi",
            "Gunther Eichhorn",
            "Michael J. Kurtz",
            "Carolyn S. Grant",
            "Edwin Henneken",
            "Markus Demleitner",
            "Donna Thompson",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "With over 20 million records, the ADS citation database is regularly used by researchers and librarians to measure the scientific impact of individuals, groups, and institutions. In addition to the traditional sources of citations, the ADS has recently added references extracted from the arXiv e-prints on a nightly basis. We review the procedures used to harvest and identify the reference data used in the creation of citations, the policies and procedures that we follow to avoid double-counting and to eliminate contributions which may not be scholarly in nature. Finally, we describe how users and institutions can easily obtain quantitative citation data from the ADS, both interactively and via web-based programming tools.   The ADS is available at http://ads.harvard.edu.",
        "published": "2006-10-03T21:29:03Z",
        "link": "http://arxiv.org/abs/cs/0610011v1",
        "categories": [
            "cs.DL",
            "astro-ph",
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Data in the ADS -- Understanding How to Use it Better",
        "authors": [
            "Carolyn S. Grant",
            "Alberto Accomazzi",
            "Donna Thompson",
            "Edwin Henneken",
            "Guenther Eichhorn",
            "Michael J. Kurtz",
            "Stephen S. Murray"
        ],
        "summary": "The Smithsonian/NASA ADS Abstract Service contains a wealth of data for astronomers and librarians alike, yet the vast majority of usage consists of rudimentary searches. Hints on how to obtain more focused search results by using more of the various capabilities of the ADS are presented, including searching by affiliation. We also discuss the classification of articles by content and by referee status.   The ADS is funded by NASA Grant NNG06GG68G-16613687.",
        "published": "2006-10-05T18:51:54Z",
        "link": "http://arxiv.org/abs/cs/0610029v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "Paper to Screen: Processing Historical Scans in the ADS",
        "authors": [
            "Donna M. Thompson",
            "Alberto Accomazzi",
            "Guenther Eichhorn",
            "Carolyn Grant",
            "Edwin Henneken",
            "Michael J. Kurtz",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "The NASA Astrophysics Data System in conjunction with the Wolbach Library at the Harvard-Smithsonian Center for Astrophysics is working on a project to microfilm historical observatory publications. The microfilm is then scanned for inclusion in the ADS. The ADS currently contains over 700,000 scanned pages of volumes of historical literature. Many of these volumes lack clear pagination or other bibliographic data that are necessary to take advantage of the searching capabilities of the ADS. This paper will address some of the interesting challenges that needed to be resolved during the processing of the Observatory Reports included in the ADS.",
        "published": "2006-10-05T18:58:26Z",
        "link": "http://arxiv.org/abs/cs/0610030v1",
        "categories": [
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Pathways: Augmenting interoperability across scholarly repositories",
        "authors": [
            "Simeon Warner",
            "Jeroen Bekaert",
            "Carl Lagoze",
            "Xiaoming Liu",
            "Sandy Payette",
            "Herbert Van de Sompel"
        ],
        "summary": "In the emerging eScience environment, repositories of papers, datasets, software, etc., should be the foundation of a global and natively-digital scholarly communications system. The current infrastructure falls far short of this goal. Cross-repository interoperability must be augmented to support the many workflows and value-chains involved in scholarly communication. This will not be achieved through the promotion of single repository architecture or content representation, but instead requires an interoperability framework to connect the many heterogeneous systems that will exist.   We present a simple data model and service architecture that augments repository interoperability to enable scholarly value-chains to be implemented. We describe an experiment that demonstrates how the proposed infrastructure can be deployed to implement the workflow involved in the creation of an overlay journal over several different repository systems (Fedora, aDORe, DSpace and arXiv).",
        "published": "2006-10-05T19:55:09Z",
        "link": "http://arxiv.org/abs/cs/0610031v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Constructing experimental indicators for Open Access documents",
        "authors": [
            "Philipp Mayr"
        ],
        "summary": "The ongoing paradigm change in the scholarly publication system ('science is turning to e-science') makes it necessary to construct alternative evaluation criteria/metrics which appropriately take into account the unique characteristics of electronic publications and other research output in digital formats. Today, major parts of scholarly Open Access (OA) publications and the self-archiving area are not well covered in the traditional citation and indexing databases. The growing share and importance of freely accessible research output demands new approaches/metrics for measuring and for evaluating of these new types of scientific publications. In this paper we propose a simple quantitative method which establishes indicators by measuring the access/download pattern of OA documents and other web entities of a single web server. The experimental indicators (search engine, backlink and direct access indicator) are constructed based on standard local web usage data. This new type of web-based indicator is developed to model the specific demand for better study/evaluation of the accessibility, visibility and interlinking of open accessible documents. We conclude that e-science will need new stable e-indicators.",
        "published": "2006-10-11T07:10:52Z",
        "link": "http://arxiv.org/abs/cs/0610056v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Usage Impact Factor: the effects of sample characteristics on   usage-based impact metrics",
        "authors": [
            "Johan Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "There exist ample demonstrations that indicators of scholarly impact analogous to the citation-based ISI Impact Factor can be derived from usage data. However, contrary to the ISI IF which is based on citation data generated by the global community of scholarly authors, so far usage can only be practically recorded at a local level leading to community-specific assessments of scholarly impact that are difficult to generalize to the global scholarly community. We define a journal Usage Impact Factor which mimics the definition of the Thomson Scientific's ISI Impact Factor. Usage Impact Factor rankings are calculated on the basis of a large-scale usage data set recorded for the California State University system from 2003 to 2005. The resulting journal rankings are then compared to Thomson Scientific's ISI Impact Factor which is used as a baseline indicator of general impact. Our results indicate that impact as derived from California State University usage reflects the particular scientific and demographic characteristics of its communities.",
        "published": "2006-10-26T16:39:53Z",
        "link": "http://arxiv.org/abs/cs/0610154v2",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Protocols for Scholarly Communication",
        "authors": [
            "Alberto Pepe",
            "Joanne Yeomans"
        ],
        "summary": "CERN, the European Organization for Nuclear Research, has operated an institutional preprint repository for more than 10 years. The repository contains over 850,000 records of which more than 450,000 are full-text OA preprints, mostly in the field of particle physics, and it is integrated with the library's holdings of books, conference proceedings, journals and other grey literature. In order to encourage effective propagation and open access to scholarly material, CERN is implementing a range of innovative library services into its document repository: automatic keywording, reference extraction, collaborative management tools and bibliometric tools. Some of these services, such as user reviewing and automatic metadata extraction, could make up an interesting testbed for future publishing solutions and certainly provide an exciting environment for e-science possibilities. The future protocol for scientific communication should naturally guide authors towards OA publication and CERN wants to help reach a full open access publishing environment for the particle physics community and the related sciences in the next few years.",
        "published": "2006-11-01T18:59:57Z",
        "link": "http://arxiv.org/abs/cs/0611005v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Intra-site Level Cultural Heritage Documentation: Combination of Survey,   Modeling and Imagery Data in a Web Information System",
        "authors": [
            "Anne Durand",
            "Pierre Drap",
            "Elise Meyer",
            "Pierre Grussenmeyer",
            "Jean-Pierre Perrin"
        ],
        "summary": "Cultural heritage documentation induces the use of computerized techniques to manage and preserve the information produced. Geographical information systems have proved their potentialities in this scope, but they are not always adapted for the management of features at the scale of a particular archaeological site. Moreover, computer applications in archaeology are often technology driven and software constrained. Thus, we propose a tool that tries to avoid these difficulties. We are developing an information system that works over the Internet and that is joined with a web site. Aims are to assist the work of archaeological sites managers and to be a documentation tool about these sites, dedicated to everyone. We devote therefore our system both to the professionals who are in charge of the site, and to the general public who visits it or who wants to have information on it. The system permits to do exploratory analyses of the data, especially at spatial and temporal levels. We propose to record metadata about the archaeological features in XML and to access these features through interactive 2D and 3D representations, and through queries systems (keywords and images). The 2D images, photos, or vectors are generated in SVG, while 3D models are generated in X3D. Archaeological features are also automatically integrated in a MySQL database. The web site is an exchange platform with the information system and is written in PHP. Our first application case is the medieval castle of Vianden, Luxembourg.",
        "published": "2006-11-08T17:35:52Z",
        "link": "http://arxiv.org/abs/cs/0611036v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "The effect of 'Open Access' upon citation impact: An analysis of ArXiv's   Condensed Matter Section",
        "authors": [
            "Henk F. Moed"
        ],
        "summary": "This article statistically analyses how the citation impact of articles deposited in the Condensed Matter section of the preprint server ArXiv (hosted by Cornell University), and subsequently published in a scientific journal, compares to that of articles in the same journal that were not deposited in that archive. Its principal aim is to further illustrate and roughly estimate the effect of two factors, 'early view' and 'quality bias', upon differences in citation impact between these two sets of papers, using citation data from Thomson Scientific's Web of Science. It presents estimates for a number of journals in the field of condensed matter physics. In order to discriminate between an 'open access' effect and an early view effect, longitudinal citation data was analysed covering a time period as long as 7 years. Quality bias was measured by calculating ArXiv citation impact differentials at the level of individual authors publishing in a journal, taking into account co-authorship. The analysis provided evidence of a strong quality bias and early view effect. Correcting for these effects, there is in a sample of 6 condensed matter physics journals studied in detail, no sign of a general 'open access advantage' of papers deposited in ArXiv. The study does provide evidence that ArXiv accelerates citation, due to the fact that that ArXiv makes papers earlier available rather than that it makes papers freely available.",
        "published": "2006-11-14T14:48:15Z",
        "link": "http://arxiv.org/abs/cs/0611060v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Wikipedia: organisation from a bottom-up approach",
        "authors": [
            "Sander Spek",
            "Eric Postma",
            "H. Jaap van den Herik"
        ],
        "summary": "Wikipedia can be considered as an extreme form of a self-managing team, as a means of labour division. One could expect that this bottom-up approach, with the absense of top-down organisational control, would lead to a chaos, but our analysis shows that this is not the case. In the Dutch Wikipedia, an integrated and coherent data structure is created, while at the same time users succeed in distributing roles by self-selection. Some users focus on an area of expertise, while others edit over the whole encyclopedic range. This constitutes our conclusion that Wikipedia, in general, is a successful example of a self-managing team.",
        "published": "2006-11-15T10:27:09Z",
        "link": "http://arxiv.org/abs/cs/0611068v2",
        "categories": [
            "cs.DL",
            "cs.CY",
            "H.3.7; H.4.3"
        ]
    },
    {
        "title": "Quantitative Analysis of the Publishing Landscape in High-Energy Physics",
        "authors": [
            "Salvatore Mele",
            "David Dallman",
            "Jens Vigen",
            "Joanne Yeomans"
        ],
        "summary": "World-wide collaboration in high-energy physics (HEP) is a tradition which dates back several decades, with scientific publications mostly coauthored by scientists from different countries. This coauthorship phenomenon makes it difficult to identify precisely the ``share'' of each country in HEP scientific production. One year's worth of HEP scientific articles published in peer-reviewed journals is analysed and their authors are uniquely assigned to countries. This method allows the first correct estimation on a ``pro rata'' basis of the share of HEP scientific publishing among several countries and institutions. The results provide an interesting insight into the geographical collaborative patterns of the HEP community. The HEP publishing landscape is further analysed to provide information on the journals favoured by the HEP community and on the geographical variation of their author bases. These results provide quantitative input to the ongoing debate on the possible transition of HEP publishing to an Open Access model.",
        "published": "2006-11-26T19:39:48Z",
        "link": "http://arxiv.org/abs/cs/0611130v1",
        "categories": [
            "cs.DL",
            "hep-ex",
            "hep-ph",
            "hep-th"
        ]
    },
    {
        "title": "Ranking Scientific Publications Using a Simple Model of Network Traffic",
        "authors": [
            "Dylan Walker",
            "Huafeng Xie",
            "Koon-Kiu Yan",
            "Sergei Maslov"
        ],
        "summary": "To account for strong aging characteristics of citation networks, we modify Google's PageRank algorithm by initially distributing random surfers exponentially with age, in favor of more recent publications. The output of this algorithm, which we call CiteRank, is interpreted as approximate traffic to individual publications in a simple model of how researchers find new information. We develop an analytical understanding of traffic flow in terms of an RPA-like model and optimize parameters of our algorithm to achieve the best performance. The results are compared for two rather different citation networks: all American Physical Society publications and the set of high-energy physics theory (hep-th) preprints. Despite major differences between these two networks, we find that their optimal parameters for the CiteRank algorithm are remarkably similar.",
        "published": "2006-12-13T20:20:28Z",
        "link": "http://arxiv.org/abs/physics/0612122v1",
        "categories": [
            "physics.soc-ph",
            "cs.DL",
            "physics.data-an"
        ]
    },
    {
        "title": "Why is Open Access Development so Successful? Stigmergic organization   and the economics of information",
        "authors": [
            "Francis Heylighen"
        ],
        "summary": "The explosive development of \"free\" or \"open source\" information goods contravenes the conventional wisdom that markets and commercial organizations are necessary to efficiently supply products. This paper proposes a theoretical explanation for this phenomenon, using concepts from economics and theories of self-organization. Once available on the Internet, information is intrinsically not a scarce good, as it can be replicated virtually without cost. Moreover, freely distributing information is profitable to its creator, since it improves the quality of the information, and enhances the creator's reputation. This provides a sufficient incentive for people to contribute to open access projects. Unlike traditional organizations, open access communities are open, distributed and self-organizing. Coordination is achieved through stigmergy: listings of \"work-in-progress\" direct potential contributors to the tasks where their contribution is most likely to be fruitful. This obviates the need both for centralized planning and for the \"invisible hand\" of the market.",
        "published": "2006-12-14T12:15:49Z",
        "link": "http://arxiv.org/abs/cs/0612071v1",
        "categories": [
            "cs.CY",
            "cs.DL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Bias in the journal impact factor",
        "authors": [
            "Jerome K Vanclay"
        ],
        "summary": "The ISI journal impact factor (JIF) is based on a sample that may represent half the whole-of-life citations to some journals, but a small fraction (<10%) of the citations accruing to other journals. This disproportionate sampling means that the JIF provides a misleading indication of the true impact of journals, biased in favour of journals that have a rapid rather than a prolonged impact. Many journals exhibit a consistent pattern of citation accrual from year to year, so it may be possible to adjust the JIF to provide a more reliable indication of a journal's impact.",
        "published": "2006-12-19T00:17:03Z",
        "link": "http://arxiv.org/abs/cs/0612091v2",
        "categories": [
            "cs.DL",
            "q-bio.OT"
        ]
    },
    {
        "title": "Electronic Laboratory Notebook Assisting Reflectance Spectrometry in   Legal Medicine",
        "authors": [
            "Lioudmila Belenkaia",
            "Michael Bohnert",
            "Andreas W. Liehr"
        ],
        "summary": "Reflectance spectrometry is a fast and reliable method for the characterisation of human skin if the spectra are analysed with respect to a physical model describing the optical properties of human skin. For a field study performed at the Institute of Legal Medicine and the Freiburg Materials Research Center of the University of Freiburg an electronic laboratory notebook has been developed, which assists in the recording, management, and analysis of reflectance spectra. The core of the electronic laboratory notebook is a MySQL database. It is filled with primary data via a web interface programmed in Java, which also enables the user to browse the database and access the results of data analysis. These are carried out by Matlab, Tcl and   Python scripts, which retrieve the primary data from the electronic laboratory notebook, perform the analysis, and store the results in the database for further usage.",
        "published": "2006-12-22T13:29:27Z",
        "link": "http://arxiv.org/abs/cs/0612123v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "cs.IR",
            "H.2.8"
        ]
    },
    {
        "title": "A New Era in Citation and Bibliometric Analyses: Web of Science, Scopus,   and Google Scholar",
        "authors": [
            "Lokman I. Meho",
            "Kiduk Yang"
        ],
        "summary": "Academic institutions, federal agencies, publishers, editors, authors, and librarians increasingly rely on citation analysis for making hiring, promotion, tenure, funding, and/or reviewer and journal evaluation and selection decisions. The Institute for Scientific Information's (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science), however, may no longer be adequate as the only or even the main sources of citations because new databases and tools that allow citation searching are now available. Whether these new databases and tools complement or represent alternatives to Web of Science (WoS) is important to explore. Using a group of 15 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. The paper discusses the strengths and weaknesses of WoS, Scopus, and GS, their overlap and uniqueness, quality and language of the citations, and the implications of the findings for citation analysis. The project involved citation searching for approximately 1,100 scholarly works published by the study group and over 200 works by a test group (an additional 10 faculty members). Overall, more than 10,000 citing and purportedly citing documents were examined. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours.",
        "published": "2006-12-23T14:47:24Z",
        "link": "http://arxiv.org/abs/cs/0612132v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "The Rise and Rise of Citation Analysis",
        "authors": [
            "L. I. Meho"
        ],
        "summary": "With the vast majority of scientific papers now available online, this paper describes how the Web is allowing physicists and information providers to measure more accurately the impact of these papers and their authors. Provides a historical background of citation analysis, impact factor, new citation data sources (e.g., Google Scholar, Scopus, NASA's Astrophysics Data System Abstract Service, MathSciNet, ScienceDirect, SciFinder Scholar, Scitation/SPIN, and SPIRES-HEP), as well as h-index, g-index, and a-index.",
        "published": "2006-12-30T19:23:36Z",
        "link": "http://arxiv.org/abs/physics/0701012v1",
        "categories": [
            "physics.soc-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Deductive Object Programming",
        "authors": [
            "Francois Colonna"
        ],
        "summary": "We propose some slight additions to O-O languages to implement the necessary features for using Deductive Object Programming (DOP). This way of programming based upon the manipulation of the Production Tree of the Objects of Interest, result in making Persistent these Objects and in sensibly lowering the code complexity.",
        "published": "2006-01-10T09:50:45Z",
        "link": "http://arxiv.org/abs/cs/0601035v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Fast Frequent Querying with Lazy Control Flow Compilation",
        "authors": [
            "Remko Tronçon",
            "Gerda Janssens",
            "Bart Demoen",
            "Henk Vandecasteele"
        ],
        "summary": "Control flow compilation is a hybrid between classical WAM compilation and meta-call, limited to the compilation of non-recursive clause bodies. This approach is used successfully for the execution of dynamically generated queries in an inductive logic programming setting (ILP). Control flow compilation reduces compilation times up to an order of magnitude, without slowing down execution. A lazy variant of control flow compilation is also presented. By compiling code by need, it removes the overhead of compiling unreached code (a frequent phenomenon in practical ILP settings), and thus reduces the size of the compiled code. Both dynamic compilation approaches have been implemented and were combined with query packs, an efficient ILP execution mechanism. It turns out that locality of data and code is important for performance. The experiments reported in the paper show that lazy control flow compilation is superior in both artificial and real life settings.",
        "published": "2006-01-16T13:11:51Z",
        "link": "http://arxiv.org/abs/cs/0601072v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "A Formal Architecture-Centric Model-Driven Approach for the Automatic   Generation of Grid Applications",
        "authors": [
            "David Manset",
            "Herve Verjus",
            "Richard McClatchey",
            "Flavio Oquendo"
        ],
        "summary": "This paper discusses the concept of model-driven software engineering applied to the Grid application domain. As an extension to this concept, the approach described here, attempts to combine both formal architecture-centric and model-driven paradigms. It is a commonly recognized statement that Grid systems have seldom been designed using formal techniques although from past experience such techniques have shown advantages. This paper advocates a formal engineering approach to Grid system developments in an effort to contribute to the rigorous development of Grids software architectures. This approach addresses quality of service and cross-platform developments by applying the model-driven paradigm to a formal architecture-centric engineering method. This combination benefits from a formal semantic description power in addition to model-based transformations. The result of such a novel combined concept promotes the re-use of design models and facilitates developments in Grid computing.",
        "published": "2006-01-28T00:08:30Z",
        "link": "http://arxiv.org/abs/cs/0601118v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "Engineering Conceptual Data Models from Domain Ontologies: A Critical   Evaluation",
        "authors": [
            "Haya El-Ghalayini",
            "Mohammed Odeh",
            "Richard McClatchey"
        ],
        "summary": "This paper studies the differences and similarities between domain ontologies and conceptual data models and the role that ontologies can play in establishing conceptual data models during the process of information systems development. A mapping algorithm has been proposed and embedded in a special purpose Transformation Engine to generate a conceptual data model from a given domain ontology. Both quantitative and qualitative methods have been adopted to critically evaluate this new approach. In addition, this paper focuses on evaluating the quality of the generated conceptual data model elements using Bunge-Wand-Weber and OntoClean ontologies. The results of this evaluation indicate that the generated conceptual data model provides a high degree of accuracy in identifying the substantial domain entities along with their attributes and relationships being derived from the consensual semantics of domain knowledge. The results are encouraging and support the potential role that this approach can take part in process of information system development.",
        "published": "2006-01-28T17:40:44Z",
        "link": "http://arxiv.org/abs/cs/0601119v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "Digital Libraries: From Process Modelling to Grid-based Service Oriented   Architecture",
        "authors": [
            "Zaheer Abbas Khan",
            "Mohammed Odeh",
            "Richard McClatchey"
        ],
        "summary": "Graphical Business Process Modelling Languages (BPML) like Role Activity Diagrams (RAD) provide ease and flexibility for modelling business behaviour. However, these languages show limited applicability in terms of enactment over distributed systems paradigms like Service Oriented Architecture (SOA) based grid computing. This paper investigates RAD modelling of a Scientific Publishing Process (SPP) for Digital Libraries (DL) and tries to determine the suitability of Pi-Calculus based formal approaches to enact SOA based grid computing. In order to achieve this purpose, the Pi-Calculus based formal transformation from a RAD model of SPP for DL draws attention towards a number of challenging issues including issues that require particular design considerations for appropriate enactment in a SOA based grid system.",
        "published": "2006-02-23T23:12:39Z",
        "link": "http://arxiv.org/abs/cs/0602082v1",
        "categories": [
            "cs.DL",
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "Object-Oriented Modeling of Programming Paradigms",
        "authors": [
            "M. H. van Emden",
            "S. C. Somosan"
        ],
        "summary": "For the right application, the use of programming paradigms such as functional or logic programming can enormously increase productivity in software development. But these powerful paradigms are tied to exotic programming languages, while the management of software development dictates standardization on a single language.   This dilemma can be resolved by using object-oriented programming in a new way. It is conventional to analyze an application by object-oriented modeling. In the new approach, the analysis identifies the paradigm that is ideal for the application; development starts with object-oriented modeling of the paradigm. In this paper we illustrate the new approach by giving examples of object-oriented modeling of dataflow and constraint programming. These examples suggest that it is no longer necessary to embody a programming paradigm in a language dedicated to it.",
        "published": "2006-03-03T05:19:42Z",
        "link": "http://arxiv.org/abs/cs/0603016v2",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.1.5; D.2.2; D.2.3; D.3.3"
        ]
    },
    {
        "title": "Language Support for Optional Functionality",
        "authors": [
            "Joy Mukherjee",
            "Srinidhi Varadarajan"
        ],
        "summary": "We recommend a programming construct - availability check - for programs that need to automatically adjust to presence or absence of segments of code. The idea is to check the existence of a valid definition before a function call is invoked. The syntax is that of a simple 'if' statement. The vision is to enable customization of application functionality through addition or removal of optional components, but without requiring complete re-building. Focus is on C-like compiled procedural languages and UNIX-based systems. Essentially, our approach attempts to combine the flexibility of dynamic libraries with the usability of utility (dependency) libraries. We outline the benefits over prevalent strategies mainly in terms of development complexity, crudely measured as lesser lines of code. We also allude to performance and flexibility facets. A Preliminary implementation and figures from early experimental evaluation are presented.",
        "published": "2006-03-06T00:46:09Z",
        "link": "http://arxiv.org/abs/cs/0603021v1",
        "categories": [
            "cs.PL",
            "cs.OS",
            "cs.SE"
        ]
    },
    {
        "title": "Inter-component communication methods in object-oriented frameworks",
        "authors": [
            "Vaghinak Petrosyan"
        ],
        "summary": "Modern frameworks for development of graphical interfaces are using the native controls of the operating system. Because of that they are using operating system events model for inter-component communication. We consider a method to increase inter-component communication speed by sending messages from one component to the other passing over the operating system. Besides the messages subscription helps to avoid receiving of unnecessary messages.",
        "published": "2006-03-09T08:44:43Z",
        "link": "http://arxiv.org/abs/cs/0603033v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Deriving Conceptual Data Models from Domain Ontologies for   Bioinformatics",
        "authors": [
            "Haya El-Ghalayini",
            "Mohammed Odeh",
            "Richard McClatchey"
        ],
        "summary": "This paper studies the role that ontologies can play in establishing conceptual data models during the process of information systems development. A mapping algorithm has been proposed and embedded in a special purpose Transformation-Engine to generate a conceptual data model from a given domain ontology. In addition, this paper focuses on applying the proposed approach to a bioinformatics context as the nature of biological data is considered a barrier in representing biological conceptual data models. Both quantitative and qualitative methods have been adopted to critically evaluate this new approach. The results of this evaluation indicate that the quality of the generated conceptual data models can reflect the problem domain entities and the associations between them. The results are encouraging and support the potential role that this approach can play in providing a suitable starting point for conceptual data model development.",
        "published": "2006-03-09T13:59:18Z",
        "link": "http://arxiv.org/abs/cs/0603037v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "minimUML: A Minimalist Approach to UML Diagraming for Early Computer   Science Education",
        "authors": [
            "Scott Turner",
            "Manuel A. Perez-Quinones",
            "Stephen H. Edwards"
        ],
        "summary": "The Unified Modeling Language (UML) is commonly used in introductory Computer Science to teach basic object-oriented design. However, there appears to be a lack of suitable software to support this task. Many of the available programs that support UML focus on developing code and not on enhancing learning. Those that were designed for educational use sometimes have poor interfaces or are missing common and important features, such as multiple selection and undo/redo. There is a need for software that is tailored to an instructional environment and has all the useful and needed functionality for that specific task. This is the purpose of minimUML. minimUML provides a minimum amount of UML, just what is commonly used in beginning programming classes, while providing a simple, usable interface. In particular, minimUML was designed to support abstract design while supplying features for exploratory learning and error avoidance. In addition, it allows for the annotation of diagrams, through text or freeform drawings, so students can receive feedback on their work. minimUML was developed with the goal of supporting ease of use, supporting novice students, and a requirement of no prior-training for its use.",
        "published": "2006-03-30T06:12:17Z",
        "link": "http://arxiv.org/abs/cs/0603121v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "K.3.2; H.5.2; D.2.2"
        ]
    },
    {
        "title": "The Case for Modeling Security, Privacy, Usability and Reliability   (SPUR) in Automotive Software",
        "authors": [
            "K. Venkatesh Prasad",
            "TJ Giuli",
            "David Watson"
        ],
        "summary": "Over the past five years, there has been considerable growth and established value in the practice of modeling automotive software requirements. Much of this growth has been centered on requirements of software associated with the established functional areas of an automobile, such as those associated with powertrain, chassis, body, safety and infotainment. This paper makes a case for modeling four additional attributes that are increasingly important as vehicles become information conduits: security, privacy, usability, and reliability. These four attributes are important in creating specifications for embedded in-vehicle automotive software.",
        "published": "2006-04-06T13:23:53Z",
        "link": "http://arxiv.org/abs/cs/0604019v1",
        "categories": [
            "cs.SE",
            "cs.CR",
            "cs.HC",
            "D.2.4; K.4.1; H.5.2; K.6.5"
        ]
    },
    {
        "title": "Octave-GTK: A GTK binding for GNU Octave",
        "authors": [
            "Muthiah Annamalai",
            "Hemant Kumar",
            "Leela Velusamy"
        ],
        "summary": "This paper discusses the problems faced with interoperability between two programming languages, with respect to GNU Octave, and GTK API written in C, to provide the GTK API on Octave.Octave-GTK is the fusion of two different API's: one exported by GNU Octave [scientific computing tool] and the other GTK [GUI toolkit]; this enables one to use GTK primitives within GNU Octave, to build graphical front ends,at the same time using octave engine for number crunching power. This paper illustrates our implementation of binding logic, and shows results extended to various other libraries using the same base code generator. Also shown, are methods of code generation, binding automation, and the niche we plan to fill in the absence of GUI in Octave. Canonical discussion of advantages, feasibility and problems faced in the process are elucidated.",
        "published": "2006-04-19T16:46:23Z",
        "link": "http://arxiv.org/abs/cs/0604073v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Applied MVC Patterns. A pattern language",
        "authors": [
            "Sergey Alpaev"
        ],
        "summary": "How to get advantages of MVC model without making applications unnecessarily complex? The full-featured MVC implementation is on the top end of ladder of complexity. The other end is meant for simple cases that do not call for such complex designs, however still in need of the advantages of MVC patterns, such as ability to change the look-and-feel. This paper presents patterns of MVC implementation that help to benefit from the paradigm and keep the right balance between flexibility and implementation complexity.",
        "published": "2006-05-05T10:25:26Z",
        "link": "http://arxiv.org/abs/cs/0605020v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "On the Design of Agent-Based Systems using UML and Extensions",
        "authors": [
            "Mihaela Dinsoreanu",
            "Ioan Salomie",
            "Kalman Pusztai"
        ],
        "summary": "The Unified Software Development Process (USDP) and UML have been now generally accepted as the standard methodology and modeling language for developing Object-Oriented Systems. Although Agent-based Systems introduces new issues, we consider that USDP and UML can be used in an extended manner for modeling Agent-based Systems. The paper presents a methodology for designing agent-based systems and the specific models expressed in an UML-based notation corresponding to each phase of the software development process. UML was extended using the provided mechanism: stereotypes. Therefore, this approach can be managed with any CASE tool supporting UML. A Case Study, the development of a specific agent-based Student Evaluation System (SAS), is presented.",
        "published": "2006-05-08T12:21:51Z",
        "link": "http://arxiv.org/abs/cs/0605031v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "A framework of reusable structures for mobile agent development",
        "authors": [
            "Tudor Marian",
            "Bogdan Dumitriu",
            "Mihaela Dinsoreanu",
            "Ioan Salomie"
        ],
        "summary": "Mobile agents research is clearly aiming towards imposing agent based development as the next generation of tools for writing software. This paper comes with its own contribution to this global goal by introducing a novel unifying framework meant to bring simplicity and interoperability to and among agent platforms as we know them today. In addition to this, we also introduce a set of agent behaviors which, although tailored for and from the area of virtual learning environments, are none the less generic enough to be used for rapid, simple, useful and reliable agent deployment. The paper also presents an illustrative case study brought forward to prove the feasibility of our design.",
        "published": "2006-05-08T12:27:59Z",
        "link": "http://arxiv.org/abs/cs/0605032v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "Mobile Agent Based Solutions for Knowledge Assessment in elearning   Environments",
        "authors": [
            "Mihaela Dinsoreanu",
            "Cristian Godja",
            "Claudiu Anghel",
            "Ioan Salomie",
            "Tom Coffey"
        ],
        "summary": "E-learning is nowadays one of the most interesting of the \"e- \" domains available through the Internet. The main problem to create a Web-based, virtual environment is to model the traditional domain and to implement the model using the most suitable technologies. We analyzed the distance learning domain and investigated the possibility to implement some e-learning services using mobile agent technologies. This paper presents a model of the Student Assessment Service (SAS) and an agent-based framework developed to be used for implementing specific applications. A specific Student Assessment application that relies on the framework was developed.",
        "published": "2006-05-08T12:37:13Z",
        "link": "http://arxiv.org/abs/cs/0605033v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.SE"
        ]
    },
    {
        "title": "An Unfolding-Based Semantics for Logic Programming with Aggregates",
        "authors": [
            "Tran Cao Son",
            "Enrico Pontelli",
            "Islam Elkabani"
        ],
        "summary": "The paper presents two equivalent definitions of answer sets for logic programs with aggregates. These definitions build on the notion of unfolding of aggregates, and they are aimed at creating methodologies to translate logic programs with aggregates to normal logic programs or positive programs, whose answer set semantics can be used to defined the semantics of the original programs. The first definition provides an alternative view of the semantics for logic programming with aggregates described by Pelov et al.   The second definition is similar to the traditional answer set definition for normal logic programs, in that, given a logic program with aggregates and an interpretation, the unfolding process produces a positive program. The paper shows how this definition can be extended to consider aggregates in the head of the rules.   The proposed views of logic programming with aggregates are simple and coincide with the ultimate stable model semantics, and with other semantic characterizations for large classes of program (e.g., programs with monotone aggregates and programs that are aggregate-stratified).   Moreover, it can be directly employed to support an implementation using available answer set solvers. The paper describes a system, called ASP^A, that is capable of computing answer sets of programs with arbitrary (e.g., recursively defined) aggregates.",
        "published": "2006-05-09T04:08:24Z",
        "link": "http://arxiv.org/abs/cs/0605038v1",
        "categories": [
            "cs.SE",
            "cs.AI"
        ]
    },
    {
        "title": "Ontological Representations of Software Patterns",
        "authors": [
            "Jean-Marc Rosengard",
            "Marian Ursu"
        ],
        "summary": "This paper is based on and advocates the trend in software engineering of extending the use of software patterns as means of structuring solutions to software development problems (be they motivated by best practice or by company interests and policies). The paper argues that, on the one hand, this development requires tools for automatic organisation, retrieval and explanation of software patterns. On the other hand, that the existence of such tools itself will facilitate the further development and employment of patterns in the software development process. The paper analyses existing pattern representations and concludes that they are inadequate for the kind of automation intended here. Adopting a standpoint similar to that taken in the semantic web, the paper proposes that feasible solutions can be built on the basis of ontological representations.",
        "published": "2006-05-14T18:31:09Z",
        "link": "http://arxiv.org/abs/cs/0605059v1",
        "categories": [
            "cs.SE",
            "cs.AI"
        ]
    },
    {
        "title": "Knowledge Flow Analysis for Security Protocols",
        "authors": [
            "Emina Torlak",
            "Marten van Dijk",
            "Blaise Gassend",
            "Daniel Jackson",
            "Srinivas Devadas"
        ],
        "summary": "Knowledge flow analysis offers a simple and flexible way to find flaws in security protocols. A protocol is described by a collection of rules constraining the propagation of knowledge amongst principals. Because this characterization corresponds closely to informal descriptions of protocols, it allows a succinct and natural formalization; because it abstracts away message ordering, and handles communications between principals and applications of cryptographic primitives uniformly, it is readily represented in a standard logic. A generic framework in the Alloy modelling language is presented, and instantiated for two standard protocols, and a new key management scheme.",
        "published": "2006-05-24T15:58:59Z",
        "link": "http://arxiv.org/abs/cs/0605109v1",
        "categories": [
            "cs.CR",
            "cs.SE"
        ]
    },
    {
        "title": "Modeling Aspect Mechanisms: A Top-Down Approach",
        "authors": [
            "Sergei Kojarski",
            "David H. Lorenz"
        ],
        "summary": "A plethora of diverse aspect mechanisms exist today, all of which integrate concerns into artifacts that exhibit crosscutting structure. What we lack and need is a characterization of the design space that these aspect mechanisms inhabit and a model description of their weaving processes. A good design space representation provides a common framework for understanding and evaluating existing mechanisms. A well-understood model of the weaving process can guide the implementor of new aspect mechanisms. It can guide the designer when mechanisms implementing new kinds of weaving are needed. It can also help teach aspect-oriented programming (AOP). In this paper we present and evaluate such a model of the design space for aspect mechanisms and their weaving processes. We model weaving, at an abstract level, as a concern integration process. We derive a weaving process model (WPM) top-down, differentiating a reactive from a nonreactive process. The model provides an in-depth explanation of the key subpro existing aspect mechanisms.",
        "published": "2006-06-01T02:14:16Z",
        "link": "http://arxiv.org/abs/cs/0606003v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.10; D.1.5; D.3.2"
        ]
    },
    {
        "title": "Building a logical model in the machining domain for CAPP expert systems",
        "authors": [
            "V. V. Kryssanov",
            "A. S. Kleshchev",
            "Y. Fukuda",
            "K. Konishi"
        ],
        "summary": "Recently, extensive efforts have been made on the application of expert system technique to solving the process planning task in the machining domain. This paper introduces a new formal method to design CAPP expert systems. The formal method is applied to provide a contour of the CAPP expert system building technology. Theoretical aspects of the formalism are described and illustrated by an example of know-how analysis. Flexible facilities to utilize multiple knowledge types and multiple planning strategies within one system are provided by the technology.",
        "published": "2006-06-07T03:46:23Z",
        "link": "http://arxiv.org/abs/cs/0606027v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.SE"
        ]
    },
    {
        "title": "Static Analysis using Parameterised Boolean Equation Systems",
        "authors": [
            "María Del Mar Gallardo",
            "Christophe Joubert",
            "Pedro Merino"
        ],
        "summary": "The well-known problem of state space explosion in model checking is even more critical when applying this technique to programming languages, mainly due to the presence of complex data structures. One recent and promising approach to deal with this problem is the construction of an abstract and correct representation of the global program state allowing to match visited states during program model exploration. In particular, one powerful method to implement abstract matching is to fill the state vector with a minimal amount of relevant variables for each program point. In this paper, we combine the on-the-fly model-checking approach (incremental construction of the program state space) and the static analysis method called influence analysis (extraction of significant variables for each program point) in order to automatically construct an abstract matching function. Firstly, we describe the problem as an alternation-free value-based mu-calculus formula, whose validity can be checked on the program model expressed as a labeled transition system (LTS). Secondly, we translate the analysis into the local resolution of a parameterised boolean equation system (PBES), whose representation enables a more efficient construction of the resulting abstract matching function. Finally, we show how our proposal may be elegantly integrated into CADP, a generic framework for both the design and analysis of distributed systems and the development of verification tools.",
        "published": "2006-06-21T07:23:25Z",
        "link": "http://arxiv.org/abs/cs/0606092v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A Product Oriented Modelling Concept: Holons for systems synchronisation   and interoperability",
        "authors": [
            "Salah Baïna",
            "Hervé Panetto",
            "Khalid Benali"
        ],
        "summary": "Nowadays, enterprises are confronted to growing needs for traceability, product genealogy and product life cycle management. To meet those needs, the enterprise and applications in the enterprise environment have to manage flows of information that relate to flows of material and that are managed in shop floor level. Nevertheless, throughout product lifecycle coordination needs to be established between reality in the physical world (physical view) and the virtual world handled by manufacturing information systems (informational view). This paper presents the \"Holon\" modelling concept as a means for the synchronisation of both physical view and informational views. Afterwards, we show how the concept of holon can play a major role in ensuring interoperability in the enterprise context.",
        "published": "2006-06-26T19:24:14Z",
        "link": "http://arxiv.org/abs/cs/0606108v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Product Centric Holons for Synchronisation and Interoperability in   Manufacturing Environments",
        "authors": [
            "Salah Baina",
            "Gérard Morel"
        ],
        "summary": "In the last few years, lot of work has been done in order to ensure enterprise applications interoperability; however, proposed solutions focus mainly on enterprise processes. Indeed, throughout product lifecycle coordination needs to be established between reality in the physical world (physical view) and the virtual world handled by manufacturing information systems (informational view). This paper presents a holonic approach that enables synchronisation of both physical and informational views. A model driven approach for interoperability is proposed to ensure interoperability of holon based models with other applications in the enterprise.",
        "published": "2006-06-27T11:21:23Z",
        "link": "http://arxiv.org/abs/cs/0606112v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A common framework for aspect mining based on crosscutting concern sorts",
        "authors": [
            "Marius Marin",
            "Leon Moonen",
            "Arie van Deursen"
        ],
        "summary": "The increasing number of aspect mining techniques proposed in literature calls for a methodological way of comparing and combining them in order to assess, and improve on, their quality. This paper addresses this situation by proposing a common framework based on crosscutting concern sorts which allows for consistent assessment, comparison and combination of aspect mining techniques. The framework identifies a set of requirements that ensure homogeneity in formulating the mining goals, presenting the results and assessing their quality.   We demonstrate feasibility of the approach by retrofitting an existing aspect mining technique to the framework, and by using it to design and implement two new mining techniques. We apply the three techniques to a known aspect mining benchmark and show how they can be consistently assessed and combined to increase the quality of the results. The techniques and combinations are implemented in FINT, our publicly available free aspect mining tool.",
        "published": "2006-06-27T12:40:04Z",
        "link": "http://arxiv.org/abs/cs/0606113v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "Formalizing typical crosscutting concerns",
        "authors": [
            "Marius Marin"
        ],
        "summary": "We present a consistent system for referring crosscutting functionality, relating crosscutting concerns to specific implementation idioms, and formalizing their underlying relations through queries. The system is based on generic crosscutting concerns that we organize and describe in a catalog.   We have designed and implemented a tool support for querying source code for instances of the proposed generic concerns and organizing them in composite concern models. The composite concern model adds a new dimension to the dominant decomposition of the system for describing and making explicit source code relations specific to crosscutting concerns implementations.   We use the proposed approach to describe crosscutting concerns in design patterns and apply the tool to an opensource system (JHotDraw).",
        "published": "2006-06-29T20:04:25Z",
        "link": "http://arxiv.org/abs/cs/0606125v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "Applying and Combining Three Different Aspect Mining Techniques",
        "authors": [
            "Mariano Ceccato",
            "Marius Marin",
            "Kim Mens",
            "Leon Moonen",
            "Paolo Tonella",
            "Tom Tourwe"
        ],
        "summary": "Understanding a software system at source-code level requires understanding the different concerns that it addresses, which in turn requires a way to identify these concerns in the source code. Whereas some concerns are explicitly represented by program entities (like classes, methods and variables) and thus are easy to identify, crosscutting concerns are not captured by a single program entity but are scattered over many program entities and are tangled with the other concerns. Because of their crosscutting nature, such crosscutting concerns are difficult to identify, and reduce the understandability of the system as a whole.   In this paper, we report on a combined experiment in which we try to identify crosscutting concerns in the JHotDraw framework automatically. We first apply three independently developed aspect mining techniques to JHotDraw and evaluate and compare their results. Based on this analysis, we present three interesting combinations of these three techniques, and show how these combinations provide a more complete coverage of the detected concerns as compared to the original techniques individually. Our results are a first step towards improving the understandability of a system that contains crosscutting concerns, and can be used as a basis for refactoring the identified crosscutting concerns into aspects.",
        "published": "2006-07-02T17:16:37Z",
        "link": "http://arxiv.org/abs/cs/0607006v1",
        "categories": [
            "cs.SE",
            "cs.PL"
        ]
    },
    {
        "title": "Use of UML and Model Transformations for Workflow Process Definitions",
        "authors": [
            "Audris Kalnins",
            "Valdis Vitolins"
        ],
        "summary": "Currently many different modeling languages are used for workflow definitions in BPM systems. Authors of this paper analyze the two most popular graphical languages, with highest possibility of wide practical usage - UML Activity diagrams (AD) and Business Process Modeling Notation (BPMN). The necessary in practice workflow aspects are briefly discussed, and on this basis a natural AD profile is proposed, which covers all of them. A functionally equivalent BPMN subset is also selected. The semantics of both languages in the context of process execution (namely, mapping to BPEL) is also analyzed in the paper. By analyzing AD and BPMN metamodels, authors conclude that an exact transformation from AD to BPMN is not trivial even for the selected subset, though these languages are considered to be similar. Authors show how this transformation could be defined in the MOLA transformation language.",
        "published": "2006-07-11T09:23:09Z",
        "link": "http://arxiv.org/abs/cs/0607044v1",
        "categories": [
            "cs.SE",
            "D.2.2; D.2.13; D.3.1"
        ]
    },
    {
        "title": "Prioritizing Software Inspection Results using Static Profiling",
        "authors": [
            "Cathal Boogerd",
            "Leon Moonen"
        ],
        "summary": "Static software checking tools are useful as an additional automated software inspection step that can easily be integrated in the development cycle and assist in creating secure, reliable and high quality code. However, an often quoted disadvantage of these tools is that they generate an overly large number of warnings, including many false positives due to the approximate analysis techniques. This information overload effectively limits their usefulness.   In this paper we present ELAN, a technique that helps the user prioritize the information generated by a software inspection tool, based on a demand-driven computation of the likelihood that execution reaches the locations for which warnings are reported. This analysis is orthogonal to other prioritization techniques known from literature, such as severity levels and statistical analysis to reduce false positives. We evaluate feasibility of our technique using a number of case studies and assess the quality of our predictions by comparing them to actual values obtained by dynamic profiling.",
        "published": "2006-07-12T20:35:10Z",
        "link": "http://arxiv.org/abs/cs/0607063v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Program Spectra Analysis in Embedded Software: A Case Study",
        "authors": [
            "Rui Abreu",
            "Peter Zoeteweij",
            "Arjan JC van Gemund"
        ],
        "summary": "Because of constraints imposed by the market, embedded software in consumer electronics is almost inevitably shipped with faults and the goal is just to reduce the inherent unreliability to an acceptable level before a product has to be released. Automatic fault diagnosis is a valuable tool to capture software faults without extra effort spent on testing. Apart from a debugging aid at design and integration time, fault diagnosis can help analyzing problems during operation, which allows for more accurate system recovery. In this paper we discuss perspectives and limitations for applying a particular fault diagnosis technique, namely the analysis of program spectra, in the area of embedded software in consumer electronics devices. We illustrate these by our first experience with a test case from industry.",
        "published": "2006-07-26T09:26:14Z",
        "link": "http://arxiv.org/abs/cs/0607116v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Web-Based Enterprise Information Systems Development: The Integrated   Methodology",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper considers software development issues for large-scale enterprise information systems (IS) with databases (DB) in global heterogeneous distributed computational environment. Due to high IT development rates, the present-day society has accumulated and rapidly increases an extremely huge data burden. Manipulating with such huge data arrays becomes an essential problem, particularly due to their global distribution, heterogeneous and weak-structured character. The conceptual approach to integrated Internet-based IS design, development and implementation is presented, including formal models, software development methodology and original software development tools for visual problem-oriented development and content management. IS implementation results proved shortening terms and reducing costs of implementation compared to commercial software available.",
        "published": "2006-07-27T07:47:35Z",
        "link": "http://arxiv.org/abs/cs/0607119v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Object-Based Groupware: Theory, Design and Implementation Issues",
        "authors": [
            "Sergey V. Zykov",
            "Gleb G. Pogodayev"
        ],
        "summary": "Document management software systems are having a wide audience at present. However, groupware as a term has a wide variety of possible definitions. Groupware classification attempt is made in this paper. Possible approaches to groupware are considered including document management, document control and mailing systems. Lattice theory and concept modelling are presented as a theoretical background for the systems in question. Current technologies in state-of-the-art document managenent software are discussed. Design and implementation aspects for user-friendly integrate enterprise systems are described. Results for a real system to be implemented are given. Perspectives of the field in question are discussed.",
        "published": "2006-07-27T10:16:16Z",
        "link": "http://arxiv.org/abs/cs/0607121v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Enterprise Content Management: Theory and Engineering for Entire   Lifecycle Support",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper considers enterprise content management (ECM) issues in global heterogeneous distributed computational environment. Present-day enterprises have accumulated a huge data burden. Manipulating with such a bulk becomes an essential problem, particularly due to its global distribution, heterogeneous and weak-structured character. The conceptual approach to integrated ECM lifecycle support is presented, including overview of formal models, software development methodology and innovative software development tools. Implementation results proved shortening terms and reducing costs of implementation compared to commercial software available.",
        "published": "2006-07-27T10:32:27Z",
        "link": "http://arxiv.org/abs/cs/0607122v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Enterprise Portal Development Tools: Problem-Oriented Approach",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper deals with problem-oriented visual information system (IS) engineering for enterprise Internet-based applications, which is a vital part of the whole development process. The suggested approach is based on semantic network theory and a novel ConceptModeller CASE tool.",
        "published": "2006-07-27T11:24:45Z",
        "link": "http://arxiv.org/abs/cs/0607123v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "ConceptModeller: a Problem-Oriented Visual SDK for Globally Distributed   Enterprise Systems",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper describes problem-oriented approach to software development. The approach is a part of the original integrated methodology of enterprise Internet-based software design and implementation. All aspects of software development, from theory to implementation, are covered.",
        "published": "2006-07-27T11:35:19Z",
        "link": "http://arxiv.org/abs/cs/0607124v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Enterprise Portal: from Model to Implementation",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Portal technology can significantly improve the entire corporate information infrastructure. The approach proposed is based on rigorous and consistent (meta)data model and provides for efficient and accurate front-end integration of heterogeneous corporate applications including enterprise resource planning (ERP) systems, multimedia data warehouses and proprietary content databases. The methodology proposed embraces entire software lifecycle; it is illustrated by an enterprise-level Intranet portal implementation.",
        "published": "2006-07-27T12:12:01Z",
        "link": "http://arxiv.org/abs/cs/0607125v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Abstract Machine as a Model of Content Management Information System",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Enterprise content management is an urgent issue of current scientific and practical activities in software design and implementation. However, papers known as yet give insufficient coverage of theoretical background of the software in question. The paper gives an attempt of building a state-based model of content management. In accordance with the theoretical principles outlined, a content management information system (CMIS) has been implemented in a large international oil-and-gas group of companies.",
        "published": "2006-07-27T12:40:21Z",
        "link": "http://arxiv.org/abs/cs/0607126v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Integrating Enterprise Software Applications with Web Portal Technology",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Web-portal based approach can significantly improve the entire corporate information infrastructure. The approach proposed provides for rapid and accurate front-end integration of heterogeneous corporate applications including enterprise resource planning (ERP) systems. Human resources ERP component and multimedia data warehouse implementations are discussed as essential instances.",
        "published": "2006-07-27T12:49:54Z",
        "link": "http://arxiv.org/abs/cs/0607127v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "The Integrated Approach to ERP: Embracing the Web",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Integrated approach to enterprise resource planning (ERP) software design and implementation can significantly improve the entire corporate information infrastructure and it helps to benefit from power of Internet services. The approach proposed provides for corporate Web portal integrity, consistency, urgency and front-end data processing. Human resources (HR) ERP component implementation is discussed as an essential instance.",
        "published": "2006-07-27T12:58:51Z",
        "link": "http://arxiv.org/abs/cs/0607128v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Enterprise Resource Planning Systems: the Integrated Approach",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Enterprise resource planning (ERP) systems enjoy an increasingly wide coverage. However, no truly integrate solution has been proposed as yet. ERP classification is given. Recent trends in commercial systems are analyzed on the basis of human resources (HR) management software. An innovative \"straight through\" design and implementation process of an open, secure, and scalable integrated event-driven enterprise solution is suggested. Implementation results are presented.",
        "published": "2006-07-27T13:07:42Z",
        "link": "http://arxiv.org/abs/cs/0607129v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Towards Implementing an Enterprise Groupware-Integrated Human Resources   Information System",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Human resources management software is having a wide audience at present. However, no truly integrate solution has been proposed yet to improve the systems concerned. Approaches to extra data collection for appraisal decision-making are considered on the concept modeling theoretical basis. Current technologies in state-of-the-art HR management software are compared. Design and implementation aspects for a Web-wired truly integrated secure and scalable event-driven enterprise system are described. Benchmark results are presented. Field perspectives are discussed.",
        "published": "2006-07-27T13:14:18Z",
        "link": "http://arxiv.org/abs/cs/0607130v1",
        "categories": [
            "cs.SE",
            "cs.CY"
        ]
    },
    {
        "title": "Morphisms of Coloured Petri Nets",
        "authors": [
            "Joachim Wehler"
        ],
        "summary": "We introduce the concept of a morphism between coloured nets. Our definition generalizes Petris definition for ordinary nets. A morphism of coloured nets maps the topological space of the underlying undirected net as well as the kernel and cokernel of the incidence map. The kernel are flows along the transition-bordered fibres of the morphism, the cokernel are classes of markings of the place-bordered fibres. The attachment of bindings, colours, flows and marking classes to a subnet is formalized by using concepts from sheaf theory. A coloured net is a sheaf-cosheaf pair over a Petri space and a morphism between coloured nets is a morphism between such pairs. Coloured nets and their morphisms form a category. We prove the existence of a product in the subcategory of sort-respecting morphisms. After introducing markings our concepts generalize to coloured Petri nets.",
        "published": "2006-08-07T19:03:07Z",
        "link": "http://arxiv.org/abs/cs/0608038v1",
        "categories": [
            "cs.SE",
            "D.2.2"
        ]
    },
    {
        "title": "Heap Reference Analysis Using Access Graphs",
        "authors": [
            "Uday Khedker",
            "Amitabha Sanyal",
            "Amey Karkare"
        ],
        "summary": "Despite significant progress in the theory and practice of program analysis, analysing properties of heap data has not reached the same level of maturity as the analysis of static and stack data. The spatial and temporal structure of stack and static data is well understood while that of heap data seems arbitrary and is unbounded. We devise bounded representations which summarize properties of the heap data. This summarization is based on the structure of the program which manipulates the heap. The resulting summary representations are certain kinds of graphs called access graphs. The boundedness of these representations and the monotonicity of the operations to manipulate them make it possible to compute them through data flow analysis.   An important application which benefits from heap reference analysis is garbage collection, where currently liveness is conservatively approximated by reachability from program variables. As a consequence, current garbage collectors leave a lot of garbage uncollected, a fact which has been confirmed by several empirical studies. We propose the first ever end-to-end static analysis to distinguish live objects from reachable objects. We use this information to make dead objects unreachable by modifying the program. This application is interesting because it requires discovering data flow information representing complex semantics. In particular, we discover four properties of heap data: liveness, aliasing, availability, and anticipability. Together, they cover all combinations of directions of analysis (i.e. forward and backward) and confluence of information (i.e. union and intersection). Our analysis can also be used for plugging memory leaks in C/C++ languages.",
        "published": "2006-08-28T11:15:00Z",
        "link": "http://arxiv.org/abs/cs/0608104v3",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.4; F.3.2"
        ]
    },
    {
        "title": "An Architectural Style for Ajax",
        "authors": [
            "Ali Mesbah",
            "Arie van Deursen"
        ],
        "summary": "A new breed of web application, dubbed AJAX, is emerging in response to a limited degree of interactivity in large-grain stateless Web interactions. At the heart of this new approach lies a single page interaction model that facilitates rich interactivity. We have studied and experimented with several AJAX frameworks trying to understand their architectural properties. In this paper, we summarize three of these frameworks and examine their properties and introduce the SPIAR architectural style. We describe the guiding software engineering principles and the constraints chosen to induce the desired properties. The style emphasizes user interface component development, and intermediary delta-communication between client/server components, to improve user interactivity and ease of development. In addition, we use the concepts and principles to discuss various open issues in AJAX frameworks and application development.",
        "published": "2006-08-29T10:10:32Z",
        "link": "http://arxiv.org/abs/cs/0608111v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Linux, Open Source and Unicode",
        "authors": [
            "Prashant"
        ],
        "summary": "The paper is taken out.",
        "published": "2006-09-06T18:12:27Z",
        "link": "http://arxiv.org/abs/cs/0609024v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "A XML Schema Definition based Universal User Interface",
        "authors": [
            "Prashant"
        ],
        "summary": "The article is taken out for change of contents.",
        "published": "2006-09-06T18:21:23Z",
        "link": "http://arxiv.org/abs/cs/0609025v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "On Verifying Complex Properties using Symbolic Shape Analysis",
        "authors": [
            "Thomas Wies",
            "Viktor Kuncak",
            "Karen Zee",
            "Andreas Podelski",
            "Martin Rinard"
        ],
        "summary": "One of the main challenges in the verification of software systems is the analysis of unbounded data structures with dynamic memory allocation, such as linked data structures and arrays. We describe Bohne, a new analysis for verifying data structures. Bohne verifies data structure operations and shows that 1) the operations preserve data structure invariants and 2) the operations satisfy their specifications expressed in terms of changes to the set of objects stored in the data structure. During the analysis, Bohne infers loop invariants in the form of disjunctions of universally quantified Boolean combinations of formulas. To synthesize loop invariants of this form, Bohne uses a combination of decision procedures for Monadic Second-Order Logic over trees, SMT-LIB decision procedures (currently CVC Lite), and an automated reasoner within the Isabelle interactive theorem prover. This architecture shows that synthesized loop invariants can serve as a useful communication mechanism between different decision procedures. Using Bohne, we have verified operations on data structures such as linked lists with iterators and back pointers, trees with and without parent pointers, two-level skip lists, array data structures, and sorted lists. We have deployed Bohne in the Hob and Jahob data structure analysis systems, enabling us to combine Bohne with analyses of data structure clients and apply it in the context of larger programs. This report describes the Bohne algorithm as well as techniques that Bohne uses to reduce the ammount of annotations and the running time of the analysis.",
        "published": "2006-09-18T14:52:16Z",
        "link": "http://arxiv.org/abs/cs/0609104v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "Verification, Validation and Integrity of Distributed and Interchanged   Rule Based Policies and Contracts in the Semantic Web",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Rule-based policy and contract systems have rarely been studied in terms of their software engineering properties. This is a serious omission, because in rule-based policy or contract representation languages rules are being used as a declarative programming language to formalize real-world decision logic and create IS production systems upon. This paper adopts an SE methodology from extreme programming, namely test driven development, and discusses how it can be adapted to verification, validation and integrity testing (V&V&I) of policy and contract specifications. Since, the test-driven approach focuses on the behavioral aspects and the drawn conclusions instead of the structure of the rule base and the causes of faults, it is independent of the complexity of the rule language and the system under test and thus much easier to use and understand for the rule engineer and the user.",
        "published": "2006-09-21T11:50:24Z",
        "link": "http://arxiv.org/abs/cs/0609119v2",
        "categories": [
            "cs.AI",
            "cs.SE",
            "K.6.3; I.2; I.2.4"
        ]
    },
    {
        "title": "Rule-based Knowledge Representation for Service Level Agreement",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Automated management and monitoring of service contracts like Service Level Agreements (SLAs) or higher-level policies is vital for efficient and reliable distributed service-oriented architectures (SOA) with high quality of ser-vice (QoS) levels. IT service provider need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technol-ogy. I propose a novel rule-based knowledge representation (KR) for SLA rules and a respective rule-based service level management (RBSLM) framework. My rule-based approach based on logic programming provides several advantages including automated rule chaining allowing for compact knowledge representation and high levels of automation as well as flexibility to adapt to rapidly changing business requirements. Therewith, I address an urgent need service-oriented busi-nesses do have nowadays which is to dynamically change their business and contractual logic in order to adapt to rapidly changing business environments and to overcome the restricting nature of slow change cycles.",
        "published": "2006-09-21T12:04:33Z",
        "link": "http://arxiv.org/abs/cs/0609120v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.LO",
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "ECA-LP / ECA-RuleML: A Homogeneous Event-Condition-Action Logic   Programming Language",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "Event-driven reactive functionalities are an urgent need in nowadays distributed service-oriented applications and (Semantic) Web-based environments. An important problem to be addressed is how to correctly and efficiently capture and process the event-based behavioral, reactive logic represented as ECA rules in combination with other conditional decision logic which is represented as derivation rules. In this paper we elaborate on a homogeneous integration approach which combines derivation rules, reaction rules (ECA rules) and other rule types such as integrity constraint into the general framework of logic programming. The developed ECA-LP language provides expressive features such as ID-based updates with support for external and self-updates of the intensional and extensional knowledge, transac-tions including integrity testing and an event algebra to define and process complex events and actions based on a novel interval-based Event Calculus variant.",
        "published": "2006-09-26T14:36:47Z",
        "link": "http://arxiv.org/abs/cs/0609143v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SE",
            "I.2"
        ]
    },
    {
        "title": "Identifying Crosscutting Concerns Using Fan-in Analysis",
        "authors": [
            "Marius Marin",
            "Arie van Deursen",
            "Leon Moonen"
        ],
        "summary": "Aspect mining is a reverse engineering process that aims at finding crosscutting concerns in existing systems. This paper proposes an aspect mining approach based on determining methods that are called from many different places, and hence have a high fan-in, which can be seen as a symptom of crosscutting functionality. The approach is semi-automatic, and consists of three steps: metric calculation, method filtering, and call site analysis. Carrying out these steps is an interactive process supported by an Eclipse plug-in called FINT. Fan-in analysis has been applied to three open source Java systems, totaling around 200,000 lines of code. The most interesting concerns identified are discussed in detail, which includes several concerns not previously discussed in the aspect-oriented literature. The results show that a significant number of crosscutting concerns can be recognized using fan-in analysis, and each of the three steps can be supported by tools.",
        "published": "2006-09-26T17:50:19Z",
        "link": "http://arxiv.org/abs/cs/0609147v2",
        "categories": [
            "cs.SE",
            "D.2.3; D.2.7; D.2.8"
        ]
    },
    {
        "title": "Symbolic Simulation-Checking of Dense-Time Systems",
        "authors": [
            "Farn Wang"
        ],
        "summary": "Intuitively, an (implementation) automata is simulated by a (specification) automata if every externally observable transition by the implementation automata can also be made by the specification automata. In this work, we present a symbolic algorithm for the simulation-checking of timed automatas. We first present a simulation-checking procedure that operates on state spaces, representable with convex polyhedra, of timed automatas. We then present techniques to represent those intermediate result convex polyhedra with zones and make the procedure an algorithm. We then discuss how to handle Zeno states in the implementation automata. Finally, we have endeavored to realize the algorithm and report the performance of our algorithm in the experiment.",
        "published": "2006-10-13T18:02:47Z",
        "link": "http://arxiv.org/abs/cs/0610085v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "D.3.1; F.1.1; F.4.3"
        ]
    },
    {
        "title": "Migrating Multi-page Web Applications to Single-page AJAX Interfaces",
        "authors": [
            "Ali Mesbah",
            "Arie van Deursen"
        ],
        "summary": "Recently, a new web development technique for creating interactive web applications, dubbed AJAX, has emerged. In this new model, the single-page web interface is composed of individual components which can be updated/replaced independently. With the rise of AJAX web applications classical multi-page web applications are becoming legacy systems. If until a year ago, the concern revolved around migrating legacy systems to web-based settings, today we have a new challenge of migrating web applications to single-page AJAX applications. Gaining an understanding of the navigational model and user interface structure of the source application is the first step in the migration process. In this paper, we explore how reverse engineering techniques can help analyze classic web applications for this purpose. Our approach, using a schema-based clustering technique, extracts a navigational model of web applications, and identifies candidate user interface components to be migrated to a single-page AJAX interface. Additionally, results of a case study, conducted to evaluate our tool, are presented.",
        "published": "2006-10-15T07:36:19Z",
        "link": "http://arxiv.org/abs/cs/0610094v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Partial Evaluation for Program Comprehension",
        "authors": [
            "Sandrine Blazy"
        ],
        "summary": "Program comprehension is the most tedious and time consuming task of software maintenance, an important phase of the software life cycle. This is particularly true while maintaining scientific application programs that have been written in Fortran for decades and that are still vital in various domains even though more modern languages are used to implement their user interfaces. Very often, programs have evolved as their application domains increase continually and have become very complex due to extensive modifications. This generality in programs is implemented by input variables whose value does not vary in the context of a given application. Thus, it is very interesting for the maintainer to propagate such information, that is to obtain a simplified program, which behaves like the initial one when used according to the restriction. We have adapted partial evaluation for program comprehension. Our partial evaluator performs mainly two tasks: constant propagation and statements simplification. It includes an interprocedural alias analysis. As our aim is program comprehension rather than optimization, there are two main differences with classical partial evaluation. We do not change the original",
        "published": "2006-10-16T09:05:05Z",
        "link": "http://arxiv.org/abs/cs/0610096v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Reuse of Specification Patterns with the B Method",
        "authors": [
            "Sandrine Blazy",
            "Frédéric Gervais",
            "Régine Laleau"
        ],
        "summary": "This paper describes an approach for reusing specification patterns. Specification patterns are design patterns that are expressed in a formal specification language. Reusing a specification pattern means instantiating it or composing it with other specification patterns. Three levels of composition are defined: juxtaposition, composition with inter-patterns links and unification. This paper shows through examples how to define specification patterns in B, how to reuse them directly in B, and also how to reuse the proofs associated with specification patterns.",
        "published": "2006-10-16T09:07:14Z",
        "link": "http://arxiv.org/abs/cs/0610097v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "ECA-RuleML: An Approach combining ECA Rules with temporal interval-based   KR Event/Action Logics and Transactional Update Logics",
        "authors": [
            "Adrian Paschke"
        ],
        "summary": "An important problem to be addressed within Event-Driven Architecture (EDA) is how to correctly and efficiently capture and process the event/action-based logic. This paper endeavors to bridge the gap between the Knowledge Representation (KR) approaches based on durable events/actions and such formalisms as event calculus, on one hand, and event-condition-action (ECA) reaction rules extending the approach of active databases that view events as instantaneous occurrences and/or sequences of events, on the other. We propose formalism based on reaction rules (ECA rules) and a novel interval-based event logic and present concrete RuleML-based syntax, semantics and implementation. We further evaluate this approach theoretically, experimentally and on an example derived from common industry use cases and illustrate its benefits.",
        "published": "2006-10-30T11:56:08Z",
        "link": "http://arxiv.org/abs/cs/0610167v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.MA",
            "cs.SE",
            "I.2; H.2.4; I.2.5; I.2.4; K.6.3"
        ]
    },
    {
        "title": "Capabilities Engineering: Constructing Change-Tolerant Systems",
        "authors": [
            "Ramya Ravichandar",
            "James D. Arthur",
            "Shawn A. Bohner"
        ],
        "summary": "We propose a Capabilities-based approach for building long-lived, complex systems that have lengthy development cycles. User needs and technology evolve during these extended development periods, and thereby, inhibit a fixed requirements-oriented solution specification. In effect, for complex emergent systems, the traditional approach of baselining requirements results in an unsatisfactory system. Therefore, we present an alternative approach, Capabilities Engineering, which mathematically exploits the structural semantics of the Function Decomposition graph - a representation of user needs - to formulate Capabilities. For any given software system, the set of derived Capabilities embodies change-tolerant characteristics. More specifically, each individual Capability is a functional abstraction constructed to be highly cohesive and to be minimally coupled with its neighbors. Moreover, the Capability set is chosen to accommodate an incremental development approach, and to reflect the constraints of technology feasibility and implementation schedules. We discuss our validation activities to empirically prove that the Capabilities-based approach results in change-tolerant systems.",
        "published": "2006-11-15T17:12:54Z",
        "link": "http://arxiv.org/abs/cs/0611071v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Reconciling Synthesis and Decomposition: A Composite Approach to   Capability Identification",
        "authors": [
            "Ramya Ravichandar",
            "James D. Arthur",
            "Robert P. Broadwater"
        ],
        "summary": "Stakeholders' expectations and technology constantly evolve during the lengthy development cycles of a large-scale computer based system. Consequently, the traditional approach of baselining requirements results in an unsatisfactory system because it is ill-equipped to accommodate such change. In contrast, systems constructed on the basis of Capabilities are more change-tolerant; Capabilities are functional abstractions that are neither as amorphous as user needs nor as rigid as system requirements. Alternatively, Capabilities are aggregates that capture desired functionality from the users' needs, and are designed to exhibit desirable software engineering characteristics of high cohesion, low coupling and optimum abstraction levels. To formulate these functional abstractions we develop and investigate two algorithms for Capability identification: Synthesis and Decomposition. The synthesis algorithm aggregates detailed rudimentary elements of the system to form Capabilities. In contrast, the decomposition algorithm determines Capabilities by recursively partitioning the overall mission of the system into more detailed entities. Empirical analysis on a small computer based library system reveals that neither approach is sufficient by itself. However, a composite algorithm based on a complementary approach reconciling the two polar perspectives results in a more feasible set of Capabilities. In particular, the composite algorithm formulates Capabilities using the cohesion and coupling measures as defined by the decomposition algorithm and the abstraction level as determined by the synthesis algorithm.",
        "published": "2006-11-15T22:32:43Z",
        "link": "http://arxiv.org/abs/cs/0611072v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Design approaches in technology enhanced learning",
        "authors": [
            "Yishay Mor",
            "Niall Winters"
        ],
        "summary": "Design is a critical to the successful development of any interactive learning environment (ILE). Moreover, in technology enhanced learning (TEL), the design process requires input from many diverse areas of expertise. As such, anyone undertaking tool development is required to directly address the design challenge from multiple perspectives. We provide a motivation and rationale for design approaches for learning technologies that draws upon Simon's seminal proposition of Design Science (Simon, 1969). We then review the application of Design Experiments (Brown, 1992) and Design Patterns (Alexander et al., 1977) and argue that a patterns approach has the potential to address many of the critical challenges faced by learning technologists.",
        "published": "2006-11-20T01:19:37Z",
        "link": "http://arxiv.org/abs/cs/0611092v1",
        "categories": [
            "cs.SE",
            "cs.CY"
        ]
    },
    {
        "title": "The Implications of Network-Centric Software Systems on Software   Architecture: A Critical Evaluation",
        "authors": [
            "Amine Chigani",
            "James D. Arthur"
        ],
        "summary": "The purpose of this paper is to evaluate the impact of emerging network-centric software systems on the field of software architecture. We first develop an insight concerning the term \"network-centric\" by presenting its origin and its implications within the context of software architecture. On the basis of this insight, we present our definition of a network-centric framework and its distinguishing characteristics. We then enumerate the challenges that face the field of software architecture as software development shifts from a platform-centric to a network-centric model. In order to face these challenges, we propose a formal approach embodied in a new architectural style that supports overcoming these challenges at the architectural level. Finally, we conclude by presenting an illustrative example to demonstrate the usefulness of the concepts of network centricity, summarizing our contributions, and linking our approach to future work that needs to be done in this area.",
        "published": "2006-11-21T19:23:32Z",
        "link": "http://arxiv.org/abs/cs/0611110v3",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Knowledge Representation Concepts for Automated SLA Management",
        "authors": [
            "Adrian Paschke",
            "Martin Bichler"
        ],
        "summary": "Outsourcing of complex IT infrastructure to IT service providers has increased substantially during the past years. IT service providers must be able to fulfil their service-quality commitments based upon predefined Service Level Agreements (SLAs) with the service customer. They need to manage, execute and maintain thousands of SLAs for different customers and different types of services, which needs new levels of flexibility and automation not available with the current technology. The complexity of contractual logic in SLAs requires new forms of knowledge representation to automatically draw inferences and execute contractual agreements. A logic-based approach provides several advantages including automated rule chaining allowing for compact knowledge representation as well as flexibility to adapt to rapidly changing business requirements. We suggest adequate logical formalisms for representation and enforcement of SLA rules and describe a proof-of-concept implementation. The article describes selected formalisms of the ContractLog KR and their adequacy for automated SLA management and presents results of experiments to demonstrate flexibility and scalability of the approach.",
        "published": "2006-11-23T13:25:45Z",
        "link": "http://arxiv.org/abs/cs/0611122v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "I.2"
        ]
    },
    {
        "title": "Developing efficient parsers in Prolog: the CLF manual (v1.0)",
        "authors": [
            "Thierry Despeyroux"
        ],
        "summary": "This document describes a couple of tools that help to quickly design and develop computer (formalized) languages. The first one use Flex to perform lexical analysis and the second is an extention of Prolog DCGs to perfom syntactical analysis. Initially designed as a new component for the Centaur system, these tools are now available independently and can be used to construct efficient Prolog parsers that can be integrated in Prolog or heterogeneous systems. This is the initial version of the CLF documentation. Updated version will be available online when necessary.",
        "published": "2006-12-18T08:40:53Z",
        "link": "http://arxiv.org/abs/cs/0612082v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Agile Adoption Process Framework",
        "authors": [
            "Ahmed Sidky",
            "James Arthur"
        ],
        "summary": "Today many organizations aspire to adopt agile processes in hope of overcoming some of the difficulties they are facing with their current software development process. There is no structured framework for the agile adoption process. This paper presents a 3-Stage process framework that assists organization and guides organizations through their agile adoption efforts. The Process Framework has been received significantly positive feedback from experts and leaders in agile adoption industry.",
        "published": "2006-12-19T16:35:46Z",
        "link": "http://arxiv.org/abs/cs/0612092v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Architecting Network-Centric Software Systems: A Style-Based Beginning",
        "authors": [
            "Amine Chigani James D. Arthur Shawn Bohner"
        ],
        "summary": "With the advent of potent network technology, software development has evolved from traditional platform-centric construction to network-centric evolution. This change involves largely the way we reason about systems as evidenced in the introduction of Network- Centric Operations (NCO). Unfortunately, it has resulted in conflicting interpretations of how to map NCO concepts to the field of software architecture. In this paper, we capture the core concepts and goals of NCO, investigate the implications of these concepts and goals on software architecture, and identify the operational characteristics that distinguish network-centric software systems from other systems. More importantly, we use architectural design principles to propose an outline for a network-centric architectural style that helps in characterizing network-centric software systems and that provides a means by which their distinguishing operational characteristics can be realized.",
        "published": "2006-12-23T01:00:32Z",
        "link": "http://arxiv.org/abs/cs/0612131v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Analyse non standard du bruit",
        "authors": [
            "Michel Fliess"
        ],
        "summary": "Thanks to the nonstandard formalization of fast oscillating functions, due to P. Cartier and Y. Perrin, an appropriate mathematical framework is derived for new non-asymptotic estimation techniques, which do not necessitate any statistical analysis of the noises corrupting any sensor. Various applications are deduced for multiplicative noises, for the length of the parametric estimation windows, and for burst errors.",
        "published": "2006-03-01T11:26:22Z",
        "link": "http://arxiv.org/abs/cs/0603003v1",
        "categories": [
            "cs.CE",
            "math.LO",
            "math.OC",
            "math.PR",
            "quant-ph"
        ]
    },
    {
        "title": "Compression ratios based on the Universal Similarity Metric still yield   protein distances far from CATH distances",
        "authors": [
            "Jairo Rocha",
            "Francesc Rosselló",
            "Joan Segura"
        ],
        "summary": "Kolmogorov complexity has inspired several alignment-free distance measures, based on the comparison of lengths of compressions, which have been applied successfully in many areas. One of these measures, the so-called Universal Similarity Metric (USM), has been used by Krasnogor and Pelta to compare simple protein contact maps, showing that it yielded good clustering on four small datasets. We report an extensive test of this metric using a much larger and representative protein dataset: the domain dataset used by Sierk and Pearson to evaluate seven protein structure comparison methods and two protein sequence comparison methods. One result is that Krasnogor-Pelta method has less domain discriminant power than any one of the methods considered by Sierk and Pearson when using these simple contact maps. In another test, we found that the USM based distance has low agreement with the CATH tree structure for the same benchmark of Sierk and Pearson. In any case, its agreement is lower than the one of a standard sequential alignment method, SSEARCH. Finally, we manually found lots of small subsets of the database that are better clustered using SSEARCH than USM, to confirm that Krasnogor-Pelta's conclusions were based on datasets that were too small.",
        "published": "2006-03-06T12:00:41Z",
        "link": "http://arxiv.org/abs/q-bio/0603007v2",
        "categories": [
            "q-bio.QM",
            "cs.CE",
            "physics.data-an",
            "q-bio.OT"
        ]
    },
    {
        "title": "Vers une commande multivariable sans modèle",
        "authors": [
            "Michel Fliess",
            "Cédric Join",
            "Mamadou Mboup",
            "Hebertt Sira-Ramirez"
        ],
        "summary": "A control strategy without any precise mathematical model is derived for linear or nonlinear systems which are assumed to be finite-dimensional. Two convincing numerical simulations are provided.",
        "published": "2006-03-07T08:15:15Z",
        "link": "http://arxiv.org/abs/math/0603155v3",
        "categories": [
            "math.OC",
            "cs.CE",
            "cs.RO",
            "physics.class-ph"
        ]
    },
    {
        "title": "The transposition distance for phylogenetic trees",
        "authors": [
            "Francesc Rossello",
            "Gabriel Valiente"
        ],
        "summary": "The search for similarity and dissimilarity measures on phylogenetic trees has been motivated by the computation of consensus trees, the search by similarity in phylogenetic databases, and the assessment of clustering results in bioinformatics. The transposition distance for fully resolved phylogenetic trees is a recent addition to the extensive collection of available metrics for comparing phylogenetic trees. In this paper, we generalize the transposition distance from fully resolved to arbitrary phylogenetic trees, through a construction that involves an embedding of the set of phylogenetic trees with a fixed number of labeled leaves into a symmetric group and a generalization of Reidys-Stadler's involution metric for RNA contact structures. We also present simple linear-time algorithms for computing it.",
        "published": "2006-04-18T19:24:33Z",
        "link": "http://arxiv.org/abs/q-bio/0604024v1",
        "categories": [
            "q-bio.PE",
            "cs.CE",
            "math.GR",
            "q-bio.OT"
        ]
    },
    {
        "title": "The emergence of knowledge exchange: an agent-based model of a software   market",
        "authors": [
            "Maria Chli",
            "Philippe De Wilde"
        ],
        "summary": "We investigate knowledge exchange among commercial organisations, the rationale behind it and its effects on the market. Knowledge exchange is known to be beneficial for industry, but in order to explain it, authors have used high level concepts like network effects, reputation and trust. We attempt to formalise a plausible and elegant explanation of how and why companies adopt information exchange and why it benefits the market as a whole when this happens. This explanation is based on a multi-agent model that simulates a market of software providers. Even though the model does not include any high-level concepts, information exchange naturally emerges during simulations as a successful profitable behaviour. The conclusions reached by this agent-based analysis are twofold: (1) A straightforward set of assumptions is enough to give rise to exchange in a software market. (2) Knowledge exchange is shown to increase the efficiency of the market.",
        "published": "2006-04-20T11:20:16Z",
        "link": "http://arxiv.org/abs/cs/0604078v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "Simplicial models of social aggregation I",
        "authors": [
            "Mirco A. Mannucci",
            "Lisa Sparks",
            "Daniele C. Struppa"
        ],
        "summary": "This paper presents the foundational ideas for a new way of modeling social aggregation. Traditional approaches have been using network theory, and the theory of random networks. Under that paradigm, every social agent is represented by a node, and every social interaction is represented by a segment connecting two nodes. Early work in family interactions, as well as more recent work in the study of terrorist organizations, shows that network modeling may be insufficient to describe the complexity of human social structures. Specifically, network theory does not seem to have enough flexibility to represent higher order aggregations, where several agents interact as a group, rather than as a collection of pairs. The model we present here uses a well established mathematical theory, the theory of simplicial complexes, to address this complex issue prevalent in interpersonal and intergroup communication. The theory enables us to provide a richer graphical representation of social interactions, and to determine quantitative mechanisms to describe the robustness of a social structure. We also propose a methodology to create random simplicial complexes, with the purpose of providing a new method to simulate computationally the creation and disgregation of social structures. Finally, we propose several measures which could be taken and observed in order to describe and study an actual social aggregation occurring in interpersonal and intergroup contexts.",
        "published": "2006-04-23T19:28:07Z",
        "link": "http://arxiv.org/abs/cs/0604090v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Analytic Properties and Covariance Functions of a New Class of   Generalized Gibbs Random Fields",
        "authors": [
            "Dionissios T. Hristopulos",
            "Samuel Elogne"
        ],
        "summary": "Spartan Spatial Random Fields (SSRFs) are generalized Gibbs random fields, equipped with a coarse-graining kernel that acts as a low-pass filter for the fluctuations. SSRFs are defined by means of physically motivated spatial interactions and a small set of free parameters (interaction couplings). This paper focuses on the FGC-SSRF model, which is defined on the Euclidean space $\\mathbb{R}^{d}$ by means of interactions proportional to the squares of the field realizations, as well as their gradient and curvature. The permissibility criteria of FGC-SSRFs are extended by considering the impact of a finite-bandwidth kernel. It is proved that the FGC-SSRFs are almost surely differentiable in the case of finite bandwidth. Asymptotic explicit expressions for the Spartan covariance function are derived for $d=1$ and $d=3$; both known and new covariance functions are obtained depending on the value of the FGC-SSRF shape parameter. Nonlinear dependence of the covariance integral scale on the FGC-SSRF characteristic length is established, and it is shown that the relation becomes linear asymptotically. The results presented in this paper are useful in random field parameter inference, as well as in spatial interpolation of irregularly-spaced samples.",
        "published": "2006-05-17T12:02:01Z",
        "link": "http://arxiv.org/abs/cs/0605073v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT",
            "J.2; J.3; G.3; I.4.4"
        ]
    },
    {
        "title": "Modeling the Dynamics of Social Networks",
        "authors": [
            "Victor V. Kryssanov",
            "Frank J. Rinaldo",
            "Evgeny L. Kuleshov",
            "Hitoshi Ogawa"
        ],
        "summary": "Modeling human dynamics responsible for the formation and evolution of the so-called social networks - structures comprised of individuals or organizations and indicating connectivities existing in a community - is a topic recently attracting a significant research interest. It has been claimed that these dynamics are scale-free in many practically important cases, such as impersonal and personal communication, auctioning in a market, accessing sites on the WWW, etc., and that human response times thus conform to the power law. While a certain amount of progress has recently been achieved in predicting the general response rate of a human population, existing formal theories of human behavior can hardly be found satisfactory to accommodate and comprehensively explain the scaling observed in social networks. In the presented study, a novel system-theoretic modeling approach is proposed and successfully applied to determine important characteristics of a communication network and to analyze consumer behavior on the WWW.",
        "published": "2006-05-24T02:13:13Z",
        "link": "http://arxiv.org/abs/cs/0605101v1",
        "categories": [
            "cs.CY",
            "cs.CE",
            "cs.CL",
            "cs.HC",
            "cs.NI",
            "physics.data-an"
        ]
    },
    {
        "title": "An Internet-enabled technology to support Evolutionary Design",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "K. Ueda"
        ],
        "summary": "This paper discusses the systematic use of product feedback information to support life-cycle design approaches and provides guidelines for developing a design at both the product and the system levels. Design activities are surveyed in the light of the product life cycle, and the design information flow is interpreted from a semiotic perspective. The natural evolution of a design is considered, the notion of design expectations is introduced, and the importance of evaluation of these expectations in dynamic environments is argued. Possible strategies for reconciliation of the expectations and environmental factors are described. An Internet-enabled technology is proposed to monitor product functionality, usage, and operational environment and supply the designer with relevant information. A pilot study of assessing design expectations of a refrigerator is outlined, and conclusions are drawn.",
        "published": "2006-05-25T04:39:11Z",
        "link": "http://arxiv.org/abs/cs/0605119v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "cs.AR",
            "cs.MA",
            "cs.NI"
        ]
    },
    {
        "title": "Understanding Design Fundamentals: How Synthesis and Analysis Drive   Creativity, Resulting in Emergence",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "S. Kitamura"
        ],
        "summary": "This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought - analogical and metaphorical reasoning - is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.",
        "published": "2006-05-25T11:35:39Z",
        "link": "http://arxiv.org/abs/cs/0605120v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "The meaning of manufacturing know-how",
        "authors": [
            "V. V. Kryssanov",
            "V. A. Abramov",
            "Y. Fukuda",
            "K. Konishi"
        ],
        "summary": "This paper investigates the phenomenon of manufacturing know-how. First, the abstract notion of knowledge is discussed, and a terminological basis is introduced to treat know-how as a kind of knowledge. Next, a brief survey of the recently reported works dealt with manufacturing know-how is presented, and an explicit definition of know-how is formulated. Finally, the problem of utilizing know-how with knowledge-based systems is analyzed, and some ideas useful for its solving are given.",
        "published": "2006-05-30T05:12:35Z",
        "link": "http://arxiv.org/abs/cs/0605138v1",
        "categories": [
            "cs.AI",
            "cs.CE"
        ]
    },
    {
        "title": "A Framework for the Development of Manufacturing Simulators: Towards New   Generation of Simulation Systems",
        "authors": [
            "V. V. Kryssanov",
            "V. A. Abramov",
            "H. Hibino",
            "Y. Fukuda"
        ],
        "summary": "In this paper, an attempt is made to systematically discuss the development of simulation systems for manufacturing system design. General requirements on manufacturing simulators are formulated and a framework to address the requirements is suggested. Problems of information representation as an activity underlying simulation are considered. This is to form the necessary mathematical foundation for manufacturing simulations. The theoretical findings are explored through a pilot study. A conclusion about the suitability of the suggested approach to the development of simulation systems for manufacturing system design is made, and implications for future research are described.",
        "published": "2006-06-01T02:36:12Z",
        "link": "http://arxiv.org/abs/cs/0606004v1",
        "categories": [
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "A Decision-Making Support System Based on Know-How",
        "authors": [
            "V. V. Kryssanov",
            "V. A. Abramov",
            "Y. Fukuda",
            "K. Konishi"
        ],
        "summary": "The research results described are concerned with: - developing a domain modeling method and tools to provide the design and implementation of decision-making support systems for computer integrated manufacturing; - building a decision-making support system based on know-how and its software environment. The research is funded by NEDO, Japan.",
        "published": "2006-06-02T03:06:07Z",
        "link": "http://arxiv.org/abs/cs/0606010v1",
        "categories": [
            "cs.CE",
            "cs.AI"
        ]
    },
    {
        "title": "A simulation engine to support production scheduling using   genetics-based machine learning",
        "authors": [
            "H. Tamaki",
            "V. V. Kryssanov",
            "S. Kitamura"
        ],
        "summary": "The ever higher complexity of manufacturing systems, continually shortening life cycles of products and their increasing variety, as well as the unstable market situation of the recent years require introducing grater flexibility and responsiveness to manufacturing processes. From this perspective, one of the critical manufacturing tasks, which traditionally attract significant attention in both academia and the industry, but which have no satisfactory universal solution, is production scheduling. This paper proposes an approach based on genetics-based machine learning (GBML) to treat the problem of flow shop scheduling. By the approach, a set of scheduling rules is represented as an individual of genetic algorithms, and the fitness of the individual is estimated based on the makespan of the schedule generated by using the rule-set. A concept of the interactive software environment consisting of a simulator and a GBML simulation engine is introduced to support human decision-making during scheduling. A pilot study is underway to evaluate the performance of the GBML technique in comparison with other methods (such as Johnson's algorithm and simulated annealing) while completing test examples.",
        "published": "2006-06-06T09:30:58Z",
        "link": "http://arxiv.org/abs/cs/0606021v1",
        "categories": [
            "cs.CE",
            "cs.AI"
        ]
    },
    {
        "title": "Building a logical model in the machining domain for CAPP expert systems",
        "authors": [
            "V. V. Kryssanov",
            "A. S. Kleshchev",
            "Y. Fukuda",
            "K. Konishi"
        ],
        "summary": "Recently, extensive efforts have been made on the application of expert system technique to solving the process planning task in the machining domain. This paper introduces a new formal method to design CAPP expert systems. The formal method is applied to provide a contour of the CAPP expert system building technology. Theoretical aspects of the formalism are described and illustrated by an example of know-how analysis. Flexible facilities to utilize multiple knowledge types and multiple planning strategies within one system are provided by the technology.",
        "published": "2006-06-07T03:46:23Z",
        "link": "http://arxiv.org/abs/cs/0606027v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.SE"
        ]
    },
    {
        "title": "Evolutionary Design: Philosophy, Theory, and Application Tactics",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "S. Kitamura"
        ],
        "summary": "Although it has contributed to remarkable improvements in some specific areas, attempts to develop a universal design theory are generally characterized by failure. This paper sketches arguments for a new approach to engineering design based on Semiotics - the science about signs. The approach is to combine different design theories over all the product life cycle stages into one coherent and traceable framework. Besides, it is to bring together the designer's and user's understandings of the notion of 'good product'. Building on the insight from natural sciences that complex systems always exhibit a self-organizing meaning-influential hierarchical dynamics, objective laws controlling product development are found through an examination of design as a semiosis process. These laws are then applied to support evolutionary design of products. An experiment validating some of the theoretical findings is outlined, and concluding remarks are given.",
        "published": "2006-06-09T08:00:37Z",
        "link": "http://arxiv.org/abs/cs/0606039v1",
        "categories": [
            "cs.CE",
            "cs.AI"
        ]
    },
    {
        "title": "Feynman Checkerboard as a Model of Discrete Space-Time",
        "authors": [
            "Edward Hanna"
        ],
        "summary": "In 1965, Feynman wrote of using a lattice containing one dimension of space and one dimension of time to derive aspects of quantum mechanics. Instead of summing the behavior of all possible paths as he did, this paper will consider the motion of single particles within this discrete Space-Time lattice, sometimes called Feynman's Checkerboard. This empirical approach yielded several predicted emergent properties for a discrete Space-Time lattice, one of which is novel and testable.",
        "published": "2006-07-06T16:35:38Z",
        "link": "http://arxiv.org/abs/cs/0607018v2",
        "categories": [
            "cs.CE",
            "I.6; J.2"
        ]
    },
    {
        "title": "Mathematical Modelling of the Thermal Accumulation in Hot Water Solar   Systems",
        "authors": [
            "Stanko Vl. Shtrakov",
            "Anton Stoilov"
        ],
        "summary": "Mathematical modelling and defining useful recommendations for construction and regimes of exploitation for hot water solar installation with thermal stratification is the main purpose of this work. A special experimental solar module for hot water was build and equipped with sufficient measure apparatus. The main concept of investigation is to optimise the stratified regime of thermal accumulation and constructive parameters of heat exchange equipment (heat serpentine in tank). Accumulation and heat exchange processes were investigated by theoretical end experimental means. Special mathematical model was composed to simulate the energy transfer in stratified tank. Computer program was developed to solve mathematical equations for thermal accumulation and energy exchange. Extensive numerical and experimental tests were carried out. A good correspondence between theoretical and experimental data was arrived. Keywords: Mathematical modelling, accumulation",
        "published": "2006-07-18T08:07:48Z",
        "link": "http://arxiv.org/abs/cs/0607083v1",
        "categories": [
            "cs.CE",
            "K.1.6"
        ]
    },
    {
        "title": "Finite element method for thermal analysis of concentrating solar   receivers",
        "authors": [
            "Stanko Shtrakov",
            "Anton Stoilov"
        ],
        "summary": "Application of finite element method and heat conductivity transfer model for calculation of temperature distribution in receiver for dish-Stirling concentrating solar system is described. The method yields discretized equations that are entirely local to the elements and provides complete geometric flexibility. A computer program solving the finite element method problem is created and great number of numerical experiments is carried out. Illustrative numerical results are given for an array of triangular elements in receiver for dish-Stirling system.",
        "published": "2006-07-19T06:58:37Z",
        "link": "http://arxiv.org/abs/cs/0607091v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Ideas by Statistical Mechanics (ISM)",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Ideas by Statistical Mechanics (ISM) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions. The program is based on the author's work in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the author's Adaptive Simulated Annealing (ASA) code for optimizations of training sets, as well as for importance-sampling to apply the author's copula financial risk-management codes, Trading in Risk Dimensions (TRD), for assessments of risk and uncertainty. This product can be used for decision support for projects ranging from diplomatic, information, military, and economic (DIME) factors of propagation/evolution of ideas, to commercial sales, trading indicators across sectors of financial markets, advertising and political campaigns, etc. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, is the proposed model. Parameters with a given subset of macrocolumns will be fit using ASA to patterns representing ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Tools of financial risk management, developed by the author to process correlated multivariate systems with differing non-Gaussian distributions using modern copula analysis, importance-sampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes.",
        "published": "2006-07-23T16:12:47Z",
        "link": "http://arxiv.org/abs/cs/0607103v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "cs.NE"
        ]
    },
    {
        "title": "Self-Replication and Self-Assembly for Manufacturing",
        "authors": [
            "Robert Ewaschuk",
            "Peter D. Turney"
        ],
        "summary": "It has been argued that a central objective of nanotechnology is to make products inexpensively, and that self-replication is an effective approach to very low-cost manufacturing. The research presented here is intended to be a step towards this vision. We describe a computational simulation of nanoscale machines floating in a virtual liquid. The machines can bond together to form strands (chains) that self-replicate and self-assemble into user-specified meshes. There are four types of machines and the sequence of machine types in a strand determines the shape of the mesh they will build. A strand may be in an unfolded state, in which the bonds are straight, or in a folded state, in which the bond angles depend on the types of machines. By choosing the sequence of machine types in a strand, the user can specify a variety of polygonal shapes. A simulation typically begins with an initial unfolded seed strand in a soup of unbonded machines. The seed strand replicates by bonding with free machines in the soup. The child strands fold into the encoded polygonal shape, and then the polygons drift together and bond to form a mesh. We demonstrate that a variety of polygonal meshes can be manufactured in the simulation, by simply changing the sequence of machine types in the seed.",
        "published": "2006-07-27T17:55:16Z",
        "link": "http://arxiv.org/abs/cs/0607133v1",
        "categories": [
            "cs.MA",
            "cs.CE",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "Stylized Facts in Internal Rates of Return on Stock Index and its   Derivative Transactions",
        "authors": [
            "Lukas Pichl",
            "Taisei Kaizoji",
            "Takuya Yamano"
        ],
        "summary": "Universal features in stock markets and their derivative markets are studied by means of probability distributions in internal rates of return on buy and sell transaction pairs. Unlike the stylized facts in log normalized returns, the probability distributions for such single asset encounters encorporate the time factor by means of the internal rate of return defined as the continuous compound interest. Resulting stylized facts are shown in the probability distributions derived from the daily series of TOPIX, S & P 500 and FTSE 100 index close values. The application of the above analysis to minute-tick data of NIKKEI 225 and its futures market, respectively, reveals an interesting diffference in the behavior of the two probability distributions, in case a threshold on the minimal duration of the long position is imposed. It is therefore suggested that the probability distributions of the internal rates of return could be used for causality mining between the underlying and derivative stock markets. The highly specific discrete spectrum, which results from noise trader strategies as opposed to the smooth distributions observed for fundamentalist strategies in single encounter transactions may be also useful in deducing the type of investment strategy from trading revenues of small portfolio investors.",
        "published": "2006-07-31T12:15:28Z",
        "link": "http://arxiv.org/abs/cs/0607140v2",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT",
            "H.1.1"
        ]
    },
    {
        "title": "Automatic Trading Agent. RMT based Portfolio Theory and Portfolio   Selection",
        "authors": [
            "Malgorzata Snarska",
            "Jakub Krzych"
        ],
        "summary": "Portfolio theory is a very powerful tool in the modern investment theory. It is helpful in estimating risk of an investor's portfolio, which arises from our lack of information, uncertainty and incomplete knowledge of reality, which forbids a perfect prediction of future price changes. Despite of many advantages this tool is not known and is not widely used among investors on Warsaw Stock Exchange. The main reason for abandoning this method is a high level of complexity and immense calculations. The aim of this paper is to introduce an automatic decision - making system, which allows a single investor to use such complex methods of Modern Portfolio Theory (MPT). The key tool in MPT is an analysis of an empirical covariance matrix. This matrix, obtained from historical data is biased by such a high amount of statistical uncertainty, that it can be seen as random. By bringing into practice the ideas of Random Matrix Theory (RMT), the noise is removed or significantly reduced, so the future risk and return are better estimated and controlled. This concepts are applied to the Warsaw Stock Exchange Simulator http://gra.onet.pl. The result of the simulation is 18 % level of gains in comparison for respective 10 % loss of the Warsaw Stock Exchange main index WIG.",
        "published": "2006-08-30T11:45:41Z",
        "link": "http://arxiv.org/abs/physics/0608293v1",
        "categories": [
            "physics.soc-ph",
            "cs.CE",
            "q-fin.PM",
            "stat.AP"
        ]
    },
    {
        "title": "A Robust Solution Procedure for Hyperelastic Solids with Large Boundary   Deformation",
        "authors": [
            "Suzanne M. Shontz",
            "Stephen A. Vavasis"
        ],
        "summary": "Compressible Mooney-Rivlin theory has been used to model hyperelastic solids, such as rubber and porous polymers, and more recently for the modeling of soft tissues for biomedical tissues, undergoing large elastic deformations. We propose a solution procedure for Lagrangian finite element discretization of a static nonlinear compressible Mooney-Rivlin hyperelastic solid. We consider the case in which the boundary condition is a large prescribed deformation, so that mesh tangling becomes an obstacle for straightforward algorithms. Our solution procedure involves a largely geometric procedure to untangle the mesh: solution of a sequence of linear systems to obtain initial guesses for interior nodal positions for which no element is inverted. After the mesh is untangled, we take Newton iterations to converge to a mechanical equilibrium. The Newton iterations are safeguarded by a line search similar to one used in optimization. Our computational results indicate that the algorithm is up to 70 times faster than a straightforward Newton continuation procedure and is also more robust (i.e., able to tolerate much larger deformations). For a few extremely large deformations, the deformed mesh could only be computed through the use of an expensive Newton continuation method while using a tight convergence tolerance and taking very small steps.",
        "published": "2006-09-01T00:07:41Z",
        "link": "http://arxiv.org/abs/cs/0609001v2",
        "categories": [
            "cs.NA",
            "cs.CE",
            "G.1.0; G.1.5; G.1.8; J.2"
        ]
    },
    {
        "title": "Modern Statistics by Kriging",
        "authors": [
            "Tomasz Suslo"
        ],
        "summary": "We present statistics (S-statistics) based only on random variable (not random value) with a mean squared error of mean estimation as a concept of error.",
        "published": "2006-09-14T12:34:59Z",
        "link": "http://arxiv.org/abs/cs/0609079v4",
        "categories": [
            "cs.NA",
            "cs.CE"
        ]
    },
    {
        "title": "Recurrence relations and fast algorithms",
        "authors": [
            "Mark Tygert"
        ],
        "summary": "We construct fast algorithms for evaluating transforms associated with families of functions which satisfy recurrence relations. These include algorithms both for computing the coefficients in linear combinations of the functions, given the values of these linear combinations at certain points, and, vice versa, for evaluating such linear combinations at those points, given the coefficients in the linear combinations; such procedures are also known as analysis and synthesis of series of certain special functions. The algorithms of the present paper are efficient in the sense that their computational costs are proportional to n (ln n) (ln(1/epsilon))^3, where n is the amount of input and output data, and epsilon is the precision of computations. Stated somewhat more precisely, we find a positive real number C such that, for any positive integer n > 10, the algorithms require at most C n (ln n) (ln(1/epsilon))^3 floating-point operations and words of memory to evaluate at n appropriately chosen points any linear combination of n special functions, given the coefficients in the linear combination, where epsilon is the precision of computations.",
        "published": "2006-09-14T17:51:11Z",
        "link": "http://arxiv.org/abs/cs/0609081v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "F.2.1; G.1.2"
        ]
    },
    {
        "title": "A comparative analysis of the geometrical surface texture of a real and   virtual model of a tooth flank of a cylindrical gear",
        "authors": [
            "Jacek Michalski",
            "Leszek Skoczylas"
        ],
        "summary": "The paper presents the methodology of modelling tooth flanks of cylindrical gears in the Cad environment. The modelling consists in a computer simulation of gear generation. A model of tooth flanks is an envelope curve of a family of envelopes that originate from the rolling motion of a solid tool model in relation to a solid model of the cylindrical gear. The surface stereometry and topography of the tooth flanks, hobbed and chiselled by Fellows method, are compared to their numerical models. Metrological measurements of the real gears were carried out using a coordinated measuring machine and a two - and a three-dimensional profilometer. A computer simulation of the gear generation was performed in the Mechanical Desktop environment.",
        "published": "2006-09-15T14:05:43Z",
        "link": "http://arxiv.org/abs/cs/0609087v1",
        "categories": [
            "cs.CE",
            "I.6.4; J.6"
        ]
    },
    {
        "title": "A Semidefinite Relaxation for Air Traffic Flow Scheduling",
        "authors": [
            "Alexandre d'Aspremont",
            "Laurent El Ghaoui"
        ],
        "summary": "We first formulate the problem of optimally scheduling air traffic low with sector capacity constraints as a mixed integer linear program. We then use semidefinite relaxation techniques to form a convex relaxation of that problem. Finally, we present a randomization algorithm to further improve the quality of the solution. Because of the specific structure of the air traffic flow problem, the relaxation has a single semidefinite constraint of size dn where d is the maximum delay and n the number of flights.",
        "published": "2006-09-26T15:34:40Z",
        "link": "http://arxiv.org/abs/cs/0609145v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Towards a Bayesian framework for option pricing",
        "authors": [
            "Henryk Gzyl",
            "Enrique ter Horst",
            "Samuel Malone"
        ],
        "summary": "In this paper, we describe a general method for constructing the posterior distribution of an option price. Our framework takes as inputs the prior distributions of the parameters of the stochastic process followed by the underlying, as well as the likelihood function implied by the observed price history for the underlying. Our work extends that of Karolyi (1993) and Darsinos and Satchell (2001), but with the crucial difference that the likelihood function we use for inference is that which is directly implied by the underlying, rather than imposed in an ad hoc manner via the introduction of a function representing \"measurement error.\" As such, an important problem still relevant for our method is that of model risk, and we address this issue by describing how to perform a Bayesian averaging of parameter inferences based on the different models considered using our framework.",
        "published": "2006-10-10T19:37:37Z",
        "link": "http://arxiv.org/abs/cs/0610053v1",
        "categories": [
            "cs.CE",
            "q-fin.PR"
        ]
    },
    {
        "title": "Doppler Spectrum Estimation by Ramanujan Fourier Transforms",
        "authors": [
            "Mohand Lagha",
            "Messaoud Bensebti"
        ],
        "summary": "The Doppler spectrum estimation of a weather radar signal in a classic way can be made by two methods, temporal one based in the autocorrelation of the successful signals, whereas the other one uses the estimation of the power spectral density PSD by using Fourier transforms. We introduces a new tool of signal processing based on Ramanujan sums cq(n), adapted to the analysis of arithmetical sequences with several resonances p/q. These sums are almost periodic according to time n of resonances and aperiodic according to the order q of resonances. New results will be supplied by the use of Ramanujan Fourier Transform (RFT) for the estimation of the Doppler spectrum for the weather radar signal.",
        "published": "2006-10-18T14:43:45Z",
        "link": "http://arxiv.org/abs/cs/0610108v2",
        "categories": [
            "cs.NA",
            "cs.CE"
        ]
    },
    {
        "title": "Classdesc and Graphcode: support for scientific programming in C++",
        "authors": [
            "Russell K. Standish",
            "Duraid Madina"
        ],
        "summary": "Object-oriented programming languages such as Java and Objective C have become popular for implementing agent-based and other object-based simulations since objects in those languages can {\\em reflect} (i.e. make runtime queries of an object's structure). This allows, for example, a fairly trivial {\\em serialisation} routine (conversion of an object into a binary representation that can be stored or passed over a network) to be written. However C++ does not offer this ability, as type information is thrown away at compile time. Yet C++ is often a preferred development environment, whether for performance reasons or for its expressive features such as operator overloading.   In scientific coding, changes to a model's codes takes place constantly, as the model is refined, and different phenomena are studied. Yet traditionally, facilities such as checkpointing, routines for initialising model parameters and analysis of model output depend on the underlying model remaining static, otherwise each time a model is modified, a whole slew of supporting routines needs to be changed to reflect the new data structures. Reflection offers the advantage of the simulation framework adapting to the underlying model without programmer intervention, reducing the effort of modifying the model.   In this paper, we present the {\\em Classdesc} system which brings many of the benefits of object reflection to C++, {\\em ClassdescMP} which dramatically simplifies coding of MPI based parallel programs and {\\em   Graphcode} a general purpose data parallel programming environment.",
        "published": "2006-10-20T05:11:21Z",
        "link": "http://arxiv.org/abs/cs/0610120v2",
        "categories": [
            "cs.MS",
            "cs.CE",
            "cs.DC",
            "D.1.3; D.1.5; D.2.12; D.3.4"
        ]
    },
    {
        "title": "Protection of the information in a complex CAD system of renovation of   industrial firms",
        "authors": [
            "Vladimir V. Migunov",
            "Rustem R. Kafiyatullov"
        ],
        "summary": "The threats to security of the information originating owing to involuntary operations of the users of a CAD, and methods of its protection implemented in a complex CAD system of renovation of firms are considered: rollback, autosave, automatic backup copying and electronic subscript. The specificity of a complex CAD is reflected in necessity of rollback and autosave both of the draw and the parametric representations of its parts, which are the information models of the problem-oriented extensions of the CAD",
        "published": "2006-11-10T07:43:33Z",
        "link": "http://arxiv.org/abs/cs/0611044v1",
        "categories": [
            "cs.CE",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The evolution of the parametric models of drawings (modules) in the   enterprises reconstruction CAD system",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "Progressing methods of drawings creating automation is discussed on the basis of so-called modules containing parametric representation of a part of the drawing and the geometrical elements. The stages of evolution of modular technology of automation of engineering are describing alternatives of applying of moduluss for simple association of elements of the drawing without parametric representation with an opportunity of its commenting, for graphic symbols creating in the schemas of automation and drawings of pipelines, for storage of the specific properties of elements, for development of the specialized parts of the project: the axonometric schemas, profiles of outboard pipe networks etc.",
        "published": "2006-11-10T07:56:19Z",
        "link": "http://arxiv.org/abs/cs/0611045v1",
        "categories": [
            "cs.CE",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "On numerical stability of recursive present value computation method",
        "authors": [
            "Argyn Kuketayev"
        ],
        "summary": "We analyze numerical stability of a recursive computation scheme of present value (PV) amd show that the absolute error increases exponentially for positive discount rates. We show that reversing the direction of calculations in the recurrence equation yields a robust PV computation routine.",
        "published": "2006-11-13T20:37:42Z",
        "link": "http://arxiv.org/abs/cs/0611049v2",
        "categories": [
            "cs.CE",
            "cs.NA",
            "G.1.0; J.1"
        ]
    },
    {
        "title": "Multivariate Integral Perturbation Techniques - I (Theory)",
        "authors": [
            "Jan W. Dash"
        ],
        "summary": "We present a quasi-analytic perturbation expansion for multivariate N-dimensional Gaussian integrals. The perturbation expansion is an infinite series of lower-dimensional integrals (one-dimensional in the simplest approximation). This perturbative idea can also be applied to multivariate Student-t integrals. We evaluate the perturbation expansion explicitly through 2nd order, and discuss the convergence, including enhancement using Pade approximants. Brief comments on potential applications in finance are given, including options, models for credit risk and derivatives, and correlation sensitivities.",
        "published": "2006-11-14T16:58:58Z",
        "link": "http://arxiv.org/abs/cs/0611061v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "B.2.4; G.1.4; G.3; J.1; J.2; J.4"
        ]
    },
    {
        "title": "A Multi-server Scheduling Framework for Resource Allocation in Wireless   Multi-carrier Networks",
        "authors": [
            "Ying Jun Zhang"
        ],
        "summary": "Multiuser resource allocation has recently been recognized as an effective methodology for enhancing the power and spectrum efficiency in OFDM (orthogonal frequency division multiplexing) systems. It is, however, not directly applicable to current packet-switched networks, because (i) most existing packet-scheduling schemes are based on a single-server model and do not serve multiple users at the same time; and (ii) the conventional separate design of MAC (medium access control) packet scheduling and PHY (physical) resource allocation yields inefficient resource utilization. In this paper, we propose a cross-layer resource allocation algorithm based on a novel multi-server scheduling framework to achieve overall high system power efficiency in packet-switched OFDM networks. Our contribution is four fold: (i) we propose and analyze a MPGPS (multi-server packetized general processor sharing) service discipline that serves multiple users at the same time and facilitates multiuser resource allocation; (ii) we present a MPGPS-based joint MAC-PHY resource allocation scheme that incorporates packet scheduling, subcarrier allocation, and power allocation in an integrated framework; (iii) by investigating the fundamental tradeoff between multiuser-diversity and queueing performance, we present an A-MPGPS (adaptive MPGPS) service discipline that strikes balance between power efficiency and queueing performance; and (iv) we extend MPGPS to an O-MPGPS (opportunistic MPGPS) service discipline to further enhance the resource utilization efficiency.",
        "published": "2006-11-16T12:33:51Z",
        "link": "http://arxiv.org/abs/cs/0611080v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "cs.NI",
            "cs.PF"
        ]
    },
    {
        "title": "Environment of development of the programs of parametric creating of the   drawings in CAD-system of renovation of the enterprises",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The main ideas, data structures, structure and realization of operations with them in environment of development of the programs of parametric creating of the drawings are considered for the needs of the automated design engineering system of renovation of the enterprises. The example of such program and example of application of this environment for creating the drawing of the base for equipment in CAD-system TechnoCAD GlassX are presented",
        "published": "2006-11-17T08:27:45Z",
        "link": "http://arxiv.org/abs/cs/0611083v1",
        "categories": [
            "cs.CE",
            "I.2.1; J.6"
        ]
    },
    {
        "title": "Coupling Methodology within the Software Platform Alliances",
        "authors": [
            "Philippe Montarnal",
            "Alain Dimier",
            "Estelle Deville",
            "Erwan Adam",
            "Jérôme Gaombalet",
            "Alain Bengaouer",
            "Laurent Loth",
            "Clément Chavant"
        ],
        "summary": "CEA, ANDRA and EDF are jointly developing the software platform ALLIANCES which aim is to produce a tool for the simulation of nuclear waste storage and disposal repository. This type of simulations deals with highly coupled thermo-hydro-mechanical and chemical (T-H-M-C) processes. A key objective of Alliances is to give the capability for coupling algorithms development between existing codes. The aim of this paper is to present coupling methodology use in the context of this software platform.",
        "published": "2006-11-24T16:43:06Z",
        "link": "http://arxiv.org/abs/cs/0611127v1",
        "categories": [
            "cs.MS",
            "cs.CE"
        ]
    },
    {
        "title": "The specifications making in complex CAD-system of renovation of the   enterprises on the basis of modules in the drawing and electronic catalogues",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The experience of automation of the specifications making of the projects of renovation of the industrial enterprises is described, being based on the special modules in the drawing containing the visible image and additional parameters, and electronic catalogues",
        "published": "2006-11-27T04:31:09Z",
        "link": "http://arxiv.org/abs/cs/0611132v1",
        "categories": [
            "cs.CE",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The modelling of the automation schemes of technological processes in   CAD-system of renovation of the enterprises",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "According to the requirements of the Russian standards, the automation schemes are necessary practically in each project of renovation of industrial buildings and facilities, in which any technological processes are realized. The model representations of the automation schemes in CAD-system TechnoCAD GlassX are described. The models follow a principle \"to exclude a repeated input operations\"",
        "published": "2006-11-27T04:39:09Z",
        "link": "http://arxiv.org/abs/cs/0611133v1",
        "categories": [
            "cs.CE",
            "I.2.1; J.6"
        ]
    },
    {
        "title": "Budget Optimization in Search-Based Advertising Auctions",
        "authors": [
            "Jon Feldman",
            "S. Muthukrishnan",
            "Martin Pal",
            "Cliff Stein"
        ],
        "summary": "Internet search companies sell advertisement slots based on users' search queries via an auction. While there has been a lot of attention on the auction process and its game-theoretic aspects, our focus is on the advertisers. In particular, the advertisers have to solve a complex optimization problem of how to place bids on the keywords of their interest so that they can maximize their return (the number of user clicks on their ads) for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywords works well. More precisely, this strategy gets at least 1-1/e fraction of the maximum clicks possible. Such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem.",
        "published": "2006-12-08T17:33:54Z",
        "link": "http://arxiv.org/abs/cs/0612052v1",
        "categories": [
            "cs.DS",
            "cs.CE",
            "cs.GT"
        ]
    },
    {
        "title": "Statistical mechanics of neocortical interactions: Portfolio of   Physiological Indicators",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "There are several kinds of non-invasive imaging methods that are used to collect data from the brain, e.g., EEG, MEG, PET, SPECT, fMRI, etc. It is difficult to get resolution of information processing using any one of these methods. Approaches to integrate data sources may help to get better resolution of data and better correlations to behavioral phenomena ranging from attention to diagnoses of disease. The approach taken here is to use algorithms developed for the author's Trading in Risk Dimensions (TRD) code using modern methods of copula portfolio risk management, with joint probability distributions derived from the author's model of statistical mechanics of neocortical interactions (SMNI). The author's Adaptive Simulated Annealing (ASA) code is for optimizations of training sets, as well as for importance-sampling. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes.",
        "published": "2006-12-18T21:02:03Z",
        "link": "http://arxiv.org/abs/cs/0612087v1",
        "categories": [
            "cs.CE",
            "cs.IT",
            "cs.NE",
            "math.IT",
            "q-bio.QM"
        ]
    },
    {
        "title": "The virtual reality framework for engineering objects",
        "authors": [
            "Petr R. Ivankov",
            "Nikolay P. Ivankov"
        ],
        "summary": "A framework for virtual reality of engineering objects has been developed. This framework may simulate different equipment related to virtual reality. Framework supports 6D dynamics, ordinary differential equations, finite formulas, vector and matrix operations. The framework also supports embedding of external software.",
        "published": "2006-12-22T19:19:41Z",
        "link": "http://arxiv.org/abs/cs/0612126v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "J.9"
        ]
    },
    {
        "title": "A library of Taylor models for PVS automatic proof checker",
        "authors": [
            "Francisco Cháves",
            "Marc Daumas"
        ],
        "summary": "We present in this paper a library to compute with Taylor models, a technique extending interval arithmetic to reduce decorrelation and to solve differential equations. Numerical software usually produces only numerical results. Our library can be used to produce both results and proofs. As seen during the development of Fermat's last theorem reported by Aczel 1996, providing a proof is not sufficient. Our library provides a proof that has been thoroughly scrutinized by a trustworthy and tireless assistant. PVS is an automatic proof assistant that has been fairly developed and used and that has no internal connection with interval arithmetic or Taylor models. We built our library so that PVS validates each result as it is produced. As producing and validating a proof, is and will certainly remain a bigger task than just producing a numerical result our library will never be a replacement to imperative implementations of Taylor models such as Cosy Infinity. Our library should mainly be used to validate small to medium size results that are involved in safety or life critical applications.",
        "published": "2006-02-03T18:29:52Z",
        "link": "http://arxiv.org/abs/cs/0602005v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "A Basic Introduction on Math-Link in Mathematica",
        "authors": [
            "Santanu K. Maiti"
        ],
        "summary": "Starting from the basic ideas of mathematica, we give a detailed description about the way of linking of external programs with mathematica through proper mathlink commands. This article may be quite helpful for the beginners to start with and write programs in mathematica.   In the first part, we illustrate how to use a mathemtica notebook and write a complete program in the notebook. Following with this, we also mention elaborately about the utility of the local and global variables those are very essential for writing a program in mathematica. All the commands needed for doing different mathematical operations can be found with some proper examples in the mathematica book written by Stephen Wolfram \\cite{wolfram}.   In the rest of this article, we concentrate our study on the most significant issue which is the process of linking of {\\em external programs} with mathematica, so-called the mathlink operation. By using proper mathlink commands one can run very tedious jobs efficiently and the operations become extremely fast.",
        "published": "2006-03-01T14:29:19Z",
        "link": "http://arxiv.org/abs/cs/0603005v4",
        "categories": [
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "BioSig - An application of Octave",
        "authors": [
            "Alois Schlögl"
        ],
        "summary": "BioSig is an open source software library for biomedical signal processing. Most users in the field are using Matlab; however, significant effort was undertaken to provide compatibility to Octave, too. This effort has been widely successful, only some non-critical components relying on a graphical user interface are missing. Now, installing BioSig on Octave is as easy as on Matlab. Moreover, a benchmark test based on BioSig has been developed and the benchmark results of several platforms are presented.",
        "published": "2006-03-01T15:45:57Z",
        "link": "http://arxiv.org/abs/cs/0603001v1",
        "categories": [
            "cs.MS",
            "J.2; J.3; J.4"
        ]
    },
    {
        "title": "Evaluation of interval extension of the power function by graph   decomposition",
        "authors": [
            "Evgueni Petrov"
        ],
        "summary": "The subject of our talk is the correct evaluation of interval extension of the function specified by the expression x^y without any constraints on the values of x and y. The core of our approach is a decomposition of the graph of x^y into a small number of parts which can be transformed into subsets of the graph of x^y for non-negative bases x. Because of this fact, evaluation of interval extension of x^y, without any constraints on x and y, is not much harder than evaluation of interval extension of x^y for non-negative bases x.",
        "published": "2006-03-13T12:05:08Z",
        "link": "http://arxiv.org/abs/cs/0603052v2",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Lanczos $τ$-method optimal algorithm in APS for approximating the   mathematical functions",
        "authors": [
            "P. N. Denisenko"
        ],
        "summary": "A new procedure is constructed by means of APS in APLAN language. The procedure solves the initial-value problem for linear differential equations of order $k$ with polynomial coefficients and regular singularity in the initialization point in the interval $[a, b]$ and computes the algebraic polynomial $y_n$ of given order $n$. A new algorithm of Lanczos $\\tau$-method is built for this procedure, the solution existence $y_n$ of the initial-value problem proved on this algorithm and also is proved the optimality by precision of order $k$ derivative of the initial-value problem solution.",
        "published": "2006-03-26T12:30:11Z",
        "link": "http://arxiv.org/abs/math/0603606v1",
        "categories": [
            "math.NA",
            "cs.MS",
            "math.CA"
        ]
    },
    {
        "title": "Sparse Matrix Implementation in Octave",
        "authors": [
            "David Bateman",
            "Andy Adler"
        ],
        "summary": "There are many classes of mathematical problems which give rise to matrices, where a large number of the elements are zero. In this case it makes sense to have a special matrix type to handle this class of problems where only the non-zero elements of the matrix are stored. Not only does this reduce the amount of memory to store the matrix, but it also means that operations on this type of matrix can take advantage of the a-priori knowledge of the positions of the non-zero elements to accelerate their calculations. A matrix type that stores only the non-zero elements is generally called sparse.   Until recently Octave has lacked a full implementation of sparse matrices. This article address the implementation of sparse matrices within Octave, including their storage, creation, fundamental algorithms used, their implementations and the basic operations and functions implemented for sparse matrices. Mathematical issues such as the return types of sparse operations, matrix fill-in and reordering for sparse matrix factorization is discussed in the context of a real example.   Benchmarking of Octave's implementation of sparse operations compared to their equivalent in Matlab are given and their implications discussed. Results are presented for multiplication and linear algebra operations for various matrix orders and densities. Furthermore, the use of Octave's sparse matrix implementation is demonstrated using a real example of a finite element model (FEM) problem. Finally, the method of using sparse matrices with Octave's oct-files is discussed. The means of creating, using and returning sparse matrices within oct-files is discussed as well as the differences between Octave's Sparse and Array classes.",
        "published": "2006-04-03T12:56:32Z",
        "link": "http://arxiv.org/abs/cs/0604006v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "UniCalc.LIN: a linear constraint solver for the UniCalc system",
        "authors": [
            "E. Petrov",
            "Yu. Kostov",
            "E. Botoeva"
        ],
        "summary": "In this short paper we present a linear constraint solver for the UniCalc system, an environment for reliable solution of mathematical modeling problems.",
        "published": "2006-04-10T06:30:02Z",
        "link": "http://arxiv.org/abs/cs/0604038v1",
        "categories": [
            "cs.MS",
            "cs.AI"
        ]
    },
    {
        "title": "A Fixed-Point Type for Octave",
        "authors": [
            "David Bateman",
            "Laurent Mazet",
            "Veronique Buzenac-Settineri",
            "Markus Muck"
        ],
        "summary": "This paper announces the availability of a fixed point toolbox for the Matlab compatible software package Octave. This toolbox is released under the GNU Public License, and can be used to model the losses in algorithms implemented in hardware. Furthermore, this paper presents as an example of the use of this toolbox, the effects of a fixed point implementation on the precision of an OFDM modulator.",
        "published": "2006-04-10T08:22:01Z",
        "link": "http://arxiv.org/abs/cs/0604039v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Univariate polynomial real root isolation: Continued Fractions revisited",
        "authors": [
            "Elias P. Tsigaridas",
            "Ioannis Z. Emiris"
        ],
        "summary": "We present algorithmic, complexity and implementation results concerning real root isolation of integer univariate polynomials using the continued fraction expansion of real algebraic numbers. One motivation is to explain the method's good performance in practice. We improve the previously known bound by a factor of $d \\tau$, where $d$ is the polynomial degree and $\\tau$ bounds the coefficient bitsize, thus matching the current record complexity for real root isolation by exact methods. Namely, the complexity bound is $\\sOB(d^4 \\tau^2)$ using the standard bound on the expected bitsize of the integers in the continued fraction expansion. We show how to compute the multiplicities within the same complexity and extend the algorithm to non square-free polynomials. Finally, we present an efficient open-source \\texttt{C++} implementation in the algebraic library \\synaps, and illustrate its efficiency as compared to other available software. We use polynomials with coefficient bitsize up to 8000 and degree up to 1000.",
        "published": "2006-04-17T10:52:35Z",
        "link": "http://arxiv.org/abs/cs/0604066v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "cs.MS"
        ]
    },
    {
        "title": "How to Run Mathematica Batch-files in Background ?",
        "authors": [
            "Santanu K. Maiti"
        ],
        "summary": "Mathematica is a versatile equipment for doing numeric and symbolic computations and it has wide spread applications in all branches of science. Mathematica has a complete consistency to design it at every stage that gives it multilevel capability and helps advanced usage evolve naturally. Mathematica functions work for any precision of number and it can be easily computed with symbols, represented graphically to get the best answer. Mathematica is a robust software development that can be used in any popular operating systems and it can be communicated with external programs by using proper mathlink commands.   Sometimes it is quite desirable to run jobs in background of a computer which can take considerable amount of time to finish, and this allows us to do work on other tasks, while keeping the jobs running. Most of us are very familiar to run jobs in background for the programs written in the languages like C, C++, F77, F90, F95, etc. But the way of running jobs, written in a mathematica notebook, in background is quite different from the conventional method. In this article, we explore how to create a mathematica batch-file from a mathematica notebook and run it in background. Here we concentrate our study only for the Unix version, but one can run mathematica programs in background for the Windows version as well by using proper mathematica batch-file.",
        "published": "2006-04-23T09:39:02Z",
        "link": "http://arxiv.org/abs/cs/0604088v3",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "A Monadic, Functional Implementation of Real Numbers",
        "authors": [
            "Russell O'Connor"
        ],
        "summary": "Large scale real number computation is an essential ingredient in several modern mathematical proofs. Because such lengthy computations cannot be verified by hand, some mathematicians want to use software proof assistants to verify the correctness of these proofs. This paper develops a new implementation of the constructive real numbers and elementary functions for such proofs by using the monad properties of the completion operation on metric spaces. Bishop and Bridges's notion of regular sequences is generalized to, what I call, regular functions which form the completion of any metric space. Using the monad operations, continuous functions on length spaces (a common subclass of metric spaces) are created by lifting continuous functions on the original space. A prototype Haskell implementation has been created. I believe that this approach yields a real number library that is reasonably efficient for computation, and still simple enough to easily verify its correctness.",
        "published": "2006-05-14T17:05:14Z",
        "link": "http://arxiv.org/abs/cs/0605058v1",
        "categories": [
            "cs.NA",
            "cs.MS"
        ]
    },
    {
        "title": "Caractéristiques arithmétiques des processeurs graphiques",
        "authors": [
            "Marc Daumas",
            "Guillaume Da Graça",
            "David Defour"
        ],
        "summary": "Les unit\\'{e}s graphiques (Graphic Processing Units- GPU) sont d\\'{e}sormais des processeurs puissants et flexibles. Les derni\\`{e}res g\\'{e}n\\'{e}rations de GPU contiennent des unit\\'{e}s programmables de traitement des sommets (vertex shader) et des pixels (pixel shader) supportant des op\\'{e}rations en virgule flottante sur 8, 16 ou 32 bits. La repr\\'{e}sentation flottante sur 32 bits correspond \\`{a} la simple pr\\'{e}cision de la norme IEEE sur l'arithm\\'{e}tique en virgule flottante (IEEE-754). Les GPU sont bien adapt\\'{e}s aux applications avec un fort parall\\'{e}lisme de donn\\'{e}es. Cependant ils ne sont que peu utilis\\'{e}s en dehors des calculs graphiques (General Purpose computation on GPU -- GPGPU). Une des raisons de cet \\'{e}tat de faits est la pauvret\\'{e} des documentations techniques fournies par les fabricants (ATI et Nvidia), particuli\\`{e}rement en ce qui concerne l'implantation des diff\\'{e}rents op\\'{e}rateurs arithm\\'{e}tiques embarqu\\'{e}s dans les diff\\'{e}rentes unit\\'{e}s de traitement. Or ces informations sont essentielles pour estimer et contr\\^{o}ler les erreurs d'arrondi ou pour mettre en oeuvre des techniques de r\\'{e}duction ou de compensation afin de travailler en pr\\'{e}cision double, quadruple ou arbitrairement \\'{e}tendue. Nous proposons dans cet article un ensemble de programmes qui permettent de d\\'{e}couvrir les caract\\'{e}ristiques principales des GPU en ce qui concerne l'arithm\\'{e}tique \\`{a} virgule flottante. Nous donnons les r\\'{e}sultats obtenus sur deux cartes graphiques r\\'{e}centes: la Nvidia 7800GTX et l'ATI RX1800XL.",
        "published": "2006-05-18T15:49:04Z",
        "link": "http://arxiv.org/abs/cs/0605081v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Mathematica: A System of Computer Programs",
        "authors": [
            "Santanu K. Maiti"
        ],
        "summary": "Starting from the basic level of mathematica here we illustrate how to use a mathematica notebook and write a program in the notebook. Next, we investigate elaborately the way of linking of external programs with mathematica, so-called the mathlink operation. Using this technique we can run very tedious jobs quite efficiently, and the operations become extremely fast. Sometimes it is quite desirable to run jobs in background of a computer which can take considerable amount of time to finish, and this allows us to do work on other tasks, while keeping the jobs running. The way of running jobs, written in a mathematica notebook, in background is quite different from the conventional methods i.e., the techniques for the programs written in other languages like C, C++, F77, F90, F95, etc. To illustrate it, in the present article we study how to create a mathematica batch-file from a mathematica notebook and run it in the background. Finally, we explore the most significant issue of this article. Here we describe the basic ideas for parallelizing a mathematica program by sharing its independent parts into all other remote computers available in the network. Doing the parallelization, we can perform large computational operations within a very short period of time, and therefore, the efficiency of the numerical works can be achieved. Parallel computation supports any version of mathematica and it also works significantly well even if different versions of mathematica are installed in different computers. All the operations studied in this article run under any supported operating system like Unix, Windows, Macintosh, etc. For the sake of our illustrations, here we concentrate all the discussions only for the Unix based operating system.",
        "published": "2006-05-20T05:43:55Z",
        "link": "http://arxiv.org/abs/cs/0605090v4",
        "categories": [
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Generalized Box-Muller method for generating q-Gaussian random deviates",
        "authors": [
            "William Thistleton",
            "Kenric Nelson",
            "John A. Marsh",
            "Constantino Tsallis"
        ],
        "summary": "Addendum: The generalized Box-M\\\"uller algorithm provides a methodology for generating q-Gaussian random variates. The parameter $-\\infty<q\\leq3$ is related to the shape of the tail decay; $q<1$ for compact-support including parabola $(q=0)$; $1<q\\leq3$ for heavy-tail including Cauchy $(q=2)$. This addendum clarifies the transformation $q'=((3q-1)/(q+1))$ within the algorithm is due to a difference in the dimensions d of the generalized logarithm and the generalized distribution. The transformation is clarified by the decomposition of $q=1+2\\kappa/(1+d\\kappa)$, where the shape parameter $-1<\\kappa\\leq\\infty$ quantifies the magnitude of the deformation from exponential. A simpler specification for the generalized Box- M\\\"uller algorithm is provided using the shape of the tail decay.   Original: The q-Gaussian distribution is known to be an attractor of certain correlated systems, and is the distribution which, under appropriate constraints, maximizes the entropy Sq, basis of nonextensive statistical mechanics. This theory is postulated as a natural extension of the standard (Boltzmann-Gibbs) statistical mechanics, and may explain the ubiquitous appearance of heavy-tailed distributions in both natural and man-made systems. The q-Gaussian distribution is also used as a numerical tool, for example as a visiting distribution in Generalized Simulated Annealing. We develop and present a simple, easy to implement numerical method for generating random deviates from a q-Gaussian distribution based upon a generalization of the well known Box-Muller method. Our method is suitable for a larger range of q values, q<3, than has previously appeared in the literature, and can generate deviates from q-Gaussian distributions of arbitrary width and center. MATLAB code showing a straightforward implementation is also included.",
        "published": "2006-05-23T18:04:56Z",
        "link": "http://arxiv.org/abs/cond-mat/0605570v3",
        "categories": [
            "cond-mat.stat-mech",
            "cs.MS"
        ]
    },
    {
        "title": "Parallel Evaluation of Mathematica Programs in Remote Computers   Available in Network",
        "authors": [
            "Santanu K. Maiti"
        ],
        "summary": "Mathematica is a powerful application package for doing mathematics and is used almost in all branches of science. It has widespread applications ranging from quantum computation, statistical analysis, number theory, zoology, astronomy, and many more. Mathematica gives a rich set of programming extensions to its end-user language, and it permits us to write programs in procedural, functional, or logic (rule-based) style, or a mixture of all three. For tasks requiring interfaces to the external environment, mathematica provides mathlink, which allows us to communicate mathematica programs with external programs written in C, C++, F77, F90, F95, Java, or other languages. It has also extensive capabilities for editing graphics, equations, text, etc.   In this article, we explore the basic mechanisms of parallelization of a mathematica program by sharing different parts of the program into all other computers available in the network. Doing the parallelization, we can perform large computational operations within a very short period of time, and therefore, the efficiency of the numerical works can be achieved. Parallel computation supports any version of mathematica and it also works as well even if different versions of mathematica are installed in different computers. The whole operation can run under any supported operating system like Unix, Windows, Macintosh, etc. Here we focus our study only for the Unix based operating system, but this method works as well for all other cases.",
        "published": "2006-06-06T12:11:23Z",
        "link": "http://arxiv.org/abs/cs/0606023v3",
        "categories": [
            "cs.MS",
            "cs.PL"
        ]
    },
    {
        "title": "Stochastic Formal Methods: An application to accuracy of numeric   software",
        "authors": [
            "Marc Daumas",
            "David Lester"
        ],
        "summary": "This paper provides a bound on the number of numeric operations (fixed or floating point) that can safely be performed before accuracy is lost. This work has important implications for control systems with safety-critical software, as these systems are now running fast enough and long enough for their errors to impact on their functionality. Furthermore, worst-case analysis would blindly advise the replacement of existing systems that have been successfully running for years. We present here a set of formal theorems validated by the PVS proof assistant. These theorems will allow code analyzing tools to produce formal certificates of accurate behavior. For example, FAA regulations for aircraft require that the probability of an error be below $10^{-9}$ for a 10 hour flight.",
        "published": "2006-06-23T08:26:56Z",
        "link": "http://arxiv.org/abs/cs/0606101v4",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Ideas by Statistical Mechanics (ISM)",
        "authors": [
            "Lester Ingber"
        ],
        "summary": "Ideas by Statistical Mechanics (ISM) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions. The program is based on the author's work in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the author's Adaptive Simulated Annealing (ASA) code for optimizations of training sets, as well as for importance-sampling to apply the author's copula financial risk-management codes, Trading in Risk Dimensions (TRD), for assessments of risk and uncertainty. This product can be used for decision support for projects ranging from diplomatic, information, military, and economic (DIME) factors of propagation/evolution of ideas, to commercial sales, trading indicators across sectors of financial markets, advertising and political campaigns, etc. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, is the proposed model. Parameters with a given subset of macrocolumns will be fit using ASA to patterns representing ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Tools of financial risk management, developed by the author to process correlated multivariate systems with differing non-Gaussian distributions using modern copula analysis, importance-sampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes.",
        "published": "2006-07-23T16:12:47Z",
        "link": "http://arxiv.org/abs/cs/0607103v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "cs.NE"
        ]
    },
    {
        "title": "On a solution to display non-filled-in quaternionic Julia sets",
        "authors": [
            "Alessandro Rosa"
        ],
        "summary": "During early 1980s, the so-called `escape time' method, developed to display the Julia sets for complex dynamical systems, was exported to quaternions in order to draw analogous pictures in this wider numerical field. Despite of the fine results in the complex plane, where all topological configurations of Julia sets have been successfully displayed, the `escape time' method fails to render properly the non-filled-in variety of quaternionic Julia sets. So their digital visualisation remained an open problem for several years. Both the solution for extending this old method to non-filled-in quaternionic Julia sets and its implementation into a program are explained here.",
        "published": "2006-08-01T19:25:17Z",
        "link": "http://arxiv.org/abs/cs/0608003v2",
        "categories": [
            "cs.GR",
            "cs.MS",
            "math.DS"
        ]
    },
    {
        "title": "One method for proving inequalities by computer",
        "authors": [
            "Branko J. Malesevic"
        ],
        "summary": "In this article we consider a method for proving a class of analytical inequalities via minimax rational approximations. All numerical calculations in this paper are given by Maple computer program.",
        "published": "2006-08-31T14:59:20Z",
        "link": "http://arxiv.org/abs/math/0608789v7",
        "categories": [
            "math.CA",
            "cs.GR",
            "cs.MS",
            "cs.NA",
            "math.GM",
            "math.NA",
            "26Dxx, 33F05, 41A20"
        ]
    },
    {
        "title": "Classifying extrema using intervals",
        "authors": [
            "Marek W. Gutowski"
        ],
        "summary": "We present a straightforward and verified method of deciding whether the n-dimensional point x (n>=1), such that \\nabla f(x)=0, is the local minimizer, maximizer or just a saddle point of a real-valued function f.   The method scales linearly with dimensionality of the problem and never produces false results.",
        "published": "2006-09-14T18:32:46Z",
        "link": "http://arxiv.org/abs/cs/0609082v1",
        "categories": [
            "cs.MS",
            "cs.CC",
            "cs.NA",
            "F.2.2; G.1.0; G.1.2; J.2"
        ]
    },
    {
        "title": "One approach to the digital visualization of hedgehogs in holomorphic   dynamics",
        "authors": [
            "Alessandro Rosa"
        ],
        "summary": "In the field of holomorphic dynamics in one complex variable, hedgehog is the local invariant set arising about a Cremer point and endowed with a very complicate shape as well as relating to very weak numerical conditions. We give a solution to the open problem of its digital visualization, featuring either a time saving approach and a far-reaching insight.",
        "published": "2006-09-22T18:53:17Z",
        "link": "http://arxiv.org/abs/cs/0609129v2",
        "categories": [
            "cs.MS",
            "math.DS"
        ]
    },
    {
        "title": "Stochastic Formal Methods for Hybrid Systems",
        "authors": [
            "Marc Daumas",
            "David Lester",
            "Erik Martin-Dorel",
            "Annick Truffert"
        ],
        "summary": "We provide a framework to bound the probability that accumulated errors were never above a given threshold on hybrid systems. Such systems are used for example to model an aircraft or a nuclear power plant on one side and its software on the other side. This report contains simple formulas based on L\\'evy's and Markov's inequalities and it presents a formal theory of random variables with a special focus on producing concrete results. We selected four very common applications that fit in our framework and cover the common practices of hybrid systems that evolve for a long time. We compute the number of bits that remain continuously significant in the first two applications with a probability of failure around one against a billion, where worst case analysis considers that no significant bit remains. We are using PVS as such formal tools force explicit statement of all hypotheses and prevent incorrect uses of theorems.",
        "published": "2006-10-18T19:57:11Z",
        "link": "http://arxiv.org/abs/cs/0610110v4",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "Classdesc and Graphcode: support for scientific programming in C++",
        "authors": [
            "Russell K. Standish",
            "Duraid Madina"
        ],
        "summary": "Object-oriented programming languages such as Java and Objective C have become popular for implementing agent-based and other object-based simulations since objects in those languages can {\\em reflect} (i.e. make runtime queries of an object's structure). This allows, for example, a fairly trivial {\\em serialisation} routine (conversion of an object into a binary representation that can be stored or passed over a network) to be written. However C++ does not offer this ability, as type information is thrown away at compile time. Yet C++ is often a preferred development environment, whether for performance reasons or for its expressive features such as operator overloading.   In scientific coding, changes to a model's codes takes place constantly, as the model is refined, and different phenomena are studied. Yet traditionally, facilities such as checkpointing, routines for initialising model parameters and analysis of model output depend on the underlying model remaining static, otherwise each time a model is modified, a whole slew of supporting routines needs to be changed to reflect the new data structures. Reflection offers the advantage of the simulation framework adapting to the underlying model without programmer intervention, reducing the effort of modifying the model.   In this paper, we present the {\\em Classdesc} system which brings many of the benefits of object reflection to C++, {\\em ClassdescMP} which dramatically simplifies coding of MPI based parallel programs and {\\em   Graphcode} a general purpose data parallel programming environment.",
        "published": "2006-10-20T05:11:21Z",
        "link": "http://arxiv.org/abs/cs/0610120v2",
        "categories": [
            "cs.MS",
            "cs.CE",
            "cs.DC",
            "D.1.3; D.1.5; D.2.12; D.3.4"
        ]
    },
    {
        "title": "Faithful Polynomial Evaluation with Compensated Horner Algorithm",
        "authors": [
            "Philippe Langlois",
            "Nicolas Louvet"
        ],
        "summary": "This paper presents two sufficient conditions to ensure a faithful evaluation of polynomial in IEEE-754 floating point arithmetic. Faithfulness means that the computed value is one of the two floating point neighbours of the exact result; it can be satisfied using a more accurate algorithm than the classic Horner scheme. One condition here provided is an apriori bound of the polynomial condition number derived from the error analysis of the compensated Horner algorithm. The second condition is both dynamic and validated to check at the running time the faithfulness of a given evaluation. Numerical experiments illustrate the behavior of these two conditions and that associated running time over-cost is really interesting.",
        "published": "2006-10-20T11:22:52Z",
        "link": "http://arxiv.org/abs/cs/0610122v1",
        "categories": [
            "cs.NA",
            "cs.MS",
            "G.4"
        ]
    },
    {
        "title": "Coupling Methodology within the Software Platform Alliances",
        "authors": [
            "Philippe Montarnal",
            "Alain Dimier",
            "Estelle Deville",
            "Erwan Adam",
            "Jérôme Gaombalet",
            "Alain Bengaouer",
            "Laurent Loth",
            "Clément Chavant"
        ],
        "summary": "CEA, ANDRA and EDF are jointly developing the software platform ALLIANCES which aim is to produce a tool for the simulation of nuclear waste storage and disposal repository. This type of simulations deals with highly coupled thermo-hydro-mechanical and chemical (T-H-M-C) processes. A key objective of Alliances is to give the capability for coupling algorithms development between existing codes. The aim of this paper is to present coupling methodology use in the context of this software platform.",
        "published": "2006-11-24T16:43:06Z",
        "link": "http://arxiv.org/abs/cs/0611127v1",
        "categories": [
            "cs.MS",
            "cs.CE"
        ]
    },
    {
        "title": "Revisiting Matrix Product on Master-Worker Platforms",
        "authors": [
            "Jack Dongarra",
            "Jean-Francois Pineau",
            "Yves Robert",
            "Zhiao Shi",
            "Frederic Vivien"
        ],
        "summary": "This paper is aimed at designing efficient parallel matrix-product algorithms for heterogeneous master-worker platforms. While matrix-product is well-understood for homogeneous 2D-arrays of processors (e.g., Cannon algorithm and ScaLAPACK outer product algorithm), there are three key hypotheses that render our work original and innovative:   - Centralized data. We assume that all matrix files originate from, and must be returned to, the master.   - Heterogeneous star-shaped platforms. We target fully heterogeneous platforms, where computational resources have different computing powers.   - Limited memory. Because we investigate the parallelization of large problems, we cannot assume that full matrix panels can be stored in the worker memories and re-used for subsequent updates (as in ScaLAPACK).   We have devised efficient algorithms for resource selection (deciding which workers to enroll) and communication ordering (both for input and result messages), and we report a set of numerical experiments on various platforms at Ecole Normale Superieure de Lyon and the University of Tennessee. However, we point out that in this first version of the report, experiments are limited to homogeneous platforms.",
        "published": "2006-12-06T14:34:29Z",
        "link": "http://arxiv.org/abs/cs/0612036v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "F.2.2"
        ]
    },
    {
        "title": "The Parma Polyhedra Library: Toward a Complete Set of Numerical   Abstractions for the Analysis and Verification of Hardware and Software   Systems",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Since its inception as a student project in 2001, initially just for the handling (as the name implies) of convex polyhedra, the Parma Polyhedra Library has been continuously improved and extended by joining scrupulous research on the theoretical foundations of (possibly non-convex) numerical abstractions to a total adherence to the best available practices in software development. Even though it is still not fully mature and functionally complete, the Parma Polyhedra Library already offers a combination of functionality, reliability, usability and performance that is not matched by similar, freely available libraries. In this paper, we present the main features of the current version of the library, emphasizing those that distinguish it from other similar libraries and those that are important for applications in the field of analysis and verification of hardware and software systems.",
        "published": "2006-12-18T10:15:38Z",
        "link": "http://arxiv.org/abs/cs/0612085v1",
        "categories": [
            "cs.MS",
            "cs.PL",
            "G.4; D.2.4"
        ]
    },
    {
        "title": "The virtual reality framework for engineering objects",
        "authors": [
            "Petr R. Ivankov",
            "Nikolay P. Ivankov"
        ],
        "summary": "A framework for virtual reality of engineering objects has been developed. This framework may simulate different equipment related to virtual reality. Framework supports 6D dynamics, ordinary differential equations, finite formulas, vector and matrix operations. The framework also supports embedding of external software.",
        "published": "2006-12-22T19:19:41Z",
        "link": "http://arxiv.org/abs/cs/0612126v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "J.9"
        ]
    },
    {
        "title": "Some notes on a method for proving inequalities by computer",
        "authors": [
            "Bojan D. Banjac",
            "Milica D. Makragic",
            "Branko J. Malesevic"
        ],
        "summary": "In this article we consider mathematical fundamentals of one method for proving inequalities by computer, based on the Remez algorithm. Using the well-known results of undecidability of the existence of zeros of real elementary functions, we demonstrate that the considered method generally in practice becomes one heuristic for the verification of inequalities. We give some improvements of the inequalities considered in the theorems for which the existing proofs have been based on the numerical verifications of Remez algorithm.",
        "published": "2006-12-31T07:16:29Z",
        "link": "http://arxiv.org/abs/math/0701020v12",
        "categories": [
            "math.CA",
            "cs.MS",
            "math.NA",
            "26D05, 26D15, 41A10"
        ]
    },
    {
        "title": "A Constructive Semantic Characterization of Aggregates in ASP",
        "authors": [
            "Tran Cao Son",
            "Enrico Pontelli"
        ],
        "summary": "This technical note describes a monotone and continuous fixpoint operator to compute the answer sets of programs with aggregates. The fixpoint operator relies on the notion of aggregate solution. Under certain conditions, this operator behaves identically to the three-valued immediate consequence operator $\\Phi^{aggr}_P$ for aggregate programs, independently proposed Pelov et al. This operator allows us to closely tie the computational complexity of the answer set checking and answer sets existence problems to the cost of checking a solution of the aggregates in the program. Finally, we relate the semantics described by the operator to other proposals for logic programming with aggregates.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2006-01-13T16:09:36Z",
        "link": "http://arxiv.org/abs/cs/0601051v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.1.6; D.3.1; D.3.2; D.3.3"
        ]
    },
    {
        "title": "The complexity of class polynomial computation via floating point   approximations",
        "authors": [
            "Andreas Enge"
        ],
        "summary": "We analyse the complexity of computing class polynomials, that are an important ingredient for CM constructions of elliptic curves, via complex floating point approximations of their roots. The heart of the algorithm is the evaluation of modular functions in several arguments. The fastest one of the presented approaches uses a technique devised by Dupont to evaluate modular functions by Newton iterations on an expression involving the arithmetic-geometric mean. It runs in time $O (|D| \\log^5 |D| \\log \\log |D|) = O (|D|^{1 + \\epsilon}) = O (h^{2 + \\epsilon})$ for any $\\epsilon > 0$, where $D$ is the CM discriminant and $h$ is the degree of the class polynomial. Another fast algorithm uses multipoint evaluation techniques known from symbolic computation; its asymptotic complexity is worse by a factor of $\\log |D|$. Up to logarithmic factors, this running time matches the size of the constructed polynomials. The estimate also relies on a new result concerning the complexity of enumerating the class group of an imaginary-quadratic order and on a rigorously proven upper bound for the height of class polynomials.",
        "published": "2006-01-24T11:01:46Z",
        "link": "http://arxiv.org/abs/cs/0601104v2",
        "categories": [
            "cs.NA",
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "Dense Linear Algebra over Finite Fields: the FFLAS and FFPACK packages",
        "authors": [
            "Jean-Guillaume Dumas",
            "Pascal Giorgi",
            "Clément Pernet"
        ],
        "summary": "In the past two decades, some major efforts have been made to reduce exact (e.g. integer, rational, polynomial) linear algebra problems to matrix multiplication in order to provide algorithms with optimal asymptotic complexity. To provide efficient implementations of such algorithms one need to be careful with the underlying arithmetic. It is well known that modular techniques such as the Chinese remainder algorithm or the p-adic lifting allow very good practical performance, especially when word size arithmetic are used. Therefore, finite field arithmetic becomes an important core for efficient exact linear algebra libraries. In this paper, we study high performance implementations of basic linear algebra routines over word size prime fields: specially the matrix multiplication; our goal being to provide an exact alternate to the numerical BLAS library. We show that this is made possible by a carefull combination of numerical computations and asymptotically faster algorithms. Our kernel has several symbolic linear algebra applications enabled by diverse matrix multiplication reductions: symbolic triangularization, system solving, determinant and matrix inverse implementations are thus studied.",
        "published": "2006-01-31T09:54:41Z",
        "link": "http://arxiv.org/abs/cs/0601133v3",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Demand Analysis with Partial Predicates",
        "authors": [
            "Julio Marino",
            "Angel Herranz",
            "Juan Jose Moreno-Navarro"
        ],
        "summary": "In order to alleviate the inefficiencies caused by the interaction of the logic and functional sides, integrated languages may take advantage of \\emph{demand} information -- i.e. knowing in advance which computations are needed and, to which extent, in a particular context. This work studies \\emph{demand analysis} -- which is closely related to \\emph{backwards strictness analysis} -- in a semantic framework of \\emph{partial predicates}, which in turn are constructive realizations of ideals in a domain. This will allow us to give a concise, unified presentation of demand analysis, to relate it to other analyses based on abstract interpretation or strictness logics, some hints for the implementation, and, more important, to prove the soundness of our analysis based on \\emph{demand equations}. There are also some innovative results. One of them is that a set constraint-based analysis has been derived in a stepwise manner using ideas taken from the area of program transformation. The other one is the possibility of using program transformation itself to perform the analysis, specially in those domains of properties where algorithms based on constraint solving are too weak.",
        "published": "2006-02-04T20:22:34Z",
        "link": "http://arxiv.org/abs/cs/0602008v1",
        "categories": [
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "Computing spectral sequences",
        "authors": [
            "A. Romero",
            "J. Rubio",
            "F. Sergeraert"
        ],
        "summary": "In this paper, a set of programs enhancing the Kenzo system is presented. Kenzo is a Common Lisp program designed for computing in Algebraic Topology, in particular it allows the user to calculate homology and homotopy groups of complicated spaces. The new programs presented here entirely compute Serre and Eilenberg-Moore spectral sequences, in particular the groups and differential maps for arbitrary r. They also determine when the spectral sequence has converged and describe the filtration of the target homology groups induced by the spectral sequence.",
        "published": "2006-02-17T12:29:57Z",
        "link": "http://arxiv.org/abs/cs/0602064v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Parallel Symbolic Computation of Curvature Invariants in General   Relativity",
        "authors": [
            "K. R. Koehler"
        ],
        "summary": "We present a practical application of parallel symbolic computation in General Relativity: the calculation of curvature invariants for large dimension. We discuss the structure of the calculations, an implementation of the technique and scaling of the computation with spacetime dimension for various invariants.",
        "published": "2006-02-19T19:28:44Z",
        "link": "http://arxiv.org/abs/cs/0602068v1",
        "categories": [
            "cs.DC",
            "cs.SC",
            "gr-qc"
        ]
    },
    {
        "title": "Computing the First Betti Numberand Describing the Connected Components   of Semi-algebraic Sets",
        "authors": [
            "Saugata Basu",
            "Richard Pollack",
            "Marie-Francoise Roy"
        ],
        "summary": "In this paper we describe a singly exponential algorithm for computing the first Betti number of a given semi-algebraic set. Singly exponential algorithms for computing the zero-th Betti number, and the Euler-Poincar\\'e characteristic, were known before. No singly exponential algorithm was known for computing any of the individual Betti numbers other than the zero-th one. We also give algorithms for obtaining semi-algebraic descriptions of the semi-algebraically connected components of any given real algebraic or semi-algebraic set in single-exponential time improving on previous results.",
        "published": "2006-03-10T20:00:21Z",
        "link": "http://arxiv.org/abs/math/0603248v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "math.AT",
            "14P10 ; 14P25"
        ]
    },
    {
        "title": "Computing the First Few Betti Numbers of Semi-algebraic Sets in Single   Exponential Time",
        "authors": [
            "Saugata Basu"
        ],
        "summary": "In this paper we describe an algorithm that takes as input a description of a semi-algebraic set $S \\subset \\R^k$, defined by a Boolean formula with atoms of the form $P > 0, P < 0, P=0$ for $P \\in {\\mathcal P} \\subset \\R[X_1,...,X_k],$ and outputs the first $\\ell+1$ Betti numbers of $S$, $b_0(S),...,b_\\ell(S).$ The complexity of the algorithm is $(sd)^{k^{O(\\ell)}},$ where where $s = #({\\mathcal P})$ and $d = \\max_{P\\in {\\mathcal P}}{\\rm deg}(P),$ which is singly exponential in $k$ for $\\ell$ any fixed constant. Previously, singly exponential time algorithms were known only for computing the Euler-Poincar\\'e characteristic, the zero-th and the first Betti numbers.",
        "published": "2006-03-10T21:32:18Z",
        "link": "http://arxiv.org/abs/math/0603263v1",
        "categories": [
            "math.AG",
            "cs.SC",
            "14P10 ; 14P25"
        ]
    },
    {
        "title": "Unary Primitive Recursive Functions",
        "authors": [
            "Daniel E. Severin"
        ],
        "summary": "In this article, we study some new characterizations of primitive recursive functions based on restricted forms of primitive recursion, improving the pioneering work of R. M. Robinson and M. D. Gladstone in this area. We reduce certain recursion schemes (mixed/pure iteration without parameters) and we characterize one-argument primitive recursive functions as the closure under substitution and iteration of certain optimal sets.",
        "published": "2006-03-16T17:20:25Z",
        "link": "http://arxiv.org/abs/cs/0603063v3",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "An Explicit Solution to Post's Problem over the Reals",
        "authors": [
            "Klaus Meer",
            "Martin Ziegler"
        ],
        "summary": "In the BCSS model of real number computations we prove a concrete and explicit semi-decidable language to be undecidable yet not reducible from (and thus strictly easier than) the real Halting Language. This solution to Post's Problem over the reals significantly differs from its classical, discrete variant where advanced diagonalization techniques are only known to yield the existence of such intermediate Turing degrees. Strengthening the above result, we construct (that is, obtain again explicitly) as well an uncountable number of incomparable semi-decidable Turing degrees below the real Halting problem in the BCSS model. Finally we show the same to hold for the linear BCSS model, that is over (R,+,-,<) rather than (R,+,-,*,/,<).",
        "published": "2006-03-17T16:12:23Z",
        "link": "http://arxiv.org/abs/cs/0603071v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Solving Sparse Integer Linear Systems",
        "authors": [
            "Wayne Eberly",
            "Mark Giesbrecht",
            "Pascal Giorgi",
            "Arne Storjohann",
            "Gilles Villard"
        ],
        "summary": "We propose a new algorithm to solve sparse linear systems of equations over the integers. This algorithm is based on a $p$-adic lifting technique combined with the use of block matrices with structured blocks. It achieves a sub-cubic complexity in terms of machine operations subject to a conjecture on the effectiveness of certain sparse projections. A LinBox-based implementation of this algorithm is demonstrated, and emphasizes the practical benefits of this new method over the previous state of the art.",
        "published": "2006-03-21T16:09:45Z",
        "link": "http://arxiv.org/abs/cs/0603082v1",
        "categories": [
            "cs.SC",
            "I.1.2"
        ]
    },
    {
        "title": "Benchmark Problems for Constraint Solving",
        "authors": [
            "Alin Suciu",
            "Rodica Potolea",
            "Tudor Muresan"
        ],
        "summary": "Constraint Programming is roughly a new software technology introduced by Jaffar and Lassez in 1987 for description and effective solving of large, particularly combinatorial, problems especially in areas of planning and scheduling. In the following we define three problems for constraint solving from the domain of electrical networks; based on them we define 43 related problems. For the defined set of problems we benchmarked five systems: ILOG OPL, AMPL, GAMS, Mathematica and UniCalc. As expected some of the systems performed very well for some problems while others performed very well on others.",
        "published": "2006-03-26T20:19:59Z",
        "link": "http://arxiv.org/abs/cs/0603099v1",
        "categories": [
            "cs.PF",
            "cs.SC"
        ]
    },
    {
        "title": "Real Computational Universality: The Word Problem for a class of groups   with infinite presentation",
        "authors": [
            "Martin Ziegler",
            "Klaus Meer"
        ],
        "summary": "The word problem for discrete groups is well-known to be undecidable by a Turing Machine; more precisely, it is reducible both to and from and thus equivalent to the discrete Halting Problem.   The present work introduces and studies a real extension of the word problem for a certain class of groups which are presented as quotient groups of a free group and a normal subgroup. Most important, the free group will be generated by an uncountable set of generators with index running over certain sets of real numbers. This allows to include many mathematically important groups which are not captured in the framework of the classical word problem.   Our contribution extends computational group theory from the discrete to the Blum-Shub-Smale (BSS) model of real number computation. We believe this to be an interesting step towards applying BSS theory, in addition to semi-algebraic geometry, also to further areas of mathematics.   The main result establishes the word problem for such groups to be not only semi-decidable (and thus reducible FROM) but also reducible TO the Halting Problem for such machines. It thus provides the first non-trivial example of a problem COMPLETE, that is, computationally universal for this model.",
        "published": "2006-04-07T23:57:46Z",
        "link": "http://arxiv.org/abs/cs/0604032v3",
        "categories": [
            "cs.LO",
            "cs.SC",
            "F.1.1; F.4.1; F.4.2"
        ]
    },
    {
        "title": "Extension of the functionality of the symbolic program FORM by external   software",
        "authors": [
            "M. Tentyukov",
            "J. A. M. Vermaseren"
        ],
        "summary": "We describe the implementation of facilities for the communication with external resources in the Symbolic Manipulation System FORM. This is done according to the POSIX standards defined for the UNIX operating system. We present a number of examples that illustrate the increased power due to these new capabilities.",
        "published": "2006-04-12T16:03:03Z",
        "link": "http://arxiv.org/abs/cs/0604052v2",
        "categories": [
            "cs.SC",
            "hep-ph",
            "I.1; I.1.3; I.1.4"
        ]
    },
    {
        "title": "Polynomial Time Nondimensionalisation of Ordinary Differential Equations   via their Lie Point Symmetries",
        "authors": [
            "Évelyne Hubert",
            "Alexandre Sedoglavic"
        ],
        "summary": "Lie group theory states that knowledge of a $m$-parameters solvable group of symmetries of a system of ordinary differential equations allows to reduce by $m$ the number of equation. We apply this principle by finding dilatations and translations that are Lie point symmetries of considered ordinary differential system. By rewriting original problem in an invariant coordinates set for these symmetries, one can reduce the involved number of parameters. This process is classically call nondimensionalisation in dimensional analysis. We present an algorithm based on this standpoint and show that its arithmetic complexity is polynomial in input's size.",
        "published": "2006-04-13T14:35:00Z",
        "link": "http://arxiv.org/abs/cs/0604060v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Univariate polynomial real root isolation: Continued Fractions revisited",
        "authors": [
            "Elias P. Tsigaridas",
            "Ioannis Z. Emiris"
        ],
        "summary": "We present algorithmic, complexity and implementation results concerning real root isolation of integer univariate polynomials using the continued fraction expansion of real algebraic numbers. One motivation is to explain the method's good performance in practice. We improve the previously known bound by a factor of $d \\tau$, where $d$ is the polynomial degree and $\\tau$ bounds the coefficient bitsize, thus matching the current record complexity for real root isolation by exact methods. Namely, the complexity bound is $\\sOB(d^4 \\tau^2)$ using the standard bound on the expected bitsize of the integers in the continued fraction expansion. We show how to compute the multiplicities within the same complexity and extend the algorithm to non square-free polynomials. Finally, we present an efficient open-source \\texttt{C++} implementation in the algebraic library \\synaps, and illustrate its efficiency as compared to other available software. We use polynomials with coefficient bitsize up to 8000 and degree up to 1000.",
        "published": "2006-04-17T10:52:35Z",
        "link": "http://arxiv.org/abs/cs/0604066v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "cs.MS"
        ]
    },
    {
        "title": "A Recursive Method for Determining the One-Dimensional Submodules of   Laurent-Ore Modules",
        "authors": [
            "Ziming Li",
            "Michael F. Singer",
            "Min Wu",
            "Dabin Zheng"
        ],
        "summary": "We present a method for determining the one-dimensional submodules of a Laurent-Ore module. The method is based on a correspondence between hyperexponential solutions of associated systems and one-dimensional submodules. The hyperexponential solutions are computed recursively by solving a sequence of first-order ordinary matrix equations. As the recursion proceeds, the matrix equations will have constant coefficients with respect to the operators that have been considered.",
        "published": "2006-04-21T16:04:40Z",
        "link": "http://arxiv.org/abs/cs/0604084v1",
        "categories": [
            "cs.SC",
            "math.CA"
        ]
    },
    {
        "title": "Fast computation of power series solutions of systems of differential   equations",
        "authors": [
            "Alin Bostan",
            "Frédéric Chyzak",
            "François Ollivier",
            "Bruno Salvy",
            "Éric Schost",
            "Alexandre Sedoglavic"
        ],
        "summary": "We propose new algorithms for the computation of the first N terms of a vector (resp. a basis) of power series solutions of a linear system of differential equations at an ordinary point, using a number of arithmetic operations which is quasi-linear with respect to N. Similar results are also given in the non-linear case. This extends previous results obtained by Brent and Kung for scalar differential equations of order one and two.",
        "published": "2006-04-25T15:25:35Z",
        "link": "http://arxiv.org/abs/cs/0604101v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Sufficient set of integrability conditions of an orthonomic system",
        "authors": [
            "M. Marvan"
        ],
        "summary": "Every orthonomic system of partial differential equations is known to possess a finite number of integrability conditions sufficient to ensure the validity of all. Herewith we offer an efficient algorithm to construct a sufficient set of integrability conditions free of redundancies.",
        "published": "2006-05-03T18:51:52Z",
        "link": "http://arxiv.org/abs/nlin/0605009v3",
        "categories": [
            "nlin.SI",
            "cs.SC"
        ]
    },
    {
        "title": "SAT Techniques for Lexicographic Path Orders",
        "authors": [
            "Harald Zankl"
        ],
        "summary": "This seminar report is concerned with expressing LPO-termination of term rewrite systems as a satisfiability problem in propositional logic. After relevant algorithms are explained, experimental results are reported.",
        "published": "2006-05-05T19:11:22Z",
        "link": "http://arxiv.org/abs/cs/0605021v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Gröbner Bases and Generation of Difference Schemes for Partial   Differential Equations",
        "authors": [
            "Vladimir P. Gerdt",
            "Yuri A. Blinkov",
            "Vladimir V. Mozzhilkin"
        ],
        "summary": "In this paper we present an algorithmic approach to the generation of fully conservative difference schemes for linear partial differential equations. The approach is based on enlargement of the equations in their integral conservation law form by extra integral relations between unknown functions and their derivatives, and on discretization of the obtained system. The structure of the discrete system depends on numerical approximation methods for the integrals occurring in the enlarged system. As a result of the discretization, a system of linear polynomial difference equations is derived for the unknown functions and their partial derivatives. A difference scheme is constructed by elimination of all the partial derivatives. The elimination can be achieved by selecting a proper elimination ranking and by computing a Gr\\\"obner basis of the linear difference ideal generated by the polynomials in the discrete system. For these purposes we use the difference form of Janet-like Gr\\\"obner bases and their implementation in Maple. As illustration of the described methods and algorithms, we construct a number of difference schemes for Burgers and Falkowich-Karman equations and discuss their numerical properties.",
        "published": "2006-05-12T13:34:41Z",
        "link": "http://arxiv.org/abs/math/0605334v1",
        "categories": [
            "math.RA",
            "cs.NA",
            "cs.SC",
            "math.NA"
        ]
    },
    {
        "title": "Low Complexity Algorithms for Linear Recurrences",
        "authors": [
            "Alin Bostan",
            "Frédéric Chyzak",
            "Bruno Salvy",
            "Thomas Cluzeau"
        ],
        "summary": "We consider two kinds of problems: the computation of polynomial and rational solutions of linear recurrences with coefficients that are polynomials with integer coefficients; indefinite and definite summation of sequences that are hypergeometric over the rational numbers. The algorithms for these tasks all involve as an intermediate quantity an integer $N$ (dispersion or root of an indicial polynomial) that is potentially exponential in the bit size of their input. Previous algorithms have a bit complexity that is at least quadratic in $N$. We revisit them and propose variants that exploit the structure of solutions and avoid expanding polynomials of degree $N$. We give two algorithms: a probabilistic one that detects the existence or absence of nonzero polynomial and rational solutions in $O(\\sqrt{N}\\log^{2}N)$ bit operations; a deterministic one that computes a compact representation of the solution in $O(N\\log^{3}N)$ bit operations. Similar speed-ups are obtained in indefinite and definite hypergeometric summation. We describe the results of an implementation.",
        "published": "2006-05-16T10:03:04Z",
        "link": "http://arxiv.org/abs/cs/0605068v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Efficient algorithm for computing the Euler-Poincaré characteristic of   a semi-algebraic set defined by few quadratic inequalities",
        "authors": [
            "Saugata Basu"
        ],
        "summary": "We present an algorithm which takes as input a closed semi-algebraic set, $S \\subset \\R^k$, defined by \\[ P_1 \\leq 0, ..., P_\\ell \\leq 0, P_i \\in \\R[X_1,...,X_k], \\deg(P_i) \\leq 2, \\] and computes the Euler-Poincar\\'e characteristic of $S$. The complexity of the algorithm is $k^{O(\\ell)}$.",
        "published": "2006-05-18T18:03:29Z",
        "link": "http://arxiv.org/abs/cs/0605082v1",
        "categories": [
            "cs.SC",
            "cs.CG"
        ]
    },
    {
        "title": "Complexity of Resolution of Parametric Systems of Polynomial Equations   and Inequations",
        "authors": [
            "Guillaume Moroz"
        ],
        "summary": "Consider a system of n polynomial equations and r polynomial inequations in n indeterminates of degree bounded by d with coefficients in a polynomial ring of s parameters with rational coefficients of bit-size at most $\\sigma$. From the real viewpoint, solving such a system often means describing some semi-algebraic sets in the parameter space over which the number of real solutions of the considered parametric system is constant. Following the works of Lazard and Rouillier, this can be done by the computation of a discriminant variety. In this report we focus on the case where for a generic specialization of the parameters the system of equations generates a radical zero-dimensional ideal, which is usual in the applications. In this case, we provide a deterministic method computing the minimal discriminant variety reducing the problem to a problem of elimination. Moreover, we prove that the degree of the computed minimal discriminant variety is bounded by $D:=(n+r)d^{(n+1)}$ and that the complexity of our method is $\\sigma^{\\mathcal{O}(1)} D^{\\mathcal{O}(n+s)}$ bit-operations on a deterministic Turing machine.",
        "published": "2006-06-07T14:44:53Z",
        "link": "http://arxiv.org/abs/cs/0606031v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "On computing fixpoints in well-structured regular model checking, with   applications to lossy channel systems",
        "authors": [
            "C. Baier",
            "N. Bertrand",
            "Ph. Schnoebelen"
        ],
        "summary": "We prove a general finite convergence theorem for \"upward-guarded\" fixpoint expressions over a well-quasi-ordered set. This has immediate applications in regular model checking of well-structured systems, where a main issue is the eventual convergence of fixpoint computations. In particular, we are able to directly obtain several new decidability results on lossy channel systems.",
        "published": "2006-06-21T15:49:13Z",
        "link": "http://arxiv.org/abs/cs/0606091v1",
        "categories": [
            "cs.SC",
            "cs.GT"
        ]
    },
    {
        "title": "The Newton Polytope of the Implicit Equation",
        "authors": [
            "Bernd Sturmfels",
            "Jenia Tevelev",
            "Josephine Yu"
        ],
        "summary": "We apply tropical geometry to study the image of a map defined by Laurent polynomials with generic coefficients. If this image is a hypersurface then our approach gives a construction of its Newton polytope.",
        "published": "2006-07-16T05:55:34Z",
        "link": "http://arxiv.org/abs/math/0607368v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.AG",
            "13P10, 14Q99, 52B20, 68W30"
        ]
    },
    {
        "title": "Extending the scalars of minimizations",
        "authors": [
            "Gérard Duchamp",
            "Eric Laugerotte",
            "Jean-Gabriel Luque"
        ],
        "summary": "In the classical theory of formal languages, finite state automata allow to recognize the words of a rational subset of $\\Sigma^*$ where $\\Sigma$ is a set of symbols (or the alphabet). Now, given a semiring $(\\K,+,.)$, one can construct $\\K$-subsets of $\\Sigma^*$ in the sense of Eilenberg, that are alternatively called noncommutative formal power series for which a framework very similar to language theory has been constructed Particular noncommutative formal power series, which are called rational series, are the behaviour of a family of weighted automata (or $\\K$-automata). In order to get an efficient encoding, it may be interesting to point out one of them with the smallest number of states. Minimization processes of $\\K$-automata already exist for $\\K$ being: {\\bf a)} a field, {\\bf b)} a noncommutative field, {\\bf c)} a PID . When $\\K$ is the bolean semiring, such a minimization process (with isomorphisms of minimal objects) is known within the category of deterministic automata. Minimal automata have been proved to be isomorphic in cases {\\bf (a)} and {\\bf (b)}. But the proof given for (b) is not constructive. In fact, it lays on the existence of a basis for a submodule of $\\K^n$. Here we give an independent algorithm which reproves this fact and an example of a pair of nonisomorphic minimal automata. Moreover, we examine the possibility of extending {\\bf (c)}. To this end, we provide an {\\em Effective Minimization Process} (or {\\em EMP}) which can be used for more general sets of coefficients.",
        "published": "2006-07-18T07:06:59Z",
        "link": "http://arxiv.org/abs/math/0607411v1",
        "categories": [
            "math.CO",
            "cs.DS",
            "cs.SC"
        ]
    },
    {
        "title": "Direct and dual laws for automata with multiplicities",
        "authors": [
            "Gérard Duchamp",
            "Marianne Flouret",
            "Eric Laugerotte",
            "Jean-Gabriel Luque"
        ],
        "summary": "We present here theoretical results coming from the implementation of the package called AMULT (automata with multiplicities in several noncommutative variables). We show that classical formulas are ``almost every time'' optimal, characterize the dual laws preserving rationality and also relators that are compatible with these laws.",
        "published": "2006-07-18T07:08:35Z",
        "link": "http://arxiv.org/abs/math/0607412v1",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.SC"
        ]
    },
    {
        "title": "Transitive factorizations of free partially commutative monoids and Lie   algebras",
        "authors": [
            "Jean-Gabriel Luque",
            "Gérard Henry Edmond Duchamp"
        ],
        "summary": "Let $\\M(A,\\theta)$ be a free partially commutative monoid. We give here a necessary and sufficient condition on a subalphabet $B\\subset A$ such that the right factor of a bisection $\\M(A,\\theta)=\\M(B,\\theta\\_B).T$ be also partially commutative free. This extends strictly the (classical) elimination theory on partial commutations and allows to construct new factorizations of $\\M(A,\\theta)$ and associated bases of $L\\_K(A,\\theta)$.",
        "published": "2006-07-18T12:42:33Z",
        "link": "http://arxiv.org/abs/math/0607420v1",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.SC",
            "math.GM"
        ]
    },
    {
        "title": "A field-theory motivated approach to symbolic computer algebra",
        "authors": [
            "Kasper Peeters"
        ],
        "summary": "Field theory is an area in physics with a deceptively compact notation. Although general purpose computer algebra systems, built around generic list-based data structures, can be used to represent and manipulate field-theory expressions, this often leads to cumbersome input formats, unexpected side-effects, or the need for a lot of special-purpose code. This makes a direct translation of problems from paper to computer and back needlessly time-consuming and error-prone. A prototype computer algebra system is presented which features TeX-like input, graph data structures, lists with Young-tableaux symmetries and a multiple-inheritance property system. The usefulness of this approach is illustrated with a number of explicit field-theory problems.",
        "published": "2006-08-01T19:05:20Z",
        "link": "http://arxiv.org/abs/cs/0608005v2",
        "categories": [
            "cs.SC",
            "gr-qc",
            "hep-th"
        ]
    },
    {
        "title": "ACD Term Rewriting",
        "authors": [
            "Gregory J. Duck",
            "Peter J. Stuckey",
            "Sebastian Brand"
        ],
        "summary": "We introduce Associative Commutative Distributive Term Rewriting (ACDTR), a rewriting language for rewriting logical formulae. ACDTR extends AC term rewriting by adding distribution of conjunction over other operators. Conjunction is vital for expressive term rewriting systems since it allows us to require that multiple conditions hold for a term rewriting rule to be used. ACDTR uses the notion of a \"conjunctive context\", which is the conjunction of constraints that must hold in the context of a term, to enable the programmer to write very expressive and targeted rewriting rules. ACDTR can be seen as a general logic programming language that extends Constraint Handling Rules and AC term rewriting. In this paper we define the semantics of ACDTR and describe our prototype implementation.",
        "published": "2006-08-03T02:55:16Z",
        "link": "http://arxiv.org/abs/cs/0608016v1",
        "categories": [
            "cs.PL",
            "cs.SC"
        ]
    },
    {
        "title": "Satisfying KBO Constraints",
        "authors": [
            "Harald Zankl",
            "Aart Middeldorp"
        ],
        "summary": "This paper presents two new approaches to prove termination of rewrite systems with the Knuth-Bendix order efficiently. The constraints for the weight function and for the precedence are encoded in (pseudo-)propositional logic and the resulting formula is tested for satisfiability. Any satisfying assignment represents a weight function and a precedence such that the induced Knuth-Bendix order orients the rules of the encoded rewrite system from left to right.",
        "published": "2006-08-06T12:42:31Z",
        "link": "http://arxiv.org/abs/cs/0608032v2",
        "categories": [
            "cs.SC",
            "cs.LO"
        ]
    },
    {
        "title": "A linear algebra approach to the differentiation index of generic DAE   systems",
        "authors": [
            "Lisi D'Alfonso",
            "Gabriela Jeronimo",
            "Pablo Solerno"
        ],
        "summary": "The notion of differentiation index for DAE systems of arbitrary order with generic second members is discussed by means of the study of the behavior of the ranks of certain Jacobian associated sub-matrices. As a by-product, we obtain upper bounds for the regularity of the Hilbert-Kolchin function and the order of the ideal associated to the DAE systems under consideration, not depending on characteristic sets. Some quantitative and algorithmic results concerning differential transcendence bases and induced equivalent explicit ODE systems are also established.",
        "published": "2006-08-15T21:57:24Z",
        "link": "http://arxiv.org/abs/cs/0608064v2",
        "categories": [
            "cs.SC",
            "math.AC"
        ]
    },
    {
        "title": "Fast algorithms for computing isogenies between elliptic curves",
        "authors": [
            "Alin Bostan",
            "Bruno Salvy",
            "Francois Morain",
            "Eric Schost"
        ],
        "summary": "We survey algorithms for computing isogenies between elliptic curves defined over a field of characteristic either 0 or a large prime. We introduce a new algorithm that computes an isogeny of degree $\\ell$ ($\\ell$ different from the characteristic) in time quasi-linear with respect to $\\ell$. This is based in particular on fast algorithms for power series expansion of the Weierstrass $\\wp$-function and related functions.",
        "published": "2006-09-06T12:10:17Z",
        "link": "http://arxiv.org/abs/cs/0609020v1",
        "categories": [
            "cs.CC",
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "On factorization and solution of multidimensional linear partial   differential equations",
        "authors": [
            "S. P. Tsarev"
        ],
        "summary": "We describe a method of obtaining closed-form complete solutions of certain second-order linear partial differential equations with more than two independent variables. This method generalizes the classical method of Laplace transformations of second-order hyperbolic equations in the plane and is based on an idea given by Ulisse Dini in 1902.",
        "published": "2006-09-13T13:28:18Z",
        "link": "http://arxiv.org/abs/cs/0609075v2",
        "categories": [
            "cs.SC",
            "nlin.SI",
            "F.2.2"
        ]
    },
    {
        "title": "Minimal Polynomials for the Coordinates of the Harborth Graph",
        "authors": [
            "Eberhard H. -A. Gerbracht"
        ],
        "summary": "The Harborth graph is the smallest known example of a 4-regular planar unit-distance graph. In this paper we give an analytical description of the coordinates of its vertices for a particular embedding in the Euclidean plane. More precisely, we show, how to calculate the minimal polynomials of the coordinates of its vertices (with the help of a computer algebra system), and list those. Furthermore some algebraic properties of these polynomials, and consequences to the structure of the Harborth graph are determined.",
        "published": "2006-09-13T17:27:55Z",
        "link": "http://arxiv.org/abs/math/0609360v3",
        "categories": [
            "math.CO",
            "cs.SC",
            "05C62 (Primary); 05C10, 13P10 (Secondary)"
        ]
    },
    {
        "title": "Fast Jacobian group operations for C_{3,4} curves over a large finite   field",
        "authors": [
            "Fatima K. Abu Salem",
            "Kamal Khuri-Makdisi"
        ],
        "summary": "Let C be an arbitrary smooth algebraic curve of genus g over a large finite field K. We revisit fast addition algorithms in the Jacobian of C due to Khuri-Makdisi (math.NT/0409209, to appear in Math. Comp.). The algorithms, which reduce to linear algebra in vector spaces of dimension O(g) once |K| >> g, and which asymptotically require O(g^{2.376}) field operations using fast linear algebra, are shown to perform efficiently even for certain low genus curves. Specifically, we provide explicit formulae for performing the group law on Jacobians of C_{3,4} curves of genus 3. We show that, typically, the addition of two distinct elements in the Jacobian of a C_{3,4} curve requires 117 multiplications and 2 inversions in K, and an element can be doubled using 129 multiplications and 2 inversions in K. This represents an improvement of approximately 20% over previous methods.",
        "published": "2006-10-03T16:13:39Z",
        "link": "http://arxiv.org/abs/math/0610121v3",
        "categories": [
            "math.NT",
            "cs.SC",
            "math.AG",
            "14Q05, 14H40, 14H45, 11Y16, 68W30"
        ]
    },
    {
        "title": "Strong bi-homogeneous Bézout theorem and its use in effective real   algebraic geometry",
        "authors": [
            "Mohab Safey El Din",
            "Philippe Trebuchet"
        ],
        "summary": "Let f1, ..., fs be a polynomial family in Q[X1,..., Xn] (with s less than n) of degree bounded by D. Suppose that f1, ..., fs generates a radical ideal, and defines a smooth algebraic variety V. Consider a projection P. We prove that the degree of the critical locus of P restricted to V is bounded by D^s(D-1)^(n-s) times binomial of n and n-s. This result is obtained in two steps. First the critical points of P restricted to V are characterized as projections of the solutions of Lagrange's system for which a bi-homogeneous structure is exhibited. Secondly we prove a bi-homogeneous B\\'ezout Theorem, which bounds the sum of the degrees of the equidimensional components of the radical of an ideal generated by a bi-homogeneous polynomial family. This result is improved when f1,..., fs is a regular sequence. Moreover, we use Lagrange's system to design an algorithm computing at least one point in each connected component of a smooth real algebraic set. This algorithm generalizes, to the non equidimensional case, the one of Safey El Din and Schost. The evaluation of the output size of this algorithm gives new upper bounds on the first Betti number of a smooth real algebraic set. Finally, we estimate its arithmetic complexity and prove that in the worst cases it is polynomial in n, s, D^s(D-1)^(n-s) and the binomial of n and n-s, and the complexity of evaluation of f1,..., fs.",
        "published": "2006-10-10T15:02:07Z",
        "link": "http://arxiv.org/abs/cs/0610051v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "List Decoding of Hermitian Codes using Groebner Bases",
        "authors": [
            "Kwankyu Lee",
            "Michael E. O'Sullivan"
        ],
        "summary": "List decoding of Hermitian codes is reformulated to allow an efficient and simple algorithm for the interpolation step. The algorithm is developed using the theory of Groebner bases of modules. The computational complexity of the algorithm seems comparable to previously known algorithms achieving the same task, and the algorithm is better suited for hardware implementation.",
        "published": "2006-10-23T07:52:42Z",
        "link": "http://arxiv.org/abs/cs/0610132v1",
        "categories": [
            "cs.IT",
            "cs.SC",
            "math.IT"
        ]
    },
    {
        "title": "Bounds on the coefficients of the characteristic and minimal polynomials",
        "authors": [
            "Jean-Guillaume Dumas"
        ],
        "summary": "This note presents absolute bounds on the size of the coefficients of the characteristic and minimal polynomials depending on the size of the coefficients of the associated matrix. Moreover, we present algorithms to compute more precise input-dependant bounds on these coefficients. Such bounds are e.g. useful to perform deterministic chinese remaindering of the characteristic or minimal polynomial of an integer matrix.",
        "published": "2006-10-24T08:24:08Z",
        "link": "http://arxiv.org/abs/cs/0610136v4",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Groebner Bases Applied to Systems of Linear Difference Equations",
        "authors": [
            "V. P. Gerdt"
        ],
        "summary": "In this paper we consider systems of partial (multidimensional) linear difference equations. Specifically, such systems arise in scientific computing under discretization of linear partial differential equations and in computational high energy physics as recurrence relations for multiloop Feynman integrals. The most universal algorithmic tool for investigation of linear difference systems is based on their transformation into an equivalent Groebner basis form. We present an algorithm for this transformation implemented in Maple. The algorithm and its implementation can be applied to automatic generation of difference schemes for linear partial differential equations and to reduction of Feynman integrals. Some illustrative examples are given.",
        "published": "2006-11-09T10:41:31Z",
        "link": "http://arxiv.org/abs/cs/0611041v1",
        "categories": [
            "cs.SC",
            "I.1.2; I.1.4"
        ]
    },
    {
        "title": "Predicate Abstraction via Symbolic Decision Procedures",
        "authors": [
            "Shuvendu K. Lahiri",
            "Thomas Ball",
            "Byron Cook"
        ],
        "summary": "We present a new approach for performing predicate abstraction based on symbolic decision procedures. Intuitively, a symbolic decision procedure for a theory takes a set of predicates in the theory and symbolically executes a decision procedure on all the subsets over the set of predicates. The result of the symbolic decision procedure is a shared expression (represented by a directed acyclic graph) that implicitly represents the answer to a predicate abstraction query.   We present symbolic decision procedures for the logic of Equality and Uninterpreted Functions (EUF) and Difference logic (DIFF) and show that these procedures run in pseudo-polynomial (rather than exponential) time. We then provide a method to construct symbolic decision procedures for simple mixed theories (including the two theories mentioned above) using an extension of the Nelson-Oppen combination method. We present preliminary evaluation of our Procedure on predicate abstraction benchmarks from device driver verification in SLAM.",
        "published": "2006-12-01T19:56:11Z",
        "link": "http://arxiv.org/abs/cs/0612003v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Acronym-Meaning Extraction from Corpora Using Multi-Tape Weighted   Finite-State Machines",
        "authors": [
            "André Kempe"
        ],
        "summary": "The automatic extraction of acronyms and their meaning from corpora is an important sub-task of text mining. It can be seen as a special case of string alignment, where a text chunk is aligned with an acronym. Alternative alignments have different cost, and ideally the least costly one should give the correct meaning of the acronym. We show how this approach can be implemented by means of a 3-tape weighted finite-state machine (3-WFSM) which reads a text chunk on tape 1 and an acronym on tape 2, and generates all alternative alignments on tape 3. The 3-WFSM can be automatically generated from a simple regular expression. No additional algorithms are required at any stage. Our 3-WFSM has a size of 27 states and 64 transitions, and finds the best analysis of an acronym in a few milliseconds.",
        "published": "2006-12-06T10:13:12Z",
        "link": "http://arxiv.org/abs/cs/0612033v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "cs.SC",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Viterbi Algorithm Generalized for n-Tape Best-Path Search",
        "authors": [
            "André Kempe"
        ],
        "summary": "We present a generalization of the Viterbi algorithm for identifying the path with minimal (resp. maximal) weight in a n-tape weighted finite-state machine (n-WFSM), that accepts a given n-tuple of input strings (s_1,... s_n). It also allows us to compile the best transduction of a given input n-tuple by a weighted (n+m)-WFSM (transducer) with n input and m output tapes. Our algorithm has a worst-case time complexity of O(|s|^n |E| log (|s|^n |Q|)), where n and |s| are the number and average length of the strings in the n-tuple, and |Q| and |E| the number of states and transitions in the n-WFSM, respectively. A straight forward alternative, consisting in intersection followed by classical shortest-distance search, operates in O(|s|^n (|E|+|Q|) log (|s|^n |Q|)) time.",
        "published": "2006-12-07T08:42:46Z",
        "link": "http://arxiv.org/abs/cs/0612041v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "cs.SC",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Explicit factors of some iterated resultants and discriminants",
        "authors": [
            "Laurent Busé",
            "Bernard Mourrain"
        ],
        "summary": "In this paper, the result of applying iterative univariate resultant constructions to multivariate polynomials is analyzed. We consider the input polynomials as generic polynomials of a given degree and exhibit explicit decompositions into irreducible factors of several constructions involving two times iterated univariate resultants and discriminants over the integer universal ring of coefficients of the entry polynomials. Cases involving from two to four generic polynomials and resultants or discriminants in one of their variables are treated. The decompositions into irreducible factors we get are obtained by exploiting fundamental properties of the univariate resultants and discriminants and induction on the degree of the polynomials. As a consequence, each irreducible factor can be separately and explicitly computed in terms of a certain multivariate resultant. With this approach, we also obtain as direct corollaries some results conjectured by Collins and McCallum which correspond to the case of polynomials whose coefficients are themselves generic polynomials in other variables. Finally, a geometric interpretation of the algebraic factorization of the iterated discriminant of a single polynomial is detailled.",
        "published": "2006-12-08T14:10:38Z",
        "link": "http://arxiv.org/abs/cs/0612050v2",
        "categories": [
            "cs.SC",
            "math.AC",
            "math.AG"
        ]
    },
    {
        "title": "Reduction of Algebraic Parametric Systems by Rectification of their   Affine Expanded Lie Symmetries",
        "authors": [
            "Alexandre Sedoglavic"
        ],
        "summary": "Lie group theory states that knowledge of a $m$-parameters solvable group of symmetries of a system of ordinary differential equations allows to reduce by $m$ the number of equations. We apply this principle by finding some \\emph{affine derivations} that induces \\emph{expanded} Lie point symmetries of considered system. By rewriting original problem in an invariant coordinates set for these symmetries, we \\emph{reduce} the number of involved parameters. We present an algorithm based on this standpoint whose arithmetic complexity is \\emph{quasi-polynomial} in input's size.",
        "published": "2006-12-19T14:58:15Z",
        "link": "http://arxiv.org/abs/cs/0612094v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "All order epsilon-expansion of Gauss hypergeometric functions with   integer and half/integer values of parameters",
        "authors": [
            "M. Yu. Kalmykov",
            "B. F. L. Ward",
            "S. Yost"
        ],
        "summary": "It is proved that the Laurent expansion of the following Gauss hypergeometric functions,   2F1(I1+a*epsilon, I2+b*ep; I3+c*epsilon;z),   2F1(I1+a*epsilon, I2+b*epsilon;I3+1/2+c*epsilon;z),   2F1(I1+1/2+a*epsilon, I2+b*epsilon; I3+c*epsilon;z),   2F1(I1+1/2+a*epsilon, I2+b*epsilon; I3+1/2+c*epsilon;z),   2F1(I1+1/2+a*epsilon,I2+1/2+b*epsilon; I3+1/2+c*epsilon;z), where I1,I2,I3 are an arbitrary integer nonnegative numbers, a,b,c are an arbitrary numbers and epsilon is an arbitrary small parameters, are expressible in terms of the harmonic polylogarithms of Remiddi and Vermaseren with polynomial coefficients. An efficient algorithm for the calculation of the higher-order coefficients of Laurent expansion is constructed. Some particular cases of Gauss hypergeometric functions are also discussed.",
        "published": "2006-12-21T17:52:31Z",
        "link": "http://arxiv.org/abs/hep-th/0612240v2",
        "categories": [
            "hep-th",
            "cs.SC",
            "hep-ph",
            "math-ph",
            "math.CA",
            "math.MP",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Symmetric Subresultants and Applications",
        "authors": [
            "Cyril Brunie",
            "Philippe Saux Picart"
        ],
        "summary": "Schur's transforms of a polynomial are used to count its roots in the unit disk. These are generalized them by introducing the sequence of symmetric sub-resultants of two polynomials. Although they do have a determinantal definition, we show that they satisfy a structure theorem which allows us to compute them with a type of Euclidean division. As a consequence, a fast algorithm based on a dichotomic process and FFT is designed. We prove also that these symmetric sub-resultants have a deep link with Toeplitz matrices. Finally, we propose a new algorithm of inversion for such matrices. It has the same cost as those already known, however it is fraction-free and consequently well adapted to computer algebra.",
        "published": "2006-12-22T08:20:11Z",
        "link": "http://arxiv.org/abs/cs/0612119v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Exploring high performance distributed file storage using LDPC codes",
        "authors": [
            "Benjamin Gaidioz",
            "Birger Koblitz",
            "Nuno Santos"
        ],
        "summary": "We explore the feasibility of implementing a reliable, high performance, distributed storage system on a commodity computing cluster. Files are distributed across storage nodes using erasure coding with small Low-Density Parity-Check (LDPC) codes which provide high reliability while keeping the storage and performance overhead small. We present performance measurements done on a prototype system comprising 50 nodes which are self organised using a peer-to-peer overlay.",
        "published": "2006-01-17T13:30:47Z",
        "link": "http://arxiv.org/abs/cs/0601078v1",
        "categories": [
            "cs.DC",
            "D.4.2"
        ]
    },
    {
        "title": "Distributed Kernel Regression: An Algorithm for Training Collaboratively",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "This paper addresses the problem of distributed learning under communication constraints, motivated by distributed signal processing in wireless sensor networks and data mining with distributed databases. After formalizing a general model for distributed learning, an algorithm for collaboratively training regularized kernel least-squares regression estimators is derived. Noting that the algorithm can be viewed as an application of successive orthogonal projection algorithms, its convergence properties are investigated and the statistical behavior of the estimator is discussed in a simplified theoretical setting.",
        "published": "2006-01-20T17:46:45Z",
        "link": "http://arxiv.org/abs/cs/0601089v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Compression Scheme for Faster and Secure Data Transmission Over Internet",
        "authors": [
            "B. S. Shajeemohan",
            "Dr. V. K. Govindan"
        ],
        "summary": "Compression algorithms reduce the redundancy in data representation to decrease the storage required for that data. Data compression offers an attractive approach to reducing communication costs by using available bandwidth effectively. Over the last decade there has been an unprecedented explosion in the amount of digital data transmitted via the Internet, representing text, images, video, sound, computer programs, etc. With this trend expected to continue, it makes sense to pursue research on developing algorithms that can most effectively use available network bandwidth by maximally compressing data. It is also important to consider the security aspects of the data being transmitted while compressing it, as most of the text data transmitted over the Internet is very much vulnerable to a multitude of attacks. This paper is focused on addressing this problem of lossless compression of text files with an added security.",
        "published": "2006-01-23T11:47:06Z",
        "link": "http://arxiv.org/abs/cs/0601097v2",
        "categories": [
            "cs.PF",
            "cs.DC"
        ]
    },
    {
        "title": "Localization in Wireless Sensor Grids",
        "authors": [
            "Chen Zhang",
            "Ted Herman"
        ],
        "summary": "This work reports experiences on using radio ranging to position sensors in a grid topology. The implementation is simple, efficient, and could be practically distributed. The paper describes an implementation and experimental result based on RSSI distance estimation. Novel techniques such as fuzzy membership functions and table lookup are used to obtain more accurate result and simplify the computation. An 86% accuracy is achieved in the experiment in spite of inaccurate RSSI distance estimates with errors up to 60%.",
        "published": "2006-01-25T23:48:12Z",
        "link": "http://arxiv.org/abs/cs/0601111v1",
        "categories": [
            "cs.DC",
            "C.2.4; C.3; J.7"
        ]
    },
    {
        "title": "Scalable Algorithms for Aggregating Disparate Forecasts of Probability",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "Daniel N. Osherson",
            "H. Vincent Poor"
        ],
        "summary": "In this paper, computational aspects of the panel aggregation problem are addressed. Motivated primarily by applications of risk assessment, an algorithm is developed for aggregating large corpora of internally incoherent probability assessments. The algorithm is characterized by a provable performance guarantee, and is demonstrated to be orders of magnitude faster than existing tools when tested on several real-world data-sets. In addition, unexpected connections between research in risk assessment and wireless sensor networks are exposed, as several key ideas are illustrated to be useful in both fields.",
        "published": "2006-01-31T03:01:35Z",
        "link": "http://arxiv.org/abs/cs/0601131v2",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An Optimal Distributed Edge-Biconnectivity Algorithm",
        "authors": [
            "David Pritchard"
        ],
        "summary": "We describe a synchronous distributed algorithm which identifies the edge-biconnected components of a connected network. It requires a leader, and uses messages of size O(log |V|). The main idea is to preorder a BFS spanning tree, and then to efficiently compute least common ancestors so as to mark cycle edges. This algorithm takes O(Diam) time and uses O(|E|) messages. Furthermore, we show that no correct singly-initiated edge-biconnectivity algorithm can beat either bound on any graph by more than a constant factor. We also describe a near-optimal local algorithm for edge-biconnectivity.",
        "published": "2006-02-05T20:47:23Z",
        "link": "http://arxiv.org/abs/cs/0602013v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Finding total unimodularity in optimization problems solved by linear   programs",
        "authors": [
            "Christoph Durr",
            "Mathilde Hurand"
        ],
        "summary": "A popular approach in combinatorial optimization is to model problems as integer linear programs. Ideally, the relaxed linear program would have only integer solutions, which happens for instance when the constraint matrix is totally unimodular. Still, sometimes it is possible to build an integer solution with the same cost from the fractional solution. Examples are two scheduling problems and the single disk prefetching/caching problem. We show that problems such as the three previously mentioned can be separated into two subproblems: (1) finding an optimal feasible set of slots, and (2) assigning the jobs or pages to the slots. It is straigthforward to show that the latter can be solved greedily. We are able to solve the former with a totally unimodular linear program, from which we obtain simple combinatorial algorithms with improved worst case running time.",
        "published": "2006-02-06T09:09:03Z",
        "link": "http://arxiv.org/abs/cs/0602016v3",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2"
        ]
    },
    {
        "title": "Bulk Scheduling with DIANA Scheduler",
        "authors": [
            "Ashiq Anjum",
            "Richard McClatchey",
            "Arshad Ali",
            "Ian Willers"
        ],
        "summary": "Results from and progress on the development of a Data Intensive and Network Aware (DIANA) Scheduling engine, primarily for data intensive sciences such as physics analysis, are described. Scientific analysis tasks can involve thousands of computing, data handling, and network resources and the size of the input and output files and the amount of overall storage space allocated to a user necessarily can have significant bearing on the scheduling of data intensive applications. If the input or output files must be retrieved from a remote location, then the time required transferring the files must also be taken into consideration when scheduling compute resources for the given application. The central problem in this study is the coordinated management of computation and data at multiple locations and not simply data movement. However, this can be a very costly operation and efficient scheduling can be a challenge if compute and data resources are mapped without network cost. We have implemented an adaptive algorithm within the DIANA Scheduler which takes into account data location and size, network performance and computation capability to make efficient global scheduling decisions. DIANA is a performance-aware as well as an economy-guided Meta Scheduler. It iteratively allocates each job to the site that is likely to produce the best performance as well as optimizing the global queue for any remaining pending jobs. Therefore it is equally suitable whether a single job is being submitted or bulk scheduling is being performed. Results suggest that considerable performance improvements are to be gained by adopting the DIANA scheduling approach.",
        "published": "2006-02-07T16:47:16Z",
        "link": "http://arxiv.org/abs/cs/0602026v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Self-stabilization of Circular Arrays of Automata",
        "authors": [
            "Leonid A. Levin"
        ],
        "summary": "[Gacs, Kurdiumov, Levin, 78] proposed simple one-dimensional cellular automata with 2 states. In an infinite array they are self-stabilizing: if all but a finite minority of automata are in the same state, the minority states disappear. Implicit in the paper was a stronger result that a sufficiently small minority of states vanish even in a finite circular array. The following note makes this strengthening explicit.",
        "published": "2006-02-09T02:07:52Z",
        "link": "http://arxiv.org/abs/cs/0602033v1",
        "categories": [
            "cs.DC",
            "cs.DM"
        ]
    },
    {
        "title": "Emergence Explained",
        "authors": [
            "Russ Abbott"
        ],
        "summary": "Emergence (macro-level effects from micro-level causes) is at the heart of the conflict between reductionism and functionalism. How can there be autonomous higher level laws of nature (the functionalist claim) if everything can be reduced to the fundamental forces of physics (the reductionist position)? We cut through this debate by applying a computer science lens to the way we view nature. We conclude (a) that what functionalism calls the special sciences (sciences other than physics) do indeed study autonomous laws and furthermore that those laws pertain to real higher level entities but (b) that interactions among such higher-level entities is epiphenomenal in that they can always be reduced to primitive physical forces. In other words, epiphenomena, which we will identify with emergent phenomena, do real higher-level work. The proposed perspective provides a framework for understanding many thorny issues including the nature of entities, stigmergy, the evolution of complexity, phase transitions, supervenience, and downward entailment. We also discuss some practical considerations pertaining to systems of systems and the limitations of modeling.",
        "published": "2006-02-12T22:11:14Z",
        "link": "http://arxiv.org/abs/cs/0602045v1",
        "categories": [
            "cs.MA",
            "cs.DC",
            "cs.GL"
        ]
    },
    {
        "title": "The Computational and Storage Potential of Volunteer Computing",
        "authors": [
            "David P. Anderson",
            "Gilles Fedak"
        ],
        "summary": "\"Volunteer computing\" uses Internet-connected computers, volunteered by their owners, as a source of computing power and storage. This paper studies the potential capacity of volunteer computing. We analyzed measurements of over 330,000 hosts participating in a volunteer computing project. These measurements include processing power, memory, disk space, network throughput, host availability, user-specified limits on resource usage, and host churn. We show that volunteer computing can support applications that are significantly more data-intensive, or have larger memory and storage requirements, than those in current projects.",
        "published": "2006-02-16T22:33:04Z",
        "link": "http://arxiv.org/abs/cs/0602061v1",
        "categories": [
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "Parallel Symbolic Computation of Curvature Invariants in General   Relativity",
        "authors": [
            "K. R. Koehler"
        ],
        "summary": "We present a practical application of parallel symbolic computation in General Relativity: the calculation of curvature invariants for large dimension. We discuss the structure of the calculations, an implementation of the technique and scaling of the computation with spacetime dimension for various invariants.",
        "published": "2006-02-19T19:28:44Z",
        "link": "http://arxiv.org/abs/cs/0602068v1",
        "categories": [
            "cs.DC",
            "cs.SC",
            "gr-qc"
        ]
    },
    {
        "title": "Associative Memory For Reversible Programming and Charge Recovery",
        "authors": [
            "John Robert Burger"
        ],
        "summary": "Presented below is an interesting type of associative memory called toggle memory based on the concept of T flip flops, as opposed to D flip flops. Toggle memory supports both reversible programming and charge recovery. Circuits designed using the principles delineated below permit matchlines to charge and discharge with near zero energy dissipation. The resulting lethargy is compensated by the massive parallelism of associative memory. Simulation indicates over 33x reduction in energy dissipation using a sinusoidal power supply at 2 MHz, assuming realistic 50 nm MOSFET models.",
        "published": "2006-02-21T21:58:06Z",
        "link": "http://arxiv.org/abs/cs/0602078v2",
        "categories": [
            "cs.AR",
            "cs.DC",
            "C.1.2; C.5.4"
        ]
    },
    {
        "title": "Final Results from and Exploitation Plans for MammoGrid",
        "authors": [
            "Chiara Del Frate",
            "Jose Galvez",
            "Tamas Hauer",
            "David Manset",
            "Richard McClatchey",
            "Mohammed Odeh",
            "Dmitry Rogulin",
            "Tony Solomonides",
            "Ruth Warren"
        ],
        "summary": "The MammoGrid project has delivered the first deployed instance of a healthgrid for clinical mammography that spans national boundaries. During the last year, the final MammoGrid prototype has undergone a series of rigorous tests undertaken by radiologists in the UK and Italy and this paper draws conclusions from those tests for the benefit of the Healthgrid community. In addition, lessons learned during the lifetime of the project are detailed and recommendations drawn for future health applications using grids. Following the completion of the project, plans have been put in place for the commercialisation of the MammoGrid system and this is also reported in this article. Particular emphasis is placed on the issues surrounding the transition from collaborative research project to a marketable product. This paper concludes by highlighting some of the potential areas of future development and research.",
        "published": "2006-03-09T13:49:14Z",
        "link": "http://arxiv.org/abs/cs/0603035v2",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Health-e-Child : An Integrated Biomedical Platform for Grid-Based   Paediatric Applications",
        "authors": [
            "Joerg Freund",
            "Dorin Comaniciu",
            "Yannis Ioannis",
            "Peiya Liu",
            "Richard McClatchey",
            "Edwin Morley-Fletcher",
            "Xavier Pennec",
            "Giacomo Pongiglione",
            "Xiang",
            "ZHOU"
        ],
        "summary": "There is a compelling demand for the integration and exploitation of heterogeneous biomedical information for improved clinical practice, medical research, and personalised healthcare across the EU. The Health-e-Child project aims at developing an integrated healthcare platform for European Paediatrics, providing seamless integration of traditional and emerging sources of biomedical information. The long-term goal of the project is to provide uninhibited access to universal biomedical knowledge repositories for personalised and preventive healthcare, large-scale information-based biomedical research and training, and informed policy making. The project focus will be on individualised disease prevention, screening, early diagnosis, therapy and follow-up of paediatric heart diseases, inflammatory diseases, and brain tumours. The project will build a Grid-enabled European network of leading clinical centres that will share and annotate biomedical data, validate systems clinically, and diffuse clinical excellence across Europe by setting up new technologies, clinical workflows, and standards. This paper outlines the design approach being adopted in Health-e-Child to enable the delivery of an integrated biomedical information platform.",
        "published": "2006-03-09T13:54:15Z",
        "link": "http://arxiv.org/abs/cs/0603036v2",
        "categories": [
            "cs.DC",
            "H.2.4; J.2"
        ]
    },
    {
        "title": "User-Relative Names for Globally Connected Personal Devices",
        "authors": [
            "Bryan Ford",
            "Jacob Strauss",
            "Chris Lesniewski-Laas",
            "Sean Rhea",
            "Frans Kaashoek",
            "Robert Morris"
        ],
        "summary": "Nontechnical users who own increasingly ubiquitous network-enabled personal devices such as laptops, digital cameras, and smart phones need a simple, intuitive, and secure way to share information and services between their devices. User Information Architecture, or UIA, is a novel naming and peer-to-peer connectivity architecture addressing this need. Users assign UIA names by \"introducing\" devices to each other on a common local-area network, but these names remain securely bound to their target as devices migrate. Multiple devices owned by the same user, once introduced, automatically merge their namespaces to form a distributed \"personal cluster\" that the owner can access or modify from any of his devices. Instead of requiring users to allocate globally unique names from a central authority, UIA enables users to assign their own \"user-relative\" names both to their own devices and to other users. With UIA, for example, Alice can always access her iPod from any of her own personal devices at any location via the name \"ipod\", and her friend Bob can access her iPod via a relative name like \"ipod.Alice\".",
        "published": "2006-03-18T17:27:22Z",
        "link": "http://arxiv.org/abs/cs/0603076v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.OS",
            "C.2.1; C.2.2"
        ]
    },
    {
        "title": "IP over P2P: Enabling Self-configuring Virtual IP Networks for Grid   Computing",
        "authors": [
            "Arijit Ganguly",
            "Abhishek Agrawal",
            "P. Oscar Boykin",
            "Renato Figueiredo"
        ],
        "summary": "Peer-to-peer (P2P) networks have mostly focused on task oriented networking, where networks are constructed for single applications, i.e. file-sharing, DNS caching, etc. In this work, we introduce IPOP, a system for creating virtual IP networks on top of a P2P overlay. IPOP enables seamless access to Grid resources spanning multiple domains by aggregating them into a virtual IP network that is completely isolated from the physical network. The virtual IP network provided by IPOP supports deployment of existing IP-based protocols over a robust, self-configuring P2P overlay. We present implementation details as well as experimental measurement results taken from LAN, WAN, and Planet-Lab tests.",
        "published": "2006-03-22T16:50:45Z",
        "link": "http://arxiv.org/abs/cs/0603087v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Remote-control and clustering of physical computations using the XML-RPC   protocol and the open-Mosix system",
        "authors": [
            "T. Blachowicz",
            "M. Wieja"
        ],
        "summary": "The applications of the remote control of physical simulations performed in clustered computers running under an open-Mosix system are presented. Results from the simulation of a 2-dimensional ferromagnetic system of spins in the Ising scheme are provided. Basic parameters of a simulated hysteresis loop like coercivity and exchange bias due to pinning of ferromagnetic spins are given. The paper describes in physicists terminology a cost effective solution which utilizes an XML-RPC protocol (Extensible Markup Language - Remote Procedure Calling) and standard C++ and Python languages.",
        "published": "2006-03-28T20:45:57Z",
        "link": "http://arxiv.org/abs/cs/0603111v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "A General Framework for Scalability and Performance Analysis of DHT   Routing Systems",
        "authors": [
            "Joseph S. Kong",
            "Jesse S. A. Bridgewater",
            "Vwani P. Roychowdhury"
        ],
        "summary": "In recent years, many DHT-based P2P systems have been proposed, analyzed, and certain deployments have reached a global scale with nearly one million nodes. One is thus faced with the question of which particular DHT system to choose, and whether some are inherently more robust and scalable.   Toward developing such a comparative framework, we present the reachable component method (RCM) for analyzing the performance of different DHT routing systems subject to random failures. We apply RCM to five DHT systems and obtain analytical expressions that characterize their routability as a continuous function of system size and node failure probability. An important consequence is that in the large-network limit, the routability of certain DHT systems go to zero for any non-zero probability of node failure. These DHT routing algorithms are therefore unscalable, while some others, including Kademlia, which powers the popular eDonkey P2P system, are found to be scalable.",
        "published": "2006-03-28T22:54:37Z",
        "link": "http://arxiv.org/abs/cs/0603112v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Dynamic View of Circular Colorings",
        "authors": [
            "Hong-Gwa Yeh"
        ],
        "summary": "The main contributions of this paper are three-fold. First, we use a dynamic approach based on Reiter's pioneering work on Karp-Miller computation graphs to give a new and short proof of Mohar's Minty-type Theorem. Second, we bridge circular colorings and discrete event dynamic systems to show that the Barbosa and Gafni's results on circular chromatic number can be generalized to edge-weighted symmetric directed graphs. Third, we use the above-mentioned dynamic view of circular colorings to construct new improved lower bounds on the circular chromatic number of a graph. We show as an example that the circular chromatic number of the line graph of the Petersen graph can be determined very easily by using these bounds.",
        "published": "2006-04-10T16:02:53Z",
        "link": "http://arxiv.org/abs/math/0604226v1",
        "categories": [
            "math.CO",
            "cs.DC",
            "cs.DM",
            "05C15 (Primary) 68R05, 90B35, 68M14, 68M20(Secondary)"
        ]
    },
    {
        "title": "Demand-driven Inlining in a Region-based Optimizer for ILP Architectures",
        "authors": [
            "Thomas P. Way",
            "Lori L. Pollock"
        ],
        "summary": "Region-based compilation repartitions a program into more desirable compilation units using profiling information and procedure inlining to enable region formation analysis. Heuristics play a key role in determining when it is most beneficial to inline procedures during region formation. An ILP optimizing compiler using a region-based approach restructures a program to better reflect dynamic behavior and increase interprocedural optimization and scheduling opportunities. This paper presents an interprocedural compilation technique which performs procedure inlining on-demand, rather than as a separate phase, to improve the ability of a region-based optimizer to control code growth, compilation time and memory usage while improving performance. The interprocedural region formation algorithm utilizes a demand-driven, heuristics-guided approach to inlining, restructuring an input program into interprocedural regions. Experimental results are presented to demonstrate the impact of the algorithm and several inlining heuristics upon a number of traditional and novel compilation characteristics within a region-based ILP compiler and simulator.",
        "published": "2006-04-11T04:15:39Z",
        "link": "http://arxiv.org/abs/cs/0604043v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.3.4"
        ]
    },
    {
        "title": "Distributed Metadata with the AMGA Metadata Catalog",
        "authors": [
            "Nuno Santos",
            "Birger Koblitz"
        ],
        "summary": "Catalog Services play a vital role on Data Grids by allowing users and applications to discover and locate the data needed. On large Data Grids, with hundreds of geographically distributed sites, centralized Catalog Services do not provide the required scalability, performance or fault-tolerance. In this article, we start by presenting and discussing the general requirements on Grid Catalogs of applications being developed by the EGEE user community. This provides the motivation for the second part of the article, where we present the replication and distribution mechanisms we have designed and implemented into the AMGA Metadata Catalog, which is part of the gLite software stack being developed for the EGEE project. Implementing these mechanisms in the catalog itself has the advantages of not requiring any special support from the relational database back-end, of being database independent, and of allowing tailoring the mechanisms to the specific requirements and characteristics of Metadata Catalogs.",
        "published": "2006-04-19T10:01:26Z",
        "link": "http://arxiv.org/abs/cs/0604071v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4"
        ]
    },
    {
        "title": "A Fast and Accurate Nonlinear Spectral Method for Image Recognition and   Registration",
        "authors": [
            "Luciano da Fontoura Costa",
            "Erik Bollt"
        ],
        "summary": "This article addresses the problem of two- and higher dimensional pattern matching, i.e. the identification of instances of a template within a larger signal space, which is a form of registration. Unlike traditional correlation, we aim at obtaining more selective matchings by considering more strict comparisons of gray-level intensity. In order to achieve fast matching, a nonlinear thresholded version of the fast Fourier transform is applied to a gray-level decomposition of the original 2D image. The potential of the method is substantiated with respect to real data involving the selective identification of neuronal cell bodies in gray-level images.",
        "published": "2006-04-24T16:38:01Z",
        "link": "http://arxiv.org/abs/cs/0604094v1",
        "categories": [
            "cs.DC",
            "cond-mat.stat-mech",
            "cs.CG",
            "cs.CV"
        ]
    },
    {
        "title": "CMS Software Distribution on the LCG and OSG Grids",
        "authors": [
            "K. Rabbertz",
            "M. Thomas",
            "S. Ashby",
            "M. Corvo",
            "S. Argirò",
            "N. Darmenov",
            "R. Darwish",
            "D. Evans",
            "B. Holzman",
            "N. Ratnikova",
            "S. Muzaffar",
            "A. Nowack",
            "T. Wildish",
            "B. Kim",
            "J. Weng",
            "V. Büge"
        ],
        "summary": "The efficient exploitation of worldwide distributed storage and computing resources available in the grids require a robust, transparent and fast deployment of experiment specific software. The approach followed by the CMS experiment at CERN in order to enable Monte-Carlo simulations, data analysis and software development in an international collaboration is presented. The current status and future improvement plans are described.",
        "published": "2006-04-27T16:14:32Z",
        "link": "http://arxiv.org/abs/cs/0604109v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Gridscape II: A Customisable and Pluggable Grid Monitoring Portal and   its Integration with Google Maps",
        "authors": [
            "Hussein Gibbins",
            "Rajkumar Buyya"
        ],
        "summary": "Grid computing has emerged as an effective means of facilitating the sharing of distributed heterogeneous resources, enabling collaboration in large scale environments. However, the nature of Grid systems, coupled with the overabundance and fragmentation of information, makes it difficult to monitor resources, services, and computations in order to plan and make decisions. In this paper we present Gridscape II, a customisable portal component that can be used on its own or plugged in to compliment existing Grid portals. Gridscape II manages the gathering of information from arbitrary, heterogeneous and distributed sources and presents them together seamlessly within a single interface. It also leverages the Google Maps API in order to provide a highly interactive user interface. Gridscape II is simple and easy to use, providing a solution to those users who do not wish to invest heavily in developing their own monitoring portal from scratch, and also for those users who want something that is easy to customise and extend for their specific needs.",
        "published": "2006-05-12T09:41:28Z",
        "link": "http://arxiv.org/abs/cs/0605053v1",
        "categories": [
            "cs.DC",
            "H.4.3"
        ]
    },
    {
        "title": "Utility Computing and Global Grids",
        "authors": [
            "Chee Shin Yeo",
            "Marcos Dias de Assuncao",
            "Jia Yu",
            "Anthony Sulistio",
            "Srikumar Venugopal",
            "Martin Placek",
            "Rajkumar Buyya"
        ],
        "summary": "This chapter focuses on the use of Grid technologies to achieve utility computing. An overview of how Grids can support utility computing is first presented through the architecture of Utility Grids. Then, utility-based resource allocation is described in detail at each level of the architecture. Finally, some industrial solutions for utility computing are discussed.",
        "published": "2006-05-12T20:36:45Z",
        "link": "http://arxiv.org/abs/cs/0605056v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "A Case for Cooperative and Incentive-Based Coupling of Distributed   Clusters",
        "authors": [
            "Rajiv Ranjan",
            "Aaron Harwood",
            "Rajkumar Buyya"
        ],
        "summary": "Research interest in Grid computing has grown significantly over the past five years. Management of distributed resources is one of the key issues in Grid computing. Central to management of resources is the effectiveness of resource allocation as it determines the overall utility of the system. The current approaches to superscheduling in a grid environment are non-coordinated since application level schedulers or brokers make scheduling decisions independently of the others in the system. Clearly, this can exacerbate the load sharing and utilization problems of distributed resources due to suboptimal schedules that are likely to occur. To overcome these limitations, we propose a mechanism for coordinated sharing of distributed clusters based on computational economy. The resulting environment, called \\emph{Grid-Federation}, allows the transparent use of resources from the federation when local resources are insufficient to meet its users' requirements. The use of computational economy methodology in coordinating resource allocation not only facilitates the QoS based scheduling, but also enhances utility delivered by resources.",
        "published": "2006-05-15T10:21:22Z",
        "link": "http://arxiv.org/abs/cs/0605060v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "SLA-Based Coordinated Superscheduling Scheme and Performance for   Computational Grids",
        "authors": [
            "Rajiv Ranjan",
            "Aaron Harwood",
            "Rajkumar Buyya"
        ],
        "summary": "The Service Level Agreement~(SLA) based grid superscheduling approach promotes coordinated resource sharing. Superscheduling is facilitated between administratively and topologically distributed grid sites by grid schedulers such as Resource brokers. In this work, we present a market-based SLA coordination mechanism. We based our SLA model on a well known \\emph{contract net protocol}.   The key advantages of our approach are that it allows:~(i) resource owners to have finer degree of control over the resource allocation that was previously not possible through traditional mechanism; and (ii) superschedulers to bid for SLA contracts in the contract net with focus on completing the job within the user specified deadline. In this work, we use simulation to show the effectiveness of our proposed approach.",
        "published": "2006-05-15T13:41:27Z",
        "link": "http://arxiv.org/abs/cs/0605057v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Circle Formation of Weak Robots and Lyndon Words",
        "authors": [
            "Yoann Dieudonné",
            "Franck Petit"
        ],
        "summary": "A Lyndon word is a non-empty word strictly smaller in the lexicographic order than any of its suffixes, except itself and the empty word. In this paper, we show how Lyndon words can be used in the distributed control of a set of n weak mobile robots. By weak, we mean that the robots are anonymous, memoryless, without any common sense of direction, and unable to communicate in an other way than observation. An efficient and simple deterministic protocol to form a regular n-gon is presented and proven for n prime.",
        "published": "2006-05-22T12:09:33Z",
        "link": "http://arxiv.org/abs/cs/0605096v1",
        "categories": [
            "cs.DC",
            "cs.RO",
            "C.2.4"
        ]
    },
    {
        "title": "General Compact Labeling Schemes for Dynamic Trees",
        "authors": [
            "Amos Korman"
        ],
        "summary": "Let $F$ be a function on pairs of vertices. An {\\em $F$- labeling scheme} is composed of a {\\em marker} algorithm for labeling the vertices of a graph with short labels, coupled with a {\\em decoder} algorithm allowing one to compute $F(u,v)$ of any two vertices $u$ and $v$ directly from their labels. As applications for labeling schemes concern mainly large and dynamically changing networks, it is of interest to study {\\em distributed dynamic} labeling schemes. This paper investigates labeling schemes for dynamic trees.   This paper presents a general method for constructing labeling schemes for dynamic trees. Our method is based on extending an existing {\\em static} tree labeling scheme to the dynamic setting. This approach fits many natural functions on trees, such as ancestry relation, routing (in both the adversary and the designer port models), nearest common ancestor etc.. Our resulting dynamic schemes incur overheads (over the static scheme) on the label size and on the communication complexity. Informally, for any function $k(n)$ and any static $F$-labeling scheme on trees, we present an $F$-labeling scheme on dynamic trees incurring multiplicative overhead factors   (over the static scheme) of $O(\\log_{k(n)} n)$ on the label size and $O(k(n)\\log_{k(n)} n)$ on the amortized message complexity. In particular, by setting $k(n)=n^{\\epsilon}$ for any $0<\\epsilon<1$, we obtain dynamic labeling schemes with asymptotically optimal label sizes and sublinear amortized message complexity for all the above mentioned functions.",
        "published": "2006-05-30T13:54:26Z",
        "link": "http://arxiv.org/abs/cs/0605141v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Affine Transformations of Loop Nests for Parallel Execution and   Distribution of Data over Processors",
        "authors": [
            "E. V. Adutskevich",
            "S. V. Bakhanovich",
            "N. A. Likhoded"
        ],
        "summary": "The paper is devoted to the problem of mapping affine loop nests onto distributed memory parallel computers. A method to find affine transformations of loop nests for parallel execution and distribution of data over processors is presented. The method tends to minimize the number of communications between processors and to improve locality of data within one processor. A problem of determination of data exchange sequence is investigated. Conditions to determine the ability to arrange broadcast is presented.",
        "published": "2006-06-07T12:59:49Z",
        "link": "http://arxiv.org/abs/cs/0606028v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Asynchronous iterative computations with Web information retrieval   structures: The PageRank case",
        "authors": [
            "Giorgos Kollias",
            "Efstratios Gallopoulos",
            "Daniel B. Szyld"
        ],
        "summary": "There are several ideas being used today for Web information retrieval, and specifically in Web search engines. The PageRank algorithm is one of those that introduce a content-neutral ranking function over Web pages. This ranking is applied to the set of pages returned by the Google search engine in response to posting a search query. PageRank is based in part on two simple common sense concepts: (i)A page is important if many important pages include links to it. (ii)A page containing many links has reduced impact on the importance of the pages it links to. In this paper we focus on asynchronous iterative schemes to compute PageRank over large sets of Web pages. The elimination of the synchronizing phases is expected to be advantageous on heterogeneous platforms. The motivation for a possible move to such large scale distributed platforms lies in the size of matrices representing Web structure. In orders of magnitude: $10^{10}$ pages with $10^{11}$ nonzero elements and $10^{12}$ bytes just to store a small percentage of the Web (the already crawled); distributed memory machines are necessary for such computations. The present research is part of our general objective, to explore the potential of asynchronous computational models as an underlying framework for very large scale computations over the Grid. The area of ``internet algorithmics'' appears to offer many occasions for computations of unprecedent dimensionality that would be good candidates for this framework.",
        "published": "2006-06-11T09:36:26Z",
        "link": "http://arxiv.org/abs/cs/0606047v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Complex Networks: New Concepts and Tools for Real-Time Imaging and   Vision",
        "authors": [
            "Luciano da Fontoura Costa"
        ],
        "summary": "This article discusses how concepts and methods of complex networks can be applied to real-time imaging and computer vision. After a brief introduction of complex networks basic concepts, their use as means to represent and characterize images, as well as for modeling visual saliency, are briefly described. The possibility to apply complex networks in order to model and simulate the performance of parallel and distributed computing systems for performance of visual methods is also proposed.",
        "published": "2006-06-13T12:53:45Z",
        "link": "http://arxiv.org/abs/cs/0606060v1",
        "categories": [
            "cs.CV",
            "cs.DC",
            "physics.soc-ph"
        ]
    },
    {
        "title": "NVision-PA: A Tool for Visual Analysis of Command Behavior Based on   Process Accounting Logs (with a Case Study in HPC Cluster Security)",
        "authors": [
            "Charis Ermopoulos",
            "William Yurcik"
        ],
        "summary": "In the UNIX/Linux environment the kernel can log every command process created by every user with process accounting. Thus process accounting logs have many potential uses, particularly the monitoring and forensic investigation of security events. Previous work successfully leveraged the use of process accounting logs to identify a difficult to detect and damaging intrusion against high performance computing (HPC) clusters, masquerade attacks, where intruders masquerade as legitimate users with purloined authentication credentials. While masqueraders on HPC clusters were found to be identifiable with a high accuracy (greater than 90%), this accuracy is still not high enough for HPC production environments where greater than 99% accuracy is needed.   This paper incrementally advances the goal of more accurately identifying masqueraders on HPC clusters by seeking to identify features within command sets that distinguish masqueraders. To accomplish this goal, we created NVision-PA, a software tool that produces text and graphic statistical summaries describing input processing accounting logs. We report NVision-PA results describing two different process accounting logs; one from Internet usage and one from HPC cluster usage. These results identify the distinguishing features of Internet users (as proxies for masqueraders) posing as clusters users. This research is both a promising next step toward creating a real-time masquerade detection sensor for production HPC clusters as well as providing another tool for system administrators to use for statistically monitoring and managing legitimate workloads (as indicated by command usage) in HPC environments.",
        "published": "2006-06-20T16:53:02Z",
        "link": "http://arxiv.org/abs/cs/0606089v1",
        "categories": [
            "cs.CR",
            "cs.DC"
        ]
    },
    {
        "title": "A verification algorithm for Declarative Concurrent Programming",
        "authors": [
            "Jean Krivine"
        ],
        "summary": "A verification method for distributed systems based on decoupling forward and backward behaviour is proposed. This method uses an event structure based algorithm that, given a CCS process, constructs its causal compression relative to a choice of observable actions. Verifying the original process equipped with distributed backtracking on non-observable actions, is equivalent to verifying its relative compression which in general is much smaller. We call this method Declarative Concurrent Programming (DCP). DCP technique compares well with direct bisimulation based methods. Benchmarks for the classic dining philosophers problem show that causal compression is rather efficient both time- and space-wise. State of the art verification tools can successfully handle more than 15 agents, whereas they can handle no more than 5 following the traditional direct method; an altogether spectacular improvement, since in this example the specification size is exponential in the number of agents.",
        "published": "2006-06-22T13:23:15Z",
        "link": "http://arxiv.org/abs/cs/0606095v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Comparison of Image Similarity Queries in P2P Systems",
        "authors": [
            "Wolfgang Mueller",
            "P. Oscar Boykin",
            "Nima Sarshar",
            "Vwani P. Roychowdhury"
        ],
        "summary": "Given some of the recent advances in Distributed Hash Table (DHT) based Peer-To-Peer (P2P) systems we ask the following questions: Are there applications where unstructured queries are still necessary (i.e., the underlying queries do not efficiently map onto any structured framework), and are there unstructured P2P systems that can deliver the high bandwidth and computing performance necessary to support such applications. Toward this end, we consider an image search application which supports queries based on image similarity metrics, such as color histogram intersection, and discuss why in this setting, standard DHT approaches are not directly applicable. We then study the feasibility of implementing such an image search system on two different unstructured P2P systems: power-law topology with percolation search, and an optimized super-node topology using structured broadcasts. We examine the average and maximum values for node bandwidth, storage and processing requirements in the percolation and super-node models, and show that current high-end computers and high-speed links have sufficient resources to enable deployments of large-scale complex image search systems.",
        "published": "2006-06-29T14:28:32Z",
        "link": "http://arxiv.org/abs/cs/0606122v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "A Novel Application of Lifting Scheme for Multiresolution Correlation of   Complex Radar Signals",
        "authors": [
            "Chinmoy Bhattacharya",
            "P. R. Mahapatra"
        ],
        "summary": "The lifting scheme of discrete wavelet transform (DWT) is now quite well established as an efficient technique for image compression, and has been incorporated into the JPEG2000 standards. However, the potential of the lifting scheme has not been exploited in the context of correlationbased processing, such as encountered in radar applications. This paper presents a complete and consistent framework for the application of DWT for correlation of complex signals. In particular, lifting scheme factorization of biorthogonal filterbanks is carried out in dual analysis basis spaces for multiresolution correlation of complex radar signals in the DWT domain only. A causal formulation of lifting for orthogonal filterbank is also developed. The resulting parallel algorithms and consequent saving of computational effort are briefly dealt with.",
        "published": "2006-07-01T16:58:49Z",
        "link": "http://arxiv.org/abs/cs/0607001v1",
        "categories": [
            "cs.DC",
            "cs.CC"
        ]
    },
    {
        "title": "The evolution of navigable small-world networks",
        "authors": [
            "Oskar Sandberg",
            "Ian Clarke"
        ],
        "summary": "Small-world networks, which combine randomized and structured elements, are seen as prevalent in nature. Several random graph models have been given for small-world networks, with one of the most fruitful, introduced by Jon Kleinberg, showing in which type of graphs it is possible to route, or navigate, between vertices with very little knowledge of the graph itself. Kleinberg's model is static, with random edges added to a fixed grid. In this paper we introduce, analyze and test a randomized algorithm which successively rewires a graph with every application. The resulting process gives a model for the evolution of small-world networks with properties similar to those studied by Kleinberg.",
        "published": "2006-07-07T13:21:09Z",
        "link": "http://arxiv.org/abs/cs/0607025v1",
        "categories": [
            "cs.DS",
            "cs.DC"
        ]
    },
    {
        "title": "A Quasi-Optimal Leader Election Algorithm in Radio Networks with   Log-Logarithmic Awake Time Slots",
        "authors": [
            "Christian Lavault",
            "Jean-François Marckert",
            "Vlady Ravelomanana"
        ],
        "summary": "Radio networks (RN) are distributed systems (\\textit{ad hoc networks}) consisting in $n \\ge 2$ radio stations. Assuming the number $n$ unknown, two distinct models of RN without collision detection (\\textit{no-CD}) are addressed: the model with \\textit{weak no-CD} RN and the one with \\textit{strong no-CD} RN. We design and analyze two distributed leader election protocols, each one running in each of the above two (no-CD RN) models, respectively. Both randomized protocols are shown to elect a leader within $\\BO(\\log{(n)})$ expected time, with no station being awake for more than $\\BO(\\log{\\log{(n)}})$ time slots (such algorithms are said to be \\textit{energy-efficient}). Therefore, a new class of efficient algorithms is set up that matchthe $\\Omega(\\log{(n)})$ time lower-bound established by Kushilevitz and Mansour.",
        "published": "2006-07-07T17:42:08Z",
        "link": "http://arxiv.org/abs/cs/0607028v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "A distributed approximation algorithm for the minimum degree minimum   weight spanning trees",
        "authors": [
            "Christian Lavault",
            "Mario Valencia-Pabon"
        ],
        "summary": "Fischer has shown how to compute a minimum weight spanning tree of degree at most $b \\Delta^* + \\lceil \\log\\_b n\\rceil$ in time $O(n^{4 + 1/\\ln b})$ for any constant $b > 1$, where $\\Delta^*$ is the value of an optimal solution and $n$ is the number of nodes in the network. In this paper, we propose a distributed version of Fischer's algorithm that requires messages and time complexity $O(n^{2 + 1/\\ln b})$, and O(n) space per node.",
        "published": "2006-07-08T08:42:59Z",
        "link": "http://arxiv.org/abs/cs/0607031v1",
        "categories": [
            "cs.DC",
            "cs.DM"
        ]
    },
    {
        "title": "Asymptotic Analysis of a Leader Election Algorithm",
        "authors": [
            "Christian Lavault",
            "Guy Louchard"
        ],
        "summary": "Itai and Rodeh showed that, on the average, the communication of a leader election algorithm takes no more than $LN$ bits, where $L \\simeq 2.441716$ and $N$ denotes the size of the ring. We give a precise asymptotic analysis of the average number of rounds M(n) required by the algorithm, proving for example that $\\dis M(\\infty) := \\lim\\_{n\\to \\infty} M(n) = 2.441715879...$, where $n$ is the number of starting candidates in the election. Accurate asymptotic expressions of the second moment $M^{(2)}(n)$ of the discrete random variable at hand, its probability distribution, and the generalization to all moments are given. Corresponding asymptotic expansions $(n\\to \\infty)$ are provided for sufficiently large $j$, where $j$ counts the number of rounds. Our numerical results show that all computations perfectly fit the observed values. Finally, we investigate the generalization to probability $t/n$, where $t$ is a non negative real parameter. The real function $\\dis M(\\infty,t) := \\lim\\_{n\\to \\infty} M(n,t)$ is shown to admit \\textit{one unique minimum} $M(\\infty,t^{*})$ on the real segment $(0,2)$. Furthermore, the variations of $M(\\infty,t)$ on thewhole real line are also studied in detail.",
        "published": "2006-07-08T08:46:32Z",
        "link": "http://arxiv.org/abs/cs/0607032v1",
        "categories": [
            "cs.DC",
            "cs.NA"
        ]
    },
    {
        "title": "Quasi-Optimal Leader Election Algorithms in Radio Networks with   Loglogarithmic Awake Time Slots",
        "authors": [
            "Christian Lavault",
            "Jean-François Marckert",
            "Vlady Ravelomanana"
        ],
        "summary": "A radio network (RN) is a distributed system consisting of $n$ radio stations. We design and analyze two distributed leader election protocols in RN where the number $n$ of radio stations is unknown. The first algorithm runs under the assumption of {\\it limited collision detection}, while the second assumes that {\\it no collision detection} is available. By ``limited collision detection'', we mean that if exactly one station sends (broadcasts) a message, then all stations (including the transmitter) that are listening at this moment receive the sent message. By contrast, the second no-collision-detection algorithm assumes that a station cannot simultaneously send and listen signals. Moreover, both protocols allow the stations to keep asleep as long as possible, thus minimizing their awake time slots (such algorithms are called {\\it energy-efficient}). Both randomized protocols in RN areshown to elect a leader in $O(\\log{(n)})$ expected time, with no station being awake for more than $O(\\log{\\log{(n)}})$ time slots. Therefore, a new class of efficient algorithms is set up that match the $\\Omega(\\log{(n)})$ time lower-bound established by Kushilevitz and Mansour.",
        "published": "2006-07-08T12:04:24Z",
        "link": "http://arxiv.org/abs/cs/0607034v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "PALS: Efficient Or-Parallelism on Beowulf Clusters",
        "authors": [
            "Enrico Pontelli",
            "Karen Villaverde",
            "Hai-Feng Guo",
            "Gopal Gupta"
        ],
        "summary": "This paper describes the development of the PALS system, an implementation of Prolog capable of efficiently exploiting or-parallelism on distributed-memory platforms--specifically Beowulf clusters. PALS makes use of a novel technique, called incremental stack-splitting. The technique proposed builds on the stack-splitting approach, previously described by the authors and experimentally validated on shared-memory systems, which in turn is an evolution of the stack-copying method used in a variety of parallel logic and constraint systems--e.g., MUSE, YAP, and Penny. The PALS system is the first distributed or-parallel implementation of Prolog based on the stack-splitting method ever realized. The results presented confirm the superiority of this method as a simple yet effective technique to transition from shared-memory to distributed-memory systems. PALS extends stack-splitting by combining it with incremental copying; the paper provides a description of the implementation of PALS, including details of how distributed scheduling is handled. We also investigate methodologies to effectively support order-sensitive predicates (e.g., side-effects) in the context of the stack-splitting scheme. Experimental results obtained from running PALS on both Shared Memory and Beowulf systems are presented and analyzed.",
        "published": "2006-07-09T19:56:05Z",
        "link": "http://arxiv.org/abs/cs/0607040v1",
        "categories": [
            "cs.DC",
            "cs.PL"
        ]
    },
    {
        "title": "Methods for Partitioning Data to Improve Parallel Execution Time for   Sorting on Heterogeneous Clusters",
        "authors": [
            "Christophe Cérin",
            "Jean-Christophe Dubacq",
            "Jean-Louis Roch",
            "the SafeScale Collaboration"
        ],
        "summary": "The aim of the paper is to introduce general techniques in order to optimize the parallel execution time of sorting on a distributed architectures with processors of various speeds. Such an application requires a partitioning step. For uniformly related processors (processors speeds are related by a constant factor), we develop a constant time technique for mastering processor load and execution time in an heterogeneous environment and also a technique to deal with unknown cost functions. For non uniformly related processors, we use a technique based on dynamic programming. Most of the time, the solutions are in O(p) (p is the number of processors), independent of the problem size n. Consequently, there is a small overhead regarding the problem we deal with but it is inherently limited by the knowing of time complexity of the portion of code following the partitioning.",
        "published": "2006-07-10T19:46:47Z",
        "link": "http://arxiv.org/abs/cs/0607041v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "F.2.2"
        ]
    },
    {
        "title": "On Some Peculiarities of Dynamic Switch between Component   Implementations in an Autonomic Computing System",
        "authors": [
            "Igor Mackarov"
        ],
        "summary": "Behavior of the delta algorithm of autonomic switch between two component implementations is considered on several examples of a client-server systems involving, in particular, periodic change of intensities of requests for the component. It is shown that in the cases of some specific combinations of elementary requests costs, the number of clients in the system, the number of requests per unit of time, and the cost of switch between the implementations, the algorithm may reveal behavior that is rather far from the desired. A sufficient criterion of a success of the algorithm is proposed based on the analysis of the accumulated implementations costs difference as a function of time. Suggestions are pointed out of practical evaluation of the algorithm functioning regarding the observations made in this paper.",
        "published": "2006-07-12T11:09:52Z",
        "link": "http://arxiv.org/abs/cs/0607061v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.NA"
        ]
    },
    {
        "title": "Web-Based Enterprise Information Systems Development: The Integrated   Methodology",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper considers software development issues for large-scale enterprise information systems (IS) with databases (DB) in global heterogeneous distributed computational environment. Due to high IT development rates, the present-day society has accumulated and rapidly increases an extremely huge data burden. Manipulating with such huge data arrays becomes an essential problem, particularly due to their global distribution, heterogeneous and weak-structured character. The conceptual approach to integrated Internet-based IS design, development and implementation is presented, including formal models, software development methodology and original software development tools for visual problem-oriented development and content management. IS implementation results proved shortening terms and reducing costs of implementation compared to commercial software available.",
        "published": "2006-07-27T07:47:35Z",
        "link": "http://arxiv.org/abs/cs/0607119v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Enterprise Content Management: Theory and Engineering for Entire   Lifecycle Support",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper considers enterprise content management (ECM) issues in global heterogeneous distributed computational environment. Present-day enterprises have accumulated a huge data burden. Manipulating with such a bulk becomes an essential problem, particularly due to its global distribution, heterogeneous and weak-structured character. The conceptual approach to integrated ECM lifecycle support is presented, including overview of formal models, software development methodology and innovative software development tools. Implementation results proved shortening terms and reducing costs of implementation compared to commercial software available.",
        "published": "2006-07-27T10:32:27Z",
        "link": "http://arxiv.org/abs/cs/0607122v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Enterprise Portal Development Tools: Problem-Oriented Approach",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper deals with problem-oriented visual information system (IS) engineering for enterprise Internet-based applications, which is a vital part of the whole development process. The suggested approach is based on semantic network theory and a novel ConceptModeller CASE tool.",
        "published": "2006-07-27T11:24:45Z",
        "link": "http://arxiv.org/abs/cs/0607123v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "ConceptModeller: a Problem-Oriented Visual SDK for Globally Distributed   Enterprise Systems",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "The paper describes problem-oriented approach to software development. The approach is a part of the original integrated methodology of enterprise Internet-based software design and implementation. All aspects of software development, from theory to implementation, are covered.",
        "published": "2006-07-27T11:35:19Z",
        "link": "http://arxiv.org/abs/cs/0607124v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Enterprise Portal: from Model to Implementation",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Portal technology can significantly improve the entire corporate information infrastructure. The approach proposed is based on rigorous and consistent (meta)data model and provides for efficient and accurate front-end integration of heterogeneous corporate applications including enterprise resource planning (ERP) systems, multimedia data warehouses and proprietary content databases. The methodology proposed embraces entire software lifecycle; it is illustrated by an enterprise-level Intranet portal implementation.",
        "published": "2006-07-27T12:12:01Z",
        "link": "http://arxiv.org/abs/cs/0607125v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Abstract Machine as a Model of Content Management Information System",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Enterprise content management is an urgent issue of current scientific and practical activities in software design and implementation. However, papers known as yet give insufficient coverage of theoretical background of the software in question. The paper gives an attempt of building a state-based model of content management. In accordance with the theoretical principles outlined, a content management information system (CMIS) has been implemented in a large international oil-and-gas group of companies.",
        "published": "2006-07-27T12:40:21Z",
        "link": "http://arxiv.org/abs/cs/0607126v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Integrating Enterprise Software Applications with Web Portal Technology",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Web-portal based approach can significantly improve the entire corporate information infrastructure. The approach proposed provides for rapid and accurate front-end integration of heterogeneous corporate applications including enterprise resource planning (ERP) systems. Human resources ERP component and multimedia data warehouse implementations are discussed as essential instances.",
        "published": "2006-07-27T12:49:54Z",
        "link": "http://arxiv.org/abs/cs/0607127v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "The Integrated Approach to ERP: Embracing the Web",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Integrated approach to enterprise resource planning (ERP) software design and implementation can significantly improve the entire corporate information infrastructure and it helps to benefit from power of Internet services. The approach proposed provides for corporate Web portal integrity, consistency, urgency and front-end data processing. Human resources (HR) ERP component implementation is discussed as an essential instance.",
        "published": "2006-07-27T12:58:51Z",
        "link": "http://arxiv.org/abs/cs/0607128v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "Enterprise Resource Planning Systems: the Integrated Approach",
        "authors": [
            "Sergey V. Zykov"
        ],
        "summary": "Enterprise resource planning (ERP) systems enjoy an increasingly wide coverage. However, no truly integrate solution has been proposed as yet. ERP classification is given. Recent trends in commercial systems are analyzed on the basis of human resources (HR) management software. An innovative \"straight through\" design and implementation process of an open, secure, and scalable integrated event-driven enterprise solution is suggested. Implementation results are presented.",
        "published": "2006-07-27T13:07:42Z",
        "link": "http://arxiv.org/abs/cs/0607129v1",
        "categories": [
            "cs.SE",
            "cs.DC"
        ]
    },
    {
        "title": "LiveWN, cpu scavenging in the Grid Era",
        "authors": [
            "Giannis Kouretis",
            "Fotis Georgatos"
        ],
        "summary": "The goal of this research is to introduce an easy and versatile way to provide and use Grid resources without the need of any OS installation or middleware configuration. At the same time we provide an excellent training tool for newer Grid users and people that want to experiment, without enforcing any installation. We have been testing it thoroughly under different circumstances with firm success.",
        "published": "2006-08-08T15:28:53Z",
        "link": "http://arxiv.org/abs/cs/0608045v2",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "From Grid Middleware to a Grid Operating System",
        "authors": [
            "Arshad Ali",
            "Richard McClatchey",
            "Ashiq Anjum",
            "Irfan Habib",
            "Kamran Soomro",
            "Mohammed Asif",
            "Ali Adil",
            "Athar Mohsin"
        ],
        "summary": "Grid computing has made substantial advances during the last decade. Grid middleware such as Globus has contributed greatly in making this possible. There are, however, significant barriers to the adoption of Grid computing in other fields, most notably day-to-day user computing environments. We will demonstrate in this paper that this is primarily due to the limitations of the existing Grid middleware which does not take into account the needs of everyday scientific and business users. In this paper we will formally advocate a Grid Operating System and propose an architecture to migrate Grid computing into a Grid operating system which we believe would help remove most of the technical barriers to the adoption of Grid computing and make it relevant to the day-to-day user. We believe this proposed transition to a Grid operating system will drive more pervasive Grid computing research and application development and deployment in future.",
        "published": "2006-08-08T17:41:26Z",
        "link": "http://arxiv.org/abs/cs/0608046v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Lessons Learned from MammoGrid for Integrated Biomedical Solutions",
        "authors": [
            "R. H. McClatchey",
            "D. Manset",
            "A. E. Solomonides"
        ],
        "summary": "This paper presents an overview of the MammoGrid project and some of its achievements. In terms of the global grid project, and European research in particular, the project has successfully demonstrated the capacity of a grid-based system to support effective collaboration between physicians, including handling and querying image databases, as well as using grid services, such as image standardization and Computer-Aided Detection (CADe) of suspect or indicative features. In terms of scientific results, in radiology, there have been significant epidemiological findings in the assessment of breast density as a risk factor, but the results for CADe are less clear-cut. Finally, the foundations of a technology transfer process to establish a working MammoGrid plus system in Spain through the company Maat GKnowledge and the collaboration of CIEMAT and hospitals in Extremadura.",
        "published": "2006-08-08T17:49:28Z",
        "link": "http://arxiv.org/abs/cs/0608047v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Bulk Scheduling with the DIANA Scheduler",
        "authors": [
            "Ashiq Anjum",
            "Richard McClatchey",
            "Arshad Ali",
            "Ian Willers"
        ],
        "summary": "Results from the research and development of a Data Intensive and Network Aware (DIANA) scheduling engine, to be used primarily for data intensive sciences such as physics analysis, are described. In Grid analyses, tasks can involve thousands of computing, data handling, and network resources. The central problem in the scheduling of these resources is the coordinated management of computation and data at multiple locations and not just data replication or movement. However, this can prove to be a rather costly operation and efficient sing can be a challenge if compute and data resources are mapped without considering network costs. We have implemented an adaptive algorithm within the so-called DIANA Scheduler which takes into account data location and size, network performance and computation capability in order to enable efficient global scheduling. DIANA is a performance-aware and economy-guided Meta Scheduler. It iteratively allocates each job to the site that is most likely to produce the best performance as well as optimizing the global queue for any remaining jobs. Therefore it is equally suitable whether a single job is being submitted or bulk scheduling is being performed. Results indicate that considerable performance improvements can be gained by adopting the DIANA scheduling approach.",
        "published": "2006-08-08T17:53:15Z",
        "link": "http://arxiv.org/abs/cs/0608048v1",
        "categories": [
            "cs.DC",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Concurrent Processing Memory",
        "authors": [
            "Chengpu Wang"
        ],
        "summary": "A theoretical memory with limited processing power and internal connectivity at each element is proposed. This memory carries out parallel processing within itself to solve generic array problems. The applicability of this in-memory finest-grain massive SIMD approach is studied in some details. For an array of N items, it reduces the total instruction cycle count of universal operations such as insertion/deletion and match finding to ~ 1, local operations such as filtering and template matching to ~ local operation size, and global operations such as sum, finding global limit and sorting to ~\\sqroot{N} instruction cycles. It eliminates most streaming activities for data processing purpose on the system bus. Yet it remains general-purposed, easy to use, pin compatible with conventional memory, and practical for implementation.",
        "published": "2006-08-15T01:42:49Z",
        "link": "http://arxiv.org/abs/cs/0608061v5",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.PF"
        ]
    },
    {
        "title": "The Aligned-Coordinated Geographical Routing for Multihop Wireless   Networks",
        "authors": [
            "Ke Liu",
            "Nael Abu-Ghazaleh"
        ],
        "summary": "The stateless, low overhead and distributed nature of the Geographic routing protocols attract a lot of research attentions recently. Since the geographic routing would face void problems, leading to complementary routing such as perimeter routing which degrades the performance of geographic routing, most research works are focus on optimizing this complementary part of geographic routing to improve it. The greedy forwarding part of geographic routing provides an optimal routing performance in terms of path stretch. If the geographic routing could adapt the greedy forwarding more, its performance would be enhanced much more than to optimize the complementary routing such as perimeter routings. Our work is the first time to do so. The aligned physical coordinate is used to do the greedy forwarding routing decision which would lead more greedy forwarding. We evaluate our design to most geographic routing protocols, showing it helps much and maintain the stateless nature of geographic routing.",
        "published": "2006-08-16T17:06:37Z",
        "link": "http://arxiv.org/abs/cs/0608068v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "JiTS: Just-in-Time Scheduling for Real-Time Sensor Data Dissemination",
        "authors": [
            "Ke Liu",
            "Nael Abu-Ghazaleh",
            "Kyoung-Don Kang"
        ],
        "summary": "We consider the problem of real-time data dissemination in wireless sensor networks, in which data are associated with deadlines and it is desired for data to reach the sink(s) by their deadlines. To this end, existing real-time data dissemination work have developed packet scheduling schemes that prioritize packets according to their deadlines. In this paper, we first demonstrate that not only the scheduling discipline but also the routing protocol has a significant impact on the success of real-time sensor data dissemination. We show that the shortest path routing using the minimum number of hops leads to considerably better performance than Geographical Forwarding, which has often been used in existing real-time data dissemination work. We also observe that packet prioritization by itself is not enough for real-time data dissemination, since many high priority packets may simultaneously contend for network resources, deteriorating the network performance. Instead, real-time packets could be judiciously delayed to avoid severe contention as long as their deadlines can be met. Based on this observation, we propose a Just-in-Time Scheduling (JiTS) algorithm for scheduling data transmissions to alleviate the shortcomings of the existing solutions. We explore several policies for non-uniformly delaying data at different intermediate nodes to account for the higher expected contention as the packet gets closer to the sink(s). By an extensive simulation study, we demonstrate that JiTS can significantly improve the deadline miss ratio and packet drop ratio compared to existing approaches in various situations. Notably, JiTS improves the performance requiring neither lower layer support nor synchronization among the sensor nodes.",
        "published": "2006-08-16T17:21:40Z",
        "link": "http://arxiv.org/abs/cs/0608069v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "The computational power of population protocols",
        "authors": [
            "Dana Angluin",
            "James Aspnes",
            "David Eisenstat",
            "Eric Ruppert"
        ],
        "summary": "We consider the model of population protocols introduced by Angluin et al., in which anonymous finite-state agents stably compute a predicate of the multiset of their inputs via two-way interactions in the all-pairs family of communication networks. We prove that all predicates stably computable in this model (and certain generalizations of it) are semilinear, answering a central open question about the power of the model. Removing the assumption of two-way interaction, we also consider several variants of the model in which agents communicate by anonymous message-passing where the recipient of each message is chosen by an adversary and the sender is not identified to the recipient. These one-way models are distinguished by whether messages are delivered immediately or after a delay, whether a sender can record that it has sent a message, and whether a recipient can queue incoming messages, refusing to accept new messages until it has had a chance to send out messages of its own. We characterize the classes of predicates stably computable in each of these one-way models using natural subclasses of the semilinear predicates.",
        "published": "2006-08-21T23:17:55Z",
        "link": "http://arxiv.org/abs/cs/0608084v1",
        "categories": [
            "cs.CC",
            "cs.DC"
        ]
    },
    {
        "title": "Self-Stabilizing Byzantine Pulse Synchronization",
        "authors": [
            "Ariel Daliot",
            "Danny Dolev"
        ],
        "summary": "The ``Pulse Synchronization'' problem can be loosely described as targeting to invoke a recurring distributed event as simultaneously as possible at the different nodes and with a frequency that is as regular as possible. This target becomes surprisingly subtle and difficult to achieve when facing both transient and permanent failures. In this paper we present an algorithm for pulse synchronization that self-stabilizes while at the same time tolerating a permanent presence of Byzantine faults. The Byzantine nodes might incessantly try to de-synchronize the correct nodes. Transient failures might throw the system into an arbitrary state in which correct nodes have no common notion what-so-ever, such as time or round numbers, and can thus not infer anything from their own local states upon the state of other correct nodes. The presented algorithm grants nodes the ability to infer that eventually all correct nodes will invoke their pulses within a very short time interval of each other and will do so regularly.   Pulse synchronization has previously been shown to be a powerful tool for designing general self-stabilizing Byzantine algorithms and is hitherto the only method that provides for the general design of efficient practical protocols in the confluence of these two fault models. The difficulty, in general, to design any algorithm in this fault model may be indicated by the remarkably few algorithms resilient to both fault models. The few published self-stabilizing Byzantine algorithms are typically complicated and sometimes converge from an arbitrary initial state only after exponential or super exponential time.",
        "published": "2006-08-24T16:07:07Z",
        "link": "http://arxiv.org/abs/cs/0608092v2",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.5"
        ]
    },
    {
        "title": "Linear-time Self-stabilizing Byzantine Clock Synchronization",
        "authors": [
            "Ariel Daliot",
            "Danny Dolev",
            "Hanna Parnas"
        ],
        "summary": "Clock synchronization is a very fundamental task in distributed system. It thus makes sense to require an underlying clock synchronization mechanism to be highly fault-tolerant. A self-stabilizing algorithm seeks to attain synchronization once lost; a Byzantine algorithm assumes synchronization is never lost and focuses on containing the influence of the permanent presence of faulty nodes. There are efficient self-stabilizing solutions for clock synchronization as well as efficient solutions that are resilient to Byzantine faults. In contrast, to the best of our knowledge there is no practical solution that is self-stabilizing while tolerating the permanent presence of Byzantine nodes. We present the first linear-time self-stabilizing Byzantine clock synchronization algorithm. Our deterministic clock synchronization algorithm is based on the observation that all clock synchronization algorithms require events for exchanging clock values and re-synchronizing the clocks to within safe bounds. These events usually need to happen synchronously at the different nodes. In classic Byzantine algorithms this is fulfilled or aided by having the clocks initially close to each other and thus the actual clock values can be used for synchronizing the events. This implies that clock values cannot differ arbitrarily, which necessarily renders these solutions to be non-stabilizing. Our scheme suggests using an underlying distributed pulse synchronization module that is uncorrelated to the clock values.",
        "published": "2006-08-25T03:11:28Z",
        "link": "http://arxiv.org/abs/cs/0608096v1",
        "categories": [
            "cs.DC",
            "C.1.4; C.2.4; D.4.5"
        ]
    },
    {
        "title": "Entity Based Peer-to-Peer in a Data Grid Environment",
        "authors": [
            "B. Hudzia",
            "L. McDermott",
            "T. N. Illahi",
            "M-T. Kechadi"
        ],
        "summary": "During the last decade there has been a huge interest in Grid technologies, and numerous Grid projects have been initiated with various visions of the Grid. While all these visions have the same goal of resource sharing, they differ in the functionality that a Grid supports, the grid characterisation, programming environments, etc. In this paper we present a new Grid system dedicated to deal with data issues, called DGET (Data Grid Environment and Tools). DGET is characterized by its peer-to-peer communication system and entity-based architecture, therefore, taking advantage of the main functionality of both systems; P2P and Grid. DGET is currently under development and a prototype implementing the main components is in its first phase of testing. In this paper we limit our description to the system architectural features and to the main differences with other systems.",
        "published": "2006-08-29T11:58:50Z",
        "link": "http://arxiv.org/abs/cs/0608112v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Java Based Architecture of P2P-Grid Middleware",
        "authors": [
            "B. Hudzia",
            "T. N. Ellahi",
            "L. McDermott",
            "T. Kechadi"
        ],
        "summary": "During the last decade there has been a huge interest in Grid technologies, and numerous Grid projects have been initiated with various visions of the Grid. While all these visions have the same goal of resource sharing, they differ in the functionality that a Grid supports, characterization, programming environments, etc. We present a new Grid system dedicated to dealing with data issues, called DGET (Data Grid Environment and Tools). DGET is characterized by its peerto- peer communication system and entity-based architecture, therefore, taking advantage of the main functionality of both systems; P2P and Grid. DGET is currently under development and a prototype implementing the main components is in its first phase of testing. In this paper we limit our description to the system architectural features and to the main differences with other systems. Keywords: Grid Computing, Peer to Peer, Peer to Peer Grid",
        "published": "2006-08-29T13:11:05Z",
        "link": "http://arxiv.org/abs/cs/0608113v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Reliable multicast fault tolerant MPI in the Grid environment",
        "authors": [
            "Benoit Hudzia",
            "Serge Petiton"
        ],
        "summary": "Grid environments have recently been developed with low stretch and overheads that increase with the logarithm of the number of nodes in the system. Getting and sending data to/from a large numbers of nodes is gaining importance due to an increasing number of independent data providers and the heterogeneity of the network/Grid. One of the key challenges is to achieve a balance between low bandwidth consumption and good reliability. In this paper we present an implementation of a reliable multicast protocol over a fault tolerant MPI: MPICHV2. It can provide one way to solve the problem of transferring large chunks of data between applications running on a grid with limited network links. We first show that we can achieve similar performance as the MPICH-P4 implementation by using multicast with data compression in a cluster. Next, we provide a theoretical cluster organization and GRID network architecture to harness the performance provided by using multicast. Finally, we present the conclusion and future work.",
        "published": "2006-08-29T13:14:29Z",
        "link": "http://arxiv.org/abs/cs/0608114v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Transparent Migration of Multi-Threaded Applications on a Java Based   Grid",
        "authors": [
            "T. N. Ellahi",
            "B. Hudzia",
            "L. McDermott",
            "T. Kechadi"
        ],
        "summary": "Grid computing has enabled pooling a very large number of heterogeneous resource administered by different security domains. Applications are dynamically deployed on the resources available at the time. Dynamic nature of the resources and applications requirements makes needs the grid middleware to support the ability of migrating a running application to a different resource. Especially, Grid applications are typically long running and thus stoping them and starting them from scratch is not a feasible option. This paper presents an overview of migration support in a java based grid middleware called DGET. Migration support in DGET includes multi-threaded migration and asynchronous migration as well.",
        "published": "2006-08-29T13:29:29Z",
        "link": "http://arxiv.org/abs/cs/0608116v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "TreeP: A Tree-Based P2P Network Architecture",
        "authors": [
            "B. Hudzia",
            "M-T. Kechadi",
            "A. Ottewill"
        ],
        "summary": "In this paper we proposed a hierarchical P2P network based on a dynamic partitioning on a 1-D space. This hierarchy is created and maintained dynamically and provides a gridmiddleware (like DGET) a P2P basic functionality for resource discovery and load-balancing.This network architecture is called TreeP (Tree based P2P network architecture) and is based on atessellation of a 1-D space. We show that this topology exploits in an efficient way theheterogeneity feature of the network while limiting the overhead introduced by the overlaymaintenance. Experimental results show that this topology is highly resilient to a large number ofnetwork failures.",
        "published": "2006-08-29T15:08:42Z",
        "link": "http://arxiv.org/abs/cs/0608118v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Context for models of concurrency",
        "authors": [
            "Peter Bubenik"
        ],
        "summary": "Many categories have been used to model concurrency. Using any of these, the challenge is to reduce a given model to a smaller representation which nevertheless preserves the relevant computer-scientific information. That is, one wants to replace a given model with a simpler model with the same directed homotopy-type. Unfortunately, the obvious definition of directed homotopy equivalence is too coarse. This paper introduces the notion of context to refine this definition.",
        "published": "2006-08-29T20:18:44Z",
        "link": "http://arxiv.org/abs/math/0608733v1",
        "categories": [
            "math.AT",
            "cs.DC"
        ]
    },
    {
        "title": "FOSS-Based Grid Computing",
        "authors": [
            "A. Mani"
        ],
        "summary": "In this expository paper we will be primarily concerned with core aspects of Grids and Grid computing using free and open-source software with some emphasis on utility computing. It is based on a technical report entitled 'Grid-Computing Using GNU/Linux' by the present author. This article was written in 2006 and should be of historical interest.",
        "published": "2006-08-30T18:59:33Z",
        "link": "http://arxiv.org/abs/cs/0608122v4",
        "categories": [
            "cs.DC",
            "cs.PL",
            "C.2.4; D.1.3"
        ]
    },
    {
        "title": "A Case for Peering of Content Delivery Networks",
        "authors": [
            "Rajkumar Buyya",
            "Al-Mukaddim Khan Pathan",
            "James Broberg",
            "Zahir Tari"
        ],
        "summary": "The proliferation of Content Delivery Networks (CDN) reveals that existing content networks are owned and operated by individual companies. As a consequence, closed delivery networks are evolved which do not cooperate with other CDNs and in practice, islands of CDNs are formed. Moreover, the logical separation between contents and services in this context results in two content networking domains. But present trends in content networks and content networking capabilities give rise to the interest in interconnecting content networks. Finding ways for distinct content networks to coordinate and cooperate with other content networks is necessary for better overall service. In addition to that, meeting the QoS requirements of users according to the negotiated Service Level Agreements between the user and the content network is a burning issue in this perspective. In this article, we present an open, scalable and Service-Oriented Architecture based system to assist the creation of open Content and Service Delivery Networks (CSDN) that scale and support sharing of resources with other CSDNs.",
        "published": "2006-09-06T23:41:32Z",
        "link": "http://arxiv.org/abs/cs/0609027v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Rational Secret Sharing and Multiparty Computation: Extended Abstract",
        "authors": [
            "Joseph Y. Halpern",
            "Vanessa Teague"
        ],
        "summary": "We consider the problems of secret sharing and multiparty computation, assuming that agents prefer to get the secret (resp., function value) to not getting it, and secondarily, prefer that as few as possible of the other agents get it. We show that, under these assumptions, neither secret sharing nor multiparty function computation is possible using a mechanism that has a fixed running time. However, we show that both are possible using randomized mechanisms with constant expected running time.",
        "published": "2006-09-07T21:48:23Z",
        "link": "http://arxiv.org/abs/cs/0609035v1",
        "categories": [
            "cs.GT",
            "cs.CR",
            "cs.DC",
            "F.m"
        ]
    },
    {
        "title": "A Non-anchored Unified Naming System for Ad Hoc Computing Environments",
        "authors": [
            "Yoo Chul Chung",
            "Dongman Lee"
        ],
        "summary": "A ubiquitous computing environment consists of many resources that need to be identified by users and applications. Users and developers require some way to identify resources by human readable names. In addition, ubiquitous computing environments impose additional requirements such as the ability to work well with ad hoc situations and the provision of names that depend on context.   The Non-anchored Unified Naming (NUN) system was designed to satisfy these requirements. It is based on relative naming among resources and provides the ability to name arbitrary types of resources. By having resources themselves take part in naming, resources are able to able contribute their specialized knowledge into the name resolution process, making context-dependent mapping of names to resources possible. The ease of which new resource types can be added makes it simple to incorporate new types of contextual information within names.   In this paper, we describe the naming system and evaluate its use.",
        "published": "2006-09-13T12:52:45Z",
        "link": "http://arxiv.org/abs/cs/0609074v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.NI"
        ]
    },
    {
        "title": "Random numbers for large scale distributed Monte Carlo simulations",
        "authors": [
            "Heiko Bauke",
            "Stephan Mertens"
        ],
        "summary": "Monte Carlo simulations are one of the major tools in statistical physics, complex system science, and other fields, and an increasing number of these simulations is run on distributed systems like clusters or grids. This raises the issue of generating random numbers in a parallel, distributed environment. In this contribution we demonstrate that multiple linear recurrences in finite fields are an ideal method to produce high quality pseudorandom numbers in sequential and parallel algorithms. Their known weakness (failure of sampling points in high dimensions) can be overcome by an appropriate delinearization that preserves all desirable properties of the underlying linear sequence.",
        "published": "2006-09-22T17:05:53Z",
        "link": "http://arxiv.org/abs/cond-mat/0609584v2",
        "categories": [
            "cond-mat.other",
            "cs.DC"
        ]
    },
    {
        "title": "Labeling Schemes with Queries",
        "authors": [
            "Amos Korman",
            "Shay Kutten"
        ],
        "summary": "We study the question of ``how robust are the known lower bounds of labeling schemes when one increases the number of consulted labels''. Let $f$ be a function on pairs of vertices. An $f$-labeling scheme for a family of graphs $\\cF$ labels the vertices of all graphs in $\\cF$ such that for every graph $G\\in\\cF$ and every two vertices $u,v\\in G$, the value $f(u,v)$ can be inferred by merely inspecting the labels of $u$ and $v$.   This paper introduces a natural generalization: the notion of $f$-labeling schemes with queries, in which the value $f(u,v)$ can be inferred by inspecting not only the labels of $u$ and $v$ but possibly the labels of some additional vertices. We show that inspecting the label of a single additional vertex (one {\\em query}) enables us to reduce the label size of many labeling schemes significantly.",
        "published": "2006-09-29T12:31:35Z",
        "link": "http://arxiv.org/abs/cs/0609163v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Characterizing Solution Concepts in Games Using Knowledge-Based Programs",
        "authors": [
            "Joseph Y. Halpern",
            "Yoram Moses"
        ],
        "summary": "We show how solution concepts in games such as Nash equilibrium, correlated equilibrium, rationalizability, and sequential equilibrium can be given a uniform definition in terms of \\emph{knowledge-based programs}. Intuitively, all solution concepts are implementations of two knowledge-based programs, one appropriate for games represented in normal form, the other for games represented in extensive form. These knowledge-based programs can be viewed as embodying rationality. The representation works even if (a) information sets do not capture an agent's knowledge, (b) uncertainty is not represented by probability, or (c) the underlying game is not common knowledge.",
        "published": "2006-10-16T13:00:33Z",
        "link": "http://arxiv.org/abs/cs/0610098v1",
        "categories": [
            "cs.GT",
            "cs.DC",
            "cs.MA"
        ]
    },
    {
        "title": "Cellular Computing and Least Squares for partial differential problems   parallel solving",
        "authors": [
            "Nicolas Fressengeas",
            "Hervé Frezza-Buet"
        ],
        "summary": "This paper shows how partial differential problems can be solved thanks to cellular computing and an adaptation of the Least Squares Finite Elements Method. As cellular computing can be implemented on distributed parallel architectures, this method allows the distribution of a resource demanding differential problem over a computer network.",
        "published": "2006-10-17T11:51:30Z",
        "link": "http://arxiv.org/abs/math-ph/0610037v8",
        "categories": [
            "math-ph",
            "cs.DC",
            "math.AP",
            "math.MP"
        ]
    },
    {
        "title": "Classdesc and Graphcode: support for scientific programming in C++",
        "authors": [
            "Russell K. Standish",
            "Duraid Madina"
        ],
        "summary": "Object-oriented programming languages such as Java and Objective C have become popular for implementing agent-based and other object-based simulations since objects in those languages can {\\em reflect} (i.e. make runtime queries of an object's structure). This allows, for example, a fairly trivial {\\em serialisation} routine (conversion of an object into a binary representation that can be stored or passed over a network) to be written. However C++ does not offer this ability, as type information is thrown away at compile time. Yet C++ is often a preferred development environment, whether for performance reasons or for its expressive features such as operator overloading.   In scientific coding, changes to a model's codes takes place constantly, as the model is refined, and different phenomena are studied. Yet traditionally, facilities such as checkpointing, routines for initialising model parameters and analysis of model output depend on the underlying model remaining static, otherwise each time a model is modified, a whole slew of supporting routines needs to be changed to reflect the new data structures. Reflection offers the advantage of the simulation framework adapting to the underlying model without programmer intervention, reducing the effort of modifying the model.   In this paper, we present the {\\em Classdesc} system which brings many of the benefits of object reflection to C++, {\\em ClassdescMP} which dramatically simplifies coding of MPI based parallel programs and {\\em   Graphcode} a general purpose data parallel programming environment.",
        "published": "2006-10-20T05:11:21Z",
        "link": "http://arxiv.org/abs/cs/0610120v2",
        "categories": [
            "cs.MS",
            "cs.CE",
            "cs.DC",
            "D.1.3; D.1.5; D.2.12; D.3.4"
        ]
    },
    {
        "title": "Scheduling and data redistribution strategies on star platforms",
        "authors": [
            "Loris Marchal",
            "Veronika Rehn",
            "Yves Robert",
            "Frédéric Vivien"
        ],
        "summary": "In this work we are interested in the problem of scheduling and redistributing data on master-slave platforms. We consider the case were the workers possess initial loads, some of which having to be redistributed in order to balance their completion times. We examine two different scenarios. The first model assumes that the data consists of independent and identical tasks. We prove the NP-completeness in the strong sense for the general case, and we present two optimal algorithms for special platform types. Furthermore we propose three heuristics for the general case. Simulations consolidate the theoretical results. The second data model is based on Divisible Load Theory. This problem can be solved in polynomial time by a combination of linear programming and simple analytical manipulations.",
        "published": "2006-10-23T07:45:01Z",
        "link": "http://arxiv.org/abs/cs/0610131v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Concurrent Calculus with Atomic Transactions",
        "authors": [
            "Lucia Acciai",
            "Michele Boreale",
            "Silvano Dal Zilio"
        ],
        "summary": "The Software Transactional Memory (STM) model is an original approach for controlling concurrent accesses to ressources without the need for explicit lock-based synchronization mechanisms. A key feature of STM is to provide a way to group sequences of read and write actions inside atomic blocks, similar to database transactions, whose whole effect should occur atomically. In this paper, we investigate STM from a process algebra perspective and define an extension of asynchronous CCS with atomic blocks of actions. Our goal is not only to set a formal ground for reasoning on STM implementations but also to understand how this model fits with other concurrency control mechanisms. We also view this calculus as a test bed for extending process calculi with atomic transactions. This is an interesting direction for investigation since, for the most part, actual works that mix transactions with process calculi consider compensating transactions, a model that lacks all the well-known ACID properties. We show that the addition of atomic transactions results in a very expressive calculus, enough to easily encode other concurrent primitives such as guarded choice and multiset-synchronization (\\`{a} la join-calculus). The correctness of our encodings is proved using a suitable notion of bisimulation equivalence. The equivalence is then applied to prove interesting ``laws of transactions'' and to obtain a simple normal form for transactions.",
        "published": "2006-10-24T08:26:04Z",
        "link": "http://arxiv.org/abs/cs/0610137v1",
        "categories": [
            "cs.LO",
            "cs.DC"
        ]
    },
    {
        "title": "A Taxonomy of Peer-to-Peer Based Complex Queries: a Grid perspective",
        "authors": [
            "Rajiv Ranjan",
            "Aaron Harwood",
            "Rajkumar Buyya"
        ],
        "summary": "Grid superscheduling requires support for efficient and scalable discovery of resources. Resource discovery activities involve searching for the appropriate resource types that match the user's job requirements. To accomplish this goal, a resource discovery system that supports the desired look-up operation is mandatory. Various kinds of solutions to this problem have been suggested, including the centralised and hierarchical information server approach. However, both of these approaches have serious limitations in regards to scalability, fault-tolerance and network congestion. To overcome these limitations, organising resource information using Peer-to-Peer (P2P) network model has been proposed. Existing approaches advocate an extension to structured P2P protocols, to support the Grid resource information system (GRIS). In this paper, we identify issues related to the design of such an efficient, scalable, fault-tolerant, consistent and practical GRIS system using a P2P network model. We compile these issues into various taxonomies in sections III and IV. Further, we look into existing works that apply P2P based network protocols to GRIS. We think that this taxonomy and its mapping to relevant systems would be useful for academic and industry based researchers who are engaged in the design of scalable Grid systems.",
        "published": "2006-10-30T08:30:17Z",
        "link": "http://arxiv.org/abs/cs/0610163v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Strategies for Replica Placement in Tree Networks",
        "authors": [
            "Yves Robert",
            "Anne Benoit",
            "Veronika Rehn"
        ],
        "summary": "In this paper, we discuss and compare several policies to place replicas in tree networks, subject to server capacity and QoS constraints. The client requests are known beforehand, while the number and location of the servers are to be determined. The standard approach in the literature is to enforce that all requests of a client be served by the closest server in the tree. We introduce and study two new policies. In the first policy, all requests from a given client are still processed by the same server, but this server can be located anywhere in the path from the client to the root. In the second policy, the requests of a given client can be processed by multiple servers. One major contribution of this paper is to assess the impact of these new policies on the total replication cost. Another important goal is to assess the impact of server heterogeneity, both from a theoretical and a practical perspective. In this paper, we establish several new complexity results, and provide several efficient polynomial heuristics for NP-complete instances of the problem. These heuristics are compared to an absolute lower bound provided by the formulation of the problem in terms of the solution of an integer linear program.",
        "published": "2006-11-08T08:16:55Z",
        "link": "http://arxiv.org/abs/cs/0611034v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Grid Added Value to Address Malaria",
        "authors": [
            "V. Breton",
            "N. Jacq",
            "M. Hofmann"
        ],
        "summary": "Through this paper, we call for a distributed, internet-based collaboration to address one of the worst plagues of our present world, malaria. The spirit is a non-proprietary peer-production of information-embedding goods. And we propose to use the grid technology to enable such a world wide \"open source\" like collaboration. The first step towards this vision has been achieved during the summer on the EGEE grid infrastructure where 46 million ligands were docked for a total amount of 80 CPU years in 6 weeks in the quest for new drugs.",
        "published": "2006-11-17T10:21:02Z",
        "link": "http://arxiv.org/abs/q-bio/0611052v1",
        "categories": [
            "q-bio.QM",
            "cs.DC"
        ]
    },
    {
        "title": "Integration and mining of malaria molecular, functional and   pharmacological data: how far are we from a chemogenomic knowledge space?",
        "authors": [
            "L. -M. Birkholtz",
            "O. Bastien",
            "G. Wells",
            "D. Grando",
            "F. Joubert",
            "V. Kasam",
            "M. Zimmermann",
            "P. Ortet",
            "N. Jacq",
            "S. Roy",
            "M. Hoffmann-Apitius",
            "V. Breton",
            "A. I. Louw",
            "E. Maréchal"
        ],
        "summary": "The organization and mining of malaria genomic and post-genomic data is highly motivated by the necessity to predict and characterize new biological targets and new drugs. Biological targets are sought in a biological space designed from the genomic data from Plasmodium falciparum, but using also the millions of genomic data from other species. Drug candidates are sought in a chemical space containing the millions of small molecules stored in public and private chemolibraries. Data management should therefore be as reliable and versatile as possible. In this context, we examined five aspects of the organization and mining of malaria genomic and post-genomic data: 1) the comparison of protein sequences including compositionally atypical malaria sequences, 2) the high throughput reconstruction of molecular phylogenies, 3) the representation of biological processes particularly metabolic pathways, 4) the versatile methods to integrate genomic data, biological representations and functional profiling obtained from X-omic experiments after drug treatments and 5) the determination and prediction of protein structures and their molecular docking with drug candidate structures. Progresses toward a grid-enabled chemogenomic knowledge space are discussed.",
        "published": "2006-11-17T10:24:23Z",
        "link": "http://arxiv.org/abs/q-bio/0611053v1",
        "categories": [
            "q-bio.QM",
            "cs.DC",
            "q-bio.GN"
        ]
    },
    {
        "title": "Grid enabled virtual screening against malaria",
        "authors": [
            "N. Jacq",
            "J. Salzemann",
            "F. Jacq",
            "Y. Legré",
            "E. Medernach",
            "J. Montagnat",
            "A. Maass",
            "M. Reichstadt",
            "H. Schwichtenberg",
            "M. Sridhar",
            "V. Kasam",
            "M. Zimmermann",
            "M. Hofmann",
            "V. Breton"
        ],
        "summary": "WISDOM is an international initiative to enable a virtual screening pipeline on a grid infrastructure. Its first attempt was to deploy large scale in silico docking on a public grid infrastructure. Protein-ligand docking is about computing the binding energy of a protein target to a library of potential drugs using a scoring algorithm. Previous deployments were either limited to one cluster, to grids of clusters in the tightly protected environment of a pharmaceutical laboratory or to pervasive grids. The first large scale docking experiment ran on the EGEE grid production service from 11 July 2005 to 19 August 2005 against targets relevant to research on malaria and saw over 41 million compounds docked for the equivalent of 80 years of CPU time. Up to 1,700 computers were simultaneously used in 15 countries around the world. Issues related to the deployment and the monitoring of the in silico docking experiment as well as experience with grid operation and services are reported in the paper. The main problem encountered for such a large scale deployment was the grid infrastructure stability. Although the overall success rate was above 80%, a lot of monitoring and supervision was still required at the application level to resubmit the jobs that failed. But the experiment demonstrated how grid infrastructures have a tremendous capacity to mobilize very large CPU resources for well targeted goals during a significant period of time. This success leads to a second computing challenge targeting Avian Flu neuraminidase N1.",
        "published": "2006-11-17T10:26:07Z",
        "link": "http://arxiv.org/abs/q-bio/0611054v1",
        "categories": [
            "q-bio.QM",
            "cs.DC"
        ]
    },
    {
        "title": "Large Scale In Silico Screening on Grid Infrastructures",
        "authors": [
            "N. Jacq",
            "V. Breton",
            "H. -Y. Chen",
            "L. -Y. Ho",
            "M. Hofmann",
            "H. -C. Lee",
            "Y. Legré",
            "S. -C. Lin",
            "A. Maass",
            "E. Medernach",
            "I. Merelli",
            "L. Milanesi",
            "G. Rastelli",
            "M. Reichstadt",
            "J. Salzemann",
            "H. Schwichtenberg",
            "M. Sridhar",
            "V. Kasam",
            "Y. -T. Wu",
            "M. Zimmermann"
        ],
        "summary": "Large-scale grid infrastructures for in silico drug discovery open opportunities of particular interest to neglected and emerging diseases. In 2005 and 2006, we have been able to deploy large scale in silico docking within the framework of the WISDOM initiative against Malaria and Avian Flu requiring about 105 years of CPU on the EGEE, Auvergrid and TWGrid infrastructures. These achievements demonstrated the relevance of large-scale grid infrastructures for the virtual screening by molecular docking. This also allowed evaluating the performances of the grid infrastructures and to identify specific issues raised by large-scale deployment.",
        "published": "2006-11-17T10:27:16Z",
        "link": "http://arxiv.org/abs/cs/0611084v1",
        "categories": [
            "cs.DC",
            "q-bio.QM"
        ]
    },
    {
        "title": "Lossy Bulk Synchronous Parallel Processing Model for Very Large Scale   Grids",
        "authors": [
            "Elankovan Sundararajan",
            "Aaron Harwood",
            "Kotagiri Ramamohanarao"
        ],
        "summary": "The performance of a parallel algorithm in a very large scale grid is significantly influenced by the underlying Internet protocols and inter-connectivity. Many grid programming platforms use TCP due to its reliability, usually with some optimizations to reduce its costs. However, TCP does not perform well in a high bandwidth and high delay network environment. On the other hand, UDP is the fastest protocol available because it omits connection setup process, acknowledgments and retransmissions sacrificing reliable transfer. Many new bulk data transfer schemes using UDP for data transmission such as RBUDP, Tsunami, and SABUL have been introduced and shown to have better performance compared to TCP. In this paper, we consider the use of UDP and examine the relationship between packet loss and speedup with respect to the number of grid nodes. Our measurement suggests that packet loss rates between 5%-15% on average are not uncommon between PlanetLab nodes that are widely distributed over the Internet. We show that transmitting multiple copies of same packet produces higher speedup. We show the minimum number of packet duplication required to maximize the possible speedup for a given number of nodes using a BSP based model. Our work demonstrates that by using an appropriate number of packet copies, we can increase performance of parallel program.",
        "published": "2006-11-20T00:20:44Z",
        "link": "http://arxiv.org/abs/cs/0611091v2",
        "categories": [
            "cs.DC",
            "cs.CC",
            "cs.PF"
        ]
    },
    {
        "title": "Analysis of an Efficient Distributed Algorithm for Mutual Exclusion   (Average-Case Analysis of Path Reversal)",
        "authors": [
            "Christian Lavault"
        ],
        "summary": "The algorithm analysed by Na\\\"{i}mi, Trehe and Arnold was the very first distributed algorithm to solve the mutual exclusion problem in complete networks by using a dynamic logical tree structure as its basic distributed data structure, viz. a path reversal transformation in rooted n-node trees; besides, it was also the first one to achieve a logarithmic average-case message complexity. The present paper proposes a direct and general approach to compute the moments of the cost of path reversal. It basically uses one-one correspondences between combinatorial structures and the associated probability generating functions: the expected cost of path reversal is thus proved to be exactly $H_{n-1}$. Moreover, time and message complexity of the algorithm as well as randomized bounds on its worst-case message complexity in arbitrary networks are also given. The average-case analysis of path reversal and the analysis of this distributed algorithm for mutual exclusion are thus fully completed in the paper. The general techniques used should also prove available and fruitful when adapted to the most efficient recent tree-based distributed algorithms for mutual exclusion which require powerful tools, particularly for average-case analyses.",
        "published": "2006-11-20T22:02:29Z",
        "link": "http://arxiv.org/abs/cs/0611098v1",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "On the Complexity of Processing Massive, Unordered, Distributed Data",
        "authors": [
            "Jon Feldman",
            "S. Muthukrishnan",
            "Anastasios Sidiropoulos",
            "Cliff Stein",
            "Zoya Svitkina"
        ],
        "summary": "An existing approach for dealing with massive data sets is to stream over the input in few passes and perform computations with sublinear resources. This method does not work for truly massive data where even making a single pass over the data with a processor is prohibitive. Successful log processing systems in practice such as Google's MapReduce and Apache's Hadoop use multiple machines. They efficiently perform a certain class of highly distributable computations defined by local computations that can be applied in any order to the input.   Motivated by the success of these systems, we introduce a simple algorithmic model for massive, unordered, distributed (mud) computation. We initiate the study of understanding its computational complexity. Our main result is a positive one: any unordered function that can be computed by a streaming algorithm can also be computed with a mud algorithm, with comparable space and communication complexity. We extend this result to some useful classes of approximate and randomized streaming algorithms. We also give negative results, using communication complexity arguments to prove that extensions to private randomness, promise problems and indeterminate functions are impossible.   We believe that the line of research we introduce in this paper has the potential for tremendous impact. The distributed systems that motivate our work successfully process data at an unprecedented scale, distributed over hundreds or even thousands of machines, and perform hundreds of such analyses each day. The mud model (and its generalizations) inspire a set of complexity-theoretic questions that lie at their heart.",
        "published": "2006-11-21T16:11:06Z",
        "link": "http://arxiv.org/abs/cs/0611108v2",
        "categories": [
            "cs.CC",
            "cs.DC"
        ]
    },
    {
        "title": "Discovering Network Topology in the Presence of Byzantine Faults",
        "authors": [
            "Mikhail Nesterenko",
            "Sébastien Tixeuil"
        ],
        "summary": "We study the problem of Byzantine-robust topology discovery in an arbitrary asynchronous network. We formally state the weak and strong versions of the problem. The weak version requires that either each node discovers the topology of the network or at least one node detects the presence of a faulty node. The strong version requires that each node discovers the topology regardless of faults. We focus on non-cryptographic solutions to these problems. We explore their bounds. We prove that the weak topology discovery problem is solvable only if the connectivity of the network exceeds the number of faults in the system. Similarly, we show that the strong version of the problem is solvable only if the network connectivity is more than twice the number of faults. We present solutions to both versions of the problem. The presented algorithms match the established graph connectivity bounds. The algorithms do not require the individual nodes to know either the diameter or the size of the network. The message complexity of both programs is low polynomial with respect to the network size. We describe how our solutions can be extended to add the property of termination, handle topology changes and perform neighborhood discovery.",
        "published": "2006-11-22T18:25:43Z",
        "link": "http://arxiv.org/abs/cs/0611116v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.OS"
        ]
    },
    {
        "title": "2FACE: Bi-Directional Face Traversal for Efficient Geometric Routing",
        "authors": [
            "Mark Miyashita",
            "Mikhail Nesterenko"
        ],
        "summary": "We propose bi-directional face traversal algorithm $2FACE$ to shorten the path the message takes to reach the destination in geometric routing. Our algorithm combines the practicality of the best single-direction traversal algorithms with the worst case message complexity of $O(|E|)$, where $E$ is the number of network edges. We apply $2FACE$ to a variety of geometric routing algorithms. Our simulation results indicate that bi-directional face traversal decreases the latency of message delivery two to three times compared to single direction face traversal. The thus selected path approaches the shortest possible route. This gain in speed comes with a similar message overhead increase. We describe an algorithm which compensates for this message overhead by recording the preferable face traversal direction. Thus, if a source has several messages to send to the destination, the subsequent messages follow the shortest route. Our simulation results show that with most geometric routing algorithms the message overhead of finding the short route by bi-directional face traversal is compensated within two to four repeat messages.",
        "published": "2006-11-22T19:28:31Z",
        "link": "http://arxiv.org/abs/cs/0611117v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.OS"
        ]
    },
    {
        "title": "Scale-Free Overlay Topologies with Hard Cutoffs for Unstructured   Peer-to-Peer Networks",
        "authors": [
            "Hasan Guclu",
            "Murat Yuksel"
        ],
        "summary": "In unstructured peer-to-peer (P2P) networks, the overlay topology (or connectivity graph) among peers is a crucial component in addition to the peer/data organization and search. Topological characteristics have profound impact on the efficiency of search on such unstructured P2P networks as well as other networks. It has been well-known that search on small-world topologies of N nodes can be as efficient as O(ln N), while scale-free (power-law) topologies offer even better search efficiencies like as good as O(lnln N) for a range of degree distribution exponents. However, generation and maintenance of such scale-free topologies are hard to realize in a distributed and potentially uncooperative environments as in the P2P networks. A key limitation of scale-free topologies is the high load (i.e. high degree) on very few number of hub nodes. In a typical unstructured P2P network, peers are not willing to maintain high degrees/loads as they may not want to store large number of entries for construction of the overlay topology. So, to achieve fairness and practicality among all peers, hard cutoffs on the number of entries are imposed by the individual peers, which limits scale-freeness of the overall topology. Thus, efficiency of the flooding search reduces as the size of the hard cutoff does. We investigate construction of scale-free topologies with hard cutoffs (i.e. there are not any major hubs) and effect of these hard cutoffs on the search efficiency. Interestingly, we observe that the efficiency of normalized flooding and random walk search algorithms increases as the hard cutoff decreases.",
        "published": "2006-11-25T21:49:55Z",
        "link": "http://arxiv.org/abs/cs/0611128v3",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Static Safety for an Actor Dedicated Process Calculus by Abstract   Interpretation",
        "authors": [
            "Pierre-Loïc Garoche",
            "Marc Pantel",
            "Xavier Thirioux"
        ],
        "summary": "The actor model eases the definition of concurrent programs with non uniform behaviors. Static analysis of such a model was previously done in a data-flow oriented way, with type systems. This approach was based on constraint set resolution and was not able to deal with precise properties for communications of behaviors. We present here a new approach, control-flow oriented, based on the abstract interpretation framework, able to deal with communication of behaviors. Within our new analyses, we are able to verify most of the previous properties we observed as well as new ones, principally based on occurrence counting.",
        "published": "2006-11-28T07:48:18Z",
        "link": "http://arxiv.org/abs/cs/0611139v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Next Generation Language Resources using GRID",
        "authors": [
            "Federico Calzolari",
            "Eva Sassolini",
            "Manuela Sassi",
            "Sebastiana Cucurullo",
            "Eugenio Picchi",
            "Francesca Bertagna",
            "Alessandro Enea",
            "Monica Monachini",
            "Claudia Soria",
            "Nicoletta Calzolari"
        ],
        "summary": "This paper presents a case study concerning the challenges and requirements posed by next generation language resources, realized as an overall model of open, distributed and collaborative language infrastructure. If a sort of \"new paradigm\" is required, we think that the emerging and still evolving technology connected to Grid computing is a very interesting and suitable one for a concrete realization of this vision. Given the current limitations of Grid computing, it is very important to test the new environment on basic language analysis tools, in order to get the feeling of what are the potentialities and possible limitations connected to its use in NLP. For this reason, we have done some experiments on a module of Linguistic Miner, i.e. the extraction of linguistic patterns from restricted domain corpora.",
        "published": "2006-11-29T10:49:23Z",
        "link": "http://arxiv.org/abs/cs/0611148v3",
        "categories": [
            "cs.DC",
            "cs.CL"
        ]
    },
    {
        "title": "Partially ordered distributed computations on asynchronous   point-to-point networks",
        "authors": [
            "Ricardo C. Correa",
            "Valmir C. Barbosa"
        ],
        "summary": "Asynchronous executions of a distributed algorithm differ from each other due to the nondeterminism in the order in which the messages exchanged are handled. In many situations of interest, the asynchronous executions induced by restricting nondeterminism are more efficient, in an application-specific sense, than the others. In this work, we define partially ordered executions of a distributed algorithm as the executions satisfying some restricted orders of their actions in two different frameworks, those of the so-called event- and pulse-driven computations. The aim of these restrictions is to characterize asynchronous executions that are likely to be more efficient for some important classes of applications. Also, an asynchronous algorithm that ensures the occurrence of partially ordered executions is given for each case. Two of the applications that we believe may benefit from the restricted nondeterminism are backtrack search, in the event-driven case, and iterative algorithms for systems of linear equations, in the pulse-driven case.",
        "published": "2006-11-30T13:01:36Z",
        "link": "http://arxiv.org/abs/cs/0611165v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Economy-based Content Replication for Peering Content Delivery Networks",
        "authors": [
            "Al-Mukaddim Khan Pathan",
            "Rajkumar Buyya",
            "James Broberg",
            "Kris Bubendorfer"
        ],
        "summary": "Existing Content Delivery Networks (CDNs) exhibit the nature of closed delivery networks which do not cooperate with other CDNs and in practice, islands of CDNs are formed. The logical separation between contents and services in this context results in two content networking domains. In addition to that, meeting the Quality of Service requirements of users according to negotiated Service Level Agreement is crucial for a CDN. Present trends in content networks and content networking capabilities give rise to the interest in interconnecting content networks. Hence, in this paper, we present an open, scalable, and Service-Oriented Architecture (SOA)-based system that assist the creation of open Content and Service Delivery Networks (CSDNs), which scale and supports sharing of resources through peering with other CSDNs. To encourage resource sharing and peering arrangements between different CDN providers at global level, we propose using market-based models by introducing an economy-based strategy for content replication.",
        "published": "2006-12-04T06:29:16Z",
        "link": "http://arxiv.org/abs/cs/0612013v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Registers",
        "authors": [
            "Paul M. B. Vitanyi"
        ],
        "summary": "Entry in: Encyclopedia of Algorithms, Ming-Yang Kao, Ed., Springer, To appear.   Synonyms: Wait-free registers, wait-free shared variables, asynchronous communication hardware. Problem Definition: Consider a system of asynchronous processes that communicate among themselves by only executing read and write operations on a set of shared variables (also known as shared registers). The system has no global clock or other synchronization primitives.",
        "published": "2006-12-05T16:51:25Z",
        "link": "http://arxiv.org/abs/cs/0612025v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Distributed Slicing in Dynamic Systems",
        "authors": [
            "Antonio Fernandez",
            "Vincent Gramoli",
            "Ernesto Jimenez",
            "Anne-Marie Kermarrec",
            "Michel Raynal"
        ],
        "summary": "Peer to peer (P2P) systems are moving from application specific architectures to a generic service oriented design philosophy. This raises interesting problems in connection with providing useful P2P middleware services that are capable of dealing with resource assignment and management in a large-scale, heterogeneous and unreliable environment. One such service, the slicing service, has been proposed to allow for an automatic partitioning of P2P networks into groups (slices) that represent a controllable amount of some resource and that are also relatively homogeneous with respect to that resource, in the face of churn and other failures. In this report we propose two algorithms to solve the distributed slicing problem. The first algorithm improves upon an existing algorithm that is based on gossip-based sorting of a set of uniform random numbers. We speed up convergence via a heuristic for gossip peer selection. The second algorithm is based on a different approach: statistical approximation of the rank of nodes in the ordering. The scalability, efficiency and resilience to dynamics of both algorithms relies on their gossip-based models. We present theoretical and experimental results to prove the viability of these algorithms.",
        "published": "2006-12-06T13:57:54Z",
        "link": "http://arxiv.org/abs/cs/0612035v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Revisiting Matrix Product on Master-Worker Platforms",
        "authors": [
            "Jack Dongarra",
            "Jean-Francois Pineau",
            "Yves Robert",
            "Zhiao Shi",
            "Frederic Vivien"
        ],
        "summary": "This paper is aimed at designing efficient parallel matrix-product algorithms for heterogeneous master-worker platforms. While matrix-product is well-understood for homogeneous 2D-arrays of processors (e.g., Cannon algorithm and ScaLAPACK outer product algorithm), there are three key hypotheses that render our work original and innovative:   - Centralized data. We assume that all matrix files originate from, and must be returned to, the master.   - Heterogeneous star-shaped platforms. We target fully heterogeneous platforms, where computational resources have different computing powers.   - Limited memory. Because we investigate the parallelization of large problems, we cannot assume that full matrix panels can be stored in the worker memories and re-used for subsequent updates (as in ScaLAPACK).   We have devised efficient algorithms for resource selection (deciding which workers to enroll) and communication ordering (both for input and result messages), and we report a set of numerical experiments on various platforms at Ecole Normale Superieure de Lyon and the University of Tennessee. However, we point out that in this first version of the report, experiments are limited to homogeneous platforms.",
        "published": "2006-12-06T14:34:29Z",
        "link": "http://arxiv.org/abs/cs/0612036v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "F.2.2"
        ]
    },
    {
        "title": "About the Lifespan of Peer to Peer Networks",
        "authors": [
            "R. Cilibrasi",
            "Z. Lotker",
            "A. Navarra",
            "S. Perennes",
            "P. Vitanyi"
        ],
        "summary": "We analyze the ability of peer to peer networks to deliver a complete file among the peers. Early on we motivate a broad generalization of network behavior organizing it into one of two successive phases. According to this view the network has two main states: first centralized - few sources (roots) hold the complete file, and next distributed - peers hold some parts (chunks) of the file such that the entire network has the whole file, but no individual has it. In the distributed state we study two scenarios, first, when the peers are ``patient'', i.e, do not leave the system until they obtain the complete file; second, peers are ``impatient'' and almost always leave the network before obtaining the complete file.",
        "published": "2006-12-07T16:20:00Z",
        "link": "http://arxiv.org/abs/cs/0612043v1",
        "categories": [
            "cs.DC",
            "cs.IR"
        ]
    },
    {
        "title": "Decentralized Maximum Likelihood Estimation for Sensor Networks Composed   of Nonlinearly Coupled Dynamical Systems",
        "authors": [
            "Sergio Barbarossa",
            "Gesualdo Scutari"
        ],
        "summary": "In this paper we propose a decentralized sensor network scheme capable to reach a globally optimum maximum likelihood (ML) estimate through self-synchronization of nonlinearly coupled dynamical systems. Each node of the network is composed of a sensor and a first-order dynamical system initialized with the local measurements. Nearby nodes interact with each other exchanging their state value and the final estimate is associated to the state derivative of each dynamical system. We derive the conditions on the coupling mechanism guaranteeing that, if the network observes one common phenomenon, each node converges to the globally optimal ML estimate. We prove that the synchronized state is globally asymptotically stable if the coupling strength exceeds a given threshold. Acting on a single parameter, the coupling strength, we show how, in the case of nonlinear coupling, the network behavior can switch from a global consensus system to a spatial clustering system. Finally, we show the effect of the network topology on the scalability properties of the network and we validate our theoretical findings with simulation results.",
        "published": "2006-12-07T17:22:48Z",
        "link": "http://arxiv.org/abs/cs/0612042v1",
        "categories": [
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Effective networks for real-time distributed processing",
        "authors": [
            "Gonzalo Travieso",
            "Luciano da Fontoura Costa"
        ],
        "summary": "The problem of real-time processing is one of the most challenging current issues in computer sciences. Because of the large amount of data to be treated in a limited period of time, parallel and distributed systems are required, whose performance depends on a series of factors including the interconnectivity of the processing elements, the application model and the communication protocol. Given their flexibility for representing and modeling natural and human-made systems (such as the Internet and WWW), complex networks have become a primary choice in many research areas. The current work presents how the concepts and methods of complex networks can be used to develop realistic models and simulations of distributed real-time system while taking into account two representative interconnection models: uniformly random and scale free (Barabasi-Albert), including the presence of background traffic of messages. The interesting obtained results include the identification of the uniformly random interconnectivity scheme as being largely more efficient than the scale-free counterpart.",
        "published": "2006-12-13T17:33:56Z",
        "link": "http://arxiv.org/abs/physics/0612134v2",
        "categories": [
            "physics.soc-ph",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Energy Efficient Randomized Communication in Unknown AdHoc Networks",
        "authors": [
            "Petra Berenbrink",
            "Colin Cooper",
            "Zengjian Hu"
        ],
        "summary": "This paper studies broadcasting and gossiping algorithms in random and general AdHoc networks. Our goal is not only to minimise the broadcasting and gossiping time, but also to minimise the energy consumption, which is measured in terms of the total number of messages (or transmissions) sent. We assume that the nodes of the network do not know the network, and that they can only send with a fixed power, meaning they can not adjust the areas sizes that their messages cover. We believe that under these circumstances the number of transmissions is a very good measure for the overall energy consumption.   For random networks, we present a broadcasting algorithm where every node transmits at most once. We show that our algorithm broadcasts in $O(\\log n)$ steps, w.h.p, where $n$ is the number of nodes. We then present a $O(d \\log n)$ ($d$ is the expected degree) gossiping algorithm using $O(\\log n)$ messages per node.   For general networks with known diameter $D$, we present a randomised broadcasting algorithm with optimal broadcasting time $O(D \\log (n/D) + \\log^2 n)$ that uses an expected number of $O(\\log^2 n / \\log (n/D))$ transmissions per node. We also show a tradeoff result between the broadcasting time and the number of transmissions: we construct a network such that any oblivious algorithmusing a time-invariant distribution requires $\\Omega(\\log^2 n / \\log (n/D))$ messages per node in order to finish broadcasting in optimal time. This demonstrates the tightness of our upper bound. We also show that no oblivious algorithm can complete broadcasting w.h.p. using $o(\\log n)$ messages per node.",
        "published": "2006-12-15T03:43:39Z",
        "link": "http://arxiv.org/abs/cs/0612074v1",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "A Byzantine Fault Tolerant Distributed Commit Protocol",
        "authors": [
            "Wenbing Zhao"
        ],
        "summary": "In this paper, we present a Byzantine fault tolerant distributed commit protocol for transactions running over untrusted networks. The traditional two-phase commit protocol is enhanced by replicating the coordinator and by running a Byzantine agreement algorithm among the coordinator replicas. Our protocol can tolerate Byzantine faults at the coordinator replicas and a subset of malicious faults at the participants. A decision certificate, which includes a set of registration records and a set of votes from participants, is used to facilitate the coordinator replicas to reach a Byzantine agreement on the outcome of each transaction. The certificate also limits the ways a faulty replica can use towards non-atomic termination of transactions, or semantically incorrect transaction outcomes.",
        "published": "2006-12-18T09:31:43Z",
        "link": "http://arxiv.org/abs/cs/0612083v3",
        "categories": [
            "cs.DC",
            "cs.DB"
        ]
    },
    {
        "title": "A Calculus for Sensor Networks",
        "authors": [
            "Miguel S. Silva",
            "Francisco Martins",
            "Luis Lopes",
            "Joao Barros"
        ],
        "summary": "We consider the problem of providing a rigorous model for programming wireless sensor networks. Assuming that collisions, packet losses, and errors are dealt with at the lower layers of the protocol stack, we propose a Calculus for Sensor Networks (CSN) that captures the main abstractions for programming applications for this class of devices. Besides providing the syntax and semantics for the calculus, we show its expressiveness by providing implementations for several examples of typical operations on sensor networks. Also included is a detailed discussion of possible extensions to CSN that enable the modeling of other important features of these networks such as sensor state, sampling strategies, and network security.",
        "published": "2006-12-19T14:45:00Z",
        "link": "http://arxiv.org/abs/cs/0612093v1",
        "categories": [
            "cs.DC",
            "cs.PL"
        ]
    },
    {
        "title": "Non-Clairvoyant Batch Sets Scheduling: Fairness is Fair enough",
        "authors": [
            "Julien Robert",
            "Nicolas Schabanel"
        ],
        "summary": "Scheduling questions arise naturally in many different areas among which operating system design, compiling,... In real life systems, the characteristics of the jobs (such as release time and processing time) are usually unknown and unpredictable beforehand. The system is typically unaware of the remaining work in each job or of the ability of the job to take advantage of more resources. Following these observations, we adopt the job model by Edmonds et al (2000, 2003) in which the jobs go through a sequence of different phases. Each phase consists of a certain quantity of work and a speed-up function that models how it takes advantage of the number of processors it receives. We consider the non-clairvoyant online setting where a collection of jobs arrives at time 0. We consider the metrics setflowtime introduced by Robert et al (2007). The goal is to minimize the sum of the completion time of the sets, where a set is completed when all of its jobs are done. If the input consists of a single set of jobs, this is simply the makespan of the jobs; and if the input consists of a collection of singleton sets, it is simply the flowtime of the jobs. We show that the non-clairvoyant strategy EQUIoEQUI that evenly splits the available processors among the still unserved sets and then evenly splits these processors among the still uncompleted jobs of each unserved set, achieves a competitive ratio (2+\\sqrt3+o(1))\\frac{ln n}{lnln n} for the setflowtime minimization and that this is asymptotically optimal (up to a constant factor), where n is the size of the largest set. For makespan minimization, we show that the non-clairvoyant strategy EQUI achieves a competitive ratio of (1+o(1))\\frac{ln n}{lnln n}, which is again asymptotically optimal.",
        "published": "2006-12-19T15:19:59Z",
        "link": "http://arxiv.org/abs/cs/0612088v2",
        "categories": [
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Towards Parallel Computing on the Internet: Applications, Architectures,   Models and Programming Tools",
        "authors": [
            "Elankovan Sundararajan",
            "Aaron Harwood"
        ],
        "summary": "The development of Internet wide resources for general purpose parallel computing poses the challenging task of matching computation and communication complexity. A number of parallel computing models exist that address this for traditional parallel architectures, and there are a number of emerging models that attempt to do this for large scale Internet-based systems like computational grids. In this survey we cover the three fundamental aspects -- application, architecture and model, and we show how they have been developed over the last decade. We also cover programming tools that are currently being used for parallel programming in computational grids. The trend in conventional computational models are to put emphasis on efficient communication between participating nodes by adapting different types of communication to network conditions. Effects of dynamism and uncertainties that arise in large scale systems are evidently important to understand and yet there is currently little work that addresses this from a parallel computing perspective.",
        "published": "2006-12-21T06:38:21Z",
        "link": "http://arxiv.org/abs/cs/0612105v2",
        "categories": [
            "cs.DC",
            "cs.PF"
        ]
    },
    {
        "title": "Heterogeneous Strong Computation Migration",
        "authors": [
            "Anolan Milanés",
            "Noemi Rodriguez",
            "Bruno Schulze"
        ],
        "summary": "The continuous increase in performance requirements, for both scientific computation and industry, motivates the need of a powerful computing infrastructure. The Grid appeared as a solution for inexpensive execution of heavy applications in a parallel and distributed manner. It allows combining resources independently of their physical location and architecture to form a global resource pool available to all grid users. However, grid environments are highly unstable and unpredictable. Adaptability is a crucial issue in this context, in order to guarantee an appropriate quality of service to users. Migration is a technique frequently used for achieving adaptation. The objective of this report is to survey the problem of strong migration in heterogeneous environments like the grids', the related implementation issues and the current solutions.",
        "published": "2006-12-22T18:27:25Z",
        "link": "http://arxiv.org/abs/cs/0612125v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Lighting Control using Pressure-Sensitive Touchpads",
        "authors": [
            "Alexander Haubold"
        ],
        "summary": "We introduce a novel approach to control physical lighting parameters by means of a pressure-sensitive touchpad. The two-dimensional area of the touchpad is subdivided into 5 virtual sliders, each controlling the intensity of a color (red, green, blue, yellow, and white). The physical interaction methodology is modeled directly after ubiquitous mechanical sliders and dimmers which tend to be used for intensity/volume control. Our abstraction to a pressure-sensitive touchpad provides advantages and introduces additional benefits over such existing devices.",
        "published": "2006-01-07T11:06:23Z",
        "link": "http://arxiv.org/abs/cs/0601021v1",
        "categories": [
            "cs.HC",
            "B.4.2; H.5.2"
        ]
    },
    {
        "title": "Prop-Based Haptic Interaction with Co-location and Immersion: an   Automotive Application",
        "authors": [
            "Michael Ortega",
            "Sabine Coquillart"
        ],
        "summary": "Most research on 3D user interfaces aims at providing only a single sensory modality. One challenge is to integrate several sensory modalities into a seamless system while preserving each modality's immersion and performance factors. This paper concerns manipulation tasks and proposes a visuo-haptic system integrating immersive visualization, tactile force and tactile feedback with co-location. An industrial application is presented.",
        "published": "2006-01-09T13:52:32Z",
        "link": "http://arxiv.org/abs/cs/0601025v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "A Visual Query Language for Complex-Value Databases",
        "authors": [
            "Christoph Koch"
        ],
        "summary": "In this paper, a visual language, VCP, for queries on complex-value databases is proposed. The main strength of the new language is that it is purely visual: (i) It has no notion of variable, quantification, partiality, join, pattern matching, regular expression, recursion, or any other construct proper to logical, functional, or other database query languages and (ii) has a very natural, strong, and intuitive design metaphor. The main operation is that of copying and pasting in a schema tree.   We show that despite its simplicity, VCP precisely captures complex-value algebra without powerset, or equivalently, monad algebra with union and difference. Thus, its expressive power is precisely that of the language that is usually considered to play the role of relational algebra for complex-value databases.",
        "published": "2006-02-03T19:22:28Z",
        "link": "http://arxiv.org/abs/cs/0602006v1",
        "categories": [
            "cs.DB",
            "cs.HC"
        ]
    },
    {
        "title": "Improving the CSIEC Project and Adapting It to the English Teaching and   Learning in China",
        "authors": [
            "Jiyou Jia",
            "Shufen Hou",
            "Weichao Chen"
        ],
        "summary": "In this paper after short review of the CSIEC project initialized by us in 2003 we present the continuing development and improvement of the CSIEC project in details, including the design of five new Microsoft agent characters representing different virtual chatting partners and the limitation of simulated dialogs in specific practical scenarios like graduate job application interview, then briefly analyze the actual conditions and features of its application field: web-based English education in China. Finally we introduce our efforts to adapt this system to the requirements of English teaching and learning in China and point out the work next to do.",
        "published": "2006-02-06T15:17:34Z",
        "link": "http://arxiv.org/abs/cs/0602018v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.MA",
            "K.3.1; I.2.7; I.2.11"
        ]
    },
    {
        "title": "minimUML: A Minimalist Approach to UML Diagraming for Early Computer   Science Education",
        "authors": [
            "Scott Turner",
            "Manuel A. Perez-Quinones",
            "Stephen H. Edwards"
        ],
        "summary": "The Unified Modeling Language (UML) is commonly used in introductory Computer Science to teach basic object-oriented design. However, there appears to be a lack of suitable software to support this task. Many of the available programs that support UML focus on developing code and not on enhancing learning. Those that were designed for educational use sometimes have poor interfaces or are missing common and important features, such as multiple selection and undo/redo. There is a need for software that is tailored to an instructional environment and has all the useful and needed functionality for that specific task. This is the purpose of minimUML. minimUML provides a minimum amount of UML, just what is commonly used in beginning programming classes, while providing a simple, usable interface. In particular, minimUML was designed to support abstract design while supplying features for exploratory learning and error avoidance. In addition, it allows for the annotation of diagrams, through text or freeform drawings, so students can receive feedback on their work. minimUML was developed with the goal of supporting ease of use, supporting novice students, and a requirement of no prior-training for its use.",
        "published": "2006-03-30T06:12:17Z",
        "link": "http://arxiv.org/abs/cs/0603121v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "K.3.2; H.5.2; D.2.2"
        ]
    },
    {
        "title": "The Case for Modeling Security, Privacy, Usability and Reliability   (SPUR) in Automotive Software",
        "authors": [
            "K. Venkatesh Prasad",
            "TJ Giuli",
            "David Watson"
        ],
        "summary": "Over the past five years, there has been considerable growth and established value in the practice of modeling automotive software requirements. Much of this growth has been centered on requirements of software associated with the established functional areas of an automobile, such as those associated with powertrain, chassis, body, safety and infotainment. This paper makes a case for modeling four additional attributes that are increasingly important as vehicles become information conduits: security, privacy, usability, and reliability. These four attributes are important in creating specifications for embedded in-vehicle automotive software.",
        "published": "2006-04-06T13:23:53Z",
        "link": "http://arxiv.org/abs/cs/0604019v1",
        "categories": [
            "cs.SE",
            "cs.CR",
            "cs.HC",
            "D.2.4; K.4.1; H.5.2; K.6.5"
        ]
    },
    {
        "title": "HCI and Educational Metrics as Tools for VLE Evaluation",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "The general set of HCI and Educational principles are considered and a classification system constructed. A frequency analysis of principles is used to obtain the most significant set. Metrics are devised to provide objective measures of these principles and a consistent testing regime devised. These principles are used to analyse Blackboard and Moodle.",
        "published": "2006-04-25T19:32:03Z",
        "link": "http://arxiv.org/abs/cs/0604102v1",
        "categories": [
            "cs.HC",
            "cs.LG"
        ]
    },
    {
        "title": "Further Evaluationh of VLEs using HCI and Educational Metrics",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "Under consideration are the general set of Human computer Interaction (HCI) and Educational principles from prominent authors in the field and the construction of a system for evaluating Virtual Learning Environments (VLEs) with respect to the application of these HCI and Educational Principles. A frequency analysis of principles is used to obtain the most significant set. Metrics are devised to provide objective measures of these principles and a consistent testing regime is introduced. These principles are used to analyse the University VLE Blackboard. An open source VLE is also constructed with similar content to Blackboard courses so that a systematic comparison can be made. HCI and Educational metrics are determined for each VLE.",
        "published": "2006-04-25T19:53:19Z",
        "link": "http://arxiv.org/abs/cs/0604103v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Modeling the Dynamics of Social Networks",
        "authors": [
            "Victor V. Kryssanov",
            "Frank J. Rinaldo",
            "Evgeny L. Kuleshov",
            "Hitoshi Ogawa"
        ],
        "summary": "Modeling human dynamics responsible for the formation and evolution of the so-called social networks - structures comprised of individuals or organizations and indicating connectivities existing in a community - is a topic recently attracting a significant research interest. It has been claimed that these dynamics are scale-free in many practically important cases, such as impersonal and personal communication, auctioning in a market, accessing sites on the WWW, etc., and that human response times thus conform to the power law. While a certain amount of progress has recently been achieved in predicting the general response rate of a human population, existing formal theories of human behavior can hardly be found satisfactory to accommodate and comprehensively explain the scaling observed in social networks. In the presented study, a novel system-theoretic modeling approach is proposed and successfully applied to determine important characteristics of a communication network and to analyze consumer behavior on the WWW.",
        "published": "2006-05-24T02:13:13Z",
        "link": "http://arxiv.org/abs/cs/0605101v1",
        "categories": [
            "cs.CY",
            "cs.CE",
            "cs.CL",
            "cs.HC",
            "cs.NI",
            "physics.data-an"
        ]
    },
    {
        "title": "Understanding Design Fundamentals: How Synthesis and Analysis Drive   Creativity, Resulting in Emergence",
        "authors": [
            "V. V. Kryssanov",
            "H. Tamaki",
            "S. Kitamura"
        ],
        "summary": "This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought - analogical and metaphorical reasoning - is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.",
        "published": "2006-05-25T11:35:39Z",
        "link": "http://arxiv.org/abs/cs/0605120v1",
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "Communication of Social Agents and the Digital City - A Semiotic   Perspective",
        "authors": [
            "Victor V. Kryssanov",
            "Masayuki Okabe",
            "Koh Kakusho",
            "Michihiko Minoh"
        ],
        "summary": "This paper investigates the concept of digital city. First, a functional analysis of a digital city is made in the light of the modern study of urbanism; similarities between the virtual and urban constructions are pointed out. Next, a semiotic perspective on the subject matter is elaborated, and a terminological basis is introduced to treat a digital city as a self-organizing meaning-producing system intended to support social or spatial navigation. An explicit definition of a digital city is formulated. Finally, the proposed approach is discussed, conclusions are given, and future work is outlined.",
        "published": "2006-05-25T11:54:35Z",
        "link": "http://arxiv.org/abs/cs/0605121v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "Modeling Hypermedia-Based Communication",
        "authors": [
            "V. V. Kryssanov",
            "K. Kakusho",
            "E. L. Kuleshov",
            "M. Minoh"
        ],
        "summary": "In this article, we explore two approaches to modeling hypermedia-based communication. It is argued that the classical conveyor-tube framework is not applicable to the case of computer- and Internet- mediated communication. We then present a simple but very general system-theoretic model of the communication process, propose its mathematical interpretation, and derive several formulas, which qualitatively and quantitatively accord with data obtained on-line. The devised theoretical results generalize and correct the Zipf-Mandelbrot law and can be used in information system design. At the paper's end, we give some conclusions and draw implications for future work.",
        "published": "2006-05-25T12:34:03Z",
        "link": "http://arxiv.org/abs/cs/0605122v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "cs.IR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Utilisation de la linguistique en reconnaissance de la parole : un   état de l'art",
        "authors": [
            "Stéphane Huet",
            "Pascale Sébillot",
            "Guillaume Gravier"
        ],
        "summary": "To transcribe speech, automatic speech recognition systems use statistical methods, particularly hidden Markov model and N-gram models. Although these techniques perform well and lead to efficient systems, they approach their maximum possibilities. It seems thus necessary, in order to outperform current results, to use additional information, especially bound to language. However, introducing such knowledge must be realized taking into account specificities of spoken language (hesitations for example) and being robust to possible misrecognized words. This document presents a state of the art of these researches, evaluating the impact of the insertion of linguistic information on the quality of the transcription.",
        "published": "2006-05-30T15:32:41Z",
        "link": "http://arxiv.org/abs/cs/0605147v1",
        "categories": [
            "cs.HC",
            "cs.CL"
        ]
    },
    {
        "title": "A Framework for the Development of Manufacturing Simulators: Towards New   Generation of Simulation Systems",
        "authors": [
            "V. V. Kryssanov",
            "V. A. Abramov",
            "H. Hibino",
            "Y. Fukuda"
        ],
        "summary": "In this paper, an attempt is made to systematically discuss the development of simulation systems for manufacturing system design. General requirements on manufacturing simulators are formulated and a framework to address the requirements is suggested. Problems of information representation as an activity underlying simulation are considered. This is to form the necessary mathematical foundation for manufacturing simulations. The theoretical findings are explored through a pilot study. A conclusion about the suitability of the suggested approach to the development of simulation systems for manufacturing system design is made, and implications for future research are described.",
        "published": "2006-06-01T02:36:12Z",
        "link": "http://arxiv.org/abs/cs/0606004v1",
        "categories": [
            "cs.CE",
            "cs.HC"
        ]
    },
    {
        "title": "A parent-centered radial layout algorithm for interactive graph   visualization and animation",
        "authors": [
            "Andrew Pavlo",
            "Christopher Homan",
            "Jonathan Schull"
        ],
        "summary": "We have developed (1) a graph visualization system that allows users to explore graphs by viewing them as a succession of spanning trees selected interactively, (2) a radial graph layout algorithm, and (3) an animation algorithm that generates meaningful visualizations and smooth transitions between graphs while minimizing edge crossings during transitions and in static layouts.   Our system is similar to the radial layout system of Yee et al. (2001), but differs primarily in that each node is positioned on a coordinate system centered on its own parent rather than on a single coordinate system for all nodes. Our system is thus easy to define recursively and lends itself to parallelization. It also guarantees that layouts have many nice properties, such as: it guarantees certain edges never cross during an animation.   We compared the layouts and transitions produced by our algorithms to those produced by Yee et al. Results from several experiments indicate that our system produces fewer edge crossings during transitions between graph drawings, and that the transitions more often involve changes in local scaling rather than structure.   These findings suggest the system has promise as an interactive graph exploration tool in a variety of settings.",
        "published": "2006-06-01T16:56:55Z",
        "link": "http://arxiv.org/abs/cs/0606007v1",
        "categories": [
            "cs.HC",
            "cs.CG",
            "cs.GR",
            "I.3.3; H.5.0"
        ]
    },
    {
        "title": "From semiotics of hypermedia to physics of semiosis: A view from system   theory",
        "authors": [
            "V. V. Kryssanov",
            "K. Kakusho"
        ],
        "summary": "Given that theoretical analysis and empirical validation is fundamental to any model, whether conceptual or formal, it is surprising that these two tools of scientific discovery are so often ignored in the contemporary studies of communication. In this paper, we pursued the ideas of a) correcting and expanding the modeling approaches of linguistics, which are otherwise inapplicable (more precisely, which should not but are widely applied), to the general case of hypermedia-based communication, and b) developing techniques for empirical validation of semiotic models, which are nowadays routinely used to explore (in fact, to conjecture about) internal mechanisms of complex systems, yet on a purely speculative basis. This study thus offers two experimentally tested substantive contributions: the formal representation of communication as the mutually-orienting behavior of coupled autonomous systems, and the mathematical interpretation of the semiosis of communication, which together offer a concrete and parsimonious understanding of diverse communication phenomena.",
        "published": "2006-06-05T03:39:09Z",
        "link": "http://arxiv.org/abs/cs/0606017v1",
        "categories": [
            "cs.HC",
            "cs.CL",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Toward Functionality Oriented Programming",
        "authors": [
            "Chengpu Wang"
        ],
        "summary": "The concept of functionality oriented programming is proposed, and some of its aspects are discussed, such as: (1) implementation independent basic types and generic collection types; (2) syntax requirements and recommendations for implementation independence; (3) unified documentation and code; (4) cross-module interface; and (5) cross-language program making scheme. A prototype example is given to demonstrate functionality oriented programming.",
        "published": "2006-06-24T18:51:54Z",
        "link": "http://arxiv.org/abs/cs/0606102v3",
        "categories": [
            "cs.PL",
            "cs.HC"
        ]
    },
    {
        "title": "Human Information Processing with the Personal Memex",
        "authors": [
            "Ingrid Burbey",
            "Gyuhyun Kwon",
            "Uma Murthy",
            "Nicholas Polys",
            "Prince Vincent"
        ],
        "summary": "In this report, we describe the work done in a project that explored the human information processing aspects of a personal memex (a memex to organize personal information). In the project, we considered the use of the personal memex, focusing on information recall, by three populations: people with Mild Cognitive Impairment, those diagnosed with Macular Degeneration, and a high-functioning population. The outcomes of the project included human information processing-centered design guidelines for the memex interface, a low-fidelity prototype, and an annotated bibliography for human information processing, usability and design literature relating to the memex and the populations we explored.",
        "published": "2006-06-26T15:58:40Z",
        "link": "http://arxiv.org/abs/cs/0606107v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Effect of Interface Style in Peer Review Comments for UML Designs",
        "authors": [
            "Scott A. Turner",
            "Manuel A. Perez-Quinones",
            "Stephen H. Edwards"
        ],
        "summary": "This paper presents our evaluation of using a Tablet-PC to provide peer-review comments in the first year Computer Science course. Our exploration consisted of an evaluation of how students write comments on other students' assignments using three different methods: pen and paper, a Tablet-PC, and a desktop computer. Our ultimate goal is to explore the effect that interface style (Tablet vs. Desktop) has on the quality and quantity of the comments provided.",
        "published": "2006-07-14T16:00:40Z",
        "link": "http://arxiv.org/abs/cs/0607072v1",
        "categories": [
            "cs.HC",
            "H.1; H.4; H.5"
        ]
    },
    {
        "title": "Where's the \"Party\" in \"Multi-Party\"? Analyzing the Structure of   Small-Group Sociable Talk",
        "authors": [
            "Paul M. Aoki",
            "Margaret H. Szymanski",
            "Luke Plurkowski",
            "James D. Thornton",
            "Allison Woodruff",
            "Weilie Yi"
        ],
        "summary": "Spontaneous multi-party interaction - conversation among groups of three or more participants - is part of daily life. While automated modeling of such interactions has received increased attention in ubiquitous computing research, there is little applied research on the organization of this highly dynamic and spontaneous sociable interaction within small groups. We report here on an applied conversation analytic study of small-group sociable talk, emphasizing structural and temporal aspects that can inform computational models. In particular, we examine the mechanics of multiple simultaneous conversational floors - how participants initiate a new floor amidst an on-going floor, and how they subsequently show their affiliation with one floor over another. We also discuss the implications of these findings for the design of \"smart\" multi-party applications.",
        "published": "2006-08-21T01:42:52Z",
        "link": "http://arxiv.org/abs/cs/0608083v2",
        "categories": [
            "cs.HC",
            "H.5.3"
        ]
    },
    {
        "title": "Social Decision Making with Multi-Relational Networks and Grammar-Based   Particle Swarms",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "Social decision support systems are able to aggregate the local perspectives of a diverse group of individuals into a global social decision. This paper presents a multi-relational network ontology and grammar-based particle swarm algorithm capable of aggregating the decisions of millions of individuals. This framework supports a diverse problem space and a broad range of vote aggregation algorithms. These algorithms account for individual expertise and representation across different domains of the group problem space. Individuals are able to pose and categorize problems, generate potential solutions, choose trusted representatives, and vote for particular solutions. Ultimately, via a social decision making algorithm, the system aggregates all the individual votes into a single collective decision.",
        "published": "2006-09-07T20:40:39Z",
        "link": "http://arxiv.org/abs/cs/0609034v1",
        "categories": [
            "cs.CY",
            "cs.HC"
        ]
    },
    {
        "title": "NectaRSS, an RSS feed ranking system that implicitly learns user   preferences",
        "authors": [
            "Juan J. Samper",
            "Pedro A. Castillo",
            "Lourdes Araujo",
            "J. J. Merelo"
        ],
        "summary": "In this paper a new RSS feed ranking method called NectaRSS is introduced. The system recommends information to a user based on his/her past choices. User preferences are automatically acquired, avoiding explicit feedback, and ranking is based on those preferences distilled to a user profile. NectaRSS uses the well-known vector space model for user profiles and new documents, and compares them using information-retrieval techniques, but introduces a novel method for user profile creation and adaptation from users' past choices. The efficiency of the proposed method has been tested by embedding it into an intelligent aggregator (RSS feed reader), which has been used by different and heterogeneous users. Besides, this paper proves that the ranking of newsitems yielded by NectaRSS improves its quality with user's choices, and its superiority over other algorithms that use a different information representation method.",
        "published": "2006-10-04T15:13:55Z",
        "link": "http://arxiv.org/abs/cs/0610019v1",
        "categories": [
            "cs.IR",
            "cs.HC"
        ]
    },
    {
        "title": "Paper to Screen: Processing Historical Scans in the ADS",
        "authors": [
            "Donna M. Thompson",
            "Alberto Accomazzi",
            "Guenther Eichhorn",
            "Carolyn Grant",
            "Edwin Henneken",
            "Michael J. Kurtz",
            "Elizabeth Bohlen",
            "Stephen S. Murray"
        ],
        "summary": "The NASA Astrophysics Data System in conjunction with the Wolbach Library at the Harvard-Smithsonian Center for Astrophysics is working on a project to microfilm historical observatory publications. The microfilm is then scanned for inclusion in the ADS. The ADS currently contains over 700,000 scanned pages of volumes of historical literature. Many of these volumes lack clear pagination or other bibliographic data that are necessary to take advantage of the searching capabilities of the ADS. This paper will address some of the interesting challenges that needed to be resolved during the processing of the Observatory Reports included in the ADS.",
        "published": "2006-10-05T18:58:26Z",
        "link": "http://arxiv.org/abs/cs/0610030v1",
        "categories": [
            "cs.DL",
            "cs.HC"
        ]
    },
    {
        "title": "Interactive Problem Solving in Prolog",
        "authors": [
            "Erik Braun",
            "Rainer Luetticke",
            "Ingo Gloeckner",
            "Hermann Helbig"
        ],
        "summary": "This paper presents an environment for solving Prolog problems which has been implemented as a module for the virtual laboratory VILAB. During the problem solving processes the learners get fast adaptive feedback. As a result analysing the learner's actions the system suggests the use of suitable auxiliary predicates which will also be checked for proper implementation. The focus of the environment has been set on robustness and the integration in VILAB.",
        "published": "2006-11-03T14:34:57Z",
        "link": "http://arxiv.org/abs/cs/0611014v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "cs.PL",
            "K.3.1; K.3.2; D.1.6; I.2.6; H.5.2"
        ]
    },
    {
        "title": "CSCR:Computer Supported Collaborative Research",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "It is suggested that a new area of CSCR (Computer Supported Collaborative Research) is distinguished from CSCW and CSCL and that the demarcation between the three areas could do with greater clarification and prescription.",
        "published": "2006-11-09T21:10:32Z",
        "link": "http://arxiv.org/abs/cs/0611042v1",
        "categories": [
            "cs.HC",
            "cs.LG"
        ]
    },
    {
        "title": "Collaborative design : managing task interdependencies and multiple   perspectives",
        "authors": [
            "Françoise Détienne"
        ],
        "summary": "This paper focuses on two characteristics of collaborative design with respect to cooperative work: the importance of work interdependencies linked to the nature of design problems; and the fundamental function of design cooperative work arrangement which is the confrontation and combination of perspectives. These two intrinsic characteristics of the design work stress specific cooperative processes: coordination processes in order to manage task interdependencies, establishment of common ground and negotiation mechanisms in order to manage the integration of multiple perspectives in design.",
        "published": "2006-11-29T16:22:18Z",
        "link": "http://arxiv.org/abs/cs/0611151v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Viewpoints in co-design: a field study in concurrent engineering",
        "authors": [
            "Françoise Détienne",
            "Géraldine Martin",
            "Elisabeth Lavigne"
        ],
        "summary": "We present a field study aimed at analysing the use of viewpoints in co-design meetings. A viewpoint is a representation characterised by a certain combination of constraints. Three types of viewpoints are distinguished: prescribed viewpoint, discipline-specific viewpoint and integrated viewpoint. The contribution of our work consists in characterising the viewpoints of various stakeholders involved in co-design (\"design office\" disciplines, and production and maintenance disciplines), the dynamics of viewpoints confrontation and the cooperative modes that enable these different viewpoints to be integrated.",
        "published": "2006-11-29T16:24:20Z",
        "link": "http://arxiv.org/abs/cs/0611152v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Changing our view on design evaluation meetings methodology: a study of   software technical review meetings",
        "authors": [
            "Patrick D'Astous",
            "Françoise Détienne",
            "Willemien Visser",
            "Pierre Robillard"
        ],
        "summary": "By contrast to design meetings, design evaluation meetings (DEMs) have generally been considered as situations in which, according to DEMs methodologies, design activities are quite marginal. In a study of DEMs in software development, i.e. in technical review meetings following a particular review methodology, we showed: (i) the occurrence of design activities as part of an argumentation process; (ii) the relative importance of cognitive synchronisation as a prerequisite for evaluation; (iii) the important role played in evaluation by argumentation that makes explicit the underlying design rationale (DR). On the basis of our results, we discuss the potential for using DR methodologies in this kind of meetings.",
        "published": "2006-11-29T16:55:30Z",
        "link": "http://arxiv.org/abs/cs/0611153v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Assessing the cognitive consequences of the object-oriented approach: a   survey of empirical research on object-oriented design by individuals and   teams",
        "authors": [
            "Françoise Détienne"
        ],
        "summary": "This paper presents a state-of-art review of empirical research on object-oriented (OO) design. Many claims about the cognitive benefits of the OO paradigm have been made by its advocates. These claims concern the ease of designing and reusing software at the individual level as well as the benefits of this paradigm at the team level. Since these claims are cognitive in nature, its seems important to assess them empirically. After a brief presentation of the main concepts of the OO paradigm, the claims about the superiority of OO design are outlined. The core of this paper consists of a review of empirical studies of OOD. We first discuss results concerning OOD by individuals. On the basis of empirical work, we (1) analyse the design activity of novice OO designers, (2) compare OO design with procedural design and, (3) discuss a typology of problems relevant for the OO approach. Then we assess the claims about naturalness and ease of OO design. The next part discusses results on OO software reuse. On the basis of empirical work, we (1) compare reuse in the OO versus procedural paradigm, (2) discuss the potential for OO software reuse and (3) analyse reuse activity in the OO paradigm. Then we assess claims on reusability. The final part reviews empirical work on OO design by teams. We present results on communication, coordination, knowledge dissemination and interactions with clients. Then we assess claims about OOD at the software design team level. In a general conclusion, we discuss the limitations of these studies and give some directions for future research.",
        "published": "2006-11-29T16:57:41Z",
        "link": "http://arxiv.org/abs/cs/0611154v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Cognitive Effort in Collective Software Design: Methodological   Perspectives in Cognitive Ergonomics",
        "authors": [
            "Françoise Détienne",
            "Jean-Marie Burkhardt",
            "Willemien Visser"
        ],
        "summary": "Empirical software engineering is concerned with measuring, or estimating, both the effort put into the software process and the quality of its product. We defend the idea that measuring process effort and product quality and establishing a relation between the two cannot be performed without a model of cognitive and collective activities involved in software design, and without measurement of these activities. This is the object of our field, i.e. Cognitive Ergonomics of design. After a brief presentation of its theoretical and methodological foundations, we will discuss a cognitive approach to design activities and its potential to provide new directions in ESE. Then we will present and discuss an illustration of the methodological directions we have proposed for the analysis and measurement of cognitive activities in the context of collective software design. The two situations analysed are technical review meetings, and Request For Comments-like procedures in Open Source Software design.",
        "published": "2006-11-30T09:50:30Z",
        "link": "http://arxiv.org/abs/cs/0611159v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Articulation entre élaboration de solutions et argumentation   polyphonique",
        "authors": [
            "Michael Baker",
            "Françoise Détienne",
            "Kristine Lundt",
            "Arnauld Séjourné"
        ],
        "summary": "In this paper, we propose an analytical framework that aims to bring out the nature of participants' contributions to co-design meetings, in a way that synthesises content and function dimensions, together with the dimension of dialogicality. We term the resulting global vision of contribution, the \"interactive profile\".",
        "published": "2006-11-30T13:28:29Z",
        "link": "http://arxiv.org/abs/cs/0611158v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Reuse of designs: Desperately seeking an interdisciplinary cognitive   approach",
        "authors": [
            "Willemien Visser",
            "Brigitte Trousse"
        ],
        "summary": "This text analyses the papers accepted for the workshop \"Reuse of designs: an interdisciplinary cognitive approach\". Several dimensions and questions considered as important (by the authors and/or by us) are addressed: What about the \"interdisciplinary cognitive\" character of the approaches adopted by the authors? Is design indeed a domain where the use of CBR is particularly suitable? Are there important distinctions between CBR and other approaches? Which types of knowledge -other than cases- is being, or might be, used in CBR systems? With respect to cases: are there different \"types\" of case and different types of case use? which formats are adopted for their representation? do cases have \"components\"? how are cases organised in the case memory? Concerning their retrieval: which types of index are used? on which types of relation is retrieval based? how does one retrieve only a selected number of cases, i.e., how does one retrieve only the \"best\" cases? which processes and strategies are used, by the system and by its user? Finally, some important aspects of CBR system development are shortly discussed: should CBR systems be assistance or autonomous systems? how can case knowledge be \"acquired\"? what about the empirical evaluation of CBR systems? The conclusion points out some lacking points: not much attention is paid to the user, and few papers have indeed adopted an interdisciplinary cognitive approach.",
        "published": "2006-11-30T22:38:49Z",
        "link": "http://arxiv.org/abs/cs/0612002v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "Object-Oriented Program Comprehension: Effect of Expertise, Task and   Phase",
        "authors": [
            "Jean-Marie Burkhardt",
            "Françoise Détienne",
            "Susan Wiedenbeck"
        ],
        "summary": "The goal of our study is to evaluate the effect on program comprehension of three factors that have not previously been studied in a single experiment. These factors are programmer expertise (expert vs. novice), programming task (documentation vs. reuse), and the development of understanding over time (phase 1 vs. phase 2). This study is carried out in the context of the mental model approach to comprehension based on van Dijk and Kintsch's model (1983). One key aspect of this model is the distinction between two kinds of representation the reader might construct from a text: 1) the textbase, which refers to what is said in the text and how it is said, and 2) the situation model, which represents the situation referred to by the text. We have evaluated the effect of the three factors mentioned above on the development of both the textbase (or program model) and the situation model in object-oriented program comprehension. We found a four-way interaction of expertise, phase, task and type of model. For the documentation group we found that experts and novices differ in the elaboration of their situation model but not their program model. There was no interaction of expertise with phase and type of model in the documentation group. For the reuse group, there was a three-way interaction between phase, expertise and type of model. For the novice reuse group, the effect of the phase was to increase the construction of the situation model but not the program model. With respect to the task, our results show that novices do not spontaneously construct a strong situation model but are able to do so if the task demands it.",
        "published": "2006-11-30T22:42:06Z",
        "link": "http://arxiv.org/abs/cs/0612004v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Quantitative Measurements of the Influence of Participant Roles during   Peer Review Meetings",
        "authors": [
            "Patrick D'Astous",
            "Pierre Robillard",
            "Françoise Détienne",
            "Willemien Visser"
        ],
        "summary": "Peer review meetings (PRMs) are formal meetings during which peers systematically analyze artifacts to improve their quality and report on non-conformities. This paper presents an approach based on protocol analysis for quantifying the influence of participant roles during PRMs. Three views are used to characterize the seven defined participant roles. The project view defines three roles supervisor, procedure expert and developer. The meeting view defines two roles: author and reviewer, and the task view defines the roles reflecting direct and indirect interest in the artifact under review. The analysis, based on log-linear modeling, shows that review activities have different patterns, depending on their focus: form or content. The influence of each role is analyzed with respect to this focus. Interpretation of the quantitative data leads to the suggestion that PRMs could be improved by creating three different types of reviews, each of which collects together specific roles: form review, cognitive synchronization review and content review.",
        "published": "2006-11-30T22:43:22Z",
        "link": "http://arxiv.org/abs/cs/0612005v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Evocation and elaboration of solutions: Different types of   problem-solving actions. An empirical study on the design of an aerospace   artifact",
        "authors": [
            "Willemien Visser"
        ],
        "summary": "An observational study was conducted on a professional designer working on a design project in aerospace industry. The protocol data were analyzed in order to gain insight into the actions the designer used for the development of a solution to the corresponding problem. Different processes are described: from the \"simple\" evocation of a solution existing in memory, to the elaboration of a \"new\" solution out of mnesic entities without any clear link to the current problem. Control is addressed in so far as it concerns the priority among the different types of development processes: the progression from evocation of a \"standard\" solution to elaboration of a \"new\" solution is supposed to correspond to the resulting order, that is, the one in which the designer's activity proceeds. Short discussions of * the double status of \"problem\" and \"solution,\" * the problem/solution knowledge units in memory and their access, and * the different abstraction levels on which problem and solution representations are developed, are illustrated by the results.",
        "published": "2006-11-30T22:49:11Z",
        "link": "http://arxiv.org/abs/cs/0612006v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Design Strategies and Knowledge in Object-Oriented Programming: Effects   of Experience",
        "authors": [
            "Françoise Détienne"
        ],
        "summary": "An empirical study was conducted to analyse design strategies and knowledge used in object-oriented software design. Eight professional programmers experienced with procedural programming languages and either experienced or not experienced in object-oriented design strategies related to two central aspects of the object-oriented paradigm: (1) associating actions, i.e., execution steps, of a complex plan to different objects and revising a complex plan, and (2) defining simple plans at different levels in the class hierarchy. As regards the development of complex plans elements attached to different objects, our results show that, for beginners in OOP, the description of objects and the description of actions are not always integrated in an early design phase, particularly for the declarative problem whereas, for the programmers experienced in OOP, the description of objects and the description of actions tend to be integrated in their first drafts of solutions whichever the problem type. The analysis of design strategies reveal the use of different knowledge according to subjects' language experience: (1) schemas related to procedural languages; actions are organized in an execution order, or (2) schemas related to object-oriented languages; actions and objects are integrated, and actions are organised around objects.",
        "published": "2006-12-01T11:42:02Z",
        "link": "http://arxiv.org/abs/cs/0612008v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Users' participation to the design process in an Open Source Software   online community",
        "authors": [
            "Flore Barcellini",
            "Françoise Détienne",
            "Jean-Marie Burkhardt"
        ],
        "summary": "The objective of this research is to analyse the ways members of open-source software communities participate in design. In particular we focus on how users of an Open Source (OS) programming language (Python) participate in adding new functionalities to the language. Indeed, in the OS communities, users are highly skilled in computer sciences; they do not correspond to the common representation of end-users and can potentially participate to the design process. Our study characterizes the Python galaxy and analyses a formal process to introduce new functionalities to the language called Python Enhancement Proposal (PEP) from the idea of language evolution to the PEP implementation. The analysis of a particular pushed-by-users PEP from one application domain community (financial), shows: that the design process is distributed and specialized between online and physical interactions spaces; and there are some cross participants between users and developers communities which may reveal boundary spanners roles.",
        "published": "2006-12-01T11:42:44Z",
        "link": "http://arxiv.org/abs/cs/0612009v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Articulation entre composantes verbale et graphico-gestuelle de   l'interaction dans des réunions de conception architecturale",
        "authors": [
            "Willemien Visser",
            "Françoise Détienne"
        ],
        "summary": "This study is focused on the role of external representations, e.g., skteches, in collaborative architectural design. In particular, we analyse (1) the use of graphico-gestural modalities and, (2) the articulation modes between graphico-gestural and verbal modalities in design interaction. We have elaborated a first classification which distinguishes between two modes of articulation, articulation in integrated activities versus articulation in parallel activities.",
        "published": "2006-12-01T11:43:08Z",
        "link": "http://arxiv.org/abs/cs/0612010v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Memory of past designs: distinctive roles in individual and collective   design",
        "authors": [
            "Françoise Détienne"
        ],
        "summary": "Empirical studies on design have emphasised the role of memory of past solutions. Design involves the use of generic knowledge as well as episodic knowledge about past designs for analogous problems : in this way, it involves the reuse of past designs. We analyse this mechanism of reuse from a socio-cognitive viewpoint. According to a purely cognitive approach, reuse involves cognitive mechanisms linked to the problem solving activity itself. Our socio-cognitive approach accounts for these phenomena as well as reuse mechanisms linked to cooperation, in particular coordination, and confrontation/integration of viewpoints.",
        "published": "2006-12-04T13:48:22Z",
        "link": "http://arxiv.org/abs/cs/0612016v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Confrontation of viewpoints in a concurrent engineering process",
        "authors": [
            "Géraldine Martin",
            "Françoise Détienne",
            "Elisabeth Lavigne"
        ],
        "summary": "We present an empirical study aimed at analysing the use of viewpoints in an industrial Concurrent Engineering context. Our focus is on the viewpoints expressed in the argumentative process taking place in evaluation meetings. Our results show that arguments enabling a viewpoint or proposal to be defended are often characterized by the use of constraints. One result involved the way in which the proposals for solutions are assessed during these meetings. We have revealed the existence of specific assessment modes in these meetings as well as their combination. Then, we show that, even if some constraints are apparently identically used by the different specialists involved in meetings, various meanings and weightings are associated with these constraints by these different specialists.",
        "published": "2006-12-04T13:49:09Z",
        "link": "http://arxiv.org/abs/cs/0612017v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Mental Representations Constructed by Experts and Novices in   Object-Oriented Program Comprehension",
        "authors": [
            "Jean-Marie Burkhardt",
            "Françoise Détienne",
            "Susan Wiedenbeck"
        ],
        "summary": "Previous studies on program comprehension were carried out largely in the context of procedural languages. Our purpose is to develop and evaluate a cognitive model of object-oriented (OO) program understanding. Our model is based on the van Dijk and Kintsch's model of text understanding (1983). One key aspect of this theoretical approach is the distinction between two kinds of representation the reader might construct from a text: the textbase and the situation model. On the basis of results of an experiment we have conducted, we evaluate the cognitive validity of this distinction in OO program understanding. We examine how the construction of these two representations is differentially affected by the programmer's expertise and how they evolve differentially over time.",
        "published": "2006-12-04T13:50:07Z",
        "link": "http://arxiv.org/abs/cs/0612018v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Multimodality and parallelism in design interaction: co-designers'   alignment and coalitions",
        "authors": [
            "Françoise Détienne",
            "Willemien Visser"
        ],
        "summary": "This paper presents an analysis of various forms of articulation between graphico-gestural and verbal modalities in parallel interactions between designers in a collaborative design situation. Based on our methodological framework, we illustrate several forms of multimodal articulations, that is, integrated and non-integrated, through extracts from a corpus on an architectural design meeting. These modes reveal alignment or disalignment between designers, with respect to the focus of their activities. They also show different forms of coalition.",
        "published": "2006-12-04T16:27:32Z",
        "link": "http://arxiv.org/abs/cs/0612021v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Both Generic Design and Different Forms of Designing",
        "authors": [
            "Willemien Visser"
        ],
        "summary": "This paper defends an augmented cognitively oriented \"generic-design hypothesis\": There are both significant similarities between the design activities implemented in different situations and crucial differences between these and other cognitive activities; yet, characteristics of a design situation (i.e., related to the designers, the artefact, and other task variables influencing these two) introduce specificities in the corresponding design activities and cognitive structures that are used. We thus combine the generic-design hypothesis with that of different \"forms\" of designing. In this paper, outlining a number of directions that need further elaboration, we propose a series of candidate dimensions underlying such forms of design.",
        "published": "2006-12-04T16:28:44Z",
        "link": "http://arxiv.org/abs/cs/0612022v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Reusing processes and documenting processes: toward an integrated   framework",
        "authors": [
            "Françoise Détienne",
            "Jean-François Rouet",
            "Jean-Marie Burkhardt",
            "Catherine Deleuze-Dordron"
        ],
        "summary": "This paper presents a cognitive typology of reuse processes, and a cognitive typology of documenting processes. Empirical studies on design with reuse and on software documenting provide evidence for a generalized cognitive model. First, these studies emphasize the cyclical nature of design: cycles of planning, writing and revising occur. Second, natural language documentation follows the hierarchy of cognitive entities manipulated during design. Similarly software reuse involves exploiting various types of knowledge depending on the phase of design in which reuse is involved. We suggest that these observations can be explained based on cognitive models of text processing: the van Dijk and Kintsch (1983) model of text comprehension, and the Hayes and Flower (1980) model of text production. Based on our generalized cognitive model, we suggest a framework for documenting reusable components.",
        "published": "2006-12-04T18:45:55Z",
        "link": "http://arxiv.org/abs/cs/0612023v2",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "Social Networks and Social Information Filtering on Digg",
        "authors": [
            "Kristina Lerman"
        ],
        "summary": "The new social media sites -- blogs, wikis, Flickr and Digg, among others -- underscore the transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. Digg is a social news aggregator which allows users to submit links to, vote on and discuss news stories. Each day Digg selects a handful of stories to feature on its front page. Rather than rely on the opinion of a few editors, Digg aggregates opinions of thousands of its users to decide which stories to promote to the front page.   Digg users can designate other users as ``friends'' and easily track friends' activities: what new stories they submitted, commented on or read. The friends interface acts as a \\emph{social filtering} system, recommending to user stories his or her friends liked or found interesting. By tracking the votes received by newly submitted stories over time, we showed that social filtering is an effective information filtering approach. Specifically, we showed that (a) users tend to like stories submitted by friends and (b) users tend to like stories their friends read and liked. As a byproduct of social filtering, social networks also play a role in promoting stories to Digg's front page, potentially leading to ``tyranny of the minority'' situation where a disproportionate number of front page stories comes from the same small group of interconnected users. Despite this, social filtering is a promising new technology that can be used to personalize and tailor information to individual users: for example, through personal front pages.",
        "published": "2006-12-07T23:38:23Z",
        "link": "http://arxiv.org/abs/cs/0612046v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Social Browsing on Flickr",
        "authors": [
            "Kristina Lerman",
            "Laurie Jones"
        ],
        "summary": "The new social media sites - blogs, wikis, del.icio.us and Flickr, among others - underscore the transformation of the Web to a participatory medium in which users are actively creating, evaluating and distributing information. The photo-sharing site Flickr, for example, allows users to upload photographs, view photos created by others, comment on those photos, etc. As is common to other social media sites, Flickr allows users to designate others as ``contacts'' and to track their activities in real time. The contacts (or friends) lists form the social network backbone of social media sites. We claim that these social networks facilitate new ways of interacting with information, e.g., through what we call social browsing. The contacts interface on Flickr enables users to see latest images submitted by their friends. Through an extensive analysis of Flickr data, we show that social browsing through the contacts' photo streams is one of the primary methods by which users find new images on Flickr. This finding has implications for creating personalized recommendation systems based on the user's declared contacts lists.",
        "published": "2006-12-07T23:41:51Z",
        "link": "http://arxiv.org/abs/cs/0612047v1",
        "categories": [
            "cs.HC",
            "cs.AI"
        ]
    },
    {
        "title": "Personal Information Ecosystems and Implications for Design",
        "authors": [
            "Manas Tungare",
            "Pardha S. Pyla",
            "Manuel Pérez-Quiñones",
            "Steve Harrison"
        ],
        "summary": "Today, people use multiple devices to fulfill their information needs. However, designers design each device individually, without accounting for the other devices that users may also use. In many cases, the applications on all these devices are designed to be functional replicates of each other. We argue that this results in an over-reliance on data synchronization across devices, version control nightmares, and increased burden of file management. In this paper, we present the idea of a \\textit{personal information ecosystem}, an analogy to biological ecosystems, which allows us to discuss the inter-relationships among these devices to fulfill the information needs of the user. There is a need for designers to design devices as part of a complete ecosystem, not as independent devices that simply share data replicated across them. To help us understand this domain and to facilitate the dialogue and study of such systems, we present the terminology, classifications of the interdependencies among different devices, and resulting implications for design.",
        "published": "2006-12-18T07:53:34Z",
        "link": "http://arxiv.org/abs/cs/0612081v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "A Review of Papers that have a bearing on an Analysis of User   Interactions in A Collaborative On-line Laboratory",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "A number of papers have been reviewed in the areas of HCI, CSCW, CSCL. These have been analyzed with a view to extract the ideas relevant to a consideration of user interactions in a collaborative on line laboratory which is being under development for use in the ITO BSc course at Southampton University. The construction of new theoretical models is to be based upon principles of collaborative HCI design and constructivist and situational educational theory. An investigation of the review papers it is hoped will lead towards a methodology/framework that can be used as guidance for collaborative learning systems and these will need to be developed alongside the requirements as they change during the development cycles. The primary outcome will be the analysis and re-design of the online e-learning laboratory together with a measure of its efficacy in the learning process.",
        "published": "2006-12-18T22:24:02Z",
        "link": "http://arxiv.org/abs/cs/0612090v1",
        "categories": [
            "cs.HC"
        ]
    }
]