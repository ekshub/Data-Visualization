[
    {
        "title": "TDLeaf(lambda): Combining Temporal Difference Learning with Game-Tree   Search",
        "authors": [
            "Jonathan Baxter",
            "Andrew Tridgell",
            "Lex Weaver"
        ],
        "summary": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda) algorithm that enables it to be used in conjunction with minimax search. We present some experiments in both chess and backgammon which demonstrate its utility and provide comparisons with TD(lambda) and another less radical variant, TD-directed(lambda). In particular, our chess program, ``KnightCap,'' used TDLeaf(lambda) to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). It improved from a 1650 rating to a 2100 rating in just 308 games. We discuss some of the reasons for this success and the relationship between our results and Tesauro's results in backgammon.",
        "published": "1999-01-05T00:56:54Z",
        "link": "http://arxiv.org/abs/cs/9901001v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "KnightCap: A chess program that learns by combining TD(lambda) with   game-tree search",
        "authors": [
            "Jonathan Baxter",
            "Andrew Tridgell",
            "Lex Weaver"
        ],
        "summary": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda) algorithm that enables it to be used in conjunction with game-tree search. We present some experiments in which our chess program ``KnightCap'' used TDLeaf(lambda) to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). The main success we report is that KnightCap improved from a 1650 rating to a 2150 rating in just 308 games and 3 days of play. As a reference, a rating of 1650 corresponds to about level B human play (on a scale from E (1000) to A (1800)), while 2150 is human master level. We discuss some of the reasons for this success, principle among them being the use of on-line, rather than self-play.",
        "published": "1999-01-10T03:21:23Z",
        "link": "http://arxiv.org/abs/cs/9901002v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Fixpoint 3-valued semantics for autoepistemic logic",
        "authors": [
            "M. Denecker",
            "V. Marek",
            "M. Truszczynski"
        ],
        "summary": "The paper presents a constructive fixpoint semantics for autoepistemic logic (AEL). This fixpoint characterizes a unique but possibly three-valued belief set of an autoepistemic theory. It may be three-valued in the sense that for a subclass of formulas F, the fixpoint may not specify whether F is believed or not. The paper presents a constructive 3-valued semantics for autoepistemic logic (AEL). We introduce a derivation operator and define the semantics as its least fixpoint. The semantics is 3-valued in the sense that, for some formulas, the least fixpoint does not specify whether they are believed or not. We show that complete fixpoints of the derivation operator correspond to Moore's stable expansions. In the case of modal representations of logic programs our least fixpoint semantics expresses well-founded semantics or 3-valued Fitting-Kunen semantics (depending on the embedding used). We show that, computationally, our semantics is simpler than the semantics proposed by Moore (assuming that the polynomial hierarchy does not collapse).",
        "published": "1999-01-12T18:44:40Z",
        "link": "http://arxiv.org/abs/cs/9901003v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.4, F.4.1, I.2.3"
        ]
    },
    {
        "title": "Extremal problems in logic programming and stable model computation",
        "authors": [
            "Pawel Cholewinski",
            "Miroslaw Truszczynski"
        ],
        "summary": "We study the following problem: given a class of logic programs C, determine the maximum number of stable models of a program from C. We establish the maximum for the class of all logic programs with at most n clauses, and for the class of all logic programs of size at most n. We also characterize the programs for which the maxima are attained. We obtain similar results for the class of all disjunctive logic programs with at most n clauses, each of length at most m, and for the class of all disjunctive logic programs of size at most n. Our results on logic programs have direct implication for the design of algorithms to compute stable models. Several such algorithms, similar in spirit to the Davis-Putnam procedure, are described in the paper. Our results imply that there is an algorithm that finds all stable models of a program with n clauses after considering the search space of size O(3^{n/3}) in the worst case. Our results also provide some insights into the question of representability of families of sets as families of stable models of logic programs.",
        "published": "1999-01-25T14:44:20Z",
        "link": "http://arxiv.org/abs/cs/9901012v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Minimum Description Length Induction, Bayesianism, and Kolmogorov   Complexity",
        "authors": [
            "Paul Vitanyi",
            "Ming Li"
        ],
        "summary": "The relationship between the Bayesian approach and the minimum description length approach is established. We sharpen and clarify the general modeling principles MDL and MML, abstracted as the ideal MDL principle and defined from Bayes's rule by means of Kolmogorov complexity. The basic condition under which the ideal principle should be applied is encapsulated as the Fundamental Inequality, which in broad terms states that the principle is valid when the data are random, relative to every contemplated hypothesis and also these hypotheses are random relative to the (universal) prior. Basically, the ideal principle states that the prior probability associated with the hypothesis should be given by the algorithmic universal probability, and the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized. If we restrict the model class to the finite sets then application of the ideal principle turns into Kolmogorov's minimal sufficient statistic. In general we show that data compression is almost always the best strategy, both in hypothesis identification and prediction.",
        "published": "1999-01-27T17:48:14Z",
        "link": "http://arxiv.org/abs/cs/9901014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LO",
            "math.IT",
            "math.PR",
            "physics.data-an",
            "E.4,F.2,H.3,I.2,I.5,I.7"
        ]
    },
    {
        "title": "Representation Theory for Default Logic",
        "authors": [
            "Victor Marek",
            "Jan Treur",
            "Miroslaw Truszczynski"
        ],
        "summary": "Default logic can be regarded as a mechanism to represent families of belief sets of a reasoning agent. As such, it is inherently second-order. In this paper, we study the problem of representability of a family of theories as the set of extensions of a default theory. We give a complete solution to the representability by means of normal default theories. We obtain partial results on representability by arbitrary default theories. We construct examples of denumerable families of non-including theories that are not representable. We also study the concept of equivalence between default theories.",
        "published": "1999-01-28T21:57:15Z",
        "link": "http://arxiv.org/abs/cs/9901016v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.4, F.4.1, I.2.3"
        ]
    },
    {
        "title": "A Discipline of Evolutionary Programming",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Genetic fitness optimization using small populations or small population updates across generations generally suffers from randomly diverging evolutions. We propose a notion of highly probable fitness optimization through feasible evolutionary computing runs on small size populations. Based on rapidly mixing Markov chains, the approach pertains to most types of evolutionary genetic algorithms, genetic programming and the like. We establish that for systems having associated rapidly mixing Markov chains and appropriate stationary distributions the new method finds optimal programs (individuals) with probability almost 1. To make the method useful would require a structured design methodology where the development of the program and the guarantee of the rapidly mixing property go hand in hand. We analyze a simple example to show that the method is implementable. More significant examples require theoretical advances, for example with respect to the Metropolis filter.",
        "published": "1999-02-02T16:17:16Z",
        "link": "http://arxiv.org/abs/cs/9902006v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.MA",
            "I.2,E.1,F.1"
        ]
    },
    {
        "title": "Resource Discovery in Trilogy",
        "authors": [
            "Franck Chevalier",
            "David Harle",
            "Geoffrey Smith"
        ],
        "summary": "Trilogy is a collaborative project whose key aim is the development of an integrated virtual laboratory to support research training within each institution and collaborative projects between the partners. In this paper, the architecture and underpinning platform of the system is described with particular emphasis being placed on the structure and the integration of the distributed database. A key element is the ontology that provides the multi-agent system with a conceptualisation specification of the domain; this ontology is explained, accompanied by a discussion how such a system is integrated and used within the virtual laboratory. Although in this paper, Telecommunications and in particular Broadband networks are used as exemplars, the underlying system principles are applicable to any domain where a combination of experimental and literature-based resources are required.",
        "published": "1999-02-08T21:23:39Z",
        "link": "http://arxiv.org/abs/cs/9902015v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.MA",
            "H.3.4;I.2.0"
        ]
    },
    {
        "title": "An Algebraic Programming Style for Numerical Software and its   Optimization",
        "authors": [
            "T. B. Dinesh",
            "M. Haveraaen",
            "J. Heering"
        ],
        "summary": "The abstract mathematical theory of partial differential equations (PDEs) is formulated in terms of manifolds, scalar fields, tensors, and the like, but these algebraic structures are hardly recognizable in actual PDE solvers. The general aim of the Sophus programming style is to bridge the gap between theory and practice in the domain of PDE solvers. Its main ingredients are a library of abstract datatypes corresponding to the algebraic structures used in the mathematical theory and an algebraic expression style similar to the expression style used in the mathematical theory. Because of its emphasis on abstract datatypes, Sophus is most naturally combined with object-oriented languages or other languages supporting abstract datatypes. The resulting source code patterns are beyond the scope of current compiler optimizations, but are sufficiently specific for a dedicated source-to-source optimizer. The limited, domain-specific, character of Sophus is the key to success here. This kind of optimization has been tested on computationally intensive Sophus style code with promising results. The general approach may be useful for other styles and in other application domains as well.",
        "published": "1999-03-01T11:03:47Z",
        "link": "http://arxiv.org/abs/cs/9903002v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "D.1.5; D.2.2; J.2"
        ]
    },
    {
        "title": "A complete anytime algorithm for balanced number partitioning",
        "authors": [
            "Stephan Mertens"
        ],
        "summary": "Given a set of numbers, the balanced partioning problem is to divide them into two subsets, so that the sum of the numbers in each subset are as nearly equal as possible, subject to the constraint that the cardinalities of the subsets be within one of each other. We combine the balanced largest differencing method (BLDM) and Korf's complete Karmarkar-Karp algorithm to get a new algorithm that optimally solves the balanced partitioning problem. For numbers with twelve significant digits or less, the algorithm can optimally solve balanced partioning problems of arbitrary size in practice. For numbers with greater precision, it first returns the BLDM solution, then continues to find better solutions as time allows.",
        "published": "1999-03-11T22:38:01Z",
        "link": "http://arxiv.org/abs/cs/9903011v1",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.AI",
            "F.2.2"
        ]
    },
    {
        "title": "Modeling Belief in Dynamic Systems, Part II: Revision and Update",
        "authors": [
            "N Friedman",
            "J. Y. Halpern"
        ],
        "summary": "The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper (Friedman & Halpern, 1997), we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.",
        "published": "1999-03-24T00:22:01Z",
        "link": "http://arxiv.org/abs/cs/9903016v1",
        "categories": [
            "cs.AI",
            "I.2"
        ]
    },
    {
        "title": "Mixing Metaphors",
        "authors": [
            "Mark Lee",
            "John Barnden"
        ],
        "summary": "Mixed metaphors have been neglected in recent metaphor research. This paper suggests that such neglect is short-sighted. Though mixing is a more complex phenomenon than straight metaphors, the same kinds of reasoning and knowledge structures are required. This paper provides an analysis of both parallel and serial mixed metaphors within the framework of an AI system which is already capable of reasoning about straight metaphorical manifestations and argues that the processes underlying mixing are central to metaphorical meaning. Therefore, any theory of metaphors must be able to account for mixing.",
        "published": "1999-04-12T11:37:49Z",
        "link": "http://arxiv.org/abs/cs/9904004v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.0; I.2.7"
        ]
    },
    {
        "title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering",
        "authors": [
            "Mats Rooth",
            "Stefan Riezler",
            "Detlef Prescher",
            "Glenn Carroll",
            "Franz Beil"
        ],
        "summary": "We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",
        "published": "1999-05-19T14:52:33Z",
        "link": "http://arxiv.org/abs/cs/9905008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.6; I.2.7; I.5.3"
        ]
    },
    {
        "title": "The Symbol Grounding Problem",
        "authors": [
            "Stevan Harnad"
        ],
        "summary": "How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) \"iconic representations,\" which are analogs of the proximal sensory projections of distal objects and events, and (2) \"categorical representations,\" which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) \"symbolic representations,\" grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., \"An X is a Y that is Z\").",
        "published": "1999-06-01T19:57:24Z",
        "link": "http://arxiv.org/abs/cs/9906002v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Learning Efficient Disambiguation",
        "authors": [
            "Khalil Sima'an"
        ],
        "summary": "This dissertation analyses the computational properties of current performance-models of natural language parsing, in particular Data Oriented Parsing (DOP), points out some of their major shortcomings and suggests suitable solutions. It provides proofs that various problems of probabilistic disambiguation are NP-Complete under instances of these performance-models, and it argues that none of these models accounts for attractive efficiency properties of human language processing in limited domains, e.g. that frequent inputs are usually processed faster than infrequent ones. The central hypothesis of this dissertation is that these shortcomings can be eliminated by specializing the performance-models to the limited domains. The dissertation addresses \"grammar and model specialization\" and presents a new framework, the Ambiguity-Reduction Specialization (ARS) framework, that formulates the necessary and sufficient conditions for successful specialization. The framework is instantiated into specialization algorithms and applied to specializing DOP. Novelties of these learning algorithms are 1) they limit the hypotheses-space to include only \"safe\" models, 2) are expressed as constrained optimization formulae that minimize the entropy of the training tree-bank given the specialized grammar, under the constraint that the size of the specialized model does not exceed a predefined maximum, and 3) they enable integrating the specialized model with the original one in a complementary manner. The dissertation provides experiments with initial implementations and compares the resulting Specialized DOP (SDOP) models to the original DOP models with encouraging results.",
        "published": "1999-06-02T15:50:26Z",
        "link": "http://arxiv.org/abs/cs/9906006v2",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.6, I.2.7, J.5, F.2"
        ]
    },
    {
        "title": "Predicate Logic with Definitions",
        "authors": [
            "Victor Makarov"
        ],
        "summary": "Predicate Logic with Definitions (PLD or D-logic) is a modification of first-order logic intended mostly for practical formalization of mathematics. The main syntactic constructs of D-logic are terms, formulas and definitions. A definition is a definition of variables, a definition of constants, or a composite definition (D-logic has also abbreviation definitions called abbreviations). Definitions can be used inside terms and formulas. This possibility alleviates introducing new quantifier-like names. Composite definitions allow constructing new definitions from existing ones.",
        "published": "1999-06-07T20:16:55Z",
        "link": "http://arxiv.org/abs/cs/9906010v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Automatically Selecting Useful Phrases for Dialogue Act Tagging",
        "authors": [
            "Ken Samuel",
            "Sandra Carberry",
            "K. Vijay-Shanker"
        ],
        "summary": "We present an empirical investigation of various ways to automatically identify phrases in a tagged corpus that are useful for dialogue act tagging. We found that a new method (which measures a phrase's deviation from an optimally-predictive phrase), enhanced with a lexical filtering mechanism, produces significantly better cues than manually-selected cue phrases, the exhaustive set of phrases in a training corpus, and phrases chosen by traditional metrics, like mutual information and information gain.",
        "published": "1999-06-18T03:25:03Z",
        "link": "http://arxiv.org/abs/cs/9906016v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "Resolving Part-of-Speech Ambiguity in the Greek Language Using Learning   Techniques",
        "authors": [
            "G. Petasis",
            "G. Paliouras",
            "V. Karkaletsis",
            "C. D. Spyropoulos",
            "I. Androutsopoulos"
        ],
        "summary": "This article investigates the use of Transformation-Based Error-Driven learning for resolving part-of-speech ambiguity in the Greek language. The aim is not only to study the performance, but also to examine its dependence on different thematic domains. Results are presented here for two different test cases: a corpus on \"management succession events\" and a general-theme corpus. The two experiments show that the performance of this method does not depend on the thematic domain of the corpus, and its accuracy for the Greek language is around 95%.",
        "published": "1999-06-22T07:41:24Z",
        "link": "http://arxiv.org/abs/cs/9906019v2",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.6 ; I.2.7"
        ]
    },
    {
        "title": "Events in Property Patterns",
        "authors": [
            "M. Chechik",
            "D. Paun"
        ],
        "summary": "A pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification was proposed by Dwyer and his collegues. The patterns enable non-experts to read and write formal specifications for realistic systems and facilitate easy conversion of specifications between formalisms, such as LTL, CTL, QRE. In this paper, we extend the pattern system with events - changes of values of variables in the context of LTL.",
        "published": "1999-06-28T17:06:51Z",
        "link": "http://arxiv.org/abs/cs/9906029v2",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CL",
            "cs.SC",
            "D.2.4;F.3.1;F.4.1;I.2.4;D.2.1"
        ]
    },
    {
        "title": "Mixing representation levels: The hybrid approach to automatic text   generation",
        "authors": [
            "Emanuele Pianta",
            "Lucia M. Tovena"
        ],
        "summary": "Natural language generation systems (NLG) map non-linguistic representations into strings of words through a number of steps using intermediate representations of various levels of abstraction. Template based systems, by contrast, tend to use only one representation level, i.e. fixed strings, which are combined, possibly in a sophisticated way, to generate the final text.   In some circumstances, it may be profitable to combine NLG and template based techniques. The issue of combining generation techniques can be seen in more abstract terms as the issue of mixing levels of representation of different degrees of linguistic abstraction. This paper aims at defining a reference architecture for systems using mixed representations. We argue that mixed representations can be used without abandoning a linguistically grounded approach to language generation.",
        "published": "1999-07-16T15:43:45Z",
        "link": "http://arxiv.org/abs/cs/9907026v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Clausal Temporal Resolution",
        "authors": [
            "Michael Fisher",
            "Clare Dixon",
            "Martin Peim"
        ],
        "summary": "In this article, we examine how clausal resolution can be applied to a specific, but widely used, non-classical logic, namely discrete linear temporal logic. Thus, we first define a normal form for temporal formulae and show how arbitrary temporal formulae can be translated into the normal form, while preserving satisfiability. We then introduce novel resolution rules that can be applied to formulae in this normal form, provide a range of examples and examine the correctness and complexity of this approach is examined and. This clausal resolution approach. Finally, we describe related work and future developments concerning this work.",
        "published": "1999-07-21T15:48:06Z",
        "link": "http://arxiv.org/abs/cs/9907032v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;F.4.1"
        ]
    },
    {
        "title": "Extending the Stable Model Semantics with More Expressive Rules",
        "authors": [
            "Patrik Simons"
        ],
        "summary": "The rules associated with propositional logic programs and the stable model semantics are not expressive enough to let one write concise programs. This problem is alleviated by introducing some new types of propositional rules. Together with a decision procedure that has been used as a base for an efficient implementation, the new rules supplant the standard ones in practical applications of the stable model semantics.",
        "published": "1999-08-06T06:01:43Z",
        "link": "http://arxiv.org/abs/cs/9908004v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3; I.2.8; F.4.1"
        ]
    },
    {
        "title": "Collective Intelligence for Control of Distributed Dynamical Systems",
        "authors": [
            "David H. Wolpert",
            "Kevin R. Wheeler",
            "Kagan Tumer"
        ],
        "summary": "We consider the El Farol bar problem, also known as the minority game (W. B. Arthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet and Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of the general problem of how to configure the nodal elements of a distributed dynamical system so that they do not ``work at cross purposes'', in that their collective dynamics avoids frustration and thereby achieves a provided global goal. We summarize a mathematical theory for such configuration applicable when (as in the bar problem) the global goal can be expressed as minimizing a global energy function and the nodes can be expressed as minimizers of local free energy functions. We show that a system designed with that theory performs nearly optimally for the bar problem.",
        "published": "1999-08-17T21:32:41Z",
        "link": "http://arxiv.org/abs/cs/9908013v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat",
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "Representing Scholarly Claims in Internet Digital Libraries: A Knowledge   Modelling Approach",
        "authors": [
            "Simon Buckingham Shum",
            "Enrico Motta",
            "John Domingue"
        ],
        "summary": "This paper is concerned with tracking and interpreting scholarly documents in distributed research communities. We argue that current approaches to document description, and current technological infrastructures particularly over the World Wide Web, provide poor support for these tasks. We describe the design of a digital library server which will enable authors to submit a summary of the contributions they claim their documents makes, and its relations to the literature. We describe a knowledge-based Web environment to support the emergence of such a community-constructed semantic hypertext, and the services it could provide to assist the interpretation of an idea or document in the context of its literature. The discussion considers in detail how the approach addresses usability issues associated with knowledge structuring environments.",
        "published": "1999-08-19T09:51:29Z",
        "link": "http://arxiv.org/abs/cs/9908015v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.3.7; H.1.2; H5.2; H.5.4; I.2.4; I.7.4"
        ]
    },
    {
        "title": "Iterative Deepening Branch and Bound",
        "authors": [
            "S. Mohanty",
            "R. N. Behera"
        ],
        "summary": "In tree search problem the best-first search algorithm needs too much of space . To remove such drawbacks of these algorithms the IDA* was developed which is both space and time cost efficient. But again IDA* can give an optimal solution for real valued problems like Flow shop scheduling, Travelling Salesman and 0/1 Knapsack due to their real valued cost estimates. Thus further modifications are done on it and the Iterative Deepening Branch and Bound Search Algorithms is developed which meets the requirements. We have tried using this algorithm for the Flow Shop Scheduling Problem and have found that it is quite effective.",
        "published": "1999-09-03T10:31:46Z",
        "link": "http://arxiv.org/abs/cs/9909003v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "The Rough Guide to Constraint Propagation",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In particular, using the notions commutativity and semi-commutativity, we show how the well-known AC-3, PC-2, DAC and DPC algorithms are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt, cs.AI/9811024.",
        "published": "1999-09-08T13:50:01Z",
        "link": "http://arxiv.org/abs/cs/9909009v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.3; I.1.2; I.2.2"
        ]
    },
    {
        "title": "Automatic Generation of Constraint Propagation Algorithms for Small   Finite Domains",
        "authors": [
            "Krzysztof R. Apt",
            "Eric Monfroy"
        ],
        "summary": "We study here constraint satisfaction problems that are based on predefined, explicitly given finite constraints. To solve them we propose a notion of rule consistency that can be expressed in terms of rules derived from the explicit representation of the initial constraints.   This notion of local consistency is weaker than arc consistency for constraints of arbitrary arity but coincides with it when all domains are unary or binary. For Boolean constraints rule consistency coincides with the closure under the well-known propagation rules for Boolean constraints.   By generalizing the format of the rules we obtain a characterization of arc consistency in terms of so-called inclusion rules. The advantage of rule consistency and this rule based characterization of the arc consistency is that the algorithms that enforce both notions can be automatically generated, as CHR rules. So these algorithms could be integrated into constraint logic programming systems such as Eclipse.   We illustrate the usefulness of this approach to constraint propagation by discussing the implementations of both algorithms and their use on various examples, including Boolean constraints, three valued logic of Kleene, constraints dealing with Waltz's language for describing polyhedreal scenes, and Allen's qualitative approach to temporal logic.",
        "published": "1999-09-08T14:18:47Z",
        "link": "http://arxiv.org/abs/cs/9909010v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.#.2; I.2.2; I.2.3"
        ]
    },
    {
        "title": "Reasoning About Common Knowledge with Infinitely Many Agents",
        "authors": [
            "Joseph Y. Halpern",
            "Richard A. Shore"
        ],
        "summary": "Complete axiomatizations and exponential-time decision procedures are provided for reasoning about knowledge and common knowledge when there are infinitely many agents. The results show that reasoning about knowledge and common knowledge with infinitely many agents is no harder than when there are finitely many agents, provided that we can check the cardinality of certain set differences G - G', where G and G' are sets of agents. Since our complexity results are independent of the cardinality of the sets G involved, they represent improvements over the previous results even with the sets of agents involved are finite. Moreover, our results make clear the extent to which issues of complexity and completeness depend on how the sets of agents involved are represented.",
        "published": "1999-09-21T20:43:46Z",
        "link": "http://arxiv.org/abs/cs/9909014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Knowledge in Multi-Agent Systems: Initial Configurations and Broadcast",
        "authors": [
            "A. R. Lomuscio",
            "R. van der Meyden",
            "M. D. Ryan"
        ],
        "summary": "The semantic framework for the modal logic of knowledge due to Halpern and Moses provides a way to ascribe knowledge to agents in distributed and multi-agent systems. In this paper we study two special cases of this framework: full systems and hypercubes. Both model static situations in which no agent has any information about another agent's state. Full systems and hypercubes are an appropriate model for the initial configurations of many systems of interest. We establish a correspondence between full systems and hypercube systems and certain classes of Kripke frames. We show that these classes of systems correspond to the same logic. Moreover, this logic is also the same as that generated by the larger class of weakly directed frames. We provide a sound and complete axiomatization, S5WDn, of this logic. Finally, we show that under certain natural assumptions, in a model where knowledge evolves over time, S5WDn characterizes the properties of knowledge not just at the initial configuration, but also at all later configurations. In particular, this holds for homogeneous broadcast systems, which capture settings in which agents are initially ignorant of each others local states, operate synchronously, have perfect recall and can communicate only by broadcasting.",
        "published": "1999-09-30T17:03:47Z",
        "link": "http://arxiv.org/abs/cs/9909019v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "PIPE: Personalizing Recommendations via Partial Evaluation",
        "authors": [
            "Naren Ramakrishnan"
        ],
        "summary": "It is shown that personalization of web content can be advantageously viewed as a form of partial evaluation --- a technique well known in the programming languages community. The basic idea is to model a recommendation space as a program, then partially evaluate this program with respect to user preferences (and features) to obtain specialized content. This technique supports both content-based and collaborative approaches, and is applicable to a range of applications that require automatic information integration from multiple web sources. The effectiveness of this methodology is illustrated by two example applications --- (i) personalizing content for visitors to the Blacksburg Electronic Village (http://www.bev.net), and (ii) locating and selecting scientific software on the Internet. The scalability of this technique is demonstrated by its ability to interface with online web ontologies that index thousands of web pages.",
        "published": "1999-10-18T15:47:29Z",
        "link": "http://arxiv.org/abs/cs/9910015v3",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.4.2"
        ]
    },
    {
        "title": "Probabilistic Agent Programs",
        "authors": [
            "Juergen Dix",
            "Mirco Nanni",
            "VS Subrahmanian"
        ],
        "summary": "Agents are small programs that autonomously take actions based on changes in their environment or ``state.'' Over the last few years, there have been an increasing number of efforts to build agents that can interact and/or collaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd Pick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top of legacy code. However, their framework assumes that agent states are completely determined, and there is no uncertainty in an agent's state. Thus, their framework allows an agent developer to specify how his agents will react when the agent is 100% sure about what is true/false in the world state. In this paper, we propose the concept of a \\emph{probabilistic agent program} and show how, given an arbitrary program written in any imperative language, we may build a declarative ``probabilistic'' agent program on top of it which supports decision making in the presence of uncertainty. We provide two alternative semantics for probabilistic agent programs. We show that the second semantics, though more epistemically appealing, is more complex to compute. We provide sound and complete algorithms to compute the semantics of \\emph{positive} agent programs.",
        "published": "1999-10-21T09:35:38Z",
        "link": "http://arxiv.org/abs/cs/9910016v1",
        "categories": [
            "cs.AI",
            "D.1.6"
        ]
    },
    {
        "title": "Cox's Theorem Revisited",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "The assumptions needed to prove Cox's Theorem are discussed and examined. Various sets of assumptions under which a Cox-style theorem can be proved are provided, although all are rather strong and, arguably, not natural.",
        "published": "1999-11-27T17:57:17Z",
        "link": "http://arxiv.org/abs/cs/9911012v2",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.7"
        ]
    },
    {
        "title": "New Error Bounds for Solomonoff Prediction",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff sequence prediction is a scheme to predict digits of binary strings without knowing the underlying probability distribution. We call a prediction scheme informed when it knows the true probability distribution of the sequence. Several new relations between universal Solomonoff sequence prediction and informed prediction and general probabilistic prediction schemes will be proved. Among others, they show that the number of errors in Solomonoff prediction is finite for computable distributions, if finite in the informed case. Deterministic variants will also be studied. The most interesting result is that the deterministic variant of Solomonoff prediction is optimal compared to any other probabilistic or deterministic prediction scheme apart from additive square root corrections only. This makes it well suited even for difficult prediction problems, where it does not suffice when the number of errors is minimal to within some factor greater than one. Solomonoff's original bound and the ones presented here complement each other in a useful way.",
        "published": "1999-12-13T08:33:43Z",
        "link": "http://arxiv.org/abs/cs/9912008v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6; F.1.3; E.4; F.2"
        ]
    },
    {
        "title": "TDLeaf(lambda): Combining Temporal Difference Learning with Game-Tree   Search",
        "authors": [
            "Jonathan Baxter",
            "Andrew Tridgell",
            "Lex Weaver"
        ],
        "summary": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda) algorithm that enables it to be used in conjunction with minimax search. We present some experiments in both chess and backgammon which demonstrate its utility and provide comparisons with TD(lambda) and another less radical variant, TD-directed(lambda). In particular, our chess program, ``KnightCap,'' used TDLeaf(lambda) to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). It improved from a 1650 rating to a 2100 rating in just 308 games. We discuss some of the reasons for this success and the relationship between our results and Tesauro's results in backgammon.",
        "published": "1999-01-05T00:56:54Z",
        "link": "http://arxiv.org/abs/cs/9901001v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "KnightCap: A chess program that learns by combining TD(lambda) with   game-tree search",
        "authors": [
            "Jonathan Baxter",
            "Andrew Tridgell",
            "Lex Weaver"
        ],
        "summary": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda) algorithm that enables it to be used in conjunction with game-tree search. We present some experiments in which our chess program ``KnightCap'' used TDLeaf(lambda) to learn its evaluation function while playing on the Free Internet Chess Server (FICS, fics.onenet.net). The main success we report is that KnightCap improved from a 1650 rating to a 2150 rating in just 308 games and 3 days of play. As a reference, a rating of 1650 corresponds to about level B human play (on a scale from E (1000) to A (1800)), while 2150 is human master level. We discuss some of the reasons for this success, principle among them being the use of on-line, rather than self-play.",
        "published": "1999-01-10T03:21:23Z",
        "link": "http://arxiv.org/abs/cs/9901002v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Minimum Description Length Induction, Bayesianism, and Kolmogorov   Complexity",
        "authors": [
            "Paul Vitanyi",
            "Ming Li"
        ],
        "summary": "The relationship between the Bayesian approach and the minimum description length approach is established. We sharpen and clarify the general modeling principles MDL and MML, abstracted as the ideal MDL principle and defined from Bayes's rule by means of Kolmogorov complexity. The basic condition under which the ideal principle should be applied is encapsulated as the Fundamental Inequality, which in broad terms states that the principle is valid when the data are random, relative to every contemplated hypothesis and also these hypotheses are random relative to the (universal) prior. Basically, the ideal principle states that the prior probability associated with the hypothesis should be given by the algorithmic universal probability, and the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized. If we restrict the model class to the finite sets then application of the ideal principle turns into Kolmogorov's minimal sufficient statistic. In general we show that data compression is almost always the best strategy, both in hypothesis identification and prediction.",
        "published": "1999-01-27T17:48:14Z",
        "link": "http://arxiv.org/abs/cs/9901014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LO",
            "math.IT",
            "math.PR",
            "physics.data-an",
            "E.4,F.2,H.3,I.2,I.5,I.7"
        ]
    },
    {
        "title": "A Discipline of Evolutionary Programming",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Genetic fitness optimization using small populations or small population updates across generations generally suffers from randomly diverging evolutions. We propose a notion of highly probable fitness optimization through feasible evolutionary computing runs on small size populations. Based on rapidly mixing Markov chains, the approach pertains to most types of evolutionary genetic algorithms, genetic programming and the like. We establish that for systems having associated rapidly mixing Markov chains and appropriate stationary distributions the new method finds optimal programs (individuals) with probability almost 1. To make the method useful would require a structured design methodology where the development of the program and the guarantee of the rapidly mixing property go hand in hand. We analyze a simple example to show that the method is implementable. More significant examples require theoretical advances, for example with respect to the Metropolis filter.",
        "published": "1999-02-02T16:17:16Z",
        "link": "http://arxiv.org/abs/cs/9902006v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.MA",
            "I.2,E.1,F.1"
        ]
    },
    {
        "title": "Probabilistic Inductive Inference:a Survey",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "Inductive inference is a recursion-theoretic theory of learning, first developed by E. M. Gold (1967). This paper surveys developments in probabilistic inductive inference. We mainly focus on finite inference of recursive functions, since this simple paradigm has produced the most interesting (and most complex) results.",
        "published": "1999-02-15T01:52:45Z",
        "link": "http://arxiv.org/abs/cs/9902026v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.LO",
            "math.LO",
            "F.1.1., F.4.1., I.2.3., I.2.6"
        ]
    },
    {
        "title": "Using Collective Intelligence to Route Internet Traffic",
        "authors": [
            "David H. Wolpert",
            "Kagan Tumer",
            "Jeremy Frank"
        ],
        "summary": "A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.",
        "published": "1999-05-10T20:52:23Z",
        "link": "http://arxiv.org/abs/cs/9905004v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat.stat-mech",
            "cs.DC",
            "cs.NI",
            "nlin.AO",
            "I.2.6; I.2.11"
        ]
    },
    {
        "title": "General Principles of Learning-Based Multi-Agent Systems",
        "authors": [
            "David H. Wolpert",
            "Kevin R. Wheeler",
            "Kagan Tumer"
        ],
        "summary": "We consider the problem of how to design large decentralized multi-agent systems (MAS's) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we do not want the agents to ``work at cross-purposes'' as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINs. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINs perform near optimally in a difficult variant of Arthur's bar problem (and in particular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINs in the leader-follower problem.",
        "published": "1999-05-10T22:20:40Z",
        "link": "http://arxiv.org/abs/cs/9905005v1",
        "categories": [
            "cs.MA",
            "adap-org",
            "cond-mat.stat-mech",
            "cs.DC",
            "cs.LG",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "An Efficient, Probabilistically Sound Algorithm for Segmentation and   Word Discovery",
        "authors": [
            "Michael R. Brent"
        ],
        "summary": "This paper presents a model-based, unsupervised algorithm for recovering word boundaries in a natural-language text from which they have been deleted. The algorithm is derived from a probability model of the source that generated the text. The fundamental structure of the model is specified abstractly so that the detailed component models of phonology, word-order, and word frequency can be replaced in a modular fashion. The model yields a language-independent, prior probability distribution on all possible sequences of all possible words over a given alphabet, based on the assumption that the input was generated by concatenating words from a fixed but unknown lexicon. The model is unusual in that it treats the generation of a complete corpus, regardless of length, as a single event in the probability space. Accordingly, the algorithm does not estimate a probability distribution on words; instead, it attempts to calculate the prior probabilities of various word sequences that could underlie the observed text. Experiments on phonemic transcripts of spontaneous speech by parents to young children suggest that this algorithm is more effective than other proposed algorithms, at least when utterance boundaries are given and the text includes a substantial number of short utterances.   Keywords: Bayesian grammar induction, probability models, minimum description length (MDL), unsupervised learning, cognitive modeling, language acquisition, segmentation",
        "published": "1999-05-12T14:25:40Z",
        "link": "http://arxiv.org/abs/cs/9905007v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.0;I.2.6;I.2.7"
        ]
    },
    {
        "title": "Inside-Outside Estimation of a Lexicalized PCFG for German",
        "authors": [
            "Franz Beil",
            "Glenn Carroll",
            "Detlef Prescher",
            "Stefan Riezler",
            "Mats Rooth"
        ],
        "summary": "The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verb-final clauses. Grammar and formalism features which make the experiment feasible are described. Successive models are evaluated on precision and recall of phrase markup.",
        "published": "1999-05-19T14:47:21Z",
        "link": "http://arxiv.org/abs/cs/9905009v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering",
        "authors": [
            "Mats Rooth",
            "Stefan Riezler",
            "Detlef Prescher",
            "Glenn Carroll",
            "Franz Beil"
        ],
        "summary": "We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",
        "published": "1999-05-19T14:52:33Z",
        "link": "http://arxiv.org/abs/cs/9905008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.6; I.2.7; I.5.3"
        ]
    },
    {
        "title": "Statistical Inference and Probabilistic Modelling for Constraint-Based   NLP",
        "authors": [
            "Stefan Riezler"
        ],
        "summary": "We present a probabilistic model for constraint-based grammars and a method for estimating the parameters of such models from incomplete, i.e., unparsed data. Whereas methods exist to estimate the parameters of probabilistic context-free grammars from incomplete data (Baum 1970), so far for probabilistic grammars involving context-dependencies only parameter estimation techniques from complete, i.e., fully parsed data have been presented (Abney 1997). However, complete-data estimation requires labor-intensive, error-prone, and grammar-specific hand-annotating of large language corpora. We present a log-linear probability model for constraint logic programming, and a general algorithm to estimate the parameters of such models from incomplete data by extending the estimation algorithm of Della-Pietra, Della-Pietra, and Lafferty (1997) to incomplete data settings.",
        "published": "1999-05-19T16:03:05Z",
        "link": "http://arxiv.org/abs/cs/9905010v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Ensembles of Radial Basis Function Networks for Spectroscopic Detection   of Cervical Pre-Cancer",
        "authors": [
            "Kagan Tumer",
            "Nirmala Ramanujam",
            "Joydeep Ghosh",
            "Rebecca Richards-Kortum"
        ],
        "summary": "The mortality related to cervical cancer can be substantially reduced through early detection and treatment. However, current detection techniques, such as Pap smear and colposcopy, fail to achieve a concurrently high sensitivity and specificity.   In vivo fluorescence spectroscopy is a technique which quickly, non-invasively and quantitatively probes the biochemical and morphological changes that occur in pre-cancerous tissue.   A multivariate statistical algorithm was used to extract clinically useful information from tissue spectra acquired from 361 cervical sites from 95 patients at 337, 380 and 460 nm excitation wavelengths. The multivariate statistical analysis was also employed to reduce the number of fluorescence excitation-emission wavelength pairs required to discriminate healthy tissue samples from pre-cancerous tissue samples. The use of connectionist methods such as multi layered perceptrons, radial basis function networks, and ensembles of such networks was investigated. RBF ensemble algorithms based on fluorescence spectra potentially provide automated, and near real-time implementation of pre-cancer detection in the hands of non-experts. The results are more reliable, direct and accurate than those achieved by either human experts or multivariate statistical algorithms.",
        "published": "1999-05-20T18:28:15Z",
        "link": "http://arxiv.org/abs/cs/9905011v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "q-bio",
            "I.5.1 ; J.3"
        ]
    },
    {
        "title": "Linear and Order Statistics Combiners for Pattern Classification",
        "authors": [
            "Kagan Tumer",
            "Joydeep Ghosh"
        ],
        "summary": "Several researchers have experimentally shown that substantial improvements can be obtained in difficult pattern recognition problems by combining or integrating the outputs of multiple classifiers. This chapter provides an analytical framework to quantify the improvements in classification results due to combining. The results apply to both linear combiners and order statistics combiners. We first show that to a first order approximation, the error rate obtained over and above the Bayes error rate, is directly proportional to the variance of the actual decision boundaries around the Bayes optimum boundary. Combining classifiers in output space reduces this variance, and hence reduces the \"added\" error. If N unbiased classifiers are combined by simple averaging, the added error rate can be reduced by a factor of N if the individual errors in approximating the decision boundaries are uncorrelated. Expressions are then derived for linear combiners which are biased or correlated, and the effect of output correlations on ensemble performance is quantified. For order statistics based non-linear combiners, we derive expressions that indicate how much the median, the maximum and in general the ith order statistic can improve classifier performance. The analysis presented here facilitates the understanding of the relationships among error rates, classifier boundary distributions, and combining in output space. Experimental results on several public domain data sets are provided to illustrate the benefits of combining and to support the analytical results.",
        "published": "1999-05-20T20:15:13Z",
        "link": "http://arxiv.org/abs/cs/9905012v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.5.1 ; I.2.6"
        ]
    },
    {
        "title": "Robust Combining of Disparate Classifiers through Order Statistics",
        "authors": [
            "Kagan Tumer",
            "Joydeep Ghosh"
        ],
        "summary": "Integrating the outputs of multiple classifiers via combiners or meta-learners has led to substantial improvements in several difficult pattern recognition problems. In the typical setting investigated till now, each classifier is trained on data taken or resampled from a common data set, or (almost) randomly selected subsets thereof, and thus experiences similar quality of training data. However, in certain situations where data is acquired and analyzed on-line at several geographically distributed locations, the quality of data may vary substantially, leading to large discrepancies in performance of individual classifiers. In this article we introduce and investigate a family of classifiers based on order statistics, for robust handling of such cases. Based on a mathematical modeling of how the decision boundaries are affected by order statistic combiners, we derive expressions for the reductions in error expected when such combiners are used. We show analytically that the selection of the median, the maximum and in general, the $i^{th}$ order statistic improves classification performance. Furthermore, we introduce the trim and spread combiners, both based on linear combinations of the ordered classifier outputs, and show that they are quite beneficial in presence of outliers or uneven classifier performance. Experimental results on several public domain data sets corroborate these findings.",
        "published": "1999-05-20T20:37:02Z",
        "link": "http://arxiv.org/abs/cs/9905013v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.NE",
            "I.5.1 ; G.3"
        ]
    },
    {
        "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function   Decomposition",
        "authors": [
            "Thomas G. Dietterich"
        ],
        "summary": "This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",
        "published": "1999-05-21T14:26:07Z",
        "link": "http://arxiv.org/abs/cs/9905014v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "State Abstraction in MAXQ Hierarchical Reinforcement Learning",
        "authors": [
            "Thomas G. Dietterich"
        ],
        "summary": "Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.",
        "published": "1999-05-21T14:49:39Z",
        "link": "http://arxiv.org/abs/cs/9905015v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Cascaded Grammatical Relation Assignment",
        "authors": [
            "Sabine Buchholz",
            "Jorn Veenstra",
            "Walter Daelemans"
        ],
        "summary": "In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.",
        "published": "1999-06-02T13:41:51Z",
        "link": "http://arxiv.org/abs/cs/9906004v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.6.2;I.7.1"
        ]
    },
    {
        "title": "Memory-Based Shallow Parsing",
        "authors": [
            "Walter Daelemans",
            "Sabine Buchholz",
            "Jorn Veenstra"
        ],
        "summary": "We present a memory-based learning (MBL) approach to shallow parsing in which POS tagging, chunking, and identification of syntactic relations are formulated as memory-based modules. The experiments reported in this paper show competitive results, the F-value for the Wall Street Journal (WSJ) treebank is: 93.8% for NP chunking, 94.7% for VP chunking, 77.1% for subject detection and 79.0% for object detection.",
        "published": "1999-06-02T13:48:48Z",
        "link": "http://arxiv.org/abs/cs/9906005v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.6.2;I.7.1"
        ]
    },
    {
        "title": "Automatically Selecting Useful Phrases for Dialogue Act Tagging",
        "authors": [
            "Ken Samuel",
            "Sandra Carberry",
            "K. Vijay-Shanker"
        ],
        "summary": "We present an empirical investigation of various ways to automatically identify phrases in a tagged corpus that are useful for dialogue act tagging. We found that a new method (which measures a phrase's deviation from an optimally-predictive phrase), enhanced with a lexical filtering mechanism, produces significantly better cues than manually-selected cue phrases, the exhaustive set of phrases in a training corpus, and phrases chosen by traditional metrics, like mutual information and information gain.",
        "published": "1999-06-18T03:25:03Z",
        "link": "http://arxiv.org/abs/cs/9906016v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "MAP Lexicon is useful for segmentation and word discovery in   child-directed speech",
        "authors": [
            "Anand Venkataraman"
        ],
        "summary": "Because of rather fundamental changes to the underlying model proposed in the paper, it has been withdrawn from the archive.",
        "published": "1999-07-06T01:44:00Z",
        "link": "http://arxiv.org/abs/cs/9907004v2",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Collective Intelligence for Control of Distributed Dynamical Systems",
        "authors": [
            "David H. Wolpert",
            "Kevin R. Wheeler",
            "Kagan Tumer"
        ],
        "summary": "We consider the El Farol bar problem, also known as the minority game (W. B. Arthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet and Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of the general problem of how to configure the nodal elements of a distributed dynamical system so that they do not ``work at cross purposes'', in that their collective dynamics avoids frustration and thereby achieves a provided global goal. We summarize a mathematical theory for such configuration applicable when (as in the bar problem) the global goal can be expressed as minimizing a global energy function and the nodes can be expressed as minimizers of local free energy functions. We show that a system designed with that theory performs nearly optimally for the bar problem.",
        "published": "1999-08-17T21:32:41Z",
        "link": "http://arxiv.org/abs/cs/9908013v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat",
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "An Introduction to Collective Intelligence",
        "authors": [
            "David H. Wolpert",
            "Kagan Tumer"
        ],
        "summary": "This paper surveys the emerging science of how to design a ``COllective INtelligence'' (COIN). A COIN is a large multi-agent system where:   (i) There is little to no centralized communication or control; and   (ii) There is a provided world utility function that rates the possible histories of the full system.   In particular, we are interested in COINs in which each agent runs a reinforcement learning (RL) algorithm. Rather than use a conventional modeling approach (e.g., model the system dynamics, and hand-tune agents to cooperate), we aim to solve the COIN design problem implicitly, via the ``adaptive'' character of the RL algorithms of each of the agents. This approach introduces an entirely new, profound design problem: Assuming the RL algorithms are able to achieve high rewards, what reward functions for the individual agents will, when pursued by those agents, result in high world utility? In other words, what reward functions will best ensure that we do not have phenomena like the tragedy of the commons, Braess's paradox, or the liquidity trap?   Although still very young, research specifically concentrating on the COIN design problem has already resulted in successes in artificial domains, in particular in packet-routing, the leader-follower problem, and in variants of Arthur's El Farol bar problem. It is expected that as it matures and draws upon other disciplines related to COINs, this research will greatly expand the range of tasks addressable by human engineers. Moreover, in addition to drawing on them, such a fully developed scie nce of COIN design may provide much insight into other already established scientific fields, such as economics, game theory, and population biology.",
        "published": "1999-08-17T22:49:19Z",
        "link": "http://arxiv.org/abs/cs/9908014v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat",
            "cs.DC",
            "cs.MA",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "A statistical model for word discovery in child directed speech",
        "authors": [
            "Anand Venkataraman"
        ],
        "summary": "A statistical model for segmentation and word discovery in child directed speech is presented. An incremental unsupervised learning algorithm to infer word boundaries based on this model is described and results of empirical tests showing that the algorithm is competitive with other models that have been used for similar tasks are also presented.",
        "published": "1999-10-13T03:25:33Z",
        "link": "http://arxiv.org/abs/cs/9910011v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "New Error Bounds for Solomonoff Prediction",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff sequence prediction is a scheme to predict digits of binary strings without knowing the underlying probability distribution. We call a prediction scheme informed when it knows the true probability distribution of the sequence. Several new relations between universal Solomonoff sequence prediction and informed prediction and general probabilistic prediction schemes will be proved. Among others, they show that the number of errors in Solomonoff prediction is finite for computable distributions, if finite in the informed case. Deterministic variants will also be studied. The most interesting result is that the deterministic variant of Solomonoff prediction is optimal compared to any other probabilistic or deterministic prediction scheme apart from additive square root corrections only. This makes it well suited even for difficult prediction problems, where it does not suffice when the number of errors is minimal to within some factor greater than one. Solomonoff's original bound and the ones presented here complement each other in a useful way.",
        "published": "1999-12-13T08:33:43Z",
        "link": "http://arxiv.org/abs/cs/9912008v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6; F.1.3; E.4; F.2"
        ]
    },
    {
        "title": "HMM Specialization with Selective Lexicalization",
        "authors": [
            "Jin-Dong Kim",
            "Sang-Zoo Lee",
            "Hae-Chang Rim"
        ],
        "summary": "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We performed a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.",
        "published": "1999-12-23T01:07:33Z",
        "link": "http://arxiv.org/abs/cs/9912016v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Average-Case Complexity of Shellsort",
        "authors": [
            "Tao Jiang",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "We prove a general lower bound on the average-case complexity of Shellsort: the average number of data-movements (and comparisons) made by a $p$-pass Shellsort for any incremental sequence is $\\Omega (pn^{1 + 1/p)$ for all $p \\leq \\log n$. Using similar arguments, we analyze the average-case complexity of several other sorting algorithms.",
        "published": "1999-01-20T16:32:01Z",
        "link": "http://arxiv.org/abs/cs/9901010v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; F.1.3"
        ]
    },
    {
        "title": "Minimum Description Length Induction, Bayesianism, and Kolmogorov   Complexity",
        "authors": [
            "Paul Vitanyi",
            "Ming Li"
        ],
        "summary": "The relationship between the Bayesian approach and the minimum description length approach is established. We sharpen and clarify the general modeling principles MDL and MML, abstracted as the ideal MDL principle and defined from Bayes's rule by means of Kolmogorov complexity. The basic condition under which the ideal principle should be applied is encapsulated as the Fundamental Inequality, which in broad terms states that the principle is valid when the data are random, relative to every contemplated hypothesis and also these hypotheses are random relative to the (universal) prior. Basically, the ideal principle states that the prior probability associated with the hypothesis should be given by the algorithmic universal probability, and the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized. If we restrict the model class to the finite sets then application of the ideal principle turns into Kolmogorov's minimal sufficient statistic. In general we show that data compression is almost always the best strategy, both in hypothesis identification and prediction.",
        "published": "1999-01-27T17:48:14Z",
        "link": "http://arxiv.org/abs/cs/9901014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LO",
            "math.IT",
            "math.PR",
            "physics.data-an",
            "E.4,F.2,H.3,I.2,I.5,I.7"
        ]
    },
    {
        "title": "PSPACE has 2-round quantum interactive proof systems",
        "authors": [
            "John Watrous"
        ],
        "summary": "In this paper we consider quantum interactive proof systems, i.e., interactive proof systems in which the prover and verifier may perform quantum computations and exchange quantum messages. It is proved that every language in PSPACE has a quantum interactive proof system that requires only two rounds of communication between the prover and verifier, while having exponentially small (one-sided) probability of error. It follows that quantum interactive proof systems are strictly more powerful than classical interactive proof systems in the constant-round case unless the polynomial time hierarchy collapses to the second level.",
        "published": "1999-01-27T21:35:26Z",
        "link": "http://arxiv.org/abs/cs/9901015v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.2;F.1.3"
        ]
    },
    {
        "title": "Mutual Search",
        "authors": [
            "Harry Buhrman",
            "Matthew Franklin",
            "Juan A. Garay",
            "Jaap-Henk Hoepman",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We introduce a search problem called ``mutual search'' where $k$ \\agents, arbitrarily distributed over $n$ sites, are required to locate one another by posing queries of the form ``Anybody at site $i$?''. We ask for the least number of queries that is necessary and sufficient. For the case of two \\agents using deterministic protocols we obtain the following worst-case results: In an oblivious setting (where all pre-planned queries are executed) there is no savings: $n-1$ queries are required and are sufficient. In a nonoblivious setting we can exploit the paradigm of ``no news is also news'' to obtain significant savings: in the synchronous case $0.586n$ queries suffice and $0.536n$ queries are required; in the asynchronous case $0.896n$ queries suffice and a fortiori 0.536 queries are required; for $o(\\sqrt{n})$ \\agents using a deterministic protocol less than $n$ queries suffice; there is a simple randomized protocol for two \\agents with worst-case expected $0.5n$ queries and all randomized protocols require at least $0.125n$ worst-case expected queries. The graph-theoretic framework we formulate for expressing and analyzing algorithms for this problem may be of independent interest.",
        "published": "1999-02-02T15:46:00Z",
        "link": "http://arxiv.org/abs/cs/9902005v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "cs.DC",
            "cs.DM",
            "cs.IR",
            "F.2,C.2,E,1,D.4.4"
        ]
    },
    {
        "title": "A Discipline of Evolutionary Programming",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Genetic fitness optimization using small populations or small population updates across generations generally suffers from randomly diverging evolutions. We propose a notion of highly probable fitness optimization through feasible evolutionary computing runs on small size populations. Based on rapidly mixing Markov chains, the approach pertains to most types of evolutionary genetic algorithms, genetic programming and the like. We establish that for systems having associated rapidly mixing Markov chains and appropriate stationary distributions the new method finds optimal programs (individuals) with probability almost 1. To make the method useful would require a structured design methodology where the development of the program and the guarantee of the rapidly mixing property go hand in hand. We analyze a simple example to show that the method is implementable. More significant examples require theoretical advances, for example with respect to the Metropolis filter.",
        "published": "1999-02-02T16:17:16Z",
        "link": "http://arxiv.org/abs/cs/9902006v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.MA",
            "I.2,E.1,F.1"
        ]
    },
    {
        "title": "A better lower bound for quantum algorithms searching an ordered list",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We show that any quantum algorithm searching an ordered list of n elements needs to examine at least 1/12 log n-O(1) of them. Classically, log n queries are both necessary and sufficient. This shows that quantum algorithms can achieve only a constant speedup for this problem. Our result improves lower bounds of Buhrman and de Wolf(quant-ph/9811046) and Farhi, Goldstone, Gutmann and Sipser (quant-ph/9812057).",
        "published": "1999-02-14T01:20:11Z",
        "link": "http://arxiv.org/abs/quant-ph/9902053v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Probabilistic Inductive Inference:a Survey",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "Inductive inference is a recursion-theoretic theory of learning, first developed by E. M. Gold (1967). This paper surveys developments in probabilistic inductive inference. We mainly focus on finite inference of recursive functions, since this simple paradigm has produced the most interesting (and most complex) results.",
        "published": "1999-02-15T01:52:45Z",
        "link": "http://arxiv.org/abs/cs/9902026v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.LO",
            "math.LO",
            "F.1.1., F.4.1., I.2.3., I.2.6"
        ]
    },
    {
        "title": "Quantum Bounded Query Complexity",
        "authors": [
            "Harry Buhrman",
            "Wim van Dam"
        ],
        "summary": "We combine the classical notions and techniques for bounded query classes with those developed in quantum computing. We give strong evidence that quantum queries to an oracle in the class NP does indeed reduce the query complexity of decision problems. Under traditional complexity assumptions, we obtain an exponential speedup between the quantum and the classical query complexity of function classes.   For decision problems and function classes we obtain the following results: o P_||^NP[2k] is included in EQP_||^NP[k] o P_||^NP[2^(k+1)-2] is included in EQP^NP[k] o FP_||^NP[2^(k+1)-2] is included in FEQP^NP[2k] o FP_||^NP is included in FEQP^NP[O(log n)] For sets A that are many-one complete for PSPACE or EXP we show that FP^A is included in FEQP^A[1]. Sets A that are many-one complete for PP have the property that FP_||^A is included in FEQP^A[1]. In general we prove that for any set A there is a set X such that FP^A is included in FEQP^X[1], establishing that no set is superterse in the quantum setting.",
        "published": "1999-03-10T02:39:59Z",
        "link": "http://arxiv.org/abs/quant-ph/9903035v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Space-Efficient Routing Tables for Almost All Networks and the   Incompressibility Method",
        "authors": [
            "Harry Buhrman",
            "Jaap-Henk Hoepman",
            "Paul Vitanyi"
        ],
        "summary": "We use the incompressibility method based on Kolmogorov complexity to determine the total number of bits of routing information for almost all network topologies. In most models for routing, for almost all labeled graphs $\\Theta (n^2)$ bits are necessary and sufficient for shortest path routing. By `almost all graphs' we mean the Kolmogorov random graphs which constitute a fraction of $1-1/n^c$ of all graphs on $n$ nodes, where $c > 0$ is an arbitrary fixed constant. There is a model for which the average case lower bound rises to $\\Omega(n^2 \\log n)$ and another model where the average case upper bound drops to $O(n \\log^2 n)$. This clearly exposes the sensitivity of such bounds to the model under consideration. If paths have to be short, but need not be shortest (if the stretch factor may be larger than 1), then much less space is needed on average, even in the more demanding models. Full-information routing requires $\\Theta (n^3)$ bits on average. For worst-case static networks we prove a $\\Omega(n^2 \\log n)$ lower bound for shortest path routing and all stretch factors $<2$ in some networks where free relabeling is not allowed.",
        "published": "1999-03-10T19:01:02Z",
        "link": "http://arxiv.org/abs/cs/9903009v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.CC",
            "cs.DS",
            "cs.NI",
            "C.2, F.2, D.4"
        ]
    },
    {
        "title": "An Almost-Quadratic Lower Bound for Quantum Formula Size",
        "authors": [
            "Vwani P. Roychowdhury",
            "Farrokh Vatan"
        ],
        "summary": "We show that Nechiporuk's method for proving lower bound for Boolean formulas can be extended to the quantum case. This leads to an n^2 / log^2 n lower bound for quantum formulas computing an explicit function. The only known previous explicit lower bound for quantum formulas (by Yao) states that the majority function does not have a linear-size quantum formula.",
        "published": "1999-03-11T23:09:24Z",
        "link": "http://arxiv.org/abs/quant-ph/9903042v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Formalization of the class of problems solvable by a nondeterministic   Turing machine",
        "authors": [
            "Anatoly D. Plotnikov"
        ],
        "summary": "The objective of this article is to formalize the definition of NP problems.   We construct a mathematical model of discrete problems as independence systems with weighted elements. We introduce two auxiliary sets that characterize the solution of the problem: the adjoint set, which contains the elements from the original set none of which can be adjoined to the already chosen solution elements; and the residual set, in which every element can be adjoined to previously chosen solution elements.   In a problem without lookahead, every adjoint set can be generated by the solution algorithm effectively, in polynomial time.   The main result of the study is the assertion that the NP class is identical with the class of problems without lookahead. Hence it follows that if we fail to find an effective (polynomial-time) solution algorithm for a given problem, then we need to look for an alternative formulation of the problem in set of problems without lookahead.",
        "published": "1999-03-16T17:13:43Z",
        "link": "http://arxiv.org/abs/cs/9903012v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2;F.1.2"
        ]
    },
    {
        "title": "Tiling with bars under tomographic constraints",
        "authors": [
            "Christoph Durr",
            "Eric Goles",
            "Ivan Rapaport",
            "Eric Remila"
        ],
        "summary": "We wish to tile a rectangle or a torus with only vertical and horizontal bars of a given length, such that the number of bars in every column and row equals given numbers. We present results for particular instances and for a more general problem, while leaving open the initial problem.",
        "published": "1999-03-31T09:58:20Z",
        "link": "http://arxiv.org/abs/cs/9903020v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "A Computer Scientist's View of Life, the Universe, and Everything",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "Is the universe computable? If so, it may be much cheaper in terms of information requirements to compute all computable universes instead of just ours. I apply basic concepts of Kolmogorov complexity theory to the set of possible universes, and chat about perceived and true randomness, life, generalization, and learning in a given universe.",
        "published": "1999-04-13T13:36:03Z",
        "link": "http://arxiv.org/abs/quant-ph/9904050v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.CY",
            "physics.comp-ph",
            "physics.pop-ph"
        ]
    },
    {
        "title": "Probabilities to accept languages by quantum finite automata",
        "authors": [
            "Andris Ambainis",
            "Richard Bonner",
            "Rusins Freivalds",
            "Arnolds Kikusts"
        ],
        "summary": "We construct a hierarchy of regular languages such that the current language in the hierarchy can be accepted by 1-way quantum finite automata with a probability smaller than the corresponding probability for the preceding language in the hierarchy. These probabilities converge to 1/2.",
        "published": "1999-04-16T17:35:20Z",
        "link": "http://arxiv.org/abs/quant-ph/9904066v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Brittle System Analysis",
        "authors": [
            "Stephen F. Bush",
            "John Hershey",
            "Kirby Vosburgh"
        ],
        "summary": "The goal of this paper is to define and analyze systems which exhibit brittle behavior. This behavior is characterized by a sudden and steep decline in performance as the system approaches the limits of tolerance. This can be due to input parameters which exceed a specified input, or environmental conditions which exceed specified operating boundaries. An analogy is made between brittle commmunication systems in particular and materials science.",
        "published": "1999-04-22T15:47:22Z",
        "link": "http://arxiv.org/abs/cs/9904016v1",
        "categories": [
            "cs.NI",
            "cs.CC",
            "cs.GL",
            "cs.PF",
            "C.2;C.4;B.8;F.2;H.1"
        ]
    },
    {
        "title": "Average-Case Quantum Query Complexity",
        "authors": [
            "Andris Ambainis",
            "Ronald de Wolf"
        ],
        "summary": "We compare classical and quantum query complexities of total Boolean functions. It is known that for worst-case complexity, the gap between quantum and classical can be at most polynomial. We show that for average-case complexity under the uniform distribution, quantum algorithms can be exponentially faster than classical algorithms. Under non-uniform distributions the gap can even be super-exponential. We also prove some general bounds for average-case complexity and show that the average-case quantum complexity of MAJORITY under the uniform distribution is nearly quadratically better than the classical complexity.",
        "published": "1999-04-23T10:46:02Z",
        "link": "http://arxiv.org/abs/quant-ph/9904079v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Bounds for Small-Error and Zero-Error Quantum Algorithms",
        "authors": [
            "H. Buhrman",
            "R. Cleve",
            "R. de Wolf",
            "Ch. Zalka"
        ],
        "summary": "We present a number of results related to quantum algorithms with small error probability and quantum algorithms that are zero-error. First, we give a tight analysis of the trade-offs between the number of queries of quantum search algorithms, their error probability, the size of the search space, and the number of solutions in this space. Using this, we deduce new lower and upper bounds for quantum versions of amplification problems. Next, we establish nearly optimal quantum-classical separations for the query complexity of monotone functions in the zero-error model (where our quantum zero-error model is defined so as to be robust when the quantum gates are noisy). Also, we present a communication complexity problem related to a total function for which there is a quantum-classical communication complexity gap in the zero-error model. Finally, we prove separations for monotone graph properties in the zero-error and other error models which imply that the evasiveness conjecture for such properties does not hold for quantum computers.",
        "published": "1999-04-26T08:54:21Z",
        "link": "http://arxiv.org/abs/cs/9904019v2",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.1; F.2.0"
        ]
    },
    {
        "title": "Optimal lower bounds for quantum automata and random access codes",
        "authors": [
            "Ashwin Nayak"
        ],
        "summary": "Consider the finite regular language L_n = {w0 : w \\in {0,1}^*, |w| \\le n}. It was shown by Ambainis, Nayak, Ta-Shma and Vazirani that while this language is accepted by a deterministic finite automaton of size O(n), any one-way quantum finite automaton (QFA) for it has size 2^{Omega(n/log n)}. This was based on the fact that the evolution of a QFA is required to be reversible. When arbitrary intermediate measurements are allowed, this intuition breaks down. Nonetheless, we show a 2^{Omega(n)} lower bound for such QFA for L_n, thus also improving the previous bound. The improved bound is obtained by simple entropy arguments based on Holevo's theorem. This method also allows us to obtain an asymptotically optimal (1-H(p))n bound for the dense quantum codes (random access codes) introduced by Ambainis et al. We then turn to Holevo's theorem, and show that in typical situations, it may be replaced by a tighter and more transparent in-probability bound.",
        "published": "1999-04-27T23:59:48Z",
        "link": "http://arxiv.org/abs/quant-ph/9904093v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum finite multitape automata",
        "authors": [
            "Andris Ambainis",
            "Richard Bonner",
            "Rusins Freivalds",
            "Marats Golovkins",
            "Marek Karpinski"
        ],
        "summary": "Quantum finite automata were introduced by C.Moore, J.P. Crutchfield, and by A.Kondacs and J.Watrous. This notion is not a generalization of the deterministic finite automata. Moreover, it was proved that not all regular languages can be recognized by quantum finite automata. A.Ambainis and R.Freivalds proved that for some languages quantum finite automata may be exponentially more concise rather than both deterministic and probabilistic finite automata. In this paper we introduce the notion of quantum finite multitape automata and prove that there is a language recognized by a quantum finite automaton but not by a deterministic or probabilistic finite automata. This is the first result on a problem which can be solved by a quantum computer but not by a deterministic or probabilistic computer. Additionally we discover unexpected probabilistic automata recognizing complicated languages.",
        "published": "1999-05-07T15:05:45Z",
        "link": "http://arxiv.org/abs/quant-ph/9905026v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.FL"
        ]
    },
    {
        "title": "Programs with Stringent Performance Objectives Will Often Exhibit   Chaotic Behavior",
        "authors": [
            "M. Chaves"
        ],
        "summary": "Software for the resolution of certain kind of problems, those that rate high in the Stringent Performance Objectives adjustment factor (IFPUG scheme), can be described using a combination of game theory and autonomous systems. From this description it can be shown that some of those problems exhibit chaotic behavior, an important fact in understanding the functioning of the related software. As a relatively simple example, it is shown that chess exhibits chaotic behavior in its configuration space. This implies that static evaluators in chess programs have intrinsic limitations.",
        "published": "1999-05-27T23:58:05Z",
        "link": "http://arxiv.org/abs/cs/9905016v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "68N05,68Q20,90D05,90D80"
        ]
    },
    {
        "title": "Algorithmic Complexity in Minority Game",
        "authors": [
            "Ricardo Mansilla Corona"
        ],
        "summary": "In this paper we introduce a new approach for the study of the complex behavior of Minority Game using the tools of algorithmic complexity, physical entropy and information theory. We show that physical complexity and mutual information function strongly depend on memory size of the agents and yields more information about the complex features of the stream of binary outcomes of the game than volatility itself.",
        "published": "1999-06-01T19:55:57Z",
        "link": "http://arxiv.org/abs/cond-mat/9906017v1",
        "categories": [
            "cond-mat.stat-mech",
            "adap-org",
            "chao-dyn",
            "cs.CC",
            "nlin.AO",
            "nlin.CD"
        ]
    },
    {
        "title": "MSO definable string transductions and two-way finite state transducers",
        "authors": [
            "Joost Engelfriet",
            "Hendrik Jan Hoogeboom"
        ],
        "summary": "String transductions that are definable in monadic second-order (mso) logic (without the use of parameters) are exactly those realized by deterministic two-way finite state transducers. Nondeterministic mso definable string transductions (i.e., those definable with the use of parameters) correspond to compositions of two nondeterministic two-way finite state transducers that have the finite visit property. Both families of mso definable string transductions are characterized in terms of Hennie machines, i.e., two-way finite state transducers with the finite visit property that are allowed to rewrite their input tape.",
        "published": "1999-06-04T13:21:01Z",
        "link": "http://arxiv.org/abs/cs/9906007v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.4.3; F.1.1"
        ]
    },
    {
        "title": "A Lower Bound on the Average-Case Complexity of Shellsort",
        "authors": [
            "Tao Jiang",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "We prove a general lower bound on the average-case complexity of Shellsort: the average number of data-movements (and comparisons) made by a $p$-pass Shellsort for any incremental sequence is $\\Omega (pn^{1 + 1/p})$ for every $p$. The proof method is an incompressibility argument based on Kolmogorov complexity. Using similar techniques, the average-case complexity of several other sorting algorithms is analyzed.",
        "published": "1999-06-04T15:11:31Z",
        "link": "http://arxiv.org/abs/cs/9906008v2",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.2; E.1"
        ]
    },
    {
        "title": "Reconstructing Polyatomic Structures from Discrete X-Rays:   NP-Completeness Proof for Three Atoms",
        "authors": [
            "Christoph Durr",
            "Marek Chrobak"
        ],
        "summary": "We address a discrete tomography problem that arises in the study of the atomic structure of crystal lattices. A polyatomic structure T can be defined as an integer lattice in dimension D>=2, whose points may be occupied by $c$ distinct types of atoms. To ``analyze'' T, we conduct ell measurements that we call_discrete X-rays_. A discrete X-ray in direction xi determines the number of atoms of each type on each line parallel to xi. Given ell such non-parallel X-rays, we wish to reconstruct T.   The complexity of the problem for c=1 (one atom type) has been completely determined by Gardner, Gritzmann and Prangenberg, who proved that the problem is NP-complete for any dimension D>=2 and ell>=3 non-parallel X-rays, and that it can be solved in polynomial time otherwise.   The NP-completeness result above clearly extends to any c>=2, and therefore when studying the polyatomic case we can assume that ell=2. As shown in another article by the same authors, this problem is also NP-complete for c>=6 atoms, even for dimension D=2 and axis-parallel X-rays. They conjecture that the problem remains NP-complete for c=3,4,5, although, as they point out, the proof idea does not seem to extend to c<=5.   We resolve the conjecture by proving that the problem is indeed NP-complete for c>=3 in 2D, even for axis-parallel X-rays. Our construction relies heavily on some structure results for the realizations of 0-1 matrices with given row and column sums.",
        "published": "1999-06-21T15:33:20Z",
        "link": "http://arxiv.org/abs/cs/9906018v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Generalization of automatic sequences for numeration systems on a   regular language",
        "authors": [
            "Michel Rigo"
        ],
        "summary": "Let L be an infinite regular language on a totally ordered alphabet (A,<). Feeding a finite deterministic automaton (with output) with the words of L enumerated lexicographically with respect to < leads to an infinite sequence over the output alphabet of the automaton. This process generalizes the concept of k-automatic sequence for abstract numeration systems on a regular language (instead of systems in base k). Here, I study the first properties of these sequences and their relations with numeration systems.",
        "published": "1999-06-22T10:01:27Z",
        "link": "http://arxiv.org/abs/cs/9906017v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.3; F.4.3"
        ]
    },
    {
        "title": "A decision procedure for well-formed linear quantum cellular automata",
        "authors": [
            "Christoph Durr",
            "Huong LeThanh",
            "Miklos Santha"
        ],
        "summary": "In this paper we introduce a new quantum computation model, the linear quantum cellular automaton. Well-formedness is an essential property for any quantum computing device since it enables us to define the probability of a configuration in an observation as the squared magnitude of its amplitude. We give an efficient algorithm which decides if a linear quantum cellular automaton is well-formed. The complexity of the algorithm is $O(n^2)$ in the algebraic model of computation if the input automaton has continuous neighborhood.",
        "published": "1999-06-23T10:48:10Z",
        "link": "http://arxiv.org/abs/cs/9906024v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "quant-ph",
            "F.1.1; F.2.1"
        ]
    },
    {
        "title": "A Foundation of Programming a Multi-Tape Quantum Turing machine",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "The notion of quantum Turing machines is a basis of quantum complexity theory. We discuss a general model of multi-tape, multi-head Quantum Turing machines with multi final states that also allow tape heads to stay still.",
        "published": "1999-06-23T15:17:21Z",
        "link": "http://arxiv.org/abs/quant-ph/9906084v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "On the Power of Positive Turing Reductions",
        "authors": [
            "Edith Hemaspaandra"
        ],
        "summary": "In the early 1980s, Selman's seminal work on positive Turing reductions showed that positive Turing reduction to NP yields no greater computational power than NP itself. Thus, positive Turing and Turing reducibility to NP differ sharply unless the polynomial hierarchy collapses.   We show that the situation is quite different for DP, the next level of the boolean hierarchy. In particular, positive Turing reduction to DP already yields all (and only) sets Turing reducibility to NP. Thus, positive Turing and Turing reducibility to DP yield the same class. Additionally, we show that an even weaker class, P(NP[1]), can be substituted for DP in this context.",
        "published": "1999-06-26T23:31:10Z",
        "link": "http://arxiv.org/abs/cs/9906028v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "Robust Reductions",
        "authors": [
            "Jin-Yi Cai",
            "Lane A. Hemaspaandra",
            "Gerd Wechsung"
        ],
        "summary": "We continue the study of robust reductions initiated by Gavalda and Balcazar. In particular, a 1991 paper of Gavalda and Balcazar claimed an optimal separation between the power of robust and nondeterministic strong reductions. Unfortunately, their proof is invalid. We re-establish their theorem.   Generalizing robust reductions, we note that robustly strong reductions are built from two restrictions, robust underproductivity and robust overproductivity, both of which have been separately studied before in other contexts. By systematically analyzing the power of these reductions, we explore the extent to which each restriction weakens the power of reductions. We show that one of these reductions yields a new, strong form of the Karp-Lipton Theorem.",
        "published": "1999-06-29T18:26:12Z",
        "link": "http://arxiv.org/abs/cs/9906033v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "A variational description of the ground state structure in random   satisfiability problems",
        "authors": [
            "Giulio Biroli",
            "Remi Monasson",
            "Martin Weigt"
        ],
        "summary": "A variational approach to finite connectivity spin-glass-like models is developed and applied to describe the structure of optimal solutions in random satisfiability problems. Our variational scheme accurately reproduces the known replica symmetric results and also allows for the inclusion of replica symmetry breaking effects. For the 3-SAT problem, we find two transitions as the ratio $\\alpha$ of logical clauses per Boolean variables increases. At the first one $\\alpha_s \\simeq 3.96$, a non-trivial organization of the solution space in geometrically separated clusters emerges. The multiplicity of these clusters as well as the typical distances between different solutions are calculated. At the second threshold $\\alpha_c \\simeq 4.48$, satisfying assignments disappear and a finite fraction $B_0 \\simeq 0.13$ of variables are overconstrained and take the same values in all optimal (though unsatisfying) assignments. These values have to be compared to $\\alpha_c \\simeq 4.27, B_0 \\simeq 0.4$ obtained from numerical experiments on small instances. Within the present variational approach, the SAT-UNSAT transition naturally appears as a mixture of a first and a second order transition. For the mixed $2+p$-SAT with $p<2/5$, the behavior is as expected much simpler: a unique smooth transition from SAT to UNSAT takes place at $\\alpha_c=1/(1-p)$.",
        "published": "1999-07-22T12:10:26Z",
        "link": "http://arxiv.org/abs/cond-mat/9907343v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Polynomial-Time Multi-Selectivity",
        "authors": [
            "Lane A. Hemaspaandra",
            "Zhigen Jiang",
            "Joerg Rothe",
            "Osamu Watanabe"
        ],
        "summary": "We introduce a generalization of Selman's P-selectivity that yields a more flexible notion of selectivity, called (polynomial-time) multi-selectivity, in which the selector is allowed to operate on multiple input strings. Since our introduction of this class, it has been used to prove the first known (and optimal) lower bounds for generalized selectivity-like classes in terms of EL_2, the second level of the extended low hierarchy. We study the resulting selectivity hierarchy, denoted by SH, which we prove does not collapse. In particular, we study the internal structure and the properties of SH and completely establish, in terms of incomparability and strict inclusion, the relations between our generalized selectivity classes and Ogihara's P-mc (polynomial-time membership-comparable) classes. Although SH is a strictly increasing infinite hierarchy, we show that the core results that hold for the P-selective sets and that prove them structurally simple also hold for SH. In particular, all sets in SH have small circuits; the NP sets in SH are in Low_2, the second level of the low hierarchy within NP; and SAT cannot be in SH unless P = NP. Finally, it is known that P-Sel, the class of P-selective sets, is not closed under union or intersection. We provide an extended selectivity hierarchy that is based on SH and that is large enough to capture those closures of the P-selective sets, and yet, in contrast with the P-mc classes, is refined enough to distinguish them.",
        "published": "1999-07-25T20:55:21Z",
        "link": "http://arxiv.org/abs/cs/9907034v1",
        "categories": [
            "cs.CC",
            "F.1.2; F.1.3"
        ]
    },
    {
        "title": "Easy Sets and Hard Certificate Schemes",
        "authors": [
            "Lane A. Hemaspaandra",
            "Joerg Rothe",
            "Gerd Wechsung"
        ],
        "summary": "Can easy sets only have easy certificate schemes? In this paper, we study the class of sets that, for all NP certificate schemes (i.e., NP machines), always have easy acceptance certificates (i.e., accepting paths) that can be computed in polynomial time. We also study the class of sets that, for all NP certificate schemes, infinitely often have easy acceptance certificates.   In particular, we provide equivalent characterizations of these classes in terms of relative generalized Kolmogorov complexity, showing that they are robust. We also provide structural conditions---regarding immunity and class collapses---that put upper and lower bounds on the sizes of these two classes. Finally, we provide negative results showing that some of our positive claims are optimal with regard to being relativizable. Our negative results are proven using a novel observation: we show that the classical ``wide spacing'' oracle construction technique yields instant non-bi-immunity results. Furthermore, we establish a result that improves upon Baker, Gill, and Solovay's classical result that NP \\neq P = NP \\cap coNP holds in some relativized world.",
        "published": "1999-07-25T21:05:19Z",
        "link": "http://arxiv.org/abs/cs/9907035v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Exact Analysis of Dodgson Elections: Lewis Carroll's 1876 Voting System   is Complete for Parallel Access to NP",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "In 1876, Lewis Carroll proposed a voting system in which the winner is the candidate who with the fewest changes in voters' preferences becomes a Condorcet winner---a candidate who beats all other candidates in pairwise majority-rule elections. Bartholdi, Tovey, and Trick provided a lower bound---NP-hardness---on the computational complexity of determining the election winner in Carroll's system. We provide a stronger lower bound and an upper bound that matches our lower bound. In particular, determining the winner in Carroll's system is complete for parallel access to NP, i.e., it is complete for $\\thetatwo$, for which it becomes the most natural complete problem known. It follows that determining the winner in Carroll's elections is not NP-complete unless the polynomial hierarchy collapses.",
        "published": "1999-07-25T21:16:56Z",
        "link": "http://arxiv.org/abs/cs/9907036v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2; J.4"
        ]
    },
    {
        "title": "Boolean Operations, Joins, and the Extended Low Hierarchy",
        "authors": [
            "Lane A. Hemaspaandra",
            "Zhigen Jiang",
            "Joerg Rothe",
            "Osamu Watanabe"
        ],
        "summary": "We prove that the join of two sets may actually fall into a lower level of the extended low hierarchy than either of the sets. In particular, there exist sets that are not in the second level of the extended low hierarchy, EL_2, yet their join is in EL_2. That is, in terms of extended lowness, the join operator can lower complexity. Since in a strong intuitive sense the join does not lower complexity, our result suggests that the extended low hierarchy is unnatural as a complexity measure. We also study the closure properties of EL_ and prove that EL_2 is not closed under certain Boolean operations. To this end, we establish the first known (and optimal) EL_2 lower bounds for certain notions generalizing Selman's P-selectivity, which may be regarded as an interesting result in its own right.",
        "published": "1999-07-25T21:30:10Z",
        "link": "http://arxiv.org/abs/cs/9907037v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "A Second Step Towards Complexity-Theoretic Analogs of Rice's Theorem",
        "authors": [
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "Rice's Theorem states that every nontrivial language property of the recursively enumerable sets is undecidable. Borchert and Stephan initiated the search for complexity-theoretic analogs of Rice's Theorem. In particular, they proved that every nontrivial counting property of circuits is UP-hard, and that a number of closely related problems are SPP-hard.   The present paper studies whether their UP-hardness result itself can be improved to SPP-hardness. We show that their UP-hardness result cannot be strengthened to SPP-hardness unless unlikely complexity class containments hold. Nonetheless, we prove that every P-constructibly bi-infinite counting property of circuits is SPP-hard. We also raise their general lower bound from unambiguous nondeterminism to constant-ambiguity nondeterminism.",
        "published": "1999-07-25T21:39:03Z",
        "link": "http://arxiv.org/abs/cs/9907038v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Raising NP Lower Bounds to Parallel NP Lower Bounds",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "A decade ago, a beautiful paper by Wagner developed a ``toolkit'' that in certain cases allows one to prove problems hard for parallel access to NP. However, the problems his toolkit applies to most directly are not overly natural. During the past year, problems that previously were known only to be NP-hard or coNP-hard have been shown to be hard even for the class of sets solvable via parallel access to NP. Many of these problems are longstanding and extremely natural, such as the Minimum Equivalent Expression problem (which was the original motivation for creating the polynomial hierarchy), the problem of determining the winner in the election system introduced by Lewis Carroll in 1876, and the problem of determining on which inputs heuristic algorithms perform well. In the present article, we survey this recent progress in raising lower bounds.",
        "published": "1999-07-25T21:47:02Z",
        "link": "http://arxiv.org/abs/cs/9907039v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Unambiguous Computation: Boolean Hierarchies and Sparse Turing-Complete   Sets",
        "authors": [
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "It is known that for any class C closed under union and intersection, the Boolean closure of C, the Boolean hierarchy over C, and the symmetric difference hierarchy over C all are equal. We prove that these equalities hold for any complexity class closed under intersection; in particular, they thus hold for unambiguous polynomial time (UP). In contrast to the NP case, we prove that the Hausdorff hierarchy and the nested difference hierarchy over UP both fail to capture the Boolean closure of UP in some relativized worlds.   Karp and Lipton proved that if nondeterministic polynomial time has sparse Turing-complete sets, then the polynomial hierarchy collapses. We establish the first consequences from the assumption that unambiguous polynomial time has sparse Turing-complete sets: (a) UP is in Low_2, where Low_2 is the second level of the low hierarchy, and (b) each level of the unambiguous polynomial hierarchy is contained one level lower in the promise unambiguous polynomial hierarchy than is otherwise known to be the case.",
        "published": "1999-07-26T10:09:58Z",
        "link": "http://arxiv.org/abs/cs/9907033v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Characterizations of the Existence of Partial and Total One-Way   Permutations",
        "authors": [
            "Joerg Rothe",
            "Lane A. Hemaspaandra"
        ],
        "summary": "In this note, we study the easy certificate classes introduced by Hemaspaandra, Rothe, and Wechsung, with regard to the question of whether or not surjective one-way functions exist. This is an important open question in cryptology. We show that the existence of partial one-way permutations can be characterized by separating P from the class of UP sets that, for all unambiguous polynomial-time Turing machines accepting them, always have easy (i.e., polynomial-time computable) certificates. This extends results of Grollmann and Selman. By Gr\\\"adel's recent results about one-way functions, this also links statements about easy certificates of NP sets with statements in finite model theory. Similarly, there exist surjective poly-one one-way functions if and only if there is a set L in P such that not all FewP machines accepting L always have easy certificates. We also establish a condition necessary and sufficient for the existence of (total) one-way permutations.",
        "published": "1999-07-26T10:42:16Z",
        "link": "http://arxiv.org/abs/cs/9907040v1",
        "categories": [
            "cs.CC",
            "cs.CR",
            "F.1.3; E.3"
        ]
    },
    {
        "title": "Restrictive Acceptance Suffices for Equivalence Problems",
        "authors": [
            "Bernd Borchert",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "One way of suggesting that an NP problem may not be NP-complete is to show that it is in the class UP. We suggest an analogous new approach---weaker in strength of evidence but more broadly applicable---to suggesting that concrete~NP problems are not NP-complete. In particular we introduce the class EP, the subclass of NP consisting of those languages accepted by NP machines that when they accept always have a number of accepting paths that is a power of two. Since if any NP-complete set is in EP then all NP sets are in EP, it follows---with whatever degree of strength one believes that EP differs from NP---that membership in EP can be viewed as evidence that a problem is not NP-complete.   We show that the negation equivalence problem for OBDDs (ordered binary decision diagrams) and the interchange equivalence problem for 2-dags are in EP. We also show that for boolean negation the equivalence problem is in EP^{NP}, thus tightening the existing NP^{NP} upper bound. We show that FewP, bounded ambiguity polynomial time, is contained in EP, a result that is not known to follow from the previous SPP upper bound. For the three problems and classes just mentioned with regard to EP, no proof of membership/containment in UP is known, and for the problem just mentioned with regard to EP^{NP}, no proof of membership in UP^{NP} is known. Thus, EP is indeed a tool that gives evidence against NP-completeness in natural cases where UP cannot currently be applied.",
        "published": "1999-07-26T10:50:48Z",
        "link": "http://arxiv.org/abs/cs/9907041v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Construction of regular languages and recognizability of polynomials",
        "authors": [
            "Michel Rigo"
        ],
        "summary": "A generalization of numeration system in which the set N of the natural numbers is recognizable by finite automata can be obtained by describing a lexicographically ordered infinite regular language. Here we show that if P belonging to Q[x] is a polynomial such that P(N) is a subset of N then we can construct a numeration system in which the set of representations of P(N) is regular. The main issue in this construction is to setup a regular language with a density function equals to P(n+1)-P(n) for n large enough.",
        "published": "1999-08-27T07:33:28Z",
        "link": "http://arxiv.org/abs/cs/9908018v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "Analysis of Quantum Functions",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "This paper initiates a systematic study of quantum functions, which are (partial) functions defined in terms of quantum mechanical computations. Of all quantum functions, we focus on resource-bounded quantum functions whose inputs are classical bit strings. We prove complexity-theoretical properties and unique characteristics of these quantum functions by recent techniques developed for the analysis of quantum computations. We also discuss relativized quantum functions that make adaptive and nonadaptive oracle queries.",
        "published": "1999-09-02T17:23:26Z",
        "link": "http://arxiv.org/abs/quant-ph/9909012v4",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Noncommutative Martin-Lof randomness : on the concept of a random   sequence of qubits",
        "authors": [
            "Gavriel Segre"
        ],
        "summary": "Martin-Lof's definition of random sequences of cbits as those not belonging to any set of constructive zero Lebesgue measure is reformulated in the language of Algebraic Probability Theory.   The adoption of the Pour-El Richards theory of computability structures on Banach spaces allows us to give a natural noncommutative extension of Martin-Lof's definition, characterizing the random elements of a chain Von Neumann algebra.   In the particular case of the minimally informative noncommutative alphabet our definition reduces to the definition of a random sequence of qubits.",
        "published": "1999-09-21T13:31:02Z",
        "link": "http://arxiv.org/abs/chao-dyn/9909031v2",
        "categories": [
            "chao-dyn",
            "adap-org",
            "cs.CC",
            "math-ph",
            "math.MP",
            "nlin.AO",
            "nlin.CD",
            "quant-ph"
        ]
    },
    {
        "title": "Query Order",
        "authors": [
            "Lane A. Hemaspaandra",
            "Harald Hempel",
            "Gerd Wechsung"
        ],
        "summary": "We study the effect of query order on computational power, and show that $\\pjk$-the languages computable via a polynomial-time machine given one query to the jth level of the boolean hierarchy followed by one query to the kth level of the boolean hierarchy-equals $\\redttnp{j+2k-1}$ if j is even and k is odd, and equals $\\redttnp{j+2k}$ otherwise. Thus, unless the polynomial hierarchy collapses, it holds that for each $1\\leq j \\leq k$: $\\pjk = \\pkj \\iff (j=k) \\lor (j{is even} \\land k=j+1)$. We extend our analysis to apply to more general query classes.",
        "published": "1999-09-30T17:06:40Z",
        "link": "http://arxiv.org/abs/cs/9909020v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Quantum Computation and Quadratically Signed Weight Enumerators",
        "authors": [
            "E. Knill",
            "R. Laflamme"
        ],
        "summary": "We prove that quantum computation is polynomially equivalent to classical probabilistic computation with an oracle for estimating the value of simple sums, quadratically signed weight enumerators. The problem of estimating these sums can be cast in terms of promise problems and has two interesting variants. An oracle for the unconstrained variant may be more powerful than quantum computation, while an oracle for a more constrained variant is efficiently solvable in the one-bit model of quantum computation. Thus, problems involving estimation of quadratically signed weight enumerators yield problems in BQP (bounded error quantum polynomial time) that are distinct from the ones studied so far, include a canonical BQP complete problem, and can be used to define and study complexity classes and their relationships to quantum computation.",
        "published": "1999-09-30T22:24:33Z",
        "link": "http://arxiv.org/abs/quant-ph/9909094v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Fixed-parameter tractability, definability, and model checking",
        "authors": [
            "Joerg Flum",
            "Martin Grohe"
        ],
        "summary": "In this article, we study parameterized complexity theory from the perspective of logic, or more specifically, descriptive complexity theory.   We propose to consider parameterized model-checking problems for various fragments of first-order logic as generic parameterized problems and show how this approach can be useful in studying both fixed-parameter tractability and intractability. For example, we establish the equivalence between the model-checking for existential first-order logic, the homomorphism problem for relational structures, and the substructure isomorphism problem. Our main tractability result shows that model-checking for first-order formulas is fixed-parameter tractable when restricted to a class of input structures with an excluded minor. On the intractability side, for every t >= 0 we prove an equivalence between model-checking for first-order formulas with t quantifier alternations and the parameterized halting problem for alternating Turing machines with t alternations. We discuss the close connection between this alternation hierarchy and Downey and Fellows' W-hierarchy.   On a more abstract level, we consider two forms of definability, called Fagin definability and slicewise definability, that are appropriate for describing parameterized problems. We give a characterization of the class FPT of all fixed-parameter tractable problems in terms of slicewise definability in finite variable least fixed-point logic, which is reminiscent of the Immerman-Vardi Theorem characterizing the class PTIME in terms of definability in least fixed-point logic.",
        "published": "1999-10-01T15:10:00Z",
        "link": "http://arxiv.org/abs/cs/9910001v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3;F.4.1"
        ]
    },
    {
        "title": "What's Up with Downward Collapse: Using the Easy-Hard Technique to Link   Boolean and Polynomial Hierarchy Collapses",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "During the past decade, nine papers have obtained increasingly strong consequences from the assumption that boolean or bounded-query hierarchies collapse. The final four papers of this nine-paper progression actually achieve downward collapse---that is, they show that high-level collapses induce collapses at (what beforehand were thought to be) lower complexity levels. For example, for each $k\\geq 2$ it is now known that if $\\psigkone=\\psigktwo$ then $\\ph=\\sigmak$. This article surveys the history, the results, and the technique---the so-called easy-hard method---of these nine papers.",
        "published": "1999-10-01T15:45:25Z",
        "link": "http://arxiv.org/abs/cs/9910002v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "R_{1-tt}^{SN}(NP) Distinguishes Robust Many-One and Turing Completeness",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "Do complexity classes have many-one complete sets if and only if they have Turing-complete sets? We prove that there is a relativized world in which a relatively natural complexity class-namely a downward closure of NP, \\rsnnp - has Turing-complete sets but has no many-one complete sets. In fact, we show that in the same relativized world this class has 2-truth-table complete sets but lacks 1-truth-table complete sets. As part of the groundwork for our result, we prove that \\rsnnp has many equivalent forms having to do with ordered and parallel access to $\\np$ and $\\npinterconp$.",
        "published": "1999-10-01T18:55:20Z",
        "link": "http://arxiv.org/abs/cs/9910003v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "An Introduction to Query Order",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] raised the following questions: If one is allowed one question to each of two different information sources, does the order in which one asks the questions affect the class of problems that one can solve with the given access? If so, which order yields the greater computational power?   The answers to these questions have been learned-inasfar as they can be learned without resolving whether or not the polynomial hierarchy collapses-for both the polynomial hierarchy and the boolean hierarchy. In the polynomial hierarchy, query order never matters. In the boolean hierarchy, query order sometimes does not matter and, unless the polynomial hierarchy collapses, sometimes does matter. Furthermore, the study of query order has yielded dividends in seemingly unrelated areas, such as bottleneck computations and downward translation of equality.   In this article, we present some of the central results on query order. The article is written in such a way as to encourage the reader to try his or her own hand at proving some of these results. We also give literature pointers to the quickly growing set of related results and applications.",
        "published": "1999-10-01T19:08:32Z",
        "link": "http://arxiv.org/abs/cs/9910004v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Query Order and the Polynomial Hierarchy",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] initiated the field of query order, which studies the ways in which computational power is affected by the order in which information sources are accessed. The present paper studies, for the first time, query order as it applies to the levels of the polynomial hierarchy. We prove that the levels of the polynomial hierarchy are order-oblivious. Yet, we also show that these ordered query classes form new levels in the polynomial hierarchy unless the polynomial hierarchy collapses. We prove that all leaf language classes - and thus essentially all standard complexity classes - inherit all order-obliviousness results that hold for P.",
        "published": "1999-10-01T19:20:07Z",
        "link": "http://arxiv.org/abs/cs/9910005v2",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Self-Specifying Machines",
        "authors": [
            "Lane A. Hemaspaandra",
            "Harald Hempel",
            "Gerd Wechsung"
        ],
        "summary": "We study the computational power of machines that specify their own acceptance types, and show that they accept exactly the languages that $\\manyonesharp$-reduce to NP sets. A natural variant accepts exactly the languages that $\\manyonesharp$-reduce to P sets. We show that these two classes coincide if and only if $\\psone = \\psnnoplusbigohone$, where the latter class denotes the sets acceptable via at most one question to $\\sharpp$ followed by at most a constant number of questions to $\\np$.",
        "published": "1999-10-01T19:29:53Z",
        "link": "http://arxiv.org/abs/cs/9910006v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "A Downward Collapse within the Polynomial Hierarchy",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "Downward collapse (a.k.a. upward separation) refers to cases where the equality of two larger classes implies the equality of two smaller classes. We provide an unqualified downward collapse result completely within the polynomial hierarchy. In particular, we prove that, for k > 2, if $\\psigkone = \\psigktwo$ then $\\sigmak = \\pik = \\ph$. We extend this to obtain a more general downward collapse result.",
        "published": "1999-10-01T19:48:25Z",
        "link": "http://arxiv.org/abs/cs/9910007v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Translating Equality Downwards",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "Downward translation of equality refers to cases where a collapse of some pair of complexity classes would induce a collapse of some other pair of complexity classes that (a priori) one expects are smaller. Recently, the first downward translation of equality was obtained that applied to the polynomial hierarchy-in particular, to bounded access to its levels [cs.CC/9910007]. In this paper, we provide a much broader downward translation that extends not only that downward translation but also that translation's elegant enhancement by Buhrman and Fortnow. Our work also sheds light on previous research on the structure of refined polynomial hierarchies, and strengthens the connection between the collapse of bounded query hierarchies and the collapse of the polynomial hierarchy.",
        "published": "1999-10-01T19:58:41Z",
        "link": "http://arxiv.org/abs/cs/9910008v2",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Almost-Everywhere Superiority for Quantum Computing",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Marius Zimand"
        ],
        "summary": "Simon as extended by Brassard and H{\\o}yer shows that there are tasks on which polynomial-time quantum machines are exponentially faster than each classical machine infinitely often. The present paper shows that there are tasks on which polynomial-time quantum machines are exponentially faster than each classical machine almost everywhere.",
        "published": "1999-10-08T03:48:56Z",
        "link": "http://arxiv.org/abs/quant-ph/9910033v4",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Communication Complexity Lower Bounds by Polynomials",
        "authors": [
            "Harry Buhrman",
            "Ronald de Wolf"
        ],
        "summary": "The quantum version of communication complexity allows the two communicating parties to exchange qubits and/or to make use of prior entanglement (shared EPR-pairs). Some lower bound techniques are available for qubit communication complexity, but except for the inner product function, no bounds are known for the model with unlimited prior entanglement. We show that the log-rank lower bound extends to the strongest model (qubit communication + unlimited prior entanglement). By relating the rank of the communication matrix to properties of polynomials, we are able to derive some strong bounds for exact protocols. In particular, we prove both the \"log-rank conjecture\" and the polynomial equivalence of quantum and classical communication complexity for various classes of functions. We also derive some weaker bounds for bounded-error quantum protocols.",
        "published": "1999-10-12T13:30:28Z",
        "link": "http://arxiv.org/abs/cs/9910010v2",
        "categories": [
            "cs.CC",
            "quant-ph",
            "E.4; F.1.1; F.2.0"
        ]
    },
    {
        "title": "The Complexity of Temporal Logic over the Reals",
        "authors": [
            "M. Reynolds"
        ],
        "summary": "It is shown that the decision problem for the temporal logic with until and since connectives over real-numbers time is PSPACE-complete.",
        "published": "1999-10-13T04:04:00Z",
        "link": "http://arxiv.org/abs/cs/9910012v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F4.1;F2.2"
        ]
    },
    {
        "title": "Numeration systems on a regular language: Arithmetic operations,   Recognizability and Formal power series",
        "authors": [
            "Michel Rigo"
        ],
        "summary": "Generalizations of numeration systems in which N is recognizable by a finite automaton are obtained by describing a lexicographically ordered infinite regular language L over a finite alphabet A. For these systems, we obtain a characterization of recognizable sets of integers in terms of rational formal series. We also show that, if the complexity of L is Theta (n^q) (resp. if L is the complement of a polynomial language), then multiplication by an integer k preserves recognizability only if k=t^{q+1} (resp. if k is not a power of the cardinality of A) for some integer t. Finally, we obtain sufficient conditions for the notions of recognizability and U-recognizability to be equivalent, where U is some positional numeration system related to a sequence of integers.",
        "published": "1999-11-08T13:03:50Z",
        "link": "http://arxiv.org/abs/cs/9911002v2",
        "categories": [
            "cs.CC",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "Graph Ramsey games",
        "authors": [
            "Wolfgang Slany"
        ],
        "summary": "We consider combinatorial avoidance and achievement games based on graph Ramsey theory: The players take turns in coloring still uncolored edges of a graph G, each player being assigned a distinct color, choosing one edge per move. In avoidance games, completing a monochromatic subgraph isomorphic to another graph A leads to immediate defeat or is forbidden and the first player that cannot move loses. In the avoidance+ variants, both players are free to choose more than one edge per move. In achievement games, the first player that completes a monochromatic subgraph isomorphic to A wins. Erdos & Selfridge (1973) were the first to identify some tractable subcases of these games, followed by a large number of further studies. We complete these investigations by settling the complexity of all unrestricted cases: We prove that general graph Ramsey avoidance, avoidance+, and achievement games and several variants thereof are PSPACE-complete. We ultra-strongly solve some nontrivial instances of graph Ramsey avoidance games that are based on symmetric binary Ramsey numbers and provide strong evidence that all other cases based on symmetric binary Ramsey numbers are effectively intractable.   Keywords: combinatorial games, graph Ramsey theory, Ramsey game, PSPACE-completeness, complexity, edge coloring, winning strategy, achievement game, avoidance game, the game of Sim, Polya's enumeration formula, probabilistic counting, machine learning, heuristics, Java applet",
        "published": "1999-11-10T11:28:11Z",
        "link": "http://arxiv.org/abs/cs/9911004v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "math.CO",
            "F.1.3; F.2.2; G.2.1; G.2.2; G.2.3; H.3.5; I.2.6; I.2.8"
        ]
    },
    {
        "title": "One-Way Functions in Worst-Case Cryptography: Algebraic and Security   Properties",
        "authors": [
            "A. Beygelzimer",
            "L. A. Hemaspaandra",
            "C. M. Homan",
            "J. Rothe"
        ],
        "summary": "We survey recent developments in the study of (worst-case) one-way functions having strong algebraic and security properties. According to [RS93], this line of research was initiated in 1984 by Rivest and Sherman who designed two-party secret-key agreement protocols that use strongly noninvertible, total, associative one-way functions as their key building blocks. If commutativity is added as an ingredient, these protocols can be used by more than two parties, as noted by Rabi and Sherman [RS93] who also developed digital signature protocols that are based on such enhanced one-way functions.   Until recently, it was an open question whether one-way functions having the algebraic and security properties that these protocols require could be created from any given one-way function. Recently, Hemaspaandra and Rothe [HR99] resolved this open issue in the affirmative, by showing that one-way functions exist if and only if strong, total, commutative, associative one-way functions exist.   We discuss this result, and the work of Rabi, Rivest, and Sherman, and recent work of Homan [Hom99] that makes progress on related issues.",
        "published": "1999-11-15T13:20:18Z",
        "link": "http://arxiv.org/abs/cs/9911007v1",
        "categories": [
            "cs.CC",
            "cs.CR",
            "F.1.3; E.3"
        ]
    },
    {
        "title": "On quantum and classical space-bounded processes with algebraic   transition amplitudes",
        "authors": [
            "John Watrous"
        ],
        "summary": "We define a class of stochastic processes based on evolutions and measurements of quantum systems, and consider the complexity of predicting their long-term behavior. It is shown that a very general class of decision problems regarding these stochastic processes can be efficiently solved classically in the space-bounded case. The following corollaries are implied by our main result: (1) Any space O(s) uniform family of quantum circuits acting on s qubits and consisting of unitary gates and measurement gates defined in a typical way by matrices of algebraic numbers can be simulated by an unbounded error space O(s) ordinary (i.e., fair-coin flipping) probabilistic Turing machine, and hence by space O(s) uniform classical (deterministic) circuits of depth O(s^2) and size 2^(O(s)). The quantum circuits are not required to operate with bounded error and may have depth exponential in s. (2) Any (unbounded error) quantum Turing machine running in space s, having arbitrary algebraic transition amplitudes, allowing unrestricted measurements during its computation, and having no restrictions on running time can be simulated by an unbounded error space O(s) ordinary probabilistic Turing machine, and hence deterministically in space O(s^2).",
        "published": "1999-11-16T21:55:07Z",
        "link": "http://arxiv.org/abs/cs/9911008v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.2; F.1.3"
        ]
    },
    {
        "title": "Two-way finite automata with quantum and classical states",
        "authors": [
            "Andris Ambainis",
            "John Watrous"
        ],
        "summary": "We introduce 2-way finite automata with quantum and classical states (2qcfa's). This is a variant on the 2-way quantum finite automata (2qfa) model which may be simpler to implement than unrestricted 2qfa's; the internal state of a 2qcfa may include a quantum part that may be in a (mixed) quantum state, but the tape head position is required to be classical.   We show two languages for which 2qcfa's are better than classical 2-way automata. First, 2qcfa's can recognize palindromes, a language that cannot be recognized by 2-way deterministic or probabilistic finite automata. Second, in polynomial time 2qcfa's can recognize {a^n b^n | n>=0}, a language that can be recognized classically by a 2-way probabilistic automaton but only in exponential time.",
        "published": "1999-11-16T22:24:01Z",
        "link": "http://arxiv.org/abs/cs/9911009v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.1"
        ]
    },
    {
        "title": "The Complexity of Poor Man's Logic",
        "authors": [
            "Edith Hemaspaandra"
        ],
        "summary": "Motivated by description logics, we investigate what happens to the complexity of modal satisfiability problems if we only allow formulas built from literals, $\\wedge$, $\\Diamond$, and $\\Box$. Previously, the only known result was that the complexity of the satisfiability problem for K dropped from PSPACE-complete to coNP-complete (Schmidt-Schauss and Smolka, 1991 and Donini et al., 1992). In this paper we show that not all modal logics behave like K. In particular, we show that the complexity of the satisfiability problem with respect to frames in which each world has at least one successor drops from PSPACE-complete to P, but that in contrast the satisfiability problem with respect to the class of frames in which each world has at most two successors remains PSPACE-complete. As a corollary of the latter result, we also solve the open problem from Donini et al.'s complexity classification of description logics (Donini et al., 1997). In the last section, we classify the complexity of the satisfiability problem for K for all other restrictions on the set of operators.",
        "published": "1999-11-28T21:34:01Z",
        "link": "http://arxiv.org/abs/cs/9911014v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "The phase transition in random Horn satisfiability and its algorithmic   implications",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "Let c>0 be a constant, and $\\Phi$ be a random Horn formula with n variables and $m=c\\cdot 2^{n}$ clauses, chosen uniformly at random (with repetition) from the set of all nonempty Horn clauses in the given variables. By analyzing \\PUR, a natural implementation of positive unit resolution, we show that $\\lim_{n\\goesto \\infty} \\PR ({$\\Phi$ is satisfiable})= 1-F(e^{-c})$, where $F(x)=(1-x)(1-x^2)(1-x^4)(1-x^8)... $. Our method also yields as a byproduct an average-case analysis of this algorithm.",
        "published": "1999-12-01T22:04:47Z",
        "link": "http://arxiv.org/abs/cs/9912001v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2;I.1.2;G.3"
        ]
    },
    {
        "title": "A Geometric Model for Information Retrieval Systems",
        "authors": [
            "Myung Ho Kim"
        ],
        "summary": "This decade has seen a great deal of progress in the development of information retrieval systems. Unfortunately, we still lack a systematic understanding of the behavior of the systems and their relationship with documents. In this paper we present a completely new approach towards the understanding of the information retrieval systems. Recently, it has been observed that retrieval systems in TREC 6 show some remarkable patterns in retrieving relevant documents. Based on the TREC 6 observations, we introduce a geometric linear model of information retrieval systems. We then apply the model to predict the number of relevant documents by the retrieval systems. The model is also scalable to a much larger data set. Although the model is developed based on the TREC 6 routing test data, I believe it can be readily applicable to other information retrieval systems. In Appendix, we explained a simple and efficient way of making a better system from the existing systems.",
        "published": "1999-12-06T03:04:43Z",
        "link": "http://arxiv.org/abs/cs/9912002v1",
        "categories": [
            "cs.IR",
            "cs.CC",
            "cs.DL",
            "H.1.1"
        ]
    },
    {
        "title": "Quantum Computing, NP-complete Problems and Chaotic Dynamics",
        "authors": [
            "Masanori Ohya",
            "Igor V. Volovich"
        ],
        "summary": "An approach to the solution of NP-complete problems based on quantum computing and chaotic dynamics is proposed. We consider the satisfiability problem and argue that the problem, in principle, can be solved in polynomial time if we combine the quantum computer with the chaotic dynamics amplifier based on the logistic map. We discuss a possible implementation of such a chaotic quantum computation by using the atomic quantum computer with quantum gates described by the Hartree-Fock equations. In this case, in principle, one can build not only standard linear quantum gates but also nonlinear gates and moreover they obey to Fermi statistics. This new type of entaglement related with Fermi statistics can be interesting also for quantum communication theory.",
        "published": "1999-12-21T22:03:01Z",
        "link": "http://arxiv.org/abs/quant-ph/9912100v1",
        "categories": [
            "quant-ph",
            "chao-dyn",
            "cond-mat.mes-hall",
            "cs.CC",
            "nlin.CD",
            "physics.atom-ph"
        ]
    },
    {
        "title": "Fixpoint 3-valued semantics for autoepistemic logic",
        "authors": [
            "M. Denecker",
            "V. Marek",
            "M. Truszczynski"
        ],
        "summary": "The paper presents a constructive fixpoint semantics for autoepistemic logic (AEL). This fixpoint characterizes a unique but possibly three-valued belief set of an autoepistemic theory. It may be three-valued in the sense that for a subclass of formulas F, the fixpoint may not specify whether F is believed or not. The paper presents a constructive 3-valued semantics for autoepistemic logic (AEL). We introduce a derivation operator and define the semantics as its least fixpoint. The semantics is 3-valued in the sense that, for some formulas, the least fixpoint does not specify whether they are believed or not. We show that complete fixpoints of the derivation operator correspond to Moore's stable expansions. In the case of modal representations of logic programs our least fixpoint semantics expresses well-founded semantics or 3-valued Fitting-Kunen semantics (depending on the embedding used). We show that, computationally, our semantics is simpler than the semantics proposed by Moore (assuming that the polynomial hierarchy does not collapse).",
        "published": "1999-01-12T18:44:40Z",
        "link": "http://arxiv.org/abs/cs/9901003v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.4, F.4.1, I.2.3"
        ]
    },
    {
        "title": "Extremal problems in logic programming and stable model computation",
        "authors": [
            "Pawel Cholewinski",
            "Miroslaw Truszczynski"
        ],
        "summary": "We study the following problem: given a class of logic programs C, determine the maximum number of stable models of a program from C. We establish the maximum for the class of all logic programs with at most n clauses, and for the class of all logic programs of size at most n. We also characterize the programs for which the maxima are attained. We obtain similar results for the class of all disjunctive logic programs with at most n clauses, each of length at most m, and for the class of all disjunctive logic programs of size at most n. Our results on logic programs have direct implication for the design of algorithms to compute stable models. Several such algorithms, similar in spirit to the Davis-Putnam procedure, are described in the paper. Our results imply that there is an algorithm that finds all stable models of a program with n clauses after considering the search space of size O(3^{n/3}) in the worst case. Our results also provide some insights into the question of representability of families of sets as families of stable models of logic programs.",
        "published": "1999-01-25T14:44:20Z",
        "link": "http://arxiv.org/abs/cs/9901012v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Minimum Description Length Induction, Bayesianism, and Kolmogorov   Complexity",
        "authors": [
            "Paul Vitanyi",
            "Ming Li"
        ],
        "summary": "The relationship between the Bayesian approach and the minimum description length approach is established. We sharpen and clarify the general modeling principles MDL and MML, abstracted as the ideal MDL principle and defined from Bayes's rule by means of Kolmogorov complexity. The basic condition under which the ideal principle should be applied is encapsulated as the Fundamental Inequality, which in broad terms states that the principle is valid when the data are random, relative to every contemplated hypothesis and also these hypotheses are random relative to the (universal) prior. Basically, the ideal principle states that the prior probability associated with the hypothesis should be given by the algorithmic universal probability, and the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized. If we restrict the model class to the finite sets then application of the ideal principle turns into Kolmogorov's minimal sufficient statistic. In general we show that data compression is almost always the best strategy, both in hypothesis identification and prediction.",
        "published": "1999-01-27T17:48:14Z",
        "link": "http://arxiv.org/abs/cs/9901014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LO",
            "math.IT",
            "math.PR",
            "physics.data-an",
            "E.4,F.2,H.3,I.2,I.5,I.7"
        ]
    },
    {
        "title": "Representation Theory for Default Logic",
        "authors": [
            "Victor Marek",
            "Jan Treur",
            "Miroslaw Truszczynski"
        ],
        "summary": "Default logic can be regarded as a mechanism to represent families of belief sets of a reasoning agent. As such, it is inherently second-order. In this paper, we study the problem of representability of a family of theories as the set of extensions of a default theory. We give a complete solution to the representability by means of normal default theories. We obtain partial results on representability by arbitrary default theories. We construct examples of denumerable families of non-including theories that are not representable. We also study the concept of equivalence between default theories.",
        "published": "1999-01-28T21:57:15Z",
        "link": "http://arxiv.org/abs/cs/9901016v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.4, F.4.1, I.2.3"
        ]
    },
    {
        "title": "Probabilistic Inductive Inference:a Survey",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "Inductive inference is a recursion-theoretic theory of learning, first developed by E. M. Gold (1967). This paper surveys developments in probabilistic inductive inference. We mainly focus on finite inference of recursive functions, since this simple paradigm has produced the most interesting (and most complex) results.",
        "published": "1999-02-15T01:52:45Z",
        "link": "http://arxiv.org/abs/cs/9902026v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.LO",
            "math.LO",
            "F.1.1., F.4.1., I.2.3., I.2.6"
        ]
    },
    {
        "title": "Designing SAT for HCP",
        "authors": [
            "Anatoly D. Plotnikov"
        ],
        "summary": "For arbitrary undirected graph $G$, we are designing SATISFIABILITY problem (SAT) for HCP, using tools of Boolean algebra only. The obtained SAT be the logic formulation of conditions for Hamiltonian cycle existence, and use $m$ Boolean variables, where $m$ is the number of graph edges. This Boolean expression is true if and only if an initial graph is Hamiltonian. That is, each satisfying assignment of the Boolean variables determines a Hamiltonian cycle of $G$, and each Hamiltonian cycle of $G$ corresponds to a satisfying assignment of the Boolean variables. In common case, the obtained Boolean expression may has an exponential length (the number of Boolean literals).",
        "published": "1999-03-05T16:10:41Z",
        "link": "http://arxiv.org/abs/cs/9903006v1",
        "categories": [
            "cs.LO",
            "F.4.1;G.2.1;G.2.2"
        ]
    },
    {
        "title": "Some Remarks on the Geometry of Grammar",
        "authors": [
            "Marc Dymetman"
        ],
        "summary": "This paper, following (Dymetman:1998), presents an approach to grammar description and processing based on the geometry of cancellation diagrams, a concept which plays a central role in combinatorial group theory (Lyndon-Schuppe:1977). The focus here is on the geometric intuitions and on relating group-theoretical diagrams to the traditional charts associated with context-free grammars and type-0 rewriting systems. The paper is structured as follows. We begin in Section 1 by analyzing charts in terms of constructs called cells, which are a geometrical counterpart to rules. Then we move in Section 2 to a presentation of cancellation diagrams and show how they can be used computationally. In Section 3 we give a formal algebraic presentation of the concept of group computation structure, which is based on the standard notions of free group and conjugacy. We then relate in Section 4 the geometric and the algebraic views of computation by using the fundamental theorem of combinatorial group theory (Rotman:1994). In Section 5 we study in more detail the relationship between the two views on the basis of a simple grammar stated as a group computation structure. In section 6 we extend this grammar to handle non-local constructs such as relative pronouns and quantifiers. We conclude in Section 7 with some brief notes on the differences between normal submonoids and normal subgroups, group computation versus rewriting systems, and the use of group morphisms to study the computational complexity of parsing and generation.",
        "published": "1999-03-05T18:25:11Z",
        "link": "http://arxiv.org/abs/cs/9903007v1",
        "categories": [
            "cs.CL",
            "cs.LO",
            "I.2.7; F.4.2"
        ]
    },
    {
        "title": "MSO definable string transductions and two-way finite state transducers",
        "authors": [
            "Joost Engelfriet",
            "Hendrik Jan Hoogeboom"
        ],
        "summary": "String transductions that are definable in monadic second-order (mso) logic (without the use of parameters) are exactly those realized by deterministic two-way finite state transducers. Nondeterministic mso definable string transductions (i.e., those definable with the use of parameters) correspond to compositions of two nondeterministic two-way finite state transducers that have the finite visit property. Both families of mso definable string transductions are characterized in terms of Hennie machines, i.e., two-way finite state transducers with the finite visit property that are allowed to rewrite their input tape.",
        "published": "1999-06-04T13:21:01Z",
        "link": "http://arxiv.org/abs/cs/9906007v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.4.3; F.1.1"
        ]
    },
    {
        "title": "Predicate Logic with Definitions",
        "authors": [
            "Victor Makarov"
        ],
        "summary": "Predicate Logic with Definitions (PLD or D-logic) is a modification of first-order logic intended mostly for practical formalization of mathematics. The main syntactic constructs of D-logic are terms, formulas and definitions. A definition is a definition of variables, a definition of constants, or a composite definition (D-logic has also abbreviation definitions called abbreviations). Definitions can be used inside terms and formulas. This possibility alleviates introducing new quantifier-like names. Composite definitions allow constructing new definitions from existing ones.",
        "published": "1999-06-07T20:16:55Z",
        "link": "http://arxiv.org/abs/cs/9906010v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Combining Inclusion Polymorphism and Parametric Polymorphism",
        "authors": [
            "Sabine Glesner",
            "Karl Stroetmann"
        ],
        "summary": "We show that the question whether a term is typable is decidable for type systems combining inclusion polymorphism with parametric polymorphism provided the type constructors are at most unary. To prove this result we first reduce the typability problem to the problem of solving a system of type inequations. The result is then obtained by showing that the solvability of the resulting system of type inequations is decidable.",
        "published": "1999-06-14T09:42:11Z",
        "link": "http://arxiv.org/abs/cs/9906013v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.3; F.3.3"
        ]
    },
    {
        "title": "No information can be conveyed by certain events: The case of the clever   widows of Fornicalia and the Stobon Oracle",
        "authors": [
            "Anand Venkataraman",
            "Ray Kemp"
        ],
        "summary": "In this short article, we look at an old logical puzzle, its solution and proof and discuss some interesting aspects concerning its representation in a logic programming language like Prolog. We also discuss an intriguing information theoretic aspect of the puzzle.",
        "published": "1999-07-08T23:16:17Z",
        "link": "http://arxiv.org/abs/cs/9907014v1",
        "categories": [
            "cs.LO",
            "cs.GL",
            "E.4; D.1.6; D.3.0"
        ]
    },
    {
        "title": "Weak length induction and slow growing depth boolean circuits",
        "authors": [
            "Satoru Kuroda"
        ],
        "summary": "We define a hierarchy of circuit complexity classes LD^i, whose depth are the inverse of a function in Ackermann hierarchy. Then we introduce extremely weak versions of length induction and construct a bounded arithmetic theory L^i_2 whose provably total functions exactly correspond to functions computable by LD^i circuits. Finally, we prove a non-conservation result between L^i_2 and a weaker theory AC^0CA which corresponds to the class AC^0. Our proof utilizes KPT witnessing theorem.",
        "published": "1999-07-16T08:41:25Z",
        "link": "http://arxiv.org/abs/cs/9907022v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "The Alma Project, or How First-Order Logic Can Help Us in Imperative   Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Andrea Schaerf"
        ],
        "summary": "The aim of the Alma project is the design of a strongly typed constraint programming language that combines the advantages of logic and imperative programming. The first stage of the project was the design and implementation of Alma-0, a small programming language that provides a support for declarative programming within the imperative programming framework. It is obtained by extending a subset of Modula-2 by a small number of features inspired by the logic programming paradigm. In this paper we discuss the rationale for the design of Alma-0, the benefits of the resulting hybrid programming framework, and the current work on adding constraint processing capabilities to the language. In particular, we discuss the role of the logical and customary variables, the interaction between the constraint store and the program, and the need for lists.",
        "published": "1999-07-19T09:36:05Z",
        "link": "http://arxiv.org/abs/cs/9907027v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2;F.3.2;F.3.3;I.2.8;I.5.5"
        ]
    },
    {
        "title": "Clausal Temporal Resolution",
        "authors": [
            "Michael Fisher",
            "Clare Dixon",
            "Martin Peim"
        ],
        "summary": "In this article, we examine how clausal resolution can be applied to a specific, but widely used, non-classical logic, namely discrete linear temporal logic. Thus, we first define a normal form for temporal formulae and show how arbitrary temporal formulae can be translated into the normal form, while preserving satisfiability. We then introduce novel resolution rules that can be applied to formulae in this normal form, provide a range of examples and examine the correctness and complexity of this approach is examined and. This clausal resolution approach. Finally, we describe related work and future developments concerning this work.",
        "published": "1999-07-21T15:48:06Z",
        "link": "http://arxiv.org/abs/cs/9907032v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3;F.4.1"
        ]
    },
    {
        "title": "Extending the Stable Model Semantics with More Expressive Rules",
        "authors": [
            "Patrik Simons"
        ],
        "summary": "The rules associated with propositional logic programs and the stable model semantics are not expressive enough to let one write concise programs. This problem is alleviated by introducing some new types of propositional rules. Together with a decision procedure that has been used as a base for an efficient implementation, the new rules supplant the standard ones in practical applications of the stable model semantics.",
        "published": "1999-08-06T06:01:43Z",
        "link": "http://arxiv.org/abs/cs/9908004v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3; I.2.8; F.4.1"
        ]
    },
    {
        "title": "Reasoning About Common Knowledge with Infinitely Many Agents",
        "authors": [
            "Joseph Y. Halpern",
            "Richard A. Shore"
        ],
        "summary": "Complete axiomatizations and exponential-time decision procedures are provided for reasoning about knowledge and common knowledge when there are infinitely many agents. The results show that reasoning about knowledge and common knowledge with infinitely many agents is no harder than when there are finitely many agents, provided that we can check the cardinality of certain set differences G - G', where G and G' are sets of agents. Since our complexity results are independent of the cardinality of the sets G involved, they represent improvements over the previous results even with the sets of agents involved are finite. Moreover, our results make clear the extent to which issues of complexity and completeness depend on how the sets of agents involved are represented.",
        "published": "1999-09-21T20:43:46Z",
        "link": "http://arxiv.org/abs/cs/9909014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Knowledge in Multi-Agent Systems: Initial Configurations and Broadcast",
        "authors": [
            "A. R. Lomuscio",
            "R. van der Meyden",
            "M. D. Ryan"
        ],
        "summary": "The semantic framework for the modal logic of knowledge due to Halpern and Moses provides a way to ascribe knowledge to agents in distributed and multi-agent systems. In this paper we study two special cases of this framework: full systems and hypercubes. Both model static situations in which no agent has any information about another agent's state. Full systems and hypercubes are an appropriate model for the initial configurations of many systems of interest. We establish a correspondence between full systems and hypercube systems and certain classes of Kripke frames. We show that these classes of systems correspond to the same logic. Moreover, this logic is also the same as that generated by the larger class of weakly directed frames. We provide a sound and complete axiomatization, S5WDn, of this logic. Finally, we show that under certain natural assumptions, in a model where knowledge evolves over time, S5WDn characterizes the properties of knowledge not just at the initial configuration, but also at all later configurations. In particular, this holds for homogeneous broadcast systems, which capture settings in which agents are initially ignorant of each others local states, operate synchronously, have perfect recall and can communicate only by broadcasting.",
        "published": "1999-09-30T17:03:47Z",
        "link": "http://arxiv.org/abs/cs/9909019v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.4"
        ]
    },
    {
        "title": "Fixed-parameter tractability, definability, and model checking",
        "authors": [
            "Joerg Flum",
            "Martin Grohe"
        ],
        "summary": "In this article, we study parameterized complexity theory from the perspective of logic, or more specifically, descriptive complexity theory.   We propose to consider parameterized model-checking problems for various fragments of first-order logic as generic parameterized problems and show how this approach can be useful in studying both fixed-parameter tractability and intractability. For example, we establish the equivalence between the model-checking for existential first-order logic, the homomorphism problem for relational structures, and the substructure isomorphism problem. Our main tractability result shows that model-checking for first-order formulas is fixed-parameter tractable when restricted to a class of input structures with an excluded minor. On the intractability side, for every t >= 0 we prove an equivalence between model-checking for first-order formulas with t quantifier alternations and the parameterized halting problem for alternating Turing machines with t alternations. We discuss the close connection between this alternation hierarchy and Downey and Fellows' W-hierarchy.   On a more abstract level, we consider two forms of definability, called Fagin definability and slicewise definability, that are appropriate for describing parameterized problems. We give a characterization of the class FPT of all fixed-parameter tractable problems in terms of slicewise definability in finite variable least fixed-point logic, which is reminiscent of the Immerman-Vardi Theorem characterizing the class PTIME in terms of definability in least fixed-point logic.",
        "published": "1999-10-01T15:10:00Z",
        "link": "http://arxiv.org/abs/cs/9910001v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3;F.4.1"
        ]
    },
    {
        "title": "The Complexity of Temporal Logic over the Reals",
        "authors": [
            "M. Reynolds"
        ],
        "summary": "It is shown that the decision problem for the temporal logic with until and since connectives over real-numbers time is PSPACE-complete.",
        "published": "1999-10-13T04:04:00Z",
        "link": "http://arxiv.org/abs/cs/9910012v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F4.1;F2.2"
        ]
    },
    {
        "title": "Processor Verification Using Efficient Reductions of the Logic of   Uninterpreted Functions to Propositional Logic",
        "authors": [
            "Randal E. Bryant",
            "Steven German",
            "Miroslav N. Velev"
        ],
        "summary": "The logic of equality with uninterpreted functions (EUF) provides a means of abstracting the manipulation of data by a processor when verifying the correctness of its control logic. By reducing formulas in this logic to propositional formulas, we can apply Boolean methods such as Ordered Binary Decision Diagrams (BDDs) and Boolean satisfiability checkers to perform the verification.   We can exploit characteristics of the formulas describing the verification conditions to greatly simplify the propositional formulas generated. In particular, we exploit the property that many equations appear only in positive form. We can therefore reduce the set of interpretations of the function symbols that must be considered to prove that a formula is universally valid to those that are ``maximally diverse.''   We present experimental results demonstrating the efficiency of this approach when verifying pipelined processors using the method proposed by Burch and Dill.",
        "published": "1999-10-14T22:41:39Z",
        "link": "http://arxiv.org/abs/cs/9910014v2",
        "categories": [
            "cs.LO",
            "cs.AR",
            "F.4.1"
        ]
    },
    {
        "title": "A System of Interaction and Structure",
        "authors": [
            "Alessio Guglielmi"
        ],
        "summary": "This paper introduces a logical system, called BV, which extends multiplicative linear logic by a non-commutative self-dual logical operator. This extension is particularly challenging for the sequent calculus, and so far it is not achieved therein. It becomes very natural in a new formalism, called the calculus of structures, which is the main contribution of this work. Structures are formulae submitted to certain equational laws typical of sequents. The calculus of structures is obtained by generalising the sequent calculus in such a way that a new top-down symmetry of derivations is observed, and it employs inference rules that rewrite inside structures at any depth. These properties, in addition to allow the design of BV, yield a modular proof of cut elimination.",
        "published": "1999-10-28T09:17:34Z",
        "link": "http://arxiv.org/abs/cs/9910023v4",
        "categories": [
            "cs.LO",
            "F.4.1; F.1.2"
        ]
    },
    {
        "title": "The Complexity of Poor Man's Logic",
        "authors": [
            "Edith Hemaspaandra"
        ],
        "summary": "Motivated by description logics, we investigate what happens to the complexity of modal satisfiability problems if we only allow formulas built from literals, $\\wedge$, $\\Diamond$, and $\\Box$. Previously, the only known result was that the complexity of the satisfiability problem for K dropped from PSPACE-complete to coNP-complete (Schmidt-Schauss and Smolka, 1991 and Donini et al., 1992). In this paper we show that not all modal logics behave like K. In particular, we show that the complexity of the satisfiability problem with respect to frames in which each world has at least one successor drops from PSPACE-complete to P, but that in contrast the satisfiability problem with respect to the class of frames in which each world has at most two successors remains PSPACE-complete. As a corollary of the latter result, we also solve the open problem from Donini et al.'s complexity classification of description logics (Donini et al., 1997). In the last section, we classify the complexity of the satisfiability problem for K for all other restrictions on the set of operators.",
        "published": "1999-11-28T21:34:01Z",
        "link": "http://arxiv.org/abs/cs/9911014v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "A Discipline of Evolutionary Programming",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Genetic fitness optimization using small populations or small population updates across generations generally suffers from randomly diverging evolutions. We propose a notion of highly probable fitness optimization through feasible evolutionary computing runs on small size populations. Based on rapidly mixing Markov chains, the approach pertains to most types of evolutionary genetic algorithms, genetic programming and the like. We establish that for systems having associated rapidly mixing Markov chains and appropriate stationary distributions the new method finds optimal programs (individuals) with probability almost 1. To make the method useful would require a structured design methodology where the development of the program and the guarantee of the rapidly mixing property go hand in hand. We analyze a simple example to show that the method is implementable. More significant examples require theoretical advances, for example with respect to the Metropolis filter.",
        "published": "1999-02-02T16:17:16Z",
        "link": "http://arxiv.org/abs/cs/9902006v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.MA",
            "I.2,E.1,F.1"
        ]
    },
    {
        "title": "Resource Discovery in Trilogy",
        "authors": [
            "Franck Chevalier",
            "David Harle",
            "Geoffrey Smith"
        ],
        "summary": "Trilogy is a collaborative project whose key aim is the development of an integrated virtual laboratory to support research training within each institution and collaborative projects between the partners. In this paper, the architecture and underpinning platform of the system is described with particular emphasis being placed on the structure and the integration of the distributed database. A key element is the ontology that provides the multi-agent system with a conceptualisation specification of the domain; this ontology is explained, accompanied by a discussion how such a system is integrated and used within the virtual laboratory. Although in this paper, Telecommunications and in particular Broadband networks are used as exemplars, the underlying system principles are applicable to any domain where a combination of experimental and literature-based resources are required.",
        "published": "1999-02-08T21:23:39Z",
        "link": "http://arxiv.org/abs/cs/9902015v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.MA",
            "H.3.4;I.2.0"
        ]
    },
    {
        "title": "SIMMUNE, a tool for simulating and analyzing immune system behavior",
        "authors": [
            "M. Meier-Schellersheim",
            "G. Mack"
        ],
        "summary": "We present a new approach to the simulation and analysis of immune system behavior. The simulations that can be done with our software package called SIMMUNE are based on immunological data that describe the behavior of immune system agents (cells, molecules) on a microscopial (i.e. agent-agent interaction) scale by defining cellular stimulus-response mechanisms. Since the behavior of the agents in SIMMUNE can be very flexibly configured, its application is not limited to immune system simulations. We outline the principles of SIMMUNE's multiscale analysis of emergent structure within the simulated immune system that allow the identification of immunological contexts using minimal a priori assumptions about the higher level organization of the immune system.",
        "published": "1999-03-28T20:46:49Z",
        "link": "http://arxiv.org/abs/cs/9903017v1",
        "categories": [
            "cs.MA",
            "q-bio",
            "I.6.3;I.6.4;I.6.5"
        ]
    },
    {
        "title": "Collective Choice Theory in Collaborative Computing",
        "authors": [
            "Walter Eaves"
        ],
        "summary": "This paper presents some fundamental collective choice theory for information system designers, particularly those working in the field of computer-supported cooperative work. This paper is focused on a presentation of Arrow's Possibility and Impossibility theorems which form the fundamental boundary on the efficacy of collective choice: voting and selection procedures. It restates the conditions that Arrow placed on collective choice functions in more rigorous second-order logic, which could be used as a set of test conditions for implementations, and a useful probabilistic result for analyzing votes on issue pairs. It also describes some simple collective choice functions. There is also some discussion of how enterprises should approach putting their resources under collective control: giving an outline of a superstructure of performative agents to carry out this function and what distributing processing technology would be needed.",
        "published": "1999-05-10T17:07:11Z",
        "link": "http://arxiv.org/abs/cs/9905003v1",
        "categories": [
            "cs.MA",
            "cs.DC",
            "H.5.3 I.2.11 J.4 K.4.1 K.4.3 F.1.2"
        ]
    },
    {
        "title": "General Principles of Learning-Based Multi-Agent Systems",
        "authors": [
            "David H. Wolpert",
            "Kevin R. Wheeler",
            "Kagan Tumer"
        ],
        "summary": "We consider the problem of how to design large decentralized multi-agent systems (MAS's) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we do not want the agents to ``work at cross-purposes'' as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINs. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINs perform near optimally in a difficult variant of Arthur's bar problem (and in particular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINs in the leader-follower problem.",
        "published": "1999-05-10T22:20:40Z",
        "link": "http://arxiv.org/abs/cs/9905005v1",
        "categories": [
            "cs.MA",
            "adap-org",
            "cond-mat.stat-mech",
            "cs.DC",
            "cs.LG",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "Collective Intelligence for Control of Distributed Dynamical Systems",
        "authors": [
            "David H. Wolpert",
            "Kevin R. Wheeler",
            "Kagan Tumer"
        ],
        "summary": "We consider the El Farol bar problem, also known as the minority game (W. B. Arthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet and Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of the general problem of how to configure the nodal elements of a distributed dynamical system so that they do not ``work at cross purposes'', in that their collective dynamics avoids frustration and thereby achieves a provided global goal. We summarize a mathematical theory for such configuration applicable when (as in the bar problem) the global goal can be expressed as minimizing a global energy function and the nodes can be expressed as minimizers of local free energy functions. We show that a system designed with that theory performs nearly optimally for the bar problem.",
        "published": "1999-08-17T21:32:41Z",
        "link": "http://arxiv.org/abs/cs/9908013v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat",
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "An Introduction to Collective Intelligence",
        "authors": [
            "David H. Wolpert",
            "Kagan Tumer"
        ],
        "summary": "This paper surveys the emerging science of how to design a ``COllective INtelligence'' (COIN). A COIN is a large multi-agent system where:   (i) There is little to no centralized communication or control; and   (ii) There is a provided world utility function that rates the possible histories of the full system.   In particular, we are interested in COINs in which each agent runs a reinforcement learning (RL) algorithm. Rather than use a conventional modeling approach (e.g., model the system dynamics, and hand-tune agents to cooperate), we aim to solve the COIN design problem implicitly, via the ``adaptive'' character of the RL algorithms of each of the agents. This approach introduces an entirely new, profound design problem: Assuming the RL algorithms are able to achieve high rewards, what reward functions for the individual agents will, when pursued by those agents, result in high world utility? In other words, what reward functions will best ensure that we do not have phenomena like the tragedy of the commons, Braess's paradox, or the liquidity trap?   Although still very young, research specifically concentrating on the COIN design problem has already resulted in successes in artificial domains, in particular in packet-routing, the leader-follower problem, and in variants of Arthur's El Farol bar problem. It is expected that as it matures and draws upon other disciplines related to COINs, this research will greatly expand the range of tasks addressable by human engineers. Moreover, in addition to drawing on them, such a fully developed scie nce of COIN design may provide much insight into other already established scientific fields, such as economics, game theory, and population biology.",
        "published": "1999-08-17T22:49:19Z",
        "link": "http://arxiv.org/abs/cs/9908014v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat",
            "cs.DC",
            "cs.MA",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "Adaptivity in Agent-Based Routing for Data Networks",
        "authors": [
            "David H. Wolpert",
            "Sergey Kirshner",
            "Chris J. Merz",
            "Kagan Tumer"
        ],
        "summary": "Adaptivity, both of the individual agents and of the interaction structure among the agents, seems indispensable for scaling up multi-agent systems (MAS's) in noisy environments. One important consideration in designing adaptive agents is choosing their action spaces to be as amenable as possible to machine learning techniques, especially to reinforcement learning (RL) techniques. One important way to have the interaction structure connecting agents itself be adaptive is to have the intentions and/or actions of the agents be in the input spaces of the other agents, much as in Stackelberg games. We consider both kinds of adaptivity in the design of a MAS to control network packet routing.   We demonstrate on the OPNET event-driven network simulator the perhaps surprising fact that simply changing the action space of the agents to be better suited to RL can result in very large improvements in their potential performance: at their best settings, our learning-amenable router agents achieve throughputs up to three and one half times better than that of the standard Bellman-Ford routing algorithm, even when the Bellman-Ford protocol traffic is maintained. We then demonstrate that much of that potential improvement can be realized by having the agents learn their settings when the agent interaction structure is itself adaptive.",
        "published": "1999-12-20T19:25:31Z",
        "link": "http://arxiv.org/abs/cs/9912011v1",
        "categories": [
            "cs.MA",
            "adap-org",
            "cs.NI",
            "nlin.AO",
            "I.2.11 ; C.2.0"
        ]
    },
    {
        "title": "Avoiding Braess' Paradox through Collective Intelligence",
        "authors": [
            "Kagan Tumer",
            "David H. Wolpert"
        ],
        "summary": "In an Ideal Shortest Path Algorithm (ISPA), at each moment each router in a network sends all of its traffic down the path that will incur the lowest cost to that traffic. In the limit of an infinitesimally small amount of traffic for a particular router, its routing that traffic via an ISPA is optimal, as far as cost incurred by that traffic is concerned. We demonstrate though that in many cases, due to the side-effects of one router's actions on another routers performance, having routers use ISPA's is suboptimal as far as global aggregate cost is concerned, even when only used to route infinitesimally small amounts of traffic. As a particular example of this we present an instance of Braess' paradox for ISPA's, in which adding new links to a network decreases overall throughput. We also demonstrate that load-balancing, in which the routing decisions are made to optimize the global cost incurred by all traffic currently being routed, is suboptimal as far as global cost averaged across time is concerned. This is also due to \"side-effects\", in this case of current routing decision on future traffic.   The theory of COllective INtelligence (COIN) is concerned precisely with the issue of avoiding such deleterious side-effects. We present key concepts from that theory and use them to derive an idealized algorithm whose performance is better than that of the ISPA, even in the infinitesimal limit. We present experiments verifying this, and also showing that a machine-learning-based version of this COIN algorithm in which costs are only imprecisely estimated (a version potentially applicable in the real world) also outperforms the ISPA, despite having access to less information than does the ISPA. In particular, this COIN algorithm avoids Braess' paradox.",
        "published": "1999-12-20T21:21:39Z",
        "link": "http://arxiv.org/abs/cs/9912012v1",
        "categories": [
            "cs.DC",
            "adap-org",
            "cs.MA",
            "cs.NI",
            "nlin.AO",
            "C.2.0; I.2.11"
        ]
    },
    {
        "title": "An Empirical Approach to Temporal Reference Resolution (journal version)",
        "authors": [
            "Janyce Wiebe",
            "Thomas P. O'Hara",
            "Thorsten Ohrstrom-Sandgren",
            "Kenneth K. McKeever"
        ],
        "summary": "Scheduling dialogs, during which people negotiate the times of appointments, are common in everyday life. This paper reports the results of an in-depth empirical investigation of resolving explicit temporal references in scheduling dialogs. There are four phases of this work: data annotation and evaluation, model development, system implementation and evaluation, and model evaluation and analysis. The system and model were developed primarily on one set of data, and then applied later to a much more complex data set, to assess the generalizability of the model for the task being performed. Many different types of empirical methods are applied to pinpoint the strengths and weaknesses of the approach. Detailed annotation instructions were developed and an intercoder reliability study was performed, showing that naive annotators can reliably perform the targeted annotations. A fully automatic system has been developed and evaluated on unseen test data, with good results on both data sets. We adopt a pure realization of a recency-based focus model to identify precisely when it is and is not adequate for the task being addressed. In addition to system results, an in-depth evaluation of the model itself is presented, based on detailed manual annotations. The results are that few errors occur specifically due to the model of focus being used, and the set of anaphoric relations defined in the model are low in ambiguity for both data sets.",
        "published": "1999-01-13T17:37:00Z",
        "link": "http://arxiv.org/abs/cs/9901005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Compacting the Penn Treebank Grammar",
        "authors": [
            "Alexander Krotov",
            "Mark Hepple",
            "Robert Gaizauskas",
            "Yorick Wilks"
        ],
        "summary": "Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules -- rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision.",
        "published": "1999-01-31T18:57:45Z",
        "link": "http://arxiv.org/abs/cs/9902001v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Automatic Identification of Subjects for Textual Documents in Digital   Libraries",
        "authors": [
            "Kuang-hua Chen"
        ],
        "summary": "The amount of electronic documents in the Internet grows very quickly. How to effectively identify subjects for documents becomes an important issue. In past, the researches focus on the behavior of nouns in documents. Although subjects are composed of nouns, the constituents that determine which nouns are subjects are not only nouns. Based on the assumption that texts are well-organized and event-driven, nouns and verbs together contribute the process of subject identification. This paper considers four factors: 1) word importance, 2) word frequency, 3) word co-occurrence, and 4) word distance and proposes a model to identify subjects for textual documents. The preliminary experiments show that the performance of the proposed model is close to that of human beings.",
        "published": "1999-02-01T11:01:23Z",
        "link": "http://arxiv.org/abs/cs/9902002v1",
        "categories": [
            "cs.DL",
            "cs.CL",
            "H.3.1; H.3.7; I.2.7"
        ]
    },
    {
        "title": "Autocatalytic Theory of Meaning",
        "authors": [
            "Mark D. Roberts"
        ],
        "summary": "Recently it has been argued that autocatalytic theory could be applied to the origin of culture. Here possible application to a theory of meaning in the philosophy of language, called radical interpretation, is commented upon and compared to previous applications.",
        "published": "1999-02-16T15:34:02Z",
        "link": "http://arxiv.org/abs/cs/9902027v1",
        "categories": [
            "cs.CL",
            "adap-org",
            "nlin.AO",
            "I.2.6; J.4; I.2.7"
        ]
    },
    {
        "title": "The \"Fodor\"-FODOR fallacy bites back",
        "authors": [
            "Yorick Wilks"
        ],
        "summary": "The paper argues that Fodor and Lepore are misguided in their attack on Pustejovsky's Generative Lexicon, largely because their argument rests on a traditional, but implausible and discredited, view of the lexicon on which it is effectively empty of content, a view that stands in the long line of explaining word meaning (a) by ostension and then (b) explaining it by means of a vacuous symbol in a lexicon, often the word itself after typographic transmogrification. (a) and (b) both share the wrong belief that to a word must correspond a simple entity that is its meaning. I then turn to the semantic rules that Pustejovsky uses and argue first that, although they have novel features, they are in a well-established Artificial Intelligence tradition of explaining meaning by reference to structures that mention other structures assigned to words that may occur in close proximity to the first. It is argued that Fodor and Lepore's view that there cannot be such rules is without foundation, and indeed systems using such rules have proved their practical worth in computational systems. Their justification descends from line of argument, whose high points were probably Wittgenstein and Quine that meaning is not to be understood by simple links to the world, ostensive or otherwise, but by the relationship of whole cultural representational structures to each other and to the world as a whole.",
        "published": "1999-02-25T14:41:24Z",
        "link": "http://arxiv.org/abs/cs/9902029v1",
        "categories": [
            "cs.CL",
            "I.2"
        ]
    },
    {
        "title": "Is Word Sense Disambiguation just one more NLP task?",
        "authors": [
            "Yorick Wilks"
        ],
        "summary": "This paper compares the tasks of part-of-speech (POS) tagging and word-sense-tagging or disambiguation (WSD), and argues that the tasks are not related by fineness of grain or anything like that, but are quite different kinds of task, particularly becuase there is nothing in POS corresponding to sense novelty. The paper also argues for the reintegration of sub-tasks that are being separated for evaluation",
        "published": "1999-02-25T14:41:32Z",
        "link": "http://arxiv.org/abs/cs/9902030v1",
        "categories": [
            "cs.CL",
            "I.2"
        ]
    },
    {
        "title": "A Formal Framework for Linguistic Annotation",
        "authors": [
            "Steven Bird",
            "Mark Liberman"
        ],
        "summary": "`Linguistic annotation' covers any descriptive or analytic notations applied to raw language data. The basic data may be in the form of time functions -- audio, video and/or physiological recordings -- or it may be textual. The added notations may include transcriptions of all sorts (from phonetic features to discourse structures), part-of-speech and sense tagging, syntactic analysis, `named entity' identification, co-reference annotation, and so on. While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem. Proposed standards, to the extent they exist, have focussed on file formats. This paper focuses instead on the logical structure of linguistic annotations. We survey a wide variety of existing annotation formats and demonstrate a common conceptual core, the annotation graph. This provides a formal framework for constructing, maintaining and searching linguistic annotations, while remaining consistent with many alternative data structures and file formats.",
        "published": "1999-03-02T12:30:55Z",
        "link": "http://arxiv.org/abs/cs/9903003v1",
        "categories": [
            "cs.CL",
            "A.1; E.2; H.2.1; H.3.3; H.3.7; I.2.7"
        ]
    },
    {
        "title": "Some Remarks on the Geometry of Grammar",
        "authors": [
            "Marc Dymetman"
        ],
        "summary": "This paper, following (Dymetman:1998), presents an approach to grammar description and processing based on the geometry of cancellation diagrams, a concept which plays a central role in combinatorial group theory (Lyndon-Schuppe:1977). The focus here is on the geometric intuitions and on relating group-theoretical diagrams to the traditional charts associated with context-free grammars and type-0 rewriting systems. The paper is structured as follows. We begin in Section 1 by analyzing charts in terms of constructs called cells, which are a geometrical counterpart to rules. Then we move in Section 2 to a presentation of cancellation diagrams and show how they can be used computationally. In Section 3 we give a formal algebraic presentation of the concept of group computation structure, which is based on the standard notions of free group and conjugacy. We then relate in Section 4 the geometric and the algebraic views of computation by using the fundamental theorem of combinatorial group theory (Rotman:1994). In Section 5 we study in more detail the relationship between the two views on the basis of a simple grammar stated as a group computation structure. In section 6 we extend this grammar to handle non-local constructs such as relative pronouns and quantifiers. We conclude in Section 7 with some brief notes on the differences between normal submonoids and normal subgroups, group computation versus rewriting systems, and the use of group morphisms to study the computational complexity of parsing and generation.",
        "published": "1999-03-05T18:25:11Z",
        "link": "http://arxiv.org/abs/cs/9903007v1",
        "categories": [
            "cs.CL",
            "cs.LO",
            "I.2.7; F.4.2"
        ]
    },
    {
        "title": "Empirically Evaluating an Adaptable Spoken Dialogue System",
        "authors": [
            "Diane J. Litman",
            "Shimei Pan"
        ],
        "summary": "Recent technological advances have made it possible to build real-time, interactive spoken dialogue systems for a wide variety of applications. However, when users do not respect the limitations of such systems, performance typically degrades. Although users differ with respect to their knowledge of system limitations, and although different dialogue strategies make system limitations more apparent to users, most current systems do not try to improve performance by adapting dialogue behavior to individual users. This paper presents an empirical evaluation of TOOT, an adaptable spoken dialogue system for retrieving train schedules on the web. We conduct an experiment in which 20 users carry out 4 tasks with both adaptable and non-adaptable versions of TOOT, resulting in a corpus of 80 dialogues. The values for a wide range of evaluation measures are then extracted from this corpus. Our results show that adaptable TOOT generally outperforms non-adaptable TOOT, and that the utility of adaptation depends on TOOT's initial dialogue strategies.",
        "published": "1999-03-05T22:03:13Z",
        "link": "http://arxiv.org/abs/cs/9903008v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Mixing Metaphors",
        "authors": [
            "Mark Lee",
            "John Barnden"
        ],
        "summary": "Mixed metaphors have been neglected in recent metaphor research. This paper suggests that such neglect is short-sighted. Though mixing is a more complex phenomenon than straight metaphors, the same kinds of reasoning and knowledge structures are required. This paper provides an analysis of both parallel and serial mixed metaphors within the framework of an AI system which is already capable of reasoning about straight metaphorical manifestations and argues that the processes underlying mixing are central to metaphorical meaning. Therefore, any theory of metaphors must be able to account for mixing.",
        "published": "1999-04-12T11:37:49Z",
        "link": "http://arxiv.org/abs/cs/9904004v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.0; I.2.7"
        ]
    },
    {
        "title": "Transducers from Rewrite Rules with Backreferences",
        "authors": [
            "Dale Gerdemann",
            "Gertjan van Noord"
        ],
        "summary": "Context sensitive rewrite rules have been widely used in several areas of natural language processing, including syntax, morphology, phonology and speech processing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various algorithms to compile such rewrite rules into finite-state transducers. The present paper extends this work by allowing a limited form of backreferencing in such rules. The explicit use of backreferencing leads to more elegant and general solutions.",
        "published": "1999-04-15T14:00:41Z",
        "link": "http://arxiv.org/abs/cs/9904008v1",
        "categories": [
            "cs.CL",
            "F.1.1; F.4.3; I.2.1; J.5"
        ]
    },
    {
        "title": "An ascription-based approach to speech acts",
        "authors": [
            "Mark Lee",
            "Yorick Wilks"
        ],
        "summary": "The two principal areas of natural language processing research in pragmatics are belief modelling and speech act processing. Belief modelling is the development of techniques to represent the mental attitudes of a dialogue participant. The latter approach, speech act processing, based on speech act theory, involves viewing dialogue in planning terms. Utterances in a dialogue are modelled as steps in a plan where understanding an utterance involves deriving the complete plan a speaker is attempting to achieve. However, previous speech act based approaches have been limited by a reliance upon relatively simplistic belief modelling techniques and their relationship to planning and plan recognition. In particular, such techniques assume precomputed nested belief structures. In this paper, we will present an approach to speech act processing based on novel belief modelling techniques where nested beliefs are propagated on demand.",
        "published": "1999-04-15T16:03:27Z",
        "link": "http://arxiv.org/abs/cs/9904009v1",
        "categories": [
            "cs.CL",
            "I.2.0; I.2.1; I.2.7"
        ]
    },
    {
        "title": "A Computational Memory and Processing Model for Processing for Prosody",
        "authors": [
            "Janet E. Cahn"
        ],
        "summary": "This paper links prosody to the information in a text and how it is processed by the speaker. It describes the operation and output of LOQ, a text-to-speech implementation that includes a model of limited attention and working memory. Attentional limitations are key. Varying the attentional parameter in the simulations varies in turn what counts as given and new in a text, and therefore, the intonational contours with which it is uttered. Currently, the system produces prosody in three different styles: child-like, adult expressive, and knowledgeable. This prosody also exhibits differences within each style -- no two simulations are alike. The limited resource approach captures some of the stylistic and individual variety found in natural prosody.",
        "published": "1999-04-24T23:45:26Z",
        "link": "http://arxiv.org/abs/cs/9904018v1",
        "categories": [
            "cs.CL",
            "I.2.7, I.2.0"
        ]
    },
    {
        "title": "Supervised Grammar Induction Using Training Data with Limited   Constituent Information",
        "authors": [
            "Rebecca Hwa"
        ],
        "summary": "Corpus-based grammar induction generally relies on hand-parsed training data to learn the structure of the language. Unfortunately, the cost of building large annotated corpora is prohibitively expensive. This work aims to improve the induction strategy when there are few labels in the training data. We show that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses. They account for only 20% of all constituents. For inducing grammars from sparsely labeled training data (e.g., only higher-level constituent labels), we propose an adaptation strategy, which produces grammars that parse almost as well as grammars induced from fully labeled corpora. Our results suggest that for a partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases.",
        "published": "1999-05-02T20:48:21Z",
        "link": "http://arxiv.org/abs/cs/9905001v1",
        "categories": [
            "cs.CL",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "An Efficient, Probabilistically Sound Algorithm for Segmentation and   Word Discovery",
        "authors": [
            "Michael R. Brent"
        ],
        "summary": "This paper presents a model-based, unsupervised algorithm for recovering word boundaries in a natural-language text from which they have been deleted. The algorithm is derived from a probability model of the source that generated the text. The fundamental structure of the model is specified abstractly so that the detailed component models of phonology, word-order, and word frequency can be replaced in a modular fashion. The model yields a language-independent, prior probability distribution on all possible sequences of all possible words over a given alphabet, based on the assumption that the input was generated by concatenating words from a fixed but unknown lexicon. The model is unusual in that it treats the generation of a complete corpus, regardless of length, as a single event in the probability space. Accordingly, the algorithm does not estimate a probability distribution on words; instead, it attempts to calculate the prior probabilities of various word sequences that could underlie the observed text. Experiments on phonemic transcripts of spontaneous speech by parents to young children suggest that this algorithm is more effective than other proposed algorithms, at least when utterance boundaries are given and the text includes a substantial number of short utterances.   Keywords: Bayesian grammar induction, probability models, minimum description length (MDL), unsupervised learning, cognitive modeling, language acquisition, segmentation",
        "published": "1999-05-12T14:25:40Z",
        "link": "http://arxiv.org/abs/cs/9905007v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.0;I.2.6;I.2.7"
        ]
    },
    {
        "title": "Inside-Outside Estimation of a Lexicalized PCFG for German",
        "authors": [
            "Franz Beil",
            "Glenn Carroll",
            "Detlef Prescher",
            "Stefan Riezler",
            "Mats Rooth"
        ],
        "summary": "The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verb-final clauses. Grammar and formalism features which make the experiment feasible are described. Successive models are evaluated on precision and recall of phrase markup.",
        "published": "1999-05-19T14:47:21Z",
        "link": "http://arxiv.org/abs/cs/9905009v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering",
        "authors": [
            "Mats Rooth",
            "Stefan Riezler",
            "Detlef Prescher",
            "Glenn Carroll",
            "Franz Beil"
        ],
        "summary": "We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.",
        "published": "1999-05-19T14:52:33Z",
        "link": "http://arxiv.org/abs/cs/9905008v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.6; I.2.7; I.5.3"
        ]
    },
    {
        "title": "Statistical Inference and Probabilistic Modelling for Constraint-Based   NLP",
        "authors": [
            "Stefan Riezler"
        ],
        "summary": "We present a probabilistic model for constraint-based grammars and a method for estimating the parameters of such models from incomplete, i.e., unparsed data. Whereas methods exist to estimate the parameters of probabilistic context-free grammars from incomplete data (Baum 1970), so far for probabilistic grammars involving context-dependencies only parameter estimation techniques from complete, i.e., fully parsed data have been presented (Abney 1997). However, complete-data estimation requires labor-intensive, error-prone, and grammar-specific hand-annotating of large language corpora. We present a log-linear probability model for constraint logic programming, and a general algorithm to estimate the parameters of such models from incomplete data by extending the estimation algorithm of Della-Pietra, Della-Pietra, and Lafferty (1997) to incomplete data settings.",
        "published": "1999-05-19T16:03:05Z",
        "link": "http://arxiv.org/abs/cs/9905010v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "The syntactic processing of particles in Japanese spoken language",
        "authors": [
            "Melanie Siegel"
        ],
        "summary": "Particles fullfill several distinct central roles in the Japanese language. They can mark arguments as well as adjuncts, can be functional or have semantic funtions. There is, however, no straightforward matching from particles to functions, as, e.g., GA can mark the subject, the object or an adjunct of a sentence. Particles can cooccur. Verbal arguments that could be identified by particles can be eliminated in the Japanese sentence. And finally, in spoken language particles are often omitted. A proper treatment of particles is thus necessary to make an analysis of Japanese sentences possible. Our treatment is based on an empirical investigation of 800 dialogues. We set up a type hierarchy of particles motivated by their subcategorizational and modificational behaviour. This type hierarchy is part of the Japanese syntax in VERBMOBIL.",
        "published": "1999-06-02T12:03:14Z",
        "link": "http://arxiv.org/abs/cs/9906003v1",
        "categories": [
            "cs.CL",
            "F.2.2"
        ]
    },
    {
        "title": "Cascaded Grammatical Relation Assignment",
        "authors": [
            "Sabine Buchholz",
            "Jorn Veenstra",
            "Walter Daelemans"
        ],
        "summary": "In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.",
        "published": "1999-06-02T13:41:51Z",
        "link": "http://arxiv.org/abs/cs/9906004v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.6.2;I.7.1"
        ]
    },
    {
        "title": "Memory-Based Shallow Parsing",
        "authors": [
            "Walter Daelemans",
            "Sabine Buchholz",
            "Jorn Veenstra"
        ],
        "summary": "We present a memory-based learning (MBL) approach to shallow parsing in which POS tagging, chunking, and identification of syntactic relations are formulated as memory-based modules. The experiments reported in this paper show competitive results, the F-value for the Wall Street Journal (WSJ) treebank is: 93.8% for NP chunking, 94.7% for VP chunking, 77.1% for subject detection and 79.0% for object detection.",
        "published": "1999-06-02T13:48:48Z",
        "link": "http://arxiv.org/abs/cs/9906005v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.6.2;I.7.1"
        ]
    },
    {
        "title": "Learning Efficient Disambiguation",
        "authors": [
            "Khalil Sima'an"
        ],
        "summary": "This dissertation analyses the computational properties of current performance-models of natural language parsing, in particular Data Oriented Parsing (DOP), points out some of their major shortcomings and suggests suitable solutions. It provides proofs that various problems of probabilistic disambiguation are NP-Complete under instances of these performance-models, and it argues that none of these models accounts for attractive efficiency properties of human language processing in limited domains, e.g. that frequent inputs are usually processed faster than infrequent ones. The central hypothesis of this dissertation is that these shortcomings can be eliminated by specializing the performance-models to the limited domains. The dissertation addresses \"grammar and model specialization\" and presents a new framework, the Ambiguity-Reduction Specialization (ARS) framework, that formulates the necessary and sufficient conditions for successful specialization. The framework is instantiated into specialization algorithms and applied to specializing DOP. Novelties of these learning algorithms are 1) they limit the hypotheses-space to include only \"safe\" models, 2) are expressed as constrained optimization formulae that minimize the entropy of the training tree-bank given the specialized grammar, under the constraint that the size of the specialized model does not exceed a predefined maximum, and 3) they enable integrating the specialized model with the original one in a complementary manner. The dissertation provides experiments with initial implementations and compares the resulting Specialized DOP (SDOP) models to the original DOP models with encouraging results.",
        "published": "1999-06-02T15:50:26Z",
        "link": "http://arxiv.org/abs/cs/9906006v2",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.6, I.2.7, J.5, F.2"
        ]
    },
    {
        "title": "Cascaded Markov Models",
        "authors": [
            "Thorsten Brants"
        ],
        "summary": "This paper presents a new approach to partial parsing of context-free structures. The approach is based on Markov Models. Each layer of the resulting structure is represented by its own Markov Model, and output of a lower layer is passed as input to the next higher layer. An empirical evaluation of the method yields very good results for NP/PP chunking of German newspaper texts.",
        "published": "1999-06-06T17:36:34Z",
        "link": "http://arxiv.org/abs/cs/9906009v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Evaluation of the NLP Components of the OVIS2 Spoken Dialogue System",
        "authors": [
            "Gert Veldhuijzen van Zanten",
            "Gosse Bouma",
            "Khalil Sima'an",
            "Gertjan van Noord",
            "Remko Bonnema"
        ],
        "summary": "The NWO Priority Programme Language and Speech Technology is a 5-year research programme aiming at the development of spoken language information systems. In the Programme, two alternative natural language processing (NLP) modules are developed in parallel: a grammar-based (conventional, rule-based) module and a data-oriented (memory-based, stochastic, DOP) module. In order to compare the NLP modules, a formal evaluation has been carried out three years after the start of the Programme. This paper describes the evaluation procedure and the evaluation results. The grammar-based component performs much better than the data-oriented one in this comparison.",
        "published": "1999-06-14T10:06:31Z",
        "link": "http://arxiv.org/abs/cs/9906014v1",
        "categories": [
            "cs.CL",
            "H.4.0;H.5.1;H.5.2;I.2.7"
        ]
    },
    {
        "title": "Learning Transformation Rules to Find Grammatical Relations",
        "authors": [
            "Lisa Ferro",
            "Marc Vilain",
            "Alexander Yeh"
        ],
        "summary": "Grammatical relationships are an important level of natural language processing. We present a trainable approach to find these relationships through transformation sequences and error-driven learning. Our approach finds grammatical relationships between core syntax groups and bypasses much of the parsing phase. On our training and test set, our procedure achieves 63.6% recall and 77.3% precision (f-score = 69.8).",
        "published": "1999-06-14T22:06:24Z",
        "link": "http://arxiv.org/abs/cs/9906015v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Resolving Part-of-Speech Ambiguity in the Greek Language Using Learning   Techniques",
        "authors": [
            "G. Petasis",
            "G. Paliouras",
            "V. Karkaletsis",
            "C. D. Spyropoulos",
            "I. Androutsopoulos"
        ],
        "summary": "This article investigates the use of Transformation-Based Error-Driven learning for resolving part-of-speech ambiguity in the Greek language. The aim is not only to study the performance, but also to examine its dependence on different thematic domains. Results are presented here for two different test cases: a corpus on \"management succession events\" and a general-theme corpus. The two experiments show that the performance of this method does not depend on the thematic domain of the corpus, and its accuracy for the Greek language is around 95%.",
        "published": "1999-06-22T07:41:24Z",
        "link": "http://arxiv.org/abs/cs/9906019v2",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.6 ; I.2.7"
        ]
    },
    {
        "title": "Temporal Meaning Representations in a Natural Language Front-End",
        "authors": [
            "I. Androutsopoulos"
        ],
        "summary": "Previous work in the context of natural language querying of temporal databases has established a method to map automatically from a large subset of English time-related questions to suitable expressions of a temporal logic-like language, called TOP. An algorithm to translate from TOP to the TSQL2 temporal database language has also been defined. This paper shows how TOP expressions could be translated into a simpler logic-like language, called BOT. BOT is very close to traditional first-order predicate logic (FOPL), and hence existing methods to manipulate FOPL expressions can be exploited to interface to time-sensitive applications other than TSQL2 databases, maintaining the existing English-to-TOP mapping.",
        "published": "1999-06-22T08:28:26Z",
        "link": "http://arxiv.org/abs/cs/9906020v1",
        "categories": [
            "cs.CL",
            "I.2.7; F.4.1; H.5.2"
        ]
    },
    {
        "title": "Mapping Multilingual Hierarchies Using Relaxation Labeling",
        "authors": [
            "J. Daude",
            "L. Padro",
            "G. Rigau"
        ],
        "summary": "This paper explores the automatic construction of a multilingual Lexical Knowledge Base from pre-existing lexical resources. We present a new and robust approach for linking already existing lexical/semantic hierarchies. We used a constraint satisfaction algorithm (relaxation labeling) to select --among all the candidate translations proposed by a bilingual dictionary-- the right English WordNet synset for each sense in a taxonomy automatically derived from a Spanish monolingual dictionary. Although on average, there are 15 possible WordNet connections for each sense in the taxonomy, the method achieves an accuracy over 80%. Finally, we also propose several ways in which this technique could be applied to enrich and improve existing lexical databases.",
        "published": "1999-06-24T16:56:45Z",
        "link": "http://arxiv.org/abs/cs/9906025v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Robust Grammatical Analysis for Spoken Dialogue Systems",
        "authors": [
            "Gertjan van Noord",
            "Gosse Bouma",
            "Rob Koeling",
            "Mark-Jan Nederhof"
        ],
        "summary": "We argue that grammatical analysis is a viable alternative to concept spotting for processing spoken input in a practical spoken dialogue system. We discuss the structure of the grammar, and a model for robust parsing which combines linguistic sources of information and statistical sources of information. We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input.",
        "published": "1999-06-25T08:16:23Z",
        "link": "http://arxiv.org/abs/cs/9906026v1",
        "categories": [
            "cs.CL",
            "H.4.0;H.5.1;H.5.2;I.2.7"
        ]
    },
    {
        "title": "Human-Computer Conversation",
        "authors": [
            "Yorick Wilks",
            "Roberta Catizone"
        ],
        "summary": "The article surveys a little of the history of the technology, sets out the main current theoretical approaches in brief, and discusses the on-going opposition between theoretical and empirical approaches. It illustrates the situation with some discussion of CONVERSE, a system that won the Loebner prize in 1997 and which displays features of both approaches.",
        "published": "1999-06-25T11:44:42Z",
        "link": "http://arxiv.org/abs/cs/9906027v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "I.2.7;H.1.2"
        ]
    },
    {
        "title": "Events in Property Patterns",
        "authors": [
            "M. Chechik",
            "D. Paun"
        ],
        "summary": "A pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification was proposed by Dwyer and his collegues. The patterns enable non-experts to read and write formal specifications for realistic systems and facilitate easy conversion of specifications between formalisms, such as LTL, CTL, QRE. In this paper, we extend the pattern system with events - changes of values of variables in the context of LTL.",
        "published": "1999-06-28T17:06:51Z",
        "link": "http://arxiv.org/abs/cs/9906029v2",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CL",
            "cs.SC",
            "D.2.4;F.3.1;F.4.1;I.2.4;D.2.1"
        ]
    },
    {
        "title": "A Unified Example-Based and Lexicalist Approach to Machine Translation",
        "authors": [
            "Davide Turcato",
            "Paul McFetridge",
            "Fred Popowich",
            "Janine Toole"
        ],
        "summary": "We present an approach to Machine Translation that combines the ideas and methodologies of the Example-Based and Lexicalist theoretical frameworks. The approach has been implemented in a multilingual Machine Translation system.",
        "published": "1999-06-30T23:06:09Z",
        "link": "http://arxiv.org/abs/cs/9906034v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Annotation graphs as a framework for multidimensional linguistic data   analysis",
        "authors": [
            "Steven Bird",
            "Mark Liberman"
        ],
        "summary": "In recent work we have presented a formal framework for linguistic annotation based on labeled acyclic digraphs. These `annotation graphs' offer a simple yet powerful method for representing complex annotation structures incorporating hierarchy and overlap. Here, we motivate and illustrate our approach using discourse-level annotations of text and speech data drawn from the CALLHOME, COCONUT, MUC-7, DAMSL and TRAINS annotation schemes. With the help of domain specialists, we have constructed a hybrid multi-level annotation for a fragment of the Boston University Radio Speech Corpus which includes the following levels: segment, word, breath, ToBI, Tilt, Treebank, coreference and named entity. We show how annotation graphs can represent hybrid multi-level structures which derive from a diverse set of file formats. We also show how the approach facilitates substantive comparison of multiple annotations of a single signal based on different theoretical models. The discussion shows how annotation graphs open the door to wide-ranging integration of tools, formats and corpora.",
        "published": "1999-07-05T14:51:26Z",
        "link": "http://arxiv.org/abs/cs/9907003v1",
        "categories": [
            "cs.CL",
            "A.1; E.2; H.2.1; H.3.3; H.3.7; I.2.7"
        ]
    },
    {
        "title": "MAP Lexicon is useful for segmentation and word discovery in   child-directed speech",
        "authors": [
            "Anand Venkataraman"
        ],
        "summary": "Because of rather fundamental changes to the underlying model proposed in the paper, it has been withdrawn from the archive.",
        "published": "1999-07-06T01:44:00Z",
        "link": "http://arxiv.org/abs/cs/9907004v2",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Representing Text Chunks",
        "authors": [
            "Erik F. Tjong Kim Sang",
            "Jorn Veenstra"
        ],
        "summary": "Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a \"convenient\" data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.",
        "published": "1999-07-06T12:44:20Z",
        "link": "http://arxiv.org/abs/cs/9907006v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Cross-Language Information Retrieval for Technical Documents",
        "authors": [
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "This paper proposes a Japanese/English cross-language information retrieval (CLIR) system targeting technical documents. Our system first translates a given query containing technical terms into the target language, and then retrieves documents relevant to the translated query. The translation of technical terms is still problematic in that technical terms are often compound words, and thus new terms can be progressively created simply by combining existing base words. In addition, Japanese often represents loanwords based on its phonogram. Consequently, existing dictionaries find it difficult to achieve sufficient coverage. To counter the first problem, we use a compound word translation method, which uses a bilingual dictionary for base words and collocational statistics to resolve translation ambiguity. For the second problem, we propose a transliteration method, which identifies phonetic equivalents in the target language. We also show the effectiveness of our system using a test collection for CLIR.",
        "published": "1999-07-06T16:25:46Z",
        "link": "http://arxiv.org/abs/cs/9907007v2",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Explanation-based Learning for Machine Translation",
        "authors": [
            "Janine Toole",
            "Fred Popowich",
            "Devlan Nicholson",
            "Davide Turcato",
            "Paul McFetridge"
        ],
        "summary": "In this paper we present an application of explanation-based learning (EBL) in the parsing module of a real-time English-Spanish machine translation system designed to translate closed captions. We discuss the efficiency/coverage trade-offs available in EBL and introduce the techniques we use to increase coverage while maintaining a high level of space and time efficiency. Our performance results indicate that this approach is effective.",
        "published": "1999-07-06T18:35:41Z",
        "link": "http://arxiv.org/abs/cs/9907008v1",
        "categories": [
            "cs.CL",
            "J.5"
        ]
    },
    {
        "title": "Language Identification With Confidence Limits",
        "authors": [
            "David Elworthy"
        ],
        "summary": "A statistical classification algorithm and its application to language identification from noisy input are described. The main innovation is to compute confidence limits on the classification, so that the algorithm terminates when enough evidence to make a clear decision has been made, and so avoiding problems with categories that have similar characteristics. A second application, to genre identification, is briefly examined. The results show that some of the problems of other language identification techniques can be avoided, and illustrate a more important point: that a statistical language process can be used to provide feedback about its own success rate.",
        "published": "1999-07-07T09:28:40Z",
        "link": "http://arxiv.org/abs/cs/9907010v1",
        "categories": [
            "cs.CL",
            "I.2.7; I.5.3"
        ]
    },
    {
        "title": "Selective Magic HPSG Parsing",
        "authors": [
            "Guido Minnen"
        ],
        "summary": "We propose a parser for constraint-logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control. The parser allows the user to apply magic compilation to specific constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal-directed fashion. State of the art top-down processing techniques are used to deal with the remaining constraints. We discuss various aspects concerning the implementation of the parser as part of a grammar development system.",
        "published": "1999-07-08T09:46:37Z",
        "link": "http://arxiv.org/abs/cs/9907012v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Corpus Annotation for Parser Evaluation",
        "authors": [
            "John Carroll",
            "Guido Minnen",
            "Ted Briscoe"
        ],
        "summary": "We describe a recently developed corpus annotation scheme for evaluating parsers that avoids shortcomings of current methods. The scheme encodes grammatical relations between heads and dependents, and has been used to mark up a new public-domain corpus of naturally occurring English text. We show how the corpus can be used to evaluate the accuracy of a robust parser, and relate the corpus to extant resources.",
        "published": "1999-07-08T10:08:59Z",
        "link": "http://arxiv.org/abs/cs/9907013v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Bootstrap Approach to Automatically Generating Lexical Transfer Rules",
        "authors": [
            "Davide Turcato",
            "Paul McFetridge",
            "Fred Popowich",
            "Janine Toole"
        ],
        "summary": "We describe a method for automatically generating Lexical Transfer Rules (LTRs) from word equivalences using transfer rule templates. Templates are skeletal LTRs, unspecified for words. New LTRs are created by instantiating a template with words, provided that the words belong to the appropriate lexical categories required by the template. We define two methods for creating an inventory of templates and using them to generate new LTRs. A simpler method consists of extracting a finite set of templates from a sample of hand coded LTRs and directly using them in the generation process. A further method consists of abstracting over the initial finite set of templates to define higher level templates, where bilingual equivalences are defined in terms of correspondences involving phrasal categories. Phrasal templates are then mapped onto sets of lexical templates with the aid of grammars. In this way an infinite set of lexical templates is recursively defined. New LTRs are created by parsing input words, matching a template at the phrasal level and using the corresponding lexical categories to instantiate the lexical template. The definition of an infinite set of templates enables the automatic creation of LTRs for multi-word, non-compositional word equivalences of any cardinality.",
        "published": "1999-07-09T22:39:52Z",
        "link": "http://arxiv.org/abs/cs/9907017v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Architectural Considerations for Conversational Systems -- The   Verbmobil/INTARC Experience",
        "authors": [
            "Guenther Goerz",
            "Joerg Spilker",
            "Volker Strom",
            "Hans Weber"
        ],
        "summary": "The paper describes the speech to speech translation system INTARC, developed during the first phase of the Verbmobil project. The general design goals of the INTARC system architecture were time synchronous processing as well as incrementality and interactivity as a means to achieve a higher degree of robustness and scalability. Interactivity means that in addition to the bottom-up (in terms of processing levels) data flow the ability to process top-down restrictions considering the same signal segment for all processing levels. The construction of INTARC 2.0, which has been operational since fall 1996, followed an engineering approach focussing on the integration of symbolic (linguistic) and stochastic (recognition) techniques which led to a generalization of the concept of a ``one pass'' beam search.",
        "published": "1999-07-14T09:21:16Z",
        "link": "http://arxiv.org/abs/cs/9907021v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Mixing representation levels: The hybrid approach to automatic text   generation",
        "authors": [
            "Emanuele Pianta",
            "Lucia M. Tovena"
        ],
        "summary": "Natural language generation systems (NLG) map non-linguistic representations into strings of words through a number of steps using intermediate representations of various levels of abstraction. Template based systems, by contrast, tend to use only one representation level, i.e. fixed strings, which are combined, possibly in a sophisticated way, to generate the final text.   In some circumstances, it may be profitable to combine NLG and template based techniques. The issue of combining generation techniques can be seen in more abstract terms as the issue of mixing levels of representation of different degrees of linguistic abstraction. This paper aims at defining a reference architecture for systems using mixed representations. We argue that mixed representations can be used without abandoning a linguistically grounded approach to language generation.",
        "published": "1999-07-16T15:43:45Z",
        "link": "http://arxiv.org/abs/cs/9907026v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Detecting Sub-Topic Correspondence through Bipartite Term Clustering",
        "authors": [
            "Zvika Marx",
            "Ido Dagan",
            "Eli Shamir"
        ],
        "summary": "This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments, enhancing common notions of text similarity. This task is addressed by coupling corresponding term subsets through bipartite clustering. The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method, providing illustrating results.",
        "published": "1999-08-01T14:02:57Z",
        "link": "http://arxiv.org/abs/cs/9908001v1",
        "categories": [
            "cs.CL",
            "I.2.6, I.2.7, H.3.1"
        ]
    },
    {
        "title": "Semantic robust parsing for noun extraction from natural language   queries",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "This paper describes how robust parsing techniques can be fruitful applied for building a query generation module which is part of a pipelined NLP architecture aimed at process natural language queries in a restricted domain. We want to show that semantic robustness represents a key issue in those NLP systems where it is more likely to have partial and ill-formed utterances due to various factors (e.g. noisy environments, low quality of speech recognition modules, etc...) and where it is necessary to succeed, even if partially, in extracting some meaningful information.",
        "published": "1999-09-02T15:53:07Z",
        "link": "http://arxiv.org/abs/cs/9909002v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A statistical model for word discovery in child directed speech",
        "authors": [
            "Anand Venkataraman"
        ],
        "summary": "A statistical model for segmentation and word discovery in child directed speech is presented. An incremental unsupervised learning algorithm to infer word boundaries based on this model is described and results of empirical tests showing that the algorithm is competitive with other models that have been used for similar tasks are also presented.",
        "published": "1999-10-13T03:25:33Z",
        "link": "http://arxiv.org/abs/cs/9910011v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Selective Sampling for Example-based Word Sense Disambiguation",
        "authors": [
            "Atsushi Fujii",
            "Kentaro Inui",
            "Takenobu Tokunaga",
            "Hozumi Tanaka"
        ],
        "summary": "This paper proposes an efficient example sampling method for example-based word sense disambiguation systems. To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required. In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search). To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation. Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectiveness of our method through experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system.",
        "published": "1999-10-23T11:19:35Z",
        "link": "http://arxiv.org/abs/cs/9910020v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Practical experiments with regular approximation of context-free   languages",
        "authors": [
            "Mark-Jan Nederhof"
        ],
        "summary": "Several methods are discussed that construct a finite automaton given a context-free grammar, including both methods that lead to subsets and those that lead to supersets of the original context-free language. Some of these methods of regular approximation are new, and some others are presented here in a more refined form with respect to existing literature. Practical experiments with the different methods of regular approximation are performed for spoken-language input: hypotheses from a speech recognizer are filtered through a finite automaton.",
        "published": "1999-10-25T15:00:52Z",
        "link": "http://arxiv.org/abs/cs/9910022v1",
        "categories": [
            "cs.CL",
            "F.4.3; F.1.1"
        ]
    },
    {
        "title": "Question Answering System Using Syntactic Information",
        "authors": [
            "M. Murata",
            "M. Utiyama",
            "H. Isahara"
        ],
        "summary": "Question answering task is now being done in TREC8 using English documents. We examined question answering task in Japanese sentences. Our method selects the answer by matching the question sentence with knowledge-based data written in natural language. We use syntactic information to obtain highly accurate answers.",
        "published": "1999-11-15T05:48:03Z",
        "link": "http://arxiv.org/abs/cs/9911006v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "One-Level Prosodic Morphology",
        "authors": [
            "Markus Walther"
        ],
        "summary": "Recent developments in theoretical linguistics have lead to a widespread acceptance of constraint-based analyses of prosodic morphology phenomena such as truncation, infixation, floating morphemes and reduplication. Of these, reduplication is particularly challenging for state-of-the-art computational morphology, since it involves copying of some part of a phonological string. In this paper I argue for certain extensions to the one-level model of phonology and morphology (Bird & Ellison 1994) to cover the computational aspects of prosodic morphology using finite-state methods. In a nutshell, enriched lexical representations provide additional automaton arcs to repeat or skip sounds and also to allow insertion of additional material. A kind of resource consciousness is introduced to control this additional freedom, distinguishing between producer and consumer arcs. The non-finite-state copying aspect of reduplication is mapped to automata intersection, itself a non-finite-state operation. Bounded local optimization prunes certain automaton arcs that fail to contribute to linguistic optimisation criteria. The paper then presents implemented case studies of Ulwa construct state infixation, German hypocoristic truncation and Tagalog over-applying reduplication that illustrate the expressive power of this approach, before its merits and limitations are discussed and possible extensions are sketched. I conclude that the one-level approach to prosodic morphology presents an attractive way of extending finite-state techniques to difficult phenomena that hitherto resisted elegant computational analyses.",
        "published": "1999-11-19T16:10:51Z",
        "link": "http://arxiv.org/abs/cs/9911011v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Resolution of Indirect Anaphora in Japanese Sentences Using Examples 'X   no Y (Y of X)'",
        "authors": [
            "M. Murata",
            "H. Isahara",
            "M. Nagao"
        ],
        "summary": "A noun phrase can indirectly refer to an entity that has already been mentioned. For example, ``I went into an old house last night. The roof was leaking badly and ...'' indicates that ``the roof'' is associated with `` an old house}'', which was mentioned in the previous sentence. This kind of reference (indirect anaphora) has not been studied well in natural language processing, but is important for coherence resolution, language understanding, and machine translation. In order to analyze indirect anaphora, we need a case frame dictionary for nouns that contains knowledge of the relationships between two nouns but no such dictionary presently exists. Therefore, we are forced to use examples of ``X no Y'' (Y of X) and a verb case frame dictionary instead. We tried estimating indirect anaphora using this information and obtained a recall rate of 63% and a precision rate of 68% on test sentences. This indicates that the information of ``X no Y'' is useful to a certain extent when we cannot make use of a noun case frame dictionary. We estimated the results that would be given by a noun case frame dictionary, and obtained recall and precision rates of 71% and 82% respectively. Finally, we proposed a way to construct a noun case frame dictionary by using examples of ``X no Y.''",
        "published": "1999-12-13T04:42:25Z",
        "link": "http://arxiv.org/abs/cs/9912003v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Pronoun Resolution in Japanese Sentences Using Surface Expressions and   Examples",
        "authors": [
            "M. Murata",
            "H. Isahara",
            "M. Nagao"
        ],
        "summary": "In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci. Unlike conventional work which was semantic markers for semantic constraints, we used examples for semantic constraints and showed in our experiments that examples are as useful as semantic markers. We also propose many new methods for estimating referents of pronouns. For example, we use the form ``X of Y'' for estimating referents of demonstrative adjectives. In addition to our new methods, we used many conventional methods. As a result, experiments using these methods obtained a precision rate of 87% in estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns for training sentences, and obtained a precision rate of 78% for test sentences.",
        "published": "1999-12-13T04:46:20Z",
        "link": "http://arxiv.org/abs/cs/9912004v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Resolution of Verb Ellipsis in Japanese Sentence using Surface   Expressions and Examples",
        "authors": [
            "M. Murata",
            "M. Nagao"
        ],
        "summary": "Verbs are sometimes omitted in Japanese sentences. It is necessary to recover omitted verbs for purposes of language understanding, machine translation, and conversational processing. This paper describes a practical way to recover omitted verbs by using surface expressions and examples. We experimented the resolution of verb ellipses by using this information, and obtained a recall rate of 73% and a precision rate of 66% on test sentences.",
        "published": "1999-12-13T05:19:46Z",
        "link": "http://arxiv.org/abs/cs/9912006v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "An Estimate of Referent of Noun Phrases in Japanese Sentences",
        "authors": [
            "M. Murata",
            "M. Nagao"
        ],
        "summary": "In machine translation and man-machine dialogue, it is important to clarify referents of noun phrases. We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors of noun phrases. Since the Japanese language has no articles, it is difficult to decide whether a noun phrase has an antecedent or not. We had previously estimated the referential properties of noun phrases that correspond to articles by using clue words in the sentences. By using these referential properties, our system determined the referents of noun phrases in Japanese sentences. Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases. As a result, on training sentences we obtained a precision rate of 82% and a recall rate of 85% in the determination of the referents of noun phrases that have antecedents. On test sentences, we obtained a precision rate of 79% and a recall rate of 77%.",
        "published": "1999-12-13T05:20:40Z",
        "link": "http://arxiv.org/abs/cs/9912005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "An Example-Based Approach to Japanese-to-English Translation of Tense,   Aspect, and Modality",
        "authors": [
            "M. Murata",
            "Q. Ma",
            "K. Uchimoto",
            "H. Isahara"
        ],
        "summary": "We have developed a new method for Japanese-to-English translation of tense, aspect, and modality that uses an example-based method. In this method the similarity between input and example sentences is defined as the degree of semantic matching between the expressions at the ends of the sentences. Our method also uses the k-nearest neighbor method in order to exclude the effects of noise; for example, wrongly tagged data in the bilingual corpora. Experiments show that our method can translate tenses, aspects, and modalities more accurately than the top-level MT software currently available on the market can. Moreover, it does not require hand-craft rules.",
        "published": "1999-12-13T06:01:19Z",
        "link": "http://arxiv.org/abs/cs/9912007v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Deduction over Mixed-Level Logic Representations for Text Passage   Retrieval",
        "authors": [
            "Michael Hess"
        ],
        "summary": "A system is described that uses a mixed-level representation of (part of) meaning of natural language documents (based on standard Horn Clause Logic) and a variable-depth search strategy that distinguishes between the different levels of abstraction in the knowledge representation to locate specific passages in the documents. Mixed-level representations as well as variable-depth search strategies are applicable in fields outside that of NLP.",
        "published": "1999-12-15T11:02:22Z",
        "link": "http://arxiv.org/abs/cs/9912009v1",
        "categories": [
            "cs.CL",
            "H.3.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "HMM Specialization with Selective Lexicalization",
        "authors": [
            "Jin-Dong Kim",
            "Sang-Zoo Lee",
            "Hae-Chang Rim"
        ],
        "summary": "We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words. Our approach examines the distribution of transitions, selects the uncommon words, and makes lexicalized states for the words. We performed a part-of-speech tagging experiment on the Brown corpus to evaluate the resultant language model and discovered that this technique improved the tagging accuracy by 0.21% at the 95% level of confidence.",
        "published": "1999-12-23T01:07:33Z",
        "link": "http://arxiv.org/abs/cs/9912016v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.6; I.2.7"
        ]
    },
    {
        "title": "Mixed-Level Knowledge Representation and Variable-Depth Inference in   Natural Language Processing",
        "authors": [
            "Michael Hess"
        ],
        "summary": "A system is described that uses a mixed-level knowledge representation based on standard Horn Clause Logic to represent (part of) the meaning of natural language documents. A variable-depth search strategy is outlined that distinguishes between the different levels of abstraction in the knowledge representation to locate specific passages in the documents. A detailed description of the linguistic aspects of the system is given. Mixed-level representations as well as variable-depth search strategies are applicable in fields outside that of NLP.",
        "published": "1999-12-23T15:48:26Z",
        "link": "http://arxiv.org/abs/cs/9912017v1",
        "categories": [
            "cs.CL",
            "H.3.1; I.2.3; I.2.7"
        ]
    },
    {
        "title": "On the geometry of similarity search: dimensionality curse and   concentration of measure",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "We suggest that the curse of dimensionality affecting the similarity-based search in large datasets is a manifestation of the phenomenon of concentration of measure on high-dimensional structures. We prove that, under certain geometric assumptions on the query domain $\\Omega$ and the dataset $X$, if $\\Omega$ satisfies the so-called concentration property, then for most query points $x^\\ast$ the ball of radius $(1+\\e)d_X(x^\\ast)$ centred at $x^\\ast$ contains either all points of $X$ or else at least $C_1\\exp(-C_2\\e^2n)$ of them. Here $d_X(x^\\ast)$ is the distance from $x^\\ast$ to the nearest neighbour in $X$ and $n$ is the dimension of $\\Omega$.",
        "published": "1999-01-12T21:56:39Z",
        "link": "http://arxiv.org/abs/cs/9901004v1",
        "categories": [
            "cs.IR",
            "cs.CG",
            "cs.DB",
            "cs.DS",
            "H.3.3;H.2.4;F.2.2"
        ]
    },
    {
        "title": "Mutual Search",
        "authors": [
            "Harry Buhrman",
            "Matthew Franklin",
            "Juan A. Garay",
            "Jaap-Henk Hoepman",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We introduce a search problem called ``mutual search'' where $k$ \\agents, arbitrarily distributed over $n$ sites, are required to locate one another by posing queries of the form ``Anybody at site $i$?''. We ask for the least number of queries that is necessary and sufficient. For the case of two \\agents using deterministic protocols we obtain the following worst-case results: In an oblivious setting (where all pre-planned queries are executed) there is no savings: $n-1$ queries are required and are sufficient. In a nonoblivious setting we can exploit the paradigm of ``no news is also news'' to obtain significant savings: in the synchronous case $0.586n$ queries suffice and $0.536n$ queries are required; in the asynchronous case $0.896n$ queries suffice and a fortiori 0.536 queries are required; for $o(\\sqrt{n})$ \\agents using a deterministic protocol less than $n$ queries suffice; there is a simple randomized protocol for two \\agents with worst-case expected $0.5n$ queries and all randomized protocols require at least $0.125n$ worst-case expected queries. The graph-theoretic framework we formulate for expressing and analyzing algorithms for this problem may be of independent interest.",
        "published": "1999-02-02T15:46:00Z",
        "link": "http://arxiv.org/abs/cs/9902005v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "cs.DC",
            "cs.DM",
            "cs.IR",
            "F.2,C.2,E,1,D.4.4"
        ]
    },
    {
        "title": "Visualization of Retrieved Documents using a Presentation Server",
        "authors": [
            "Sa-Kwang Song",
            "Sung Hyon Myaeng"
        ],
        "summary": "In any search-based digital library (DL) systems dealing with a non-trivial number of documents, users are often required to go through a long list of short document descriptions in order to identify what they are looking for. To tackle the problem, a variety of document organization algorithms and/or visualization techniques have been used to guide users in selecting relevant documents. Since these techniques require heavy computations, however, we developed a presentation server designed to serve as an intermediary between retrieval servers and clients equipped with a visualization interface. In addition, we designed our own visual interface by which users can view a set of documents from different perspectives through layers of document maps. We finally ran experiments to show that the visual interface, in conjunction with the presentation server, indeed helps users in selecting relevant documents from the retrieval results.",
        "published": "1999-02-10T09:30:59Z",
        "link": "http://arxiv.org/abs/cs/9902021v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.3; H.3.4"
        ]
    },
    {
        "title": "A Scrollbar-based Visualization for Document Navigation",
        "authors": [
            "Donald Byrd"
        ],
        "summary": "We are interested in questions of improving user control in best-match text-retrieval systems, specifically questions as to whether simple visualizations that nonetheless go beyond the minimal ones generally available can significantly help users. Recently, we have been investigating ways to help users decide-given a set of documents retrieved by a query-which documents and passages are worth closer examination. We built a document viewer incorporating a visualization centered around a novel content-displaying scrollbar and color term highlighting, and studied whether the visualization is helpful to non-expert searchers. Participants' reaction to the visualization was very positive, while the objective results were inconclusive.",
        "published": "1999-02-24T17:10:46Z",
        "link": "http://arxiv.org/abs/cs/9902028v2",
        "categories": [
            "cs.IR",
            "cs.HC",
            "H5.2; H3.3; I7.2; H3.7"
        ]
    },
    {
        "title": "A Proposal for the Establishment of Review Boards - a flexible approach   to the selection of academic knowledge",
        "authors": [
            "Bruce Edmonds"
        ],
        "summary": "Paper journals use a small number of trusted academics to select information on behalf of all their readers. This inflexibility in the selection was justified due to the expense of publishing. The advent of cheap distribution via the internet allows a new trade-off between time and expense and the flexibility of the selection process. This paper explores one such possible process one where the role of mark-up and archiving is separated from that of review. The idea is that authors publish their papers on their own web pages or in a public paper archive, a board of reviewers judge that paper on a number of different criteria. The detailed results of the reviews are stored in such a way as to enable readers to use these judgements to find the papers they want using search engines on the web. Thus instead of journals using generic selection criteria readers can set their own to suit their needs. The resulting system might be even cheaper than web-journals to implement.",
        "published": "1999-04-01T07:54:36Z",
        "link": "http://arxiv.org/abs/cs/9904001v1",
        "categories": [
            "cs.CY",
            "cs.DL",
            "cs.IR",
            "K.4.3; K.3.m;H.5.3"
        ]
    },
    {
        "title": "A geometric framework for modelling similarity search",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "The aim of this paper is to propose a geometric framework for modelling similarity search in large and multidimensional data spaces of general nature, which seems to be flexible enough to address such issues as analysis of complexity, indexability, and the `curse of dimensionality.' Such a framework is provided by the concept of the so-called similarity workload, which is a probability metric space $\\Omega$ (query domain) with a distinguished finite subspace $X$ (dataset), together with an assembly of concepts, techniques, and results from metric geometry. They include such notions as metric transform, $\\e$-entropy, and the phenomenon of concentration of measure on high-dimensional structures. In particular, we discuss the relevance of the latter to understanding the curse of dimensionality. As some of those concepts and techniques are being currently reinvented by the database community, it seems desirable to try and bridge the gap between database research and the relevant work already done in geometry and analysis.",
        "published": "1999-04-07T04:16:02Z",
        "link": "http://arxiv.org/abs/cs/9904002v2",
        "categories": [
            "cs.IR",
            "cs.DB",
            "cs.DS",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "Raising Reliability of Web Search Tool Research through Replication and   Chaos Theory",
        "authors": [
            "Scott Nicholson"
        ],
        "summary": "Because the World Wide Web is a dynamic collection of information, the Web search tools (or \"search engines\") that index the Web are dynamic. Traditional information retrieval evaluation techniques may not provide reliable results when applied to the Web search tools. This study is the result of ten replications of the classic 1996 Ding and Marchionini Web search tool research. It explores the effects that replication can have on transforming unreliable results from one iteration into replicable and therefore reliable results after multiple iterations.",
        "published": "1999-07-27T16:42:18Z",
        "link": "http://arxiv.org/abs/cs/9907042v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.4; H.3.5"
        ]
    },
    {
        "title": "Representing Scholarly Claims in Internet Digital Libraries: A Knowledge   Modelling Approach",
        "authors": [
            "Simon Buckingham Shum",
            "Enrico Motta",
            "John Domingue"
        ],
        "summary": "This paper is concerned with tracking and interpreting scholarly documents in distributed research communities. We argue that current approaches to document description, and current technological infrastructures particularly over the World Wide Web, provide poor support for these tasks. We describe the design of a digital library server which will enable authors to submit a summary of the contributions they claim their documents makes, and its relations to the literature. We describe a knowledge-based Web environment to support the emergence of such a community-constructed semantic hypertext, and the services it could provide to assist the interpretation of an idea or document in the context of its literature. The discussion considers in detail how the approach addresses usability issues associated with knowledge structuring environments.",
        "published": "1999-08-19T09:51:29Z",
        "link": "http://arxiv.org/abs/cs/9908015v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.3.7; H.1.2; H5.2; H.5.4; I.2.4; I.7.4"
        ]
    },
    {
        "title": "PIPE: Personalizing Recommendations via Partial Evaluation",
        "authors": [
            "Naren Ramakrishnan"
        ],
        "summary": "It is shown that personalization of web content can be advantageously viewed as a form of partial evaluation --- a technique well known in the programming languages community. The basic idea is to model a recommendation space as a program, then partially evaluate this program with respect to user preferences (and features) to obtain specialized content. This technique supports both content-based and collaborative approaches, and is applicable to a range of applications that require automatic information integration from multiple web sources. The effectiveness of this methodology is illustrated by two example applications --- (i) personalizing content for visitors to the Blacksburg Electronic Village (http://www.bev.net), and (ii) locating and selecting scientific software on the Internet. The scalability of this technique is demonstrated by its ability to interface with online web ontologies that index thousands of web pages.",
        "published": "1999-10-18T15:47:29Z",
        "link": "http://arxiv.org/abs/cs/9910015v3",
        "categories": [
            "cs.IR",
            "cs.AI",
            "H.4.2"
        ]
    },
    {
        "title": "A Geometric Model for Information Retrieval Systems",
        "authors": [
            "Myung Ho Kim"
        ],
        "summary": "This decade has seen a great deal of progress in the development of information retrieval systems. Unfortunately, we still lack a systematic understanding of the behavior of the systems and their relationship with documents. In this paper we present a completely new approach towards the understanding of the information retrieval systems. Recently, it has been observed that retrieval systems in TREC 6 show some remarkable patterns in retrieving relevant documents. Based on the TREC 6 observations, we introduce a geometric linear model of information retrieval systems. We then apply the model to predict the number of relevant documents by the retrieval systems. The model is also scalable to a much larger data set. Although the model is developed based on the TREC 6 routing test data, I believe it can be readily applicable to other information retrieval systems. In Appendix, we explained a simple and efficient way of making a better system from the existing systems.",
        "published": "1999-12-06T03:04:43Z",
        "link": "http://arxiv.org/abs/cs/9912002v1",
        "categories": [
            "cs.IR",
            "cs.CC",
            "cs.DL",
            "H.1.1"
        ]
    },
    {
        "title": "Object Oriented and Functional Programming for Symbolic Manipulation",
        "authors": [
            "Alexander Yu. Vlasov"
        ],
        "summary": "The advantages of mixed approach with using different kinds of programming techniques for symbolic manipulation are discussed. The main purpose of approach offered is merge the methods of object oriented programming that convenient for presentation data and algorithms for user with advantages of functional languages for data manipulation, internal presentation, and portability of software.",
        "published": "1999-01-13T20:40:04Z",
        "link": "http://arxiv.org/abs/cs/9901006v1",
        "categories": [
            "cs.SC",
            "cs.PL",
            "I.1.0; D.1.1; D.1.5; D.3.2"
        ]
    },
    {
        "title": "Universal Object Oriented Languages and Computer Algebra",
        "authors": [
            "Alexander Yu. Vlasov"
        ],
        "summary": "The universal object oriented languages made programming more simple and efficient. In the article is considered possibilities of using similar methods in computer algebra. A clear and powerful universal language is useful if particular problem was not implemented in standard software packages like REDUCE, MATHEMATICA, etc. and if the using of internal programming languages of the packages looks not very efficient.   Functional languages like LISP had some advantages and traditions for algebraic and symbolic manipulations. Functional and object oriented programming are not incompatible ones. An extension of the model of an object for manipulation with pure functions and algebraic expressions is considered.",
        "published": "1999-01-15T19:28:58Z",
        "link": "http://arxiv.org/abs/cs/9901007v1",
        "categories": [
            "cs.PL",
            "D.1.5; D.3.2"
        ]
    },
    {
        "title": "Perpetual Adaptation of Software to Hardware: An Extensible Architecture   for Providing Code Optimization as a Central System Service",
        "authors": [
            "Thomas Kistler",
            "Michael Franz"
        ],
        "summary": "We present an open architecture for just-in-time code generation and dynamic code optimization that is flexible, customizable, and extensible. While previous research has primarily investigated functional aspects of such a system, architectural aspects have so far remained unexplored. In this paper, we argue that these properties are important to generate optimal code for a variety of hardware architectures and different processor generations within processor families. These properties are also important to make system-level code generation useful in practice.",
        "published": "1999-03-22T21:24:35Z",
        "link": "http://arxiv.org/abs/cs/9903014v1",
        "categories": [
            "cs.OS",
            "cs.PL",
            "D.3.4"
        ]
    },
    {
        "title": "WebScript -- A Scripting Language for the Web",
        "authors": [
            "Yin Zhang"
        ],
        "summary": "WebScript is a scripting language for processing Web documents. Designed as an extension to Jacl, the Java implementation of Tcl, WebScript allows programmers to manipulate HTML in the same way as Tcl manipulates text strings and GUI elements. This leads to a completely new way of writing the next generation of Web applications. This paper presents the motivation behind the design and implementation of WebScript, an overview of its major features, as well as some demonstrations of its power.",
        "published": "1999-04-21T19:21:24Z",
        "link": "http://arxiv.org/abs/cs/9904011v1",
        "categories": [
            "cs.NI",
            "cs.PL",
            "D.3.m; H.5.4; I.7.m"
        ]
    },
    {
        "title": "A Machine-Independent Debugger--Revisited",
        "authors": [
            "David R. Hanson"
        ],
        "summary": "Most debuggers are notoriously machine-dependent, but some recent research prototypes achieve varying degrees of machine-independence with novel designs. Cdb, a simple source-level debugger for C, is completely independent of its target architecture. This independence is achieved by embedding symbol tables and debugging code in the target program, which costs both time and space. This paper describes a revised design and implementation of cdb that reduces the space cost by nearly one-half and the time cost by 13% by storing symbol tables in external files. A symbol table is defined by a 31-line grammar in the Abstract Syntax Description Language (ASDL). ASDL is a domain-specific language for specifying tree data structures. The ASDL tools accept an ASDL grammar and generate code to construct, read, and write these data structures. Using ASDL automates implementing parts of the debugger, and the grammar documents the symbol table concisely. Using ASDL also suggested simplifications to the interface between the debugger and the target program. Perhaps most important, ASDL emphasizes that symbol tables are data structures, not file formats. Many of the pitfalls of working with low-level file formats can be avoided by focusing instead on high-level data structures and automating the implementation details.",
        "published": "1999-04-23T18:34:04Z",
        "link": "http://arxiv.org/abs/cs/9904017v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.4"
        ]
    },
    {
        "title": "DRAFT : Task System and Item Architecture (TSIA)",
        "authors": [
            "Burkhard D. Burow"
        ],
        "summary": "During its execution, a task is independent of all other tasks. For an application which executes in terms of tasks, the application definition can be free of the details of the execution. Many projects have demonstrated that a task system (TS) can provide such an application with a parallel, distributed, heterogeneous, adaptive, dynamic, real-time, interactive, reliable, secure or other execution. A task consists of items and thus the application is defined in terms of items. An item architecture (IA) can support arrays, routines and other structures of items, thus allowing for a structured application definition. Taking properties from many projects, the support can extend through to currying, application defined types, conditional items, streams and other definition elements. A task system and item architecture (TSIA) thus promises unprecedented levels of support for application execution and definition.",
        "published": "1999-05-05T01:43:13Z",
        "link": "http://arxiv.org/abs/cs/9905002v1",
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.OS",
            "A.1;D.1.1;D.1.3;D.1.4;D.2.3;D.2.11;D.3.2;D.3.3;D.3.4;D.4.1;D.4.5;\n  D.4.7;E.1;F.1.2;F.3.3"
        ]
    },
    {
        "title": "Combining Inclusion Polymorphism and Parametric Polymorphism",
        "authors": [
            "Sabine Glesner",
            "Karl Stroetmann"
        ],
        "summary": "We show that the question whether a term is typable is decidable for type systems combining inclusion polymorphism with parametric polymorphism provided the type constructors are at most unary. To prove this result we first reduce the typability problem to the problem of solving a system of type inequations. The result is then obtained by showing that the solvability of the resulting system of type inequations is decidable.",
        "published": "1999-06-14T09:42:11Z",
        "link": "http://arxiv.org/abs/cs/9906013v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.3; F.3.3"
        ]
    },
    {
        "title": "The Alma Project, or How First-Order Logic Can Help Us in Imperative   Programming",
        "authors": [
            "Krzysztof R. Apt",
            "Andrea Schaerf"
        ],
        "summary": "The aim of the Alma project is the design of a strongly typed constraint programming language that combines the advantages of logic and imperative programming. The first stage of the project was the design and implementation of Alma-0, a small programming language that provides a support for declarative programming within the imperative programming framework. It is obtained by extending a subset of Modula-2 by a small number of features inspired by the logic programming paradigm. In this paper we discuss the rationale for the design of Alma-0, the benefits of the resulting hybrid programming framework, and the current work on adding constraint processing capabilities to the language. In particular, we discuss the role of the logical and customary variables, the interaction between the constraint store and the program, and the need for lists.",
        "published": "1999-07-19T09:36:05Z",
        "link": "http://arxiv.org/abs/cs/9907027v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2;F.3.2;F.3.3;I.2.8;I.5.5"
        ]
    },
    {
        "title": "After Compilers and Operating Systems : The Third Advance in Application   Support",
        "authors": [
            "Burkhard D. Burow"
        ],
        "summary": "After compilers and operating systems, TSIAs are the third advance in application support. A compiler supports a high level application definition in a programming language. An operating system supports a high level interface to the resources used by an application execution. A Task System and Item Architecture (TSIA) provides an application with a transparent reliable, distributed, heterogeneous, adaptive, dynamic, real-time, interactive, parallel, secure or other execution. In addition to supporting the application execution, a TSIA also supports the application definition. This run-time support for the definition is complementary to the compile-time support of a compiler. For example, this allows a language similar to Fortran or C to deliver features promised by functional computing. While many TSIAs exist, they previously have not been recognized as such and have served only a particular type of application. Existing TSIAs and other projects demonstrate that TSIAs are feasible for most applications. As the next paradigm for application support, the TSIA simplifies and unifies existing computing practice and research. By solving many outstanding problems, the TSIA opens many, many new opportunities for computing.",
        "published": "1999-08-03T14:50:09Z",
        "link": "http://arxiv.org/abs/cs/9908002v1",
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.OS",
            "A.1;D.1.1;D.1.3;D.1.4;D.2.11;D.3.2;D.3.3;D.3.4;D.4.5;D.4.7;E.1;F.1.2"
        ]
    },
    {
        "title": "The Rough Guide to Constraint Propagation",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We provide here a simple, yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way. In particular, using the notions commutativity and semi-commutativity, we show how the well-known AC-3, PC-2, DAC and DPC algorithms are instances of a single generic algorithm. The work reported here extends and simplifies that of Apt, cs.AI/9811024.",
        "published": "1999-09-08T13:50:01Z",
        "link": "http://arxiv.org/abs/cs/9909009v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.3; I.1.2; I.2.2"
        ]
    },
    {
        "title": "Automatic Generation of Constraint Propagation Algorithms for Small   Finite Domains",
        "authors": [
            "Krzysztof R. Apt",
            "Eric Monfroy"
        ],
        "summary": "We study here constraint satisfaction problems that are based on predefined, explicitly given finite constraints. To solve them we propose a notion of rule consistency that can be expressed in terms of rules derived from the explicit representation of the initial constraints.   This notion of local consistency is weaker than arc consistency for constraints of arbitrary arity but coincides with it when all domains are unary or binary. For Boolean constraints rule consistency coincides with the closure under the well-known propagation rules for Boolean constraints.   By generalizing the format of the rules we obtain a characterization of arc consistency in terms of so-called inclusion rules. The advantage of rule consistency and this rule based characterization of the arc consistency is that the algorithms that enforce both notions can be automatically generated, as CHR rules. So these algorithms could be integrated into constraint logic programming systems such as Eclipse.   We illustrate the usefulness of this approach to constraint propagation by discussing the implementations of both algorithms and their use on various examples, including Boolean constraints, three valued logic of Kleene, constraints dealing with Waltz's language for describing polyhedreal scenes, and Allen's qualitative approach to temporal logic.",
        "published": "1999-09-08T14:18:47Z",
        "link": "http://arxiv.org/abs/cs/9909010v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.#.2; I.2.2; I.2.3"
        ]
    },
    {
        "title": "Semantics of Programming Languages: A Tool-Oriented Approach",
        "authors": [
            "Jan Heering",
            "Paul Klint"
        ],
        "summary": "By paying more attention to semantics-based tool generation, programming language semantics can significantly increase its impact. Ultimately, this may lead to ``Language Design Assistants'' incorporating substantial amounts of semantic knowledge.",
        "published": "1999-11-04T11:14:45Z",
        "link": "http://arxiv.org/abs/cs/9911001v2",
        "categories": [
            "cs.PL",
            "D.2.2; D.3.1; D.3.4; F.3.2"
        ]
    },
    {
        "title": "On the geometry of similarity search: dimensionality curse and   concentration of measure",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "We suggest that the curse of dimensionality affecting the similarity-based search in large datasets is a manifestation of the phenomenon of concentration of measure on high-dimensional structures. We prove that, under certain geometric assumptions on the query domain $\\Omega$ and the dataset $X$, if $\\Omega$ satisfies the so-called concentration property, then for most query points $x^\\ast$ the ball of radius $(1+\\e)d_X(x^\\ast)$ centred at $x^\\ast$ contains either all points of $X$ or else at least $C_1\\exp(-C_2\\e^2n)$ of them. Here $d_X(x^\\ast)$ is the distance from $x^\\ast$ to the nearest neighbour in $X$ and $n$ is the dimension of $\\Omega$.",
        "published": "1999-01-12T21:56:39Z",
        "link": "http://arxiv.org/abs/cs/9901004v1",
        "categories": [
            "cs.IR",
            "cs.CG",
            "cs.DB",
            "cs.DS",
            "H.3.3;H.2.4;F.2.2"
        ]
    },
    {
        "title": "Analysis of approximate nearest neighbor searching with clustered point   sets",
        "authors": [
            "Songrit Maneewongvatana",
            "David M. Mount"
        ],
        "summary": "We present an empirical analysis of data structures for approximate nearest neighbor searching. We compare the well-known optimized kd-tree splitting method against two alternative splitting methods. The first, called the sliding-midpoint method, which attempts to balance the goals of producing subdivision cells of bounded aspect ratio, while not producing any empty cells. The second, called the minimum-ambiguity method is a query-based approach. In addition to the data points, it is also given a training set of query points for preprocessing. It employs a simple greedy algorithm to select the splitting plane that minimizes the average amount of ambiguity in the choice of the nearest neighbor for the training points. We provide an empirical analysis comparing these two methods against the optimized kd-tree construction for a number of synthetically generated data and query sets. We demonstrate that for clustered data and query sets, these algorithms can provide significant improvements over the standard kd-tree construction for approximate nearest neighbor searching.",
        "published": "1999-01-26T21:23:37Z",
        "link": "http://arxiv.org/abs/cs/9901013v1",
        "categories": [
            "cs.CG",
            "E.1; F.2.2"
        ]
    },
    {
        "title": "The Average-Case Area of Heilbronn-Type Triangles",
        "authors": [
            "Tao Jiang",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "From among $ {n \\choose 3}$ triangles with vertices chosen from $n$ points in the unit square, let $T$ be the one with the smallest area, and let $A$ be the area of $T$. Heilbronn's triangle problem asks for the maximum value assumed by $A$ over all choices of $n$ points. We consider the average-case: If the $n$ points are chosen independently and at random (with a uniform distribution), then there exist positive constants $c$ and $C$ such that $c/n^3 < \\mu_n < C/n^3$ for all large enough values of $n$, where $\\mu_n$ is the expectation of $A$. Moreover, $c/n^3 < A < C/n^3$, with probability close to one. Our proof uses the incompressibility method based on Kolmogorov complexity; it actually determines the area of the smallest triangle for an arrangement in ``general position.''",
        "published": "1999-02-05T12:37:45Z",
        "link": "http://arxiv.org/abs/math/9902043v5",
        "categories": [
            "math.CO",
            "cs.CG",
            "cs.DM",
            "math.LO",
            "math.MG",
            "math.PR",
            "52C10"
        ]
    },
    {
        "title": "Zero-Parity Stabbing Information",
        "authors": [
            "Joseph O'Rourke",
            "Irena Pashchenko"
        ],
        "summary": "Everett et al. introduced several varieties of stabbing information for the lines determined by pairs of vertices of a simple polygon P, and established their relationships to vertex visibility and other combinatorial data. In the same spirit, we define the ``zero-parity (ZP) stabbing information'' to be a natural weakening of their ``weak stabbing information,'' retaining only the distinction among {zero, odd, even>0} in the number of polygon edges stabbed. Whereas the weak stabbing information's relation to visibility remains an open problem, we completely settle the analogous questions for zero-parity information, with three results: (1) ZP information is insufficient to distinguish internal from external visibility graph edges; (2) but it does suffice for all polygons that avoid a certain complex substructure; and (3) the natural generalization of ZP information to the continuous case of smooth curves does distinguish internal from external visibility.",
        "published": "1999-06-22T20:32:57Z",
        "link": "http://arxiv.org/abs/cs/9906022v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Computational Geometry Column 35",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The subquadratic algorithm of Kapoor for finding shortest paths on a polyhedron is described.",
        "published": "1999-06-22T20:40:51Z",
        "link": "http://arxiv.org/abs/cs/9906023v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Setting Parameters by Example",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We introduce a class of \"inverse parametric optimization\" problems, in which one is given both a parametric optimization problem and a desired optimal solution; the task is to determine parameter values that lead to the given solution. We describe algorithms for solving such problems for minimum spanning trees, shortest paths, and other \"optimal subgraph\" problems, and discuss applications in multicast routing, vehicle path planning, resource allocation, and board game programming.",
        "published": "1999-07-02T21:09:55Z",
        "link": "http://arxiv.org/abs/cs/9907001v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; I.2.6"
        ]
    },
    {
        "title": "Hinged Dissection of Polyominoes and Polyforms",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "David Eppstein",
            "Greg N. Frederickson",
            "Erich Friedman"
        ],
        "summary": "A hinged dissection of a set of polygons S is a collection of polygonal pieces hinged together at vertices that can be folded into any member of S. We present a hinged dissection of all edge-to-edge gluings of n congruent copies of a polygon P that join corresponding edges of P. This construction uses kn pieces, where k is the number of vertices of P. When P is a regular polygon, we show how to reduce the number of pieces to ceiling(k/2)*(n-1). In particular, we consider polyominoes (made up of unit squares), polyiamonds (made up of equilateral triangles), and polyhexes (made up of regular hexagons). We also give a hinged dissection of all polyabolos (made up of right isosceles triangles), which do not fall under the general result mentioned above. Finally, we show that if P can be hinged into Q, then any edge-to-edge gluing of n congruent copies of P can be hinged into any edge-to-edge gluing of n congruent copies of Q.",
        "published": "1999-07-10T21:29:56Z",
        "link": "http://arxiv.org/abs/cs/9907018v3",
        "categories": [
            "cs.CG",
            "cs.DM",
            "G.2.1; F.2.2"
        ]
    },
    {
        "title": "Improved Incremental Randomized Delaunay Triangulation",
        "authors": [
            "Olivier Devillers"
        ],
        "summary": "We propose a new data structure to compute the Delaunay triangulation of a set of points in the plane. It combines good worst case complexity, fast behavior on real data, and small memory occupation.   The location structure is organized into several levels. The lowest level just consists of the triangulation, then each level contains the triangulation of a small sample of the levels below. Point location is done by marching in a triangulation to determine the nearest neighbor of the query at that level, then the march restarts from that neighbor at the level below. Using a small sample (3%) allows a small memory occupation; the march and the use of the nearest neighbor to change levels quickly locate the query.",
        "published": "1999-07-16T12:44:07Z",
        "link": "http://arxiv.org/abs/cs/9907024v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "On Deletion in Delaunay Triangulation",
        "authors": [
            "Olivier Devillers"
        ],
        "summary": "This paper presents how the space of spheres and shelling may be used to delete a point from a $d$-dimensional triangulation efficiently. In dimension two, if k is the degree of the deleted vertex, the complexity is O(k log k), but we notice that this number only applies to low cost operations, while time consuming computations are only done a linear number of times.   This algorithm may be viewed as a variation of Heller's algorithm, which is popular in the geographic information system community. Unfortunately, Heller algorithm is false, as explained in this paper.",
        "published": "1999-07-16T13:25:04Z",
        "link": "http://arxiv.org/abs/cs/9907023v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "The union of unit balls has quadratic complexity, even if they all   contain the origin",
        "authors": [
            "Herve Bronnimann",
            "Olivier Devillers"
        ],
        "summary": "We provide a lower bound construction showing that the union of unit balls in three-dimensional space has quadratic complexity, even if they all contain the origin. This settles a conjecture of Sharir.",
        "published": "1999-07-16T15:20:14Z",
        "link": "http://arxiv.org/abs/cs/9907025v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Further Results on Arithmetic Filters for Geometric Predicates",
        "authors": [
            "Olivier Devillers",
            "Franco P. Preparata"
        ],
        "summary": "An efficient technique to solve precision problems consists in using exact computations. For geometric predicates, using systematically expensive exact computations can be avoided by the use of filters. The predicate is first evaluated using rounding computations, and an error estimation gives a certificate of the validity of the result. In this note, we studies the statistical efficiency of filters for cosphericity predicate with an assumption of regular distribution of the points. We prove that the expected value of the polynomial corresponding to the in sphere test is greater than epsilon with probability O(epsilon log 1/epsilon) improving the results of a previous paper by the same authors.",
        "published": "1999-07-19T15:09:50Z",
        "link": "http://arxiv.org/abs/cs/9907028v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "A Probabilistic Analysis of the Power of Arithmetic Filters",
        "authors": [
            "Olivier Devillers",
            "Franco P. Preparata"
        ],
        "summary": "The assumption of real-number arithmetic, which is at the basis of conventional geometric algorithms, has been seriously challenged in recent years, since digital computers do not exhibit such capability.   A geometric predicate usually consists of evaluating the sign of some algebraic expression. In most cases, rounded computations yield a reliable result, but sometimes rounded arithmetic introduces errors which may invalidate the algorithms. The rounded arithmetic may produce an incorrect result only if the exact absolute value of the algebraic expression is smaller than some (small) varepsilon, which represents the largest error that may arise in the evaluation of the expression. The threshold varepsilon depends on the structure of the expression and on the adopted computer arithmetic, assuming that the input operands are error-free.   A pair (arithmetic engine,threshold) is an \"arithmetic filter\". In this paper we develop a general technique for assessing the efficacy of an arithmetic filter. The analysis consists of evaluating both the threshold and the probability of failure of the filter.   To exemplify the approach, under the assumption that the input points be chosen randomly in a unit ball or unit cube with uniform density, we analyze the two important predicates \"which-side\" and \"insphere\". We show that the probability that the absolute values of the corresponding determinants be no larger than some positive value V, with emphasis on small V, is Theta(V) for the which-side predicate, while for the insphere predicate it is Theta(V^(2/3)) in dimension 1, O(sqrt(V)) in dimension 2, and O(sqrt(V) ln(1/V)) in higher dimensions. Constants are small, and are given in the paper.",
        "published": "1999-07-19T15:22:19Z",
        "link": "http://arxiv.org/abs/cs/9907029v2",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Algorithms for Coloring Quadtrees",
        "authors": [
            "David Eppstein",
            "Marshall W. Bern",
            "Brad Hutchings"
        ],
        "summary": "We describe simple linear time algorithms for coloring the squares of balanced and unbalanced quadtrees so that no two adjacent squares are given the same color. If squares sharing sides are defined as adjacent, we color balanced quadtrees with three colors, and unbalanced quadtrees with four colors; these results are both tight, as some quadtrees require this many colors. If squares sharing corners are defined as adjacent, we color balanced or unbalanced quadtrees with six colors; for some quadtrees, at least five colors are required.",
        "published": "1999-07-19T20:55:39Z",
        "link": "http://arxiv.org/abs/cs/9907030v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Beta-Skeletons have Unbounded Dilation",
        "authors": [
            "David Eppstein"
        ],
        "summary": "A fractal construction shows that, for any beta>0, the beta-skeleton of a point set can have arbitrarily large dilation. In particular this applies to the Gabriel graph.",
        "published": "1999-07-20T20:21:22Z",
        "link": "http://arxiv.org/abs/cs/9907031v1",
        "categories": [
            "cs.CG",
            "math.MG",
            "F.2.2"
        ]
    },
    {
        "title": "Ununfoldable Polyhedra with Convex Faces",
        "authors": [
            "Marshall Bern",
            "Erik D. Demaine",
            "David Eppstein",
            "Eric Kuo",
            "Andrea Mantler",
            "Jack Snoeyink"
        ],
        "summary": "Unfolding a convex polyhedron into a simple planar polygon is a well-studied problem. In this paper, we study the limits of unfoldability by studying nonconvex polyhedra with the same combinatorial structure as convex polyhedra. In particular, we give two examples of polyhedra, one with 24 convex faces and one with 36 triangular faces, that cannot be unfolded by cutting along edges. We further show that such a polyhedron can indeed be unfolded if cuts are allowed to cross faces. Finally, we prove that ``open'' polyhedra with triangular faces may not be unfoldable no matter how they are cut.",
        "published": "1999-08-03T17:37:04Z",
        "link": "http://arxiv.org/abs/cs/9908003v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "G.2.1; F.2.2"
        ]
    },
    {
        "title": "Polygonal Chains Cannot Lock in 4D",
        "authors": [
            "Roxana Cocan",
            "Joseph O'Rourke"
        ],
        "summary": "We prove that, in all dimensions d>=4, every simple open polygonal chain and every tree may be straightened, and every simple closed polygonal chain may be convexified. These reconfigurations can be achieved by algorithms that use polynomial time in the number of vertices, and result in a polynomial number of ``moves.'' These results contrast to those known for d=2, where trees can ``lock,'' and for d=3, where open and closed chains can lock.",
        "published": "1999-08-11T16:09:59Z",
        "link": "http://arxiv.org/abs/cs/9908005v3",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Computational Geometry Column 36",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "Two results in \"computational origami\" are illustrated.",
        "published": "1999-08-11T17:14:00Z",
        "link": "http://arxiv.org/abs/cs/9908006v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Computational Geometry Column 37",
        "authors": [
            "Erik D. Demaine",
            "Joseph O'Rourke"
        ],
        "summary": "Open problems from the 15th Annual ACM Symposium on Computational Geometry.",
        "published": "1999-08-11T17:19:00Z",
        "link": "http://arxiv.org/abs/cs/9908007v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Quadrilateral Meshing by Circle Packing",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We use circle-packing methods to generate quadrilateral meshes for polygonal domains, with guaranteed bounds both on the quality and the number of elements. We show that these methods can generate meshes of several types: (1) the elements form the cells of a Voronoi diagram, (2) all elements have two opposite right angles, (3) all elements are kites, or (4) all angles are at most 120 degrees. In each case the total number of elements is O(n), where n is the number of input vertices.",
        "published": "1999-08-19T20:40:36Z",
        "link": "http://arxiv.org/abs/cs/9908016v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Emerging Challenges in Computational Topology",
        "authors": [
            "Marshall Bern",
            "David Eppstein",
            "Pankaj K. Agarwal",
            "Nina Amenta",
            "Paul Chew",
            "Tamal Dey",
            "David P. Dobkin",
            "Herbert Edelsbrunner",
            "Cindy Grimm",
            "Leonidas J. Guibas",
            "John Harer",
            "Joel Hass",
            "Andrew Hicks",
            "Carroll K. Johnson",
            "Gilad Lerman",
            "David Letscher",
            "Paul Plassmann",
            "Eric Sedgwick",
            "Jack Snoeyink",
            "Jeff Weeks",
            "Chee Yap",
            "Denis Zorin"
        ],
        "summary": "Here we present the results of the NSF-funded Workshop on Computational Topology, which met on June 11 and 12 in Miami Beach, Florida. This report identifies important problems involving both computation and topology.",
        "published": "1999-09-01T20:33:20Z",
        "link": "http://arxiv.org/abs/cs/9909001v1",
        "categories": [
            "cs.CG",
            "math.GT",
            "F.2.2; I.2.9; I.2.10; I.3.5; J.2"
        ]
    },
    {
        "title": "Convex Tours of Bounded Curvature",
        "authors": [
            "Jean-Daniel Boissonnat",
            "Jurek Czyzowicz",
            "Olivier Devillers",
            "Jean-Marc Robert",
            "Mariette Yvinec"
        ],
        "summary": "We consider the motion planning problem for a point constrained to move along a smooth closed convex path of bounded curvature. The workspace of the moving point is bounded by a convex polygon with m vertices, containing an obstacle in a form of a simple polygon with $n$ vertices. We present an O(m+n) time algorithm finding the path, going around the obstacle, whose curvature is the smallest possible.",
        "published": "1999-09-03T15:09:50Z",
        "link": "http://arxiv.org/abs/cs/9909004v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Computing largest circles separating two sets of segments",
        "authors": [
            "Jean-Daniel Boissonnat",
            "Jurek Czyzowicz",
            "Olivier Devillers",
            "Jorge Urrutia",
            "Mariette Yvinec"
        ],
        "summary": "A circle $C$ separates two planar sets if it encloses one of the sets and its open interior disk does not meet the other set. A separating circle is a largest one if it cannot be locally increased while still separating the two given sets. An Theta(n log n) optimal algorithm is proposed to find all largest circles separating two given sets of line segments when line segments are allowed to meet only at their endpoints. In the general case, when line segments may intersect $\\Omega(n^2)$ times, our algorithm can be adapted to work in O(n alpha(n) log n) time and O(n \\alpha(n)) space, where alpha(n) represents the extremely slowly growing inverse of the Ackermann function.",
        "published": "1999-09-03T15:29:28Z",
        "link": "http://arxiv.org/abs/cs/9909005v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Motion Planning of Legged Robots",
        "authors": [
            "Jean-Daniel Boissonnat",
            "Olivier Devillers",
            "Sylvain Lazard"
        ],
        "summary": "We study the problem of computing the free space F of a simple legged robot called the spider robot. The body of this robot is a single point and the legs are attached to the body. The robot is subject to two constraints: each leg has a maximal extension R (accessibility constraint) and the body of the robot must lie above the convex hull of its feet (stability constraint). Moreover, the robot can only put its feet on some regions, called the foothold regions. The free space F is the set of positions of the body of the robot such that there exists a set of accessible footholds for which the robot is stable. We present an efficient algorithm that computes F in O(n2 log n) time using O(n2 alpha(n)) space for n discrete point footholds where alpha(n) is an extremely slowly growing function (alpha(n) <= 3 for any practical value of n). We also present an algorithm for computing F when the foothold regions are pairwise disjoint polygons with n edges in total. This algorithm computes F in O(n2 alpha8(n) log n) time using O(n2 alpha8(n)) space (alpha8(n) is also an extremely slowly growing function). These results are close to optimal since Omega(n2) is a lower bound for the size of F.",
        "published": "1999-09-03T16:18:49Z",
        "link": "http://arxiv.org/abs/cs/9909006v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Circular Separability of Polygons",
        "authors": [
            "Jean-Daniel Boissonnat",
            "Jurek Czyzowicz",
            "Olivier Devillers",
            "Mariette Yvinec"
        ],
        "summary": "Two planar sets are circularly separable if there exists a circle enclosing one of the sets and whose open interior disk does not intersect the other set.   This paper studies two problems related to circular separability. A linear-time algorithm is proposed to decide if two polygons are circularly separable. The algorithm outputs the smallest separating circle. The second problem asks for the largest circle included in a preprocessed, convex polygon, under some point and/or line constraints. The resulting circle must contain the query points and it must lie in the halfplanes delimited by the query lines.",
        "published": "1999-09-03T16:51:23Z",
        "link": "http://arxiv.org/abs/cs/9909007v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Finding an ordinary conic and an ordinary hyperplane",
        "authors": [
            "Olivier Devillers",
            "Asish Mukhopadhyay"
        ],
        "summary": "Given a finite set of non-collinear points in the plane, there exists a line that passes through exactly two points. Such a line is called an ordinary line. An efficient algorithm for computing such a line was proposed by Mukhopadhyay et al. In this note we extend this result in two directions. We first show how to use this algorithm to compute an ordinary conic, that is, a conic passing through exactly five points, assuming that all the points do not lie on the same conic. Both our proofs of existence and the consequent algorithms are simpler than previous ones. We next show how to compute an ordinary hyperplane in three and higher dimensions.",
        "published": "1999-09-27T11:55:29Z",
        "link": "http://arxiv.org/abs/cs/9909017v1",
        "categories": [
            "cs.CG",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Geometric compression for progressive transmission",
        "authors": [
            "Olivier Devillers",
            "Pierre-Maris Gandoin"
        ],
        "summary": "The compression of geometric structures is a relatively new field of data compression. Since about 1995, several articles have dealt with the coding of meshes, using for most of them the following approach: the vertices of the mesh are coded in an order such that it contains partially the topology of the mesh. In the same time, some simple rules attempt to predict the position of the current vertex from the positions of its neighbours that have been previously coded. In this article, we describe a compression algorithm whose principle is completely different: the order of the vertices is used to compress their coordinates, and then the topology of the mesh is reconstructed from the vertices. This algorithm, particularly suited for terrain models, achieves compression factors that are slightly greater than those of the currently available algorithms, and moreover, it allows progressive and interactive transmission of the meshes.",
        "published": "1999-09-28T06:56:27Z",
        "link": "http://arxiv.org/abs/cs/9909018v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Locked and Unlocked Polygonal Chains in 3D",
        "authors": [
            "T. Biedl",
            "E. Demaine",
            "M. Demaine",
            "S. Lazard",
            "A. Lubiw",
            "J. O'Rourke",
            "M. Overmars",
            "S. Robbins",
            "I. Streinu",
            "G. Toussaint",
            "S. Whitesides"
        ],
        "summary": "In this paper, we study movements of simple polygonal chains in 3D. We say that an open, simple polygonal chain can be straightened if it can be continuously reconfigured to a straight sequence of segments in such a manner that both the length of each link and the simplicity of the chain are maintained throughout the movement. The analogous concept for closed chains is convexification: reconfiguration to a planar convex polygon. Chains that cannot be straightened or convexified are called locked. While there are open chains in 3D that are locked, we show that if an open chain has a simple orthogonal projection onto some plane, it can be straightened. For closed chains, we show that there are unknotted but locked closed chains, and we provide an algorithm for convexifying a planar simple polygon in 3D. All our algorithms require only O(n) basic ``moves'' and run in linear time.",
        "published": "1999-10-08T12:04:16Z",
        "link": "http://arxiv.org/abs/cs/9910009v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Finite-resolution hidden surface removal",
        "authors": [
            "Jeff Erickson"
        ],
        "summary": "We propose a hybrid image-space/object-space solution to the classical hidden surface removal problem: Given n disjoint triangles in Real^3 and p sample points (``pixels'') in the xy-plane, determine the first triangle directly behind each pixel. Our algorithm constructs the sampled visibility map of the triangles with respect to the pixels, which is the subset of the trapezoids in a trapezoidal decomposition of the analytic visibility map that contain at least one pixel. The sampled visibility map adapts to local changes in image complexity, and its complexity is bounded both by the number of pixels and by the complexity of the analytic visibility map. Our algorithm runs in time O(n^{1+e} + n^{2/3+e}t^{2/3} + p), where t is the output size and e is any positive constant. This is nearly optimal in the worst case and compares favorably with the best output-sensitive algorithms for both ray casting and analytic hidden surface removal. In the special case where the pixels form a regular grid, a sweepline variant of our algorithm runs in time O(n^{1+e} + n^{2/3+e}t^{2/3} + t log p), which is usually sublinear in the number of pixels.",
        "published": "1999-10-21T21:51:18Z",
        "link": "http://arxiv.org/abs/cs/9910017v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "I.3.7,F.2.2"
        ]
    },
    {
        "title": "On Reconfiguring Tree Linkages: Trees can Lock",
        "authors": [
            "Therese Biedl",
            "Erik Demaine",
            "Martin Demaine",
            "Sylvain Lazard",
            "Anna Lubiw",
            "Joseph O'Rourke",
            "Steve Robbins",
            "Ileana Streinu",
            "Godfried Toussaint",
            "Sue Whitesides"
        ],
        "summary": "It has recently been shown that any simple (i.e. nonintersecting) polygonal chain in the plane can be reconfigured to lie on a straight line, and any simple polygon can be reconfigured to be convex. This result cannot be extended to tree linkages: we show that there are trees with two simple configurations that are not connected by a motion that preserves simplicity throughout the motion. Indeed, we prove that an $N$-link tree can have $2^{\\Omega(N)}$ equivalence classes of configurations.",
        "published": "1999-11-01T17:50:19Z",
        "link": "http://arxiv.org/abs/cs/9910024v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F2.2"
        ]
    },
    {
        "title": "PushPush is NP-hard in 3D",
        "authors": [
            "Joseph O'Rourke",
            "The Smith Problem Solving Group"
        ],
        "summary": "We prove that a particular pushing-blocks puzzle is intractable in 3D. The puzzle, inspired by the game PushPush, consists of unit square blocks on an integer lattice. An agent may push blocks (but never pull them) in attempting to move between given start and goal positions. In the PushPush version, the agent can only push one block at a time, and moreover, each block, when pushed, slides the maximal extent of its free range. We prove this version is NP-hard in 3D by reduction from SAT. The corresponding problem in 2D remains open.",
        "published": "1999-11-28T15:43:50Z",
        "link": "http://arxiv.org/abs/cs/9911013v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Multivariate Regression Depth",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "The regression depth of a hyperplane with respect to a set of n points in R^d is the minimum number of points the hyperplane must pass through in a rotation to vertical. We generalize hyperplane regression depth to k-flats for any k between 0 and d-1. The k=0 case gives the classical notion of center points. We prove that for any k and d, deep k-flats exist, that is, for any set of n points there always exists a k-flat with depth at least a constant fraction of n. As a consequence, we derive a linear-time (1+epsilon)-approximation algorithm for the deepest flat.",
        "published": "1999-12-20T21:50:14Z",
        "link": "http://arxiv.org/abs/cs/9912013v1",
        "categories": [
            "cs.CG",
            "math.CO",
            "G.3"
        ]
    },
    {
        "title": "Geometric compression for progressive transmission",
        "authors": [
            "Olivier Devillers",
            "Pierre-Maris Gandoin"
        ],
        "summary": "The compression of geometric structures is a relatively new field of data compression. Since about 1995, several articles have dealt with the coding of meshes, using for most of them the following approach: the vertices of the mesh are coded in an order such that it contains partially the topology of the mesh. In the same time, some simple rules attempt to predict the position of the current vertex from the positions of its neighbours that have been previously coded. In this article, we describe a compression algorithm whose principle is completely different: the order of the vertices is used to compress their coordinates, and then the topology of the mesh is reconstructed from the vertices. This algorithm, particularly suited for terrain models, achieves compression factors that are slightly greater than those of the currently available algorithms, and moreover, it allows progressive and interactive transmission of the meshes.",
        "published": "1999-09-28T06:56:27Z",
        "link": "http://arxiv.org/abs/cs/9909018v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Finite-resolution hidden surface removal",
        "authors": [
            "Jeff Erickson"
        ],
        "summary": "We propose a hybrid image-space/object-space solution to the classical hidden surface removal problem: Given n disjoint triangles in Real^3 and p sample points (``pixels'') in the xy-plane, determine the first triangle directly behind each pixel. Our algorithm constructs the sampled visibility map of the triangles with respect to the pixels, which is the subset of the trapezoids in a trapezoidal decomposition of the analytic visibility map that contain at least one pixel. The sampled visibility map adapts to local changes in image complexity, and its complexity is bounded both by the number of pixels and by the complexity of the analytic visibility map. Our algorithm runs in time O(n^{1+e} + n^{2/3+e}t^{2/3} + p), where t is the output size and e is any positive constant. This is nearly optimal in the worst case and compares favorably with the best output-sensitive algorithms for both ray casting and analytic hidden surface removal. In the special case where the pixels form a regular grid, a sweepline variant of our algorithm runs in time O(n^{1+e} + n^{2/3+e}t^{2/3} + t log p), which is usually sublinear in the number of pixels.",
        "published": "1999-10-21T21:51:18Z",
        "link": "http://arxiv.org/abs/cs/9910017v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "I.3.7,F.2.2"
        ]
    },
    {
        "title": "On the geometry of similarity search: dimensionality curse and   concentration of measure",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "We suggest that the curse of dimensionality affecting the similarity-based search in large datasets is a manifestation of the phenomenon of concentration of measure on high-dimensional structures. We prove that, under certain geometric assumptions on the query domain $\\Omega$ and the dataset $X$, if $\\Omega$ satisfies the so-called concentration property, then for most query points $x^\\ast$ the ball of radius $(1+\\e)d_X(x^\\ast)$ centred at $x^\\ast$ contains either all points of $X$ or else at least $C_1\\exp(-C_2\\e^2n)$ of them. Here $d_X(x^\\ast)$ is the distance from $x^\\ast$ to the nearest neighbour in $X$ and $n$ is the dimension of $\\Omega$.",
        "published": "1999-01-12T21:56:39Z",
        "link": "http://arxiv.org/abs/cs/9901004v1",
        "categories": [
            "cs.IR",
            "cs.CG",
            "cs.DB",
            "cs.DS",
            "H.3.3;H.2.4;F.2.2"
        ]
    },
    {
        "title": "Mutual Search",
        "authors": [
            "Harry Buhrman",
            "Matthew Franklin",
            "Juan A. Garay",
            "Jaap-Henk Hoepman",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We introduce a search problem called ``mutual search'' where $k$ \\agents, arbitrarily distributed over $n$ sites, are required to locate one another by posing queries of the form ``Anybody at site $i$?''. We ask for the least number of queries that is necessary and sufficient. For the case of two \\agents using deterministic protocols we obtain the following worst-case results: In an oblivious setting (where all pre-planned queries are executed) there is no savings: $n-1$ queries are required and are sufficient. In a nonoblivious setting we can exploit the paradigm of ``no news is also news'' to obtain significant savings: in the synchronous case $0.586n$ queries suffice and $0.536n$ queries are required; in the asynchronous case $0.896n$ queries suffice and a fortiori 0.536 queries are required; for $o(\\sqrt{n})$ \\agents using a deterministic protocol less than $n$ queries suffice; there is a simple randomized protocol for two \\agents with worst-case expected $0.5n$ queries and all randomized protocols require at least $0.125n$ worst-case expected queries. The graph-theoretic framework we formulate for expressing and analyzing algorithms for this problem may be of independent interest.",
        "published": "1999-02-02T15:46:00Z",
        "link": "http://arxiv.org/abs/cs/9902005v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "cs.DC",
            "cs.DM",
            "cs.IR",
            "F.2,C.2,E,1,D.4.4"
        ]
    },
    {
        "title": "ZBroker: A Query Routing Broker for Z39.50 Databases",
        "authors": [
            "Yong Lin",
            "Jian Xu",
            "Ee-Peng Lim",
            "Wee-Keong Ng"
        ],
        "summary": "A query routing broker is a software agent that determines from a large set of accessing information sources the ones most relevant to a user's information need. As the number of information sources on the Internet increases dramatically, future users will have to rely on query routing brokers to decide a small number of information sources to query without incurring too much query processing overheads. In this paper, we describe a query routing broker known as ZBroker developed for bibliographic database servers that support the Z39.50 protocol. ZBroker samples the content of each bibliographic database by using training queries and their results, and summarizes the bibliographic database content into a knowledge base. We present the design and implementation of ZBroker and describe its Web-based user interface.",
        "published": "1999-02-09T03:50:24Z",
        "link": "http://arxiv.org/abs/cs/9902018v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H3.3"
        ]
    },
    {
        "title": "Not Available",
        "authors": [
            "Not Available"
        ],
        "summary": "withdrawn by author",
        "published": "1999-02-09T04:02:55Z",
        "link": "http://arxiv.org/abs/cs/9902017v2",
        "categories": [
            "cs.DL",
            "cs.DB",
            "Not Available"
        ]
    },
    {
        "title": "A geometric framework for modelling similarity search",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "The aim of this paper is to propose a geometric framework for modelling similarity search in large and multidimensional data spaces of general nature, which seems to be flexible enough to address such issues as analysis of complexity, indexability, and the `curse of dimensionality.' Such a framework is provided by the concept of the so-called similarity workload, which is a probability metric space $\\Omega$ (query domain) with a distinguished finite subspace $X$ (dataset), together with an assembly of concepts, techniques, and results from metric geometry. They include such notions as metric transform, $\\e$-entropy, and the phenomenon of concentration of measure on high-dimensional structures. In particular, we discuss the relevance of the latter to understanding the curse of dimensionality. As some of those concepts and techniques are being currently reinvented by the database community, it seems desirable to try and bridge the gap between database research and the relevant work already done in geometry and analysis.",
        "published": "1999-04-07T04:16:02Z",
        "link": "http://arxiv.org/abs/cs/9904002v2",
        "categories": [
            "cs.IR",
            "cs.DB",
            "cs.DS",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "Designing and Mining Multi-Terabyte Astronomy Archives: The Sloan   Digital Sky Survey",
        "authors": [
            "Alexander S. Szalay",
            "Peter Kunszt",
            "Ani Thakar",
            "Jim Gray"
        ],
        "summary": "The next-generation astronomy digital archives will cover most of the universe at fine resolution in many wave-lengths, from X-rays to ultraviolet, optical, and infrared. The archives will be stored at diverse geographical locations. One of the first of these projects, the Sloan Digital Sky Survey (SDSS) will create a 5-wavelength catalog over 10,000 square degrees of the sky (see http://www.sdss.org/). The 200 million objects in the multi-terabyte database will have mostly numerical attributes, defining a space of 100+ dimensions. Points in this space have highly correlated distributions.   The archive will enable astronomers to explore the data interactively. Data access will be aided by a multidimensional spatial index and other indices. The data will be partitioned in many ways. Small tag objects consisting of the most popular attributes speed up frequent searches. Splitting the data among multiple servers enables parallel, scalable I/O and applies parallel processing to the data. Hashing techniques allow efficient clustering and pair-wise comparison algorithms that parallelize nicely. Randomly sampled subsets allow debugging otherwise large queries at the desktop. Central servers will operate a data pump that supports sweeping searches that touch most of the data. The anticipated queries require special operators related to angular distances and complex similarity tests of object properties, like shapes, colors, velocity vectors, or temporal behaviors. These issues pose interesting data management challenges.",
        "published": "1999-07-06T22:56:47Z",
        "link": "http://arxiv.org/abs/cs/9907009v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2, H.2.8,H.3.5, h.3.7"
        ]
    },
    {
        "title": "Microsoft TerraServer: A Spatial Data Warehouse",
        "authors": [
            "Tom Barclay Jim Gray Don Slutz"
        ],
        "summary": "The TerraServer stores aerial, satellite, and topographic images of the earth in a SQL database available via the Internet. It is the world's largest online atlas, combining five terabytes of image data from the United States Geological Survey (USGS) and SPIN-2. This report describes the system-redesign based on our experience over the last year. It also reports usage and operations results over the last year -- over 2 billion web hits and over 20 Terabytes of imagry served over the Internet. Internet browsers provide intuitive spatial and text interfaces to the data. Users need no special hardware, software, or knowledge to locate and browse imagery. This paper describes how terabytes of \"Internet unfriendly\" geo-spatial images were scrubbed and edited into hundreds of millions of \"Internet friendly\" image tiles and loaded into a SQL data warehouse. Microsoft TerraServer demonstrates that general-purpose relational database technology can manage large scale image repositories, and shows that web browsers can be a good geospatial image presentation system.",
        "published": "1999-07-09T21:30:11Z",
        "link": "http://arxiv.org/abs/cs/9907016v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2, H.2.4, H.2.8, H.3.5, H.5.1,J.2"
        ]
    },
    {
        "title": "A simple C++ library for manipulating scientific data sets as structured   data",
        "authors": [
            "Christoph Best"
        ],
        "summary": "Representing scientific data sets efficiently on external storage usually involves converting them to a byte string representation using specialized reader/writer routines. The resulting storage files are frequently difficult to interpret without these specialized routines as they do not contain information about the logical structure of the data. Avoiding such problems usually involves heavy-weight data format libraries or data base systems. We present a simple C++ library that allows to create and access data files that store structured data. The structure of the data is described by a data type that can be built from elementary data types (integer and floating-point numbers, byte strings) and composite data types (arrays, structures, unions). An abstract data access class presents the data to the application. Different actual data file structures can be implemented under this layer. This method is particularly suited to applications that require complex data structures, e.g. molecular dynamics simulations. Extensions such as late type binding and object persistence are discussed.",
        "published": "1999-07-30T08:35:55Z",
        "link": "http://arxiv.org/abs/cs/9907043v1",
        "categories": [
            "cs.CE",
            "cs.DB",
            "E.5"
        ]
    },
    {
        "title": "Least expected cost query optimization: an exercise in utility",
        "authors": [
            "Francis C. Chu",
            "Joseph Y. Halpern",
            "Praveen Seshadri"
        ],
        "summary": "We identify two unreasonable, though standard, assumptions made by database query optimizers that can adversely affect the quality of the chosen evaluation plans. One assumption is that it is enough to optimize for the expected case---that is, the case where various parameters (like available memory) take on their expected value. The other assumption is that the parameters are constant throughout the execution of the query. We present an algorithm based on the ``System R''-style query optimization algorithm that does not rely on these assumptions. The algorithm we present chooses the plan of the least expected cost instead of the plan of least cost given some fixed value of the parameters. In execution environments that exhibit a high degree of variability, our techniques should result in better performance.",
        "published": "1999-09-21T21:20:20Z",
        "link": "http://arxiv.org/abs/cs/9909016v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "Consistent Checkpointing in Distributed Databases: Towards a Formal   Approach",
        "authors": [
            "R. Baldoni",
            "F. Quaglia",
            "M. Raynal"
        ],
        "summary": "Whether it is for audit or for recovery purposes, data checkpointing is an important problem of distributed database systems. Actually, transactions establish dependence relations on data checkpoints taken by data object managers. So, given an arbitrary set of data checkpoints (including at least a single data checkpoint from a data manager, and at most a data checkpoint from each data manager), an important question is the following one: ``Can these data checkpoints be members of a same consistent global checkpoint?''. This paper answers this question by providing a necessary and sufficient condition suited for database systems. Moreover, to show the usefulness of this condition, two {\\em non-intrusive} data checkpointing protocols are derived from this condition. It is also interesting to note that this paper, by exhibiting ``correspondences'', establishes a bridge between the data object/transaction model and the process/message-passing model.",
        "published": "1999-10-22T15:17:29Z",
        "link": "http://arxiv.org/abs/cs/9910019v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "C.2.4; H.2"
        ]
    },
    {
        "title": "Efficient and Extensible Algorithms for Multi Query Optimization",
        "authors": [
            "Prasan Roy",
            "S. Seshadri",
            "S. Sudarshan",
            "Siddhesh Bhobe"
        ],
        "summary": "Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multi-query optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space.   In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.",
        "published": "1999-10-25T16:30:20Z",
        "link": "http://arxiv.org/abs/cs/9910021v1",
        "categories": [
            "cs.DB",
            "H.2.4;H.2.7"
        ]
    },
    {
        "title": "Comparative Analysis of Five XML Query Languages",
        "authors": [
            "Angela Bonifati",
            "Stefano Ceri"
        ],
        "summary": "XML is becoming the most relevant new standard for data representation and exchange on the WWW. Novel languages for extracting and restructuring the XML content have been proposed, some in the tradition of database query languages (i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query language has yet been decided, but the discussion is ongoing within the World Wide Web Consortium and within many academic institutions and Internet-related major companies. We present a comparison of five, representative query languages for XML, highlighting their common features and differences.",
        "published": "1999-12-22T15:25:55Z",
        "link": "http://arxiv.org/abs/cs/9912015v1",
        "categories": [
            "cs.DB",
            "H.2; H.2.3; I.7; I.7.1; I.7.2"
        ]
    },
    {
        "title": "Minimum Description Length Induction, Bayesianism, and Kolmogorov   Complexity",
        "authors": [
            "Paul Vitanyi",
            "Ming Li"
        ],
        "summary": "The relationship between the Bayesian approach and the minimum description length approach is established. We sharpen and clarify the general modeling principles MDL and MML, abstracted as the ideal MDL principle and defined from Bayes's rule by means of Kolmogorov complexity. The basic condition under which the ideal principle should be applied is encapsulated as the Fundamental Inequality, which in broad terms states that the principle is valid when the data are random, relative to every contemplated hypothesis and also these hypotheses are random relative to the (universal) prior. Basically, the ideal principle states that the prior probability associated with the hypothesis should be given by the algorithmic universal probability, and the sum of the log universal probability of the model plus the log of the probability of the data given the model should be minimized. If we restrict the model class to the finite sets then application of the ideal principle turns into Kolmogorov's minimal sufficient statistic. In general we show that data compression is almost always the best strategy, both in hypothesis identification and prediction.",
        "published": "1999-01-27T17:48:14Z",
        "link": "http://arxiv.org/abs/cs/9901014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "cs.LO",
            "math.IT",
            "math.PR",
            "physics.data-an",
            "E.4,F.2,H.3,I.2,I.5,I.7"
        ]
    },
    {
        "title": "Duality between Multidimensional Convolutional Codes and Systems",
        "authors": [
            "Heide Gluesing-Luerssen",
            "Joachim Rosenthal",
            "Paul Weiner"
        ],
        "summary": "Multidimensional convolutional codes generalize (one dimensional) convolutional codes and they correspond under a natural duality to multidimensional systems widely studied in the systems literature.",
        "published": "1999-05-07T19:56:54Z",
        "link": "http://arxiv.org/abs/math/9905046v1",
        "categories": [
            "math.OC",
            "cs.IT",
            "math.AC",
            "math.AG",
            "math.IT",
            "93C35; 94B10"
        ]
    },
    {
        "title": "On Bounded-Weight Error-Correcting Codes",
        "authors": [
            "Russell Bent",
            "Michael Schear",
            "Lane A. Hemaspaandra",
            "Gabriel Istrate"
        ],
        "summary": "This paper computationally obtains optimal bounded-weight, binary, error-correcting codes for a variety of distance bounds and dimensions. We compare the sizes of our codes to the sizes of optimal constant-weight, binary, error-correcting codes, and evaluate the differences.",
        "published": "1999-06-01T18:24:00Z",
        "link": "http://arxiv.org/abs/cs/9906001v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "H.1.1; E.4"
        ]
    },
    {
        "title": "Coding Theory and Uniform Distributions",
        "authors": [
            "M. M. Skriganov"
        ],
        "summary": "In the present paper we introduce and study finite point subsets of a special kind, called optimum distributions, in the n-dimensional unit cube. Such distributions are closely related with known (delta,s,n)-nets of low discrepancy. It turns out that optimum distributions have a rich combinatorial structure. Namely, we show that optimum distributions can be characterized completely as maximum distance separable codes with respect to a non-Hamming metric. Weight spectra of such codes can be evaluated precisely. We also consider linear codes and distributions and study their general properties including the duality with respect to a suitable inner product. The corresponding generalized MacWilliams identities for weight enumerators are briefly discussed. Broad classes of linear maximum distance separable codes and linear optimum distributions are explicitely constructed in the paper by the Hermite interpolations over finite fields.",
        "published": "1999-09-28T08:33:05Z",
        "link": "http://arxiv.org/abs/math/9909163v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "math.NT",
            "11K38, 11T71, 94B60"
        ]
    },
    {
        "title": "Efficient sphere-covering and converse measure concentration via   generalized coding theorems",
        "authors": [
            "Ioannis Kontoyiannis"
        ],
        "summary": "Suppose A is a finite set equipped with a probability measure P and let M be a ``mass'' function on A. We give a probabilistic characterization of the most efficient way in which A^n can be almost-covered using spheres of a fixed radius. An almost-covering is a subset C_n of A^n, such that the union of the spheres centered at the points of C_n has probability close to one with respect to the product measure P^n. An efficient covering is one with small mass M^n(C_n); n is typically large. With different choices for M and the geometry on A our results give various corollaries as special cases, including Shannon's data compression theorem, a version of Stein's lemma (in hypothesis testing), and a new converse to some measure concentration inequalities on discrete spaces. Under mild conditions, we generalize our results to abstract spaces and non-product measures.",
        "published": "1999-10-12T20:28:46Z",
        "link": "http://arxiv.org/abs/math/9910062v2",
        "categories": [
            "math.PR",
            "cs.IT",
            "math.FA",
            "math.IT",
            "60E15, 28A35 (primary), 94A15, 60F10 (secondary)"
        ]
    },
    {
        "title": "Rational points, genus and asymptotic behaviour in reduced algebraic   curves over finite fields",
        "authors": [
            "J. I. Farran"
        ],
        "summary": "The number A(q) shows the asymptotic behaviour of the quotient of the number of rational points over the genus of non-singular absolutely irreducible curves over a finite field Fq. Research on bounds for A(q) is closely connected with the so-called asymptotic main problem in Coding Theory. In this paper, we study some generalizations of this number for non-irreducible curves, their connection with A(q) and its application in Coding Theory.",
        "published": "1999-10-27T14:57:20Z",
        "link": "http://arxiv.org/abs/math/9910149v1",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.IT",
            "math.NT",
            "14Q05"
        ]
    },
    {
        "title": "Decoding Algebraic Geometry codes by a key equation",
        "authors": [
            "J. I. Farran"
        ],
        "summary": "A new effective decoding algorithm is presented for arbitrary algebraic-geometric codes on the basis of solving a generalized key equation with the majority coset scheme of Duursma. It is an improvement of Ehrhard's algorithm, since the method corrects up to the half of the Goppa distance with complexity order O(n**2.81), and with no further assumption on the degree of the divisor G.",
        "published": "1999-10-27T15:39:48Z",
        "link": "http://arxiv.org/abs/math/9910151v1",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.IT",
            "math.NA",
            "14Q05 (Primary) 11T71 (Secondary)"
        ]
    },
    {
        "title": "Computing Weierstrass semigroups and the Feng-Rao distance from singular   plane models",
        "authors": [
            "A. Campillo",
            "J. I. Farran"
        ],
        "summary": "We present an algorithm to compute the Weierstrass semigroup at a point P together with functions for each value in the semigroup, provided P is the only branch at infinity of a singular plane model for the curve. As a byproduct, the method also provides us with a basis for the spaces L(mP) and the computation of the Feng-Rao distance for the corresponding array of geometric Goppa codes. A general computation of the Feng-Rao distance is also obtained. Everything can be applied to the decoding problem by using the majority scheme of Feng and Rao.",
        "published": "1999-10-28T09:27:47Z",
        "link": "http://arxiv.org/abs/math/9910155v1",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.IT",
            "math.NA",
            "14Q05 (Primary) 11T71 (Secondary)"
        ]
    },
    {
        "title": "Polynomial method in coding and information theory",
        "authors": [
            "A. Ashikhmin",
            "A. Barg",
            "S. Litsyn"
        ],
        "summary": "Polynomial, or Delsarte's, method in coding theory accounts for a variety of structural results on, and bounds on the size of, extremal configurations (codes and designs) in various metric spaces. In recent works of the authors the applicability of the method was extended to cover a wider range of problems in coding and information theory. In this paper we present a general framework for the method which includes previous results as particular cases. We explain how this generalization leads to new asymptotic bounds on the performance of codes in binary-input memoryless channels and the Gaussian channel, which improve the results of Shannon et al. of 1959-67, and to a number of other results in combinatorial coding theory.",
        "published": "1999-10-31T00:03:37Z",
        "link": "http://arxiv.org/abs/math/9910175v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94A24, 94B27, 05E30, 33C45"
        ]
    },
    {
        "title": "On the parameters of Algebraic Geometry codes related to Arf semigroups",
        "authors": [
            "A. Campillo",
            "J. I. Farran",
            "C. Munuera"
        ],
        "summary": "In this paper we compute the order (or Feng-Rao) bound on the minimum distance of one-point algebraic geometry codes, when the Weierstrass semigroup at the point Q is an Arf semigroup. The results developed to that purpose also provide the dimension of the improved geometric Goppa codes related to those.",
        "published": "1999-11-04T15:57:56Z",
        "link": "http://arxiv.org/abs/math/9911025v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "11T71 (Primary) 14Q05 (Secondary)"
        ]
    },
    {
        "title": "Cortical Potential Distributions and Cognitive Information Processing",
        "authors": [
            "Henry C. Tuckwell"
        ],
        "summary": "The use of cortical field potentials rather than the details of spike trains as the basis for cognitive information processing is proposed. This results in a space of cognitive elements with natural metrics. Sets of spike trains may also be considered to be points in a multidimensional metric space. The closeness of sets of spike trains in such a space implies the closeness of points in the resulting function space of potential distributions.",
        "published": "1999-02-01T12:54:37Z",
        "link": "http://arxiv.org/abs/cond-mat/9902011v1",
        "categories": [
            "cond-mat.dis-nn",
            "adap-org",
            "cond-mat.stat-mech",
            "cs.NE",
            "math-ph",
            "math.MP",
            "nlin.AO",
            "physics.bio-ph",
            "q-bio"
        ]
    },
    {
        "title": "A Discipline of Evolutionary Programming",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Genetic fitness optimization using small populations or small population updates across generations generally suffers from randomly diverging evolutions. We propose a notion of highly probable fitness optimization through feasible evolutionary computing runs on small size populations. Based on rapidly mixing Markov chains, the approach pertains to most types of evolutionary genetic algorithms, genetic programming and the like. We establish that for systems having associated rapidly mixing Markov chains and appropriate stationary distributions the new method finds optimal programs (individuals) with probability almost 1. To make the method useful would require a structured design methodology where the development of the program and the guarantee of the rapidly mixing property go hand in hand. We analyze a simple example to show that the method is implementable. More significant examples require theoretical advances, for example with respect to the Metropolis filter.",
        "published": "1999-02-02T16:17:16Z",
        "link": "http://arxiv.org/abs/cs/9902006v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.MA",
            "I.2,E.1,F.1"
        ]
    },
    {
        "title": "An Efficient Mean Field Approach to the Set Covering Problem",
        "authors": [
            "Mattias Ohlsson",
            "Carsten Peterson",
            "Bo Söderberg"
        ],
        "summary": "A mean field feedback artificial neural network algorithm is developed and explored for the set covering problem. A convenient encoding of the inequality constraints is achieved by means of a multilinear penalty function. An approximate energy minimum is obtained by iterating a set of mean field equations, in combination with annealing. The approach is numerically tested against a set of publicly available test problems with sizes ranging up to 5x10^3 rows and 10^6 columns. When comparing the performance with exact results for sizes where these are available, the approach yields results within a few percent from the optimal solutions. Comparisons with other approximate methods also come out well, in particular given the very low CPU consumption required -- typically a few seconds. Arbitrary problems can be processed using the algorithm via a public domain server.",
        "published": "1999-02-12T09:18:53Z",
        "link": "http://arxiv.org/abs/cs/9902025v1",
        "categories": [
            "cs.NE",
            "G.1.6"
        ]
    },
    {
        "title": "Evolution of genetic organization in digital organisms",
        "authors": [
            "Charles Ofria",
            "Christoph Adami"
        ],
        "summary": "We examine the evolution of expression patterns and the organization of genetic information in populations of self-replicating digital organisms. Seeding the experiments with a linearly expressed ancestor, we witness the development of complex, parallel secondary expression patterns. Using principles from information theory, we demonstrate an evolutionary pressure towards overlapping expressions causing variation (and hence further evolution) to sharply drop. Finally, we compare the overlapping sections of dominant genomes to those portions which are singly expressed and observe a significant difference in the entropy of their encoding.",
        "published": "1999-03-06T00:39:40Z",
        "link": "http://arxiv.org/abs/adap-org/9903003v1",
        "categories": [
            "adap-org",
            "cs.NE",
            "nlin.AO",
            "q-bio.PE"
        ]
    },
    {
        "title": "Ensembles of Radial Basis Function Networks for Spectroscopic Detection   of Cervical Pre-Cancer",
        "authors": [
            "Kagan Tumer",
            "Nirmala Ramanujam",
            "Joydeep Ghosh",
            "Rebecca Richards-Kortum"
        ],
        "summary": "The mortality related to cervical cancer can be substantially reduced through early detection and treatment. However, current detection techniques, such as Pap smear and colposcopy, fail to achieve a concurrently high sensitivity and specificity.   In vivo fluorescence spectroscopy is a technique which quickly, non-invasively and quantitatively probes the biochemical and morphological changes that occur in pre-cancerous tissue.   A multivariate statistical algorithm was used to extract clinically useful information from tissue spectra acquired from 361 cervical sites from 95 patients at 337, 380 and 460 nm excitation wavelengths. The multivariate statistical analysis was also employed to reduce the number of fluorescence excitation-emission wavelength pairs required to discriminate healthy tissue samples from pre-cancerous tissue samples. The use of connectionist methods such as multi layered perceptrons, radial basis function networks, and ensembles of such networks was investigated. RBF ensemble algorithms based on fluorescence spectra potentially provide automated, and near real-time implementation of pre-cancer detection in the hands of non-experts. The results are more reliable, direct and accurate than those achieved by either human experts or multivariate statistical algorithms.",
        "published": "1999-05-20T18:28:15Z",
        "link": "http://arxiv.org/abs/cs/9905011v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "q-bio",
            "I.5.1 ; J.3"
        ]
    },
    {
        "title": "Linear and Order Statistics Combiners for Pattern Classification",
        "authors": [
            "Kagan Tumer",
            "Joydeep Ghosh"
        ],
        "summary": "Several researchers have experimentally shown that substantial improvements can be obtained in difficult pattern recognition problems by combining or integrating the outputs of multiple classifiers. This chapter provides an analytical framework to quantify the improvements in classification results due to combining. The results apply to both linear combiners and order statistics combiners. We first show that to a first order approximation, the error rate obtained over and above the Bayes error rate, is directly proportional to the variance of the actual decision boundaries around the Bayes optimum boundary. Combining classifiers in output space reduces this variance, and hence reduces the \"added\" error. If N unbiased classifiers are combined by simple averaging, the added error rate can be reduced by a factor of N if the individual errors in approximating the decision boundaries are uncorrelated. Expressions are then derived for linear combiners which are biased or correlated, and the effect of output correlations on ensemble performance is quantified. For order statistics based non-linear combiners, we derive expressions that indicate how much the median, the maximum and in general the ith order statistic can improve classifier performance. The analysis presented here facilitates the understanding of the relationships among error rates, classifier boundary distributions, and combining in output space. Experimental results on several public domain data sets are provided to illustrate the benefits of combining and to support the analytical results.",
        "published": "1999-05-20T20:15:13Z",
        "link": "http://arxiv.org/abs/cs/9905012v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.5.1 ; I.2.6"
        ]
    },
    {
        "title": "Robust Combining of Disparate Classifiers through Order Statistics",
        "authors": [
            "Kagan Tumer",
            "Joydeep Ghosh"
        ],
        "summary": "Integrating the outputs of multiple classifiers via combiners or meta-learners has led to substantial improvements in several difficult pattern recognition problems. In the typical setting investigated till now, each classifier is trained on data taken or resampled from a common data set, or (almost) randomly selected subsets thereof, and thus experiences similar quality of training data. However, in certain situations where data is acquired and analyzed on-line at several geographically distributed locations, the quality of data may vary substantially, leading to large discrepancies in performance of individual classifiers. In this article we introduce and investigate a family of classifiers based on order statistics, for robust handling of such cases. Based on a mathematical modeling of how the decision boundaries are affected by order statistic combiners, we derive expressions for the reductions in error expected when such combiners are used. We show analytically that the selection of the median, the maximum and in general, the $i^{th}$ order statistic improves classification performance. Furthermore, we introduce the trim and spread combiners, both based on linear combinations of the ordered classifier outputs, and show that they are quite beneficial in presence of outliers or uneven classifier performance. Experimental results on several public domain data sets corroborate these findings.",
        "published": "1999-05-20T20:37:02Z",
        "link": "http://arxiv.org/abs/cs/9905013v1",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.NE",
            "I.5.1 ; G.3"
        ]
    },
    {
        "title": "Ocular dominance patterns in mammalian visual cortex: A wire length   minimization approach",
        "authors": [
            "Dmitri B. Chklovskii",
            "Alexei A. Koulakov"
        ],
        "summary": "We propose a theory for ocular dominance (OD) patterns in mammalian primary visual cortex. This theory is based on the premise that OD pattern is an adaptation to minimize the length of intra-cortical wiring. Thus we can understand the existing OD patterns by solving a wire length minimization problem. We divide all the neurons into two classes: left-eye dominated and right-eye dominated. We find that segregation of neurons into monocular regions reduces wire length if the number of connections with the neurons of the same class differs from that with the other class. The shape of the regions depends on the relative fraction of neurons in the two classes. If the numbers are close we find that the optimal OD pattern consists of interdigitating stripes. If one class is less numerous than the other, the optimal OD pattern consists of patches of the first class neurons in the sea of the other class neurons. We predict the transition from stripes to patches when the fraction of neurons dominated by the ipsilateral eye is about 40%. This prediction agrees with the data in macaque and Cebus monkeys. This theory can be applied to other binary cortical systems.",
        "published": "1999-06-14T23:59:59Z",
        "link": "http://arxiv.org/abs/cond-mat/9906206v1",
        "categories": [
            "cond-mat.soft",
            "cond-mat.dis-nn",
            "cs.NE",
            "physics.bio-ph",
            "q-bio"
        ]
    },
    {
        "title": "The importance of quantum decoherence in brain processes",
        "authors": [
            "Max Tegmark"
        ],
        "summary": "Based on a calculation of neural decoherence rates, we argue that that the degrees of freedom of the human brain that relate to cognitive processes should be thought of as a classical rather than quantum system, i.e., that there is nothing fundamentally wrong with the current classical approach to neural network simulations. We find that the decoherence timescales ~10^{-13}-10^{-20} seconds are typically much shorter than the relevant dynamical timescales (~0.001-0.1 seconds), both for regular neuron firing and for kink-like polarization excitations in microtubules. This conclusion disagrees with suggestions by Penrose and others that the brain acts as a quantum computer, and that quantum coherence is related to consciousness in a fundamental way.",
        "published": "1999-07-05T10:33:19Z",
        "link": "http://arxiv.org/abs/quant-ph/9907009v2",
        "categories": [
            "quant-ph",
            "cond-mat.dis-nn",
            "cs.NE",
            "physics.bio-ph",
            "q-bio"
        ]
    },
    {
        "title": "Genetic Algorithms in Time-Dependent Environments",
        "authors": [
            "Christopher Ronnewinkel",
            "Claus O. Wilke",
            "Thomas Martinetz"
        ],
        "summary": "The influence of time-dependent fitnesses on the infinite population dynamics of simple genetic algorithms (without crossover) is analyzed. Based on general arguments, a schematic phase diagram is constructed that allows one to characterize the asymptotic states in dependence on the mutation rate and the time scale of changes. Furthermore, the notion of regular changes is raised for which the population can be shown to converge towards a generalized quasispecies. Based on this, error thresholds and an optimal mutation rate are approximately calculated for a generational genetic algorithm with a moving needle-in-the-haystack landscape. The so found phase diagram is fully consistent with our general considerations.",
        "published": "1999-11-04T10:39:08Z",
        "link": "http://arxiv.org/abs/physics/9911006v1",
        "categories": [
            "physics.bio-ph",
            "adap-org",
            "cs.NE",
            "nlin.AO",
            "q-bio"
        ]
    },
    {
        "title": "On the geometry of similarity search: dimensionality curse and   concentration of measure",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "We suggest that the curse of dimensionality affecting the similarity-based search in large datasets is a manifestation of the phenomenon of concentration of measure on high-dimensional structures. We prove that, under certain geometric assumptions on the query domain $\\Omega$ and the dataset $X$, if $\\Omega$ satisfies the so-called concentration property, then for most query points $x^\\ast$ the ball of radius $(1+\\e)d_X(x^\\ast)$ centred at $x^\\ast$ contains either all points of $X$ or else at least $C_1\\exp(-C_2\\e^2n)$ of them. Here $d_X(x^\\ast)$ is the distance from $x^\\ast$ to the nearest neighbour in $X$ and $n$ is the dimension of $\\Omega$.",
        "published": "1999-01-12T21:56:39Z",
        "link": "http://arxiv.org/abs/cs/9901004v1",
        "categories": [
            "cs.IR",
            "cs.CG",
            "cs.DB",
            "cs.DS",
            "H.3.3;H.2.4;F.2.2"
        ]
    },
    {
        "title": "Average-Case Complexity of Shellsort",
        "authors": [
            "Tao Jiang",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "We prove a general lower bound on the average-case complexity of Shellsort: the average number of data-movements (and comparisons) made by a $p$-pass Shellsort for any incremental sequence is $\\Omega (pn^{1 + 1/p)$ for all $p \\leq \\log n$. Using similar arguments, we analyze the average-case complexity of several other sorting algorithms.",
        "published": "1999-01-20T16:32:01Z",
        "link": "http://arxiv.org/abs/cs/9901010v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; F.1.3"
        ]
    },
    {
        "title": "Mutual Search",
        "authors": [
            "Harry Buhrman",
            "Matthew Franklin",
            "Juan A. Garay",
            "Jaap-Henk Hoepman",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We introduce a search problem called ``mutual search'' where $k$ \\agents, arbitrarily distributed over $n$ sites, are required to locate one another by posing queries of the form ``Anybody at site $i$?''. We ask for the least number of queries that is necessary and sufficient. For the case of two \\agents using deterministic protocols we obtain the following worst-case results: In an oblivious setting (where all pre-planned queries are executed) there is no savings: $n-1$ queries are required and are sufficient. In a nonoblivious setting we can exploit the paradigm of ``no news is also news'' to obtain significant savings: in the synchronous case $0.586n$ queries suffice and $0.536n$ queries are required; in the asynchronous case $0.896n$ queries suffice and a fortiori 0.536 queries are required; for $o(\\sqrt{n})$ \\agents using a deterministic protocol less than $n$ queries suffice; there is a simple randomized protocol for two \\agents with worst-case expected $0.5n$ queries and all randomized protocols require at least $0.125n$ worst-case expected queries. The graph-theoretic framework we formulate for expressing and analyzing algorithms for this problem may be of independent interest.",
        "published": "1999-02-02T15:46:00Z",
        "link": "http://arxiv.org/abs/cs/9902005v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "cs.DC",
            "cs.DM",
            "cs.IR",
            "F.2,C.2,E,1,D.4.4"
        ]
    },
    {
        "title": "A Discipline of Evolutionary Programming",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Genetic fitness optimization using small populations or small population updates across generations generally suffers from randomly diverging evolutions. We propose a notion of highly probable fitness optimization through feasible evolutionary computing runs on small size populations. Based on rapidly mixing Markov chains, the approach pertains to most types of evolutionary genetic algorithms, genetic programming and the like. We establish that for systems having associated rapidly mixing Markov chains and appropriate stationary distributions the new method finds optimal programs (individuals) with probability almost 1. To make the method useful would require a structured design methodology where the development of the program and the guarantee of the rapidly mixing property go hand in hand. We analyze a simple example to show that the method is implementable. More significant examples require theoretical advances, for example with respect to the Metropolis filter.",
        "published": "1999-02-02T16:17:16Z",
        "link": "http://arxiv.org/abs/cs/9902006v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.MA",
            "I.2,E.1,F.1"
        ]
    },
    {
        "title": "A better lower bound for quantum algorithms searching an ordered list",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We show that any quantum algorithm searching an ordered list of n elements needs to examine at least 1/12 log n-O(1) of them. Classically, log n queries are both necessary and sufficient. This shows that quantum algorithms can achieve only a constant speedup for this problem. Our result improves lower bounds of Buhrman and de Wolf(quant-ph/9811046) and Farhi, Goldstone, Gutmann and Sipser (quant-ph/9812057).",
        "published": "1999-02-14T01:20:11Z",
        "link": "http://arxiv.org/abs/quant-ph/9902053v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Space-Efficient Routing Tables for Almost All Networks and the   Incompressibility Method",
        "authors": [
            "Harry Buhrman",
            "Jaap-Henk Hoepman",
            "Paul Vitanyi"
        ],
        "summary": "We use the incompressibility method based on Kolmogorov complexity to determine the total number of bits of routing information for almost all network topologies. In most models for routing, for almost all labeled graphs $\\Theta (n^2)$ bits are necessary and sufficient for shortest path routing. By `almost all graphs' we mean the Kolmogorov random graphs which constitute a fraction of $1-1/n^c$ of all graphs on $n$ nodes, where $c > 0$ is an arbitrary fixed constant. There is a model for which the average case lower bound rises to $\\Omega(n^2 \\log n)$ and another model where the average case upper bound drops to $O(n \\log^2 n)$. This clearly exposes the sensitivity of such bounds to the model under consideration. If paths have to be short, but need not be shortest (if the stretch factor may be larger than 1), then much less space is needed on average, even in the more demanding models. Full-information routing requires $\\Theta (n^3)$ bits on average. For worst-case static networks we prove a $\\Omega(n^2 \\log n)$ lower bound for shortest path routing and all stretch factors $<2$ in some networks where free relabeling is not allowed.",
        "published": "1999-03-10T19:01:02Z",
        "link": "http://arxiv.org/abs/cs/9903009v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.CC",
            "cs.DS",
            "cs.NI",
            "C.2, F.2, D.4"
        ]
    },
    {
        "title": "A class of problems of NP to be worth to search an efficient solving   algorithm",
        "authors": [
            "Anatoly D. Plotnikov"
        ],
        "summary": "We examine possibility to design an efficient solving algorithm for problems of the class \\np. It is introduced a classification of \\np problems by the property that a partial solution of size $k$ can be extended into a partial solution of size $k+1$ in polynomial time. It is defined an unique class problems to be worth to search an efficient solving algorithm. The problems, which are outside of this class, are inherently exponential. We show that the Hamiltonian cycle problem is inherently exponential.",
        "published": "1999-03-11T19:36:05Z",
        "link": "http://arxiv.org/abs/cs/9903010v1",
        "categories": [
            "cs.DS",
            "F.2.2;G.2.1;G.2.2"
        ]
    },
    {
        "title": "A complete anytime algorithm for balanced number partitioning",
        "authors": [
            "Stephan Mertens"
        ],
        "summary": "Given a set of numbers, the balanced partioning problem is to divide them into two subsets, so that the sum of the numbers in each subset are as nearly equal as possible, subject to the constraint that the cardinalities of the subsets be within one of each other. We combine the balanced largest differencing method (BLDM) and Korf's complete Karmarkar-Karp algorithm to get a new algorithm that optimally solves the balanced partitioning problem. For numbers with twelve significant digits or less, the algorithm can optimally solve balanced partioning problems of arbitrary size in practice. For numbers with greater precision, it first returns the BLDM solution, then continues to find better solutions as time allows.",
        "published": "1999-03-11T22:38:01Z",
        "link": "http://arxiv.org/abs/cs/9903011v1",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.AI",
            "F.2.2"
        ]
    },
    {
        "title": "Formalization of the class of problems solvable by a nondeterministic   Turing machine",
        "authors": [
            "Anatoly D. Plotnikov"
        ],
        "summary": "The objective of this article is to formalize the definition of NP problems.   We construct a mathematical model of discrete problems as independence systems with weighted elements. We introduce two auxiliary sets that characterize the solution of the problem: the adjoint set, which contains the elements from the original set none of which can be adjoined to the already chosen solution elements; and the residual set, in which every element can be adjoined to previously chosen solution elements.   In a problem without lookahead, every adjoint set can be generated by the solution algorithm effectively, in polynomial time.   The main result of the study is the assertion that the NP class is identical with the class of problems without lookahead. Hence it follows that if we fail to find an effective (polynomial-time) solution algorithm for a given problem, then we need to look for an alternative formulation of the problem in set of problems without lookahead.",
        "published": "1999-03-16T17:13:43Z",
        "link": "http://arxiv.org/abs/cs/9903012v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2;F.1.2"
        ]
    },
    {
        "title": "Tiling with bars under tomographic constraints",
        "authors": [
            "Christoph Durr",
            "Eric Goles",
            "Ivan Rapaport",
            "Eric Remila"
        ],
        "summary": "We wish to tile a rectangle or a torus with only vertical and horizontal bars of a given length, such that the number of bars in every column and row equals given numbers. We present results for particular instances and for a more general problem, while leaving open the initial problem.",
        "published": "1999-03-31T09:58:20Z",
        "link": "http://arxiv.org/abs/cs/9903020v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "A geometric framework for modelling similarity search",
        "authors": [
            "Vladimir Pestov"
        ],
        "summary": "The aim of this paper is to propose a geometric framework for modelling similarity search in large and multidimensional data spaces of general nature, which seems to be flexible enough to address such issues as analysis of complexity, indexability, and the `curse of dimensionality.' Such a framework is provided by the concept of the so-called similarity workload, which is a probability metric space $\\Omega$ (query domain) with a distinguished finite subspace $X$ (dataset), together with an assembly of concepts, techniques, and results from metric geometry. They include such notions as metric transform, $\\e$-entropy, and the phenomenon of concentration of measure on high-dimensional structures. In particular, we discuss the relevance of the latter to understanding the curse of dimensionality. As some of those concepts and techniques are being currently reinvented by the database community, it seems desirable to try and bridge the gap between database research and the relevant work already done in geometry and analysis.",
        "published": "1999-04-07T04:16:02Z",
        "link": "http://arxiv.org/abs/cs/9904002v2",
        "categories": [
            "cs.IR",
            "cs.DB",
            "cs.DS",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "A Lower Bound on the Average-Case Complexity of Shellsort",
        "authors": [
            "Tao Jiang",
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "We prove a general lower bound on the average-case complexity of Shellsort: the average number of data-movements (and comparisons) made by a $p$-pass Shellsort for any incremental sequence is $\\Omega (pn^{1 + 1/p})$ for every $p$. The proof method is an incompressibility argument based on Kolmogorov complexity. Using similar techniques, the average-case complexity of several other sorting algorithms is analyzed.",
        "published": "1999-06-04T15:11:31Z",
        "link": "http://arxiv.org/abs/cs/9906008v2",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.2.2; E.1"
        ]
    },
    {
        "title": "Reconstructing Polyatomic Structures from Discrete X-Rays:   NP-Completeness Proof for Three Atoms",
        "authors": [
            "Christoph Durr",
            "Marek Chrobak"
        ],
        "summary": "We address a discrete tomography problem that arises in the study of the atomic structure of crystal lattices. A polyatomic structure T can be defined as an integer lattice in dimension D>=2, whose points may be occupied by $c$ distinct types of atoms. To ``analyze'' T, we conduct ell measurements that we call_discrete X-rays_. A discrete X-ray in direction xi determines the number of atoms of each type on each line parallel to xi. Given ell such non-parallel X-rays, we wish to reconstruct T.   The complexity of the problem for c=1 (one atom type) has been completely determined by Gardner, Gritzmann and Prangenberg, who proved that the problem is NP-complete for any dimension D>=2 and ell>=3 non-parallel X-rays, and that it can be solved in polynomial time otherwise.   The NP-completeness result above clearly extends to any c>=2, and therefore when studying the polyatomic case we can assume that ell=2. As shown in another article by the same authors, this problem is also NP-complete for c>=6 atoms, even for dimension D=2 and axis-parallel X-rays. They conjecture that the problem remains NP-complete for c=3,4,5, although, as they point out, the proof idea does not seem to extend to c<=5.   We resolve the conjecture by proving that the problem is indeed NP-complete for c>=3 in 2D, even for axis-parallel X-rays. Our construction relies heavily on some structure results for the realizations of 0-1 matrices with given row and column sums.",
        "published": "1999-06-21T15:33:20Z",
        "link": "http://arxiv.org/abs/cs/9906018v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Reconstructing hv-Convex Polyominoes from Orthogonal Projections",
        "authors": [
            "Christoph Durr",
            "Marek Chrobak"
        ],
        "summary": "Tomography is the area of reconstructing objects from projections. Here we wish to reconstruct a set of cells in a two dimensional grid, given the number of cells in every row and column. The set is required to be an hv-convex polyomino, that is all its cells must be connected and the cells in every row and column must be consecutive. A simple, polynomial algorithm for reconstructing hv-convex polyominoes is provided, which is several orders of magnitudes faster than the best previously known algorithm from Barcucci et al. In addition, the problem of reconstructing a special class of centered hv-convex polyominoes is addressed. (An object is centered if it contains a row whose length equals the total width of the object). It is shown that in this case the reconstruction problem can be solved in linear time.",
        "published": "1999-06-22T09:56:53Z",
        "link": "http://arxiv.org/abs/cs/9906021v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "A decision procedure for well-formed linear quantum cellular automata",
        "authors": [
            "Christoph Durr",
            "Huong LeThanh",
            "Miklos Santha"
        ],
        "summary": "In this paper we introduce a new quantum computation model, the linear quantum cellular automaton. Well-formedness is an essential property for any quantum computing device since it enables us to define the probability of a configuration in an observation as the squared magnitude of its amplitude. We give an efficient algorithm which decides if a linear quantum cellular automaton is well-formed. The complexity of the algorithm is $O(n^2)$ in the algebraic model of computation if the input automaton has continuous neighborhood.",
        "published": "1999-06-23T10:48:10Z",
        "link": "http://arxiv.org/abs/cs/9906024v2",
        "categories": [
            "cs.DS",
            "cs.CC",
            "quant-ph",
            "F.1.1; F.2.1"
        ]
    },
    {
        "title": "Setting Parameters by Example",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We introduce a class of \"inverse parametric optimization\" problems, in which one is given both a parametric optimization problem and a desired optimal solution; the task is to determine parameter values that lead to the given solution. We describe algorithms for solving such problems for minimum spanning trees, shortest paths, and other \"optimal subgraph\" problems, and discuss applications in multicast routing, vehicle path planning, resource allocation, and board game programming.",
        "published": "1999-07-02T21:09:55Z",
        "link": "http://arxiv.org/abs/cs/9907001v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; I.2.6"
        ]
    },
    {
        "title": "Reducing Randomness via Irrational Numbers",
        "authors": [
            "Zhi-Zhong Chen",
            "Ming-Yang Kao"
        ],
        "summary": "We propose a general methodology for testing whether a given polynomial with integer coefficients is identically zero. The methodology evaluates the polynomial at efficiently computable approximations of suitable irrational points. In contrast to the classical technique of DeMillo, Lipton, Schwartz, and Zippel, this methodology can decrease the error probability by increasing the precision of the approximations instead of using more random bits. Consequently, randomized algorithms that use the classical technique can generally be improved using the new methodology. To demonstrate the methodology, we discuss two nontrivial applications. The first is to decide whether a graph has a perfect matching in parallel. Our new NC algorithm uses fewer random bits while doing less work than the previously best NC algorithm by Chari, Rohatgi, and Srinivasan. The second application is to test the equality of two multisets of integers. Our new algorithm improves upon the previously best algorithms by Blum and Kannan and can speed up their checking algorithm for sorting programs on a large range of inputs.",
        "published": "1999-07-07T23:39:27Z",
        "link": "http://arxiv.org/abs/cs/9907011v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.0; F.1.2; F.2.2; F.2.3; G2; G3"
        ]
    },
    {
        "title": "Linear-Time Approximation Algorithms for Computing Numerical Summation   with Provably Small Errors",
        "authors": [
            "Ming-Yang Kao",
            "Jie Wang"
        ],
        "summary": "Given a multiset $X=\\{x_1,..., x_n\\}$ of real numbers, the {\\it floating-point set summation} problem asks for $S_n=x_1+...+x_n$. Let $E^*_n$ denote the minimum worst-case error over all possible orderings of evaluating $S_n$. We prove that if $X$ has both positive and negative numbers, it is NP-hard to compute $S_n$ with the worst-case error equal to $E^*_n$. We then give the first known polynomial-time approximation algorithm that has a provably small error for arbitrary $X$. Our algorithm incurs a worst-case error at most $2(\\mix)E^*_n$.\\footnote{All logarithms $\\log$ in this paper are base 2.} After $X$ is sorted, it runs in O(n) time. For the case where $X$ is either all positive or all negative, we give another approximation algorithm with a worst-case error at most $\\lceil\\log\\log n\\rceil E^*_n$. Even for unsorted $X$, this algorithm runs in O(n) time. Previously, the best linear-time approximation algorithm had a worst-case error at most $\\lceil\\log n\\rceil E^*_n$, while $E^*_n$ was known to be attainable in $O(n \\log n)$ time using Huffman coding.",
        "published": "1999-07-09T18:23:17Z",
        "link": "http://arxiv.org/abs/cs/9907015v2",
        "categories": [
            "cs.DS",
            "cs.NA",
            "math.NA",
            "G.1; F.2"
        ]
    },
    {
        "title": "Map Graphs",
        "authors": [
            "Zhi-Zhong Chen",
            "Michelangelo Grigni",
            "Christos Papadimitriou"
        ],
        "summary": "We consider a modified notion of planarity, in which two nations of a map are considered adjacent when they share any point of their boundaries (not necessarily an edge, as planarity requires). Such adjacencies define a map graph. We give an NP characterization for such graphs, and a cubic time recognition algorithm for a restricted version: given a graph, decide whether it is realized by adjacencies in a map without holes, in which at most four nations meet at any point.",
        "published": "1999-10-13T21:41:19Z",
        "link": "http://arxiv.org/abs/cs/9910013v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.2; F.2.2"
        ]
    },
    {
        "title": "Subgraph Isomorphism in Planar Graphs and Related Problems",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We solve the subgraph isomorphism problem in planar graphs in linear time, for any pattern of constant size. Our results are based on a technique of partitioning the planar graph into pieces of small tree-width, and applying dynamic programming within each piece. The same methods can be used to solve other planar graph problems including connectivity, diameter, girth, induced subgraph isomorphism, and shortest paths.",
        "published": "1999-11-09T18:58:58Z",
        "link": "http://arxiv.org/abs/cs/9911003v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "The phase transition in random Horn satisfiability and its algorithmic   implications",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "Let c>0 be a constant, and $\\Phi$ be a random Horn formula with n variables and $m=c\\cdot 2^{n}$ clauses, chosen uniformly at random (with repetition) from the set of all nonempty Horn clauses in the given variables. By analyzing \\PUR, a natural implementation of positive unit resolution, we show that $\\lim_{n\\goesto \\infty} \\PR ({$\\Phi$ is satisfiable})= 1-F(e^{-c})$, where $F(x)=(1-x)(1-x^2)(1-x^4)(1-x^8)... $. Our method also yields as a byproduct an average-case analysis of this algorithm.",
        "published": "1999-12-01T22:04:47Z",
        "link": "http://arxiv.org/abs/cs/9912001v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2;I.1.2;G.3"
        ]
    },
    {
        "title": "Fast Hierarchical Clustering and Other Applications of Dynamic Closest   Pairs",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We develop data structures for dynamic closest pair problems with arbitrary distance functions, that do not necessarily come from any geometric structure on the objects. Based on a technique previously used by the author for Euclidean closest pairs, we show how to insert and delete objects from an n-object set, maintaining the closest pair, in O(n log^2 n) time per update and O(n) space. With quadratic space, we can instead use a quadtree-like structure to achieve an optimal time bound, O(n) per update. We apply these data structures to hierarchical clustering, greedy matching, and TSP heuristics, and discuss other potential applications in machine learning, Groebner bases, and local improvement algorithms for partition and placement problems. Experiments show our new methods to be faster in practice than previously used heuristics.",
        "published": "1999-12-22T01:42:51Z",
        "link": "http://arxiv.org/abs/cs/9912014v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Additive models in high dimensions",
        "authors": [
            "Markus Hegland",
            "Vladimir Pestov"
        ],
        "summary": "We discuss some aspects of approximating functions on high-dimensional data sets with additive functions or ANOVA decompositions, that is, sums of functions depending on fewer variables each. It is seen that under appropriate smoothness conditions, the errors of the ANOVA decompositions are of order $O(n^{m/2})$ for approximations using sums of functions of up to $m$ variables under some mild restrictions on the (possibly dependent) predictor variables. Several simulated examples illustrate this behaviour.",
        "published": "1999-12-30T07:50:11Z",
        "link": "http://arxiv.org/abs/cs/9912020v2",
        "categories": [
            "cs.DS",
            "H.2.8"
        ]
    },
    {
        "title": "Competition and cooperation: Libraries and publishers in the transition   to electronic scholarly journals",
        "authors": [
            "Andrew Odlyzko"
        ],
        "summary": "The conversion of scholarly journals to digital format is proceeding rapidly, especially for those from large commercial and learned society publishers. This conversion offers the best hope for survival for such publishers. The infamous \"journal crisis\" is more of a library cost crisis than a publisher pricing problem, with internal library costs much higher than the amount spent on purchasing books and journals. Therefore publishers may be able to retain or even increase their revenues and profits, while at the same time providing a superior service. To do this, they will have to take over many of the function of libraries, and they can do that only in the digital domain. This paper examines publishers' strategies, how they are likely to evolve, and how they will affect libraries.",
        "published": "1999-01-20T13:01:09Z",
        "link": "http://arxiv.org/abs/cs/9901009v1",
        "categories": [
            "cs.DL",
            "H.3.7; K.4.1; K.4.4"
        ]
    },
    {
        "title": "Automatic Identification of Subjects for Textual Documents in Digital   Libraries",
        "authors": [
            "Kuang-hua Chen"
        ],
        "summary": "The amount of electronic documents in the Internet grows very quickly. How to effectively identify subjects for documents becomes an important issue. In past, the researches focus on the behavior of nouns in documents. Although subjects are composed of nouns, the constituents that determine which nouns are subjects are not only nouns. Based on the assumption that texts are well-organized and event-driven, nouns and verbs together contribute the process of subject identification. This paper considers four factors: 1) word importance, 2) word frequency, 3) word co-occurrence, and 4) word distance and proposes a model to identify subjects for textual documents. The preliminary experiments show that the performance of the proposed model is close to that of human beings.",
        "published": "1999-02-01T11:01:23Z",
        "link": "http://arxiv.org/abs/cs/9902002v1",
        "categories": [
            "cs.DL",
            "cs.CL",
            "H.3.1; H.3.7; I.2.7"
        ]
    },
    {
        "title": "MyLibrary: A Model for Implementing a User-centered, Customizable   Interface to a Library's Collection of Information Resources",
        "authors": [
            "Eric Lease Morgan"
        ],
        "summary": "The paper describes an extensible model for implementing a user-centered, customizable interface to a library's collection of information resources. This model, called MyLibrary, integrates the principles of librarianship (collection, organization, dissemination, and evaluation) with globally networked computing resources creating a dynamic, customer-driven front-end to any library's set of materials. The model supports a framework for libraries to provide enhanced access to local and remote sets of data, information, and knowledge. At the same, the model does not overwhelm its users with too much information because the users control exactly how much information is displayed to them at any given time. The model is active and not passive; direct human interaction, computer mediated guidance and communication technologies, as well as current awareness services all play indispensable roles in this system.",
        "published": "1999-02-02T13:41:56Z",
        "link": "http://arxiv.org/abs/cs/9902003v1",
        "categories": [
            "cs.DL",
            "H.1; H.5.2; H.5.3; J.1"
        ]
    },
    {
        "title": "The Alex Catalogue, A Collection of Digital Texts with Automatic Methods   for Acquisition and Cataloging, User-Defined Typography, Cross-searching of   Indexed Content, and a Sense of Community",
        "authors": [
            "Eric Lease Morgan"
        ],
        "summary": "This paper describes the Alex Catalogue of Electronic Texts, the only Internet-accessible collection of digital documents allowing the user to 1) dynamically create customized, typographically readable documents on demand, 2) search the content of one or more documents from the collection simultaneously, 3) create sets of documents from the collection for review and annotation, and 4) publish these sets of annotated documents in turn fostering a sense of community around the Catalogue. More than a just a collection of links that will break over time, Alex is an archive of electronic texts providing unprecedented access to its content and features allowing it to meet the needs of a wide variety of users and settings. Furthermore, the process of maintaining the Catalogue is streamlined with tools for automatic acquisition and cataloging making it possible to sustain the service with a minimum of personnel.",
        "published": "1999-02-02T14:14:42Z",
        "link": "http://arxiv.org/abs/cs/9902004v1",
        "categories": [
            "cs.DL",
            "H.3; H.5.2; J.1"
        ]
    },
    {
        "title": "KEA: Practical Automatic Keyphrase Extraction",
        "authors": [
            "Ian H. Witten",
            "Gordon W. Paynter",
            "Eibe Frank",
            "Carl Gutwin",
            "Craig G. Nevill-Manning"
        ],
        "summary": "Keyphrases provide semantic metadata that summarize and characterize documents. This paper describes Kea, an algorithm for automatically extracting keyphrases from text. Kea identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machine-learning algorithm to predict which candidates are good keyphrases. The machine learning scheme first builds a prediction model using training documents with known keyphrases, and then uses the model to find keyphrases in new documents. We use a large test corpus to evaluate Kea's effectiveness in terms of how many author-assigned keyphrases are correctly identified. The system is simple, robust, and publicly available.",
        "published": "1999-02-05T03:15:45Z",
        "link": "http://arxiv.org/abs/cs/9902007v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Quality of OCR for Degraded Text Images",
        "authors": [
            "Roger T. Hartley",
            "Kathleen Crumpton"
        ],
        "summary": "Commercial OCR packages work best with high-quality scanned images. They often produce poor results when the image is degraded, either because the original itself was poor quality, or because of excessive photocopying. The ability to predict the word failure rate of OCR from a statistical analysis of the image can help in making decisions in the trade-off between the success rate of OCR and the cost of human correction of errors. This paper describes an investigation of OCR of degraded text images using a standard OCR engine (Adobe Capture). The documents were selected from those in the archive at Los Alamos National Laboratory. By introducing noise in a controlled manner into perfect documents, we show how the quality of OCR can be predicted from the nature of the noise. The preliminary results show that a simple noise model can give good prediction of the number of OCR errors.",
        "published": "1999-02-05T20:52:05Z",
        "link": "http://arxiv.org/abs/cs/9902009v1",
        "categories": [
            "cs.DL",
            "I.7.5"
        ]
    },
    {
        "title": "Content-Based Book Recommending Using Learning for Text Categorization",
        "authors": [
            "Raymond J. Mooney",
            "Loriene Roy"
        ],
        "summary": "Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes. Most existing recommender systems use social filtering methods that base recommendations on other users' preferences. By contrast, content-based methods use information about an item itself to make suggestions. This approach has the advantage of being able to recommended previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.",
        "published": "1999-02-07T20:03:20Z",
        "link": "http://arxiv.org/abs/cs/9902011v1",
        "categories": [
            "cs.DL",
            "H.3.7; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Digital Library Technology for Locating and Accessing Scientific Data",
        "authors": [
            "Robert E. McGrath",
            "Joe Futrelle",
            "Ray Plante",
            "Damien Guillaume"
        ],
        "summary": "In this paper we describe our efforts to bring scientific data into the digital library. This has required extension of the standard WWW, and also the extension of metadata standards far beyond the Dublin Core. Our system demonstrates this technology for real scientific data from astronomy.",
        "published": "1999-02-07T20:06:01Z",
        "link": "http://arxiv.org/abs/cs/9902012v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Resource Discovery in Trilogy",
        "authors": [
            "Franck Chevalier",
            "David Harle",
            "Geoffrey Smith"
        ],
        "summary": "Trilogy is a collaborative project whose key aim is the development of an integrated virtual laboratory to support research training within each institution and collaborative projects between the partners. In this paper, the architecture and underpinning platform of the system is described with particular emphasis being placed on the structure and the integration of the distributed database. A key element is the ontology that provides the multi-agent system with a conceptualisation specification of the domain; this ontology is explained, accompanied by a discussion how such a system is integrated and used within the virtual laboratory. Although in this paper, Telecommunications and in particular Broadband networks are used as exemplars, the underlying system principles are applicable to any domain where a combination of experimental and literature-based resources are required.",
        "published": "1999-02-08T21:23:39Z",
        "link": "http://arxiv.org/abs/cs/9902015v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.MA",
            "H.3.4;I.2.0"
        ]
    },
    {
        "title": "Use and usability in a digital library search system",
        "authors": [
            "Robert K. France",
            "Lucy Terry Nowell",
            "Edward A. Fox",
            "Rani A. Saad",
            "Jianxin Zhao"
        ],
        "summary": "Digital libraries must reach out to users from all walks of life, serving information needs at all levels. To do this, they must attain high standards of usability over an extremely broad audience. This paper details the evolution of one important digital library component as it has grown in functionality and usefulness over several years of use by a live, unrestricted community. Central to its evolution have been user studies, analysis of use patterns, and formative usability evaluation. We extrapolate that all three components are necessary in the production of successful digital library systems.",
        "published": "1999-02-08T23:08:11Z",
        "link": "http://arxiv.org/abs/cs/9902013v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Multimedia Description Framework (MDF) for Content Description of   Audio/Video Documents",
        "authors": [
            "Michael J. Hu",
            "Ye Jian"
        ],
        "summary": "MPEG is undertaking a new initiative to standardize content description of audio and video data/documents. When it is finalized in 2001, MPEG-7 is expected to provide standardized description schemes for concise and unambiguous content description of data/documents of complex media types. Meanwhile, other meta-data or description schemes, such as Dublin Core, XML, etc., are becoming popular in different application domains. In this paper, we propose the Multimedia Description Framework (MDF), which is designated to accommodate multiple description (meta-data) schemes, both MPEG-7 and non-MPEG-7, into integrated architecture. We will use examples to show how MDF description makes use of combined strength of different description schemes to enhance its expression power and flexibility. We conclude the paper with discussion of using MDF description of a movie video to search/retrieve required scene clips from the movie, on the MDF prototype system we have implemented.",
        "published": "1999-02-09T02:17:19Z",
        "link": "http://arxiv.org/abs/cs/9902016v1",
        "categories": [
            "cs.DL",
            "H3.3; H3.7"
        ]
    },
    {
        "title": "ZBroker: A Query Routing Broker for Z39.50 Databases",
        "authors": [
            "Yong Lin",
            "Jian Xu",
            "Ee-Peng Lim",
            "Wee-Keong Ng"
        ],
        "summary": "A query routing broker is a software agent that determines from a large set of accessing information sources the ones most relevant to a user's information need. As the number of information sources on the Internet increases dramatically, future users will have to rely on query routing brokers to decide a small number of information sources to query without incurring too much query processing overheads. In this paper, we describe a query routing broker known as ZBroker developed for bibliographic database servers that support the Z39.50 protocol. ZBroker samples the content of each bibliographic database by using training queries and their results, and summarizes the bibliographic database content into a knowledge base. We present the design and implementation of ZBroker and describe its Web-based user interface.",
        "published": "1999-02-09T03:50:24Z",
        "link": "http://arxiv.org/abs/cs/9902018v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H3.3"
        ]
    },
    {
        "title": "Not Available",
        "authors": [
            "Not Available"
        ],
        "summary": "withdrawn by author",
        "published": "1999-02-09T04:02:55Z",
        "link": "http://arxiv.org/abs/cs/9902017v2",
        "categories": [
            "cs.DL",
            "cs.DB",
            "Not Available"
        ]
    },
    {
        "title": "Multimodal Surrogates for Video Browsing",
        "authors": [
            "Wei Ding",
            "Gary Marchionini",
            "Dagobert Soergel"
        ],
        "summary": "Three types of video surrogates - visual (keyframes), verbal (keywords/phrases), and combination of the two - were designed and studied in a qualitative investigation of user cognitive processes. The results favor the combined surrogates in which verbal information and images reinforce each other, lead to better comprehension, and may actually require less processing time. The results also highlight image features users found most helpful. These findings will inform the interface design and video representation for video retrieval and browsing.",
        "published": "1999-02-09T04:56:59Z",
        "link": "http://arxiv.org/abs/cs/9902019v1",
        "categories": [
            "cs.DL",
            "cs.HC",
            "H5.1; H3.1"
        ]
    },
    {
        "title": "Using Query Mediators for Distributed Searching in Federated Digital   Libraries",
        "authors": [
            "Naomi Dushay",
            "James C. French",
            "Carl Lagoze"
        ],
        "summary": "We describe an architecture and investigate the characteristics of distributed searching in federated digital libraries. We introduce the notion of a query mediator as a digital library service responsible for selecting among available search engines, routing queries to those search engines, and aggregating results. We examine operational data from the NCSTRL distributed digital library that reveals a number of characteristics of distributed resource discovery. These include availability and response time of indexers and the distinction between the query mediator view of these characteristics and the indexer view.",
        "published": "1999-02-09T05:18:56Z",
        "link": "http://arxiv.org/abs/cs/9902020v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Visualization of Retrieved Documents using a Presentation Server",
        "authors": [
            "Sa-Kwang Song",
            "Sung Hyon Myaeng"
        ],
        "summary": "In any search-based digital library (DL) systems dealing with a non-trivial number of documents, users are often required to go through a long list of short document descriptions in order to identify what they are looking for. To tackle the problem, a variety of document organization algorithms and/or visualization techniques have been used to guide users in selecting relevant documents. Since these techniques require heavy computations, however, we developed a presentation server designed to serve as an intermediary between retrieval servers and clients equipped with a visualization interface. In addition, we designed our own visual interface by which users can view a set of documents from different perspectives through layers of document maps. We finally ran experiments to show that the visual interface, in conjunction with the presentation server, indeed helps users in selecting relevant documents from the retrieval results.",
        "published": "1999-02-10T09:30:59Z",
        "link": "http://arxiv.org/abs/cs/9902021v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "H.3.3; H.3.4"
        ]
    },
    {
        "title": "Semi-Automatic Indexing of Multilingual Documents",
        "authors": [
            "Ulrich Schiel",
            "Ianna M. Sodre Ferreira de Souza",
            "Edberto Ferneda"
        ],
        "summary": "With the growing significance of digital libraries and the Internet, more and more electronic texts become accessible to a wide and geographically disperse public. This requires adequate tools to facilitate indexing, storage, and retrieval of documents written in different languages. We present a method for semi-automatic indexing of electronic documents and construction of a multilingual thesaurus, which can be used for query formulation and information retrieval. We use special dictionaries and user interaction in order to solve ambiguities and find adequate canonical terms in the language and adequate abstract language-independent terms. The abstract thesaurus is updated incrementally by new indexed documents and is used to search document concerning terms in a query to the document base.",
        "published": "1999-02-11T00:19:25Z",
        "link": "http://arxiv.org/abs/cs/9902022v1",
        "categories": [
            "cs.DL",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "A New Ranking Principle for Multimedia Information Retrieval",
        "authors": [
            "Martin Wechsler",
            "Peter Schauble"
        ],
        "summary": "A theoretic framework for multimedia information retrieval is introduced which guarantees optimal retrieval effectiveness. In particular, a Ranking Principle for Distributed Multimedia-Documents (RPDM) is described together with an algorithm that satisfies this principle. Finally, the RPDM is shown to be a generalization of the Probability Ranking principle (PRP) which guarantees optimal retrieval effectiveness in the case of text document retrieval. The PRP justifies theoretically the relevance ranking adopted by modern search engines. In contrast to the classical PRP, the new RPDM takes into account transmission and inspection time, and most importantly, aspectual recall rather than simple recall.",
        "published": "1999-02-11T12:33:12Z",
        "link": "http://arxiv.org/abs/cs/9902023v1",
        "categories": [
            "cs.DL",
            "H.3.3;H.3.4;H.3.7;H.2.4;C.2.4"
        ]
    },
    {
        "title": "A Proposal for the Establishment of Review Boards - a flexible approach   to the selection of academic knowledge",
        "authors": [
            "Bruce Edmonds"
        ],
        "summary": "Paper journals use a small number of trusted academics to select information on behalf of all their readers. This inflexibility in the selection was justified due to the expense of publishing. The advent of cheap distribution via the internet allows a new trade-off between time and expense and the flexibility of the selection process. This paper explores one such possible process one where the role of mark-up and archiving is separated from that of review. The idea is that authors publish their papers on their own web pages or in a public paper archive, a board of reviewers judge that paper on a number of different criteria. The detailed results of the reviews are stored in such a way as to enable readers to use these judgements to find the papers they want using search engines on the web. Thus instead of journals using generic selection criteria readers can set their own to suit their needs. The resulting system might be even cheaper than web-journals to implement.",
        "published": "1999-04-01T07:54:36Z",
        "link": "http://arxiv.org/abs/cs/9904001v1",
        "categories": [
            "cs.CY",
            "cs.DL",
            "cs.IR",
            "K.4.3; K.3.m;H.5.3"
        ]
    },
    {
        "title": "Designing and Mining Multi-Terabyte Astronomy Archives: The Sloan   Digital Sky Survey",
        "authors": [
            "Alexander S. Szalay",
            "Peter Kunszt",
            "Ani Thakar",
            "Jim Gray"
        ],
        "summary": "The next-generation astronomy digital archives will cover most of the universe at fine resolution in many wave-lengths, from X-rays to ultraviolet, optical, and infrared. The archives will be stored at diverse geographical locations. One of the first of these projects, the Sloan Digital Sky Survey (SDSS) will create a 5-wavelength catalog over 10,000 square degrees of the sky (see http://www.sdss.org/). The 200 million objects in the multi-terabyte database will have mostly numerical attributes, defining a space of 100+ dimensions. Points in this space have highly correlated distributions.   The archive will enable astronomers to explore the data interactively. Data access will be aided by a multidimensional spatial index and other indices. The data will be partitioned in many ways. Small tag objects consisting of the most popular attributes speed up frequent searches. Splitting the data among multiple servers enables parallel, scalable I/O and applies parallel processing to the data. Hashing techniques allow efficient clustering and pair-wise comparison algorithms that parallelize nicely. Randomly sampled subsets allow debugging otherwise large queries at the desktop. Central servers will operate a data pump that supports sweeping searches that touch most of the data. The anticipated queries require special operators related to angular distances and complex similarity tests of object properties, like shapes, colors, velocity vectors, or temporal behaviors. These issues pose interesting data management challenges.",
        "published": "1999-07-06T22:56:47Z",
        "link": "http://arxiv.org/abs/cs/9907009v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2, H.2.8,H.3.5, h.3.7"
        ]
    },
    {
        "title": "Microsoft TerraServer: A Spatial Data Warehouse",
        "authors": [
            "Tom Barclay Jim Gray Don Slutz"
        ],
        "summary": "The TerraServer stores aerial, satellite, and topographic images of the earth in a SQL database available via the Internet. It is the world's largest online atlas, combining five terabytes of image data from the United States Geological Survey (USGS) and SPIN-2. This report describes the system-redesign based on our experience over the last year. It also reports usage and operations results over the last year -- over 2 billion web hits and over 20 Terabytes of imagry served over the Internet. Internet browsers provide intuitive spatial and text interfaces to the data. Users need no special hardware, software, or knowledge to locate and browse imagery. This paper describes how terabytes of \"Internet unfriendly\" geo-spatial images were scrubbed and edited into hundreds of millions of \"Internet friendly\" image tiles and loaded into a SQL data warehouse. Microsoft TerraServer demonstrates that general-purpose relational database technology can manage large scale image repositories, and shows that web browsers can be a good geospatial image presentation system.",
        "published": "1999-07-09T21:30:11Z",
        "link": "http://arxiv.org/abs/cs/9907016v1",
        "categories": [
            "cs.DB",
            "cs.DL",
            "H.2, H.2.4, H.2.8, H.3.5, H.5.1,J.2"
        ]
    },
    {
        "title": "Raising Reliability of Web Search Tool Research through Replication and   Chaos Theory",
        "authors": [
            "Scott Nicholson"
        ],
        "summary": "Because the World Wide Web is a dynamic collection of information, the Web search tools (or \"search engines\") that index the Web are dynamic. Traditional information retrieval evaluation techniques may not provide reliable results when applied to the Web search tools. This study is the result of ten replications of the classic 1996 Ding and Marchionini Web search tool research. It explores the effects that replication can have on transforming unreliable results from one iteration into replicable and therefore reliable results after multiple iterations.",
        "published": "1999-07-27T16:42:18Z",
        "link": "http://arxiv.org/abs/cs/9907042v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.4; H.3.5"
        ]
    },
    {
        "title": "Safe Deals Between Strangers",
        "authors": [
            "H. M. Gladney"
        ],
        "summary": "E-business, information serving, and ubiquitous computing will create heavy request traffic from strangers or even incognitos. Such requests must be managed automatically. Two ways of doing this are well known: giving every incognito consumer the same treatment, and rendering service in return for money. However, different behavior will be often wanted, e.g., for a university library with different access policies for undergraduates, graduate students, faculty, alumni, citizens of the same state, and everyone else.   For a data or process server contacted by client machines on behalf of users not previously known, we show how to provide reliable automatic access administration conforming to service agreements. Implementations scale well from very small collections of consumers and producers to immense client/server networks. Servers can deliver information, effect state changes, and control external equipment.   Consumer privacy is easily addressed by the same protocol. We support consumer privacy, but allow servers to deny their resources to incognitos. A protocol variant even protects against statistical attacks by consortia of service organizations.   One e-commerce application would put the consumer's tokens on a smart card whose readers are in vending kiosks. In e-business we can simplify supply chain administration. Our method can also be used in sensitive networks without introducing new security loopholes.",
        "published": "1999-08-17T01:09:54Z",
        "link": "http://arxiv.org/abs/cs/9908012v1",
        "categories": [
            "cs.CR",
            "cs.DL",
            "D.4.6; H.2.7; K.6.5"
        ]
    },
    {
        "title": "Representing Scholarly Claims in Internet Digital Libraries: A Knowledge   Modelling Approach",
        "authors": [
            "Simon Buckingham Shum",
            "Enrico Motta",
            "John Domingue"
        ],
        "summary": "This paper is concerned with tracking and interpreting scholarly documents in distributed research communities. We argue that current approaches to document description, and current technological infrastructures particularly over the World Wide Web, provide poor support for these tasks. We describe the design of a digital library server which will enable authors to submit a summary of the contributions they claim their documents makes, and its relations to the literature. We describe a knowledge-based Web environment to support the emergence of such a community-constructed semantic hypertext, and the services it could provide to assist the interpretation of an idea or document in the context of its literature. The discussion considers in detail how the approach addresses usability issues associated with knowledge structuring environments.",
        "published": "1999-08-19T09:51:29Z",
        "link": "http://arxiv.org/abs/cs/9908015v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.3.7; H.1.2; H5.2; H.5.4; I.2.4; I.7.4"
        ]
    },
    {
        "title": "A Geometric Model for Information Retrieval Systems",
        "authors": [
            "Myung Ho Kim"
        ],
        "summary": "This decade has seen a great deal of progress in the development of information retrieval systems. Unfortunately, we still lack a systematic understanding of the behavior of the systems and their relationship with documents. In this paper we present a completely new approach towards the understanding of the information retrieval systems. Recently, it has been observed that retrieval systems in TREC 6 show some remarkable patterns in retrieving relevant documents. Based on the TREC 6 observations, we introduce a geometric linear model of information retrieval systems. We then apply the model to predict the number of relevant documents by the retrieval systems. The model is also scalable to a much larger data set. Although the model is developed based on the TREC 6 routing test data, I believe it can be readily applicable to other information retrieval systems. In Appendix, we explained a simple and efficient way of making a better system from the existing systems.",
        "published": "1999-12-06T03:04:43Z",
        "link": "http://arxiv.org/abs/cs/9912002v1",
        "categories": [
            "cs.IR",
            "cs.CC",
            "cs.DL",
            "H.1.1"
        ]
    },
    {
        "title": "Managing Object-Oriented Integration and Regression Testing",
        "authors": [
            "Mario Winter"
        ],
        "summary": "Systematic testing of object-oriented software turned out to be much more complex than testing conventional software. Especially the highly incremental and iterative development cycle demands both many more changes and partially implemented resp. re-implemented classes. Much more integration and regression testing has to be done to reach stable stages during the development. In this presentation we propose a diagram capturing all possible dependencies and interactions in an object-oriented program. Then we give algorithms and coverage criteria to identify integration resp. regression test strategys and all test cases to be executed after some implementation resp. modification activities. Finally, we summarize some practical experiences and heuristics.",
        "published": "1999-02-05T12:01:56Z",
        "link": "http://arxiv.org/abs/cs/9902008v1",
        "categories": [
            "cs.SE",
            "D.2.5; D.2.9"
        ]
    },
    {
        "title": "An Algebraic Programming Style for Numerical Software and its   Optimization",
        "authors": [
            "T. B. Dinesh",
            "M. Haveraaen",
            "J. Heering"
        ],
        "summary": "The abstract mathematical theory of partial differential equations (PDEs) is formulated in terms of manifolds, scalar fields, tensors, and the like, but these algebraic structures are hardly recognizable in actual PDE solvers. The general aim of the Sophus programming style is to bridge the gap between theory and practice in the domain of PDE solvers. Its main ingredients are a library of abstract datatypes corresponding to the algebraic structures used in the mathematical theory and an algebraic expression style similar to the expression style used in the mathematical theory. Because of its emphasis on abstract datatypes, Sophus is most naturally combined with object-oriented languages or other languages supporting abstract datatypes. The resulting source code patterns are beyond the scope of current compiler optimizations, but are sufficiently specific for a dedicated source-to-source optimizer. The limited, domain-specific, character of Sophus is the key to success here. This kind of optimization has been tested on computationally intensive Sophus style code with promising results. The general approach may be useful for other styles and in other application domains as well.",
        "published": "1999-03-01T11:03:47Z",
        "link": "http://arxiv.org/abs/cs/9903002v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "D.1.5; D.2.2; J.2"
        ]
    },
    {
        "title": "LuaJava - A Scripting Tool for Java",
        "authors": [
            "Carlos Cassino",
            "Roberto Ierusalimschy",
            "Noemi Rodriguez"
        ],
        "summary": "Scripting languages are becoming more and more important as a tool for software development, as they provide great flexibility for rapid prototyping and for configuring componentware applications. In this paper we present LuaJava, a scripting tool for Java. LuaJava adopts Lua, a dynamically typed interpreted language, as its script language. Great emphasis is given to the transparency of the integration between the two languages, so that objects from one language can be used inside the other like native objects. The final result of this integration is a tool that allows the construction of configurable Java applications, using off-the-shelf components, in a high abstraction level.",
        "published": "1999-03-30T11:28:44Z",
        "link": "http://arxiv.org/abs/cs/9903018v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "A Machine-Independent Debugger--Revisited",
        "authors": [
            "David R. Hanson"
        ],
        "summary": "Most debuggers are notoriously machine-dependent, but some recent research prototypes achieve varying degrees of machine-independence with novel designs. Cdb, a simple source-level debugger for C, is completely independent of its target architecture. This independence is achieved by embedding symbol tables and debugging code in the target program, which costs both time and space. This paper describes a revised design and implementation of cdb that reduces the space cost by nearly one-half and the time cost by 13% by storing symbol tables in external files. A symbol table is defined by a 31-line grammar in the Abstract Syntax Description Language (ASDL). ASDL is a domain-specific language for specifying tree data structures. The ASDL tools accept an ASDL grammar and generate code to construct, read, and write these data structures. Using ASDL automates implementing parts of the debugger, and the grammar documents the symbol table concisely. Using ASDL also suggested simplifications to the interface between the debugger and the target program. Perhaps most important, ASDL emphasizes that symbol tables are data structures, not file formats. Many of the pitfalls of working with low-level file formats can be avoided by focusing instead on high-level data structures and automating the implementation details.",
        "published": "1999-04-23T18:34:04Z",
        "link": "http://arxiv.org/abs/cs/9904017v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.3.4"
        ]
    },
    {
        "title": "Events in Property Patterns",
        "authors": [
            "M. Chechik",
            "D. Paun"
        ],
        "summary": "A pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification was proposed by Dwyer and his collegues. The patterns enable non-experts to read and write formal specifications for realistic systems and facilitate easy conversion of specifications between formalisms, such as LTL, CTL, QRE. In this paper, we extend the pattern system with events - changes of values of variables in the context of LTL.",
        "published": "1999-06-28T17:06:51Z",
        "link": "http://arxiv.org/abs/cs/9906029v2",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CL",
            "cs.SC",
            "D.2.4;F.3.1;F.4.1;I.2.4;D.2.1"
        ]
    },
    {
        "title": "SCR3: towards usability of formal methods",
        "authors": [
            "M. Chechik"
        ],
        "summary": "This paper gives an overview of SCR3 -- a toolset designed to increase the usability of formal methods for software development. Formal requirements are specified in SCR3 in an easy to use and review format, and then used in checking requirements for correctness and in verifying consistency between annotated code and requirements.   In this paper we discuss motivations behind this work, describe several tools which are part of SCR3, and illustrate their operation on an example of a Cruise Control system.",
        "published": "1999-06-28T17:30:59Z",
        "link": "http://arxiv.org/abs/cs/9906030v1",
        "categories": [
            "cs.SE",
            "D.2.4;D.2.6;D.2.2;D.2.1"
        ]
    },
    {
        "title": "Events in Linear-Time Properties",
        "authors": [
            "D. Paun",
            "M. Chechik"
        ],
        "summary": "For over a decade, researchers in formal methods tried to create formalisms that permit natural specification of systems and allow mathematical reasoning about their correctness. The availability of fully-automated reasoning tools enables more non-specialists to use formal methods effectively --- their responsibility reduces to just specifying the model and expressing the desired properties. Thus, it is essential that these properties be represented in a language that is easy to use and sufficiently expressive.   Linear-time temporal logic is a formalism that has been extensively used by researchers for specifying properties of systems. When such properties are closed under stuttering, i.e. their interpretation is not modified by transitions that leave the system in the same state, verification tools can utilize a partial-order reduction technique to reduce the size of the model and thus analyze larger systems. If LTL formulas do not contain the ``next'' operator, the formulas are closed under stuttering, but the resulting language is not expressive enough to capture many important properties, e.g., properties involving events. Determining if an arbitrary LTL formula is closed under stuttering is hard --- it has been proven to be PSPACE-complete.   In this paper we relax the restriction on LTL that guarantees closure under stuttering, introduce the notion of edges in the context of LTL, and provide theorems that enable syntactic reasoning about closure under stuttering of LTL formulas.",
        "published": "1999-06-28T18:06:27Z",
        "link": "http://arxiv.org/abs/cs/9906031v1",
        "categories": [
            "cs.SE",
            "D.2.4;F.3.1;F.4.1;I.2.4;D.2.1"
        ]
    },
    {
        "title": "Formal Modeling in a Commercial Setting: A Case Study",
        "authors": [
            "A. Wong",
            "M. Chechik"
        ],
        "summary": "This paper describes a case study conducted in collaboration with Nortel to demonstrate the feasibility of applying formal modeling techniques to telecommunication systems. A formal description language, SDL, was chosen by our qualitative CASE tool evaluation to model a multimedia-messaging system described by an 80-page natural language specification. Our model was used to identify errors in the software requirements document and to derive test suites, shadowing the existing development process and keeping track of a variety of productivity data.",
        "published": "1999-06-29T15:29:42Z",
        "link": "http://arxiv.org/abs/cs/9906032v1",
        "categories": [
            "cs.SE",
            "F.3.1;K.6.3;D.2.7;D.2.5;D.2.1;C.3;D.2.4"
        ]
    },
    {
        "title": "A Reasonable C++ Wrappered Java Native Interface",
        "authors": [
            "Craig Bordelon"
        ],
        "summary": "A reasonable C++ Java Native Interface (JNI) technique termed C++ Wrappered JNI (C++WJ) is presented. The technique simplifies current error-prone JNI development by wrappering JNI calls. Provided development is done with the aid of a C++ compiler, C++WJ offers type checking and behind the scenes caching. A tool (jH) patterned on javah automates the creation of C++WJ classes.   The paper presents the rationale behind the choices that led to C++WJ. Handling of Java class and interface hierarchy including Java type downcasts is discussed. Efficiency considerations in the C++WJ lead to two flavors of C++ classes: jtypes and Jtypes. A jtype is a lightweight less than full wrapper of a JNI object reference. A Jtype is a heavyweight full wrapper of a JNI object reference.",
        "published": "1999-07-12T13:34:21Z",
        "link": "http://arxiv.org/abs/cs/9907019v1",
        "categories": [
            "cs.SE",
            "D.2.3"
        ]
    },
    {
        "title": "Computation in an algebra of test selection criteria",
        "authors": [
            "Jan Pachl",
            "Shmuel Zaks"
        ],
        "summary": "One of the key concepts in testing is that of adequate test sets. A test selection criterion decides which test sets are adequate. In this paper, a language schema for specifying a large class of test selection criteria is developed; the schema is based on two operations for building complex criteria from simple ones. Basic algebraic properties of the two operations are derived.   In the second part of the paper, a simple language-an instance of the general schema-is studied in detail, with the goal of generating small adequate test sets automatically. It is shown that one version of the problem is intractable, while another is solvable by an efficient algorithm. An implementation of the algorithm is described.",
        "published": "1999-12-24T15:59:23Z",
        "link": "http://arxiv.org/abs/cs/9912018v1",
        "categories": [
            "cs.SE",
            "D.2.5; B.8.1"
        ]
    },
    {
        "title": "Fast Computational Algorithms for the Discrete Wavelet Transform and   Applications of Localized Orthonormal Bases in Signal Classification",
        "authors": [
            "Eirik Fossgaard"
        ],
        "summary": "We construct an algorithm for implementing the discrete wavelet transform by means of matrices in SO_2(R) for orthonormal compactly supported wavelets and matrices in SL_m(R), m > = 2, for compactly supported biorthogonal wavelets. We show that in 1 dimension the total operation count using this algorithm can be reduced to about 50% of the conventional convolution and downsampling by 2-operation for both orthonormal and biorthogonal filters. In the special case of biorthogonal symmetric odd-odd filters, we show an implementation yielding a total operation count of about 38% of the conventional method. In 2 dimensions we show an implementation of this algorithm yielding a reduction in the total operation count of about 70% when the filters are orthonormal, a reduction of about 62% for general biorthogonal filters, and a reduction of about 70% if the filters are symmetric odd-odd length filters. We further extend these results to 3 dimensions. We also show how the SO_2(R)-method for implementing the discrete wavelet transform may be exploited to compute short FIR filters, and we construct edge mappings where we try to improve upon the degree of preservation of regularity in the conventional methods. We also consider a two-class waveform discrimination problem. A statistical space-frequency analysis is performed on a training data set using the LDB-algorithm of N.Saito and R.Coifman. The success of the algorithm on this particular problem is evaluated on a disjoint test data set.",
        "published": "1999-01-16T16:54:01Z",
        "link": "http://arxiv.org/abs/cs/9901008v1",
        "categories": [
            "cs.MS",
            "cs.CE",
            "F.2.1; G.4; I.5.4"
        ]
    },
    {
        "title": "Algorithms of Two-Level Parallelization for DSMC of Unsteady Flows in   Molecular Gasdynamics",
        "authors": [
            "Alexander V. Bogdanov",
            "Nick Yu. Bykov",
            "Igor A. Grishin",
            "Gregory O. Khanlarov",
            "German A. Lukianov",
            "Vladimir V. Zakharov"
        ],
        "summary": "The general scheme of two-level parallelization (TLP) for direct simulation Monte Carlo of unsteady gas flows on shared memory multiprocessor computers has been described. The high efficient algorithm of parallel independent runs is used on the first level. The data parallelization is employed for the second one. Two versions of TLP algorithm are elaborated with static and dynamic load balancing. The method of dynamic processor reallocation is used for dynamic load balancing. Two gasdynamic unsteady problems were used to study speedup and efficiency of the algorithms. The conditions of efficient application field for the algorithms have been determined.",
        "published": "1999-02-11T13:13:04Z",
        "link": "http://arxiv.org/abs/cs/9902024v1",
        "categories": [
            "cs.CE",
            "cs.PF",
            "G.1.0;G.3;I.6.8"
        ]
    },
    {
        "title": "An Algebraic Programming Style for Numerical Software and its   Optimization",
        "authors": [
            "T. B. Dinesh",
            "M. Haveraaen",
            "J. Heering"
        ],
        "summary": "The abstract mathematical theory of partial differential equations (PDEs) is formulated in terms of manifolds, scalar fields, tensors, and the like, but these algebraic structures are hardly recognizable in actual PDE solvers. The general aim of the Sophus programming style is to bridge the gap between theory and practice in the domain of PDE solvers. Its main ingredients are a library of abstract datatypes corresponding to the algebraic structures used in the mathematical theory and an algebraic expression style similar to the expression style used in the mathematical theory. Because of its emphasis on abstract datatypes, Sophus is most naturally combined with object-oriented languages or other languages supporting abstract datatypes. The resulting source code patterns are beyond the scope of current compiler optimizations, but are sufficiently specific for a dedicated source-to-source optimizer. The limited, domain-specific, character of Sophus is the key to success here. This kind of optimization has been tested on computationally intensive Sophus style code with promising results. The general approach may be useful for other styles and in other application domains as well.",
        "published": "1999-03-01T11:03:47Z",
        "link": "http://arxiv.org/abs/cs/9903002v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "D.1.5; D.2.2; J.2"
        ]
    },
    {
        "title": "The Structure of Weighting Coefficient Matrices of Harmonic Differential   Quadrature and Its Applications",
        "authors": [
            "W. Chen",
            "W. Wang",
            "T. Zhong"
        ],
        "summary": "The structure of weighting coefficient matrices of Harmonic Differential Quadrature (HDQ) is found to be either centrosymmetric or skew centrosymmetric depending on the order of the corresponding derivatives. The properties of both matrices are briefly discussed in this paper. It is noted that the computational effort of the harmonic quadrature for some problems can be further reduced up to 75 per cent by using the properties of the above-mentioned matrices.",
        "published": "1999-04-11T08:21:13Z",
        "link": "http://arxiv.org/abs/cs/9904003v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "G.1.3, G.1.8"
        ]
    },
    {
        "title": "Jacobian matrix: a bridge between linear and nonlinear polynomial-only   problems",
        "authors": [
            "W. Chen"
        ],
        "summary": "By using the Hadamard matrix product concept, this paper introduces two generalized matrix formulation forms of numerical analogue of nonlinear differential operators. The SJT matrix-vector product approach is found to be a simple, efficient and accurate technique in the calculation of the Jacobian matrix of the nonlinear discretization by finite difference, finite volume, collocation, dual reciprocity BEM or radial functions based numerical methods. We also present and prove simple underlying relationship (theorem (3.1)) between general nonlinear analogue polynomials and their corresponding Jacobian matrices, which forms the basis of this paper. By means of theorem 3.1, stability analysis of numerical solutions of nonlinear initial value problems can be easily handled based on the well-known results for linear problems. Theorem 3.1 also leads naturally to the straightforward extension of various linear iterative algorithms such as the SOR, Gauss-Seidel and Jacobi methods to nonlinear algebraic equations. Since an exact alternative of the quasi-Newton equation is established via theorem 3.1, we derive a modified BFGS quasi-Newton method. A simple formula is also given to examine the deviation between the approximate and exact Jacobian matrices. Furthermore, in order to avoid the evaluation of the Jacobian matrix and its inverse, the pseudo-Jacobian matrix is introduced with a general applicability of any nonlinear systems of equations. It should be pointed out that a large class of real-world nonlinear problems can be modeled or numerically discretized polynomial-only algebraic system of equations. The results presented here are in general applicable for all these problems. This paper can be considered as a starting point in the research of nonlinear computation and analysis from an innovative viewpoint.",
        "published": "1999-04-15T12:31:21Z",
        "link": "http://arxiv.org/abs/cs/9904006v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "G.1.3, G.1.8"
        ]
    },
    {
        "title": "The Study on the Nonlinear Computations of the DQ and DC Methods",
        "authors": [
            "W. Chen",
            "Tingxiu Zhong"
        ],
        "summary": "This paper points out that the differential quadrature (DQ) and differential cubature (DC) methods due to their global domain property are more efficient for nonlinear problems than the traditional numerical techniques such as finite element and finite difference methods. By introducing the Hadamard product of matrices, we obtain an explicit matrix formulation for the DQ and DC solutions of nonlinear differential and integro-differential equations. Due to its simplicity and flexibility, the present Hadamard product approach makes the DQ and DC methods much easier to be used. Many studies on the Hadamard product can be fully exploited for the DQ and DC nonlinear computations. Furthermore, we first present SJT product of matrix and vector to compute accurately and efficiently the Frechet derivative matrix in the Newton-Raphson method for the solution of the nonlinear formulations. We also propose a simple approach to simplify the DQ or DC formulations for some nonlinear differential operators and thus the computational efficiency of these methods is improved significantly. We give the matrix multiplication formulas to compute efficiently the weighting coefficient matrices of the DC method. The spherical harmonics are suggested as the test functions in the DC method to handle the nonlinear differential equations occurring in global and hemispheric weather forecasting problems. Some examples are analyzed to demonstrate the simplicity and efficiency of the presented techniques. It is emphasized that innovations presented are applicable to the nonlinear computations of the other numerical methods as well.",
        "published": "1999-04-15T13:24:56Z",
        "link": "http://arxiv.org/abs/cs/9904007v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "G.1.3, G.1.8"
        ]
    },
    {
        "title": "Hadamard product nonlinear formulation of Galerkin and finite element   methods",
        "authors": [
            "W. Chen"
        ],
        "summary": "A novel nonlinear formulation of the finite element and Galerkin methods is presented here, which leads to the Hadamard product expression of the resultant nonlinear algebraic analogue. The presented formulation attains the advantages of weak formulation in the standard finite element and Galerkin schemes and avoids the costly repeated numerical integration of the Jacobian matrix via the recently developed SJT product approach. This also provides possibility of the nonlinear decoupling computations.",
        "published": "1999-04-28T13:12:47Z",
        "link": "http://arxiv.org/abs/cs/9904021v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "G.1.2; G.1.5; G.1.8, G.1.3"
        ]
    },
    {
        "title": "Programs with Stringent Performance Objectives Will Often Exhibit   Chaotic Behavior",
        "authors": [
            "M. Chaves"
        ],
        "summary": "Software for the resolution of certain kind of problems, those that rate high in the Stringent Performance Objectives adjustment factor (IFPUG scheme), can be described using a combination of game theory and autonomous systems. From this description it can be shown that some of those problems exhibit chaotic behavior, an important fact in understanding the functioning of the related software. As a relatively simple example, it is shown that chess exhibits chaotic behavior in its configuration space. This implies that static evaluators in chess programs have intrinsic limitations.",
        "published": "1999-05-27T23:58:05Z",
        "link": "http://arxiv.org/abs/cs/9905016v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "68N05,68Q20,90D05,90D80"
        ]
    },
    {
        "title": "A Newton method without evaluation of nonlinear function values",
        "authors": [
            "W. Chen"
        ],
        "summary": "The present author recently proposed and proved a relationship theorem between nonlinear polynomial equations and the corresponding Jacobian matrix. By using this theorem, this paper derives a Newton iterative formula without requiring the evaluation of nonlinear function values in the solution of nonlinear polynomial-only problems.",
        "published": "1999-06-09T12:27:03Z",
        "link": "http://arxiv.org/abs/cs/9906011v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "G.1.3; G.1.5; G.1.2"
        ]
    },
    {
        "title": "The application of special matrix product to differential quadrature   solution of geometrically nonlinear bending of orthotropic rectangular plates",
        "authors": [
            "W. Chen",
            "C. Shu",
            "W. He"
        ],
        "summary": "The Hadamard and SJT product of matrices are two types of special matrix product. The latter was first defined by Chen. In this study, they are applied to the differential quadrature (DQ) solution of geometrically nonlinear bending of isotropic and orthotropic rectangular plates. By using the Hadamard product, the nonlinear formulations are greatly simplified, while the SJT product approach minimizes the effort to evaluate the Jacobian derivative matrix in the Newton-Raphson method for solving the resultant nonlinear formulations. In addition, the coupled nonlinear formulations for the present problems can easily be decoupled by means of the Hadamard and SJT product. Therefore, the size of the simultaneous nonlinear algebraic equations is reduced by two-thirds and the computing effort and storage requirements are alleviated greatly. Two recent approaches applying the multiple boundary conditions are employed in the present DQ nonlinear computations. The solution accuracies are improved obviously in comparison to the previously given by Bert et al. The numerical results and detailed solution procedures are provided to demonstrate the superb efficiency, accuracy and simplicity of the new approaches in applying DQ method for nonlinear computations.",
        "published": "1999-06-09T12:47:13Z",
        "link": "http://arxiv.org/abs/cs/9906012v1",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "G.1.3; G.1.5; G.1.2; G.1.8"
        ]
    },
    {
        "title": "Generalized linearization in nonlinear modeling of data",
        "authors": [
            "W. Chen"
        ],
        "summary": "The principal innovative idea in this paper is to transform the original complex nonlinear modeling problem into a combination of linear problem and very simple nonlinear problems. The key step is the generalized linearization of nonlinear terms. This paper only presents the introductory strategy of this methodology. The practical numerical experiments will be provided subsequently.",
        "published": "1999-07-12T11:34:56Z",
        "link": "http://arxiv.org/abs/cs/9907020v2",
        "categories": [
            "cs.CE",
            "cs.NA",
            "math.NA",
            "G.1.3; G.1.8"
        ]
    },
    {
        "title": "A simple C++ library for manipulating scientific data sets as structured   data",
        "authors": [
            "Christoph Best"
        ],
        "summary": "Representing scientific data sets efficiently on external storage usually involves converting them to a byte string representation using specialized reader/writer routines. The resulting storage files are frequently difficult to interpret without these specialized routines as they do not contain information about the logical structure of the data. Avoiding such problems usually involves heavy-weight data format libraries or data base systems. We present a simple C++ library that allows to create and access data files that store structured data. The structure of the data is described by a data type that can be built from elementary data types (integer and floating-point numbers, byte strings) and composite data types (arrays, structures, unions). An abstract data access class presents the data to the application. Different actual data file structures can be implemented under this layer. This method is particularly suited to applications that require complex data structures, e.g. molecular dynamics simulations. Extensions such as late type binding and object persistence are discussed.",
        "published": "1999-07-30T08:35:55Z",
        "link": "http://arxiv.org/abs/cs/9907043v1",
        "categories": [
            "cs.CE",
            "cs.DB",
            "E.5"
        ]
    },
    {
        "title": "Seeing the Forest in the Tree: Applying VRML to Mathematical Problems in   Number Theory",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "We show how VRML (Virtual Reality Modeling Language) can provide potentially powerful insight into the 3x + 1 problem via the introduction of a unique geometrical object, called the 'G-cell', akin to a fractal generator. We present an example of a VRML world developed programmatically with the G-cell. The role of VRML as a tool for furthering the understanding the 3x+1 problem is potentially significant for several reasons: a) VRML permits the observer to zoom into the geometric structure at all scales (up to limitations of the computing platform). b) VRML enables rotation to alter comparative visual perspective (similar to Tukey's data-spinning concept). c) VRML facilitates the demonstration of interesting tree features between collaborators on the internet who might otherwise have difficulty conveying their ideas unambiguously. d) VRML promises to reveal any dimensional dependencies among 3x+1 sequences.",
        "published": "1999-12-31T18:36:38Z",
        "link": "http://arxiv.org/abs/cs/9912021v2",
        "categories": [
            "cs.MS",
            "cs.CE",
            "G.2.2;G.4;H.5.1;I.3.2;I.3.7;I.6.8;I.7.2;J.2;K.3.1"
        ]
    },
    {
        "title": "Fast Computational Algorithms for the Discrete Wavelet Transform and   Applications of Localized Orthonormal Bases in Signal Classification",
        "authors": [
            "Eirik Fossgaard"
        ],
        "summary": "We construct an algorithm for implementing the discrete wavelet transform by means of matrices in SO_2(R) for orthonormal compactly supported wavelets and matrices in SL_m(R), m > = 2, for compactly supported biorthogonal wavelets. We show that in 1 dimension the total operation count using this algorithm can be reduced to about 50% of the conventional convolution and downsampling by 2-operation for both orthonormal and biorthogonal filters. In the special case of biorthogonal symmetric odd-odd filters, we show an implementation yielding a total operation count of about 38% of the conventional method. In 2 dimensions we show an implementation of this algorithm yielding a reduction in the total operation count of about 70% when the filters are orthonormal, a reduction of about 62% for general biorthogonal filters, and a reduction of about 70% if the filters are symmetric odd-odd length filters. We further extend these results to 3 dimensions. We also show how the SO_2(R)-method for implementing the discrete wavelet transform may be exploited to compute short FIR filters, and we construct edge mappings where we try to improve upon the degree of preservation of regularity in the conventional methods. We also consider a two-class waveform discrimination problem. A statistical space-frequency analysis is performed on a training data set using the LDB-algorithm of N.Saito and R.Coifman. The success of the algorithm on this particular problem is evaluated on a disjoint test data set.",
        "published": "1999-01-16T16:54:01Z",
        "link": "http://arxiv.org/abs/cs/9901008v1",
        "categories": [
            "cs.MS",
            "cs.CE",
            "F.2.1; G.4; I.5.4"
        ]
    },
    {
        "title": "An Algebraic Programming Style for Numerical Software and its   Optimization",
        "authors": [
            "T. B. Dinesh",
            "M. Haveraaen",
            "J. Heering"
        ],
        "summary": "The abstract mathematical theory of partial differential equations (PDEs) is formulated in terms of manifolds, scalar fields, tensors, and the like, but these algebraic structures are hardly recognizable in actual PDE solvers. The general aim of the Sophus programming style is to bridge the gap between theory and practice in the domain of PDE solvers. Its main ingredients are a library of abstract datatypes corresponding to the algebraic structures used in the mathematical theory and an algebraic expression style similar to the expression style used in the mathematical theory. Because of its emphasis on abstract datatypes, Sophus is most naturally combined with object-oriented languages or other languages supporting abstract datatypes. The resulting source code patterns are beyond the scope of current compiler optimizations, but are sufficiently specific for a dedicated source-to-source optimizer. The limited, domain-specific, character of Sophus is the key to success here. This kind of optimization has been tested on computationally intensive Sophus style code with promising results. The general approach may be useful for other styles and in other application domains as well.",
        "published": "1999-03-01T11:03:47Z",
        "link": "http://arxiv.org/abs/cs/9903002v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CE",
            "cs.MS",
            "D.1.5; D.2.2; J.2"
        ]
    },
    {
        "title": "An Implementation of the Bestvina-Handel Algorithm for Surface   Homeomorphisms",
        "authors": [
            "Peter Brinkmann"
        ],
        "summary": "Bestvina and Handel have found an effective algorithm that determines whether a given homeomorphism of an orientable, possibly punctured surface is pseudo-Anosov. We present a software package in Java that realizes this algorithm for surfaces with one puncture. Moreover, the package allows the user to define homeomorphisms in terms of Dehn twists, and in the pseudo-Anosov case it generates images of train tracks in the sense of Bestvina-Handel.",
        "published": "1999-05-25T20:13:09Z",
        "link": "http://arxiv.org/abs/math/9905155v2",
        "categories": [
            "math.GR",
            "cs.MS",
            "math.GT"
        ]
    },
    {
        "title": "Seeing the Forest in the Tree: Applying VRML to Mathematical Problems in   Number Theory",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "We show how VRML (Virtual Reality Modeling Language) can provide potentially powerful insight into the 3x + 1 problem via the introduction of a unique geometrical object, called the 'G-cell', akin to a fractal generator. We present an example of a VRML world developed programmatically with the G-cell. The role of VRML as a tool for furthering the understanding the 3x+1 problem is potentially significant for several reasons: a) VRML permits the observer to zoom into the geometric structure at all scales (up to limitations of the computing platform). b) VRML enables rotation to alter comparative visual perspective (similar to Tukey's data-spinning concept). c) VRML facilitates the demonstration of interesting tree features between collaborators on the internet who might otherwise have difficulty conveying their ideas unambiguously. d) VRML promises to reveal any dimensional dependencies among 3x+1 sequences.",
        "published": "1999-12-31T18:36:38Z",
        "link": "http://arxiv.org/abs/cs/9912021v2",
        "categories": [
            "cs.MS",
            "cs.CE",
            "G.2.2;G.4;H.5.1;I.3.2;I.3.7;I.6.8;I.7.2;J.2;K.3.1"
        ]
    },
    {
        "title": "Object Oriented and Functional Programming for Symbolic Manipulation",
        "authors": [
            "Alexander Yu. Vlasov"
        ],
        "summary": "The advantages of mixed approach with using different kinds of programming techniques for symbolic manipulation are discussed. The main purpose of approach offered is merge the methods of object oriented programming that convenient for presentation data and algorithms for user with advantages of functional languages for data manipulation, internal presentation, and portability of software.",
        "published": "1999-01-13T20:40:04Z",
        "link": "http://arxiv.org/abs/cs/9901006v1",
        "categories": [
            "cs.SC",
            "cs.PL",
            "I.1.0; D.1.1; D.1.5; D.3.2"
        ]
    },
    {
        "title": "Events in Property Patterns",
        "authors": [
            "M. Chechik",
            "D. Paun"
        ],
        "summary": "A pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification was proposed by Dwyer and his collegues. The patterns enable non-experts to read and write formal specifications for realistic systems and facilitate easy conversion of specifications between formalisms, such as LTL, CTL, QRE. In this paper, we extend the pattern system with events - changes of values of variables in the context of LTL.",
        "published": "1999-06-28T17:06:51Z",
        "link": "http://arxiv.org/abs/cs/9906029v2",
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.CL",
            "cs.SC",
            "D.2.4;F.3.1;F.4.1;I.2.4;D.2.1"
        ]
    },
    {
        "title": "Mutual Search",
        "authors": [
            "Harry Buhrman",
            "Matthew Franklin",
            "Juan A. Garay",
            "Jaap-Henk Hoepman",
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We introduce a search problem called ``mutual search'' where $k$ \\agents, arbitrarily distributed over $n$ sites, are required to locate one another by posing queries of the form ``Anybody at site $i$?''. We ask for the least number of queries that is necessary and sufficient. For the case of two \\agents using deterministic protocols we obtain the following worst-case results: In an oblivious setting (where all pre-planned queries are executed) there is no savings: $n-1$ queries are required and are sufficient. In a nonoblivious setting we can exploit the paradigm of ``no news is also news'' to obtain significant savings: in the synchronous case $0.586n$ queries suffice and $0.536n$ queries are required; in the asynchronous case $0.896n$ queries suffice and a fortiori 0.536 queries are required; for $o(\\sqrt{n})$ \\agents using a deterministic protocol less than $n$ queries suffice; there is a simple randomized protocol for two \\agents with worst-case expected $0.5n$ queries and all randomized protocols require at least $0.125n$ worst-case expected queries. The graph-theoretic framework we formulate for expressing and analyzing algorithms for this problem may be of independent interest.",
        "published": "1999-02-02T15:46:00Z",
        "link": "http://arxiv.org/abs/cs/9902005v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DB",
            "cs.DC",
            "cs.DM",
            "cs.IR",
            "F.2,C.2,E,1,D.4.4"
        ]
    },
    {
        "title": "A Flit Level Simulator for Wormhole Routing",
        "authors": [
            "Denvil Smith"
        ],
        "summary": "Wormhole routing, the latest switching technique to be utilized by massively parallel computers, enjoys the distinct advantage of a low latency when compared to other switching techniques. This low latency is due to the nearly distance insensitive routing traits in the absence of channel contention. The low latency of wormhole routing brings about a liability of this switching technique, a chance of deadlock. Deadlock is a concern in wormhole routed networks due to the fact a message does not release its allocated resources until all flits of a message have completely traversed the router in which these resources are associated. The deadlock condition is addressed in the routing algorithm. Simulation tools are currently needed that will aid in the size and number of resources necessary to obtain the optimum utilization of network resources for an algorithm. Some of these resources include the topology of the network along with the number of nodes for the topology, the size of the message, and the number and size of buffers at each router.",
        "published": "1999-03-04T02:13:35Z",
        "link": "http://arxiv.org/abs/cs/9903004v4",
        "categories": [
            "cs.DC",
            "cs.OS",
            "B.4.4; C.2.6; D.4.4"
        ]
    },
    {
        "title": "Space-Efficient Routing Tables for Almost All Networks and the   Incompressibility Method",
        "authors": [
            "Harry Buhrman",
            "Jaap-Henk Hoepman",
            "Paul Vitanyi"
        ],
        "summary": "We use the incompressibility method based on Kolmogorov complexity to determine the total number of bits of routing information for almost all network topologies. In most models for routing, for almost all labeled graphs $\\Theta (n^2)$ bits are necessary and sufficient for shortest path routing. By `almost all graphs' we mean the Kolmogorov random graphs which constitute a fraction of $1-1/n^c$ of all graphs on $n$ nodes, where $c > 0$ is an arbitrary fixed constant. There is a model for which the average case lower bound rises to $\\Omega(n^2 \\log n)$ and another model where the average case upper bound drops to $O(n \\log^2 n)$. This clearly exposes the sensitivity of such bounds to the model under consideration. If paths have to be short, but need not be shortest (if the stretch factor may be larger than 1), then much less space is needed on average, even in the more demanding models. Full-information routing requires $\\Theta (n^3)$ bits on average. For worst-case static networks we prove a $\\Omega(n^2 \\log n)$ lower bound for shortest path routing and all stretch factors $<2$ in some networks where free relabeling is not allowed.",
        "published": "1999-03-10T19:01:02Z",
        "link": "http://arxiv.org/abs/cs/9903009v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.CC",
            "cs.DS",
            "cs.NI",
            "C.2, F.2, D.4"
        ]
    },
    {
        "title": "ODP channel objects that provide services transparently for distributing   processing systems",
        "authors": [
            "Walter Eaves"
        ],
        "summary": "This paper describes an architecture for a distributing processing system that would allow remote procedure calls to invoke other services as messages are passed between clients and servers. It proposes that an additional class of data processing objects be located in the software communications channel. The objects in this channel would then be used to enforce protocols on client-server applications without any additional effort by the application programmers. For example, services such as key-management, time-stamping, sequencing and encryption can be implemented at different levels of the software communications stack to provide a complete authentication service. A distributing processing environment could be used to control broadband network data delivery. Architectures and invocation semantics are discussed, Example classes and interfaces for channel objects are given in the Java programming language.",
        "published": "1999-04-26T21:13:00Z",
        "link": "http://arxiv.org/abs/cs/9904020v1",
        "categories": [
            "cs.DC",
            "cs.OS",
            "C.2.0; C.2.4; D.4.7; H.5.1; K.6.4; D.4.6"
        ]
    },
    {
        "title": "DRAFT : Task System and Item Architecture (TSIA)",
        "authors": [
            "Burkhard D. Burow"
        ],
        "summary": "During its execution, a task is independent of all other tasks. For an application which executes in terms of tasks, the application definition can be free of the details of the execution. Many projects have demonstrated that a task system (TS) can provide such an application with a parallel, distributed, heterogeneous, adaptive, dynamic, real-time, interactive, reliable, secure or other execution. A task consists of items and thus the application is defined in terms of items. An item architecture (IA) can support arrays, routines and other structures of items, thus allowing for a structured application definition. Taking properties from many projects, the support can extend through to currying, application defined types, conditional items, streams and other definition elements. A task system and item architecture (TSIA) thus promises unprecedented levels of support for application execution and definition.",
        "published": "1999-05-05T01:43:13Z",
        "link": "http://arxiv.org/abs/cs/9905002v1",
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.OS",
            "A.1;D.1.1;D.1.3;D.1.4;D.2.3;D.2.11;D.3.2;D.3.3;D.3.4;D.4.1;D.4.5;\n  D.4.7;E.1;F.1.2;F.3.3"
        ]
    },
    {
        "title": "Collective Choice Theory in Collaborative Computing",
        "authors": [
            "Walter Eaves"
        ],
        "summary": "This paper presents some fundamental collective choice theory for information system designers, particularly those working in the field of computer-supported cooperative work. This paper is focused on a presentation of Arrow's Possibility and Impossibility theorems which form the fundamental boundary on the efficacy of collective choice: voting and selection procedures. It restates the conditions that Arrow placed on collective choice functions in more rigorous second-order logic, which could be used as a set of test conditions for implementations, and a useful probabilistic result for analyzing votes on issue pairs. It also describes some simple collective choice functions. There is also some discussion of how enterprises should approach putting their resources under collective control: giving an outline of a superstructure of performative agents to carry out this function and what distributing processing technology would be needed.",
        "published": "1999-05-10T17:07:11Z",
        "link": "http://arxiv.org/abs/cs/9905003v1",
        "categories": [
            "cs.MA",
            "cs.DC",
            "H.5.3 I.2.11 J.4 K.4.1 K.4.3 F.1.2"
        ]
    },
    {
        "title": "Using Collective Intelligence to Route Internet Traffic",
        "authors": [
            "David H. Wolpert",
            "Kagan Tumer",
            "Jeremy Frank"
        ],
        "summary": "A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.",
        "published": "1999-05-10T20:52:23Z",
        "link": "http://arxiv.org/abs/cs/9905004v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat.stat-mech",
            "cs.DC",
            "cs.NI",
            "nlin.AO",
            "I.2.6; I.2.11"
        ]
    },
    {
        "title": "General Principles of Learning-Based Multi-Agent Systems",
        "authors": [
            "David H. Wolpert",
            "Kevin R. Wheeler",
            "Kagan Tumer"
        ],
        "summary": "We consider the problem of how to design large decentralized multi-agent systems (MAS's) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we do not want the agents to ``work at cross-purposes'' as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINs. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINs perform near optimally in a difficult variant of Arthur's bar problem (and in particular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINs in the leader-follower problem.",
        "published": "1999-05-10T22:20:40Z",
        "link": "http://arxiv.org/abs/cs/9905005v1",
        "categories": [
            "cs.MA",
            "adap-org",
            "cond-mat.stat-mech",
            "cs.DC",
            "cs.LG",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "After Compilers and Operating Systems : The Third Advance in Application   Support",
        "authors": [
            "Burkhard D. Burow"
        ],
        "summary": "After compilers and operating systems, TSIAs are the third advance in application support. A compiler supports a high level application definition in a programming language. An operating system supports a high level interface to the resources used by an application execution. A Task System and Item Architecture (TSIA) provides an application with a transparent reliable, distributed, heterogeneous, adaptive, dynamic, real-time, interactive, parallel, secure or other execution. In addition to supporting the application execution, a TSIA also supports the application definition. This run-time support for the definition is complementary to the compile-time support of a compiler. For example, this allows a language similar to Fortran or C to deliver features promised by functional computing. While many TSIAs exist, they previously have not been recognized as such and have served only a particular type of application. Existing TSIAs and other projects demonstrate that TSIAs are feasible for most applications. As the next paradigm for application support, the TSIA simplifies and unifies existing computing practice and research. By solving many outstanding problems, the TSIA opens many, many new opportunities for computing.",
        "published": "1999-08-03T14:50:09Z",
        "link": "http://arxiv.org/abs/cs/9908002v1",
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.OS",
            "A.1;D.1.1;D.1.3;D.1.4;D.2.11;D.3.2;D.3.3;D.3.4;D.4.5;D.4.7;E.1;F.1.2"
        ]
    },
    {
        "title": "Secure Multicast in a WAN",
        "authors": [
            "Dahlia Malkhi",
            "Michael Merritt",
            "Ohad Rodeh"
        ],
        "summary": "A secure reliable multicast protocol enables a process to send a message to a group of recipients such that all correct destinations receive the same message, despite the malicious efforts of fewer than a third of the total number of processes, including the sender. This has been sh own to be a useful tool in building secure distributed services, albeit with a cost that typically grows linearly with the size of the system. For very large networks, for which this is prohibitive, we present two approaches for reducing the cost: First, we show a protocol whose cost is on the order of the number of tolerated failures. Secondly, we show how relaxing the consistency requirement to a probabilistic guarantee can reduce the associated cost, effectively to a constant.",
        "published": "1999-08-12T17:40:08Z",
        "link": "http://arxiv.org/abs/cs/9908008v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "c.2.0;c.2.4;c.4"
        ]
    },
    {
        "title": "On Propagating Updates in a Byzantine Environment",
        "authors": [
            "Dahlia Malkhi",
            "Yishay Mansour",
            "Michael Reiter"
        ],
        "summary": "We study how to efficiently diffuse updates to a large distributed system of data replicas, some of which may exhibit arbitrary (Byzantine) failures. We assume that strictly fewer than $t$ replicas fail, and that each update is initially received by at least $t$ correct replicas. The goal is to diffuse each update to all correct replicas while ensuring that correct replicas accept no updates generated spuriously by faulty replicas. To achieve reliable diffusion, each correct replica accepts an update only after receiving it from at least $t$ others. We provide the first analysis of epidemic-style protocols for such environments. This analysis is fundamentally different from known analyses for the benign case due to our treatment of fully Byzantine failures---which, among other things, precludes the use of digital signatures for authenticating forwarded updates. We propose two epidemic-style diffusion algorithms and two measures that characterize the efficiency of diffusion algorithms in general. We characterize both of our algorithms according to these measures, and also prove lower bounds with regards to these measures that show that our algorithms are close to optimal.",
        "published": "1999-08-12T18:00:51Z",
        "link": "http://arxiv.org/abs/cs/9908010v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "C.2.0;C.2.4;C.4"
        ]
    },
    {
        "title": "The Load and Availability of Byzantine Quorum Systems",
        "authors": [
            "Dahlia Malkhi",
            "Michael Reiter",
            "Avishai Wool"
        ],
        "summary": "Replicated services accessed via {\\em quorums} enable each access to be performed at only a subset (quorum) of the servers, and achieve consistency across accesses by requiring any two quorums to intersect. Recently, $b$-masking quorum systems, whose intersections contain at least $2b+1$ servers, have been proposed to construct replicated services tolerant of $b$ arbitrary (Byzantine) server failures. In this paper we consider a hybrid fault model allowing benign failures in addition to the Byzantine ones. We present four novel constructions for $b$-masking quorum systems in this model, each of which has optimal {\\em load} (the probability of access of the busiest server) or optimal availability (probability of some quorum surviving failures). To show optimality we also prove lower bounds on the load and availability of any $b$-masking quorum system in this model.",
        "published": "1999-08-12T18:06:08Z",
        "link": "http://arxiv.org/abs/cs/9908011v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "C.2.4;C.4"
        ]
    },
    {
        "title": "Collective Intelligence for Control of Distributed Dynamical Systems",
        "authors": [
            "David H. Wolpert",
            "Kevin R. Wheeler",
            "Kagan Tumer"
        ],
        "summary": "We consider the El Farol bar problem, also known as the minority game (W. B. Arthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet and Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of the general problem of how to configure the nodal elements of a distributed dynamical system so that they do not ``work at cross purposes'', in that their collective dynamics avoids frustration and thereby achieves a provided global goal. We summarize a mathematical theory for such configuration applicable when (as in the bar problem) the global goal can be expressed as minimizing a global energy function and the nodes can be expressed as minimizers of local free energy functions. We show that a system designed with that theory performs nearly optimally for the bar problem.",
        "published": "1999-08-17T21:32:41Z",
        "link": "http://arxiv.org/abs/cs/9908013v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat",
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "An Introduction to Collective Intelligence",
        "authors": [
            "David H. Wolpert",
            "Kagan Tumer"
        ],
        "summary": "This paper surveys the emerging science of how to design a ``COllective INtelligence'' (COIN). A COIN is a large multi-agent system where:   (i) There is little to no centralized communication or control; and   (ii) There is a provided world utility function that rates the possible histories of the full system.   In particular, we are interested in COINs in which each agent runs a reinforcement learning (RL) algorithm. Rather than use a conventional modeling approach (e.g., model the system dynamics, and hand-tune agents to cooperate), we aim to solve the COIN design problem implicitly, via the ``adaptive'' character of the RL algorithms of each of the agents. This approach introduces an entirely new, profound design problem: Assuming the RL algorithms are able to achieve high rewards, what reward functions for the individual agents will, when pursued by those agents, result in high world utility? In other words, what reward functions will best ensure that we do not have phenomena like the tragedy of the commons, Braess's paradox, or the liquidity trap?   Although still very young, research specifically concentrating on the COIN design problem has already resulted in successes in artificial domains, in particular in packet-routing, the leader-follower problem, and in variants of Arthur's El Farol bar problem. It is expected that as it matures and draws upon other disciplines related to COINs, this research will greatly expand the range of tasks addressable by human engineers. Moreover, in addition to drawing on them, such a fully developed scie nce of COIN design may provide much insight into other already established scientific fields, such as economics, game theory, and population biology.",
        "published": "1999-08-17T22:49:19Z",
        "link": "http://arxiv.org/abs/cs/9908014v1",
        "categories": [
            "cs.LG",
            "adap-org",
            "cond-mat",
            "cs.DC",
            "cs.MA",
            "nlin.AO",
            "I.2.6 ; I.2.11"
        ]
    },
    {
        "title": "From Massively Parallel Algorithms and Fluctuating Time Horizons to   Non-equilibrium Surface Growth",
        "authors": [
            "G. Korniss",
            "Z. Toroczkai",
            "M. A. Novotny",
            "P. A. Rikvold"
        ],
        "summary": "We study the asymptotic scaling properties of a massively parallel algorithm for discrete-event simulations where the discrete events are Poisson arrivals. The evolution of the simulated time horizon is analogous to a non-equilibrium surface. Monte Carlo simulations and a coarse-grained approximation indicate that the macroscopic landscape in the steady state is governed by the Edwards-Wilkinson Hamiltonian. Since the efficiency of the algorithm corresponds to the density of local minima in the associated surface, our results imply that the algorithm is asymptotically scalable.",
        "published": "1999-09-07T20:44:54Z",
        "link": "http://arxiv.org/abs/cond-mat/9909114v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Distributed Algorithms in Multihop Broadcast Networks",
        "authors": [
            "Israel Cidon",
            "Osnat Mokryn"
        ],
        "summary": "Broadcast networks are often used in modern communication systems. A common broadcast network is a single hop shared media system, where a transmitted message is heard by all neighbors, such as some LAN networks. In this work we consider a more complex environment, in which a transmitted message is heard only by a group of neighbors, such as Ad-Hoc networks, satellite and radio networks, as well as wireless multistation backbone system for mobile communication. It is important to design efficient algorithms for such environments. Our main result is a new Leader Election algorithm, with O(n) time complexity and O(n*lg(n)) message transmission complexity. Our distributed solution uses a propagation of information with feedback (PIF) building block tuned to the broadcast media, and a special counting and joining approach for the election procedure phase. The latter is required for achieving the linear time. It is demonstrated that the broadcast model requires solutions which are different from the known point-to-point model.",
        "published": "1999-09-08T14:49:05Z",
        "link": "http://arxiv.org/abs/cs/9909011v1",
        "categories": [
            "cs.DC",
            "cs.NI",
            "g.2.2;c.2.4"
        ]
    },
    {
        "title": "Self-stabilizing mutual exclusion on a ring, even if K=N",
        "authors": [
            "Jaap-Henk Hoepman"
        ],
        "summary": "We show that, contrary to common belief, Dijkstra's self-stabilizing mutual exclusion algorithm on a ring [Dij74,Dij82] also stabilizes when the number of states per node is one less than the number of nodes on the ring.",
        "published": "1999-09-21T13:39:03Z",
        "link": "http://arxiv.org/abs/cs/9909013v1",
        "categories": [
            "cs.DC",
            "D.4.5; D.1.3"
        ]
    },
    {
        "title": "A decision-theoretic approach to reliable message delivery",
        "authors": [
            "Francis C. Chu",
            "Joseph Y. Halpern"
        ],
        "summary": "We argue that the tools of decision theory need to be taken more seriously in the specification and analysis of systems. We illustrate this by considering a simple problem involving reliable communication, showing how considerations of utility and probability can be used to decide when it is worth sending heartbeat messages and, if they are sent, how often they should be sent.",
        "published": "1999-09-21T20:51:37Z",
        "link": "http://arxiv.org/abs/cs/9909015v1",
        "categories": [
            "cs.DC",
            "F.3.1, C.2.4, C.2.1, D.4.7"
        ]
    },
    {
        "title": "Consistent Checkpointing in Distributed Databases: Towards a Formal   Approach",
        "authors": [
            "R. Baldoni",
            "F. Quaglia",
            "M. Raynal"
        ],
        "summary": "Whether it is for audit or for recovery purposes, data checkpointing is an important problem of distributed database systems. Actually, transactions establish dependence relations on data checkpoints taken by data object managers. So, given an arbitrary set of data checkpoints (including at least a single data checkpoint from a data manager, and at most a data checkpoint from each data manager), an important question is the following one: ``Can these data checkpoints be members of a same consistent global checkpoint?''. This paper answers this question by providing a necessary and sufficient condition suited for database systems. Moreover, to show the usefulness of this condition, two {\\em non-intrusive} data checkpointing protocols are derived from this condition. It is also interesting to note that this paper, by exhibiting ``correspondences'', establishes a bridge between the data object/transaction model and the process/message-passing model.",
        "published": "1999-10-22T15:17:29Z",
        "link": "http://arxiv.org/abs/cs/9910019v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "C.2.4; H.2"
        ]
    },
    {
        "title": "Interfacing Interpreted and Compiled Languages to Support Applications   on a Massively Parallel Network of Workstations (MP-NOW)",
        "authors": [
            "Jeremy Kepner",
            "Maya Gokhale",
            "Ron Minnich",
            "Aaron Marks",
            "John DeGood"
        ],
        "summary": "Astronomers are increasingly using Massively Parallel Network of Workstations (MP-NOW) to address their most challenging computing problems. Fully exploiting these systems is made more difficult as more and more modeling and data analysis software is written in interpreted languages (such as IDL, MATLAB, and Mathematica) which do not lend themselves to parallel computing. We present a specific example of a very simple, but generic solution to this problem. Our example uses an interpreted language (IDL) to set up a calculation and then interfaces with a computational kernel written in a compiled language (C). The IDL code then calls the C code as an external library. We have added to the computational kernel an additional layer, which manages multiple copies of the kernel running on a MP-NOW and returns the results back to the interpreted layer. Our implementation uses The Next generation Taskbag (TNT) library developed at Sarnoff to provide an efficient means for implementing task parallelism. A test problem (taken from Astronomy) has been implemented on the Sarnoff Cyclone computer which consists of 160 heterogeneous nodes connected by a ``fat'' tree 100 Mb/s switched Ethernet running the RedHat Linux and FreeBSD operating systems. Our first results in this ongoing project have demonstrated the feasibility of this approach and produced speedups of greater than 50 on 60 processors.",
        "published": "1999-12-07T22:37:23Z",
        "link": "http://arxiv.org/abs/astro-ph/9912134v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Scalability Terminology: Farms, Clones, Partitions, Packs, RACS and RAPS",
        "authors": [
            "Bill Devlin",
            "Jim Gray",
            "Bill Laing",
            "George Spix"
        ],
        "summary": "Defines a vocabulary for scaleable systems: Geoplexes, Farms, Clones, RACS, RAPS, clones, partitions, and packs and dicusses the design tradeoffs of using clones, partitons, and packs.",
        "published": "1999-12-18T02:19:42Z",
        "link": "http://arxiv.org/abs/cs/9912010v1",
        "categories": [
            "cs.AR",
            "cs.DC",
            "C.0"
        ]
    },
    {
        "title": "Avoiding Braess' Paradox through Collective Intelligence",
        "authors": [
            "Kagan Tumer",
            "David H. Wolpert"
        ],
        "summary": "In an Ideal Shortest Path Algorithm (ISPA), at each moment each router in a network sends all of its traffic down the path that will incur the lowest cost to that traffic. In the limit of an infinitesimally small amount of traffic for a particular router, its routing that traffic via an ISPA is optimal, as far as cost incurred by that traffic is concerned. We demonstrate though that in many cases, due to the side-effects of one router's actions on another routers performance, having routers use ISPA's is suboptimal as far as global aggregate cost is concerned, even when only used to route infinitesimally small amounts of traffic. As a particular example of this we present an instance of Braess' paradox for ISPA's, in which adding new links to a network decreases overall throughput. We also demonstrate that load-balancing, in which the routing decisions are made to optimize the global cost incurred by all traffic currently being routed, is suboptimal as far as global cost averaged across time is concerned. This is also due to \"side-effects\", in this case of current routing decision on future traffic.   The theory of COllective INtelligence (COIN) is concerned precisely with the issue of avoiding such deleterious side-effects. We present key concepts from that theory and use them to derive an idealized algorithm whose performance is better than that of the ISPA, even in the infinitesimal limit. We present experiments verifying this, and also showing that a machine-learning-based version of this COIN algorithm in which costs are only imprecisely estimated (a version potentially applicable in the real world) also outperforms the ISPA, despite having access to less information than does the ISPA. In particular, this COIN algorithm avoids Braess' paradox.",
        "published": "1999-12-20T21:21:39Z",
        "link": "http://arxiv.org/abs/cs/9912012v1",
        "categories": [
            "cs.DC",
            "adap-org",
            "cs.MA",
            "cs.NI",
            "nlin.AO",
            "C.2.0; I.2.11"
        ]
    },
    {
        "title": "Differential interactive games: The short-term predictions",
        "authors": [
            "Denis V. Juriev"
        ],
        "summary": "Procedures of the short-term predictions for processes in general 2-person differential interactive games are proposed. Their effectiveness is discussed.",
        "published": "1999-01-19T07:58:55Z",
        "link": "http://arxiv.org/abs/math/9901074v2",
        "categories": [
            "math.HO",
            "cs.HC",
            "90D25 (Primary) 90D05, 49N55, 34H05, 93C41, 93B52 (Secondary)"
        ]
    },
    {
        "title": "Multimodal Surrogates for Video Browsing",
        "authors": [
            "Wei Ding",
            "Gary Marchionini",
            "Dagobert Soergel"
        ],
        "summary": "Three types of video surrogates - visual (keyframes), verbal (keywords/phrases), and combination of the two - were designed and studied in a qualitative investigation of user cognitive processes. The results favor the combined surrogates in which verbal information and images reinforce each other, lead to better comprehension, and may actually require less processing time. The results also highlight image features users found most helpful. These findings will inform the interface design and video representation for video retrieval and browsing.",
        "published": "1999-02-09T04:56:59Z",
        "link": "http://arxiv.org/abs/cs/9902019v1",
        "categories": [
            "cs.DL",
            "cs.HC",
            "H5.1; H3.1"
        ]
    },
    {
        "title": "A Scrollbar-based Visualization for Document Navigation",
        "authors": [
            "Donald Byrd"
        ],
        "summary": "We are interested in questions of improving user control in best-match text-retrieval systems, specifically questions as to whether simple visualizations that nonetheless go beyond the minimal ones generally available can significantly help users. Recently, we have been investigating ways to help users decide-given a set of documents retrieved by a query-which documents and passages are worth closer examination. We built a document viewer incorporating a visualization centered around a novel content-displaying scrollbar and color term highlighting, and studied whether the visualization is helpful to non-expert searchers. Participants' reaction to the visualization was very positive, while the objective results were inconclusive.",
        "published": "1999-02-24T17:10:46Z",
        "link": "http://arxiv.org/abs/cs/9902028v2",
        "categories": [
            "cs.IR",
            "cs.HC",
            "H5.2; H3.3; I7.2; H3.7"
        ]
    },
    {
        "title": "Workflow Automation with Lotus Notes for the Governmental Administrative   Information System",
        "authors": [
            "Saulius Maskeliunas"
        ],
        "summary": "The paper presents an introductory overview of the workflow automation area, outlining the main types, basic technologies, the essential features of workflow applications. Two sorts of process models for the definition of workflows (according to the conversation-based and activity-based methodologies) are sketched. Later on, the nature of Lotus Notes and its capabilities (as an environment for workflow management systems development) are indicated. Concluding, the experience of automating administrative workflows (developing a Subsystem of Inter-institutional Document Management of the VADIS project) is briefly outlined.",
        "published": "1999-03-30T21:28:07Z",
        "link": "http://arxiv.org/abs/cs/9903019v2",
        "categories": [
            "cs.HC",
            "H.4.1; H.5.3; J.1"
        ]
    },
    {
        "title": "Beyond Concern: Understanding Net Users' Attitudes About Online Privacy",
        "authors": [
            "Lorrie Faith Cranor",
            "Joseph Reagle",
            "Mark S. Ackerman"
        ],
        "summary": "People are concerned about privacy, particularly on the Internet. While many studies have provided evidence of this concern, few have explored the nature of the concern in detail, especially for the online environment. With this study, we have tried to better understand the nature of online privacy concerns; we look beyond the fact that people are concerned and attempt to understand how they are concerned. We hope our results will help inform both policy decisions as well as the development of technology tools that can assist Internet users in protecting their privacy.   We present results here from the analysis of 381 questionnaires completed between November 6 and November 13, 1998 by American Internet users. The sample was drawn from the FamilyPC magazine/Digital Research, Inc. Family Panel. While this is not a statistically representative sample of US Internet users, our respondents are heavy Internet users, and quite possibly lead innovators. As such, we believe that this sample is important for understanding the future Internet user population.",
        "published": "1999-04-18T19:04:58Z",
        "link": "http://arxiv.org/abs/cs/9904010v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "K.4.1"
        ]
    },
    {
        "title": "Human-Computer Conversation",
        "authors": [
            "Yorick Wilks",
            "Roberta Catizone"
        ],
        "summary": "The article surveys a little of the history of the technology, sets out the main current theoretical approaches in brief, and discusses the on-going opposition between theoretical and empirical approaches. It illustrates the situation with some discussion of CONVERSE, a system that won the Loebner prize in 1997 and which displays features of both approaches.",
        "published": "1999-06-25T11:44:42Z",
        "link": "http://arxiv.org/abs/cs/9906027v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "I.2.7;H.1.2"
        ]
    },
    {
        "title": "Representing Scholarly Claims in Internet Digital Libraries: A Knowledge   Modelling Approach",
        "authors": [
            "Simon Buckingham Shum",
            "Enrico Motta",
            "John Domingue"
        ],
        "summary": "This paper is concerned with tracking and interpreting scholarly documents in distributed research communities. We argue that current approaches to document description, and current technological infrastructures particularly over the World Wide Web, provide poor support for these tasks. We describe the design of a digital library server which will enable authors to submit a summary of the contributions they claim their documents makes, and its relations to the literature. We describe a knowledge-based Web environment to support the emergence of such a community-constructed semantic hypertext, and the services it could provide to assist the interpretation of an idea or document in the context of its literature. The discussion considers in detail how the approach addresses usability issues associated with knowledge structuring environments.",
        "published": "1999-08-19T09:51:29Z",
        "link": "http://arxiv.org/abs/cs/9908015v1",
        "categories": [
            "cs.DL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.3.7; H.1.2; H5.2; H.5.4; I.2.4; I.7.4"
        ]
    }
]