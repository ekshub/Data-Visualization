[
    {
        "title": "Cyborg Systems as Platforms for Computer-Vision Algorithm-Development   for Astrobiology",
        "authors": [
            "Patrick C. McGuire",
            "J. A. Rodriguez-Manfredi",
            "E. Sebastian-Martinez",
            "J. Gomez-Elvira",
            "E. Diaz-Martinez",
            "J. Ormo",
            "K. Neuffer",
            "A. Giaquinta",
            "F. Camps-Martinez",
            "A. Lepinette-Malvitte",
            "J. Perez-Mercader",
            "H. Ritter",
            "M. Oesker",
            "J. Ontrup",
            "J. Walter"
        ],
        "summary": "Employing the allegorical imagery from the film \"The Matrix\", we motivate and discuss our `Cyborg Astrobiologist' research program. In this research program, we are using a wearable computer and video camcorder in order to test and train a computer-vision system to be a field-geologist and field-astrobiologist.",
        "published": "2004-01-02T12:39:15Z",
        "link": "http://arxiv.org/abs/cs/0401004v2",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "I.4.0; I.4.6; I.4.8; I.4.9; I.5.4; I.5.5; J.2; I.2.5; I.2.10"
        ]
    },
    {
        "title": "Unifying Computing and Cognition: The SP Theory and its Applications",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This book develops the conjecture that all kinds of information processing in computers and in brains may usefully be understood as \"information compression by multiple alignment, unification and search\". This \"SP theory\", which has been under development since 1987, provides a unified view of such things as the workings of a universal Turing machine, the nature of 'knowledge', the interpretation and production of natural language, pattern recognition and best-match information retrieval, several kinds of probabilistic reasoning, planning and problem solving, unsupervised learning, and a range of concepts in mathematics and logic. The theory also provides a basis for the design of an 'SP' computer with several potential advantages compared with traditional digital computers.",
        "published": "2004-01-13T16:16:07Z",
        "link": "http://arxiv.org/abs/cs/0401009v1",
        "categories": [
            "cs.AI",
            "F.1.0; I.2.0"
        ]
    },
    {
        "title": "Corollaries on the fixpoint completion: studying the stable semantics by   means of the Clark completion",
        "authors": [
            "Pascal Hitzler"
        ],
        "summary": "The fixpoint completion fix(P) of a normal logic program P is a program transformation such that the stable models of P are exactly the models of the Clark completion of fix(P). This is well-known and was studied by Dung and Kanchanasut (1989). The correspondence, however, goes much further: The Gelfond-Lifschitz operator of P coincides with the immediate consequence operator of fix(P), as shown by Wendt (2002), and even carries over to standard operators used for characterizing the well-founded and the Kripke-Kleene semantics. We will apply this knowledge to the study of the stable semantics, and this will allow us to almost effortlessly derive new results concerning fixed-point and metric-based semantics, and neural-symbolic integration.",
        "published": "2004-02-09T11:03:20Z",
        "link": "http://arxiv.org/abs/cs/0402013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Self-Organising Networks for Classification: developing Applications to   Science Analysis for Astroparticle Physics",
        "authors": [
            "A. De Angelis",
            "P. Boinee",
            "M. Frailis",
            "E. Milotti"
        ],
        "summary": "Physics analysis in astroparticle experiments requires the capability of recognizing new phenomena; in order to establish what is new, it is important to develop tools for automatic classification, able to compare the final result with data from different detectors. A typical example is the problem of Gamma Ray Burst detection, classification, and possible association to known sources: for this task physicists will need in the next years tools to associate data from optical databases, from satellite experiments (EGRET, GLAST), and from Cherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS).",
        "published": "2004-02-09T19:44:33Z",
        "link": "http://arxiv.org/abs/cs/0402014v1",
        "categories": [
            "cs.NE",
            "astro-ph",
            "cs.AI",
            "I.5.1; I.5.3"
        ]
    },
    {
        "title": "The Munich Rent Advisor: A Success for Logic Programming on the Internet",
        "authors": [
            "Thom Fruehwirth",
            "Slim Abdennadher"
        ],
        "summary": "Most cities in Germany regularly publish a booklet called the {\\em Mietspiegel}. It basically contains a verbal description of an expert system. It allows the calculation of the estimated fair rent for a flat. By hand, one may need a weekend to do so. With our computerized version, the {\\em Munich Rent Advisor}, the user just fills in a form in a few minutes and the rent is calculated immediately. We also extended the functionality and applicability of the {\\em Mietspiegel} so that the user need not answer all questions on the form. The key to computing with partial information using high-level programming was to use constraint logic programming. We rely on the internet, and more specifically the World Wide Web, to provide this service to a broad user group. More than ten thousand people have used our service in the last three years. This article describes the experiences in implementing and using the {\\em Munich Rent Advisor}. Our results suggests that logic programming with constraints can be an important ingredient in intelligent internet systems.",
        "published": "2004-02-10T15:10:36Z",
        "link": "http://arxiv.org/abs/cs/0402019v1",
        "categories": [
            "cs.AI",
            "cs.DS",
            "D.3.3;G.1.6"
        ]
    },
    {
        "title": "Parameter-less hierarchical BOA",
        "authors": [
            "Martin Pelikan",
            "Tz-Kai Lin"
        ],
        "summary": "The parameter-less hierarchical Bayesian optimization algorithm (hBOA) enables the use of hBOA without the need for tuning parameters for solving each problem instance. There are three crucial parameters in hBOA: (1) the selection pressure, (2) the window size for restricted tournaments, and (3) the population size. Although both the selection pressure and the window size influence hBOA performance, performance should remain low-order polynomial with standard choices of these two parameters. However, there is no standard population size that would work for all problems of interest and the population size must thus be eliminated in a different way. To eliminate the population size, the parameter-less hBOA adopts the population-sizing technique of the parameter-less genetic algorithm. Based on the existing theory, the parameter-less hBOA should be able to solve nearly decomposable and hierarchical problems in quadratic or subquadratic number of function evaluations without the need for setting any parameters whatsoever. A number of experiments are presented to verify scalability of the parameter-less hBOA.",
        "published": "2004-02-15T06:56:45Z",
        "link": "http://arxiv.org/abs/cs/0402031v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G.1.6; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Computational complexity and simulation of rare events of Ising spin   glasses",
        "authors": [
            "Martin Pelikan",
            "Jiri Ocenasek",
            "Simon Trebst",
            "Matthias Troyer",
            "Fabien Alet"
        ],
        "summary": "We discuss the computational complexity of random 2D Ising spin glasses, which represent an interesting class of constraint satisfaction problems for black box optimization. Two extremal cases are considered: (1) the +/- J spin glass, and (2) the Gaussian spin glass. We also study a smooth transition between these two extremal cases. The computational complexity of all studied spin glass systems is found to be dominated by rare events of extremely hard spin glass samples. We show that complexity of all studied spin glass systems is closely related to Frechet extremal value distribution. In a hybrid algorithm that combines the hierarchical Bayesian optimization algorithm (hBOA) with a deterministic bit-flip hill climber, the number of steps performed by both the global searcher (hBOA) and the local searcher follow Frechet distributions. Nonetheless, unlike in methods based purely on local search, the parameters of these distributions confirm good scalability of hBOA with local search. We further argue that standard performance measures for optimization algorithms--such as the average number of evaluations until convergence--can be misleading. Finally, our results indicate that for highly multimodal constraint satisfaction problems, such as Ising spin glasses, recombination-based search can provide qualitatively better results than mutation-based search.",
        "published": "2004-02-15T06:58:09Z",
        "link": "http://arxiv.org/abs/cs/0402030v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G.1.6; I.2.8; I.2.6; J.2"
        ]
    },
    {
        "title": "Fitness inheritance in the Bayesian optimization algorithm",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry"
        ],
        "summary": "This paper describes how fitness inheritance can be used to estimate fitness for a proportion of newly sampled candidate solutions in the Bayesian optimization algorithm (BOA). The goal of estimating fitness for some candidate solutions is to reduce the number of fitness evaluations for problems where fitness evaluation is expensive. Bayesian networks used in BOA to model promising solutions and generate the new ones are extended to allow not only for modeling and sampling candidate solutions, but also for estimating their fitness. The results indicate that fitness inheritance is a promising concept in BOA, because population-sizing requirements for building appropriate models of promising solutions lead to good fitness estimates even if only a small proportion of candidate solutions is evaluated using the actual fitness function. This can lead to a reduction of the number of actual fitness evaluations by a factor of 30 or more.",
        "published": "2004-02-15T07:40:45Z",
        "link": "http://arxiv.org/abs/cs/0402032v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Recycling Computed Answers in Rewrite Systems for Abduction",
        "authors": [
            "Fangzhen Lin",
            "Jia-Huai You"
        ],
        "summary": "In rule-based systems, goal-oriented computations correspond naturally to the possible ways that an observation may be explained. In some applications, we need to compute explanations for a series of observations with the same domain. The question whether previously computed answers can be recycled arises. A yes answer could result in substantial savings of repeated computations. For systems based on classic logic, the answer is YES. For nonmonotonic systems however, one tends to believe that the answer should be NO, since recycling is a form of adding information. In this paper, we show that computed answers can always be recycled, in a nontrivial way, for the class of rewrite procedures that we proposed earlier for logic programs with negation. We present some experimental results on an encoding of the logistics domain.",
        "published": "2004-02-16T06:15:05Z",
        "link": "http://arxiv.org/abs/cs/0402033v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Memory As A Monadic Control Construct In Problem-Solving",
        "authors": [
            "Jean-Marie Chauvet"
        ],
        "summary": "Recent advances in programming languages study and design have established a standard way of grounding computational systems representation in category theory. These formal results led to a better understanding of issues of control and side-effects in functional and imperative languages. This framework can be successfully applied to the investigation of the performance of Artificial Intelligence (AI) inference and cognitive systems. In this paper, we delineate a categorical formalisation of memory as a control structure driving performance in inference systems. Abstracting away control mechanisms from three widely used representations of memory in cognitive systems (scripts, production rules and clusters) we explain how categorical triples capture the interaction between learning and problem-solving.",
        "published": "2004-02-16T17:02:29Z",
        "link": "http://arxiv.org/abs/cs/0402035v1",
        "categories": [
            "cs.AI",
            "6Q655"
        ]
    },
    {
        "title": "The Complexity of Modified Instances",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "In this paper we study the complexity of solving a problem when a solution of a similar instance is known. This problem is relevant whenever instances may change from time to time, and known solutions may not remain valid after the change. We consider two scenarios: in the first one, what is known is only a solution of the problem before the change; in the second case, we assume that some additional information, found during the search for this solution, is also known. In the first setting, the techniques from the theory of NP-completeness suffice to show complexity results. In the second case, negative results can only be proved using the techniques of compilability, and are often related to the size of considered changes.",
        "published": "2004-02-23T18:08:48Z",
        "link": "http://arxiv.org/abs/cs/0402053v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.1.3; I.2.8"
        ]
    },
    {
        "title": "Integrating Defeasible Argumentation and Machine Learning Techniques",
        "authors": [
            "Sergio Alejandro Gomez",
            "Carlos Ivan Chesñevar"
        ],
        "summary": "The field of machine learning (ML) is concerned with the question of how to construct algorithms that automatically improve with experience. In recent years many successful ML applications have been developed, such as datamining programs, information-filtering systems, etc. Although ML algorithms allow the detection and extraction of interesting patterns of data for several kinds of problems, most of these algorithms are based on quantitative reasoning, as they rely on training data in order to infer so-called target functions.   In the last years defeasible argumentation has proven to be a sound setting to formalize common-sense qualitative reasoning. This approach can be combined with other inference techniques, such as those provided by machine learning theory.   In this paper we outline different alternatives for combining defeasible argumentation and machine learning techniques. We suggest how different aspects of a generic argument-based framework can be integrated with other ML-based approaches.",
        "published": "2004-02-25T18:02:29Z",
        "link": "http://arxiv.org/abs/cs/0402057v2",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Evolving a Stigmergic Self-Organized Data-Mining",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "summary": "Self-organizing complex systems typically are comprised of a large number of frequently similar components or events. Through their process, a pattern at the global-level of a system emerges solely from numerous interactions among the lower-level components of the system. Moreover, the rules specifying interactions among the system's components are executed using only local information, without reference to the global pattern, which, as in many real-world problems is not easily accessible or possible to be found. Stigmergy, a kind of indirect communication and learning by the environment found in social insects is a well know example of self-organization, providing not only vital clues in order to understand how the components can interact to produce a complex pattern, as can pinpoint simple biological non-linear rules and methods to achieve improved artificial intelligent adaptive categorization systems, critical for Data-Mining. On the present work it is our intention to show that a new type of Data-Mining can be designed based on Stigmergic paradigms, taking profit of several natural features of this phenomenon. By hybridizing bio-inspired Swarm Intelligence with Evolutionary Computation we seek for an entire distributed, adaptive, collective and cooperative self-organized Data-Mining. As a real-world, real-time test bed for our proposal, World-Wide-Web Mining will be used. Having that purpose in mind, Web usage Data was collected from the Monash University's Web site (Australia), with over 7 million hits every week. Results are compared to other recent systems, showing that the system presented is by far promising.",
        "published": "2004-02-28T23:50:45Z",
        "link": "http://arxiv.org/abs/cs/0403001v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "I.2.11"
        ]
    },
    {
        "title": "Epistemic Foundation of Stable Model Semantics",
        "authors": [
            "Y. Loyer",
            "U. Straccia"
        ],
        "summary": "Stable model semantics has become a very popular approach for the management of negation in logic programming. This approach relies mainly on the closed world assumption to complete the available knowledge and its formulation has its basis in the so-called Gelfond-Lifschitz transformation.   The primary goal of this work is to present an alternative and epistemic-based characterization of stable model semantics, to the Gelfond-Lifschitz transformation. In particular, we show that stable model semantics can be defined entirely as an extension of the Kripke-Kleene semantics. Indeed, we show that the closed world assumption can be seen as an additional source of `falsehood' to be added cumulatively to the Kripke-Kleene semantics. Our approach is purely algebraic and can abstract from the particular formalism of choice as it is based on monotone operators (under the knowledge order) over bilattices only.",
        "published": "2004-03-02T15:45:29Z",
        "link": "http://arxiv.org/abs/cs/0403002v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The role of behavior modifiers in representation development",
        "authors": [
            "Carlos R. de la Mora B.",
            "Carlos Gershenson",
            "Angelica Garcia-Vega"
        ],
        "summary": "We address the problem of the development of representations and their relationship to the environment. We study a software agent which develops in a network a representation of its simple environment which captures and integrates the relationships between agent and environment through a closure mechanism. The inclusion of a variable behavior modifier allows better representation development. This can be confirmed with an internal description of the closure mechanism, and with an external description of the properties of the representation network.",
        "published": "2004-03-05T12:53:57Z",
        "link": "http://arxiv.org/abs/cs/0403006v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Demolishing Searle's Chinese Room",
        "authors": [
            "Wolfram Schmied"
        ],
        "summary": "Searle's Chinese Room argument is refuted by showing that he has actually given two different versions of the room, which fail for different reasons. Hence, Searle does not achieve his stated goal of showing ``that a system could have input and output capabilities that duplicated those of a native Chinese speaker and still not understand Chinese''.",
        "published": "2004-03-08T17:50:32Z",
        "link": "http://arxiv.org/abs/cs/0403009v2",
        "categories": [
            "cs.AI",
            "cs.GL",
            "I.2.0"
        ]
    },
    {
        "title": "A Comparative Study of Arithmetic Constraints on Integer Intervals",
        "authors": [
            "Krzysztof R. Apt",
            "Peter Zoeteweij"
        ],
        "summary": "We propose here a number of approaches to implement constraint propagation for arithmetic constraints on integer intervals. To this end we introduce integer interval arithmetic. Each approach is explained using appropriate proof rules that reduce the variable domains. We compare these approaches using a set of benchmarks.",
        "published": "2004-03-12T08:37:54Z",
        "link": "http://arxiv.org/abs/cs/0403016v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3"
        ]
    },
    {
        "title": "Distribution of Mutual Information from Complete and Incomplete Data",
        "authors": [
            "Marcus Hutter",
            "Marco Zaffalon"
        ],
        "summary": "Mutual information is widely used, in a descriptive way, to measure the stochastic dependence of categorical random variables. In order to address questions such as the reliability of the descriptive value, one must consider sample-to-population inferential approaches. This paper deals with the posterior distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean, and analytical approximations for the variance, skewness and kurtosis are derived. These approximations have a guaranteed accuracy level of the order O(1/n^3), where n is the sample size. Leading order approximations for the mean and the variance are derived in the case of incomplete samples. The derived analytical expressions allow the distribution of mutual information to be approximated reliably and quickly. In fact, the derived expressions can be computed with the same order of complexity needed for descriptive mutual information. This makes the distribution of mutual information become a concrete alternative to descriptive mutual information in many applications which would benefit from moving to the inductive side. Some of these prospective applications are discussed, and one of them, namely feature selection, is shown to perform significantly better when inductive mutual information is used.",
        "published": "2004-03-15T16:33:55Z",
        "link": "http://arxiv.org/abs/cs/0403025v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2"
        ]
    },
    {
        "title": "Intelligent encoding and economical communication in the visual stream",
        "authors": [
            "Andras Lorincz"
        ],
        "summary": "The theory of computational complexity is used to underpin a recent model of neocortical sensory processing. We argue that encoding into reconstruction networks is appealing for communicating agents using Hebbian learning and working on hard combinatorial problems, which are easy to verify. Computational definition of the concept of intelligence is provided. Simulations illustrate the idea.",
        "published": "2004-03-16T14:57:29Z",
        "link": "http://arxiv.org/abs/q-bio/0403022v1",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.CC",
            "nlin.AO"
        ]
    },
    {
        "title": "Where Fail-Safe Default Logics Fail",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "Reiter's original definition of default logic allows for the application of a default that contradicts a previously applied one. We call failure this condition. The possibility of generating failures has been in the past considered as a semantical problem, and variants have been proposed to solve it. We show that it is instead a computational feature that is needed to encode some domains into default logic.",
        "published": "2004-03-19T15:20:54Z",
        "link": "http://arxiv.org/abs/cs/0403032v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Concept of E-machine: How does a \"dynamical\" brain learn to process   \"symbolic\" information? Part I",
        "authors": [
            "Victor Eliashberg"
        ],
        "summary": "The human brain has many remarkable information processing characteristics that deeply puzzle scientists and engineers. Among the most important and the most intriguing of these characteristics are the brain's broad universality as a learning system and its mysterious ability to dynamically change (reconfigure) its behavior depending on a combinatorial number of different contexts.   This paper discusses a class of hypothetically brain-like dynamically reconfigurable associative learning systems that shed light on the possible nature of these brain's properties. The systems are arranged on the general principle referred to as the concept of E-machine.   The paper addresses the following questions:   1. How can \"dynamical\" neural networks function as universal programmable \"symbolic\" machines?   2. What kind of a universal programmable symbolic machine can form arbitrarily complex software in the process of programming similar to the process of biological associative learning?   3. How can a universal learning machine dynamically reconfigure its software depending on a combinatorial number of possible contexts?",
        "published": "2004-03-19T17:13:55Z",
        "link": "http://arxiv.org/abs/cs/0403031v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.0"
        ]
    },
    {
        "title": "Tournament versus Fitness Uniform Selection",
        "authors": [
            "Shane Legg",
            "Marcus Hutter",
            "Akshat Kumar"
        ],
        "summary": "In evolutionary algorithms a critical parameter that must be tuned is that of selection pressure. If it is set too low then the rate of convergence towards the optimum is likely to be slow. Alternatively if the selection pressure is set too high the system is likely to become stuck in a local optimum due to a loss of diversity in the population. The recent Fitness Uniform Selection Scheme (FUSS) is a conceptually simple but somewhat radical approach to addressing this problem - rather than biasing the selection towards higher fitness, FUSS biases selection towards sparsely populated fitness levels. In this paper we compare the relative performance of FUSS with the well known tournament selection scheme on a range of problems.",
        "published": "2004-03-23T15:17:53Z",
        "link": "http://arxiv.org/abs/cs/0403038v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2; I.2.6; I.2.8; F.2"
        ]
    },
    {
        "title": "A Flexible Rule Compiler for Speech Synthesis",
        "authors": [
            "Wojciech Skut",
            "Stefan Ulrich",
            "Kathrine Hammervold"
        ],
        "summary": "We present a flexible rule compiler developed for a text-to-speech (TTS) system. The compiler converts a set of rules into a finite-state transducer (FST). The input and output of the FST are subject to parameterization, so that the system can be applied to strings and sequences of feature-structures. The resulting transducer is guaranteed to realize a function (as opposed to a relation), and therefore can be implemented as a deterministic device (either a deterministic FST or a bimachine).",
        "published": "2004-03-23T21:06:11Z",
        "link": "http://arxiv.org/abs/cs/0403039v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.5.2; F.4.3"
        ]
    },
    {
        "title": "Protocol Requirements for Self-organizing Artifacts: Towards an Ambient   Intelligence",
        "authors": [
            "Carlos Gershenson",
            "Francis Heylighen"
        ],
        "summary": "We discuss which properties common-use artifacts should have to collaborate without human intervention. We conceive how devices, such as mobile phones, PDAs, and home appliances, could be seamlessly integrated to provide an \"ambient intelligence\" that responds to the user's desires without requiring explicit programming or commands. While the hardware and software technology to build such systems already exists, as yet there is no standard protocol that can learn new meanings. We propose the first steps in the development of such a protocol, which would need to be adaptive, extensible, and open to the community, while promoting self-organization. We argue that devices, interacting through \"game-like\" moves, can learn to agree about how to communicate, with whom to cooperate, and how to delegate and coordinate specialized tasks. Thus, they may evolve a distributed cognition or collective intelligence capable of tackling complex tasks.",
        "published": "2004-04-01T16:14:54Z",
        "link": "http://arxiv.org/abs/nlin/0404004v2",
        "categories": [
            "nlin.AO",
            "cs.AI"
        ]
    },
    {
        "title": "Parametric external predicates for the DLV System",
        "authors": [
            "G. Ianni",
            "F. Calimeri",
            "A. Pietramala",
            "M. C. Santoro"
        ],
        "summary": "This document describes syntax, semantics and implementation guidelines in order to enrich the DLV system with the possibility to make external C function calls. This feature is realized by the introduction of parametric external predicates, whose extension is not specified through a logic program but implicitly computed through external code.",
        "published": "2004-04-05T17:15:45Z",
        "link": "http://arxiv.org/abs/cs/0404011v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Toward the Implementation of Functions in the DLV System (Preliminary   Technical Report)",
        "authors": [
            "Francesco Calimeri",
            "Nicola Leone"
        ],
        "summary": "This document describes the functions as they are treated in the DLV system. We give first the language, then specify the main implementation issues.",
        "published": "2004-04-05T17:23:07Z",
        "link": "http://arxiv.org/abs/cs/0404012v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "NLML--a Markup Language to Describe the Unlimited English Grammar",
        "authors": [
            "Jiyou Jia"
        ],
        "summary": "In this paper we present NLML (Natural Language Markup Language), a markup language to describe the syntactic and semantic structure of any grammatically correct English expression. At first the related works are analyzed to demonstrate the necessity of the NLML: simple form, easy management and direct storage. Then the description of the English grammar with NLML is introduced in details in three levels: sentences (with different complexities, voices, moods, and tenses), clause (relative clause and noun clause) and phrase (noun phrase, verb phrase, prepositional phrase, adjective phrase, adverb phrase and predicate phrase). At last the application fields of the NLML in NLP are shown with two typical examples: NLOJM (Natural Language Object Modal in Java) and NLDB (Natural Language Database).",
        "published": "2004-04-07T06:54:45Z",
        "link": "http://arxiv.org/abs/cs/0404018v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Computability Logic: a formal theory of interaction",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic is a formal theory of (interactive) computability in the same sense as classical logic is a formal theory of truth. This approach was initiated very recently in \"Introduction to computability logic\" (Annals of Pure and Applied Logic 123 (2003), pp.1-99). The present paper reintroduces computability logic in a more compact and less technical way. It is written in a semitutorial style with a general computer science, logic or mathematics audience in mind. An Internet source on the subject is available at http://www.cis.upenn.edu/~giorgi/cl.html, and additional material at http://www.csc.villanova.edu/~japaridz/CL/gsoll.html .",
        "published": "2004-04-09T01:42:00Z",
        "link": "http://arxiv.org/abs/cs/0404024v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "XML framework for concept description and knowledge representation",
        "authors": [
            "Andreas de Vries"
        ],
        "summary": "An XML framework for concept description is given, based upon the fact that the tree structure of XML implies the logical structure of concepts as defined by attributional calculus. Especially, the attribute-value representation is implementable in the XML framework. Since the attribute-value representation is an important way to represent knowledge in AI, the framework offers a further and simpler way than the powerful RDF technology.",
        "published": "2004-04-14T15:36:43Z",
        "link": "http://arxiv.org/abs/cs/0404030v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.7.2; E.2; H.1.1; G.2.3"
        ]
    },
    {
        "title": "When Do Differences Matter? On-Line Feature Extraction Through Cognitive   Economy",
        "authors": [
            "David J. Finton"
        ],
        "summary": "For an intelligent agent to be truly autonomous, it must be able to adapt its representation to the requirements of its task as it interacts with the world. Most current approaches to on-line feature extraction are ad hoc; in contrast, this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy. The algorithm incorporates an active form of Q-learning, and partitions continuous state-spaces by merging and splitting Voronoi regions. The experiments illustrate a new methodology for testing and comparing representations by means of learning curves. Results from the puck-on-a-hill task demonstrate the algorithm's ability to learn effective representations, superior to those produced by some other, well-known, methods.",
        "published": "2004-04-15T02:59:10Z",
        "link": "http://arxiv.org/abs/cs/0404032v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "I.2.6; I.2.4; I.2.8"
        ]
    },
    {
        "title": "2-Sat Sub-Clauses and the Hypernodal Structure of the 3-Sat Problem",
        "authors": [
            "D. B. Powell"
        ],
        "summary": "Like simpler graphs, nested (hypernodal) graphs consist of two components: a set of nodes and a set of edges, where each edge connects a pair of nodes. In the hypernodal graph model, however, a node may contain other graphs, so that a node may be contained in a graph that it contains. The inherently recursive structure of the hypernodal graph model aptly characterizes both the structure and dynamic of the 3-sat problem, a broadly applicable, though intractable, computer science problem. In this paper I first discuss the structure of the 3-sat problem, analyzing the relation of 3-sat to 2-sat, a related, though tractable problem. I then discuss sub-clauses and sub-clause thresholds and the transformation of sub-clauses into implication graphs, demonstrating how combinations of implication graphs are equivalent to hypernodal graphs. I conclude with a brief discussion of the use of hypernodal graphs to model the 3-sat problem, illustrating how hypernodal graphs model both the conditions for satisfiability and the process by which particular 3-sat assignments either succeed or fail.",
        "published": "2004-04-20T08:23:43Z",
        "link": "http://arxiv.org/abs/cs/0404038v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "G.2.1; G.2.2"
        ]
    },
    {
        "title": "Speculation on graph computation architectures and computing via   synchronization",
        "authors": [
            "Bayle Shanks"
        ],
        "summary": "A speculative overview of a future topic of research. The paper is a collection of ideas concerning two related areas:   1) Graph computation machines (\"computing with graphs\"). This is the class of models of computation in which the state of the computation is represented as a graph or network.   2) Arc-based neural networks, which store information not as activation in the nodes, but rather by adding and deleting arcs. Sometimes the arcs may be interpreted as synchronization.   Warnings to readers: this is not the sort of thing that one might submit to a journal or conference. No proofs are presented. The presentation is informal, and written at an introductory level. You'll probably want to wait for a more concise presentation.",
        "published": "2004-04-22T07:29:19Z",
        "link": "http://arxiv.org/abs/cs/0404045v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.1.1; J.3; I.2.m"
        ]
    },
    {
        "title": "Exploiting Cross-Document Relations for Multi-document Evolving   Summarization",
        "authors": [
            "Stergos D. Afantenos",
            "Irene Doura",
            "Eleni Kapellou",
            "Vangelis Karkaletsis"
        ],
        "summary": "This paper presents a methodology for summarization from multiple documents which are about a specific topic. It is based on the specification and identification of the cross-document relations that occur among textual elements within those documents. Our methodology involves the specification of the topic-specific entities, the messages conveyed for the specific entities by certain textual elements and the specification of the relations that can hold among these messages. The above resources are necessary for setting up a specific topic for our query-based summarization approach which uses these resources to identify the query-specific messages within the documents and the query-specific relations that connect these messages across documents.",
        "published": "2004-04-23T10:44:24Z",
        "link": "http://arxiv.org/abs/cs/0404049v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Knowledge And The Action Description Language A",
        "authors": [
            "Jorge Lobo",
            "Gisela Mendez",
            "Stuart R. Taylor"
        ],
        "summary": "We introduce Ak, an extension of the action description language A (Gelfond and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing actions to increase an agent's knowledge of the world and non-deterministic actions to remove knowledge. We include complex plans involving conditionals and loops in our query language for hypothetical reasoning. We also present a translation of Ak domain descriptions into epistemic logic programs.",
        "published": "2004-04-24T14:16:04Z",
        "link": "http://arxiv.org/abs/cs/0404051v1",
        "categories": [
            "cs.AI",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Convergence of Discrete MDL for Sequential Prediction",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We study the properties of the Minimum Description Length principle for sequence prediction, considering a two-part MDL estimator which is chosen from a countable class of models. This applies in particular to the important case of universal sequence prediction, where the model class corresponds to all algorithms for some fixed universal Turing machine (this correspondence is by enumerable semimeasures, hence the resulting models are stochastic). We prove convergence theorems similar to Solomonoff's theorem of universal induction, which also holds for general Bayes mixtures. The bound characterizing the convergence speed for MDL predictions is exponentially larger as compared to Bayes mixtures. We observe that there are at least three different ways of using MDL for prediction. One of these has worse prediction properties, for which predictions only converge if the MDL estimator stabilizes. We establish sufficient conditions for this to occur. Finally, some immediate consequences for complexity relations and randomness criteria are proven.",
        "published": "2004-04-28T15:58:35Z",
        "link": "http://arxiv.org/abs/cs/0404057v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.ST",
            "stat.TH",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Splitting an operator: Algebraic modularity results for logics with   fixpoint semantics",
        "authors": [
            "Joost Vennekens",
            "David Gilis",
            "Marc Denecker"
        ],
        "summary": "It is well known that, under certain conditions, it is possible to split logic programs under stable model semantics, i.e. to divide such a program into a number of different \"levels\", such that the models of the entire program can be constructed by incrementally constructing models for each level. Similar results exist for other non-monotonic formalisms, such as auto-epistemic logic and default logic. In this work, we present a general, algebraicsplitting theory for logics with a fixpoint semantics. Together with the framework of approximation theory, a general fixpoint theory for arbitrary operators, this gives us a uniform and powerful way of deriving splitting results for each logic with a fixpoint semantics. We demonstrate the usefulness of these results, by generalizing existing results for logic programming, auto-epistemic logic and default logic.",
        "published": "2004-05-03T09:05:14Z",
        "link": "http://arxiv.org/abs/cs/0405002v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Quantum Computers",
        "authors": [
            "Archil Avaliani"
        ],
        "summary": "This research paper gives an overview of quantum computers - description of their operation, differences between quantum and silicon computers, major construction problems of a quantum computer and many other basic aspects. No special scientific knowledge is necessary for the reader.",
        "published": "2004-05-03T20:25:00Z",
        "link": "http://arxiv.org/abs/cs/0405004v1",
        "categories": [
            "cs.AI",
            "cs.AR",
            "B.0; C.0; K.4.0; I.0"
        ]
    },
    {
        "title": "\"In vivo\" spam filtering: A challenge problem for data mining",
        "authors": [
            "Tom Fawcett"
        ],
        "summary": "Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email communication. Many data mining researchers have addressed the problem of detecting spam, generally by treating it as a static text classification problem. True in vivo spam filtering has characteristics that make it a rich and challenging domain for data mining. Indeed, real-world datasets with these characteristics are typically difficult to acquire and to share. This paper demonstrates some of these characteristics and argues that researchers should pursue in vivo spam filtering as an accessible domain for investigating them.",
        "published": "2004-05-04T18:56:09Z",
        "link": "http://arxiv.org/abs/cs/0405007v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "A Comparative Study of Fuzzy Classification Methods on Breast Cancer   Data",
        "authors": [
            "Ravi Jain",
            "Ajith Abraham"
        ],
        "summary": "In this paper, we examine the performance of four fuzzy rule generation methods on Wisconsin breast cancer data. The first method generates fuzzy if then rules using the mean and the standard deviation of attribute values. The second approach generates fuzzy if then rules using the histogram of attributes values. The third procedure generates fuzzy if then rules with certainty of each attribute into homogeneous fuzzy sets. In the fourth approach, only overlapping areas are partitioned. The first two approaches generate a single fuzzy if then rule for each class by specifying the membership function of each antecedent fuzzy set using the information about attribute values of training patterns. The other two approaches are based on fuzzy grids with homogeneous fuzzy partitions of each attribute. The performance of each approach is evaluated on breast cancer data sets. Simulation results show that the Modified grid approach has a high classification rate of 99.73 %.",
        "published": "2004-05-04T23:02:53Z",
        "link": "http://arxiv.org/abs/cs/0405008v1",
        "categories": [
            "cs.AI",
            "I.2.1"
        ]
    },
    {
        "title": "Intelligent Systems: Architectures and Perspectives",
        "authors": [
            "Ajith Abraham"
        ],
        "summary": "The integration of different learning and adaptation techniques to overcome individual limitations and to achieve synergetic effects through the hybridization or fusion of these techniques has, in recent years, contributed to a large number of new intelligent system designs. Computational intelligence is an innovative framework for constructing intelligent hybrid architectures involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic Reasoning (PR) and derivative free optimization techniques such as Evolutionary Computation (EC). Most of these hybridization approaches, however, follow an ad hoc design methodology, justified by success in certain application domains. Due to the lack of a common framework it often remains difficult to compare the various hybrid systems conceptually and to evaluate their performance comparatively. This chapter introduces the different generic architectures for integrating intelligent systems. The designing aspects and perspectives of different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC systems are presented. Some conclusions are also provided towards the end.",
        "published": "2004-05-04T23:48:39Z",
        "link": "http://arxiv.org/abs/cs/0405009v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "A Neuro-Fuzzy Approach for Modelling Electricity Demand in Victoria",
        "authors": [
            "Ajith Abraham",
            "Baikunth Nath"
        ],
        "summary": "Neuro-fuzzy systems have attracted growing interest of researchers in various scientific and engineering areas due to the increasing need of intelligent systems. This paper evaluates the use of two popular soft computing techniques and conventional statistical approach based on Box--Jenkins autoregressive integrated moving average (ARIMA) model to predict electricity demand in the State of Victoria, Australia. The soft computing methods considered are an evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN) trained using scaled conjugate gradient algorithm (CGA) and backpropagation (BP) algorithm. The forecast accuracy is compared with the forecasts used by Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we considered load demand patterns for 10 consecutive months taken every 30 min for training the different prediction models. Test results show that the neuro-fuzzy system performed better than neural networks, ARIMA model and the VPX forecasts.",
        "published": "2004-05-05T00:27:53Z",
        "link": "http://arxiv.org/abs/cs/0405010v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques",
        "authors": [
            "Ajith Abraham"
        ],
        "summary": "Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS) have attracted the growing interest of researchers in various scientific and engineering areas due to the growing need of adaptive intelligent systems to solve the real world problems. ANN learns from scratch by adjusting the interconnections between layers. FIS is a popular computing framework based on the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The advantages of a combination of ANN and FIS are obvious. There are several approaches to integrate ANN and FIS and very often it depends on the application. We broadly classify the integration of ANN and FIS into three categories namely concurrent model, cooperative model and fully fused model. This paper starts with a discussion of the features of each model and generalize the advantages and deficiencies of each model. We further focus the review on the different types of fused neuro-fuzzy systems and citing the advantages and disadvantages of each model.",
        "published": "2004-05-05T00:32:52Z",
        "link": "http://arxiv.org/abs/cs/0405011v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Is Neural Network a Reliable Forecaster on Earth? A MARS Query!",
        "authors": [
            "Ajith Abraham",
            "Dan Steinberg"
        ],
        "summary": "Long-term rainfall prediction is a challenging task especially in the modern world where we are facing the major environmental problem of global warming. In general, climate and rainfall are highly non-linear phenomena in nature exhibiting what is known as the butterfly effect. While some regions of the world are noticing a systematic decrease in annual rainfall, others notice increases in flooding and severe storms. The global nature of this phenomenon is very complicated and requires sophisticated computer modeling and simulation to predict accurately. In this paper, we report a performance analysis for Multivariate Adaptive Regression Splines (MARS)and artificial neural networks for one month ahead prediction of rainfall. To evaluate the prediction efficiency, we made use of 87 years of rainfall data in Kerala state, the southern part of the Indian peninsula situated at latitude -longitude pairs (8o29'N - 76o57' E). We used an artificial neural network trained using the scaled conjugate gradient algorithm. The neural network and MARS were trained with 40 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that MARS is a good forecasting tool and performed better than the considered neural network.",
        "published": "2004-05-05T00:36:17Z",
        "link": "http://arxiv.org/abs/cs/0405012v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "DCT Based Texture Classification Using Soft Computing Approach",
        "authors": [
            "Golam Sorwar",
            "Ajith Abraham"
        ],
        "summary": "Classification of texture pattern is one of the most important problems in pattern recognition. In this paper, we present a classification method based on the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works on gray level image, the color scheme of each image is transformed into gray levels. For classifying the images using DCT we used two popular soft computing techniques namely neurocomputing and neuro-fuzzy computing. We used a feedforward neural network trained using the backpropagation learning and an evolving fuzzy neural network to classify the textures. The soft computing models were trained using 80% of the texture data and remaining was used for testing and validation purposes. A performance comparison was made among the soft computing models for the texture classification problem. We also analyzed the effects of prolonged training of neural networks. It is observed that the proposed neuro-fuzzy model performed better than neural network.",
        "published": "2004-05-05T00:44:12Z",
        "link": "http://arxiv.org/abs/cs/0405013v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Estimating Genome Reversal Distance by Genetic Algorithm",
        "authors": [
            "Andy AuYeung",
            "Ajith Abraham"
        ],
        "summary": "Sorting by reversals is an important problem in inferring the evolutionary relationship between two genomes. The problem of sorting unsigned permutation has been proven to be NP-hard. The best guaranteed error bounded is the 3/2- approximation algorithm. However, the problem of sorting signed permutation can be solved easily. Fast algorithms have been developed both for finding the sorting sequence and finding the reversal distance of signed permutation. In this paper, we present a way to view the problem of sorting unsigned permutation as signed permutation. And the problem can then be seen as searching an optimal signed permutation in all n2 corresponding signed permutations. We use genetic algorithm to conduct the search. Our experimental result shows that the proposed method outperform the 3/2-approximation algorithm.",
        "published": "2004-05-05T00:57:34Z",
        "link": "http://arxiv.org/abs/cs/0405014v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Intrusion Detection Systems Using Adaptive Regression Splines",
        "authors": [
            "Srinivas Mukkamala",
            "Andrew H. Sung",
            "Ajith Abraham",
            "Vitorino Ramos"
        ],
        "summary": "Past few years have witnessed a growing recognition of intelligent techniques for the construction of efficient and reliable intrusion detection systems. Due to increasing incidents of cyber attacks, building effective intrusion detection systems (IDS) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. In this paper, we report a performance analysis between Multivariate Adaptive Regression Splines (MARS), neural networks and support vector machines. The MARS procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. A brief comparison of different neural network learning algorithms is also given.",
        "published": "2004-05-05T02:22:16Z",
        "link": "http://arxiv.org/abs/cs/0405016v1",
        "categories": [
            "cs.AI",
            "C.2.0"
        ]
    },
    {
        "title": "Data Mining Approach for Analyzing Call Center Performance",
        "authors": [
            "Marcin Paprzycki",
            "Ajith Abraham",
            "Ruiyuan Guo"
        ],
        "summary": "The aim of our research was to apply well-known data mining techniques (such as linear neural networks, multi-layered perceptrons, probabilistic neural networks, classification and regression trees, support vector machines and finally a hybrid decision tree neural network approach) to the problem of predicting the quality of service in call centers; based on the performance data actually collected in a call center of a large insurance company. Our aim was two-fold. First, to compare the performance of models built using the above-mentioned techniques and, second, to analyze the characteristics of the input sensitivity in order to better understand the relationship between the perform-ance evaluation process and the actual performance and in this way help improve the performance of call centers. In this paper we summarize our findings.",
        "published": "2004-05-05T02:27:43Z",
        "link": "http://arxiv.org/abs/cs/0405017v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Modeling Chaotic Behavior of Stock Indices Using Intelligent Paradigms",
        "authors": [
            "Ajith Abraham",
            "Ninan Sajith Philip",
            "P. Saratchandran"
        ],
        "summary": "The use of intelligent systems for stock market predictions has been widely established. In this paper, we investigate how the seemingly chaotic behavior of stock markets could be well represented using several connectionist paradigms and soft computing techniques. To demonstrate the different techniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4 year's NIFTY index values. This paper investigates the development of a reliable and efficient technique to model the seemingly chaotic behavior of stock markets. We considered an artificial neural network trained using Levenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper briefly explains how the different connectionist paradigms could be formulated using different learning methods and then investigates whether they can provide the required level of performance, which are sufficiently good and robust so as to provide a reliable forecast model for stock market indices. Experiment results reveal that all the connectionist paradigms considered could represent the stock indices behavior very accurately.",
        "published": "2004-05-05T02:38:25Z",
        "link": "http://arxiv.org/abs/cs/0405018v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Hybrid Fuzzy-Linear Programming Approach for Multi Criteria Decision   Making Problems",
        "authors": [
            "Sonja Petrovic-Lazarevic",
            "Ajith Abraham"
        ],
        "summary": "The purpose of this paper is to point to the usefulness of applying a linear mathematical formulation of fuzzy multiple criteria objective decision methods in organising business activities. In this respect fuzzy parameters of linear programming are modelled by preference-based membership functions. This paper begins with an introduction and some related research followed by some fundamentals of fuzzy set theory and technical concepts of fuzzy multiple objective decision models. Further a real case study of a manufacturing plant and the implementation of the proposed technique is presented. Empirical results clearly show the superiority of the fuzzy technique in optimising individual objective functions when compared to non-fuzzy approach. Furthermore, for the problem considered, the optimal solution helps to infer that by incorporating fuzziness in a linear programming model either in constraints, or both in objective functions and constraints, provides a similar (or even better) level of satisfaction for obtained results compared to non-fuzzy linear programming.",
        "published": "2004-05-05T02:44:41Z",
        "link": "http://arxiv.org/abs/cs/0405019v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Meta-Learning Evolutionary Artificial Neural Networks",
        "authors": [
            "Ajith Abraham"
        ],
        "summary": "In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial Neural Network), an automatic computational framework for the adaptive optimization of artificial neural networks wherein the neural network architecture, activation function, connection weights; learning algorithm and its parameters are adapted according to the problem. We explored the performance of MLEANN and conventionally designed artificial neural networks for function approximation problems. To evaluate the comparative performance, we used three different well-known chaotic time series. We also present the state of the art popular neural network learning algorithms and some experimentation results related to convergence speed and generalization performance. We explored the performance of backpropagation algorithm; conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt algorithm for the three chaotic time series. Performances of the different learning algorithms were evaluated when the activation functions and architecture were changed. We further present the theoretical background, algorithm, design strategy and further demonstrate how effective and inevitable is the proposed MLEANN framework to design a neural network, which is smaller, faster and with a better generalization performance.",
        "published": "2004-05-06T13:44:20Z",
        "link": "http://arxiv.org/abs/cs/0405024v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "The Largest Compatible Subset Problem for Phylogenetic Data",
        "authors": [
            "Andy Auyeung",
            "Ajith Abraham"
        ],
        "summary": "The phylogenetic tree construction is to infer the evolutionary relationship between species from the experimental data. However, the experimental data are often imperfect and conflicting each others. Therefore, it is important to extract the motif from the imperfect data. The largest compatible subset problem is that, given a set of experimental data, we want to discard the minimum such that the remaining is compatible. The largest compatible subset problem can be viewed as the vertex cover problem in the graph theory that has been proven to be NP-hard. In this paper, we propose a hybrid Evolutionary Computing (EC) method for this problem. The proposed method combines the EC approach and the algorithmic approach for special structured graphs. As a result, the complexity of the problem is dramatically reduced. Experiments were performed on randomly generated graphs with different edge densities. The vertex covers produced by the proposed method were then compared to the vertex covers produced by a 2-approximation algorithm. The experimental results showed that the proposed method consistently outperformed a classical 2- approximation algorithm. Furthermore, a significant improvement was found when the graph density was small.",
        "published": "2004-05-06T13:52:23Z",
        "link": "http://arxiv.org/abs/cs/0405025v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "A Concurrent Fuzzy-Neural Network Approach for Decision Support Systems",
        "authors": [
            "Cong Tran",
            "Ajith Abraham",
            "Lakhmi Jain"
        ],
        "summary": "Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing technologies that underlie the conception, design and utilization of intelligent systems. Several works have been done where engineers and scientists have applied intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a concurrent fuzzy-neural network approach combining unsupervised and supervised learning techniques to develop the Tactical Air Combat Decision Support System (TACDSS). Experiment results clearly demonstrate the efficiency of the proposed technique.",
        "published": "2004-05-06T13:58:41Z",
        "link": "http://arxiv.org/abs/cs/0405026v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Evolution of a Subsumption Architecture Neurocontroller",
        "authors": [
            "Julian Togelius"
        ],
        "summary": "An approach to robotics called layered evolution and merging features from the subsumption architecture into evolutionary robotics is presented, and its advantages are discussed. This approach is used to construct a layered controller for a simulated robot that learns which light source to approach in an environment with obstacles. The evolvability and performance of layered evolution on this task is compared to (standard) monolithic evolution, incremental and modularised evolution. To corroborate the hypothesis that a layered controller performs at least as well as an integrated one, the evolved layers are merged back into a single network. On the grounds of the test results, it is argued that layered evolution provides a superior approach for many tasks, and it is suggested that this approach may be the key to scaling up evolutionary robotics.",
        "published": "2004-05-06T19:37:07Z",
        "link": "http://arxiv.org/abs/cs/0405027v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.9; I.2.6"
        ]
    },
    {
        "title": "Business Intelligence from Web Usage Mining",
        "authors": [
            "Ajith Abraham"
        ],
        "summary": "The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. In this paper, we present the important concepts of Web usage mining and its various practical applications. We further present a novel approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy inference system to analyze the Web site visitor trends. A hybrid evolutionary fuzzy clustering algorithm is proposed in this paper to optimally segregate similar user interests. The clustered data is then used to analyze the trends using a Takagi-Sugeno fuzzy inference system learned using a combination of evolutionary algorithm and neural network learning. Proposed approach is compared with self-organizing maps (to discover patterns) and several function approximation techniques like neural networks, linear genetic programming and Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are graphically illustrated and the practical significance is discussed in detail. Empirical results clearly show that the proposed Web usage-mining framework is efficient.",
        "published": "2004-05-06T23:54:39Z",
        "link": "http://arxiv.org/abs/cs/0405030v1",
        "categories": [
            "cs.AI",
            "1.2.0"
        ]
    },
    {
        "title": "Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic   Approach for Tactical Air Combat Decision Support System",
        "authors": [
            "Cong Tran",
            "Lakhmi Jain",
            "Ajith Abraham"
        ],
        "summary": "Normally a decision support system is build to solve problem where multi-criteria decisions are involved. The knowledge base is the vital part of the decision support containing the information or data that is used in decision-making process. This is the field where engineers and scientists have applied several intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a hybrid neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference system for the Tactical Air Combat Decision Support System (TACDSS). Some simulation results demonstrating the difference of the learning techniques and are also provided.",
        "published": "2004-05-06T23:58:46Z",
        "link": "http://arxiv.org/abs/cs/0405031v1",
        "categories": [
            "cs.AI",
            "1.2.0"
        ]
    },
    {
        "title": "EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using   Neural Network Learning and Evolutionary Computation",
        "authors": [
            "Ajith Abraham"
        ],
        "summary": "Several adaptation techniques have been investigated to optimize fuzzy inference systems. Neural network learning algorithms have been used to determine the parameters of fuzzy inference system. Such models are often called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model there is no guarantee that the neural network learning algorithm converges and the tuning of fuzzy inference system will be successful. Success of evolutionary search procedures for optimization of fuzzy inference system is well proven and established in many application areas. In this paper, we will explore how the optimization of fuzzy inference systems could be further improved using a meta-heuristic approach combining neural network learning and evolutionary computation. The proposed technique could be considered as a methodology to integrate neural networks, fuzzy inference systems and evolutionary search procedures. We present the theoretical frameworks and some experimental results to demonstrate the efficiency of the proposed technique.",
        "published": "2004-05-07T00:01:54Z",
        "link": "http://arxiv.org/abs/cs/0405032v1",
        "categories": [
            "cs.AI",
            "1.2.0"
        ]
    },
    {
        "title": "Optimization of Evolutionary Neural Networks Using Hybrid Learning   Algorithms",
        "authors": [
            "Ajith Abraham"
        ],
        "summary": "Evolutionary artificial neural networks (EANNs) refer to a special class of artificial neural networks (ANNs) in which evolution is another fundamental form of adaptation in addition to learning. Evolutionary algorithms are used to adapt the connection weights, network architecture and learning algorithms according to the problem environment. Even though evolutionary algorithms are well known as efficient global search algorithms, very often they miss the best local solutions in the complex solution space. In this paper, we propose a hybrid meta-heuristic learning approach combining evolutionary learning and local search methods (using 1st and 2nd order error information) to improve the learning and faster convergence obtained using a direct evolutionary approach. The proposed technique is tested on three different chaotic time series and the test results are compared with some popular neuro-fuzzy systems and a recently developed cutting angle method of global optimization. Empirical results reveal that the proposed technique is efficient in spite of the computational complexity.",
        "published": "2004-05-07T00:08:16Z",
        "link": "http://arxiv.org/abs/cs/0405033v1",
        "categories": [
            "cs.AI",
            "1.2.0"
        ]
    },
    {
        "title": "Analysis of Hybrid Soft and Hard Computing Techniques for Forex   Monitoring Systems",
        "authors": [
            "Ajith Abraham"
        ],
        "summary": "In a universe with a single currency, there would be no foreign exchange market, no foreign exchange rates, and no foreign exchange. Over the past twenty-five years, the way the market has performed those tasks has changed enormously. The need for intelligent monitoring systems has become a necessity to keep track of the complex forex market. The vast currency market is a foreign concept to the average individual. However, once it is broken down into simple terms, the average individual can begin to understand the foreign exchange market and use it as a financial instrument for future investing. In this paper, we attempt to compare the performance of hybrid soft computing and hard computing techniques to predict the average monthly forex rates one month ahead. The soft computing models considered are a neural network trained by the scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive Regression Splines (MARS), Classification and Regression Trees (CART) and a hybrid CART-MARS technique. We considered the exchange rates of Australian dollar with respect to US dollar, Singapore dollar, New Zealand dollar, Japanese yen and United Kingdom pounds. The models were trained using 70% of the data and remaining was used for testing and validation purposes. It is observed that the proposed hybrid models could predict the forex rates more accurately than all the techniques when applied individually. Empirical results also reveal that the hybrid hard computing approach also improved some of our previous work using a neuro-fuzzy approach.",
        "published": "2004-05-07T00:10:07Z",
        "link": "http://arxiv.org/abs/cs/0405028v1",
        "categories": [
            "cs.AI",
            "1.2.0"
        ]
    },
    {
        "title": "Deductive Algorithmic Knowledge",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "The framework of algorithmic knowledge assumes that agents use algorithms to compute the facts they explicitly know. In many cases of interest, a deductive system, rather than a particular algorithm, captures the formal reasoning used by the agents to compute what they explicitly know. We introduce a logic for reasoning about both implicit and explicit knowledge with the latter defined with respect to a deductive system formalizing a logical theory for agents. The highly structured nature of deductive systems leads to very natural axiomatizations of the resulting logic when interpreted over any fixed deductive system. The decision problem for the logic, in the presence of a single agent, is NP-complete in general, no harder than propositional logic. It remains NP-complete when we fix a deductive system that is decidable in nondeterministic polynomial time. These results extend in a straightforward way to multiple agents.",
        "published": "2004-05-11T18:01:39Z",
        "link": "http://arxiv.org/abs/cs/0405038v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Prediction with Expert Advice by Following the Perturbed Leader for   General Weights",
        "authors": [
            "Marcus Hutter",
            "Jan Poland"
        ],
        "summary": "When applying aggregating strategies to Prediction with Expert Advice, the learning rate must be adaptively tuned. The natural choice of sqrt(complexity/current loss) renders the analysis of Weighted Majority derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative \"Follow the Perturbed Leader\" (FPL) algorithm from Kalai (2003} (based on Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are (to our knowledge) new.",
        "published": "2004-05-12T16:41:01Z",
        "link": "http://arxiv.org/abs/cs/0405043v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Export Behaviour Modeling Using EvoNF Approach",
        "authors": [
            "Ron Edwards",
            "Ajith Abraham",
            "Sonja Petrovic-Lazarevic"
        ],
        "summary": "The academic literature suggests that the extent of exporting by multinational corporation subsidiaries (MCS) depends on their product manufactured, resources, tax protection, customers and markets, involvement strategy, financial independence and suppliers' relationship with a multinational corporation (MNC). The aim of this paper is to model the complex export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order to determine the actual volume of MCS export output (sales exported). The proposed fuzzy inference system is optimised by using neural network learning and evolutionary computation. Empirical results clearly show that the proposed approach could model the export behaviour reasonable well compared to a direct neural network approach.",
        "published": "2004-05-16T03:24:55Z",
        "link": "http://arxiv.org/abs/cs/0405049v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Traffic Accident Analysis Using Decision Trees and Neural Networks",
        "authors": [
            "Miao M. Chong",
            "Ajith Abraham",
            "Marcin Paprzycki"
        ],
        "summary": "The costs of fatalities and injuries due to traffic accident have a great impact on society. This paper presents our research to model the severity of injury resulting from traffic accidents using artificial neural networks and decision trees. We have applied them to an actual data set obtained from the National Automotive Sampling System (NASS) General Estimates System (GES). Experiment results reveal that in all the cases the decision tree outperforms the neural network. Our research analysis also shows that the three most important factors in fatal injury are: driver's seat belt usage, light condition of the roadway, and driver's alcohol usage.",
        "published": "2004-05-16T03:33:20Z",
        "link": "http://arxiv.org/abs/cs/0405050v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Short Term Load Forecasting Models in Czech Republic Using Soft   Computing Paradigms",
        "authors": [
            "Muhammad Riaz Khan",
            "Ajith Abraham"
        ],
        "summary": "This paper presents a comparative study of six soft computing models namely multilayer perceptron networks, Elman recurrent neural network, radial basis function network, Hopfield model, fuzzy inference system and hybrid fuzzy neural network for the hourly electricity demand forecast of Czech Republic. The soft computing models were trained and tested using the actual hourly load data for seven years. A comparison of the proposed techniques is presented for predicting 2 day ahead demands for electricity. Simulation results indicate that hybrid fuzzy neural network and radial basis function networks are the best candidates for the analysis and forecasting of electricity demand.",
        "published": "2004-05-16T03:44:06Z",
        "link": "http://arxiv.org/abs/cs/0405051v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Decision Support Systems Using Intelligent Paradigms",
        "authors": [
            "Cong Tran",
            "Ajith Abraham",
            "Lakhmi Jain"
        ],
        "summary": "Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing (SC) technologies that underlie the conception, design and utilization of intelligent systems. In this paper, we present different SC paradigms involving an artificial neural network trained using the scaled conjugate gradient algorithm, two different fuzzy inference methods optimised using neural network learning/evolutionary algorithms and regression trees for developing intelligent decision support systems. We demonstrate the efficiency of the different algorithms by developing a decision support system for a Tactical Air Combat Environment (TACE). Some empirical comparisons between the different algorithms are also provided.",
        "published": "2004-05-16T03:50:05Z",
        "link": "http://arxiv.org/abs/cs/0405052v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Regression with respect to sensing actions and partial states",
        "authors": [
            "Le-Chi Tuan",
            "Chitta Baral",
            "Tran Cao Son"
        ],
        "summary": "In this paper, we present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions. We consider binary domains and employ the 0-approximation [Son & Baral 2001] to define the regression function. In binary domains, the use of 0-approximation means using 3-valued states. Although planning using this approach is incomplete with respect to the full semantics, we adopt it to have a lower complexity. We prove the soundness and completeness of our regression formulation with respect to the definition of progression. More specifically, we show that (i) a plan obtained through regression for a planning problem is indeed a progression solution of that planning problem, and that (ii) for each plan found through progression, using regression one obtains that plan or an equivalent one. We then develop a conditional planner that utilizes our regression function. We prove the soundness and completeness of our planning algorithm and present experimental results with respect to several well known planning problems in the literature.",
        "published": "2004-05-21T12:43:19Z",
        "link": "http://arxiv.org/abs/cs/0405071v1",
        "categories": [
            "cs.AI",
            "I.2.4; I.2.8"
        ]
    },
    {
        "title": "Propositional Defeasible Logic has Linear Complexity",
        "authors": [
            "Michael J. Maher"
        ],
        "summary": "Defeasible logic is a rule-based nonmonotonic logic, with both strict and defeasible rules, and a priority relation on rules. We show that inference in the propositional form of the logic can be performed in linear time. This contrasts markedly with most other propositional nonmonotonic logics, in which inference is intractable.",
        "published": "2004-05-24T15:45:59Z",
        "link": "http://arxiv.org/abs/cs/0405090v1",
        "categories": [
            "cs.AI",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "A Logic for Reasoning about Evidence",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "We introduce a logic for reasoning about evidence that essentially views evidence as a function from prior beliefs (before making an observation) to posterior beliefs (after making the observation). We provide a sound and complete axiomatization for the logic, and consider the complexity of the decision problem. Although the reasoning in the logic is mainly propositional, we allow variables representing numbers and quantification over them. This expressive power seems necessary to capture important properties of evidence.",
        "published": "2004-05-26T17:08:38Z",
        "link": "http://arxiv.org/abs/cs/0405098v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "Pruning Search Space in Defeasible Argumentation",
        "authors": [
            "Carlos Iván Chesñevar",
            "Guillermo Ricardo Simari",
            "Alejandro Javier García"
        ],
        "summary": "Defeasible argumentation has experienced a considerable growth in AI in the last decade. Theoretical results have been combined with development of practical applications in AI & Law, Case-Based Reasoning and various knowledge-based systems. However, the dialectical process associated with inference is computationally expensive. This paper focuses on speeding up this inference process by pruning the involved search space. Our approach is twofold. On one hand, we identify distinguished literals for computing defeat. On the other hand, we restrict ourselves to a subset of all possible conflicting arguments by introducing dialectical constraints.",
        "published": "2004-05-27T18:43:39Z",
        "link": "http://arxiv.org/abs/cs/0405106v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A Framework for Combining Defeasible Argumentation with Labeled   Deduction",
        "authors": [
            "Carlos Iván Chesñevar",
            "Guillermo Ricardo Simari"
        ],
        "summary": "In the last years, there has been an increasing demand of a variety of logical systems, prompted mostly by applications of logic in AI and other related areas. Labeled Deductive Systems (LDS) were developed as a flexible methodology to formalize such a kind of complex logical systems. Defeasible argumentation has proven to be a successful approach to formalizing commonsense reasoning, encompassing many other alternative formalisms for defeasible reasoning. Argument-based frameworks share some common notions (such as the concept of argument, defeater, etc.) along with a number of particular features which make it difficult to compare them with each other from a logical viewpoint. This paper introduces LDSar, a LDS for defeasible argumentation in which many important issues concerning defeasible argumentation are captured within a unified logical framework. We also discuss some logical properties and extensions that emerge from the proposed framework.",
        "published": "2004-05-27T18:54:31Z",
        "link": "http://arxiv.org/abs/cs/0405107v1",
        "categories": [
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "A proposal to design expert system for the calculations in the domain of   QFT",
        "authors": [
            "Andrea Severe"
        ],
        "summary": "Main purposes of the paper are followings: 1) To show examples of the calculations in domain of QFT via ``derivative rules'' of an expert system; 2) To consider advantages and disadvantage that technology of the calculations; 3) To reflect about how one would develop new physical theories, what knowledge would be useful in their investigations and how this problem can be connected with designing an expert system.",
        "published": "2004-05-31T10:50:23Z",
        "link": "http://arxiv.org/abs/cs/0405113v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Directional Consistency for Continuous Numerical Constraints",
        "authors": [
            "Frederic Goualard",
            "Laurent Granvilliers"
        ],
        "summary": "Bounds consistency is usually enforced on continuous constraints by first decomposing them into binary and ternary primitives. This decomposition has long been shown to drastically slow down the computation of solutions. To tackle this, Benhamou et al. have introduced an algorithm that avoids formally decomposing constraints. Its better efficiency compared to the former method has already been experimentally demonstrated. It is shown here that their algorithm implements a strategy to enforce on a continuous constraint a consistency akin to Directional Bounds Consistency as introduced by Dechter and Pearl for discrete problems. The algorithm is analyzed in this framework, and compared with algorithms that enforce bounds consistency. These theoretical results are eventually contrasted with new experimental results on standard benchmarks from the interval constraint community.",
        "published": "2004-06-16T16:33:39Z",
        "link": "http://arxiv.org/abs/cs/0406025v1",
        "categories": [
            "cs.AI",
            "cs.MS"
        ]
    },
    {
        "title": "A Dynamic Clustering-Based Markov Model for Web Usage Mining",
        "authors": [
            "José Borges",
            "Mark Levene"
        ],
        "summary": "Markov models have been widely utilized for modelling user web navigation behaviour. In this work we propose a dynamic clustering-based method to increase a Markov model's accuracy in representing a collection of user web navigation sessions. The method makes use of the state cloning concept to duplicate states in a way that separates in-links whose corresponding second-order probabilities diverge. In addition, the new method incorporates a clustering technique which determines an effcient way to assign in-links with similar second-order probabilities to the same clone. We report on experiments conducted with both real and random data and we provide a comparison with the N-gram Markov concept. The results show that the number of additional states induced by the dynamic clustering method can be controlled through a threshold parameter, and suggest that the method's performance is linear time in the size of the model.",
        "published": "2004-06-17T13:38:17Z",
        "link": "http://arxiv.org/abs/cs/0406032v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "A New Approach to Draw Detection by Move Repetition in Computer Chess   Programming",
        "authors": [
            "Vladan Vuckovic",
            "Djordje Vidanovic"
        ],
        "summary": "We will try to tackle both the theoretical and practical aspects of a very important problem in chess programming as stated in the title of this article - the issue of draw detection by move repetition. The standard approach that has so far been employed in most chess programs is based on utilising positional matrices in original and compressed format as well as on the implementation of the so-called bitboard format.   The new approach that we will be trying to introduce is based on using variant strings generated by the search algorithm (searcher) during the tree expansion in decision making. We hope to prove that this approach is more efficient than the standard treatment of the issue, especially in positions with few pieces (endgames). To illustrate what we have in mind a machine language routine that implements our theoretical assumptions is attached. The routine is part of the Axon chess program, developed by the authors. Axon, in its current incarnation, plays chess at master strength (ca. 2400-2450 Elo, based on both Axon vs computer programs and Axon vs human masters in over 3000 games altogether).",
        "published": "2004-06-21T13:42:03Z",
        "link": "http://arxiv.org/abs/cs/0406038v1",
        "categories": [
            "cs.AI",
            "I.2.1"
        ]
    },
    {
        "title": "Self-organizing neural networks in classification and image recognition",
        "authors": [
            "G. A. Ososkov",
            "S. G. Dmitrievskiy",
            "A. V. Stadnik"
        ],
        "summary": "Self-organizing neural networks are used for brick finding in OPERA experiment. Self-organizing neural networks and wavelet analysis used for recognition and extraction of car numbers from images.",
        "published": "2004-06-24T13:14:58Z",
        "link": "http://arxiv.org/abs/cs/0406047v1",
        "categories": [
            "cs.CV",
            "cs.AI"
        ]
    },
    {
        "title": "Web Services: A Process Algebra Approach",
        "authors": [
            "Andrea Ferrara"
        ],
        "summary": "It is now well-admitted that formal methods are helpful for many issues raised in the Web service area. In this paper we present a framework for the design and verification of WSs using process algebras and their tools. We define a two-way mapping between abstract specifications written using these calculi and executable Web services written in BPEL4WS. Several choices are available: design and correct errors in BPEL4WS, using process algebra verification tools, or design and correct in process algebra and automatically obtaining the corresponding BPEL4WS code. The approaches can be combined. Process algebra are not useful only for temporal logic verification: we remark the use of simulation/bisimulation both for verification and for the hierarchical refinement design method. It is worth noting that our approach allows the use of any process algebra depending on the needs of the user at different levels (expressiveness, existence of reasoning tools, user expertise).",
        "published": "2004-06-28T15:42:22Z",
        "link": "http://arxiv.org/abs/cs/0406055v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "P=NP",
        "authors": [
            "Selmer Bringsjord",
            "Joshua Taylor"
        ],
        "summary": "We claim to resolve the P=?NP problem via a formal argument for P=NP.",
        "published": "2004-06-28T19:11:18Z",
        "link": "http://arxiv.org/abs/cs/0406056v1",
        "categories": [
            "cs.CC",
            "cs.AI"
        ]
    },
    {
        "title": "Autogenic Training With Natural Language Processing Modules: A Recent   Tool For Certain Neuro Cognitive Studies",
        "authors": [
            "S. Ravichandran",
            "M. N. Karthik"
        ],
        "summary": "Learning to respond to voice-text input involves the subject's ability in understanding the phonetic and text based contents and his/her ability to communicate based on his/her experience. The neuro-cognitive facility of the subject has to support two important domains in order to make the learning process complete. In many cases, though the understanding is complete, the response is partial. This is one valid reason why we need to support the information from the subject with scalable techniques such as Natural Language Processing (NLP) for abstraction of the contents from the output. This paper explores the feasibility of using NLP modules interlaced with Neural Networks to perform the required task in autogenic training related to medical applications.",
        "published": "2004-07-02T20:15:02Z",
        "link": "http://arxiv.org/abs/cs/0407008v1",
        "categories": [
            "cs.AI",
            "I.2.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Search Using N-gram Technique Based Statistical Analysis for Knowledge   Extraction in Case Based Reasoning Systems",
        "authors": [
            "M. N. Karthik",
            "Moshe Davis"
        ],
        "summary": "Searching techniques for Case Based Reasoning systems involve extensive methods of elimination. In this paper, we look at a new method of arriving at the right solution by performing a series of transformations upon the data. These involve N-gram based comparison and deduction of the input data with the case data, using Morphemes and Phonemes as the deciding parameters. A similar technique for eliminating possible errors using a noise removal function is performed. The error tracking and elimination is performed through a statistical analysis of obtained data, where the entire data set is analyzed as sub-categories of various etymological derivatives. A probability analysis for the closest match is then performed, which yields the final expression. This final expression is referred to the Case Base. The output is redirected through an Expert System based on best possible match. The threshold for the match is customizable, and could be set by the Knowledge-Architect.",
        "published": "2004-07-02T20:22:18Z",
        "link": "http://arxiv.org/abs/cs/0407009v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.3; I.2.1"
        ]
    },
    {
        "title": "Learning for Adaptive Real-time Search",
        "authors": [
            "Vadim Bulitko"
        ],
        "summary": "Real-time heuristic search is a popular model of acting and learning in intelligent autonomous agents. Learning real-time search agents improve their performance over time by acquiring and refining a value function guiding the application of their actions. As computing the perfect value function is typically intractable, a heuristic approximation is acquired instead. Most studies of learning in real-time search (and reinforcement learning) assume that a simple value-function-greedy policy is used to select actions. This is in contrast to practice, where high-performance is usually attained by interleaving planning and acting via a lookahead search of a non-trivial depth. In this paper, we take a step toward bridging this gap and propose a novel algorithm that (i) learns a heuristic function to be used specifically with a lookahead-based policy, (ii) selects the lookahead depth adaptively in each state, (iii) gives the user control over the trade-off between exploration and exploitation. We extensively evaluate the algorithm in the sliding tile puzzle testbed comparing it to the classical LRTA* and the more recent weighted LRTA*, bounded LRTA*, and FALCONS. Improvements of 5 to 30 folds in convergence speed are observed.",
        "published": "2004-07-06T22:18:25Z",
        "link": "http://arxiv.org/abs/cs/0407016v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Multi-agent coordination using nearest neighbor rules: revisiting the   Vicsek model",
        "authors": [
            "Sanjiang Li",
            "Huaiqing Wang"
        ],
        "summary": "Recently, Jadbabaie, Lin, and Morse (IEEE TAC, 48(6)2003:988-1001) offered a mathematical analysis of the discrete time model of groups of mobile autonomous agents raised by Vicsek et al. in 1995. In their paper, Jadbabaie et al. showed that all agents shall move in the same heading, provided that these agents are periodically linked together. This paper sharpens this result by showing that coordination will be reached under a very weak condition that requires all agents are finally linked together. This condition is also strictly weaker than the one Jadbabaie et al. desired.",
        "published": "2004-07-09T02:28:02Z",
        "link": "http://arxiv.org/abs/cs/0407021v2",
        "categories": [
            "cs.MA",
            "cs.AI"
        ]
    },
    {
        "title": "Application of Artificial Neural Network in Jitter Analysis of   Dispersion-Managed Communication System",
        "authors": [
            "F. P. Zen",
            "B. E. Gunara",
            "W. Hidayat",
            "Z. A. Thalib",
            "H. Zainuddin",
            "J. Aminuddin"
        ],
        "summary": "Artificial Neural Network (ANN) is used as numerical methode in solving modified Nonlinear Schroedinger (NLS) equation with Dispersion Managed System (DMS) for jitter analysis. We take the optical axis z and the time t as input, and then some relevant values such as the change of position and the center frequency of the pulse, and further the mean square time of incoming pulse which are needed for jitter analysis. It shows that ANN yields numerical solutions which are adaptive with respect to the numerical errors and also verifies the previous numerical results using conventional numerical method. Our result indicates that DMS can minimize the timing jitter induced by some amplifiers.",
        "published": "2004-07-14T03:04:16Z",
        "link": "http://arxiv.org/abs/nlin/0407032v1",
        "categories": [
            "nlin.PS",
            "cs.AI",
            "cs.NA"
        ]
    },
    {
        "title": "On the Complexity of Case-Based Planning",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "We analyze the computational complexity of problems related to case-based planning: planning when a plan for a similar instance is known, and planning from a library of plans. We prove that planning from a single case has the same complexity than generative planning (i.e., planning \"from scratch\"); using an extended definition of cases, complexity is reduced if the domain stored in the case is similar to the one to search plans for. Planning from a library of cases is shown to have the same complexity. In both cases, the complexity of planning remains, in the worst case, PSPACE-complete.",
        "published": "2004-07-15T10:13:28Z",
        "link": "http://arxiv.org/abs/cs/0407034v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.8"
        ]
    },
    {
        "title": "Generalized Evolutionary Algorithm based on Tsallis Statistics",
        "authors": [
            "Ambedkar Dukkipati",
            "M. Narasimha Murty",
            "Shalabh Bhatnagar"
        ],
        "summary": "Generalized evolutionary algorithm based on Tsallis canonical distribution is proposed. The algorithm uses Tsallis generalized canonical distribution to weigh the configurations for `selection' instead of Gibbs-Boltzmann distribution. Our simulation results show that for an appropriate choice of non-extensive index that is offered by Tsallis statistics, evolutionary algorithms based on this generalization outperform algorithms based on Gibbs-Boltzmann distribution.",
        "published": "2004-07-16T06:08:22Z",
        "link": "http://arxiv.org/abs/cs/0407037v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "On the Convergence Speed of MDL Predictions for Bernoulli Sequences",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We consider the Minimum Description Length principle for online sequence prediction. If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is bounded, implying convergence with probability one, and (b) it additionally specifies a `rate of convergence'. Generally, for MDL only exponential loss bounds hold, as opposed to the linear bounds for a Bayes mixture. We show that this is even the case if the model class contains only Bernoulli distributions. We derive a new upper bound on the prediction error for countable Bernoulli classes. This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes. The results apply to many Machine Learning tasks including classification and hypothesis testing. We provide arguments that our theorems generalize to countable classes of i.i.d. models.",
        "published": "2004-07-16T10:36:49Z",
        "link": "http://arxiv.org/abs/cs/0407039v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.PR",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Decomposition Based Search - A theoretical and experimental evaluation",
        "authors": [
            "W. J. van Hoeve",
            "M. Milano"
        ],
        "summary": "In this paper we present and evaluate a search strategy called Decomposition Based Search (DBS) which is based on two steps: subproblem generation and subproblem solution. The generation of subproblems is done through value ranking and domain splitting. Subdomains are explored so as to generate, according to the heuristic chosen, promising subproblems first.   We show that two well known search strategies, Limited Discrepancy Search (LDS) and Iterative Broadening (IB), can be seen as special cases of DBS. First we present a tuning of DBS that visits the same search nodes as IB, but avoids restarts. Then we compare both theoretically and computationally DBS and LDS using the same heuristic. We prove that DBS has a higher probability of being successful than LDS on a comparable number of nodes, under realistic assumptions. Experiments on a constraint satisfaction problem and an optimization problem show that DBS is indeed very effective if compared to LDS.",
        "published": "2004-07-16T13:38:19Z",
        "link": "http://arxiv.org/abs/cs/0407040v1",
        "categories": [
            "cs.AI",
            "I.2.8; D.3.3"
        ]
    },
    {
        "title": "Postponing Branching Decisions",
        "authors": [
            "Willem Jan van Hoeve",
            "Michela Milano"
        ],
        "summary": "Solution techniques for Constraint Satisfaction and Optimisation Problems often make use of backtrack search methods, exploiting variable and value ordering heuristics. In this paper, we propose and analyse a very simple method to apply in case the value ordering heuristic produces ties: postponing the branching decision. To this end, we group together values in a tie, branch on this sub-domain, and defer the decision among them to lower levels of the search tree. We show theoretically and experimentally that this simple modification can dramatically improve the efficiency of the search strategy. Although in practise similar methods may have been applied already, to our knowledge, no empirical or theoretical study has been proposed in the literature to identify when and to what extent this strategy should be used.",
        "published": "2004-07-16T14:37:11Z",
        "link": "http://arxiv.org/abs/cs/0407042v1",
        "categories": [
            "cs.AI",
            "I.2.8; D.3.3"
        ]
    },
    {
        "title": "Reduced cost-based ranking for generating promising subproblems",
        "authors": [
            "M. Milano",
            "W. J. van Hoeve"
        ],
        "summary": "In this paper, we propose an effective search procedure that interleaves two steps: subproblem generation and subproblem solution. We mainly focus on the first part. It consists of a variable domain value ranking based on reduced costs. Exploiting the ranking, we generate, in a Limited Discrepancy Search tree, the most promising subproblems first. An interesting result is that reduced costs provide a very precise ranking that allows to almost always find the optimal solution in the first generated subproblem, even if its dimension is significantly smaller than that of the original problem. Concerning the proof of optimality, we exploit a way to increase the lower bound for subproblems at higher discrepancies. We show experimental results on the TSP and its time constrained variant to show the effectiveness of the proposed approach, but the technique could be generalized for other problems.",
        "published": "2004-07-16T14:53:21Z",
        "link": "http://arxiv.org/abs/cs/0407044v1",
        "categories": [
            "cs.AI",
            "I.2.8; G.1.6; D.3.3"
        ]
    },
    {
        "title": "Channel-Independent and Sensor-Independent Stimulus Representations",
        "authors": [
            "David N. Levin"
        ],
        "summary": "This paper shows how a machine, which observes stimuli through an uncharacterized, uncalibrated channel and sensor, can glean machine-independent information (i.e., channel- and sensor-independent information) about the stimuli. First, we demonstrate that a machine defines a specific coordinate system on the stimulus state space, with the nature of that coordinate system depending on the device's channel and sensor. Thus, machines with different channels and sensors \"see\" the same stimulus trajectory through state space, but in different machine-specific coordinate systems. For a large variety of physical stimuli, statistical properties of that trajectory endow the stimulus configuration space with differential geometric structure (a metric and parallel transfer procedure), which can then be used to represent relative stimulus configurations in a coordinate-system-independent manner (and, therefore, in a channel- and sensor-independent manner). The resulting description is an \"inner\" property of the stimulus time series in the sense that it does not depend on extrinsic factors like the observer's choice of a coordinate system in which the stimulus is viewed (i.e., the observer's choice of channel and sensor). This methodology is illustrated with analytic examples and with a numerically simulated experiment. In an intelligent sensory device, this kind of representation \"engine\" could function as a \"front-end\" that passes channel/sensor-independent stimulus representations to a pattern recognition module. After a pattern recognizer has been trained in one of these devices, it could be used without change in other devices having different channels and sensors.",
        "published": "2004-07-19T17:13:34Z",
        "link": "http://arxiv.org/abs/cs/0407047v3",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.5; I.5.4.m; I.5.2.b; I.2.10.f; I.2.4.j; I.2.7.g; I.5.4.b; I.2;\n  I.2.0.b"
        ]
    },
    {
        "title": "Preferred Answer Sets for Ordered Logic Programs",
        "authors": [
            "Davy Van Nieuwenborgh",
            "Dirk Vermeir"
        ],
        "summary": "We extend answer set semantics to deal with inconsistent programs (containing classical negation), by finding a ``best'' answer set. Within the context of inconsistent programs, it is natural to have a partial order on rules, representing a preference for satisfying certain rules, possibly at the cost of violating less important ones. We show that such a rule order induces a natural order on extended answer sets, the minimal elements of which we call preferred answer sets. We characterize the expressiveness of the resulting semantics and show that it can simulate negation as failure, disjunction and some other formalisms such as logic programs with ordered disjunction. The approach is shown to be useful in several application areas, e.g. repairing database, where minimal repairs correspond to preferred answer sets.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2004-07-19T22:37:43Z",
        "link": "http://arxiv.org/abs/cs/0407049v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "From truth to computability I",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "The recently initiated approach called computability logic is a formal theory of interactive computation. See a comprehensive online source on the subject at http://www.cis.upenn.edu/~giorgi/cl.html . The present paper contains a soundness and completeness proof for the deductive system CL3 which axiomatizes the most basic first-order fragment of computability logic called the finite-depth, elementary-base fragment. Among the potential application areas for this result are the theory of interactive computation, constructive applied theories, knowledgebase systems, systems for resource-bound planning and action. This paper is self-contained as it reintroduces all relevant definitions as well as main motivations.",
        "published": "2004-07-21T03:58:22Z",
        "link": "http://arxiv.org/abs/cs/0407054v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.GT",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Universal Convergence of Semimeasures on Individual Random Sequences",
        "authors": [
            "Marcus Hutter",
            "Andrej Muchnik"
        ],
        "summary": "Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.",
        "published": "2004-07-23T12:43:28Z",
        "link": "http://arxiv.org/abs/cs/0407057v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "math.IT",
            "math.PR",
            "I.2.6; E.4; G.3; F.1.3"
        ]
    },
    {
        "title": "A Sequent Calculus and a Theorem Prover for Standard Conditional Logics",
        "authors": [
            "Nicola Olivetti",
            "Gian Luca Pozzato",
            "Camilla Schwind"
        ],
        "summary": "In this paper we present a cut-free sequent calculus, called SeqS, for some standard conditional logics, namely CK, CK+ID, CK+MP and CK+MP+ID. The calculus uses labels and transition formulas and can be used to prove decidability and space complexity bounds for the respective logics. We also present CondLean, a theorem prover for these logics implementing SeqS calculi written in SICStus Prolog.",
        "published": "2004-07-29T14:17:19Z",
        "link": "http://arxiv.org/abs/cs/0407064v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "A Simple Proportional Conflict Redistribution Rule",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "One proposes a first alternative rule of combination to WAO (Weighted Average Operator) proposed recently by Josang, Daniel and Vannoorenberghe, called Proportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are particular cases of WO (the Weighted Operator) because the conflicting mass is redistributed with respect to some weighting factors. In this first PCR rule, the proportionalization is done for each non-empty set with respect to the non-zero sum of its corresponding mass matrix - instead of its mass column average as in WAO, but the results are the same as Ph. Smets has pointed out. Also, we extend WAO (which herein gives no solution) for the degenerate case when all column sums of all non-empty sets are zero, and then the conflicting mass is transferred to the non-empty disjunctive form of all non-empty sets together; but if this disjunctive form happens to be empty, then one considers an open world (i.e. the frame of discernment might contain new hypotheses) and thus all conflicting mass is transferred to the empty set. In addition to WAO, we propose a general formula for PCR1 (WAO for non-degenerate cases).",
        "published": "2004-08-03T16:08:37Z",
        "link": "http://arxiv.org/abs/cs/0408010v5",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "An Algorithm for Quasi-Associative and Quasi-Markovian Rules of   Combination in Information Fusion",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "In this paper one proposes a simple algorithm of combining the fusion rules, those rules which first use the conjunctive rule and then the transfer of conflicting mass to the non-empty sets, in such a way that they gain the property of associativity and fulfill the Markovian requirement for dynamic fusion. Also, a new rule, SDL-improved, is presented.",
        "published": "2004-08-08T19:41:23Z",
        "link": "http://arxiv.org/abs/cs/0408021v2",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "On Global Warming (Softening Global Constraints)",
        "authors": [
            "Willem Jan van Hoeve",
            "Gilles Pesant",
            "Louis-Martin Rousseau"
        ],
        "summary": "We describe soft versions of the global cardinality constraint and the regular constraint, with efficient filtering algorithms maintaining domain consistency. For both constraints, the softening is achieved by augmenting the underlying graph. The softened constraints can be used to extend the meta-constraint framework for over-constrained problems proposed by Petit, Regin and Bessiere.",
        "published": "2004-08-09T12:06:04Z",
        "link": "http://arxiv.org/abs/cs/0408023v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.2; D.3.3; G.2.2"
        ]
    },
    {
        "title": "From spin glasses to hard satisfiable formulas",
        "authors": [
            "Haixia Jia",
            "Cristopher Moore",
            "Bart Selman"
        ],
        "summary": "We introduce a highly structured family of hard satisfiable 3-SAT formulas corresponding to an ordered spin-glass model from statistical physics. This model has provably \"glassy\" behavior; that is, it has many local optima with large energy barriers between them, so that local search algorithms get stuck and have difficulty finding the true ground state, i.e., the unique satisfying assignment. We test the hardness of our formulas with two Davis-Putnam solvers, Satz and zChaff, the recently introduced Survey Propagation (SP), and two local search algorithms, Walksat and Record-to-Record Travel (RRT). We compare our formulas to random 3-XOR-SAT formulas and to two other generators of hard satisfiable instances, the minimum disagreement parity formulas of Crawford et al., and Hirsch's hgen. For the complete solvers the running time of our formulas grows exponentially in sqrt(n), and exceeds that of random 3-XOR-SAT formulas for small problem sizes. SP is unable to solve our formulas with as few as 25 variables. For Walksat, our formulas appear to be harder than any other known generator of satisfiable instances. Finally, our formulas can be solved efficiently by RRT but only if the parameter d is tuned to the height of the barriers between local minima, and we use this parameter to measure the barrier heights in random 3-XOR-SAT formulas as well.",
        "published": "2004-08-09T21:54:07Z",
        "link": "http://arxiv.org/abs/cond-mat/0408190v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.AI"
        ]
    },
    {
        "title": "Learning a Machine for the Decision in a Partially Observable Markov   Universe",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "In this paper, we are interested in optimal decisions in a partially observable Markov universe. Our viewpoint departs from the dynamic programming viewpoint: we are directly approximating an optimal strategic tree depending on the observation. This approximation is made by means of a parameterized probabilistic law. In this paper, a particular family of hidden Markov models, with input and output, is considered as a learning framework. A method for optimizing the parameters of these HMMs is proposed and applied. This optimization method is based on the cross-entropic principle.",
        "published": "2004-08-11T06:38:50Z",
        "link": "http://arxiv.org/abs/math/0408146v1",
        "categories": [
            "math.GM",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Multi-dimensional Type Theory: Rules, Categories, and Combinators for   Syntax and Semantics",
        "authors": [
            "Jørgen Villadsen"
        ],
        "summary": "We investigate the possibility of modelling the syntax and semantics of natural language by constraints, or rules, imposed by the multi-dimensional type theory Nabla. The only multiplicity we explicitly consider is two, namely one dimension for the syntax and one dimension for the semantics, but the general perspective is important. For example, issues of pragmatics could be handled as additional dimensions.   One of the main problems addressed is the rather complicated repertoire of operations that exists besides the notion of categories in traditional Montague grammar. For the syntax we use a categorial grammar along the lines of Lambek. For the semantics we use so-called lexical and logical combinators inspired by work in natural logic. Nabla provides a concise interpretation and a sequent calculus as the basis for implementations.",
        "published": "2004-08-15T08:51:19Z",
        "link": "http://arxiv.org/abs/cs/0408037v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "I.2.7"
        ]
    },
    {
        "title": "FLUX: A Logic Programming Method for Reasoning Agents",
        "authors": [
            "Michael Thielscher"
        ],
        "summary": "FLUX is a programming method for the design of agents that reason logically about their actions and sensor information in the presence of incomplete knowledge. The core of FLUX is a system of Constraint Handling Rules, which enables agents to maintain an internal model of their environment by which they control their own behavior. The general action representation formalism of the fluent calculus provides the formal semantics for the constraint solver. FLUX exhibits excellent computational behavior due to both a carefully restricted expressiveness and the inference paradigm of progression.",
        "published": "2004-08-19T14:47:51Z",
        "link": "http://arxiv.org/abs/cs/0408044v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A CHR-based Implementation of Known Arc-Consistency",
        "authors": [
            "Marco Alberti",
            "Marco Gavanelli",
            "Evelina Lamma",
            "Paola Mello",
            "Michela Milano"
        ],
        "summary": "In classical CLP(FD) systems, domains of variables are completely known at the beginning of the constraint propagation process. However, in systems interacting with an external environment, acquiring the whole domains of variables before the beginning of constraint propagation may cause waste of computation time, or even obsolescence of the acquired data at the time of use.   For such cases, the Interactive Constraint Satisfaction Problem (ICSP) model has been proposed as an extension of the CSP model, to make it possible to start constraint propagation even when domains are not fully known, performing acquisition of domain elements only when necessary, and without the need for restarting the propagation after every acquisition.   In this paper, we show how a solver for the two sorted CLP language, defined in previous work, to express ICSPs, has been implemented in the Constraint Handling Rules (CHR) language, a declarative language particularly suitable for high level implementation of constraint solvers.",
        "published": "2004-08-24T10:15:13Z",
        "link": "http://arxiv.org/abs/cs/0408056v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Cauchy Annealing Schedule: An Annealing Schedule for Boltzmann Selection   Scheme in Evolutionary Algorithms",
        "authors": [
            "Ambedkar Dukkipati",
            "M. Narasimha Murty",
            "Shalabh Bhatnagar"
        ],
        "summary": "Boltzmann selection is an important selection mechanism in evolutionary algorithms as it has theoretical properties which help in theoretical analysis. However, Boltzmann selection is not used in practice because a good annealing schedule for the `inverse temperature' parameter is lacking. In this paper we propose a Cauchy annealing schedule for Boltzmann selection scheme based on a hypothesis that selection-strength should increase as evolutionary process goes on and distance between two selection strengths should decrease for the process to converge. To formalize these aspects, we develop formalism for selection mechanisms using fitness distributions and give an appropriate measure for selection-strength. In this paper, we prove an important result, by which we derive an annealing schedule called Cauchy annealing schedule. We demonstrate the novelty of proposed annealing schedule using simulations in the framework of genetic algorithms.",
        "published": "2004-08-24T11:21:06Z",
        "link": "http://arxiv.org/abs/cs/0408055v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "The role of robust semantic analysis in spoken language dialogue systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "In this paper we summarized a framework for designing grammar-based procedure for the automatic extraction of the semantic content from spoken queries. Starting with a case study and following an approach which combines the notions of fuzziness and robustness in sentence parsing, we showed we built practical domain-dependent rules which can be applied whenever it is possible to superimpose a sentence-level semantic structure to a text without relying on a previous deep syntactical analysis. This kind of procedure can be also profitably used as a pre-processing tool in order to cut out part of the sentence which have been recognized to have no relevance in the understanding process. In the case of particular dialogue applications where there is no need to build a complex semantic structure (e.g. word spotting or excerpting) the presented methodology may represent an efficient alternative solution to a sequential composition of deep linguistic analysis modules. Even if the query generation problem may not seem a critical application it should be held in mind that the sentence processing must be done on-line. Having this kind of constraints we cannot design our system without caring for efficiency and thus provide an immediate response. Another critical issue is related to whole robustness of the system. In our case study we tried to make experiences on how it is possible to deal with an unreliable and noisy input without asking the user for any repetition or clarification. This may correspond to a similar problem one may have when processing text coming from informal writing such as e-mails, news and in many cases Web pages where it is often the case to have irrelevant surrounding information.",
        "published": "2004-08-25T19:37:59Z",
        "link": "http://arxiv.org/abs/cs/0408057v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "H.5.2;H.3.1;H.3.4"
        ]
    },
    {
        "title": "Proportional Conflict Redistribution Rules for Information Fusion",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "In this paper we propose five versions of a Proportional Conflict Redistribution rule (PCR) for information fusion together with several examples. From PCR1 to PCR2, PCR3, PCR4, PCR5 one increases the complexity of the rules and also the exactitude of the redistribution of conflicting masses. PCR1 restricted from the hyper-power set to the power set and without degenerate cases gives the same result as the Weighted Average Operator (WAO) proposed recently by J{\\o}sang, Daniel and Vannoorenberghe but does not satisfy the neutrality property of vacuous belief assignment. That's why improved PCR rules are proposed in this paper. PCR4 is an improvement of minC and Dempster's rules. The PCR rules redistribute the conflicting mass, after the conjunctive rule has been applied, proportionally with some functions depending on the masses assigned to their corresponding columns in the mass matrix. There are infinitely many ways these functions (weighting factors) can be chosen depending on the complexity one wants to deal with in specific applications and fusion systems. Any fusion combination rule is at some degree ad-hoc.",
        "published": "2004-08-28T03:08:39Z",
        "link": "http://arxiv.org/abs/cs/0408064v3",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "The Integration of Connectionism and First-Order Knowledge   Representation and Reasoning as a Challenge for Artificial Intelligence",
        "authors": [
            "Sebastian Bader",
            "Pascal Hitzler",
            "Steffen Hoelldobler"
        ],
        "summary": "Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.",
        "published": "2004-08-31T16:16:28Z",
        "link": "http://arxiv.org/abs/cs/0408069v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.NE",
            "I.2.3,I.2.6"
        ]
    },
    {
        "title": "Default reasoning over domains and concept hierarchies",
        "authors": [
            "Pascal Hitzler"
        ],
        "summary": "W.C. Rounds and G.-Q. Zhang (2001) have proposed to study a form of disjunctive logic programming generalized to algebraic domains. This system allows reasoning with information which is hierarchically structured and forms a (suitable) domain. We extend this framework to include reasoning with default negation, giving rise to a new nonmonotonic reasoning framework on hierarchical knowledge which encompasses answer set programming with extended disjunctive logic programs. We also show that the hierarchically structured knowledge on which programming in this paradigm can be done, arises very naturally from formal concept analysis. Together, we obtain a default reasoning paradigm for conceptual knowledge which is in accordance with mainstream developments in nonmonotonic reasoning.",
        "published": "2004-09-01T19:22:01Z",
        "link": "http://arxiv.org/abs/cs/0409002v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4; D.1.6"
        ]
    },
    {
        "title": "ScheduleNanny: Using GPS to Learn the User's Significant Locations,   Travel Times and Schedule",
        "authors": [
            "Parth Bhawalkar",
            "Victor Bigio",
            "Adam Davis",
            "Karthik Narayanaswami",
            "Femi Olumoko"
        ],
        "summary": "As computing technology becomes more pervasive, personal devices such as the PDA, cell-phone, and notebook should use context to determine how to act. Location is one form of context that can be used in many ways. We present a multiple-device system that collects and clusters GPS data into significant locations. These locations are then used to determine travel times and a probabilistic model of the user's schedule, which is used to intelligently alert the user. We evaluate our system and suggest how it should be integrated with a variety of applications.",
        "published": "2004-09-02T15:28:53Z",
        "link": "http://arxiv.org/abs/cs/0409003v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.HC",
            "F.2.2; I.5.3; H.5.3; H.5.m"
        ]
    },
    {
        "title": "The Generalized Pignistic Transformation",
        "authors": [
            "Jean Dezert",
            "Florentin Smarandache",
            "Milan Daniel"
        ],
        "summary": "This paper presents in detail the generalized pignistic transformation (GPT) succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a tool for decision process. The GPT allows to provide a subjective probability measure from any generalized basic belief assignment given by any corpus of evidence. We mainly focus our presentation on the 3D case and provide the complete result obtained by the GPT and its validation drawn from the probability theory.",
        "published": "2004-09-06T17:47:06Z",
        "link": "http://arxiv.org/abs/cs/0409007v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Outlier Detection by Logic Programming",
        "authors": [
            "Fabrizio Angiulli",
            "Gianluigi Greco",
            "Luigi Palopoli"
        ],
        "summary": "The development of effective knowledge discovery techniques has become in the recent few years a very active research area due to the important impact it has in several relevant application areas. One interesting task thereof is that of singling out anomalous individuals from a given population, e.g., to detect rare events in time-series analysis settings, or to identify objects whose behavior is deviant w.r.t. a codified standard set of \"social\" rules. Such exceptional individuals are usually referred to as outliers in the literature.   Recently, outlier detection has also emerged as a relevant KR&R problem. In this paper, we formally state the concept of outliers by generalizing in several respects an approach recently proposed in the context of default logic, for instance, by having outliers not being restricted to single individuals but, rather, in the more general case, to correspond to entire (sub)theories. We do that within the context of logic programming and, mainly through examples, we discuss its potential practical impact in applications. The formalization we propose is a novel one and helps in shedding some light on the real nature of outliers. Moreover, as a major contribution of this work, we illustrate the exploitation of minimality criteria in outlier detection. The computational complexity of outlier detection problems arising in this novel setting is thoroughly investigated and accounted for in the paper as well. Finally, we also propose a rewriting algorithm that transforms any outlier detection problem into an equivalent inference problem under the stable model semantics, thereby making outlier computation effective and realizable on top of any stable model solver.",
        "published": "2004-09-09T14:20:02Z",
        "link": "http://arxiv.org/abs/cs/0409019v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Unification of Fusion Theories",
        "authors": [
            "Florentin Smarandache"
        ],
        "summary": "Since no fusion theory neither rule fully satisfy all needed applications, the author proposes a Unification of Fusion Theories and a combination of fusion rules in solving problems/applications. For each particular application, one selects the most appropriate model, rule(s), and algorithm of implementation. We are working in the unification of the fusion theories and rules, which looks like a cooking recipe, better we'd say like a logical chart for a computer programmer, but we don't see another method to comprise/unify all things. The unification scenario presented herein, which is now in an incipient form, should periodically be updated incorporating new discoveries from the fusion and engineering research.",
        "published": "2004-09-23T02:02:44Z",
        "link": "http://arxiv.org/abs/cs/0409040v3",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Augmenting ALC(D) (atemporal) roles and (aspatial) concrete domain with   temporal roles and a spatial concrete domain -first results",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We consider the well-known family ALC(D) of description logics with a concrete domain, and provide first results on a framework obtained by augmenting ALC(D) atemporal roles and aspatial concrete domain with temporal roles and a spatial concrete domain.",
        "published": "2004-09-24T14:56:23Z",
        "link": "http://arxiv.org/abs/cs/0409045v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.3; I.2.4; I.2.m)"
        ]
    },
    {
        "title": "A TCSP-like decidable constraint language generalising existing cardinal   direction relations",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We define a quantitative constraint language subsuming two calculi well-known in QSR (Qualitative Spatial Reasoning): Frank's cone-shaped and projection-based calculi of cardinal direction relations. We show how to solve a CSP (Constraint Satisfaction Problem) expressed in the language.",
        "published": "2004-09-24T15:12:58Z",
        "link": "http://arxiv.org/abs/cs/0409046v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.3; I.2.4; I.2.m)"
        ]
    },
    {
        "title": "An ALC(D)-based combination of temporal constraints and spatial   constraints suitable for continuous (spatial) change",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We present a family of spatio-temporal theories suitable for continuous spatial change in general, and for continuous motion of spatial scenes in particular. The family is obtained by spatio-temporalising the well-known ALC(D) family of Description Logics (DLs) with a concrete domain D, as follows, where TCSPs denotes \"Temporal Constraint Satisfaction Problems\", a well-known constraint-based framework:   (1) temporalisation of the roles, so that they consist of TCSP constraints (specifically, of an adaptation of TCSP constraints to interval variables); and   (2) spatialisation of the concrete domain D: the concrete domain is now $D_x$, and is generated by a spatial Relation Algebra (RA) $x$, in the style of the Region-Connection Calculus RCC8.   We assume durative truth (i.e., holding during a durative interval). We also assume the homogeneity property (if a truth holds during a given interval, it holds during all of its subintervals). Among other things, these assumptions raise the \"conflicting\" problem of overlapping truths, which the work solves with the use of a specific partition of the 13 atomic relations of Allen's interval algebra.",
        "published": "2004-09-24T15:22:52Z",
        "link": "http://arxiv.org/abs/cs/0409047v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.3; I.2.4; I.2.m)"
        ]
    },
    {
        "title": "Applying Policy Iteration for Training Recurrent Neural Networks",
        "authors": [
            "I. Szita",
            "A. Lorincz"
        ],
        "summary": "Recurrent neural networks are often used for learning time-series data. Based on a few assumptions we model this learning task as a minimization problem of a nonlinear least-squares cost function. The special structure of the cost function allows us to build a connection to reinforcement learning. We exploit this connection and derive a convergent, policy iteration-based algorithm. Furthermore, we argue that RNN training can be fit naturally into the reinforcement learning framework.",
        "published": "2004-10-02T07:19:49Z",
        "link": "http://arxiv.org/abs/cs/0410004v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ]
    },
    {
        "title": "Normal forms for Answer Sets Programming",
        "authors": [
            "Stefania Costantini",
            "Alessandro Provetti"
        ],
        "summary": "Normal forms for logic programs under stable/answer set semantics are introduced. We argue that these forms can simplify the study of program properties, mainly consistency. The first normal form, called the {\\em kernel} of the program, is useful for studying existence and number of answer sets. A kernel program is composed of the atoms which are undefined in the Well-founded semantics, which are those that directly affect the existence of answer sets. The body of rules is composed of negative literals only. Thus, the kernel form tends to be significantly more compact than other formulations. Also, it is possible to check consistency of kernel programs in terms of colorings of the Extended Dependency Graph program representation which we previously developed. The second normal form is called {\\em 3-kernel.} A 3-kernel program is composed of the atoms which are undefined in the Well-founded semantics. Rules in 3-kernel programs have at most two conditions, and each rule either belongs to a cycle, or defines a connection between cycles. 3-kernel programs may have positive conditions. The 3-kernel normal form is very useful for the static analysis of program consistency, i.e., the syntactic characterization of existence of answer sets. This result can be obtained thanks to a novel graph-like representation of programs, called Cycle Graph which presented in the companion article \\cite{Cos04b}.",
        "published": "2004-10-06T15:01:50Z",
        "link": "http://arxiv.org/abs/cs/0410014v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "L1 regularization is better than L2 for learning and predicting chaotic   systems",
        "authors": [
            "Z. Szabo",
            "A. Lorincz"
        ],
        "summary": "Emergent behaviors are in the focus of recent research interest. It is then of considerable importance to investigate what optimizations suit the learning and prediction of chaotic systems, the putative candidates for emergence. We have compared L1 and L2 regularizations on predicting chaotic time series using linear recurrent neural networks. The internal representation and the weights of the networks were optimized in a unifying framework. Computational tests on different problems indicate considerable advantages for the L1 regularization: It had considerably better learning time and better interpolating capabilities. We shall argue that optimization viewed as a maximum likelihood estimation justifies our results, because L1 regularization fits heavy-tailed distributions -- an apparently general feature of emergent systems -- better.",
        "published": "2004-10-07T10:57:08Z",
        "link": "http://arxiv.org/abs/cs/0410015v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "An In-Depth Look at Information Fusion Rules & the Unification of Fusion   Theories",
        "authors": [
            "Florentin Smarandache"
        ],
        "summary": "This paper may look like a glossary of the fusion rules and we also introduce new ones presenting their formulas and examples: Conjunctive, Disjunctive, Exclusive Disjunctive, Mixed Conjunctive-Disjunctive rules, Conditional rule, Dempster's, Yager's, Smets' TBM rule, Dubois-Prade's, Dezert-Smarandache classical and hybrid rules, Murphy's average rule, Inagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and, as particular cases: Iganaki's parameterized rule, Weighting Average Operator, minC (M. Daniel), and newly Proportional Conflict Redistribution rules (Smarandache-Dezert) among which PCR5 is the most exact way of redistribution of the conflicting mass to non-empty sets following the path of the conjunctive rule], Zhang's Center Combination rule, Convolutive x-Averaging, Consensus Operator (Josang), Cautious Rule (Smets), ?-junctions rules (Smets), etc. and three new T-norm & T-conorm rules adjusted from fuzzy and neutrosophic sets to information fusion (Tchamova-Smarandache). Introducing the degree of union and degree of inclusion with respect to the cardinal of sets not with the fuzzy set point of view, besides that of intersection, many fusion rules can be improved. There are corner cases where each rule might have difficulties working or may not get an expected result.",
        "published": "2004-10-14T22:53:46Z",
        "link": "http://arxiv.org/abs/cs/0410033v2",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Intransitivity and Vagueness",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "There are many examples in the literature that suggest that indistinguishability is intransitive, despite the fact that the indistinguishability relation is typically taken to be an equivalence relation (and thus transitive). It is shown that if the uncertainty perception and the question of when an agent reports that two things are indistinguishable are both carefully modeled, the problems disappear, and indistinguishability can indeed be taken to be an equivalence relation. Moreover, this model also suggests a logic of vagueness that seems to solve many of the problems related to vagueness discussed in the philosophical literature. In particular, it is shown here how the logic can handle the sorites paradox.",
        "published": "2004-10-19T17:31:11Z",
        "link": "http://arxiv.org/abs/cs/0410049v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Sleeping Beauty Reconsidered: Conditioning and Reflection in   Asynchronous Systems",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "A careful analysis of conditioning in the Sleeping Beauty problem is done, using the formal model for reasoning about knowledge and probability developed by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as revealing problems with conditioning in the presence of imperfect recall, the analysis done here reveals that the problems are not so much due to imperfect recall as to asynchrony. The implications of this analysis for van Fraassen's Reflection Principle and Savage's Sure-Thing Principle are considered.",
        "published": "2004-10-19T17:31:44Z",
        "link": "http://arxiv.org/abs/cs/0410050v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Robust Dialogue Understanding in HERALD",
        "authors": [
            "Vincenzo Pallotta",
            "Afzal Ballim"
        ],
        "summary": "We tackle the problem of robust dialogue processing from the perspective of language engineering. We propose an agent-oriented architecture that allows us a flexible way of composing robust processors. Our approach is based on Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP agent model can be enriched with special features and components that allow us to deal with classical problems of dialogue understanding.",
        "published": "2004-10-22T23:41:52Z",
        "link": "http://arxiv.org/abs/cs/0410058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.MA",
            "cs.SE",
            "H.5.2, I.2.7, I.2.11"
        ]
    },
    {
        "title": "Semantic filtering by inference on domain knowledge in spoken dialogue   systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "General natural dialogue processing requires large amounts of domain knowledge as well as linguistic knowledge in order to ensure acceptable coverage and understanding. There are several ways of integrating lexical resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at different levels of dialogue processing. We concentrate in this paper on how to exploit domain knowledge for filtering interpretation hypotheses generated by a robust semantic parser. We use domain knowledge to semantically constrain the hypothesis space. Moreover, adding an inference mechanism allows us to complete the interpretation when information is not explicitly available. Further, we discuss briefly how this can be generalized towards a predictive natural interactive system.",
        "published": "2004-10-23T00:20:06Z",
        "link": "http://arxiv.org/abs/cs/0410060v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.5.2;H.3.1;H.3.4"
        ]
    },
    {
        "title": "Analyzing and Improving Performance of a Class of Anomaly-based   Intrusion Detectors",
        "authors": [
            "Zhuowei Li",
            "Amitabha Das"
        ],
        "summary": "Anomaly-based intrusion detection (AID) techniques are useful for detecting novel intrusions into computing resources. One of the most successful AID detectors proposed to date is stide, which is based on analysis of system call sequences. In this paper, we present a detailed formal framework to analyze, understand and improve the performance of stide and similar AID techniques. Several important properties of stide-like detectors are established through formal proofs, and validated by carefully conducted experiments using test datasets. Finally, the framework is utilized to design two applications to improve the cost and performance of stide-like detectors which are based on sequence analysis. The first application reduces the cost of developing AID detectors by identifying the critical sections in the training dataset, and the second application identifies the intrusion context in the intrusive dataset, that helps to fine-tune the detectors. Such fine-tuning in turn helps to improve detection rate and reduce false alarm rate, thereby increasing the effectiveness and efficiency of the intrusion detectors.",
        "published": "2004-10-26T02:57:56Z",
        "link": "http://arxiv.org/abs/cs/0410068v1",
        "categories": [
            "cs.CR",
            "cs.AI"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: First Field Experience",
        "authors": [
            "Patrick C. McGuire",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Jose Antonio Rodriguez-Manfredi",
            "Javier Gomez-Elvira",
            "Helge Ritter",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist and field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: first, detection and accounting for shadows caused by 3D jagged edges in the outcrop; second, reincorporation of more sophisticated texture-analysis algorithms into the system; third, creation of hardware and software capabilities to control the camera's zoom lens in an intelligent manner; and fourth, development of algorithms for interpretation of complex geological scenery. Nonetheless, despite these technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",
        "published": "2004-10-27T09:14:01Z",
        "link": "http://arxiv.org/abs/cs/0410071v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "q-bio.NC",
            "I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9"
        ]
    },
    {
        "title": "Intuitionistic computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic (CL) is a systematic formal theory of computational tasks and resources, which, in a sense, can be seen as a semantics-based alternative to (the syntactically introduced) linear logic. With its expressive and flexible language, where formulas represent computational problems and \"truth\" is understood as algorithmic solvability, CL potentially offers a comprehensive logical basis for constructive applied theories and computing systems inherently requiring constructive and computationally meaningful underlying logics.   Among the best known constructivistic logics is Heyting's intuitionistic calculus INT, whose language can be seen as a special fragment of that of CL. The constructivistic philosophy of INT, however, has never really found an intuitively convincing and mathematically strict semantical justification. CL has good claims to provide such a justification and hence a materialization of Kolmogorov's known thesis \"INT = logic of problems\". The present paper contains a soundness proof for INT with respect to the CL semantics. A comprehensive online source on CL is available at http://www.cis.upenn.edu/~giorgi/cl.html",
        "published": "2004-11-04T18:15:19Z",
        "link": "http://arxiv.org/abs/cs/0411008v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.4.1; F.1.2"
        ]
    },
    {
        "title": "Bounded Input Bounded Predefined Control Bounded Output",
        "authors": [
            "Ziny Flikop"
        ],
        "summary": "The paper is an attempt to generalize a methodology, which is similar to the bounded-input bounded-output method currently widely used for the system stability studies. The presented earlier methodology allows decomposition of input space into bounded subspaces and defining for each subspace its bounding surface. It also defines a corresponding predefined control, which maps any point of a bounded input into a desired bounded output subspace. This methodology was improved by providing a mechanism for the fast defining a bounded surface. This paper presents enhanced bounded-input bounded-predefined-control bounded-output approach, which provides adaptability feature to the control and allows transferring of a controlled system along a suboptimal trajectory.",
        "published": "2004-11-08T01:52:58Z",
        "link": "http://arxiv.org/abs/cs/0411015v1",
        "categories": [
            "cs.AI",
            "I.2.8 I.2.9"
        ]
    },
    {
        "title": "Intelligent search strategies based on adaptive Constraint Handling   Rules",
        "authors": [
            "Armin Wolf"
        ],
        "summary": "The most advanced implementation of adaptive constraint processing with Constraint Handling Rules (CHR) allows the application of intelligent search strategies to solve Constraint Satisfaction Problems (CSP). This presentation compares an improved version of conflict-directed backjumping and two variants of dynamic backtracking with respect to chronological backtracking on some of the AIM instances which are a benchmark set of random 3-SAT problems. A CHR implementation of a Boolean constraint solver combined with these different search strategies in Java is thus being compared with a CHR implementation of the same Boolean constraint solver combined with chronological backtracking in SICStus Prolog. This comparison shows that the addition of ``intelligence'' to the search process may reduce the number of search steps dramatically. Furthermore, the runtime of their Java implementations is in most cases faster than the implementations of chronological backtracking. More specifically, conflict-directed backjumping is even faster than the SICStus Prolog implementation of chronological backtracking, although our Java implementation of CHR lacks the optimisations made in the SICStus Prolog system. To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2004-11-08T08:32:40Z",
        "link": "http://arxiv.org/abs/cs/0411016v2",
        "categories": [
            "cs.AI",
            "cs.PL",
            "I.1.4"
        ]
    },
    {
        "title": "Topological Navigation of Simulated Robots using Occupancy Grid",
        "authors": [
            "Richard Szabo"
        ],
        "summary": "Formerly I presented a metric navigation method in the Webots mobile robot simulator. The navigating Khepera-like robot builds an occupancy grid of the environment and explores the square-shaped room around with a value iteration algorithm. Now I created a topological navigation procedure based on the occupancy grid process. The extension by a skeletonization algorithm results a graph of important places and the connecting routes among them. I also show the significant time profit gained during the process.",
        "published": "2004-11-08T20:22:52Z",
        "link": "http://arxiv.org/abs/cs/0411022v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Bionic Humans Using EAP as Artificial Muscles Reality and Challenges",
        "authors": [
            "Yoseph Bar-Cohen"
        ],
        "summary": "For many years, the idea of a human with bionic muscles immediately conjures up science fiction images of a TV series superhuman character that was implanted with bionic muscles and portrayed with strength and speed far superior to any normal human. As fantastic as this idea may seem, recent developments in electroactive polymers (EAP) may one day make such bionics possible. Polymers that exhibit large displacement in response to stimulation that is other than electrical signal were known for many years. Initially, EAP received relatively little attention due to their limited actuation capability. However, in the recent years, the view of the EAP materials has changed due to the introduction of effective new materials that significantly surpassed the capability of the widely used piezoelectric polymer, PVDF. As this technology continues to evolve, novel mechanisms that are biologically inspired are expected to emerge. EAP materials can potentially provide actuation with lifelike response and more flexible configurations. While further improvements in performance and robustness are still needed, there already have been several reported successes. In recognition of the need for cooperation in this multidisciplinary field, the author initiated and organized a series of international forums that are leading to a growing number of research and development projects and to great advances in the field. In 1999, he challenged the worldwide science and engineering community of EAP experts to develop a robotic arm that is actuated by artificial muscles to win a wrestling match against a human opponent. In this paper, the field of EAP as artificial muscles will be reviewed covering the state of the art, the challenges and the vision for the progress in future years.",
        "published": "2004-11-08T20:32:11Z",
        "link": "http://arxiv.org/abs/cs/0411025v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Artificial Intelligence and Systems Theory: Applied to Cooperative   Robots",
        "authors": [
            "Pedro U. Lima",
            "Luis M. M. Custodio"
        ],
        "summary": "This paper describes an approach to the design of a population of cooperative robots based on concepts borrowed from Systems Theory and Artificial Intelligence. The research has been developed under the SocRob project, carried out by the Intelligent Systems Laboratory at the Institute for Systems and Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the project stands both for \"Society of Robots\" and \"Soccer Robots\", the case study where we are testing our population of robots. Designing soccer robots is a very challenging problem, where the robots must act not only to shoot a ball towards the goal, but also to detect and avoid static (walls, stopped robots) and dynamic (moving robots) obstacles. Furthermore, they must cooperate to defeat an opposing team. Our past and current research in soccer robotics includes cooperative sensor fusion for world modeling, object recognition and tracking, robot navigation, multi-robot distributed task planning and coordination, including cooperative reinforcement learning in cooperative and adversarial environments, and behavior-based architectures for real time task execution of cooperating robot teams.",
        "published": "2004-11-08T20:41:44Z",
        "link": "http://arxiv.org/abs/cs/0411018v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Generating Conditional Probabilities for Bayesian Networks: Easing the   Knowledge Acquisition Problem",
        "authors": [
            "Balaram Das"
        ],
        "summary": "The number of probability distributions required to populate a conditional probability table (CPT) in a Bayesian network, grows exponentially with the number of parent-nodes associated with that table. If the table is to be populated through knowledge elicited from a domain expert then the sheer magnitude of the task forms a considerable cognitive barrier. In this paper we devise an algorithm to populate the CPT while easing the extent of knowledge acquisition. The input to the algorithm consists of a set of weights that quantify the relative strengths of the influences of the parent-nodes on the child-node, and a set of probability distributions the number of which grows only linearly with the number of associated parent-nodes. These are elicited from the domain expert. The set of probabilities are obtained by taking into consideration the heuristics that experts use while arriving at probabilistic estimations. The algorithm is used to populate the CPT by computing appropriate weighted sums of the elicited distributions. We invoke the methods of information geometry to demonstrate how these weighted sums capture the expert's judgemental strategy.",
        "published": "2004-11-12T00:42:55Z",
        "link": "http://arxiv.org/abs/cs/0411034v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "A FP-Tree Based Approach for Mining All Strongly Correlated Pairs   without Candidate Generation",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "Given a user-specified minimum correlation threshold and a transaction database, the problem of mining all-strong correlated pairs is to find all item pairs with Pearson's correlation coefficients above the threshold . Despite the use of upper bound based pruning technique in the Taper algorithm [1], when the number of items and transactions are very large, candidate pair generation and test is still costly. To avoid the costly test of a large number of candidate pairs, in this paper, we propose an efficient algorithm, called Tcp, based on the well-known FP-tree data structure, for mining the complete set of all-strong correlated item pairs. Our experimental results on both synthetic and real world datasets show that, Tcp's performance is significantly better than that of the previously developed Taper algorithm over practical ranges of correlation threshold specifications.",
        "published": "2004-11-12T12:02:17Z",
        "link": "http://arxiv.org/abs/cs/0411035v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Comparing Multi-Target Trackers on Different Force Unit Levels",
        "authors": [
            "Hedvig Sidenbladh",
            "Pontus Svenson",
            "Johan Schubert"
        ],
        "summary": "Consider the problem of tracking a set of moving targets. Apart from the tracking result, it is often important to know where the tracking fails, either to steer sensors to that part of the state-space, or to inform a human operator about the status and quality of the obtained information. An intuitive quality measure is the correlation between two tracking results based on uncorrelated observations. In the case of Bayesian trackers such a correlation measure could be the Kullback-Leibler difference.   We focus on a scenario with a large number of military units moving in some terrain. The units are observed by several types of sensors and \"meta-sensors\" with force aggregation capabilities. The sensors register units of different size. Two separate multi-target probability hypothesis density (PHD) particle filters are used to track some type of units (e.g., companies) and their sub-units (e.g., platoons), respectively, based on observations of units of those sizes. Each observation is used in one filter only.   Although the state-space may well be the same in both filters, the posterior PHD distributions are not directly comparable -- one unit might correspond to three or four spatially distributed sub-units. Therefore, we introduce a mapping function between distributions for different unit size, based on doctrine knowledge of unit configuration.   The mapped distributions can now be compared -- locally or globally -- using some measure, which gives the correlation between two PHD distributions in a bounded volume of the state-space. To locate areas where the tracking fails, a discretized quality map of the state-space can be generated by applying the measure locally to different parts of the space.",
        "published": "2004-11-19T13:12:40Z",
        "link": "http://arxiv.org/abs/cs/0411071v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Extremal optimization for sensor report pre-processing",
        "authors": [
            "Pontus Svenson"
        ],
        "summary": "We describe the recently introduced extremal optimization algorithm and apply it to target detection and association problems arising in pre-processing for multi-target tracking.   Here we consider the problem of pre-processing for multiple target tracking when the number of sensor reports received is very large and arrives in large bursts. In this case, it is sometimes necessary to pre-process reports before sending them to tracking modules in the fusion system. The pre-processing step associates reports to known tracks (or initializes new tracks for reports on objects that have not been seen before). It could also be used as a pre-process step before clustering, e.g., in order to test how many clusters to use.   The pre-processing is done by solving an approximate version of the original problem. In this approximation, not all pair-wise conflicts are calculated. The approximation relies on knowing how many such pair-wise conflicts that are necessary to compute. To determine this, results on phase-transitions occurring when coloring (or clustering) large random instances of a particular graph ensemble are used.",
        "published": "2004-11-19T13:37:40Z",
        "link": "http://arxiv.org/abs/cs/0411072v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "A Note on the PAC Bayesian Theorem",
        "authors": [
            "Andreas Maurer"
        ],
        "summary": "We prove general exponential moment inequalities for averages of [0,1]-valued iid random variables and use them to tighten the PAC Bayesian Theorem. The logarithmic dependence on the sample count in the enumerator of the PAC Bayesian bound is halved.",
        "published": "2004-11-30T08:36:59Z",
        "link": "http://arxiv.org/abs/cs/0411099v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.5.1"
        ]
    },
    {
        "title": "Self-Organizing Traffic Lights",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "Steering traffic in cities is a very complex task, since improving efficiency involves the coordination of many actors. Traditional approaches attempt to optimize traffic lights for a particular density and configuration of traffic. The disadvantage of this lies in the fact that traffic densities and configurations change constantly. Traffic seems to be an adaptation problem rather than an optimization problem. We propose a simple and feasible alternative, in which traffic lights self-organize to improve traffic flow. We use a multi-agent simulation to study three self-organizing methods, which are able to outperform traditional rigid and adaptive methods. Using simple rules and no direct communication, traffic lights are able to self-organize and adapt to changing traffic conditions, reducing waiting times, number of stopped cars, and increasing average speeds.",
        "published": "2004-11-30T17:25:00Z",
        "link": "http://arxiv.org/abs/nlin/0411066v2",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Ranking Pages by Topology and Popularity within Web Sites",
        "authors": [
            "Jose Borges",
            "Mark Levene"
        ],
        "summary": "We compare two link analysis ranking methods of web pages in a site. The first, called Site Rank, is an adaptation of PageRank to the granularity of a web site and the second, called Popularity Rank, is based on the frequencies of user clicks on the outlinks in a page that are captured by navigation sessions of users through the web site. We ran experiments on artificially created web sites of different sizes and on two real data sets, employing the relative entropy to compare the distributions of the two ranking methods. For the real data sets we also employ a nonparametric measure, called Spearman's footrule, which we use to compare the top-ten web pages ranked by the two methods. Our main result is that the distributions of the Popularity Rank and Site Rank are surprisingly close to each other, implying that the topology of a web site is very instrumental in guiding users through the site. Thus, in practice, the Site Rank provides a reasonable first order approximation of the aggregate behaviour of users within a web site given by the Popularity Rank.",
        "published": "2004-12-01T13:22:47Z",
        "link": "http://arxiv.org/abs/cs/0412002v3",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Modeling Complex Higher Order Patterns",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The goal of this paper is to show that generalizing the notion of frequent patterns can be useful in extending association analysis to more complex higher order patterns. To that end, we describe a general framework for modeling a complex pattern based on evaluating the interestingness of its sub-patterns. A key goal of any framework is to allow people to more easily express, explore, and communicate ideas, and hence, we illustrate how our framework can be used to describe a variety of commonly used patterns, such as frequent patterns, frequent closed patterns, indirect association patterns, hub patterns and authority patterns. To further illustrate the usefulness of the framework, we also present two new kinds of patterns that derived from the framework: clique pattern and bi-clique pattern and illustrate their practical use.",
        "published": "2004-12-04T12:28:29Z",
        "link": "http://arxiv.org/abs/cs/0412018v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "A Link Clustering Based Approach for Clustering Categorical Data",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "Categorical data clustering (CDC) and link clustering (LC) have been considered as separate research and application areas. The main focus of this paper is to investigate the commonalities between these two problems and the uses of these commonalities for the creation of new clustering algorithms for categorical data based on cross-fertilization between the two disjoint research fields. More precisely, we formally transform the CDC problem into an LC problem, and apply LC approach for clustering categorical data. Experimental results on real datasets show that LC based clustering method is competitive with existing CDC algorithms with respect to clustering accuracy.",
        "published": "2004-12-04T12:41:08Z",
        "link": "http://arxiv.org/abs/cs/0412019v1",
        "categories": [
            "cs.DL",
            "cs.AI"
        ]
    },
    {
        "title": "Finite Domain Bounds Consistency Revisited",
        "authors": [
            "Chiu Wo Choi",
            "Warwick Harvey",
            "Jimmy Ho-Man Lee",
            "Peter J. Stuckey"
        ],
        "summary": "A widely adopted approach to solving constraint satisfaction problems combines systematic tree search with constraint propagation for pruning the search space. Constraint propagation is performed by propagators implementing a certain notion of consistency. Bounds consistency is the method of choice for building propagators for arithmetic constraints and several global constraints in the finite integer domain. However, there has been some confusion in the definition of bounds consistency. In this paper we clarify the differences and similarities among the three commonly used notions of bounds consistency.",
        "published": "2004-12-06T08:04:03Z",
        "link": "http://arxiv.org/abs/cs/0412021v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Multidimensional data classification with artificial neural networks",
        "authors": [
            "P. Boinee",
            "F. Barbarino",
            "A. De Angelis"
        ],
        "summary": "Multi-dimensional data classification is an important and challenging problem in many astro-particle experiments. Neural networks have proved to be versatile and robust in multi-dimensional data classification. In this article we shall study the classification of gamma from the hadrons for the MAGIC Experiment. Two neural networks have been used for the classification task. One is Multi-Layer Perceptron based on supervised learning and other is Self-Organising Map (SOM), which is based on unsupervised learning technique. The results have been shown and the possible ways of combining these networks have been proposed to yield better and faster classification results.",
        "published": "2004-12-06T20:23:15Z",
        "link": "http://arxiv.org/abs/cs/0412023v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.1.1; K.3.2; I.2.6"
        ]
    },
    {
        "title": "Removing Propagation Redundant Constraints in Redundant Modeling",
        "authors": [
            "Chiu Wo Choi",
            "Jimmy Ho-Man Lee",
            "Peter J. Stuckey"
        ],
        "summary": "A widely adopted approach to solving constraint satisfaction problems combines systematic tree search with various degrees of constraint propagation for pruning the search space. One common technique to improve the execution efficiency is to add redundant constraints, which are constraints logically implied by others in the problem model. However, some redundant constraints are propagation redundant and hence do not contribute additional propagation information to the constraint solver. Redundant constraints arise naturally in the process of redundant modeling where two models of the same problem are connected and combined through channeling constraints. In this paper, we give general theorems for proving propagation redundancy of one constraint with respect to channeling constraints and constraints in the other model. We illustrate, on problems from CSPlib (http://www.csplib.org/), how detecting and removing propagation redundant constraints in redundant modeling can significantly speed up constraint solving.",
        "published": "2004-12-07T06:31:34Z",
        "link": "http://arxiv.org/abs/cs/0412026v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.3.3; F.4.1"
        ]
    },
    {
        "title": "An Efficient and Flexible Engine for Computing Fixed Points",
        "authors": [
            "Hai-Feng Guo",
            "Gopal Gupta"
        ],
        "summary": "An efficient and flexible engine for computing fixed points is critical for many practical applications. In this paper, we firstly present a goal-directed fixed point computation strategy in the logic programming paradigm. The strategy adopts a tabled resolution (or memorized resolution) to mimic the efficient semi-naive bottom-up computation. Its main idea is to dynamically identify and record those clauses that will lead to recursive variant calls, and then repetitively apply those alternatives incrementally until the fixed point is reached. Secondly, there are many situations in which a fixed point contains a large number or even infinite number of solutions. In these cases, a fixed point computation engine may not be efficient enough or feasible at all. We present a mode-declaration scheme which provides the capabilities to reduce a fixed point from a big solution set to a preferred small one, or from an infeasible infinite set to a finite one. The mode declaration scheme can be characterized as a meta-level operation over the original fixed point. We show the correctness of the mode declaration scheme. Thirdly, the mode-declaration scheme provides a new declarative method for dynamic programming, which is typically used for solving optimization problems. There is no need to define the value of an optimal solution recursively, instead, defining a general solution suffices. The optimal value as well as its corresponding concrete solution can be derived implicitly and automatically using a mode-directed fixed point computation engine. Finally, this fixed point computation engine has been successfully implemented in a commercial Prolog system. Experimental results are shown to indicate that the mode declaration improves both time and space performances in solving dynamic programming problems.",
        "published": "2004-12-09T22:59:37Z",
        "link": "http://arxiv.org/abs/cs/0412041v2",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Neural Networks in Mobile Robot Motion",
        "authors": [
            "Danica Janglova"
        ],
        "summary": "This paper deals with a path planning and intelligent control of an autonomous robot which should move safely in partially structured environment. This environment may involve any number of obstacles of arbitrary shape and size; some of them are allowed to move. We describe our approach to solving the motion-planning problem in mobile robot control using neural networks-based technique. Our method of the construction of a collision-free path for moving robot among obstacles is based on two neural networks. The first neural network is used to determine the \"free\" space using ultrasound range finder data. The second neural network \"finds\" a safe direction for the next robot section of the path in the workspace while avoiding the nearest obstacles. Simulation examples of generated path with proposed techniques will be presented.",
        "published": "2004-12-11T12:32:10Z",
        "link": "http://arxiv.org/abs/cs/0412049v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Clustering Categorical Data Streams",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng",
            "Joshua Zhexue Huang"
        ],
        "summary": "The data stream model has been defined for new classes of applications involving massive data being generated at a fast pace. Web click stream analysis and detection of network intrusions are two examples. Cluster analysis on data streams becomes more difficult, because the data objects in a data stream must be accessed in order and can be read only once or few times with limited resources. Recently, a few clustering algorithms have been developed for analyzing numeric data streams. However, to our knowledge to date, no algorithm exists for clustering categorical data streams. In this paper, we propose an efficient clustering algorithm for analyzing categorical data streams. It has been proved that the proposed algorithm uses small memory footprints. We provide empirical analysis on the performance of the algorithm in clustering both synthetic and real data streams",
        "published": "2004-12-13T06:14:20Z",
        "link": "http://arxiv.org/abs/cs/0412058v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Vector Symbolic Architectures answer Jackendoff's challenges for   cognitive neuroscience",
        "authors": [
            "Ross W. Gayler"
        ],
        "summary": "Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.",
        "published": "2004-12-13T08:00:55Z",
        "link": "http://arxiv.org/abs/cs/0412059v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.5.1; I.2.0, I.2.6"
        ]
    },
    {
        "title": "ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion   Detection System",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "summary": "Security of computers and the networks that connect them is increasingly becoming of great significance. Computer security is defined as the protection of computing systems against threats to confidentiality, integrity, and availability. There are two types of intruders: the external intruders who are unauthorized users of the machines they attack, and internal intruders, who have permission to access the system with some restrictions. Due to the fact that it is more and more improbable to a system administrator to recognize and manually intervene to stop an attack, there is an increasing recognition that ID systems should have a lot to earn on following its basic principles on the behavior of complex natural systems, namely in what refers to self-organization, allowing for a real distributed and collective perception of this phenomena. With that aim in mind, the present work presents a self-organized ant colony based intrusion detection system (ANTIDS) to detect intrusions in a network infrastructure. The performance is compared among conventional soft computing paradigms like Decision Trees, Support Vector Machines and Linear Genetic Programming to model fast, online and efficient intrusion detection systems.",
        "published": "2004-12-17T13:17:01Z",
        "link": "http://arxiv.org/abs/cs/0412068v1",
        "categories": [
            "cs.CR",
            "cs.AI",
            "H.3.3; I.2.11;I.5"
        ]
    },
    {
        "title": "Swarming around Shellfish Larvae",
        "authors": [
            "Vitorino Ramos",
            "Jonathan Campbell",
            "John Slater",
            "John Gillespie",
            "Ivan F. Bendezu",
            "Fionn Murtagh"
        ],
        "summary": "The collection of wild larvae seed as a source of raw material is a major sub industry of shellfish aquaculture. To predict when, where and in what quantities wild seed will be available, it is necessary to track the appearance and growth of planktonic larvae. One of the most difficult groups to identify, particularly at the species level are the Bivalvia. This difficulty arises from the fact that fundamentally all bivalve larvae have a similar shape and colour. Identification based on gross morphological appearance is limited by the time-consuming nature of the microscopic examination and by the limited availability of expertise in this field. Molecular and immunological methods are also being studied. We describe the application of computational pattern recognition methods to the automated identification and size analysis of scallop larvae. For identification, the shape features used are binary invariant moments; that is, the features are invariant to shift (position within the image), scale (induced either by growth or differential image magnification) and rotation. Images of a sample of scallop and non-scallop larvae covering a range of maturities have been analysed. In order to overcome the automatic identification, as well as to allow the system to receive new unknown samples at any moment, a self-organized and unsupervised ant-like clustering algorithm based on Swarm Intelligence is proposed, followed by simple k-NNR nearest neighbour classification on the final map. Results achieve a full recognition rate of 100% under several situations (k =1 or 3).",
        "published": "2004-12-17T13:30:30Z",
        "link": "http://arxiv.org/abs/cs/0412069v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "Web Usage Mining Using Artificial Ant Colony Clustering and Genetic   Programming",
        "authors": [
            "Ajith Abraham",
            "Vitorino Ramos"
        ],
        "summary": "The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. The study of ant colonies behavior and their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences, because it provides models of distributed adaptive organization, which are useful to solve difficult optimization, classification, and distributed control problems, among others. In this paper, we propose an ant clustering algorithm to discover Web usage patterns (data clusters) and a linear genetic programming approach to analyze the visitor trends. Empirical results clearly shows that ant colony clustering performs well when compared to a self-organizing map (for clustering Web usage patterns) even though the performance accuracy is not that efficient when comparared to evolutionary-fuzzy clustering (i-miner) approach. KEYWORDS: Web Usage Mining, Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Linear Genetic Programming.",
        "published": "2004-12-17T14:58:38Z",
        "link": "http://arxiv.org/abs/cs/0412071v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "Swarms on Continuous Data",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "summary": "While being it extremely important, many Exploratory Data Analysis (EDA) systems have the inhability to perform classification and visualization in a continuous basis or to self-organize new data-items into the older ones (evenmore into new labels if necessary), which can be crucial in KDD - Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online forms of Web Applications are just one example). This disadvantge is also present in more recent approaches using Self-Organizing Maps. On the present work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems a robust online classifier is presented, which produces class decisions on a continuous stream data, allowing for continuous mappings. Results show that increasingly better results are achieved, as demonstraded by other authors in different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous Classification.",
        "published": "2004-12-17T15:05:40Z",
        "link": "http://arxiv.org/abs/cs/0412072v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "Self-Organizing the Abstract: Canvas as a Swarm Habitat for Collective   Memory, Perception and Cooperative Distributed Creativity",
        "authors": [
            "Vitorino Ramos"
        ],
        "summary": "Past experiences under the designation of \"Swarm Paintings\" conducted in 2001, not only confirmed the possibility of realizing an artificial art (thus non-human), as introduced into the process the questioning of creative migration, specifically from the computer monitors to the canvas via a robotic harm. In more recent self-organized based research we seek to develop and profound the initial ideas by using a swarm of autonomous robots (ARTsBOT project 2002-03), that \"live\" avoiding the purpose of being merely a simple perpetrator of order streams coming from an external computer, but instead, that actually co-evolve within the canvas space, acting (that is, laying ink) according to simple inner threshold stimulus response functions, reacting simultaneously to the chromatic stimulus present in the canvas environment done by the passage of their team-mates, as well as by the distributed feedback, affecting their future collective behaviour. In parallel, and in what respects to certain types of collective systems, we seek to confirm, in a physically embedded way, that the emergence of order (even as a concept) seems to be found at a lower level of complexity, based on simple and basic interchange of information, and on the local dynamic of parts, who, by self-organizing mechanisms tend to form an lived whole, innovative and adapting, allowing for emergent open-ended creative and distributed production. KEYWORDS: ArtSBots Project, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation, Art and Complexity, ArtBots: The Robot Talent Show.",
        "published": "2004-12-17T15:36:05Z",
        "link": "http://arxiv.org/abs/cs/0412073v1",
        "categories": [
            "cs.MM",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Self-Organized Stigmergic Document Maps: Environment as a Mechanism for   Context Learning",
        "authors": [
            "Vitorino Ramos",
            "Juan J. Merelo"
        ],
        "summary": "Social insect societies and more specifically ant colonies, are distributed systems that, in spite of the simplicity of their individuals, present a highly structured social organization. As a result of this organization, ant colonies can accomplish complex tasks that in some cases exceed the individual capabilities of a single ant. The study of ant colonies behavior and of their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences, because it provides models of distributed adaptive organization which are useful to solve difficult optimization, classification, and distributed control problems, among others. In the present work we overview some models derived from the observation of real ants, emphasizing the role played by stigmergy as distributed communication paradigm, and we present a novel strategy to tackle unsupervised clustering as well as data retrieval problems. The present ant clustering system (ACLUSTER) avoids not only short-term memory based strategies, as well as the use of several artificial ant types (using different speeds), present in some recent approaches. Moreover and according to our knowledge, this is also the first application of ant systems into textual document clustering. KEYWORDS: Swarm Intelligence, Ant Systems, Unsupervised Clustering, Data Retrieval, Data Mining, Distributed Computing, Document Maps, Textual Document Clustering.",
        "published": "2004-12-17T15:47:44Z",
        "link": "http://arxiv.org/abs/cs/0412075v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "Clustering Techniques for Marbles Classification",
        "authors": [
            "J. R. Caldas-Pinto",
            "Pedro Pina",
            "Vitorino Ramos",
            "Mario Ramalho"
        ],
        "summary": "Automatic marbles classification based on their visual appearance is an important industrial issue. However, there is no definitive solution to the problem mainly due to the presence of randomly distributed high number of different colours and its subjective evaluation by the human expert. In this paper we present a study of segmentation techniques, we evaluate they overall performance using a training set and standard quality measures and finally we apply different clustering techniques to automatically classify the marbles. KEYWORDS: Segmentation, Clustering, Quadtrees, Learning Vector Quantization (LVQ), Simulated Annealing (SA).",
        "published": "2004-12-17T15:55:46Z",
        "link": "http://arxiv.org/abs/cs/0412076v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.2; I.5"
        ]
    },
    {
        "title": "On the Implicit and on the Artificial - Morphogenesis and Emergent   Aesthetics in Autonomous Collective Systems",
        "authors": [
            "Vitorino Ramos"
        ],
        "summary": "Imagine a \"machine\" where there is no pre-commitment to any particular representational scheme: the desired behaviour is distributed and roughly specified simultaneously among many parts, but there is minimal specification of the mechanism required to generate that behaviour, i.e. the global behaviour evolves from the many relations of multiple simple behaviours. A machine that lives to and from/with Synergy. An artificial super-organism that avoids specific constraints and emerges within multiple low-level implicit bio-inspired mechanisms. KEYWORDS: Complex Science, ArtSBots Project, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation, Art and Complexity, ArtBots: The Robot Talent Show.",
        "published": "2004-12-17T16:15:08Z",
        "link": "http://arxiv.org/abs/cs/0412077v1",
        "categories": [
            "cs.AI",
            "cs.MM",
            "I.2; I.6"
        ]
    },
    {
        "title": "The MC2 Project [Machines of Collective Conscience]: A possible walk, up   to Life-like Complexity and Behaviour, from bottom, basic and simple   bio-inspired heuristics - a walk, up into the morphogenesis of information",
        "authors": [
            "Vitorino Ramos"
        ],
        "summary": "Synergy (from the Greek word synergos), broadly defined, refers to combined or co-operative effects produced by two or more elements (parts or individuals). The definition is often associated with the holistic conviction quote that \"the whole is greater than the sum of its parts\" (Aristotle, in Metaphysics), or the whole cannot exceed the sum of the energies invested in each of its parts (e.g. first law of thermodynamics) even if it is more accurate to say that the functional effects produced by wholes are different from what the parts can produce alone. Synergy is a ubiquitous phenomena in nature and human societies alike. One well know example is provided by the emergence of self-organization in social insects, via direct or indirect interactions. The latter types are more subtle and defined as stigmergy to explain task coordination and regulation in the context of nest reconstruction in termites. An example, could be provided by two individuals, who interact indirectly when one of them modifies the environment and the other responds to the new environment at a later time. In other words, stigmergy could be defined as a particular case of environmental or spatial synergy. The system is purely holistic, and their properties are intrinsically emergent and autocatalytic. On the present work we present a \"machine\" where there is no precommitment to any particular representational scheme: the desired behaviour is distributed and roughly specified simultaneously among many parts, but there is minimal specification of the mechanism required to generate that behaviour, i.e. the global behaviour evolves from the many relations of multiple simple behaviours.",
        "published": "2004-12-17T16:28:26Z",
        "link": "http://arxiv.org/abs/cs/0412079v1",
        "categories": [
            "cs.AI",
            "cs.MM",
            "I.2.11"
        ]
    },
    {
        "title": "The Biological Concept of Neoteny in Evolutionary Colour Image   Segmentation - Simple Experiments in Simple Non-Memetic Genetic Algorithms",
        "authors": [
            "Vitorino Ramos"
        ],
        "summary": "Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of existing organisms may give rise, under certain circumstances, to wholly new organisms. Although the present work does not pretend to model or simulate the biological details of such a concept in any way, these ideas were incorporated by a rather simple abstract computational strategy, in order to allow (if possible) for faster convergence into simple non-memetic Genetic Algorithms, i.e. without using local improvement procedures (e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic Algorithm was used for colour image segmentation purposes by using K-mean unsupervised clustering methods, namely for guiding the evolutionary algorithm in his search for finding the optimal or sub-optimal data partition. Average results suggest that the use of neotonic strategies by employing juvenile genotypes into the later generations and the use of linear-dynamic mutation rates instead of constant, can increase fitness values by 58% comparing to classical Genetic Algorithms, independently from the starting population characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.",
        "published": "2004-12-17T16:39:33Z",
        "link": "http://arxiv.org/abs/cs/0412080v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2; I.5"
        ]
    },
    {
        "title": "Artificial Neoteny in Evolutionary Image Segmentation",
        "authors": [
            "Vitorino Ramos"
        ],
        "summary": "Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of existing organisms may give rise, under certain circumstances, to wholly new organisms. Although the present work does not pretend to model or simulate the biological details of such a concept in any way, these ideas were incorporated by a rather simple abstract computational strategy, in order to allow (if possible) for faster convergence into simple non-memetic Genetic Algorithms, i.e. without using local improvement procedures (e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic Algorithm was used for colour image segmentation purposes by using K-mean unsupervised clustering methods, namely for guiding the evolutionary algorithm in his search for finding the optimal or sub-optimal data partition. Average results suggest that the use of neotonic strategies by employing juvenile genotypes into the later generations and the use of linear-dynamic mutation rates instead of constant, can increase fitness values by 58% comparing to classical Genetic Algorithms, independently from the starting population characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.",
        "published": "2004-12-17T16:44:54Z",
        "link": "http://arxiv.org/abs/cs/0412081v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2; I.5"
        ]
    },
    {
        "title": "Line and Word Matching in Old Documents",
        "authors": [
            "A. Marcolino",
            "Vitorino Ramos",
            "Mario Ramalho",
            "J. R. Caldas Pinto"
        ],
        "summary": "This paper is concerned with the problem of establishing an index based on word matching. It is assumed that the book was digitised as better as possible and some pre-processing techniques were already applied as line orientation correction and some noise removal. However two main factor are responsible for being not possible to apply ordinary optical character recognition techniques (OCR): the presence of antique fonts and the degraded state of many characters due to unrecoverable original time degradation. In this paper we make a short introduction to word segmentation that involves finding the lines that characterise a word. After we discuss different approaches for word matching and how they can be combined to obtain an ordered list for candidate words for the matching. This discussion will be illustrated by examples.",
        "published": "2004-12-17T16:58:52Z",
        "link": "http://arxiv.org/abs/cs/0412083v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.2; I.5"
        ]
    },
    {
        "title": "Map Segmentation by Colour Cube Genetic K-Mean Clustering",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "summary": "Segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. In this work, a method is described for evolving adaptive procedures for these problems. In many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be mapped into topological domains. We formulate the segmentation problem upon such images as an optimisation problem and adopt evolutionary strategy of Genetic Algorithms for the clustering of small regions in colour feature space. The present approach uses k-Means unsupervised clustering methods into Genetic Algorithms, namely for guiding this last Evolutionary Algorithm in his search for finding the optimal or sub-optimal data partition, task that as we know, requires a non-trivial search because of its NP-complete nature. To solve this task, the appropriate genetic coding is also discussed, since this is a key aspect in the implementation. Our purpose is to demonstrate the efficiency of Genetic Algorithms to automatic and unsupervised texture segmentation. Some examples in Colour Maps are presented and overall results discussed. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.",
        "published": "2004-12-17T17:04:16Z",
        "link": "http://arxiv.org/abs/cs/0412084v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2; I.5"
        ]
    },
    {
        "title": "Artificial Ant Colonies in Digital Image Habitats - A Mass Behaviour   Effect Study on Pattern Recognition",
        "authors": [
            "Vitorino Ramos",
            "Filipe Almeida"
        ],
        "summary": "Some recent studies have pointed that, the self-organization of neurons into brain-like structures, and the self-organization of ants into a swarm are similar in many respects. If possible to implement, these features could lead to important developments in pattern recognition systems, where perceptive capabilities can emerge and evolve from the interaction of many simple local rules. The principle of the method is inspired by the work of Chialvo and Millonas who developed the first numerical simulation in which swarm cognitive map formation could be explained. From this point, an extended model is presented in order to deal with digital image habitats, in which artificial ants could be able to react to the environment and perceive it. Evolution of pheromone fields point that artificial ant colonies could react and adapt appropriately to any type of digital habitat. KEYWORDS: Swarm Intelligence, Self-Organization, Stigmergy, Artificial Ant Systems, Pattern Recognition and Perception, Image Segmentation, Gestalt Perception Theory, Distributed Computation.",
        "published": "2004-12-17T17:19:03Z",
        "link": "http://arxiv.org/abs/cs/0412086v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.2; I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "Image Colour Segmentation by Genetic Algorithms",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "summary": "Segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. In this work, a method is described for evolving adaptive procedures for these problems. In many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be mapped into topological domains. We formulate the segmentation problem upon such images as an optimisation problem and adopt evolutionary strategy of Genetic Algorithms for the clustering of small regions in colour feature space. The present approach uses k-Means unsupervised clustering methods into Genetic Algorithms, namely for guiding this last Evolutionary Algorithm in his search for finding the optimal or sub-optimal data partition, task that as we know, requires a non-trivial search because of its intrinsic NP-complete nature. To solve this task, the appropriate genetic coding is also discussed, since this is a key aspect in the implementation. Our purpose is to demonstrate the efficiency of Genetic Algorithms to automatic and unsupervised texture segmentation. Some examples in Colour Maps, Ornamental Stones and in Human Skin Mark segmentation are presented and overall results discussed. KEYWORDS: Genetic Algorithms, Colour Image Segmentation, Classification, Clustering.",
        "published": "2004-12-17T17:29:27Z",
        "link": "http://arxiv.org/abs/cs/0412087v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.2; I.5"
        ]
    },
    {
        "title": "On Image Filtering, Noise and Morphological Size Intensity Diagrams",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "summary": "In the absence of a pure noise-free image it is hard to define what noise is, in any original noisy image, and as a consequence also where it is, and in what amount. In fact, the definition of noise depends largely on our own aim in the whole image analysis process, and (perhaps more important) in our self-perception of noise. For instance, when we perceive noise as disconnected and small it is normal to use MM-ASF filters to treat it. There is two evidences of this. First, in many instances there is no ideal and pure noise-free image to compare our filtering process (nothing but our self-perception of its pure image); second, and related with this first point, MM transformations that we chose are only based on our self - and perhaps - fuzzy notion. The present proposal combines the results of two MM filtering transformations (FT1, FT2) and makes use of some measures and quantitative relations on their Size/Intensity Diagrams to find the most appropriate noise removal process. Results can also be used for finding the most appropriate stop criteria, and the right sequence of MM operators combination on Alternating Sequential Filters (ASF), if these measures are applied, for instance, on a Genetic Algorithm's target function.",
        "published": "2004-12-17T18:58:31Z",
        "link": "http://arxiv.org/abs/cs/0412088v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.5; I.2"
        ]
    },
    {
        "title": "From Feature Extraction to Classification: A multidisciplinary Approach   applied to Portuguese Granites",
        "authors": [
            "Vitorino Ramos",
            "Pedro Pina",
            "Fernando Muge"
        ],
        "summary": "The purpose of this paper is to present a complete methodology based on a multidisciplinary approach, that goes from the extraction of features till the classification of a set of different portuguese granites. The set of tools to extract the features that characterise polished surfaces of the granites is mainly based on mathematical morphology. The classification methodology is based on a genetic algorithm capable of search the input feature space used by the nearest neighbour rule classifier. Results show that is adequate to perform feature reduction and simultaneous improve the recognition rate. Moreover, the present methodology represents a robust strategy to understand the proper nature of the images treated, and their discriminant features. KEYWORDS: Portuguese grey granites, feature extraction, mathematical morphology, feature reduction, genetic algorithms, nearest neighbour rule classifiers (k-NNR).",
        "published": "2004-12-17T19:04:35Z",
        "link": "http://arxiv.org/abs/cs/0412066v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.2; I.5"
        ]
    },
    {
        "title": "Less is More - Genetic Optimisation of Nearest Neighbour Classifiers",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "summary": "The present paper deals with optimisation of Nearest Neighbour rule Classifiers via Genetic Algorithms. The methodology consists on implement a Genetic Algorithm capable of search the input feature space used by the NNR classifier. Results show that is adequate to perform feature reduction and simultaneous improve the Recognition Rate. Some practical examples prove that is possible to Recognise Portuguese Granites in 100%, with only 3 morphological features (from an original set of 117 features), which is well suited for real time applications. Moreover, the present method represents a robust strategy to understand the proper nature of the images treated, and their discriminant features. KEYWORDS: Feature Reduction, Genetic Algorithms, Nearest Neighbour Rule Classifiers (k-NNR).",
        "published": "2004-12-17T19:09:43Z",
        "link": "http://arxiv.org/abs/cs/0412070v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "I.2; I.5"
        ]
    },
    {
        "title": "The Combination of Paradoxical, Uncertain, and Imprecise Sources of   Information based on DSmT and Neutro-Fuzzy Inference",
        "authors": [
            "Florentin Smarandache",
            "Jean Dezert"
        ],
        "summary": "The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been, and still remains today, of primal importance for the development of reliable modern information systems involving artificial reasoning. In this chapter, we present a survey of our recent theory of plausible and paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the literature, developed for dealing with imprecise, uncertain and paradoxical sources of information. We focus our presentation here rather on the foundations of DSmT, and on the two important new rules of combination, than on browsing specific applications of DSmT available in literature. Several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach. The last part of this chapter concerns the presentation of the neutrosophic logic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and neutrosophic logic are useful tools in decision making after fusioning the information using the DSm hybrid rule of combination of masses.",
        "published": "2004-12-19T14:56:11Z",
        "link": "http://arxiv.org/abs/cs/0412091v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "The Google Similarity Distance",
        "authors": [
            "Rudi Cilibrasi",
            "Paul M. B. Vitanyi"
        ],
        "summary": "Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers the equivalent of `society' is `database,' and the equivalent of `use' is `way to search the database.' We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts we use the world-wide-web as database, and Google as search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the world-wide-web using Google page counts. The world-wide-web is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies, and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87% with the expert crafted WordNet categories.",
        "published": "2004-12-21T16:05:36Z",
        "link": "http://arxiv.org/abs/cs/0412098v3",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "On the existence of stable models of non-stratified logic programs",
        "authors": [
            "Stefania Costantini"
        ],
        "summary": "This paper introduces a fundamental result, which is relevant for Answer Set programming, and planning. For the first time since the definition of the stable model semantics, the class of logic programs for which a stable model exists is given a syntactic characterization. This condition may have a practical importance both for defining new algorithms for checking consistency and computing answer sets, and for improving the existing systems. The approach of this paper is to introduce a new canonical form (to which any logic program can be reduced to), to focus the attention on cyclic dependencies. The technical result is then given in terms of programs in canonical form (canonical programs), without loss of generality. The result is based on identifying the cycles contained in the program, showing that stable models of the overall program are composed of stable models of suitable sub-programs, corresponding to the cycles, and on defining the Cycle Graph. Each vertex of this graph corresponds to one cycle, and each edge corresponds to onehandle, which is a literal containing an atom that, occurring in both cycles, actually determines a connection between them. In fact, the truth value of the handle in the cycle where it appears as the head of a rule, influences the truth value of the atoms of the cycle(s) where it occurs in the body. We can therefore introduce the concept of a handle path, connecting different cycles. If for every odd cycle we can find a handle path with certain properties, then the existence of stable model is guaranteed.",
        "published": "2004-12-23T12:02:11Z",
        "link": "http://arxiv.org/abs/cs/0412105v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "About Unitary Rating Score Constructing",
        "authors": [
            "Kromer Victor"
        ],
        "summary": "It is offered to pool test points of different subjects and different aspects of the same subject together in order to get the unitary rating score, by the way of nonlinear transformation of indicator points in accordance with Zipf's distribution. It is proposed to use the well-studied distribution of Intellectuality Quotient IQ as the reference distribution for latent variable \"progress in studies\".",
        "published": "2004-01-08T07:50:51Z",
        "link": "http://arxiv.org/abs/cs/0401005v1",
        "categories": [
            "cs.LG",
            "1.2.6"
        ]
    },
    {
        "title": "Parametric Inference for Biological Sequence Analysis",
        "authors": [
            "Lior Pachter",
            "Bernd Sturmfels"
        ],
        "summary": "One of the major successes in computational biology has been the unification, using the graphical model formalism, of a multitude of algorithms for annotating and comparing biological sequences. Graphical models that have been applied towards these problems include hidden Markov models for annotation, tree models for phylogenetics, and pair hidden Markov models for alignment. A single algorithm, the sum-product algorithm, solves many of the inference problems associated with different statistical models. This paper introduces the \\emph{polytope propagation algorithm} for computing the Newton polytope of an observation from a graphical model. This algorithm is a geometric version of the sum-product algorithm and is used to analyze the parametric behavior of maximum a posteriori inference calculations for graphical models.",
        "published": "2004-01-26T03:50:03Z",
        "link": "http://arxiv.org/abs/q-bio/0401033v1",
        "categories": [
            "q-bio.GN",
            "cs.LG",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "A Numerical Example on the Principles of Stochastic Discrimination",
        "authors": [
            "Tin Kam Ho"
        ],
        "summary": "Studies on ensemble methods for classification suffer from the difficulty of modeling the complementary strengths of the components. Kleinberg's theory of stochastic discrimination (SD) addresses this rigorously via mathematical notions of enrichment, uniformity, and projectability of an ensemble. We explain these concepts via a very simple numerical example that captures the basic principles of the SD theory and method. We focus on a fundamental symmetry in point set covering that is the key observation leading to the foundation of the theory. We believe a better understanding of the SD method will lead to developments of better tools for analyzing other ensemble methods.",
        "published": "2004-02-11T15:45:14Z",
        "link": "http://arxiv.org/abs/cs/0402021v1",
        "categories": [
            "cs.CV",
            "cs.LG",
            "I.5.0"
        ]
    },
    {
        "title": "Fluctuation-dissipation theorem and models of learning",
        "authors": [
            "Ilya Nemenman"
        ],
        "summary": "Advances in statistical learning theory have resulted in a multitude of different designs of learning machines. But which ones are implemented by brains and other biological information processors? We analyze how various abstract Bayesian learners perform on different data and argue that it is difficult to determine which learning-theoretic computation is performed by a particular organism using just its performance in learning a stationary target (learning curve). Basing on the fluctuation-dissipation relation in statistical physics, we then discuss a different experimental setup that might be able to solve the problem.",
        "published": "2004-02-12T22:36:01Z",
        "link": "http://arxiv.org/abs/q-bio/0402029v2",
        "categories": [
            "q-bio.NC",
            "cs.LG",
            "nlin.AO",
            "physics.data-an"
        ]
    },
    {
        "title": "Fitness inheritance in the Bayesian optimization algorithm",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry"
        ],
        "summary": "This paper describes how fitness inheritance can be used to estimate fitness for a proportion of newly sampled candidate solutions in the Bayesian optimization algorithm (BOA). The goal of estimating fitness for some candidate solutions is to reduce the number of fitness evaluations for problems where fitness evaluation is expensive. Bayesian networks used in BOA to model promising solutions and generate the new ones are extended to allow not only for modeling and sampling candidate solutions, but also for estimating their fitness. The results indicate that fitness inheritance is a promising concept in BOA, because population-sizing requirements for building appropriate models of promising solutions lead to good fitness estimates even if only a small proportion of candidate solutions is evaluated using the actual fitness function. This can lead to a reduction of the number of actual fitness evaluations by a factor of 30 or more.",
        "published": "2004-02-15T07:40:45Z",
        "link": "http://arxiv.org/abs/cs/0402032v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Distribution of Mutual Information from Complete and Incomplete Data",
        "authors": [
            "Marcus Hutter",
            "Marco Zaffalon"
        ],
        "summary": "Mutual information is widely used, in a descriptive way, to measure the stochastic dependence of categorical random variables. In order to address questions such as the reliability of the descriptive value, one must consider sample-to-population inferential approaches. This paper deals with the posterior distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean, and analytical approximations for the variance, skewness and kurtosis are derived. These approximations have a guaranteed accuracy level of the order O(1/n^3), where n is the sample size. Leading order approximations for the mean and the variance are derived in the case of incomplete samples. The derived analytical expressions allow the distribution of mutual information to be approximated reliably and quickly. In fact, the derived expressions can be computed with the same order of complexity needed for descriptive mutual information. This makes the distribution of mutual information become a concrete alternative to descriptive mutual information in many applications which would benefit from moving to the inductive side. Some of these prospective applications are discussed, and one of them, namely feature selection, is shown to perform significantly better when inductive mutual information is used.",
        "published": "2004-03-15T16:33:55Z",
        "link": "http://arxiv.org/abs/cs/0403025v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2"
        ]
    },
    {
        "title": "Concept of E-machine: How does a \"dynamical\" brain learn to process   \"symbolic\" information? Part I",
        "authors": [
            "Victor Eliashberg"
        ],
        "summary": "The human brain has many remarkable information processing characteristics that deeply puzzle scientists and engineers. Among the most important and the most intriguing of these characteristics are the brain's broad universality as a learning system and its mysterious ability to dynamically change (reconfigure) its behavior depending on a combinatorial number of different contexts.   This paper discusses a class of hypothetically brain-like dynamically reconfigurable associative learning systems that shed light on the possible nature of these brain's properties. The systems are arranged on the general principle referred to as the concept of E-machine.   The paper addresses the following questions:   1. How can \"dynamical\" neural networks function as universal programmable \"symbolic\" machines?   2. What kind of a universal programmable symbolic machine can form arbitrarily complex software in the process of programming similar to the process of biological associative learning?   3. How can a universal learning machine dynamically reconfigure its software depending on a combinatorial number of possible contexts?",
        "published": "2004-03-19T17:13:55Z",
        "link": "http://arxiv.org/abs/cs/0403031v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.0"
        ]
    },
    {
        "title": "Tournament versus Fitness Uniform Selection",
        "authors": [
            "Shane Legg",
            "Marcus Hutter",
            "Akshat Kumar"
        ],
        "summary": "In evolutionary algorithms a critical parameter that must be tuned is that of selection pressure. If it is set too low then the rate of convergence towards the optimum is likely to be slow. Alternatively if the selection pressure is set too high the system is likely to become stuck in a local optimum due to a loss of diversity in the population. The recent Fitness Uniform Selection Scheme (FUSS) is a conceptually simple but somewhat radical approach to addressing this problem - rather than biasing the selection towards higher fitness, FUSS biases selection towards sparsely populated fitness levels. In this paper we compare the relative performance of FUSS with the well known tournament selection scheme on a range of problems.",
        "published": "2004-03-23T15:17:53Z",
        "link": "http://arxiv.org/abs/cs/0403038v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2; I.2.6; I.2.8; F.2"
        ]
    },
    {
        "title": "When Do Differences Matter? On-Line Feature Extraction Through Cognitive   Economy",
        "authors": [
            "David J. Finton"
        ],
        "summary": "For an intelligent agent to be truly autonomous, it must be able to adapt its representation to the requirements of its task as it interacts with the world. Most current approaches to on-line feature extraction are ad hoc; in contrast, this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy. The algorithm incorporates an active form of Q-learning, and partitions continuous state-spaces by merging and splitting Voronoi regions. The experiments illustrate a new methodology for testing and comparing representations by means of learning curves. Results from the puck-on-a-hill task demonstrate the algorithm's ability to learn effective representations, superior to those produced by some other, well-known, methods.",
        "published": "2004-04-15T02:59:10Z",
        "link": "http://arxiv.org/abs/cs/0404032v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "I.2.6; I.2.4; I.2.8"
        ]
    },
    {
        "title": "Metrics for more than two points at once",
        "authors": [
            "David H. Wolpert"
        ],
        "summary": "The conventional definition of a topological metric over a space specifies properties that must be obeyed by any measure of \"how separated\" two points in that space are. Here it is shown how to extend that definition, and in particular the triangle inequality, to concern arbitrary numbers of points. Such a measure of how separated the points within a collection are can be bootstrapped, to measure \"how separated\" from each other are two (or more) collections. The measure presented here also allows fractional membership of an element in a collection. This means it directly concerns measures of ``how spread out\" a probability distribution over a space is. When such a measure is bootstrapped to compare two collections, it allows us to measure how separated two probability distributions are, or more generally, how separated a distribution of distributions is.",
        "published": "2004-04-16T02:31:43Z",
        "link": "http://arxiv.org/abs/nlin/0404032v1",
        "categories": [
            "nlin.AO",
            "cond-mat.other",
            "cs.LG",
            "math.GM"
        ]
    },
    {
        "title": "Convergence of Discrete MDL for Sequential Prediction",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We study the properties of the Minimum Description Length principle for sequence prediction, considering a two-part MDL estimator which is chosen from a countable class of models. This applies in particular to the important case of universal sequence prediction, where the model class corresponds to all algorithms for some fixed universal Turing machine (this correspondence is by enumerable semimeasures, hence the resulting models are stochastic). We prove convergence theorems similar to Solomonoff's theorem of universal induction, which also holds for general Bayes mixtures. The bound characterizing the convergence speed for MDL predictions is exponentially larger as compared to Bayes mixtures. We observe that there are at least three different ways of using MDL for prediction. One of these has worse prediction properties, for which predictions only converge if the MDL estimator stabilizes. We establish sufficient conditions for this to occur. Finally, some immediate consequences for complexity relations and randomness criteria are proven.",
        "published": "2004-04-28T15:58:35Z",
        "link": "http://arxiv.org/abs/cs/0404057v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.ST",
            "stat.TH",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Prediction with Expert Advice by Following the Perturbed Leader for   General Weights",
        "authors": [
            "Marcus Hutter",
            "Jan Poland"
        ],
        "summary": "When applying aggregating strategies to Prediction with Expert Advice, the learning rate must be adaptively tuned. The natural choice of sqrt(complexity/current loss) renders the analysis of Weighted Majority derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative \"Follow the Perturbed Leader\" (FPL) algorithm from Kalai (2003} (based on Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are (to our knowledge) new.",
        "published": "2004-05-12T16:41:01Z",
        "link": "http://arxiv.org/abs/cs/0405043v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Knowledge Reduction and Discovery based on Demarcation Information",
        "authors": [
            "Yuguo He"
        ],
        "summary": "Knowledge reduction, includes attribute reduction and value reduction, is an important topic in rough set literature. It is also closely relevant to other fields, such as machine learning and data mining. In this paper, an algorithm called TWI-SQUEEZE is proposed. It can find a reduct, or an irreducible attribute subset after two scans. Its soundness and computational complexity are given, which show that it is the fastest algorithm at present. A measure of variety is brought forward, of which algorithm TWI-SQUEEZE can be regarded as an application. The author also argues the rightness of this measure as a measure of information, which can make it a unified measure for \"differentiation, a concept appeared in cognitive psychology literature. Value reduction is another important aspect of knowledge reduction. It is interesting that using the same algorithm we can execute a complete value reduction efficiently. The complete knowledge reduction, which results in an irreducible table, can therefore be accomplished after four scans of table. The byproducts of reduction are two classifiers of different styles. In this paper, various cases and models will be discussed to prove the efficiency and effectiveness of the algorithm. Some topics, such as how to integrate user preference to find a local optimal attribute subset will also be discussed.",
        "published": "2004-05-27T11:26:18Z",
        "link": "http://arxiv.org/abs/cs/0405104v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "cs.IT",
            "math.IT",
            "H.2.8;I.2.6; H.1.1;I.5.2"
        ]
    },
    {
        "title": "A tutorial introduction to the minimum description length principle",
        "authors": [
            "Peter Grunwald"
        ],
        "summary": "This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection \"Advances in Minimum Description Length: Theory and Application\" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).",
        "published": "2004-06-04T09:11:18Z",
        "link": "http://arxiv.org/abs/math/0406077v1",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "stat.TH",
            "6201; 6801; 68T05; 68T10; 9401"
        ]
    },
    {
        "title": "Blind Construction of Optimal Nonlinear Recursive Predictors for   Discrete Sequences",
        "authors": [
            "Cosma Rohilla Shalizi",
            "Kristina Lisa Shalizi"
        ],
        "summary": "We present a new method for nonlinear prediction of discrete random sequences under minimal structural assumptions. We give a mathematical construction for optimal predictors of such processes, in the form of hidden Markov models. We then describe an algorithm, CSSR (Causal-State Splitting Reconstruction), which approximates the ideal predictor from data. We discuss the reliability of CSSR, its data requirements, and its performance in simulations. Finally, we compare our approach to existing methods using variable-length Markov models and cross-validated hidden Markov models, and show theoretically and experimentally that our method delivers results superior to the former and at least comparable to the latter.",
        "published": "2004-06-06T18:57:05Z",
        "link": "http://arxiv.org/abs/cs/0406011v1",
        "categories": [
            "cs.LG",
            "math.ST",
            "nlin.CD",
            "physics.data-an",
            "stat.TH",
            "I.2.6"
        ]
    },
    {
        "title": "Suboptimal behaviour of Bayes and MDL in classification under   misspecification",
        "authors": [
            "Peter Grunwald",
            "John Langford"
        ],
        "summary": "We show that forms of Bayesian and MDL inference that are often applied to classification problems can be *inconsistent*. This means there exists a learning problem such that for all amounts of data the generalization errors of the MDL classifier and the Bayes classifier relative to the Bayesian posterior both remain bounded away from the smallest achievable generalization error.",
        "published": "2004-06-10T16:36:54Z",
        "link": "http://arxiv.org/abs/math/0406221v1",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "stat.TH",
            "62A01; 68T05; 68T10"
        ]
    },
    {
        "title": "Learning for Adaptive Real-time Search",
        "authors": [
            "Vadim Bulitko"
        ],
        "summary": "Real-time heuristic search is a popular model of acting and learning in intelligent autonomous agents. Learning real-time search agents improve their performance over time by acquiring and refining a value function guiding the application of their actions. As computing the perfect value function is typically intractable, a heuristic approximation is acquired instead. Most studies of learning in real-time search (and reinforcement learning) assume that a simple value-function-greedy policy is used to select actions. This is in contrast to practice, where high-performance is usually attained by interleaving planning and acting via a lookahead search of a non-trivial depth. In this paper, we take a step toward bridging this gap and propose a novel algorithm that (i) learns a heuristic function to be used specifically with a lookahead-based policy, (ii) selects the lookahead depth adaptively in each state, (iii) gives the user control over the trade-off between exploration and exploitation. We extensively evaluate the algorithm in the sliding tile puzzle testbed comparing it to the classical LRTA* and the more recent weighted LRTA*, bounded LRTA*, and FALCONS. Improvements of 5 to 30 folds in convergence speed are observed.",
        "published": "2004-07-06T22:18:25Z",
        "link": "http://arxiv.org/abs/cs/0407016v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "On the Convergence Speed of MDL Predictions for Bernoulli Sequences",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We consider the Minimum Description Length principle for online sequence prediction. If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is bounded, implying convergence with probability one, and (b) it additionally specifies a `rate of convergence'. Generally, for MDL only exponential loss bounds hold, as opposed to the linear bounds for a Bayes mixture. We show that this is even the case if the model class contains only Bernoulli distributions. We derive a new upper bound on the prediction error for countable Bernoulli classes. This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes. The results apply to many Machine Learning tasks including classification and hypothesis testing. We provide arguments that our theorems generalize to countable classes of i.i.d. models.",
        "published": "2004-07-16T10:36:49Z",
        "link": "http://arxiv.org/abs/cs/0407039v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.PR",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Universal Convergence of Semimeasures on Individual Random Sequences",
        "authors": [
            "Marcus Hutter",
            "Andrej Muchnik"
        ],
        "summary": "Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.",
        "published": "2004-07-23T12:43:28Z",
        "link": "http://arxiv.org/abs/cs/0407057v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "math.IT",
            "math.PR",
            "I.2.6; E.4; G.3; F.1.3"
        ]
    },
    {
        "title": "Word Sense Disambiguation by Web Mining for Word Co-occurrence   Probabilities",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word \\hbox{co-occurrence} probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler.",
        "published": "2004-07-29T19:46:01Z",
        "link": "http://arxiv.org/abs/cs/0407065v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.3; I.2.6; I.2.7; J.5"
        ]
    },
    {
        "title": "Semantic Linking - a Context-Based Approach to Interactivity in   Hypermedia",
        "authors": [
            "Michael Engelhardt",
            "Thomas C. Schmidt"
        ],
        "summary": "The semantic Web initiates new, high level access schemes to online content and applications. One area of superior need for a redefined content exploration is given by on-line educational applications and their concepts of interactivity in the framework of open hypermedia systems. In the present paper we discuss aspects and opportunities of gaining interactivity schemes from semantic notions of components. A transition from standard educational annotation to semantic statements of hyperlinks is discussed. Further on we introduce the concept of semantic link contexts as an approach to manage a coherent rhetoric of linking. A practical implementation is introduced, as well. Our semantic hyperlink implementation is based on the more general Multimedia Information Repository MIR, an open hypermedia system supporting the standards XML, Corba and JNDI.",
        "published": "2004-07-31T14:04:04Z",
        "link": "http://arxiv.org/abs/cs/0408001v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.5.4; H.2.4; H.3.4; H.5.1; C.2.4; K.3.1"
        ]
    },
    {
        "title": "Hypermedia Learning Objects System - On the Way to a Semantic   Educational Web",
        "authors": [
            "Michael Engelhardt",
            "Andreas Kárpáti",
            "Torsten Rack",
            "Ivette Schmidt",
            "Thomas C. Schmidt"
        ],
        "summary": "While eLearning systems become more and more popular in daily education, available applications lack opportunities to structure, annotate and manage their contents in a high-level fashion. General efforts to improve these deficits are taken by initiatives to define rich meta data sets and a semanticWeb layer. In the present paper we introduce Hylos, an online learning system. Hylos is based on a cellular eLearning Object (ELO) information model encapsulating meta data conforming to the LOM standard. Content management is provisioned on this semantic meta data level and allows for variable, dynamically adaptable access structures. Context aware multifunctional links permit a systematic navigation depending on the learners and didactic needs, thereby exploring the capabilities of the semantic web. Hylos is built upon the more general Multimedia Information Repository (MIR) and the MIR adaptive context linking environment (MIRaCLE), its linking extension. MIR is an open system supporting the standards XML, Corba and JNDI. Hylos benefits from manageable information structures, sophisticated access logic and high-level authoring tools like the ELO editor responsible for the semi-manual creation of meta data and WYSIWYG like content editing.",
        "published": "2004-07-31T22:16:37Z",
        "link": "http://arxiv.org/abs/cs/0408004v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.5.4; H.2.4; H.3.4; H.5.1; C.2.4; K.3.1"
        ]
    },
    {
        "title": "Online convex optimization in the bandit setting: gradient descent   without a gradient",
        "authors": [
            "Abraham D. Flaxman",
            "Adam Tauman Kalai",
            "H. Brendan McMahan"
        ],
        "summary": "We consider a the general online convex optimization framework introduced by Zinkevich. In this setting, there is a sequence of convex functions. Each period, we must choose a signle point (from some feasible set) and pay a cost equal to the value of the next function on our chosen point. Zinkevich shows that, if the each function is revealed after the choice is made, then one can achieve vanishingly small regret relative the best single decision chosen in hindsight.   We extend this to the bandit setting where we do not find out the entire functions but rather just their value at our chosen point. We show how to get vanishingly small regret in this setting.   Our approach uses a simple approximation of the gradient that is computed from evaluating a function at a single (random) point. We show that this estimate is sufficient to mimic Zinkevich's gradient descent online analysis, with access to the gradient (only being able to evaluate the function at a single point).",
        "published": "2004-08-02T21:24:41Z",
        "link": "http://arxiv.org/abs/cs/0408007v1",
        "categories": [
            "cs.LG",
            "cs.CC"
        ]
    },
    {
        "title": "Learning a Machine for the Decision in a Partially Observable Markov   Universe",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "In this paper, we are interested in optimal decisions in a partially observable Markov universe. Our viewpoint departs from the dynamic programming viewpoint: we are directly approximating an optimal strategic tree depending on the observation. This approximation is made by means of a parameterized probabilistic law. In this paper, a particular family of hidden Markov models, with input and output, is considered as a learning framework. A method for optimizing the parameters of these HMMs is proposed and applied. This optimization method is based on the cross-entropic principle.",
        "published": "2004-08-11T06:38:50Z",
        "link": "http://arxiv.org/abs/math/0408146v1",
        "categories": [
            "math.GM",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Stability and Diversity in Collective Adaptation",
        "authors": [
            "Yuzuru Sato",
            "Eizo Akiyama",
            "James P. Crutchfield"
        ],
        "summary": "We derive a class of macroscopic differential equations that describe collective adaptation, starting from a discrete-time stochastic microscopic model. The behavior of each agent is a dynamic balance between adaptation that locally achieves the best action and memory loss that leads to randomized behavior. We show that, although individual agents interact with their environment and other agents in a purely self-interested way, macroscopic behavior can be interpreted as game dynamics. Application to several familiar, explicit game interactions shows that the adaptation dynamics exhibits a diversity of collective behaviors. The simplicity of the assumptions underlying the macroscopic equations suggests that these behaviors should be expected broadly in collective adaptation. We also analyze the adaptation dynamics from an information-theoretic viewpoint and discuss self-organization induced by information flux between agents, giving a novel view of collective adaptation.",
        "published": "2004-08-20T05:17:14Z",
        "link": "http://arxiv.org/abs/nlin/0408039v2",
        "categories": [
            "nlin.AO",
            "cs.LG",
            "math.DS",
            "nlin.CD",
            "stat.ML"
        ]
    },
    {
        "title": "Journal of New Democratic Methods: An Introduction",
        "authors": [
            "John David Funge"
        ],
        "summary": "This paper describes a new breed of academic journals that use statistical machine learning techniques to make them more democratic. In particular, not only can anyone submit an article, but anyone can also become a reviewer. Machine learning is used to decide which reviewers accurately represent the views of the journal's readers and thus deserve to have their opinions carry more weight. The paper concentrates on describing a specific experimental prototype of a democratic journal called the Journal of New Democratic Methods (JNDM). The paper also mentions the wider implications that machine learning and the techniques used in the JNDM may have for representative democracy in general.",
        "published": "2004-08-21T16:57:34Z",
        "link": "http://arxiv.org/abs/cs/0408048v1",
        "categories": [
            "cs.CY",
            "cs.LG",
            "I.2.6; J.1; K.4.3"
        ]
    },
    {
        "title": "Non-negative matrix factorization with sparseness constraints",
        "authors": [
            "Patrik O. Hoyer"
        ],
        "summary": "Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of `sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.",
        "published": "2004-08-25T20:25:43Z",
        "link": "http://arxiv.org/abs/cs/0408058v1",
        "categories": [
            "cs.LG",
            "cs.NE"
        ]
    },
    {
        "title": "Applying Policy Iteration for Training Recurrent Neural Networks",
        "authors": [
            "I. Szita",
            "A. Lorincz"
        ],
        "summary": "Recurrent neural networks are often used for learning time-series data. Based on a few assumptions we model this learning task as a minimization problem of a nonlinear least-squares cost function. The special structure of the cost function allows us to build a connection to reinforcement learning. We exploit this connection and derive a convergent, policy iteration-based algorithm. Furthermore, we argue that RNN training can be fit naturally into the reinforcement learning framework.",
        "published": "2004-10-02T07:19:49Z",
        "link": "http://arxiv.org/abs/cs/0410004v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ]
    },
    {
        "title": "L1 regularization is better than L2 for learning and predicting chaotic   systems",
        "authors": [
            "Z. Szabo",
            "A. Lorincz"
        ],
        "summary": "Emergent behaviors are in the focus of recent research interest. It is then of considerable importance to investigate what optimizations suit the learning and prediction of chaotic systems, the putative candidates for emergence. We have compared L1 and L2 regularizations on predicting chaotic time series using linear recurrent neural networks. The internal representation and the weights of the networks were optimized in a unifying framework. Computational tests on different problems indicate considerable advantages for the L1 regularization: It had considerably better learning time and better interpolating capabilities. We shall argue that optimization viewed as a maximum likelihood estimation justifies our results, because L1 regularization fits heavy-tailed distributions -- an apparently general feature of emergent systems -- better.",
        "published": "2004-10-07T10:57:08Z",
        "link": "http://arxiv.org/abs/cs/0410015v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Automated Pattern Detection--An Algorithm for Constructing Optimally   Synchronizing Multi-Regular Language Filters",
        "authors": [
            "Carl S. McTague",
            "James P. Crutchfield"
        ],
        "summary": "In the computational-mechanics structural analysis of one-dimensional cellular automata the following automata-theoretic analogue of the \\emph{change-point problem} from time series analysis arises: \\emph{Given a string $\\sigma$ and a collection $\\{\\mc{D}_i\\}$ of finite automata, identify the regions of $\\sigma$ that belong to each $\\mc{D}_i$ and, in particular, the boundaries separating them.} We present two methods for solving this \\emph{multi-regular language filtering problem}. The first, although providing the ideal solution, requires a stack, has a worst-case compute time that grows quadratically in $\\sigma$'s length and conditions its output at any point on arbitrarily long windows of future input. The second method is to algorithmically construct a transducer that approximates the first algorithm. In contrast to the stack-based algorithm, however, the transducer requires only a finite amount of memory, runs in linear time, and gives immediate output for each letter read; it is, moreover, the best possible finite-state approximation with these three features.",
        "published": "2004-10-07T17:20:56Z",
        "link": "http://arxiv.org/abs/cs/0410017v1",
        "categories": [
            "cs.CV",
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.DS",
            "cs.IR",
            "cs.LG",
            "nlin.AO",
            "nlin.CG",
            "nlin.PS",
            "physics.comp-ph",
            "q-bio.GN"
        ]
    },
    {
        "title": "Self-Organised Factorial Encoding of a Toroidal Manifold",
        "authors": [
            "Stephen Luttrell"
        ],
        "summary": "It is shown analytically how a neural network can be used optimally to encode input data that is derived from a toroidal manifold. The case of a 2-layer network is considered, where the output is assumed to be a set of discrete neural firing events. The network objective function measures the average Euclidean error that occurs when the network attempts to reconstruct its input from its output. This optimisation problem is solved analytically for a toroidal input manifold, and two types of solution are obtained: a joint encoder in which the network acts as a soft vector quantiser, and a factorial encoder in which the network acts as a pair of soft vector quantisers (one for each of the circular subspaces of the torus). The factorial encoder is favoured for small network sizes when the number of observed firing events is large. Such self-organised factorial encoding may be used to restrict the size of network that is required to perform a given encoding task, and will decompose an input manifold into its constituent submanifolds.",
        "published": "2004-10-15T20:25:24Z",
        "link": "http://arxiv.org/abs/cs/0410036v2",
        "categories": [
            "cs.LG",
            "cs.CV",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Neural Architectures for Robot Intelligence",
        "authors": [
            "H. Ritter",
            "J. J. Steil",
            "C. Noelker",
            "F. Roethling",
            "P. C. McGuire"
        ],
        "summary": "We argue that the direct experimental approaches to elucidate the architecture of higher brains may benefit from insights gained from exploring the possibilities and limits of artificial control architectures for robot systems. We present some of our recent work that has been motivated by that view and that is centered around the study of various aspects of hand actions since these are intimately linked with many higher cognitive abilities. As examples, we report on the development of a modular system for the recognition of continuous hand postures based on neural nets, the use of vision and tactile sensing for guiding prehensile movements of a multifingered hand, and the recognition and use of hand gestures for robot teaching.   Regarding the issue of learning, we propose to view real-world learning from the perspective of data mining and to focus more strongly on the imitation of observed actions instead of purely reinforcement-based exploration. As a concrete example of such an effort we report on the status of an ongoing project in our lab in which a robot equipped with an attention system with a neurally inspired architecture is taught actions by using hand gestures in conjunction with speech commands. We point out some of the lessons learnt from this system, and discuss how systems of this kind can contribute to the study of issues at the junction between natural and artificial cognitive systems.",
        "published": "2004-10-18T10:50:28Z",
        "link": "http://arxiv.org/abs/cs/0410042v1",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.HC",
            "cs.LG",
            "cs.NE",
            "q-bio.NC",
            "I.2.9; I.2.10; I.2.6; H.1.2; H.2.8; I.5.4"
        ]
    },
    {
        "title": "Improved Bounds on Quantum Learning Algorithms",
        "authors": [
            "Alp Atici",
            "Rocco A. Servedio"
        ],
        "summary": "In this article we give several new results on the complexity of algorithms that learn Boolean functions from quantum queries and quantum examples.   Hunziker et al. conjectured that for any class C of Boolean functions, the number of quantum black-box queries which are required to exactly identify an unknown function from C is $O(\\frac{\\log |C|}{\\sqrt{{\\hat{\\gamma}}^{C}}})$, where $\\hat{\\gamma}^{C}$ is a combinatorial parameter of the class C. We essentially resolve this conjecture in the affirmative by giving a quantum algorithm that, for any class C, identifies any unknown function from C using $O(\\frac{\\log |C| \\log \\log |C|}{\\sqrt{{\\hat{\\gamma}}^{C}}})$ quantum black-box queries.   We consider a range of natural problems intermediate between the exact learning problem (in which the learner must obtain all bits of information about the black-box function) and the usual problem of computing a predicate (in which the learner must obtain only one bit of information about the black-box function). We give positive and negative results on when the quantum and classical query complexities of these intermediate problems are polynomially related to each other.   Finally, we improve the known lower bounds on the number of quantum examples (as opposed to quantum black-box queries) required for $(\\epsilon,\\delta)$-PAC learning any concept class of Vapnik-Chervonenkis dimension d over the domain $\\{0,1\\}^n$ from $\\Omega(\\frac{d}{n})$ to $\\Omega(\\frac{1}{\\epsilon}\\log \\frac{1}{\\delta}+d+\\frac{\\sqrt{d}}{\\epsilon})$. This new lower bound comes closer to matching known upper bounds for classical PAC learning.",
        "published": "2004-11-18T20:14:16Z",
        "link": "http://arxiv.org/abs/quant-ph/0411140v2",
        "categories": [
            "quant-ph",
            "cs.LG"
        ]
    },
    {
        "title": "Fast Non-Parametric Bayesian Inference on Infinite Trees",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Given i.i.d. data from an unknown distribution, we consider the problem of predicting future items. An adaptive way to estimate the probability density is to recursively subdivide the domain to an appropriate data-dependent granularity. A Bayesian would assign a data-independent prior probability to \"subdivide\", which leads to a prior over infinite(ly many) trees. We derive an exact, fast, and simple inference algorithm for such a prior, for the data evidence, the predictive distribution, the effective model dimension, and other quantities.",
        "published": "2004-11-23T16:39:07Z",
        "link": "http://arxiv.org/abs/math/0411515v1",
        "categories": [
            "math.ST",
            "cs.LG",
            "math.PR",
            "stat.TH",
            "62G07; 60B10; 68W99"
        ]
    },
    {
        "title": "A Note on the PAC Bayesian Theorem",
        "authors": [
            "Andreas Maurer"
        ],
        "summary": "We prove general exponential moment inequalities for averages of [0,1]-valued iid random variables and use them to tighten the PAC Bayesian Theorem. The logarithmic dependence on the sample count in the enumerator of the PAC Bayesian bound is halved.",
        "published": "2004-11-30T08:36:59Z",
        "link": "http://arxiv.org/abs/cs/0411099v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.5.1"
        ]
    },
    {
        "title": "Mining Heterogeneous Multivariate Time-Series for Learning Meaningful   Patterns: Application to Home Health Telecare",
        "authors": [
            "Florence Duchene",
            "Catherine Garbay",
            "Vincent Rialle"
        ],
        "summary": "For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.",
        "published": "2004-12-01T16:32:49Z",
        "link": "http://arxiv.org/abs/cs/0412003v1",
        "categories": [
            "cs.LG",
            "G.3"
        ]
    },
    {
        "title": "Human-Level Performance on Word Analogy Questions by Latent Relational   Analysis",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, machine translation, and information retrieval. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason/stone is analogous to the pair carpenter/wood. Past work on semantic similarity measures has mainly been concerned with attributional similarity. Recently the Vector Space Model (VSM) of information retrieval has been adapted to the task of measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus (they are not predefined), (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data (it is also used this way in Latent Semantic Analysis), and (3) automatically generated synonyms are used to explore reformulations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying noun-modifier relations, LRA achieves similar gains over the VSM, while using a smaller corpus.",
        "published": "2004-12-06T21:50:18Z",
        "link": "http://arxiv.org/abs/cs/0412024v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "The Google Similarity Distance",
        "authors": [
            "Rudi Cilibrasi",
            "Paul M. B. Vitanyi"
        ],
        "summary": "Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers the equivalent of `society' is `database,' and the equivalent of `use' is `way to search the database.' We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts we use the world-wide-web as database, and Google as search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the world-wide-web using Google page counts. The world-wide-web is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies, and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87% with the expert crafted WordNet categories.",
        "published": "2004-12-21T16:05:36Z",
        "link": "http://arxiv.org/abs/cs/0412098v3",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "Online Learning of Aggregate Knowledge about Non-linear Preferences   Applied to Negotiating Prices and Bundles",
        "authors": [
            "Koye Somefun",
            "Tomas Klos",
            "Han La Poutré"
        ],
        "summary": "In this paper, we consider a form of multi-issue negotiation where a shop negotiates both the contents and the price of bundles of goods with his customers. We present some key insights about, as well as a procedure for, locating mutually beneficial alternatives to the bundle currently under negotiation. The essence of our approach lies in combining aggregate (anonymous) knowledge of customer preferences with current data about the ongoing negotiation process. The developed procedure either works with already obtained aggregate knowledge or, in the absence of such knowledge, learns the relevant information online. We conduct computer experiments with simulated customers that have_nonlinear_ preferences. We show how, for various types of customers, with distinct negotiation heuristics, our procedure (with and without the necessary aggregate knowledge) increases the speed with which deals are reached, as well as the number and the Pareto efficiency of the deals reached compared to a benchmark.",
        "published": "2004-12-23T15:21:40Z",
        "link": "http://arxiv.org/abs/cs/0412106v1",
        "categories": [
            "cs.MA",
            "cs.GT",
            "cs.LG"
        ]
    },
    {
        "title": "Heuristic average-case analysis of the backtrack resolution of random   3-Satisfiability instances",
        "authors": [
            "Simona Cocco",
            "Remi Monasson"
        ],
        "summary": "An analysis of the average-case complexity of solving random 3-Satisfiability (SAT) instances with backtrack algorithms is presented. We first interpret previous rigorous works in a unifying framework based on the statistical physics notions of dynamical trajectories, phase diagram and growth process. It is argued that, under the action of the Davis--Putnam--Loveland--Logemann (DPLL) algorithm, 3-SAT instances are turned into 2+p-SAT instances whose characteristic parameters (ratio alpha of clauses per variable, fraction p of 3-clauses) can be followed during the operation, and define resolution trajectories. Depending on the location of trajectories in the phase diagram of the 2+p-SAT model, easy (polynomial) or hard (exponential) resolutions are generated. Three regimes are identified, depending on the ratio alpha of the 3-SAT instance to be solved. Lower sat phase: for small ratios, DPLL almost surely finds a solution in a time growing linearly with the number N of variables. Upper sat phase: for intermediate ratios, instances are almost surely satisfiable but finding a solution requires exponential time (2 ^ (N omega) with omega>0) with high probability. Unsat phase: for large ratios, there is almost always no solution and proofs of refutation are exponential. An analysis of the growth of the search tree in both upper sat and unsat regimes is presented, and allows us to estimate omega as a function of alpha. This analysis is based on an exact relationship between the average size of the search tree and the powers of the evolution operator encoding the elementary steps of the search heuristic.",
        "published": "2004-01-14T12:47:57Z",
        "link": "http://arxiv.org/abs/cs/0401011v1",
        "categories": [
            "cs.DS",
            "cond-mat.stat-mech",
            "cs.CC",
            "A.0"
        ]
    },
    {
        "title": "Updating Schemes in Random Boolean Networks: Do They Really Matter?",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "In this paper we try to end the debate concerning the suitability of different updating schemes in random Boolean networks (RBNs). We quantify for the first time loose attractors in asyncrhonous RBNs, which allows us to analyze the complexity reduction related to different updating schemes. We also report that all updating schemes yield very similar critical stability values, meaning that the \"edge of chaos\" does not depend much on the updating scheme. After discussion, we conclude that synchonous RBNs are justifiable theoretical models of biological networks.",
        "published": "2004-02-05T15:08:26Z",
        "link": "http://arxiv.org/abs/nlin/0402006v3",
        "categories": [
            "nlin.AO",
            "cond-mat.other",
            "cs.CC",
            "nlin.CG",
            "q-bio.MN",
            "q-bio.OT",
            "q-bio.QM"
        ]
    },
    {
        "title": "Limitations of Quantum Advice and One-Way Communication",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "Although a quantum state requires exponentially many classical bits to describe, the laws of quantum mechanics impose severe restrictions on how that state can be accessed. This paper shows in three settings that quantum messages have only limited advantages over classical ones.   First, we show that $\\mathsf{BQP/qpoly}\\subseteq\\mathsf{PP/poly}$, where $\\mathsf{BQP/qpoly}$ is the class of problems solvable in quantum polynomial time, given a polynomial-size \"quantum advice state\" that depends only on the input length. This resolves a question of Buhrman, and means that we should not hope for an unrelativized separation between quantum and classical advice. Underlying our complexity result is a general new relation between deterministic and quantum one-way communication complexities, which applies to partial as well as total functions.   Second, we construct an oracle relative to which $\\mathsf{NP}\\not \\subset \\mathsf{BQP/qpoly}$. To do so, we use the polynomial method to give the first correct proof of a direct product theorem for quantum search. This theorem has other applications; for example, it can be used to fix a result of Klauck about quantum time-space tradeoffs for sorting.   Third, we introduce a new trace distance method for proving lower bounds on quantum one-way communication complexity. Using this method, we obtain optimal quantum lower bounds for two problems of Ambainis, for which no nontrivial lower bounds were previously known even for classical randomized protocols.   A preliminary version of this paper appeared in the 2004 Conference on Computational Complexity (CCC).",
        "published": "2004-02-15T04:30:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0402095v5",
        "categories": [
            "quant-ph",
            "cs.CC",
            "81P68, 81P05, 68Q10, 68Q15, 42A05",
            "F.1.2; F.1.3"
        ]
    },
    {
        "title": "Kolmogorov complexity and symmetric relational structures",
        "authors": [
            "W. L. Fouché",
            "P. H. Potgieter"
        ],
        "summary": "We study partitions of Fra\\\"{\\i}ss\\'{e} limits of classes of finite relational structures where the partitions are encoded by infinite binary sequences which are random in the sense of Kolmogorov, Chaitin and Solomonoff. It is shown that partition by a random sequence of a Fra\\\"{\\i}ss\\'{e} limit preserves the limit property of the object.",
        "published": "2004-02-16T10:07:12Z",
        "link": "http://arxiv.org/abs/cs/0402034v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.1; ; F.4.1; G.2.3"
        ]
    },
    {
        "title": "Quantum and Classical Strong Direct Product Theorems and Optimal   Time-Space Tradeoffs",
        "authors": [
            "Hartmut Klauck",
            "Robert Spalek",
            "Ronald de Wolf"
        ],
        "summary": "A strong direct product theorem says that if we want to compute k independent instances of a function, using less than k times the resources needed for one instance, then our overall success probability will be exponentially small in k. We establish such theorems for the classical as well as quantum query complexity of the OR function. This implies slightly weaker direct product results for all total functions. We prove a similar result for quantum communication protocols computing k instances of the Disjointness function.   Our direct product theorems imply a time-space tradeoff T^2*S=Omega(N^3) for sorting N items on a quantum computer, which is optimal up to polylog factors. They also give several tight time-space and communication-space tradeoffs for the problems of Boolean matrix-vector multiplication and matrix multiplication.",
        "published": "2004-02-18T15:05:15Z",
        "link": "http://arxiv.org/abs/quant-ph/0402123v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of Modified Instances",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "In this paper we study the complexity of solving a problem when a solution of a similar instance is known. This problem is relevant whenever instances may change from time to time, and known solutions may not remain valid after the change. We consider two scenarios: in the first one, what is known is only a solution of the problem before the change; in the second case, we assume that some additional information, found during the search for this solution, is also known. In the first setting, the techniques from the theory of NP-completeness suffice to show complexity results. In the second case, negative results can only be proved using the techniques of compilability, and are often related to the size of considered changes.",
        "published": "2004-02-23T18:08:48Z",
        "link": "http://arxiv.org/abs/cs/0402053v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "F.1.3; I.2.8"
        ]
    },
    {
        "title": "Quantum Identification of Boolean Oracles",
        "authors": [
            "Andris Ambainis",
            "Kazuo Iwama",
            "Akinori Kawachi",
            "Hiroyuki Masuda",
            "Raymond H. Putra",
            "Shigeru Yamashita"
        ],
        "summary": "The oracle identification problem (OIP) is, given a set $S$ of $M$ Boolean oracles out of $2^{N}$ ones, to determine which oracle in $S$ is the current black-box oracle. We can exploit the information that candidates of the current oracle is restricted to $S$. The OIP contains several concrete problems such as the original Grover search and the Bernstein-Vazirani problem. Our interest is in the quantum query complexity, for which we present several upper and lower bounds. They are quite general and mostly optimal: (i) The query complexity of OIP is $O(\\sqrt{N\\log M \\log N}\\log\\log M)$ for {\\it any} $S$ such that $M = |S| > N$, which is better than the obvious bound $N$ if $M < 2^{N/\\log^{3}N}$. (ii) It is $O(\\sqrt{N})$ for {\\it any} $S$ if $|S| = N$, which includes the upper bound for the Grover search as a special case. (iii) For a wide range of oracles ($|S| = N$) such as random oracles and balanced oracles, the query complexity is $\\Theta(\\sqrt{N/K})$, where $K$ is a simple parameter determined by $S$.",
        "published": "2004-03-07T12:43:36Z",
        "link": "http://arxiv.org/abs/quant-ph/0403056v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Intelligent encoding and economical communication in the visual stream",
        "authors": [
            "Andras Lorincz"
        ],
        "summary": "The theory of computational complexity is used to underpin a recent model of neocortical sensory processing. We argue that encoding into reconstruction networks is appealing for communicating agents using Hebbian learning and working on hard combinatorial problems, which are easy to verify. Computational definition of the concept of intelligence is provided. Simulations illustrate the idea.",
        "published": "2004-03-16T14:57:29Z",
        "link": "http://arxiv.org/abs/q-bio/0403022v1",
        "categories": [
            "q-bio.NC",
            "cs.AI",
            "cs.CC",
            "nlin.AO"
        ]
    },
    {
        "title": "Improved Lower Bounds for Locally Decodable Codes and Private   Information Retrieval",
        "authors": [
            "Stephanie Wehner",
            "Ronald de Wolf"
        ],
        "summary": "We prove new lower bounds for locally decodable codes and private information retrieval. We show that a 2-query LDC encoding n-bit strings over an l-bit alphabet, where the decoder only uses b bits of each queried position of the codeword, needs code length m = exp(Omega(n/(2^b Sum_{i=0}^b {l choose i}))) Similarly, a 2-server PIR scheme with an n-bit database and t-bit queries, where the user only needs b bits from each of the two l-bit answers, unknown to the servers, satisfies t = Omega(n/(2^b Sum_{i=0}^b {l choose i})). This implies that several known PIR schemes are close to optimal. Our results generalize those of Goldreich et al. who proved roughly the same bounds for linear LDCs and PIRs. Like earlier work by Kerenidis and de Wolf, our classical lower bounds are proved using quantum computational techniques. In particular, we give a tight analysis of how well a 2-input function can be computed from a quantum superposition of both inputs.",
        "published": "2004-03-19T12:30:21Z",
        "link": "http://arxiv.org/abs/quant-ph/0403140v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Threshold values, stability analysis and high-q asymptotics for the   coloring problem on random graphs",
        "authors": [
            "Florent Krzakala",
            "Andrea Pagnani",
            "Martin Weigt"
        ],
        "summary": "We consider the problem of coloring Erdos-Renyi and regular random graphs of finite connectivity using q colors. It has been studied so far using the cavity approach within the so-called one-step replica symmetry breaking (1RSB) ansatz. We derive a general criterion for the validity of this ansatz and, applying it to the ground state, we provide evidence that the 1RSB solution gives exact threshold values c_q for the q-COL/UNCOL phase transition. We also study the asymptotic thresholds for q >> 1 finding c_q = 2qlog(q)-log(q)-1+o(1) in perfect agreement with rigorous mathematical bounds, as well as the nature of excited states, and give a global phase diagram of the problem.",
        "published": "2004-03-30T14:47:25Z",
        "link": "http://arxiv.org/abs/cond-mat/0403725v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Decidability and Universality in Symbolic Dynamical Systems",
        "authors": [
            "Jean-Charles Delvenne",
            "Petr Kurka",
            "Vincent Blondel"
        ],
        "summary": "Many different definitions of computational universality for various types of dynamical systems have flourished since Turing's work. We propose a general definition of universality that applies to arbitrary discrete time symbolic dynamical systems. Universality of a system is defined as undecidability of a model-checking problem. For Turing machines, counter machines and tag systems, our definition coincides with the classical one. It yields, however, a new definition for cellular automata and subshifts. Our definition is robust with respect to initial condition, which is a desirable feature for physical realizability.   We derive necessary conditions for undecidability and universality. For instance, a universal system must have a sensitive point and a proper subsystem. We conjecture that universal systems have infinite number of subsystems. We also discuss the thesis according to which computation should occur at the `edge of chaos' and we exhibit a universal chaotic system.",
        "published": "2004-04-08T02:45:34Z",
        "link": "http://arxiv.org/abs/cs/0404021v4",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Travelling Salesman Problem with a Center",
        "authors": [
            "Adam Lipowski",
            "Dorota Lipowska"
        ],
        "summary": "We study a travelling salesman problem where the path is optimized with a cost function that includes its length $L$ as well as a certain measure $C$ of its distance from the geometrical center of the graph. Using simulated annealing (SA) we show that such a problem has a transition point that separates two phases differing in the scaling behaviour of $L$ and $C$, in efficiency of SA, and in the shape of minimal paths.",
        "published": "2004-04-18T20:13:14Z",
        "link": "http://arxiv.org/abs/cond-mat/0404424v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "2-Sat Sub-Clauses and the Hypernodal Structure of the 3-Sat Problem",
        "authors": [
            "D. B. Powell"
        ],
        "summary": "Like simpler graphs, nested (hypernodal) graphs consist of two components: a set of nodes and a set of edges, where each edge connects a pair of nodes. In the hypernodal graph model, however, a node may contain other graphs, so that a node may be contained in a graph that it contains. The inherently recursive structure of the hypernodal graph model aptly characterizes both the structure and dynamic of the 3-sat problem, a broadly applicable, though intractable, computer science problem. In this paper I first discuss the structure of the 3-sat problem, analyzing the relation of 3-sat to 2-sat, a related, though tractable problem. I then discuss sub-clauses and sub-clause thresholds and the transformation of sub-clauses into implication graphs, demonstrating how combinations of implication graphs are equivalent to hypernodal graphs. I conclude with a brief discussion of the use of hypernodal graphs to model the 3-sat problem, illustrating how hypernodal graphs model both the conditions for satisfiability and the process by which particular 3-sat assignments either succeed or fail.",
        "published": "2004-04-20T08:23:43Z",
        "link": "http://arxiv.org/abs/cs/0404038v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "G.2.1; G.2.2"
        ]
    },
    {
        "title": "Algorithms for Estimating Information Distance with Application to   Bioinformatics and Linguistics",
        "authors": [
            "Alexei Kaltchenko"
        ],
        "summary": "After reviewing unnormalized and normalized information distances based on incomputable notions of Kolmogorov complexity, we discuss how Kolmogorov complexity can be approximated by data compression algorithms. We argue that optimal algorithms for data compression with side information can be successfully used to approximate the normalized distance. Next, we discuss an alternative information distance, which is based on relative entropy rate (also known as Kullback-Leibler divergence), and compression-based algorithms for its estimation. Based on available biological and linguistic data, we arrive to unexpected conclusion that in Bioinformatics and Computational Linguistics this alternative distance is more relevant and important than the ones based on Kolmogorov complexity.",
        "published": "2004-04-20T15:18:43Z",
        "link": "http://arxiv.org/abs/cs/0404039v1",
        "categories": [
            "cs.CC",
            "cs.CE",
            "q-bio.GN",
            "J.3; E.4"
        ]
    },
    {
        "title": "A note on dimensions of polynomial size circuits",
        "authors": [
            "Xiaoyang Gu"
        ],
        "summary": "In this paper, we use resource-bounded dimension theory to investigate polynomial size circuits. We show that for every $i\\geq 0$, $\\Ppoly$ has $i$th order scaled $\\pthree$-strong dimension 0. We also show that $\\Ppoly^\\io$ has $\\pthree$-dimension 1/2, $\\pthree$-strong dimension 1. Our results improve previous measure results of Lutz (1992) and dimension results of Hitchcock and Vinodchandran (2004).",
        "published": "2004-04-22T14:45:48Z",
        "link": "http://arxiv.org/abs/cs/0404044v2",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Maximum-likelihood decoding of Reed-Solomon Codes is NP-hard",
        "authors": [
            "Venkatesan Guruswami",
            "Alexander Vardy"
        ],
        "summary": "Maximum-likelihood decoding is one of the central algorithmic problems in coding theory. It has been known for over 25 years that maximum-likelihood decoding of general linear codes is NP-hard. Nevertheless, it was so far unknown whether maximum- likelihood decoding remains hard for any specific family of codes with nontrivial algebraic structure. In this paper, we prove that maximum-likelihood decoding is NP-hard for the family of Reed-Solomon codes. We moreover show that maximum-likelihood decoding of Reed-Solomon codes remains hard even with unlimited preprocessing, thereby strengthening a result of Bruck and Naor.",
        "published": "2004-05-04T04:46:32Z",
        "link": "http://arxiv.org/abs/cs/0405005v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.IT",
            "math.IT",
            "E.4; F.1.3; F.2.1"
        ]
    },
    {
        "title": "Computing Multi-Homogeneous Bezout Numbers is Hard",
        "authors": [
            "Gregorio Malajovich",
            "Klaus Meer"
        ],
        "summary": "The multi-homogeneous Bezout number is a bound for the number of solutions of a system of multi-homogeneous polynomial equations, in a suitable product of projective spaces.   Given an arbitrary, not necessarily multi-homogeneous system, one can ask for the optimal multi-homogenization that would minimize the Bezout number.   In this paper, it is proved that the problem of computing, or even estimating the optimal multi-homogeneous Bezout number is actually NP-hard.   In terms of approximation theory for combinatorial optimization, the problem of computing the best multi-homogeneous structure does not belong to APX, unless P = NP.   Moreover, polynomial time algorithms for estimating the minimal multi-homogeneous Bezout number up to a fixed factor cannot exist even in a randomized setting, unless BPP contains NP.",
        "published": "2004-05-05T14:33:19Z",
        "link": "http://arxiv.org/abs/cs/0405021v1",
        "categories": [
            "cs.CC",
            "cs.SC",
            "F.2.1;G.1.5"
        ]
    },
    {
        "title": "Critical behaviour of combinatorial search algorithms, and the   unitary-propagation universality class",
        "authors": [
            "Christophe Deroulers",
            "Rémi Monasson"
        ],
        "summary": "The probability P(alpha, N) that search algorithms for random Satisfiability problems successfully find a solution is studied as a function of the ratio alpha of constraints per variable and the number N of variables. P is shown to be finite if alpha lies below an algorithm--dependent threshold alpha\\_A, and exponentially small in N above. The critical behaviour is universal for all algorithms based on the widely-used unitary propagation rule: P[ (1 + epsilon) alpha\\_A, N] ~ exp[-N^(1/6) Phi(epsilon N^(1/3)) ]. Exponents are related to the critical behaviour of random graphs, and the scaling function Phi is exactly calculated through a mapping onto a diffusion-and-death problem.",
        "published": "2004-05-14T13:03:27Z",
        "link": "http://arxiv.org/abs/cond-mat/0405319v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Note on Counting Eulerian Circuits",
        "authors": [
            "Graham R. Brightwell",
            "Peter Winkler"
        ],
        "summary": "We show that the problem of counting the number of Eulerian circuits in an undirected graph is complete for the class #P.",
        "published": "2004-05-18T19:47:33Z",
        "link": "http://arxiv.org/abs/cs/0405067v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Dichotomy Theorems for Alternation-Bounded Quantified Boolean Formulas",
        "authors": [
            "Edith Hemaspaandra"
        ],
        "summary": "In 1978, Schaefer proved his famous dichotomy theorem for generalized satisfiability problems. He defined an infinite number of propositional satisfiability problems, showed that all these problems are either in P or NP-complete, and gave a simple criterion to determine which of the two cases holds. This result is surprising in light of Ladner's theorem, which implies that there are an infinite number of complexity classes between P and NP-complete (under the assumption that P is not equal to NP).   Schaefer also stated a dichotomy theorem for quantified generalized Boolean formulas, but this theorem was only recently proven by Creignou, Khanna, and Sudan, and independently by Dalmau: Determining truth of quantified Boolean formulas is either PSPACE-complete or in P.   This paper looks at alternation-bounded quantified generalized Boolean formulas. In their unrestricted forms, these problems are the canonical problems complete for the levels of the polynomial hierarchy. In this paper, we prove dichotomy theorems for alternation-bounded quantified generalized Boolean formulas, by showing that these problems are either $\\Sigma_i^p$-complete or in P, and we give a simple criterion to determine which of the two cases holds. This is the first result that obtains dichotomy for an infinite number of classes at once.",
        "published": "2004-06-02T23:17:20Z",
        "link": "http://arxiv.org/abs/cs/0406006v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Implementation of Logical Functions in the Game of Life",
        "authors": [
            "J. -P. Rennard"
        ],
        "summary": "The Game of Life cellular automaton is a classical example of a massively parallel collision-based computing device. The automaton exhibits mobile patterns, gliders, and generators of the mobile patterns, glider guns, in its evolution. We show how to construct the basic logical operations, AND, OR, NOT in space-time configurations of the cellular automaton. Also decomposition of complicated Boolean functions is discussed. Advantages of our technique are demonstrated on an example of binary adder, realized via collision of glider streams.",
        "published": "2004-06-04T14:53:51Z",
        "link": "http://arxiv.org/abs/cs/0406009v1",
        "categories": [
            "cs.CC",
            "F.1.1"
        ]
    },
    {
        "title": "The hidden subgroup problem and permutation group theory",
        "authors": [
            "Julia Kempe",
            "Aner Shalev"
        ],
        "summary": "We employ concepts and tools from the theory of finite permutation groups in order to analyse the Hidden Subgroup Problem via Quantum Fourier Sampling (QFS) for the symmetric group. We show that under very general conditions both the weak and the random-strong form (strong form with random choices of basis) of QFS fail to provide any advantage over classical exhaustive search. In particular we give a complete characterisation of polynomial size subgroups, and of primitive subgroups, that can be distinguished from the identity subgroup with the above methods. Furthermore, assuming a plausible group theoretic conjecture for which we give supporting evidence, we show that weak and random-strong QFS for the symmetric group have no advantage whatsoever over classical search.",
        "published": "2004-06-08T16:35:53Z",
        "link": "http://arxiv.org/abs/quant-ph/0406046v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Alchemistry of the P versus NP question",
        "authors": [
            "Bonifac Donat"
        ],
        "summary": "Are P and NP provably inseparable ? Take a look at some unorthodox, guilty mentioned folklore and related unpublished results.",
        "published": "2004-06-21T20:17:28Z",
        "link": "http://arxiv.org/abs/cs/0406040v2",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Non-computable Julia sets",
        "authors": [
            "Mark Braverman",
            "Michael Yampolsky"
        ],
        "summary": "We show that under the definition of computability which is natural from the point of view of applications, there exist non-computable quadratic Julia sets.",
        "published": "2004-06-22T13:33:48Z",
        "link": "http://arxiv.org/abs/math/0406416v2",
        "categories": [
            "math.DS",
            "cs.CC",
            "37F50, 68Q17"
        ]
    },
    {
        "title": "On the Computational Complexity of the Forcing Chromatic Number",
        "authors": [
            "Frank Harary",
            "Wolfgang Slany",
            "Oleg Verbitsky"
        ],
        "summary": "We consider vertex colorings of graphs in which adjacent vertices have distinct colors. A graph is $s$-chromatic if it is colorable in $s$ colors and any coloring of it uses at least $s$ colors. The forcing chromatic number $F(G)$ of an $s$-chromatic graph $G$ is the smallest number of vertices which must be colored so that, with the restriction that $s$ colors are used, every remaining vertex has its color determined uniquely. We estimate the computational complexity of $F(G)$ relating it to the complexity class US introduced by Blass and Gurevich. We prove that recognizing if $F(G)\\le 2$ is US-hard with respect to polynomial-time many-one reductions. Moreover, this problem is coNP-hard even under the promises that $F(G)\\le 3$ and $G$ is 3-chromatic. On the other hand, recognizing if $F(G)\\le k$, for each constant $k$, is reducible to a problem in US via disjunctive truth-table reduction.   Similar results are obtained also for forcing variants of the clique and the domination numbers of a graph.",
        "published": "2004-06-23T15:21:46Z",
        "link": "http://arxiv.org/abs/cs/0406044v4",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of the Local Hamiltonian Problem",
        "authors": [
            "Julia Kempe",
            "Alexei Kitaev",
            "Oded Regev"
        ],
        "summary": "The k-local Hamiltonian problem is a natural complete problem for the complexity class QMA, the quantum analog of NP. It is similar in spirit to MAX-k-SAT, which is NP-complete for k<=2. It was known that the problem is QMA-complete for any k <= 3. On the other hand 1-local Hamiltonian is in P, and hence not believed to be QMA-complete. The complexity of the 2-local Hamiltonian problem has long been outstanding. Here we settle the question and show that it is QMA-complete. We provide two independent proofs; our first proof uses only elementary linear algebra. Our second proof uses a powerful technique for analyzing the sum of two Hamiltonians; this technique is based on perturbation theory and we believe that it might prove useful elsewhere. Using our techniques we also show that adiabatic computation with two-local interactions on qubits is equivalent to standard quantum computation.",
        "published": "2004-06-24T19:23:16Z",
        "link": "http://arxiv.org/abs/quant-ph/0406180v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Improved Simulation of Stabilizer Circuits",
        "authors": [
            "Scott Aaronson",
            "Daniel Gottesman"
        ],
        "summary": "The Gottesman-Knill theorem says that a stabilizer circuit -- that is, a quantum circuit consisting solely of CNOT, Hadamard, and phase gates -- can be simulated efficiently on a classical computer. This paper improves that theorem in several directions. First, by removing the need for Gaussian elimination, we make the simulation algorithm much faster at the cost of a factor-2 increase in the number of bits needed to represent a state. We have implemented the improved algorithm in a freely-available program called CHP (CNOT-Hadamard-Phase), which can handle thousands of qubits easily. Second, we show that the problem of simulating stabilizer circuits is complete for the classical complexity class ParityL, which means that stabilizer circuits are probably not even universal for classical computation. Third, we give efficient algorithms for computing the inner product between two stabilizer states, putting any n-qubit stabilizer circuit into a \"canonical form\" that requires at most O(n^2/log n) gates, and other useful tasks. Fourth, we extend our simulation algorithm to circuits acting on mixed states, circuits containing a limited number of non-stabilizer gates, and circuits acting on general tensor-product initial states but containing only a limited number of measurements.",
        "published": "2004-06-25T17:57:42Z",
        "link": "http://arxiv.org/abs/quant-ph/0406196v5",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "P=NP",
        "authors": [
            "Selmer Bringsjord",
            "Joshua Taylor"
        ],
        "summary": "We claim to resolve the P=?NP problem via a formal argument for P=NP.",
        "published": "2004-06-28T19:11:18Z",
        "link": "http://arxiv.org/abs/cs/0406056v1",
        "categories": [
            "cs.CC",
            "cs.AI"
        ]
    },
    {
        "title": "The Complexity of Agreement",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "A celebrated 1976 theorem of Aumann asserts that honest, rational Bayesian agents with common priors will never \"agree to disagree\": if their opinions about any topic are common knowledge, then those opinions must be equal. Economists have written numerous papers examining the assumptions behind this theorem. But two key questions went unaddressed: first, can the agents reach agreement after a conversation of reasonable length? Second, can the computations needed for that conversation be performed efficiently? This paper answers both questions in the affirmative, thereby strengthening Aumann's original conclusion.   We first show that, for two agents with a common prior to agree within epsilon about the expectation of a [0,1] variable with high probability over their prior, it suffices for them to exchange order 1/epsilon^2 bits. This bound is completely independent of the number of bits n of relevant knowledge that the agents have. We then extend the bound to three or more agents; and we give an example where the economists' \"standard protocol\" (which consists of repeatedly announcing one's current expectation) nearly saturates the bound, while a new \"attenuated protocol\" does better. Finally, we give a protocol that would cause two Bayesians to agree within epsilon after exchanging order 1/epsilon^2 messages, and that can be simulated by agents with limited computational resources. By this we mean that, after examining the agents' knowledge and a transcript of their conversation, no one would be able to distinguish the agents from perfect Bayesians. The time used by the simulation procedure is exponential in 1/epsilon^6 but not in n.",
        "published": "2004-06-30T06:28:35Z",
        "link": "http://arxiv.org/abs/cs/0406061v1",
        "categories": [
            "cs.CC",
            "cs.GT"
        ]
    },
    {
        "title": "Classically-Controlled Quantum Computation",
        "authors": [
            "Simon Perdrix",
            "Philippe Jorrand"
        ],
        "summary": "Quantum computations usually take place under the control of the classical world. We introduce a Classically-controlled Quantum Turing Machine (CQTM) which is a Turing Machine (TM) with a quantum tape for acting on quantum data, and a classical transition function for a formalized classical control. In CQTM, unitary transformations and measurements are allowed. We show that any classical TM is simulated by a CQTM without loss of efficiency. The gap between classical and quantum computations, already pointed out in the framework of measurement-based quantum computation is confirmed. To appreciate the similarity of programming classical TM and CQTM, examples are given.",
        "published": "2004-07-01T14:21:03Z",
        "link": "http://arxiv.org/abs/quant-ph/0407008v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "An extension of Chaitin's halting probability Ωto a measurement   operator in an infinite dimensional quantum system",
        "authors": [
            "Kohtaro Tadaki"
        ],
        "summary": "This paper proposes an extension of Chaitin's halting probability \\Omega to a measurement operator in an infinite dimensional quantum system. Chaitin's \\Omega is defined as the probability that the universal self-delimiting Turing machine U halts, and plays a central role in the development of algorithmic information theory. In the theory, there are two equivalent ways to define the program-size complexity H(s) of a given finite binary string s. In the standard way, H(s) is defined as the length of the shortest input string for U to output s. In the other way, the so-called universal probability m is introduced first, and then H(s) is defined as -log_2 m(s) without reference to the concept of program-size.   Mathematically, the statistics of outcomes in a quantum measurement are described by a positive operator-valued measure (POVM) in the most general setting. Based on the theory of computability structures on a Banach space developed by Pour-El and Richards, we extend the universal probability to an analogue of POVM in an infinite dimensional quantum system, called a universal semi-POVM. We also give another characterization of Chaitin's \\Omega numbers by universal probabilities. Then, based on this characterization, we propose to define an extension of \\Omega as a sum of the POVM elements of a universal semi-POVM. The validity of this definition is discussed.   In what follows, we introduce an operator version \\hat{H}(s) of H(s) in a Hilbert space of infinite dimension using a universal semi-POVM, and study its properties.",
        "published": "2004-07-05T06:57:46Z",
        "link": "http://arxiv.org/abs/quant-ph/0407023v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Resource Bounded Immunity and Simplicity",
        "authors": [
            "Tomoyuki Yamakami",
            "Toshio Suzuki"
        ],
        "summary": "Revisiting the thirty years-old notions of resource-bounded immunity and simplicity, we investigate the structural characteristics of various immunity notions: strong immunity, almost immunity, and hyperimmunity as well as their corresponding simplicity notions. We also study limited immunity and simplicity, called k-immunity and feasible k-immunity, and their simplicity notions. Finally, we propose the k-immune hypothesis as a working hypothesis that guarantees the existence of simple sets in NP.",
        "published": "2004-07-06T01:36:12Z",
        "link": "http://arxiv.org/abs/cs/0407015v2",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "On the Complexity of Case-Based Planning",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "We analyze the computational complexity of problems related to case-based planning: planning when a plan for a similar instance is known, and planning from a library of plans. We prove that planning from a single case has the same complexity than generative planning (i.e., planning \"from scratch\"); using an extended definition of cases, complexity is reduced if the domain stored in the case is similar to the one to search plans for. Planning from a library of cases is shown to have the same complexity. In both cases, the complexity of planning remains, in the worst case, PSPACE-complete.",
        "published": "2004-07-15T10:13:28Z",
        "link": "http://arxiv.org/abs/cs/0407034v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.8"
        ]
    },
    {
        "title": "On the hardness of distinguishing mixed-state quantum computations",
        "authors": [
            "Bill Rosgen",
            "John Watrous"
        ],
        "summary": "This paper considers the following problem. Two mixed-state quantum circuits Q and R are given, and the goal is to determine which of two possibilities holds: (i) Q and R act nearly identically on all possible quantum state inputs, or (ii) there exists some input state that Q and R transform into almost perfectly distinguishable outputs. This problem may be viewed as an abstraction of the following problem: given two physical processes described by sequences of local interactions, are the processes effectively the same or are they different? We prove that this problem is a complete promise problem for the class QIP of problems having quantum interactive proof systems, and is therefore PSPACE-hard. This is in sharp contrast to the fact that the analogous problem for classical (probabilistic) circuits is in AM, and for unitary quantum circuits is in QMA.",
        "published": "2004-07-22T20:45:17Z",
        "link": "http://arxiv.org/abs/cs/0407056v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.2; F.1.3"
        ]
    },
    {
        "title": "Universal Convergence of Semimeasures on Individual Random Sequences",
        "authors": [
            "Marcus Hutter",
            "Andrej Muchnik"
        ],
        "summary": "Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.",
        "published": "2004-07-23T12:43:28Z",
        "link": "http://arxiv.org/abs/cs/0407057v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "math.IT",
            "math.PR",
            "I.2.6; E.4; G.3; F.1.3"
        ]
    },
    {
        "title": "Internal Turing Machines",
        "authors": [
            "Ken Loo"
        ],
        "summary": "Using nonstandard analysis, we will extend the classical Turing machines into the internal Turing machines. The internal Turing machines have the capability to work with infinite ($*$-finite) number of bits while keeping the finite combinatoric structures of the classical Turing machines. We will show the following. The internal deterministic Turing machines can do in $*$-polynomial time what a classical deterministic Turing machine can do in an arbitrary finite amount of time. Given an element of $<M;x>\\in HALT$ (more precisely, the $*$-embedding of $HALT$), there is an internal deterministic Turing machine which will take $<M;x>$ as input and halt in the $\"yes\"$ state. The language ${}^*Halt$ can not be decided by the internal deterministic Turing machines. The internal deterministic Turing machines can be viewed as the asymptotic behavior of finite precision approximation to real number computations. It is possible to use the internal probabilistic Turing machines to simulate finite state quantum mechanics with infinite precision. This simulation suggests that no information can be transmitted instantaneously and at the same time, the Turing machine model can simulate instantaneous collapse of the wave function. The internal deterministic Turing machines are powerful, but if $P \\neq NP$, then there are internal problems which the internal deterministic Turing machines can solve but not in $*$-polynomial time.",
        "published": "2004-07-23T23:47:41Z",
        "link": "http://arxiv.org/abs/math-ph/0407056v2",
        "categories": [
            "math-ph",
            "cs.CC",
            "math.LO",
            "math.MP",
            "quant-ph"
        ]
    },
    {
        "title": "Introduction to Random Boolean Networks",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "The goal of this tutorial is to promote interest in the study of random Boolean networks (RBNs). These can be very interesting models, since one does not have to assume any functionality or particular connectivity of the networks to study their generic properties. Like this, RBNs have been used for exploring the configurations where life could emerge. The fact that RBNs are a generalization of cellular automata makes their research a very important topic. The tutorial, intended for a broad audience, presents the state of the art in RBNs, spanning over several lines of research carried out by different groups. We focus on research done within artificial life, as we cannot exhaust the abundant research done over the decades related to RBNs.",
        "published": "2004-08-02T19:58:23Z",
        "link": "http://arxiv.org/abs/nlin/0408006v3",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.CC",
            "nlin.CG",
            "q-bio.MN",
            "q-bio.QM"
        ]
    },
    {
        "title": "Online convex optimization in the bandit setting: gradient descent   without a gradient",
        "authors": [
            "Abraham D. Flaxman",
            "Adam Tauman Kalai",
            "H. Brendan McMahan"
        ],
        "summary": "We consider a the general online convex optimization framework introduced by Zinkevich. In this setting, there is a sequence of convex functions. Each period, we must choose a signle point (from some feasible set) and pay a cost equal to the value of the next function on our chosen point. Zinkevich shows that, if the each function is revealed after the choice is made, then one can achieve vanishingly small regret relative the best single decision chosen in hindsight.   We extend this to the bandit setting where we do not find out the entire functions but rather just their value at our chosen point. We show how to get vanishingly small regret in this setting.   Our approach uses a simple approximation of the gradient that is computed from evaluating a function at a single (random) point. We show that this estimate is sufficient to mimic Zinkevich's gradient descent online analysis, with access to the gradient (only being able to evaluate the function at a single point).",
        "published": "2004-08-02T21:24:41Z",
        "link": "http://arxiv.org/abs/cs/0408007v1",
        "categories": [
            "cs.LG",
            "cs.CC"
        ]
    },
    {
        "title": "Perfect Delaunay Polytopes and Perfect Inhomogeneous Forms",
        "authors": [
            "Robert Erdahl",
            "Andrei Ordine",
            "Konstantin Rybnikov"
        ],
        "summary": "A lattice Delaunay polytope D is called perfect if it has the property that there is a unique circumscribing ellipsoid with interior free of lattice points, and with the surface containing only those lattice points that are the vertices of D. An inhomogeneous quadratic form is called perfect if it is determined by such a circumscribing ''empty ellipsoid'' uniquely up to a scale factor. Perfect inhomogeneous forms are associated with perfect Delaunay polytopes in much the way that perfect homogeneous forms are associated with perfect point lattices. We have been able to construct some infinite sequences of perfect Delaunay polytopes, one perfect polytope in each successive dimension starting at some initial dimension; we have been able to construct an infinite number of such infinite sequences. Perfect Delaunay polytopes are intimately related to the theory of Delaunay polytopes, and to Voronoi's theory of lattice types.",
        "published": "2004-08-09T20:34:25Z",
        "link": "http://arxiv.org/abs/math/0408122v3",
        "categories": [
            "math.NT",
            "cs.CC",
            "cs.CG",
            "math.MG",
            "quant-ph",
            "Primary: 11H50 and 11H55. Secondary: 11H06, 52C07, 52C22"
        ]
    },
    {
        "title": "Computational complexity and fundamental limitations to fermionic   quantum Monte Carlo simulations",
        "authors": [
            "Matthias Troyer",
            "Uwe-Jens Wiese"
        ],
        "summary": "Quantum Monte Carlo simulations, while being efficient for bosons, suffer from the \"negative sign problem'' when applied to fermions - causing an exponential increase of the computing time with the number of particles. A polynomial time solution to the sign problem is highly desired since it would provide an unbiased and numerically exact method to simulate correlated quantum systems. Here we show, that such a solution is almost certainly unattainable by proving that the sign problem is NP-hard, implying that a generic solution of the sign problem would also solve all problems in the complexity class NP (nondeterministic polynomial) in polynomial time.",
        "published": "2004-08-16T20:00:26Z",
        "link": "http://arxiv.org/abs/cond-mat/0408370v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.str-el",
            "cs.CC",
            "hep-lat",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Fractal geometry of literature: first attempt to Shakespeare's works",
        "authors": [
            "Ali Eftekhari"
        ],
        "summary": "It was demonstrated that there is a geometrical order in the structure of literature. Fractal geometry as a modern mathematical approach and a new geometrical viewpoint on natural objects including both processes and structures was employed for analysis of literature. As the first study, the works of William Shakespeare were chosen as the most important items in western literature. By counting the number of letters applied in a manuscript, it is possible to study the whole manuscript statistically. A novel method based on basic assumption of fractal geometry was proposed for the calculation of fractal dimensions of the literature. The results were compared with Zipf's law. Zipf's law was successfully used for letters instead of words. Two new concepts namely Zipf's dimension and Zipf's order were also introduced. It was found that changes of both fractal dimension and Zipf's dimension are similar and dependent on the manuscript length. Interestingly, direct plotting the data obtained in semi-logarithmic and logarithmic forms also led to a power-law.",
        "published": "2004-08-17T10:14:22Z",
        "link": "http://arxiv.org/abs/cs/0408041v1",
        "categories": [
            "cs.CL",
            "cs.CC"
        ]
    },
    {
        "title": "The Arithmetical Complexity of Dimension and Randomness",
        "authors": [
            "John M. Hitchcock",
            "Jack H. Lutz",
            "Sebastiaan A. Terwijn"
        ],
        "summary": "Constructive dimension and constructive strong dimension are effectivizations of the Hausdorff and packing dimensions, respectively. Each infinite binary sequence A is assigned a dimension dim(A) in [0,1] and a strong dimension Dim(A) in [0,1].   Let DIM^alpha and DIMstr^alpha be the classes of all sequences of dimension alpha and of strong dimension alpha, respectively. We show that DIM^0 is properly Pi^0_2, and that for all Delta^0_2-computable alpha in (0,1], DIM^alpha is properly Pi^0_3.   To classify the strong dimension classes, we use a more powerful effective Borel hierarchy where a co-enumerable predicate is used rather than a enumerable predicate in the definition of the Sigma^0_1 level. For all Delta^0_2-computable alpha in [0,1), we show that DIMstr^alpha is properly in the Pi^0_3 level of this hierarchy. We show that DIMstr^1 is properly in the Pi^0_2 level of this hierarchy.   We also prove that the class of Schnorr random sequences and the class of computably random sequences are properly Pi^0_3.",
        "published": "2004-08-18T16:23:27Z",
        "link": "http://arxiv.org/abs/cs/0408043v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Robust Locally Testable Codes and Products of Codes",
        "authors": [
            "Eli Ben-Sasson",
            "Madhu Sudan"
        ],
        "summary": "We continue the investigation of locally testable codes, i.e., error-correcting codes for whom membership of a given word in the code can be tested probabilistically by examining it in very few locations. We give two general results on local testability: First, motivated by the recently proposed notion of {\\em robust} probabilistically checkable proofs, we introduce the notion of {\\em robust} local testability of codes. We relate this notion to a product of codes introduced by Tanner, and show a very simple composition lemma for this notion. Next, we show that codes built by tensor products can be tested robustly and somewhat locally, by applying a variant of a test and proof technique introduced by Raz and Safra in the context of testing low-degree multivariate polynomials (which are a special case of tensor codes).   Combining these two results gives us a generic construction of codes of inverse polynomial rate, that are testable with poly-logarithmically many queries. We note these locally testable tensor codes can be obtained from {\\em any} linear error correcting code with good distance. Previous results on local testability, albeit much stronger quantitatively, rely heavily on algebraic properties of the underlying codes.",
        "published": "2004-08-30T16:36:38Z",
        "link": "http://arxiv.org/abs/cs/0408066v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "A New Look at Survey Propagation and its Generalizations",
        "authors": [
            "Eliza N. Maneva",
            "Elchanan Mossel",
            "Martin J. Wainwright"
        ],
        "summary": "This paper provides a new conceptual perspective on survey propagation, which is an iterative algorithm recently introduced by the statistical physics community that is very effective in solving random k-SAT problems even with densities close to the satisfiability threshold. We first describe how any SAT formula can be associated with a novel family of Markov random fields (MRFs), parameterized by a real number \\rho \\in [0,1]. We then show that applying belief propagation--a well-known ``message-passing'' technique for estimating marginal probabilities--to this family of MRFs recovers a known family of algorithms, ranging from pure survey propagation at one extreme (\\rho = 1) to standard belief propagation on the uniform distribution over SAT assignments at the other extreme (\\rho = 0). Configurations in these MRFs have a natural interpretation as partial satisfiability assignments, on which a partial order can be defined. We isolate cores as minimal elements in this partial ordering, which are also fixed points of survey propagation and the only assignments with positive probability in the MRF for \\rho=1. Our experimental results for k=3 suggest that solutions of random formulas typically do not possess non-trivial cores. This makes it necessary to study the structure of the space of partial assignments for \\rho<1 and investigate the role of assignments that are very close to being cores. To that end, we investigate the associated lattice structure, and prove a weight-preserving identity that shows how any MRF with \\rho>0 can be viewed as a ``smoothed'' version of the uniform distribution over satisfying assignments (\\rho=0). Finally, we isolate properties of Gibbs sampling and message-passing algorithms that are typical for an ensemble of k-SAT problems.",
        "published": "2004-09-08T01:27:14Z",
        "link": "http://arxiv.org/abs/cs/0409012v3",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The strength of replacement in weak arithmetic",
        "authors": [
            "Stephen Cook",
            "Neil Thapen"
        ],
        "summary": "The replacement (or collection or choice) axiom scheme asserts bounded quantifier exchange. We prove the independence of this scheme from various weak theories of arithmetic, sometimes under a complexity assumption.",
        "published": "2004-09-08T16:18:37Z",
        "link": "http://arxiv.org/abs/cs/0409015v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Poisson convergence in the restricted $k$-partioning problem",
        "authors": [
            "Anton Bovier",
            "Irina Kurkova"
        ],
        "summary": "The randomized $k$-number partitioning problem is the task to distribute $N$ i.i.d. random variables into $k$ groups in such a way that the sums of the variables in each group are as similar as possible. The restricted $k$-partitioning problem refers to the case where the number of elements in each group is fixed to $N/k$. In the case $k=2$ it has been shown that the properly rescaled differences of the two sums in the close to optimal partitions converge to a Poisson point process, as if they were independent random variables. We generalize this result to the case $k>2$ in the restricted problem and show that the vector of differences between the $k$ sums converges to a $k-1$-dimensional Poisson point process.",
        "published": "2004-09-21T08:03:37Z",
        "link": "http://arxiv.org/abs/cond-mat/0409532v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "NP - P is not empty",
        "authors": [
            "Marius Constantin Ionescu"
        ],
        "summary": "We present the MEoP problem that decides the existence of solutions to certain modular equations over prime numbers and show how this separates the complexity class NP from its subclass P",
        "published": "2004-09-21T20:29:42Z",
        "link": "http://arxiv.org/abs/cs/0409039v12",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Inapproximability of Combinatorial Optimization Problems",
        "authors": [
            "Luca Trevisan"
        ],
        "summary": "We survey results on the hardness of approximating combinatorial optimization problems.",
        "published": "2004-09-24T02:13:23Z",
        "link": "http://arxiv.org/abs/cs/0409043v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Some Applications of Coding Theory in Computational Complexity",
        "authors": [
            "Luca Trevisan"
        ],
        "summary": "Error-correcting codes and related combinatorial constructs play an important role in several recent (and old) results in computational complexity theory. In this paper we survey results on locally-testable and locally-decodable error-correcting codes, and their applications to complexity theory and to cryptography.   Locally decodable codes are error-correcting codes with sub-linear time error-correcting algorithms. They are related to private information retrieval (a type of cryptographic protocol), and they are used in average-case complexity and to construct ``hard-core predicates'' for one-way permutations.   Locally testable codes are error-correcting codes with sub-linear time error-detection algorithms, and they are the combinatorial core of probabilistically checkable proofs.",
        "published": "2004-09-24T02:38:24Z",
        "link": "http://arxiv.org/abs/cs/0409044v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Quantum Complexity Classes",
        "authors": [
            "Tereza Tusarova"
        ],
        "summary": "In our thesis, we try to shed more light onto the complexity of quantum complexity classes by refining the related part of the hierarchy. First, we review the basic concepts of quantum computing in general. Then, inspired by BQP, we define new complexity classes. They are placed between BPP and PSPACE. We show that they incorporate the current important quantum algorithms. Furthermore, the importance of the unitarity constraint given by quantum mechanics is revealed. Without this requirement, we naturally arrive at the class AWPP, which was up to now thought to be just an artificially defined class. We hope that some of our newly defined classes could find their use in proving results about BQP.",
        "published": "2004-09-26T19:28:38Z",
        "link": "http://arxiv.org/abs/cs/0409051v1",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "An Application of Quantum Finite Automata to Interactive Proof Systems",
        "authors": [
            "Harumichi Nishimura",
            "Tomoyuki Yamakami"
        ],
        "summary": "Quantum finite automata have been studied intensively since their introduction in late 1990s as a natural model of a quantum computer with finite-dimensional quantum memory space. This paper seeks their direct application to interactive proof systems in which a mighty quantum prover communicates with a quantum-automaton verifier through a common communication cell. Our quantum interactive proof systems are juxtaposed to Dwork-Stockmeyer's classical interactive proof systems whose verifiers are two-way probabilistic automata. We demonstrate strengths and weaknesses of our systems and further study how various restrictions on the behaviors of quantum-automaton verifiers affect the power of quantum interactive proof systems.",
        "published": "2004-10-05T19:59:49Z",
        "link": "http://arxiv.org/abs/quant-ph/0410040v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Complexity Results in Graph Reconstruction",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Stanislaw P. Radziszowski",
            "Rahul Tripathi"
        ],
        "summary": "We investigate the relative complexity of the graph isomorphism problem (GI) and problems related to the reconstruction of a graph from its vertex-deleted or edge-deleted subgraphs (in particular, deck checking (DC) and legitimate deck (LD) problems). We show that these problems are closely related for all amounts $c \\geq 1$ of deletion:   1) $GI \\equiv^{l}_{iso} VDC_{c}$, $GI \\equiv^{l}_{iso} EDC_{c}$, $GI \\leq^{l}_{m} LVD_c$, and $GI \\equiv^{p}_{iso} LED_c$.   2) For all $k \\geq 2$, $GI \\equiv^{p}_{iso} k-VDC_c$ and $GI \\equiv^{p}_{iso} k-EDC_c$.   3) For all $k \\geq 2$, $GI \\leq^{l}_{m} k-LVD_c$.   4)$GI \\equiv^{p}_{iso} 2-LVC_c$.   5) For all $k \\geq 2$, $GI \\equiv^{p}_{iso} k-LED_c$.   For many of these results, even the $c = 1$ case was not previously known.   Similar to the definition of reconstruction numbers $vrn_{\\exists}(G)$ [HP85] and $ern_{\\exists}(G)$ (see page 120 of [LS03]), we introduce two new graph parameters, $vrn_{\\forall}(G)$ and $ern_{\\forall}(G)$, and give an example of a family $\\{G_n\\}_{n \\geq 4}$ of graphs on $n$ vertices for which $vrn_{\\exists}(G_n) < vrn_{\\forall}(G_n)$. For every $k \\geq 2$ and $n \\geq 1$, we show that there exists a collection of $k$ graphs on $(2^{k-1}+1)n+k$ vertices with $2^{n}$ 1-vertex-preimages, i.e., one has families of graph collections whose number of 1-vertex-preimages is huge relative to the size of the graphs involved.",
        "published": "2004-10-11T00:57:19Z",
        "link": "http://arxiv.org/abs/cs/0410021v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "Balanced Boolean functions that can be evaluated so that every input bit   is unlikely to be read",
        "authors": [
            "Itai Benjamini",
            "Oded Schramm",
            "David B. Wilson"
        ],
        "summary": "A Boolean function of n bits is balanced if it takes the value 1 with probability 1/2. We exhibit a balanced Boolean function with a randomized evaluation procedure (with probability 0 of making a mistake) so that on uniformly random inputs, no input bit is read with probability more than Theta(n^{-1/2} sqrt{log n}). We give a balanced monotone Boolean function for which the corresponding probability is Theta(n^{-1/3} log n). We then show that for any randomized algorithm for evaluating a balanced Boolean function, when the input bits are uniformly random, there is some input bit that is read with probability at least Theta(n^{-1/2}). For balanced monotone Boolean functions, there is some input bit that is read with probability at least Theta(n^{-1/3}).",
        "published": "2004-10-12T00:09:35Z",
        "link": "http://arxiv.org/abs/math/0410282v1",
        "categories": [
            "math.PR",
            "cs.CC",
            "60C05, 60J80"
        ]
    },
    {
        "title": "All Superlinear Inverse Schemes are coNP-Hard",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "How hard is it to invert NP-problems? We show that all superlinearly certified inverses of NP problems are coNP-hard. To do so, we develop a novel proof technique that builds diagonalizations against certificates directly into a circuit.",
        "published": "2004-10-12T02:38:40Z",
        "link": "http://arxiv.org/abs/cs/0410023v1",
        "categories": [
            "cs.CC",
            "cs.CR",
            "F.1.3"
        ]
    },
    {
        "title": "The state complexity of L^2 and L^k",
        "authors": [
            "Narad Rampersad"
        ],
        "summary": "We show that if M is a DFA with n states over an arbitrary alphabet and L = L(M), then the worst-case state complexity of L^2 is n*2^n - 2^{n-1}. If, however, M is a DFA over a unary alphabet, then the worst-case state complexity of L^k is kn-k+1 for all k >= 2.",
        "published": "2004-10-14T15:43:46Z",
        "link": "http://arxiv.org/abs/cs/0410032v2",
        "categories": [
            "cs.CC",
            "cs.FL",
            "F.1.1"
        ]
    },
    {
        "title": "Overhead-Free Computation, DCFLs, and CFLs",
        "authors": [
            "Lane A. Hemaspaandra",
            "Proshanto Mukherji",
            "Till Tantau"
        ],
        "summary": "We study Turing machines that are allowed absolutely no space overhead. The only work space the machines have, beyond the fixed amount of memory implicit in their finite-state control, is that which they can create by cannibalizing the input bits' own space. This model more closely reflects the fixed-sized memory of real computers than does the standard complexity-theoretic model of linear space.   Though some context-sensitive languages cannot be accepted by such machines, we show that all context-free languages can be accepted nondeterministically in polynomial time with absolutely no space overhead, and that all deterministic context-free languages can be accepted deterministically in polynomial time with absolutely no space overhead.",
        "published": "2004-10-15T18:18:22Z",
        "link": "http://arxiv.org/abs/cs/0410035v1",
        "categories": [
            "cs.CC",
            "F.4.3; F.1.1"
        ]
    },
    {
        "title": "Hardware-Oriented Group Solutions for Hard Problems",
        "authors": [
            "Mark Burgin"
        ],
        "summary": "Group and individual solutions are considered for hard problems such as satisfiability problem. Time-space trade-off in a structured active memory provides means to achieve lower time complexity for solutions of these problems.",
        "published": "2004-10-15T23:12:12Z",
        "link": "http://arxiv.org/abs/cs/0410037v1",
        "categories": [
            "cs.CC",
            "F.2.0; F.1.3; F.2.2"
        ]
    },
    {
        "title": "Generalized Counters and Reversal Complexity",
        "authors": [
            "M. V. Panduranga Rao"
        ],
        "summary": "We generalize the definition of a counter and counter reversal complexity and investigate the power of generalized deterministic counter automata in terms of language recognition.",
        "published": "2004-10-25T19:36:07Z",
        "link": "http://arxiv.org/abs/cs/0410057v2",
        "categories": [
            "cs.CC",
            "F.1.1"
        ]
    },
    {
        "title": "Filled Julia sets with empty interior are computable",
        "authors": [
            "I. Binder",
            "M. Braverman",
            "M. Yampolsky"
        ],
        "summary": "We show that if a polynomial filled Julia set has empty interior, then it is computable.",
        "published": "2004-10-27T19:53:18Z",
        "link": "http://arxiv.org/abs/math/0410580v3",
        "categories": [
            "math.DS",
            "cs.CC",
            "37F10"
        ]
    },
    {
        "title": "Long Range Frustrations in a Spin Glass Model of the Vertex Cover   Problem",
        "authors": [
            "Haijun Zhou"
        ],
        "summary": "In a spin glass system on a random graph, some vertices have their spins changing among different configurations of a ground--state domain. Long range frustrations may exist among these unfrozen vertices in the sense that certain combinations of spin values for these vertices may never appear in any configuration of this domain. We present a mean field theory to tackle such long range frustrations and apply it to the NP-hard minimum vertex cover (hard-core gas condensation) problem. Our analytical results on the ground-state energy density and on the fraction of frozen vertices are in good agreement with known numerical and mathematical results.",
        "published": "2004-11-03T12:02:20Z",
        "link": "http://arxiv.org/abs/cond-mat/0411077v4",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Long range frustration in finite connectivity spin glasses: A mean field   theory and its application to the random $K$-satisfiability problem",
        "authors": [
            "Haijun Zhou"
        ],
        "summary": "Shortened abstract: A mean field theory of long range frustration is constructed for spin glass systems with quenched randomness of vertex--vertex connections and of spin--spin coupling strengths. This theory is applied to a spin glass model of the random $K$-satisfiability problem (K=2 or K=3).   The zero--temperature phase diagram of the $\\pm J$ Viana--Bray model is also determined, which is identical to that of the random 2-SAT problem. The predicted phase transition between a non-frustrated and a long--rangely frustrated spin glass phase might also be observable in real materials at a finite temperature.",
        "published": "2004-11-03T12:16:08Z",
        "link": "http://arxiv.org/abs/cond-mat/0411079v3",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Basic properties for sand automata",
        "authors": [
            "Julien Cervelle",
            "Enrico Formenti",
            "Benoit Masson"
        ],
        "summary": "We prove several results about the relations between injectivity and surjectivity for sand automata. Moreover, we begin the exploration of the dynamical behavior of sand automata proving that the property of nilpotency is undecidable. We believe that the proof technique used for this last result might reveal useful for many other results in this context.",
        "published": "2004-11-04T12:33:37Z",
        "link": "http://arxiv.org/abs/cs/0411007v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "First Steps in Algorithmic Fewnomial Theory",
        "authors": [
            "Frederic Bihan",
            "J. Maurice Rojas",
            "Casey E. Stella"
        ],
        "summary": "Fewnomial theory began with explicit bounds -- solely in terms of the number of variables and monomial terms -- on the number of real roots of systems of polynomial equations. Here we take the next logical step of investigating the corresponding existence problem: Let FEAS_R denote the problem of deciding whether a given system of multivariate polynomial equations with integer coefficients has a real root or not. We describe a phase-transition for when m is large enough to make FEAS_R be NP-hard, when restricted to inputs consisting of a single n-variate polynomial with exactly m monomial terms: polynomial-time for m<=n+2 (for any fixed n) and NP-hardness for m<=n+n^{epsilon} (for n varying and any fixed epsilon>0). Because of important connections between FEAS_R and A-discriminants, we then study some new families of A-discriminants whose signs can be decided within polynomial-time. (A-discriminants contain all known resultants as special cases, and the latter objects are central in algorithmic algebraic geometry.) Baker's Theorem from diophantine approximation arises as a key tool. Along the way, we also derive new quantitative bounds on the real zero sets of n-variate (n+2)-nomials.",
        "published": "2004-11-05T01:05:15Z",
        "link": "http://arxiv.org/abs/math/0411107v6",
        "categories": [
            "math.AG",
            "cs.CC",
            "math.AC"
        ]
    },
    {
        "title": "Quantum Communication Cannot Simulate a Public Coin",
        "authors": [
            "Dmytro Gavinsky",
            "Julia Kempe",
            "Ronald de Wolf"
        ],
        "summary": "We study the simultaneous message passing model of communication complexity. Building on the quantum fingerprinting protocol of Buhrman et al., Yao recently showed that a large class of efficient classical public-coin protocols can be turned into efficient quantum protocols without public coin. This raises the question whether this can be done always, i.e. whether quantum communication can always replace a public coin in the SMP model. We answer this question in the negative, exhibiting a communication problem where classical communication with public coin is exponentially more efficient than quantum communication. Together with a separation in the other direction due to Bar-Yossef et al., this shows that the quantum SMP model is incomparable with the classical public-coin SMP model.   In addition we give a characterization of the power of quantum fingerprinting by means of a connection to geometrical tools from machine learning, a quadratic improvement of Yao's simulation, and a nearly tight analysis of the Hamming distance problem from Yao's paper.",
        "published": "2004-11-08T11:11:02Z",
        "link": "http://arxiv.org/abs/quant-ph/0411051v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "On Invariance and Convergence in Time Complexity theory",
        "authors": [
            "Mircea Alexandru Popescu Moscu"
        ],
        "summary": "This article introduces three invariance principles under which P is different from NP. In the second part a theorem of convergence is proven. This theorem states that for any language L there exists an infinite sequence of languages from O(n) that converges to L.",
        "published": "2004-11-11T21:32:52Z",
        "link": "http://arxiv.org/abs/cs/0411033v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Note on Bulk Quantum Turing Machine",
        "authors": [
            "Tetsushi Matsui"
        ],
        "summary": "Recently, among experiments for realization of quantum computers, NMR quantum computers have achieved the most impressive succession. There is a model of the NMR quantum computation,namely Atsumi and Nishino's bulk quantum Turing Machine. It assumes, however, an unnatural assumption with quantum mechanics. We, then, define a more natural and quantum mechanically realizable modified bulk quantum Turing Machine, and show its computational ability by comparing complexity classes with quantum Turing Machine's counter part.",
        "published": "2004-11-12T18:56:15Z",
        "link": "http://arxiv.org/abs/cs/0411037v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Do All Elliptic Curves of the Same Order Have the Same Difficulty of   Discrete Log?",
        "authors": [
            "David Jao",
            "Stephen D. Miller",
            "Ramarathnam Venkatesan"
        ],
        "summary": "The aim of this paper is to justify the common cryptographic practice of selecting elliptic curves using their order as the primary criterion. We can formalize this issue by asking whether the discrete log problem (DLOG) has the same difficulty for all curves over a given finite field with the same order. We prove that this is essentially true by showing polynomial time random reducibility of DLOG among such curves, assuming the Generalized Riemann Hypothesis (GRH). We do so by constructing certain expander graphs, similar to Ramanujan graphs, with elliptic curves as nodes and low degree isogenies as edges.   The result is obtained from the rapid mixing of random walks on this graph. Our proof works only for curves with (nearly) the same endomorphism rings. Without this technical restriction such a DLOG equivalence might be false; however, in practice the restriction may be moot, because all known polynomial time techniques for constructing equal order curves produce only curves with nearly equal endomorphism rings.",
        "published": "2004-11-17T17:50:05Z",
        "link": "http://arxiv.org/abs/math/0411378v3",
        "categories": [
            "math.NT",
            "cs.CC",
            "cs.CR",
            "math.AG",
            "math.CO"
        ]
    },
    {
        "title": "Lower bounds on the Deterministic and Quantum Communication Complexity   of Hamming Distance",
        "authors": [
            "Andris Ambainis",
            "William Gasarch",
            "Aravind Srinavasan",
            "Andrey Utis"
        ],
        "summary": "Alice and Bob want to know if two strings of length n are almost equal. That is, do they differ on \\textit{at most} a bits? Let 0\\leq a\\leq n-1. We show that any deterministic protocol, as well as any error-free quantum protocol (C* version), for this problem requires at least n-2 bits of communication. We show the same bounds for the problem of determining if two strings differ in exactly a bits. We also prove a lower bound of n/2-1 for error-free Q* quantum protocols. Our results are obtained by lower-bounding the ranks of the appropriate matrices.",
        "published": "2004-11-20T19:55:19Z",
        "link": "http://arxiv.org/abs/cs/0411076v2",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "Signals for Cellular Automata in dimension 2 or higher",
        "authors": [
            "Jean-Christophe Dubacq",
            "Veronique Terrier"
        ],
        "summary": "We investigate how increasing the dimension of the array can help to draw signals on cellular automata.We show the existence of a gap of constructible signals in any dimension. We exhibit two cellular automata in dimension 2 to show that increasing the dimension allows to reduce the number of states required for some constructions.",
        "published": "2004-12-03T16:35:17Z",
        "link": "http://arxiv.org/abs/cs/0412013v1",
        "categories": [
            "cs.CC",
            "cs.DC",
            "cs.DM",
            "math.CO"
        ]
    },
    {
        "title": "Zeno machines and hypercomputation",
        "authors": [
            "Petrus H. Potgieter"
        ],
        "summary": "This paper reviews the Church-Turing Thesis (or rather, theses) with reference to their origin and application and considers some models of \"hypercomputation\", concentrating on perhaps the most straight-forward option: Zeno machines (Turing machines with accelerating clock). The halting problem is briefly discussed in a general context and the suggestion that it is an inevitable companion of any reasonable computational model is emphasised. It is hinted that claims to have \"broken the Turing barrier\" could be toned down and that the important and well-founded role of Turing computability in the mathematical sciences stands unchallenged.",
        "published": "2004-12-06T12:18:05Z",
        "link": "http://arxiv.org/abs/cs/0412022v3",
        "categories": [
            "cs.CC",
            "F.1.1"
        ]
    },
    {
        "title": "The approximability of three-valued MAX CSP",
        "authors": [
            "Peter Jonsson",
            "Mikael Klasson",
            "Andrei Krokhin"
        ],
        "summary": "In the maximum constraint satisfaction problem (Max CSP), one is given a finite collection of (possibly weighted) constraints on overlapping sets of variables, and the goal is to assign values from a given domain to the variables so as to maximize the number (or the total weight, for the weighted case) of satisfied constraints. This problem is NP-hard in general, and, therefore, it is natural to study how restricting the allowed types of constraints affects the approximability of the problem. It is known that every Boolean (that is, two-valued) Max CSP problem with a finite set of allowed constraint types is either solvable exactly in polynomial time or else APX-complete (and hence can have no polynomial time approximation scheme unless P=NP. It has been an open problem for several years whether this result can be extended to non-Boolean Max CSP, which is much more difficult to analyze than the Boolean case. In this paper, we make the first step in this direction by establishing this result for Max CSP over a three-element domain. Moreover, we present a simple description of all polynomial-time solvable cases of our problem. This description uses the well-known algebraic combinatorial property of supermodularity. We also show that every hard three-valued Max CSP problem contains, in a certain specified sense, one of the two basic hard Max CSP problems which are the Maximum k-colourable subgraph problems for k=2,3.",
        "published": "2004-12-10T15:34:54Z",
        "link": "http://arxiv.org/abs/cs/0412042v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "On computing fixed points for generalized sandpiles",
        "authors": [
            "Enrico Formenti",
            "Benoit Masson"
        ],
        "summary": "We prove fixed points results for sandpiles starting with arbitrary initial conditions. We give an effective algorithm for computing such fixed points, and we refine it in the particular case of SPM.",
        "published": "2004-12-11T07:51:35Z",
        "link": "http://arxiv.org/abs/cs/0412048v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Quantum and Classical Communication-Space Tradeoffs from Rectangle   Bounds",
        "authors": [
            "Hartmut Klauck"
        ],
        "summary": "We derive lower bounds for tradeoffs between the communication C and space S for communicating circuits. The first such bound applies to quantum circuits. If for any function f with image Z the multicolor discrepancy of the communication matrix of f is 1/2^d, then any bounded error quantum protocol with space S, in which Alice receives some l inputs, Bob r inputs, and they compute f(x_i,y_j) for the lr pairs of inputs (x_i,y_j) needs communication C=\\Omega(lrd \\log |Z|/S). In particular, n\\times n-matrix multiplication over a finite field F requires C=\\Theta(n^3\\log^2 |F|/S). We then turn to randomized bounded error protocols, and derive the bound C=\\Omega(n^3/S^2) for Boolean matrix multiplication, utilizing a new direct product result for the one-sided rectangle lower bound on randomized communication complexity. This implies a separation between quantum and randomized protocols.",
        "published": "2004-12-11T17:41:12Z",
        "link": "http://arxiv.org/abs/quant-ph/0412088v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Isomorphic Implication",
        "authors": [
            "Michael Bauland",
            "Edith Hemaspaandra"
        ],
        "summary": "We study the isomorphic implication problem for Boolean constraints. We show that this is a natural analog of the subgraph isomorphism problem. We prove that, depending on the set of constraints, this problem is in P, NP-complete, or NP-hard, coNP-hard, and in parallel access to NP. We show how to extend the NP-hardness and coNP-hardness to hardness for parallel access to NP for some cases, and conjecture that this can be done in all cases.",
        "published": "2004-12-14T17:15:29Z",
        "link": "http://arxiv.org/abs/cs/0412062v2",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Limits on Efficient Computation in the Physical World",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "More than a speculative technology, quantum computing seems to challenge our most basic intuitions about how the physical world should behave. In this thesis I show that, while some intuitions from classical computer science must be jettisoned in the light of modern physics, many others emerge nearly unscathed; and I use powerful tools from computational complexity theory to help determine which are which.",
        "published": "2004-12-20T07:36:02Z",
        "link": "http://arxiv.org/abs/quant-ph/0412143v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Complexity of Self-Assembled Shapes",
        "authors": [
            "David Soloveichik",
            "Erik Winfree"
        ],
        "summary": "The connection between self-assembly and computation suggests that a shape can be considered the output of a self-assembly ``program,'' a set of tiles that fit together to create a shape. It seems plausible that the size of the smallest self-assembly program that builds a shape and the shape's descriptional (Kolmogorov) complexity should be related. We show that when using a notion of a shape that is independent of scale, this is indeed so: in the Tile Assembly Model, the minimal number of distinct tile types necessary to self-assemble a shape, at some scale, can be bounded both above and below in terms of the shape's Kolmogorov complexity. As part of the proof of the main result, we sketch a general method for converting a program outputting a shape as a list of locations into a set of tile types that self-assembles into a scaled up version of that shape. Our result implies, somewhat counter-intuitively, that self-assembly of a scaled-up version of a shape often requires fewer tile types. Furthermore, the independence of scale in self-assembly theory appears to play the same crucial role as the independence of running time in the theory of computability. This leads to an elegant formulation of languages of shapes generated by self-assembly. Considering functions from integers to shapes, we show that the running-time complexity, with respect to Turing machines, is polynomially equivalent to the scale complexity of the same function implemented via self-assembly by a finite set of tile types. Our results also hold for shapes defined by Wang tiling -- where there is no sense of a self-assembly process -- except that here time complexity must be measured with respect to non-deterministic Turing machines.",
        "published": "2004-12-21T10:30:46Z",
        "link": "http://arxiv.org/abs/cs/0412096v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "The Computational Power of Benenson Automata",
        "authors": [
            "David Soloveichik",
            "Erik Winfree"
        ],
        "summary": "The development of autonomous molecular computers capable of making independent decisions in vivo regarding local drug administration may revolutionize medical science. Recently Benenson at el (2004) have envisioned one form such a ``smart drug'' may take by implementing an in vitro scheme, in which a long DNA state molecule is cut repeatedly by a restriction enzyme in a manner dependent upon the presence of particular short DNA ``rule molecules.'' To analyze the potential of their scheme in terms of the kinds of computations it can perform, we study an abstraction assuming that a certain class of restriction enzymes is available and reactions occur without error. We also discuss how our molecular algorithms could perform with known restriction enzymes. By exhibiting a way to simulate arbitrary circuits, we show that these ``Benenson automata'' are capable of computing arbitrary Boolean functions. Further, we show that they are able to compute efficiently exactly those functions computable by log-depth circuits. Computationally, we formalize a new variant of limited width branching programs with a molecular implementation.",
        "published": "2004-12-21T10:57:15Z",
        "link": "http://arxiv.org/abs/cs/0412097v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.3"
        ]
    },
    {
        "title": "Quantum Interactive Proofs with Competing Provers",
        "authors": [
            "Gus Gutoski",
            "John Watrous"
        ],
        "summary": "This paper studies quantum refereed games, which are quantum interactive proof systems with two competing provers: one that tries to convince the verifier to accept and the other that tries to convince the verifier to reject. We prove that every language having an ordinary quantum interactive proof system also has a quantum refereed game in which the verifier exchanges just one round of messages with each prover. A key part of our proof is the fact that there exists a single quantum measurement that reliably distinguishes between mixed states chosen arbitrarily from disjoint convex sets having large minimal trace distance from one another. We also show how to reduce the probability of error for some classes of quantum refereed games.",
        "published": "2004-12-22T19:39:24Z",
        "link": "http://arxiv.org/abs/cs/0412102v1",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "Quantum Computing, Postselection, and Probabilistic Polynomial-Time",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "I study the class of problems efficiently solvable by a quantum computer, given the ability to \"postselect\" on the outcomes of measurements. I prove that this class coincides with a classical complexity class called PP, or Probabilistic Polynomial-Time. Using this result, I show that several simple changes to the axioms of quantum mechanics would let us solve PP-complete problems efficiently. The result also implies, as an easy corollary, a celebrated theorem of Beigel, Reingold, and Spielman that PP is closed under intersection, as well as a generalization of that theorem due to Fortnow and Reingold. This illustrates that quantum computing can yield new and simpler proofs of major results about classical computation.",
        "published": "2004-12-23T15:09:31Z",
        "link": "http://arxiv.org/abs/quant-ph/0412187v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Query Answering in Peer-to-Peer Data Exchange Systems",
        "authors": [
            "Leopoldo Bertossi",
            "Loreto Bravo"
        ],
        "summary": "The problem of answering queries posed to a peer who is a member of a peer-to-peer data exchange system is studied. The answers have to be consistent wrt to both the local semantic constraints and the data exchange constraints with other peers; and must also respect certain trust relationships between peers. A semantics for peer consistent answers under exchange constraints and trust relationships is introduced and some techniques for obtaining those answers are presented.",
        "published": "2004-01-20T19:26:35Z",
        "link": "http://arxiv.org/abs/cs/0401015v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.4;F.4.1;I.2.3"
        ]
    },
    {
        "title": "Generalized Strong Preservation by Abstract Interpretation",
        "authors": [
            "Francesco Ranzato",
            "Francesco Tapparo"
        ],
        "summary": "Standard abstract model checking relies on abstract Kripke structures which approximate concrete models by gluing together indistinguishable states, namely by a partition of the concrete state space. Strong preservation for a specification language L encodes the equivalence of concrete and abstract model checking of formulas in L. We show how abstract interpretation can be used to design abstract models that are more general than abstract Kripke structures. Accordingly, strong preservation is generalized to abstract interpretation-based models and precisely related to the concept of completeness in abstract interpretation. The problem of minimally refining an abstract model in order to make it strongly preserving for some language L can be formulated as a minimal domain refinement in abstract interpretation in order to get completeness w.r.t. the logical/temporal operators of L. It turns out that this refined strongly preserving abstract model always exists and can be characterized as a greatest fixed point. As a consequence, some well-known behavioural equivalences, like bisimulation, simulation and stuttering, and their corresponding partition refinement algorithms can be elegantly characterized in abstract interpretation as completeness properties and refinements.",
        "published": "2004-01-21T14:28:32Z",
        "link": "http://arxiv.org/abs/cs/0401016v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.2.4; F.3.1; F.3.2"
        ]
    },
    {
        "title": "Deciding Disjunctive Linear Arithmetic with SAT",
        "authors": [
            "Ofer Strichman"
        ],
        "summary": "Disjunctive Linear Arithmetic (DLA) is a major decidable theory that is supported by almost all existing theorem provers. The theory consists of Boolean combinations of predicates of the form $\\Sigma_{j=1}^{n}a_j\\cdot x_j \\le b$, where the coefficients $a_j$, the bound $b$ and the variables $x_1 >... x_n$ are of type Real ($\\mathbb{R}$). We show a reduction to propositional logic from disjunctive linear arithmetic based on Fourier-Motzkin elimination. While the complexity of this procedure is not better than competing techniques, it has practical advantages in solving verification problems. It also promotes the option of deciding a combination of theories by reducing them to this logic. Results from experiments show that this method has a strong advantage over existing techniques when there are many disjunctions in the formula.",
        "published": "2004-02-01T12:42:20Z",
        "link": "http://arxiv.org/abs/cs/0402002v1",
        "categories": [
            "cs.LO",
            "B.1.4"
        ]
    },
    {
        "title": "The logic of entanglement",
        "authors": [
            "Bob Coecke"
        ],
        "summary": "We expose the information flow capabilities of pure bipartite entanglement as a theorem -- which embodies the exact statement on the `seemingly acausal flow of information' in protocols such as teleportation. We use this theorem to re-design and analyze known protocols (e.g. logic gate teleportation and entanglement swapping) and show how to produce some new ones (e.g. parallel composition of logic gates). We also show how our results extend to the multipartite case and how they indicate that entanglement can be measured in terms of `information flow capabilities'. Ultimately, we propose a scheme for automated design of protocols involving measurements, local unitary transformations and classical communication.",
        "published": "2004-02-02T17:22:38Z",
        "link": "http://arxiv.org/abs/quant-ph/0402014v2",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math-ph",
            "math.CT",
            "math.MP"
        ]
    },
    {
        "title": "Encapsulation for Practical Simplification Procedures",
        "authors": [
            "Olga Shumsky Matlin",
            "William McCune"
        ],
        "summary": "ACL2 was used to prove properties of two simplification procedures. The procedures differ in complexity but solve the same programming problem that arises in the context of a resolution/paramodulation theorem proving system. Term rewriting is at the core of the two procedures, but details of the rewriting procedure itself are irrelevant. The ACL2 encapsulate construct was used to assert the existence of the rewriting function and to state some of its properties. Termination, irreducibility, and soundness properties were established for each procedure. The availability of the encapsulation mechanism in ACL2 is considered essential to rapid and efficient verification of this kind of algorithm.",
        "published": "2004-02-03T19:04:02Z",
        "link": "http://arxiv.org/abs/cs/0402010v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Corollaries on the fixpoint completion: studying the stable semantics by   means of the Clark completion",
        "authors": [
            "Pascal Hitzler"
        ],
        "summary": "The fixpoint completion fix(P) of a normal logic program P is a program transformation such that the stable models of P are exactly the models of the Clark completion of fix(P). This is well-known and was studied by Dung and Kanchanasut (1989). The correspondence, however, goes much further: The Gelfond-Lifschitz operator of P coincides with the immediate consequence operator of fix(P), as shown by Wendt (2002), and even carries over to standard operators used for characterizing the well-founded and the Kripke-Kleene semantics. We will apply this knowledge to the study of the stable semantics, and this will allow us to almost effortlessly derive new results concerning fixed-point and metric-based semantics, and neural-symbolic integration.",
        "published": "2004-02-09T11:03:20Z",
        "link": "http://arxiv.org/abs/cs/0402013v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Towards a Mathematical Theory of the Delays of the Asynchronous Circuits",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "The inequations of the delays of the asynchronous circuits are written, by making use of pseudo-Boolean differential calculus. We consider these efforts to be a possible starting point in the semi-formalized reconstruction of the digital electrical engineering (which is a non-formalized theory).",
        "published": "2004-02-17T12:11:56Z",
        "link": "http://arxiv.org/abs/cs/0402038v1",
        "categories": [
            "cs.LO",
            "I.6.0"
        ]
    },
    {
        "title": "On the Inertia of the Asynchronous Circuits",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "We present the bounded delays, the absolute inertia and the relative inertia.",
        "published": "2004-02-17T12:54:00Z",
        "link": "http://arxiv.org/abs/cs/0402039v1",
        "categories": [
            "cs.LO",
            "H.1.0"
        ]
    },
    {
        "title": "Defining the Delays of the Asynchronous Circuits",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "We define the delays of a circuit, as well as the properties of determinism, order, time invariance, constancy, symmetry and the serial connection.",
        "published": "2004-02-17T13:01:05Z",
        "link": "http://arxiv.org/abs/cs/0402040v1",
        "categories": [
            "cs.LO",
            "H.1.0"
        ]
    },
    {
        "title": "Examples of Models of the Asynchronous Circuits",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "We define the delays of a circuit, as well as the properties of determinism, order, time invariance, constancy, symmetry and the serial connection.",
        "published": "2004-02-17T13:07:24Z",
        "link": "http://arxiv.org/abs/cs/0402041v1",
        "categories": [
            "cs.LO",
            "H.1.0"
        ]
    },
    {
        "title": "A categorical semantics of quantum protocols",
        "authors": [
            "Samson Abramsky",
            "Bob Coecke"
        ],
        "summary": "We study quantum information and computation from a novel point of view. Our approach is based on recasting the standard axiomatic presentation of quantum mechanics, due to von Neumann, at a more abstract level, of compact closed categories with biproducts. We show how the essential structures found in key quantum information protocols such as teleportation, logic-gate teleportation, and entanglement-swapping can be captured at this abstract level. Moreover, from the combination of the --apparently purely qualitative-- structures of compact closure and biproducts there emerge `scalars` and a `Born rule'. This abstract and structural point of view opens up new possibilities for describing and reasoning about quantum systems. It also shows the degrees of axiomatic freedom: we can show what requirements are placed on the (semi)ring of scalars C(I,I), where C is the category and I is the tensor unit, in order to perform various protocols such as teleportation. Our formalism captures both the information-flow aspect of the protocols (see quant-ph/0402014), and the branching due to quantum indeterminism. This contrasts with the standard accounts, in which the classical information flows are `outside' the usual quantum-mechanical formalism.",
        "published": "2004-02-18T19:37:24Z",
        "link": "http://arxiv.org/abs/quant-ph/0402130v5",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math-ph",
            "math.CT",
            "math.MP"
        ]
    },
    {
        "title": "Anonymity and Information Hiding in Multiagent Systems",
        "authors": [
            "Joseph Y. Halpern",
            "Kevin R. O'Neill"
        ],
        "summary": "We provide a framework for reasoning about information-hiding requirements in multiagent systems and for reasoning about anonymity in particular. Our framework employs the modal logic of knowledge within the context of the runs and systems framework, much in the spirit of our earlier work on secrecy [Halpern and O'Neill 2002]. We give several definitions of anonymity with respect to agents, actions, and observers in multiagent systems, and we relate our definitions of anonymity to other definitions of information hiding, such as secrecy. We also give probabilistic definitions of anonymity that are able to quantify an observer s uncertainty about the state of the system. Finally, we relate our definitions of anonymity to other formalizations of anonymity and information hiding, including definitions of anonymity in the process algebra CSP and definitions of information hiding using function views.",
        "published": "2004-02-18T20:46:52Z",
        "link": "http://arxiv.org/abs/cs/0402042v2",
        "categories": [
            "cs.CR",
            "cs.LO",
            "cs.MA",
            "D.4.6; D.2.1"
        ]
    },
    {
        "title": "Transformation Rules for Locally Stratified Constraint Logic Programs",
        "authors": [
            "Fabio Fioravanti",
            "Alberto Pettorossi",
            "Maurizio Proietti"
        ],
        "summary": "We propose a set of transformation rules for constraint logic programs with negation. We assume that every program is locally stratified and, thus, it has a unique perfect model. We give sufficient conditions which ensure that the proposed set of transformation rules preserves the perfect model of the programs. Our rules extend in some respects the rules for logic programs and constraint logic programs already considered in the literature and, in particular, they include a rule for unfolding a clause with respect to a negative literal.",
        "published": "2004-02-20T14:09:02Z",
        "link": "http://arxiv.org/abs/cs/0402048v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "Light types for polynomial time computation in lambda-calculus",
        "authors": [
            "Patrick Baillot",
            "Kazushige Terui"
        ],
        "summary": "We propose a new type system for lambda-calculus ensuring that well-typed programs can be executed in polynomial time: Dual light affine logic (DLAL).   DLAL has a simple type language with a linear and an intuitionistic type arrow, and one modality. It corresponds to a fragment of Light affine logic (LAL). We show that contrarily to LAL, DLAL ensures good properties on lambda-terms: subject reduction is satisfied and a well-typed term admits a polynomial bound on the reduction by any strategy. We establish that as LAL, DLAL allows to represent all polytime functions. Finally we give a type inference procedure for propositional DLAL.",
        "published": "2004-02-26T15:47:36Z",
        "link": "http://arxiv.org/abs/cs/0402059v2",
        "categories": [
            "cs.LO",
            "F.4"
        ]
    },
    {
        "title": "Polymorphic lemmas and definitions in Lambda Prolog and Twelf",
        "authors": [
            "Andrew W. Appel",
            "Amy P. Felty"
        ],
        "summary": "Lambda Prolog is known to be well-suited for expressing and implementing logics and inference systems. We show that lemmas and definitions in such logics can be implemented with a great economy of expression. We encode a higher-order logic using an encoding that maps both terms and types of the object logic (higher-order logic) to terms of the metalanguage (Lambda Prolog). We discuss both the Terzo and Teyjus implementations of Lambda Prolog. We also encode the same logic in Twelf and compare the features of these two metalanguages for our purposes.",
        "published": "2004-03-09T02:38:37Z",
        "link": "http://arxiv.org/abs/cs/0403010v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1; D.2.4; I.2.3; D.1.6"
        ]
    },
    {
        "title": "Uniform Proofs of Order Independence for Various Strategy Elimination   Procedures",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "We provide elementary and uniform proofs of order independence for various strategy elimination procedures for finite strategic games, both for dominance by pure and by mixed strategies. The proofs follow the same pattern and focus on the structural properties of the dominance relations. They rely on Newman's Lemma established in 1942 and related results on the abstract reduction systems.",
        "published": "2004-03-15T07:13:07Z",
        "link": "http://arxiv.org/abs/cs/0403024v2",
        "categories": [
            "cs.GT",
            "cs.LO",
            "J.4"
        ]
    },
    {
        "title": "An Application of Rational Trees in a Logic Programming Interpreter for   a Procedural Language",
        "authors": [
            "Manuel Carro"
        ],
        "summary": "We describe here a simple application of rational trees to the implementation of an interpreter for a procedural language written in a logic programming language. This is possible in languages designed to support rational trees (such as Prolog II and its descendants), but also in traditional Prolog, whose data structures are initially based on Herbrand terms, but in which implementations often omit the occurs check needed to avoid the creation of infinite data structures. We provide code implementing two interpreters, one of which needs non-occurs-check unification, which makes it faster (and more economic). We provide experimental data supporting this, and we argue that rational trees are interesting enough as to receive thorough support inside the language.",
        "published": "2004-03-16T16:48:38Z",
        "link": "http://arxiv.org/abs/cs/0403028v1",
        "categories": [
            "cs.DS",
            "cs.LO",
            "D.1.6 ; D.3.2 ; D.3.3 ; D.3.4 ; E.1 ; E.2"
        ]
    },
    {
        "title": "Where Fail-Safe Default Logics Fail",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "Reiter's original definition of default logic allows for the application of a default that contradicts a previously applied one. We call failure this condition. The possibility of generating failures has been in the past considered as a semantical problem, and variants have been proposed to solve it. We show that it is instead a computational feature that is needed to encode some domains into default logic.",
        "published": "2004-03-19T15:20:54Z",
        "link": "http://arxiv.org/abs/cs/0403032v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3"
        ]
    },
    {
        "title": "Integrating design synthesis and assembly of structured objects in a   visual design language",
        "authors": [
            "Omid Banyasad",
            "Philip T. Cox"
        ],
        "summary": "Computer Aided Design systems provide tools for building and manipulating models of solid objects. Some also provide access to programming languages so that parametrised designs can be expressed. There is a sharp distinction, therefore, between building models, a concrete graphical editing activity, and programming, an abstract, textual, algorithm-construction activity. The recently proposed Language for Structured Design (LSD) was motivated by a desire to combine the design and programming activities in one language. LSD achieves this by extending a visual logic programming language to incorporate the notions of solids and operations on solids. Here we investigate another aspect of the LSD approach; namely, that by using visual logic programming as the engine to drive the parametrised assembly of objects, we also gain the powerful symbolic problem-solving capability that is the forte of logic programming languages. This allows the designer/programmer to work at a higher level, giving declarative specifications of a design in order to obtain the design descriptions. Hence LSD integrates problem solving, design synthesis, and prototype assembly in a single homogeneous programming/design environment. We demonstrate this specification-to-final-assembly capability using the masterkeying problem for designing systems of locks and keys.",
        "published": "2004-03-21T20:30:58Z",
        "link": "http://arxiv.org/abs/cs/0403033v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; D.1.7; J.6"
        ]
    },
    {
        "title": "A Theory of Computation Based on Quantum Logic (I)",
        "authors": [
            "Mingsheng Ying"
        ],
        "summary": "The (meta)logic underlying classical theory of computation is Boolean (two-valued) logic. Quantum logic was proposed by Birkhoff and von Neumann as a logic of quantum mechanics more than sixty years ago. The major difference between Boolean logic and quantum logic is that the latter does not enjoy distributivity in general. The rapid development of quantum computation in recent years stimulates us to establish a theory of computation based on quantum logic. The present paper is the first step toward such a new theory and it focuses on the simplest models of computation, namely finite automata. It is found that the universal validity of many properties of automata depend heavily upon the distributivity of the underlying logic. This indicates that these properties does not universally hold in the realm of quantum logic. On the other hand, we show that a local validity of them can be recovered by imposing a certain commutativity to the (atomic) statements about the automata under consideration. This reveals an essential difference between the classical theory of computation and the computation theory based on quantum logic.",
        "published": "2004-03-29T15:20:32Z",
        "link": "http://arxiv.org/abs/cs/0403041v1",
        "categories": [
            "cs.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Scalable Probabilistic Models for 802.11 Protocol Verification",
        "authors": [
            "Amitabha Roy",
            "K. Gopinath"
        ],
        "summary": "The IEEE 802.11 protocol is a popular standard for wireless local area networks. Its medium access control layer (MAC) is a carrier sense multiple access with collision avoidance (CSMA/CA) design and includes an exponential backoff mechanism that makes it a possible target for probabilistic model checking. In this work, we identify ways to increase the scope of application of probabilistic model checking to the 802.11 MAC. Current techniques do not scale to networks of even moderate size. To work around this problem, we identify properties of the protocol that can be used to simplify the models and make verification feasible. Using these observations, we directly optimize the probabilistic timed automata models while preserving probabilistic reachability measures. We substantiate our claims of significant reduction by our results from using the probabilistic model checker PRISM.",
        "published": "2004-03-31T13:42:22Z",
        "link": "http://arxiv.org/abs/cs/0403044v2",
        "categories": [
            "cs.LO",
            "cs.NI",
            "F.4.1"
        ]
    },
    {
        "title": "Decidability and Universality in Symbolic Dynamical Systems",
        "authors": [
            "Jean-Charles Delvenne",
            "Petr Kurka",
            "Vincent Blondel"
        ],
        "summary": "Many different definitions of computational universality for various types of dynamical systems have flourished since Turing's work. We propose a general definition of universality that applies to arbitrary discrete time symbolic dynamical systems. Universality of a system is defined as undecidability of a model-checking problem. For Turing machines, counter machines and tag systems, our definition coincides with the classical one. It yields, however, a new definition for cellular automata and subshifts. Our definition is robust with respect to initial condition, which is a desirable feature for physical realizability.   We derive necessary conditions for undecidability and universality. For instance, a universal system must have a sensitive point and a proper subsystem. We conjecture that universal systems have infinite number of subsystems. We also discuss the thesis according to which computation should occur at the `edge of chaos' and we exhibit a universal chaotic system.",
        "published": "2004-04-08T02:45:34Z",
        "link": "http://arxiv.org/abs/cs/0404021v4",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Propositional computability logic I",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "In the same sense as classical logic is a formal theory of truth, the recently initiated approach called computability logic is a formal theory of computability. It understands (interactive) computational problems as games played by a machine against the environment, their computability as existence of a machine that always wins the game, logical operators as operations on computational problems, and validity of a logical formula as being a scheme of \"always computable\" problems. The present contribution gives a detailed exposition of a soundness and completeness proof for an axiomatization of one of the most basic fragments of computability logic. The logical vocabulary of this fragment contains operators for the so called parallel and choice operations, and its atoms represent elementary problems, i.e. predicates in the standard sense. This article is self-contained as it explains all relevant concepts. While not technically necessary, however, familiarity with the foundational paper \"Introduction to computability logic\" [Annals of Pure and Applied Logic 123 (2003), pp.1-99] would greatly help the reader in understanding the philosophy, underlying motivations, potential and utility of computability logic, -- the context that determines the value of the present results. Online introduction to the subject is available at http://www.cis.upenn.edu/~giorgi/cl.html and http://www.csc.villanova.edu/~japaridz/CL/gsoll.html .",
        "published": "2004-04-09T00:55:23Z",
        "link": "http://arxiv.org/abs/cs/0404023v2",
        "categories": [
            "cs.LO",
            "math.LO",
            "F.1.1; F.1.2; F.4.1"
        ]
    },
    {
        "title": "Computability Logic: a formal theory of interaction",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic is a formal theory of (interactive) computability in the same sense as classical logic is a formal theory of truth. This approach was initiated very recently in \"Introduction to computability logic\" (Annals of Pure and Applied Logic 123 (2003), pp.1-99). The present paper reintroduces computability logic in a more compact and less technical way. It is written in a semitutorial style with a general computer science, logic or mathematics audience in mind. An Internet source on the subject is available at http://www.cis.upenn.edu/~giorgi/cl.html, and additional material at http://www.csc.villanova.edu/~japaridz/CL/gsoll.html .",
        "published": "2004-04-09T01:42:00Z",
        "link": "http://arxiv.org/abs/cs/0404024v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "XML framework for concept description and knowledge representation",
        "authors": [
            "Andreas de Vries"
        ],
        "summary": "An XML framework for concept description is given, based upon the fact that the tree structure of XML implies the logical structure of concepts as defined by attributional calculus. Especially, the attribute-value representation is implementable in the XML framework. Since the attribute-value representation is an important way to represent knowledge in AI, the framework offers a further and simpler way than the powerful RDF technology.",
        "published": "2004-04-14T15:36:43Z",
        "link": "http://arxiv.org/abs/cs/0404030v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.7.2; E.2; H.1.1; G.2.3"
        ]
    },
    {
        "title": "Model-checking Driven Black-box Testing Algorithms for Systems with   Unspecified Components",
        "authors": [
            "Gaoyan Xie",
            "Zhe Dang"
        ],
        "summary": "Component-based software development has posed a serious challenge to system verification since externally-obtained components could be a new source of system failures. This issue can not be completely solved by either model-checking or traditional software testing techniques alone due to several reasons:   1) externally obtained components are usually unspecified/partially specified; 2)it is generally difficult to establish an adequacy criteria for testing a component; 3)components may be used to dynamically upgrade a system.   This paper introduces a new approach (called {\\em model-checking driven black-box testing}) that combines model-checking with traditional black-box software testing to tackle the problem in a complete, sound, and automatic way.   The idea is to, with respect to some requirement (expressed in CTL or LTL) about the system, use model-checking techniques to derive a condition (expressed in communication graphs) for an unspecified component such that the system satisfies the requirement iff the condition is satisfied by the component, and which can be established by testing the component with test cases generated from the condition on-the-fly. In this paper, we present model-checking driven black-box testing algorithms to handle both CTL and LTL requirements.   We also illustrate the idea through some examples.",
        "published": "2004-04-19T15:02:15Z",
        "link": "http://arxiv.org/abs/cs/0404037v2",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.2.4;D.2.5;F.4.1"
        ]
    },
    {
        "title": "Incompleteness of States w.r.t. Traces in Model Checking",
        "authors": [
            "Roberto Giacobazzi",
            "Francesco Ranzato"
        ],
        "summary": "Cousot and Cousot introduced and studied a general past/future-time specification language, called mu*-calculus, featuring a natural time-symmetric trace-based semantics. The standard state-based semantics of the mu*-calculus is an abstract interpretation of its trace-based semantics, which turns out to be incomplete (i.e., trace-incomplete), even for finite systems. As a consequence, standard state-based model checking of the mu*-calculus is incomplete w.r.t. trace-based model checking. This paper shows that any refinement or abstraction of the domain of sets of states induces a corresponding semantics which is still trace-incomplete for any propositional fragment of the mu*-calculus. This derives from a number of results, one for each incomplete logical/temporal connective of the mu*-calculus, that characterize the structure of models, i.e. transition systems, whose corresponding state-based semantics of the mu*-calculus is trace-complete.",
        "published": "2004-04-23T08:49:18Z",
        "link": "http://arxiv.org/abs/cs/0404048v2",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1; F.3.2"
        ]
    },
    {
        "title": "A lambda calculus for quantum computation with classical control",
        "authors": [
            "Peter Selinger",
            "Benoit Valiron"
        ],
        "summary": "The objective of this paper is to develop a functional programming language for quantum computers. We develop a lambda calculus for the classical control model, following the first author's work on quantum flow-charts. We define a call-by-value operational semantics, and we give a type system using affine intuitionistic linear logic. The main results of this paper are the safety properties of the language and the development of a type inference algorithm.",
        "published": "2004-04-27T21:33:52Z",
        "link": "http://arxiv.org/abs/cs/0404056v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Splitting an operator: Algebraic modularity results for logics with   fixpoint semantics",
        "authors": [
            "Joost Vennekens",
            "David Gilis",
            "Marc Denecker"
        ],
        "summary": "It is well known that, under certain conditions, it is possible to split logic programs under stable model semantics, i.e. to divide such a program into a number of different \"levels\", such that the models of the entire program can be constructed by incrementally constructing models for each level. Similar results exist for other non-monotonic formalisms, such as auto-epistemic logic and default logic. In this work, we present a general, algebraicsplitting theory for logics with a fixpoint semantics. Together with the framework of approximation theory, a general fixpoint theory for arbitrary operators, this gives us a uniform and powerful way of deriving splitting results for each logic with a fixpoint semantics. We demonstrate the usefulness of these results, by generalizing existing results for logic programming, auto-epistemic logic and default logic.",
        "published": "2004-05-03T09:05:14Z",
        "link": "http://arxiv.org/abs/cs/0405002v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Deductive Algorithmic Knowledge",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "The framework of algorithmic knowledge assumes that agents use algorithms to compute the facts they explicitly know. In many cases of interest, a deductive system, rather than a particular algorithm, captures the formal reasoning used by the agents to compute what they explicitly know. We introduce a logic for reasoning about both implicit and explicit knowledge with the latter defined with respect to a deductive system formalizing a logical theory for agents. The highly structured nature of deductive systems leads to very natural axiomatizations of the resulting logic when interpreted over any fixed deductive system. The decision problem for the logic, in the presence of a single agent, is NP-complete in general, no harder than propositional logic. It remains NP-complete when we fix a deductive system that is decidable in nondeterministic polynomial time. These results extend in a straightforward way to multiple agents.",
        "published": "2004-05-11T18:01:39Z",
        "link": "http://arxiv.org/abs/cs/0405038v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "A Logic for Reasoning about Digital Rights",
        "authors": [
            "Riccardo Pucella",
            "Vicky Weissman"
        ],
        "summary": "We present a logic for reasoning about licenses, which are ``terms of use'' for digital resources. The logic provides a language for writing both properties of licenses and specifications that govern a client's actions. We discuss the complexity of checking properties and specifications written in our logic and propose a technique for verification. A key feature of our approach is that it is essentially parameterized by the language in which the licenses are written, provided that this language can be given a trace-based semantics. We consider two license languages to illustrate this flexibility.",
        "published": "2004-05-18T18:22:33Z",
        "link": "http://arxiv.org/abs/cs/0405066v1",
        "categories": [
            "cs.CR",
            "cs.LO",
            "D.4.6; K.6.5; F.4.1"
        ]
    },
    {
        "title": "An Analysis of Lambek's Production Machines",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "Lambek's production machines may be used to generate and recognize sentences in a subset of the language described by a production grammar. We determine in this paper the subset of the language of a grammar generated and recognized by such machines.",
        "published": "2004-05-23T19:22:10Z",
        "link": "http://arxiv.org/abs/cs/0405081v1",
        "categories": [
            "cs.LO",
            "F.1.1;F.4.2"
        ]
    },
    {
        "title": "A Coalgebraic Approach to Kleene Algebra with Tests",
        "authors": [
            "Hubie Chen",
            "Riccardo Pucella"
        ],
        "summary": "Kleene algebra with tests is an extension of Kleene algebra, the algebra of regular expressions, which can be used to reason about programs. We develop a coalgebraic theory of Kleene algebra with tests, along the lines of the coalgebraic theory of regular expressions based on deterministic automata. Since the known automata-theoretic presentation of Kleene algebra with tests does not lend itself to a coalgebraic theory, we define a new interpretation of Kleene algebra with tests expressions and a corresponding automata-theoretic presentation. One outcome of the theory is a coinductive proof principle, that can be used to establish equivalence of our Kleene algebra with tests expressions.",
        "published": "2004-05-26T16:49:44Z",
        "link": "http://arxiv.org/abs/cs/0405097v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1;I.1.1;I.1.3"
        ]
    },
    {
        "title": "A Logic for Reasoning about Evidence",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "We introduce a logic for reasoning about evidence that essentially views evidence as a function from prior beliefs (before making an observation) to posterior beliefs (after making the observation). We provide a sound and complete axiomatization for the logic, and consider the complexity of the decision problem. Although the reasoning in the logic is mainly propositional, we allow variables representing numbers and quantification over them. This expressive power seems necessary to capture important properties of evidence.",
        "published": "2004-05-26T17:08:38Z",
        "link": "http://arxiv.org/abs/cs/0405098v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "On model checking data-independent systems with arrays without reset",
        "authors": [
            "R. S. Lazic",
            "T. C. Newcomb",
            "A. W. Roscoe"
        ],
        "summary": "A system is data-independent with respect to a data type X iff the operations it can perform on values of type X are restricted to just equality testing. The system may also store, input and output values of type X. We study model checking of systems which are data-independent with respect to two distinct type variables X and Y, and may in addition use arrays with indices from X and values from Y . Our main interest is the following parameterised model-checking problem: whether a given program satisfies a given temporal-logic formula for all non-empty nite instances of X and Y . Initially, we consider instead the abstraction where X and Y are infinite and where partial functions with finite domains are used to model arrays. Using a translation to data-independent systems without arrays, we show that the u-calculus model-checking problem is decidable for these systems. From this result, we can deduce properties of all systems with finite instances of X and Y . We show that there is a procedure for the above parameterised model-checking problem of the universal fragment of the u-calculus, such that it always terminates but may give false negatives. We also deduce that the parameterised model-checking problem of the universal disjunction-free fragment of the u-calculus is decidable. Practical motivations for model checking data-independent systems with arrays include verification of memory and cache systems, where X is the type of memory addresses, and Y the type of storable values. As an example we verify a fault-tolerant memory interface over a set of unreliable memories.",
        "published": "2004-05-27T03:25:52Z",
        "link": "http://arxiv.org/abs/cs/0405103v1",
        "categories": [
            "cs.LO",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Dichotomy Theorems for Alternation-Bounded Quantified Boolean Formulas",
        "authors": [
            "Edith Hemaspaandra"
        ],
        "summary": "In 1978, Schaefer proved his famous dichotomy theorem for generalized satisfiability problems. He defined an infinite number of propositional satisfiability problems, showed that all these problems are either in P or NP-complete, and gave a simple criterion to determine which of the two cases holds. This result is surprising in light of Ladner's theorem, which implies that there are an infinite number of complexity classes between P and NP-complete (under the assumption that P is not equal to NP).   Schaefer also stated a dichotomy theorem for quantified generalized Boolean formulas, but this theorem was only recently proven by Creignou, Khanna, and Sudan, and independently by Dalmau: Determining truth of quantified Boolean formulas is either PSPACE-complete or in P.   This paper looks at alternation-bounded quantified generalized Boolean formulas. In their unrestricted forms, these problems are the canonical problems complete for the levels of the polynomial hierarchy. In this paper, we prove dichotomy theorems for alternation-bounded quantified generalized Boolean formulas, by showing that these problems are either $\\Sigma_i^p$-complete or in P, and we give a simple criterion to determine which of the two cases holds. This is the first result that obtains dichotomy for an infinite number of classes at once.",
        "published": "2004-06-02T23:17:20Z",
        "link": "http://arxiv.org/abs/cs/0406006v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Optimization of Bound Disjunctive Queries with Constraints",
        "authors": [
            "G. Greco",
            "S. Greco",
            "I. Trubtsyna",
            "E. Zumpano"
        ],
        "summary": "\"To Appear in Theory and Practice of Logic Programming (TPLP)\" This paper presents a technique for the optimization of bound queries over disjunctive deductive databases with constraints. The proposed approach is an extension of the well-known Magic-Set technique and is well-suited for being integrated in current bottom-up (stable) model inference engines. More specifically, it is based on the exploitation of binding propagation techniques which reduce the size of the data relevant to answer the query and, consequently, reduces both the complexity of computing a single model and the number of models to be considered. The motivation of this work stems from the observation that traditional binding propagation optimization techniques for bottom-up model generator systems, simulating the goal driven evaluation of top-down engines, are only suitable for positive (disjunctive) queries, while hard problems are expressed using unstratified negation. The main contribution of the paper consists in the extension of a previous technique, defined for positive disjunctive queries, to queries containing both disjunctive heads and constraints (a simple and expressive form of unstratified negation). As the usual way of expressing declaratively hard problems is based on the guess-and-check technique, where the guess part is expressed by means of disjunctive rules and the check part is expressed by means of constraints, the technique proposed here is highly relevant for the optimization of queries expressing hard problems. The value of the technique has been proved by several experiments.",
        "published": "2004-06-07T12:13:58Z",
        "link": "http://arxiv.org/abs/cs/0406013v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Abstract Canonical Inference",
        "authors": [
            "Maria Paola Bonacina",
            "Nachum Dershowitz"
        ],
        "summary": "An abstract framework of canonical inference is used to explore how different proof orderings induce different variants of saturation and completeness. Notions like completion, paramodulation, saturation, redundancy elimination, and rewrite-system reduction are connected to proof orderings. Fairness of deductive mechanisms is defined in terms of proof orderings, distinguishing between (ordinary) \"fairness,\" which yields completeness, and \"uniform fairness,\" which yields saturation.",
        "published": "2004-06-17T10:13:04Z",
        "link": "http://arxiv.org/abs/cs/0406030v2",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "Propositional Computability Logic II",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic is a formal theory of computational tasks and resources. Its formulas represent interactive computational problems, logical operators stand for operations on computational problems, and validity of a formula is understood as being a scheme of problems that always have algorithmic solutions. A comprehensive online source on the subject is available at http://www.cis.upenn.edu/~giorgi/cl.html . The earlier article \"Propositional computability logic I\" proved soundness and completeness for the (in a sense) minimal nontrivial fragment CL1 of computability logic. The present paper extends that result to the significantly more expressive propositional system CL2. What makes CL2 more expressive than CL1 is the presence of two sorts of atoms in its language: elementary atoms, representing elementary computational problems (i.e. predicates), and general atoms, representing arbitrary computational problems. CL2 conservatively extends CL1, with the latter being nothing but the general-atom-free fragment of the former.",
        "published": "2004-06-18T16:55:18Z",
        "link": "http://arxiv.org/abs/cs/0406037v2",
        "categories": [
            "cs.LO",
            "cs.GT",
            "math.LO",
            "F.4.1; F.1.2"
        ]
    },
    {
        "title": "A possible hypercomputational quantum algorithm",
        "authors": [
            "Andrés Sicard",
            "Mario Vélez",
            "Juan Ospina"
        ],
        "summary": "The term `hypermachine' denotes any data processing device (theoretical or that can be implemented) capable of carrying out tasks that cannot be performed by a Turing machine. We present a possible quantum algorithm for a classically non-computable decision problem, Hilbert's tenth problem; more specifically, we present a possible hypercomputation model based on quantum computation. Our algorithm is inspired by the one proposed by Tien D. Kieu, but we have selected the infinite square well instead of the (one-dimensional) simple harmonic oscillator as the underlying physical system. Our model exploits the quantum adiabatic process and the characteristics of the representation of the dynamical Lie algebra su(1,1) associated to the infinite square well.",
        "published": "2004-06-18T22:06:19Z",
        "link": "http://arxiv.org/abs/quant-ph/0406137v2",
        "categories": [
            "quant-ph",
            "cs.LO"
        ]
    },
    {
        "title": "Alchemistry of the P versus NP question",
        "authors": [
            "Bonifac Donat"
        ],
        "summary": "Are P and NP provably inseparable ? Take a look at some unorthodox, guilty mentioned folklore and related unpublished results.",
        "published": "2004-06-21T20:17:28Z",
        "link": "http://arxiv.org/abs/cs/0406040v2",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Non-Termination Inference of Logic Programs",
        "authors": [
            "Etienne Payet",
            "Fred Mesnard"
        ],
        "summary": "We present a static analysis technique for non-termination inference of logic programs. Our framework relies on an extension of the subsumption test, where some specific argument positions can be instantiated while others are generalized. We give syntactic criteria to statically identify such argument positions from the text of a program. Atomic left looping queries are generated bottom-up from selected subsets of the binary unfoldings of the program of interest. We propose a set of correct algorithms for automating the approach. Then, non-termination inference is tailored to attempt proofs of optimality of left termination conditions computed by a termination inference tool. An experimental evaluation is reported. When termination and non-termination analysis produce complementary results for a logic procedure, then with respect to the leftmost selection rule and the language used to describe sets of atomic queries, each analysis is optimal and together, they induce a characterization of the operational behavior of the logic procedure.",
        "published": "2004-06-22T11:44:07Z",
        "link": "http://arxiv.org/abs/cs/0406041v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Predicate Abstraction with Indexed Predicates",
        "authors": [
            "Shuvendu K. Lahiri",
            "Randal E. Bryant"
        ],
        "summary": "Predicate abstraction provides a powerful tool for verifying properties of infinite-state systems using a combination of a decision procedure for a subset of first-order logic and symbolic methods originally developed for finite-state model checking. We consider models containing first-order state variables, where the system state includes mutable functions and predicates. Such a model can describe systems containing arbitrarily large memories, buffers, and arrays of identical processes. We describe a form of predicate abstraction that constructs a formula over a set of universally quantified variables to describe invariant properties of the first-order state variables. We provide a formal justification of the soundness of our approach and describe how it has been used to verify several hardware and software designs, including a directory-based cache coherence protocol.",
        "published": "2004-07-02T06:17:13Z",
        "link": "http://arxiv.org/abs/cs/0407006v1",
        "categories": [
            "cs.LO",
            "F.3.1"
        ]
    },
    {
        "title": "The semijoin algebra and the guarded fragment",
        "authors": [
            "Dirk Leinders",
            "Jerzy Tyszkiewicz",
            "Jan Van den Bussche"
        ],
        "summary": "The semijoin algebra is the variant of the relational algebra obtained by replacing the join operator by the semijoin operator. We discuss some interesting connections between the semijoin algebra and the guarded fragment of first-order logic. We also provide an Ehrenfeucht-Fraisse game, characterizing the discerning power of the semijoin algebra. This game gives a method for showing that certain queries are not expressible in the semijoin algebra.",
        "published": "2004-07-02T15:44:32Z",
        "link": "http://arxiv.org/abs/cs/0407007v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3;F.4.1"
        ]
    },
    {
        "title": "On Modal Logics of Partial Recursive Functions",
        "authors": [
            "Pavel Naumov"
        ],
        "summary": "The classical propositional logic is known to be sound and complete with respect to the set semantics that interprets connectives as set operations. The paper extends propositional language by a new binary modality that corresponds to partial recursive function type constructor under the above interpretation. The cases of deterministic and non-deterministic functions are considered and for both of them semantically complete modal logics are described and decidability of these logics is established.",
        "published": "2004-07-12T22:53:33Z",
        "link": "http://arxiv.org/abs/cs/0407031v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "The First-Order Theory of Sets with Cardinality Constraints is Decidable",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We show that the decidability of the first-order theory of the language that combines Boolean algebras of sets of uninterpreted elements with Presburger arithmetic operations. We thereby disprove a recent conjecture that this theory is undecidable. Our language allows relating the cardinalities of sets to the values of integer variables, and can distinguish finite and infinite sets. We use quantifier elimination to show the decidability and obtain an elementary upper bound on the complexity.   Precise program analyses can use our decidability result to verify representation invariants of data structures that use an integer field to represent the number of stored elements.",
        "published": "2004-07-17T04:22:39Z",
        "link": "http://arxiv.org/abs/cs/0407045v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Preferred Answer Sets for Ordered Logic Programs",
        "authors": [
            "Davy Van Nieuwenborgh",
            "Dirk Vermeir"
        ],
        "summary": "We extend answer set semantics to deal with inconsistent programs (containing classical negation), by finding a ``best'' answer set. Within the context of inconsistent programs, it is natural to have a partial order on rules, representing a preference for satisfying certain rules, possibly at the cost of violating less important ones. We show that such a rule order induces a natural order on extended answer sets, the minimal elements of which we call preferred answer sets. We characterize the expressiveness of the resulting semantics and show that it can simulate negation as failure, disjunction and some other formalisms such as logic programs with ordered disjunction. The approach is shown to be useful in several application areas, e.g. repairing database, where minimal repairs correspond to preferred answer sets.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2004-07-19T22:37:43Z",
        "link": "http://arxiv.org/abs/cs/0407049v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "From truth to computability I",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "The recently initiated approach called computability logic is a formal theory of interactive computation. See a comprehensive online source on the subject at http://www.cis.upenn.edu/~giorgi/cl.html . The present paper contains a soundness and completeness proof for the deductive system CL3 which axiomatizes the most basic first-order fragment of computability logic called the finite-depth, elementary-base fragment. Among the potential application areas for this result are the theory of interactive computation, constructive applied theories, knowledgebase systems, systems for resource-bound planning and action. This paper is self-contained as it reintroduces all relevant definitions as well as main motivations.",
        "published": "2004-07-21T03:58:22Z",
        "link": "http://arxiv.org/abs/cs/0407054v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.GT",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "PELCR: Parallel Environment for Optimal Lambda-Calculus Reduction",
        "authors": [
            "M. Pedicini",
            "F. Quaglia"
        ],
        "summary": "In this article we present the implementation of an environment supporting L\\'evy's \\emph{optimal reduction} for the $\\lambda$-calculus \\cite{Lev78} on parallel (or distributed) computing systems. In a similar approach to Lamping's one in \\cite{Lamping90}, we base our work on a graph reduction technique known as \\emph{directed virtual reduction} \\cite{DPR97} which is actually a restriction of Danos-Regnier virtual reduction \\cite{DanosRegnier93}.   The environment, which we refer to as PELCR (Parallel Environment for optimal Lambda-Calculus Reduction) relies on a strategy for directed virtual reduction, namely {\\em half combustion}, which we introduce in this article. While developing PELCR we have adopted both a message aggregation technique, allowing a reduction of the communication overhead, and a fair policy for distributing dynamically originated load among processors.   We also present an experimental study demonstrating the ability of PELCR to definitely exploit parallelism intrinsic to $\\lambda$-terms while performing the reduction. By the results we show how PELCR allows achieving up to 70/80% of the ideal speedup on last generation multiprocessor computing systems. As a last note, the software modules have been developed with the {\\tt C} language and using a standard interface for message passing, i.e. MPI, thus making PELCR itself a highly portable software package.",
        "published": "2004-07-22T17:52:39Z",
        "link": "http://arxiv.org/abs/cs/0407055v2",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.4.1"
        ]
    },
    {
        "title": "A Sequent Calculus and a Theorem Prover for Standard Conditional Logics",
        "authors": [
            "Nicola Olivetti",
            "Gian Luca Pozzato",
            "Camilla Schwind"
        ],
        "summary": "In this paper we present a cut-free sequent calculus, called SeqS, for some standard conditional logics, namely CK, CK+ID, CK+MP and CK+MP+ID. The calculus uses labels and transition formulas and can be used to prove decidability and space complexity bounds for the respective logics. We also present CondLean, a theorem prover for these logics implementing SeqS calculi written in SICStus Prolog.",
        "published": "2004-07-29T14:17:19Z",
        "link": "http://arxiv.org/abs/cs/0407064v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Typestate Checking and Regular Graph Constraints",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We introduce regular graph constraints and explore their decidability properties. The motivation for regular graph constraints is 1) type checking of changing types of objects in the presence of linked data structures, 2) shape analysis techniques, and 3) generalization of similar constraints over trees and grids. We define a subclass of graphs called heaps as an abstraction of the data structures that a program constructs during its execution. We prove that determining the validity of implication for regular graph constraints over the class of heaps is undecidable. We show undecidability by exhibiting a characterization of certain \"corresponder graphs\" in terms of presence and absence of homomorphisms to a finite number of fixed graphs. The undecidability of implication of regular graph constraints implies that there is no algorithm that will verify that procedure preconditions are met or that the invariants are maintained when these properties are expressed in any specification language at least as expressive as regular graph constraints.",
        "published": "2004-08-05T03:41:24Z",
        "link": "http://arxiv.org/abs/cs/0408014v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On the Theory of Structural Subtyping",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We show that the first-order theory of structural subtyping of non-recursive types is decidable. Let $\\Sigma$ be a language consisting of function symbols (representing type constructors) and $C$ a decidable structure in the relational language $L$ containing a binary relation $\\leq$. $C$ represents primitive types; $\\leq$ represents a subtype ordering. We introduce the notion of $\\Sigma$-term-power of $C$, which generalizes the structure arising in structural subtyping. The domain of the $\\Sigma$-term-power of $C$ is the set of $\\Sigma$-terms over the set of elements of $C$. We show that the decidability of the first-order theory of $C$ implies the decidability of the first-order theory of the $\\Sigma$-term-power of $C$. Our decision procedure makes use of quantifier elimination for term algebras and Feferman-Vaught theorem. Our result implies the decidability of the first-order theory of structural subtyping of non-recursive types.",
        "published": "2004-08-05T04:12:25Z",
        "link": "http://arxiv.org/abs/cs/0408015v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On Role Logic",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We present role logic, a notation for describing properties of relational structures in shape analysis, databases, and knowledge bases. We construct role logic using the ideas of de Bruijn's notation for lambda calculus, an encoding of first-order logic in lambda calculus, and a simple rule for implicit arguments of unary and binary predicates. The unrestricted version of role logic has the expressive power of first-order logic with transitive closure. Using a syntactic restriction on role logic formulas, we identify a natural fragment RL^2 of role logic. We show that the RL^2 fragment has the same expressive power as two-variable logic with counting C^2 and is therefore decidable. We present a translation of an imperative language into the decidable fragment RL^2, which allows compositional verification of programs that manipulate relational structures. In addition, we show how RL^2 encodes boolean shape analysis constraints and an expressive description logic.",
        "published": "2004-08-05T23:01:20Z",
        "link": "http://arxiv.org/abs/cs/0408018v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On Generalized Records and Spatial Conjunction in Role Logic",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We have previously introduced role logic as a notation for describing properties of relational structures in shape analysis, databases and knowledge bases. A natural fragment of role logic corresponds to two-variable logic with counting and is therefore decidable. We show how to use role logic to describe open and closed records, as well the dual of records, inverse records. We observe that the spatial conjunction operation of separation logic naturally models record concatenation. Moreover, we show how to eliminate the spatial conjunction of formulas of quantifier depth one in first-order logic with counting. As a result, allowing spatial conjunction of formulas of quantifier depth one preserves the decidability of two-variable logic with counting. This result applies to two-variable role logic fragment as well. The resulting logic smoothly integrates type system and predicate calculus notation and can be viewed as a natural generalization of the notation for constraints arising in role analysis and similar shape analysis approaches.",
        "published": "2004-08-05T23:25:20Z",
        "link": "http://arxiv.org/abs/cs/0408019v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Logical Interpretation of a Reversible Measurement in Quantum Computing",
        "authors": [
            "Giulia Battilotti",
            "Paola Zizzi"
        ],
        "summary": "We give the logical description of a new kind of quantum measurement that is a reversible operation performed by an hypothetical insider observer, or, which is the same, a quantum measurement made in a quantum space background, like the fuzzy sphere. The result is that the non-contradiction and the excluded middle principles are both invalidated, leading to a paraconsistent, symmetric logic. Our conjecture is that, in this setting, one can develop the adequate logic of quantum computing. The role of standard quantum logic is then confined to describe the projective measurement scheme.",
        "published": "2004-08-11T17:27:42Z",
        "link": "http://arxiv.org/abs/quant-ph/0408068v2",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math-ph",
            "math.LO",
            "math.MP"
        ]
    },
    {
        "title": "Multi-dimensional Type Theory: Rules, Categories, and Combinators for   Syntax and Semantics",
        "authors": [
            "Jørgen Villadsen"
        ],
        "summary": "We investigate the possibility of modelling the syntax and semantics of natural language by constraints, or rules, imposed by the multi-dimensional type theory Nabla. The only multiplicity we explicitly consider is two, namely one dimension for the syntax and one dimension for the semantics, but the general perspective is important. For example, issues of pragmatics could be handled as additional dimensions.   One of the main problems addressed is the rather complicated repertoire of operations that exists besides the notion of categories in traditional Montague grammar. For the syntax we use a categorial grammar along the lines of Lambek. For the semantics we use so-called lexical and logical combinators inspired by work in natural logic. Nabla provides a concise interpretation and a sequent calculus as the basis for implementations.",
        "published": "2004-08-15T08:51:19Z",
        "link": "http://arxiv.org/abs/cs/0408037v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "I.2.7"
        ]
    },
    {
        "title": "The Arithmetical Complexity of Dimension and Randomness",
        "authors": [
            "John M. Hitchcock",
            "Jack H. Lutz",
            "Sebastiaan A. Terwijn"
        ],
        "summary": "Constructive dimension and constructive strong dimension are effectivizations of the Hausdorff and packing dimensions, respectively. Each infinite binary sequence A is assigned a dimension dim(A) in [0,1] and a strong dimension Dim(A) in [0,1].   Let DIM^alpha and DIMstr^alpha be the classes of all sequences of dimension alpha and of strong dimension alpha, respectively. We show that DIM^0 is properly Pi^0_2, and that for all Delta^0_2-computable alpha in (0,1], DIM^alpha is properly Pi^0_3.   To classify the strong dimension classes, we use a more powerful effective Borel hierarchy where a co-enumerable predicate is used rather than a enumerable predicate in the definition of the Sigma^0_1 level. For all Delta^0_2-computable alpha in [0,1), we show that DIMstr^alpha is properly in the Pi^0_3 level of this hierarchy. We show that DIMstr^1 is properly in the Pi^0_2 level of this hierarchy.   We also prove that the class of Schnorr random sequences and the class of computably random sequences are properly Pi^0_3.",
        "published": "2004-08-18T16:23:27Z",
        "link": "http://arxiv.org/abs/cs/0408043v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "On computing the fixpoint of a set of boolean equations",
        "authors": [
            "Viktor Kuncak",
            "K. Rustan M. Leino"
        ],
        "summary": "This paper presents a method for computing a least fixpoint of a system of equations over booleans. The resulting computation can be significantly shorter than the result of iteratively evaluating the entire system until a fixpoint is reached.",
        "published": "2004-08-19T17:24:49Z",
        "link": "http://arxiv.org/abs/cs/0408045v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.4; D.3.1; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "A CHR-based Implementation of Known Arc-Consistency",
        "authors": [
            "Marco Alberti",
            "Marco Gavanelli",
            "Evelina Lamma",
            "Paola Mello",
            "Michela Milano"
        ],
        "summary": "In classical CLP(FD) systems, domains of variables are completely known at the beginning of the constraint propagation process. However, in systems interacting with an external environment, acquiring the whole domains of variables before the beginning of constraint propagation may cause waste of computation time, or even obsolescence of the acquired data at the time of use.   For such cases, the Interactive Constraint Satisfaction Problem (ICSP) model has been proposed as an extension of the CSP model, to make it possible to start constraint propagation even when domains are not fully known, performing acquisition of domain elements only when necessary, and without the need for restarting the propagation after every acquisition.   In this paper, we show how a solver for the two sorted CLP language, defined in previous work, to express ICSPs, has been implemented in the Constraint Handling Rules (CHR) language, a declarative language particularly suitable for high level implementation of constraint solvers.",
        "published": "2004-08-24T10:15:13Z",
        "link": "http://arxiv.org/abs/cs/0408056v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "The Integration of Connectionism and First-Order Knowledge   Representation and Reasoning as a Challenge for Artificial Intelligence",
        "authors": [
            "Sebastian Bader",
            "Pascal Hitzler",
            "Steffen Hoelldobler"
        ],
        "summary": "Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.",
        "published": "2004-08-31T16:16:28Z",
        "link": "http://arxiv.org/abs/cs/0408069v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.NE",
            "I.2.3,I.2.6"
        ]
    },
    {
        "title": "Default reasoning over domains and concept hierarchies",
        "authors": [
            "Pascal Hitzler"
        ],
        "summary": "W.C. Rounds and G.-Q. Zhang (2001) have proposed to study a form of disjunctive logic programming generalized to algebraic domains. This system allows reasoning with information which is hierarchically structured and forms a (suitable) domain. We extend this framework to include reasoning with default negation, giving rise to a new nonmonotonic reasoning framework on hierarchical knowledge which encompasses answer set programming with extended disjunctive logic programs. We also show that the hierarchically structured knowledge on which programming in this paradigm can be done, arises very naturally from formal concept analysis. Together, we obtain a default reasoning paradigm for conceptual knowledge which is in accordance with mainstream developments in nonmonotonic reasoning.",
        "published": "2004-09-01T19:22:01Z",
        "link": "http://arxiv.org/abs/cs/0409002v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4; D.1.6"
        ]
    },
    {
        "title": "The strength of replacement in weak arithmetic",
        "authors": [
            "Stephen Cook",
            "Neil Thapen"
        ],
        "summary": "The replacement (or collection or choice) axiom scheme asserts bounded quantifier exchange. We prove the independence of this scheme from various weak theories of arithmetic, sometimes under a complexity assumption.",
        "published": "2004-09-08T16:18:37Z",
        "link": "http://arxiv.org/abs/cs/0409015v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Outlier Detection by Logic Programming",
        "authors": [
            "Fabrizio Angiulli",
            "Gianluigi Greco",
            "Luigi Palopoli"
        ],
        "summary": "The development of effective knowledge discovery techniques has become in the recent few years a very active research area due to the important impact it has in several relevant application areas. One interesting task thereof is that of singling out anomalous individuals from a given population, e.g., to detect rare events in time-series analysis settings, or to identify objects whose behavior is deviant w.r.t. a codified standard set of \"social\" rules. Such exceptional individuals are usually referred to as outliers in the literature.   Recently, outlier detection has also emerged as a relevant KR&R problem. In this paper, we formally state the concept of outliers by generalizing in several respects an approach recently proposed in the context of default logic, for instance, by having outliers not being restricted to single individuals but, rather, in the more general case, to correspond to entire (sub)theories. We do that within the context of logic programming and, mainly through examples, we discuss its potential practical impact in applications. The formalization we propose is a novel one and helps in shedding some light on the real nature of outliers. Moreover, as a major contribution of this work, we illustrate the exploitation of minimality criteria in outlier detection. The computational complexity of outlier detection problems arising in this novel setting is thoroughly investigated and accounted for in the paper as well. Finally, we also propose a rewriting algorithm that transforms any outlier detection problem into an equivalent inference problem under the stable model semantics, thereby making outlier computation effective and realizable on top of any stable model solver.",
        "published": "2004-09-09T14:20:02Z",
        "link": "http://arxiv.org/abs/cs/0409019v2",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Automatic Generation of CHR Constraint Solvers",
        "authors": [
            "Slim Abdennadher",
            "Christophe Rigotti"
        ],
        "summary": "In this paper, we present a framework for automatic generation of CHR solvers given the logical specification of the constraints. This approach takes advantage of the power of tabled resolution for constraint logic programming, in order to check the validity of the rules. Compared to previous works where different methods for automatic generation of constraint solvers have been proposed, our approach enables the generation of more expressive rules (even recursive and splitting rules) that can be used directly as CHR solvers.",
        "published": "2004-09-14T20:55:23Z",
        "link": "http://arxiv.org/abs/cs/0409030v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2; I.2.2"
        ]
    },
    {
        "title": "Augmenting ALC(D) (atemporal) roles and (aspatial) concrete domain with   temporal roles and a spatial concrete domain -first results",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We consider the well-known family ALC(D) of description logics with a concrete domain, and provide first results on a framework obtained by augmenting ALC(D) atemporal roles and aspatial concrete domain with temporal roles and a spatial concrete domain.",
        "published": "2004-09-24T14:56:23Z",
        "link": "http://arxiv.org/abs/cs/0409045v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.3; I.2.4; I.2.m)"
        ]
    },
    {
        "title": "A TCSP-like decidable constraint language generalising existing cardinal   direction relations",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We define a quantitative constraint language subsuming two calculi well-known in QSR (Qualitative Spatial Reasoning): Frank's cone-shaped and projection-based calculi of cardinal direction relations. We show how to solve a CSP (Constraint Satisfaction Problem) expressed in the language.",
        "published": "2004-09-24T15:12:58Z",
        "link": "http://arxiv.org/abs/cs/0409046v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.3; I.2.4; I.2.m)"
        ]
    },
    {
        "title": "An ALC(D)-based combination of temporal constraints and spatial   constraints suitable for continuous (spatial) change",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We present a family of spatio-temporal theories suitable for continuous spatial change in general, and for continuous motion of spatial scenes in particular. The family is obtained by spatio-temporalising the well-known ALC(D) family of Description Logics (DLs) with a concrete domain D, as follows, where TCSPs denotes \"Temporal Constraint Satisfaction Problems\", a well-known constraint-based framework:   (1) temporalisation of the roles, so that they consist of TCSP constraints (specifically, of an adaptation of TCSP constraints to interval variables); and   (2) spatialisation of the concrete domain D: the concrete domain is now $D_x$, and is generated by a spatial Relation Algebra (RA) $x$, in the style of the Region-Connection Calculus RCC8.   We assume durative truth (i.e., holding during a durative interval). We also assume the homogeneity property (if a truth holds during a given interval, it holds during all of its subintervals). Among other things, these assumptions raise the \"conflicting\" problem of overlapping truths, which the work solves with the use of a specific partition of the 13 atomic relations of Allen's interval algebra.",
        "published": "2004-09-24T15:22:52Z",
        "link": "http://arxiv.org/abs/cs/0409047v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.3; I.2.4; I.2.m)"
        ]
    },
    {
        "title": "Better Quasi-Ordered Transition Systems",
        "authors": [
            "Parosh Aziz Abdulla",
            "Aletta Nylen"
        ],
        "summary": "Many existing algorithms for model checking of infinite-state systems operate on constraints which are used to represent (potentially infinite) sets of states. A general powerful technique which can be employed for proving termination of these algorithms is that of well quasi-orderings. Several methodologies have been proposed for derivation of new well quasi-ordered constraint systems. However, many of these constraint systems suffer from a \"constraint explosion problem\", as the number of the generated constraints grows exponentially with the size of the problem. In this paper, we demonstrate that a refinement of the theory of well quasi-orderings, called the theory of better quasi-orderings, is more appropriate for symbolic model checking, since it allows inventing constraint systems which are both well quasi-ordered and compact. As a main application, we introduce existential zones, a constraint system for verification of systems with unboundedly many clocks and use our methodology to prove that existential zones are better quasi-ordered. We show how to use existential zones in verification of timed Petri nets and present some experimental results. Also, we apply our methodology to derive new constraint systems for verification of broadcast protocols, lossy channel systems, and integral relational automata. The new constraint systems are exponentially more succinct than existing ones, and their well quasi-ordering cannot be shown by previous methods in the literature.",
        "published": "2004-09-26T21:54:12Z",
        "link": "http://arxiv.org/abs/cs/0409052v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Nondeterministic Linear Logic",
        "authors": [
            "Satoshi Matsuoka"
        ],
        "summary": "In this paper, we introduce Linear Logic with a nondeterministic facility, which has a self-dual additive connective. In the system the proof net technology is available in a natural way. The important point is that nondeterminism in the system is expressed by the process of normalization, not by proof search. Moreover we can incorporate the system into Light Linear Logic and Elementary Linear Logic developed by J.-Y.Girard recently: Nondeterministic Light Linear Logic and Nondeterministic Elementary Linear Logic are defined in a very natural way.",
        "published": "2004-10-14T05:28:06Z",
        "link": "http://arxiv.org/abs/cs/0410029v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Weak Typed Boehm Theorem on IMLL",
        "authors": [
            "Satoshi Matsuoka"
        ],
        "summary": "In the Boehm theorem workshop on Crete island, Zoran Petric called Statman's ``Typical Ambiguity theorem'' typed Boehm theorem. Moreover, he gave a new proof of the theorem based on set-theoretical models of the simply typed lambda calculus. In this paper, we study the linear version of the typed Boehm theorem on a fragment of Intuitionistic Linear Logic. We show that in the multiplicative fragment of intuitionistic linear logic without the multiplicative unit 1 (for short IMLL) weak typed Boehm theorem holds. The system IMLL exactly corresponds to the linear lambda calculus without exponentials, additives and logical constants. The system IMLL also exactly corresponds to the free symmetric monoidal closed category without the unit object. As far as we know, our separation result is the first one with regard to these systems in a purely syntactical manner.",
        "published": "2004-10-14T07:20:40Z",
        "link": "http://arxiv.org/abs/cs/0410030v12",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "P-time Completeness of Light Linear Logic and its Nondeterministic   Extension",
        "authors": [
            "Satoshi Matsuoka"
        ],
        "summary": "In CSL'99 Roversi pointed out that the Turing machine encoding of Girard's seminal paper \"Light Linear Logic\" has a flaw. Moreover he presented a working version of the encoding in Light Affine Logic, but not in Light Linear Logic. In this paper we present a working version of the encoding in Light Linear Logic. The idea of the encoding is based on a remark of Girard's tutorial paper on Linear Logic. The encoding is also an example which shows usefulness of additive connectives. Moreover we also consider a nondeterministic extension of Light Linear Logic. We show that the extended system is NP-complete in the same meaning as P-completeness of Light Linear Logic.",
        "published": "2004-10-15T13:43:51Z",
        "link": "http://arxiv.org/abs/cs/0410034v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Turing Machine with Faults, Failures and Recovery",
        "authors": [
            "Alex Vinokur"
        ],
        "summary": "A Turing machine with faults, failures and recovery (TMF) is described. TMF is (weakly) non-deterministic Turing machine consisting of five semi-infinite tapes (Master Tape, Synchro Tape, Backup Tape, Backup Synchro Tape, User Tape) and four controlling components (Program, Daemon, Apparatus, User). Computational process consists of three phases (Program Phase, Failure Phase, Repair Phase). C++ Simulator of a Turing machine with faults, failures and recovery has been developed.",
        "published": "2004-10-20T12:43:35Z",
        "link": "http://arxiv.org/abs/cs/0410051v1",
        "categories": [
            "cs.LO",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Interval Neutrosophic Logics: Theory and Applications",
        "authors": [
            "Haibin Wang",
            "Florentin Smarandache",
            "Yanqing Zhang",
            "Rajshekhar Sunderraman"
        ],
        "summary": "In this paper, we present the interval neutrosophic logics which generalizes the fuzzy logic, paraconsistent logic, intuitionistic fuzzy logic and many other non-classical and non-standard logics. We will give the formal definition of interval neutrosophic propositional calculus and interval neutrosophic predicate calculus. Then we give one application of interval neutrosophic logics to do approximate reasoning.",
        "published": "2004-10-21T20:24:28Z",
        "link": "http://arxiv.org/abs/cs/0410056v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A Categorical View on Algebraic Lattices in Formal Concept Analysis",
        "authors": [
            "Pascal Hitzler",
            "Markus Krötzsch",
            "Guo-Qiang Zhang"
        ],
        "summary": "Formal concept analysis has grown from a new branch of the mathematical field of lattice theory to a widely recognized tool in Computer Science and elsewhere. In order to fully benefit from this theory, we believe that it can be enriched with notions such as approximation by computation or representability. The latter are commonly studied in denotational semantics and domain theory and captured most prominently by the notion of algebraicity, e.g. of lattices. In this paper, we explore the notion of algebraicity in formal concept analysis from a category-theoretical perspective. To this end, we build on the the notion of approximable concept with a suitable category and show that the latter is equivalent to the category of algebraic lattices. At the same time, the paper provides a relatively comprehensive account of the representation theory of algebraic lattices in the framework of Stone duality, relating well-known structures such as Scott information systems with further formalisms from logic, topology, domains and lattice theory.",
        "published": "2004-10-25T15:01:33Z",
        "link": "http://arxiv.org/abs/cs/0410065v1",
        "categories": [
            "cs.LO",
            "F.3.2;I.2.4"
        ]
    },
    {
        "title": "Temporal logic with predicate abstraction",
        "authors": [
            "Alexei Lisitsa",
            "Igor Potapov"
        ],
        "summary": "A predicate linear temporal logic LTL_{\\lambda,=} without quantifiers but with predicate abstraction mechanism and equality is considered. The models of LTL_{\\lambda,=} can be naturally seen as the systems of pebbles (flexible constants) moving over the elements of some (possibly infinite) domain. This allows to use LTL_{\\lambda,=} for the specification of dynamic systems using some resources, such as processes using memory locations, mobile agents occupying some sites, etc. On the other hand we show that LTL_{\\lambda,=} is not recursively axiomatizable and, therefore, fully automated verification of LTL_{\\lambda,=} specifications is not, in general, possible.",
        "published": "2004-10-27T17:21:52Z",
        "link": "http://arxiv.org/abs/cs/0410072v1",
        "categories": [
            "cs.LO",
            "cs.CL",
            "F.1.1; F.3.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "On Spatial Conjunction as Second-Order Logic",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "Spatial conjunction is a powerful construct for reasoning about dynamically allocated data structures, as well as concurrent, distributed and mobile computation. While researchers have identified many uses of spatial conjunction, its precise expressive power compared to traditional logical constructs was not previously known. In this paper we establish the expressive power of spatial conjunction. We construct an embedding from first-order logic with spatial conjunction into second-order logic, and more surprisingly, an embedding from full second order logic into first-order logic with spatial conjunction. These embeddings show that the satisfiability of formulas in first-order logic with spatial conjunction is equivalent to the satisfiability of formulas in second-order logic. These results explain the great expressive power of spatial conjunction and can be used to show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidable logic. As one example, we show that adding unrestricted spatial conjunction to two-variable logic leads to undecidability. On the side of decidability, the embedding into second-order logic immediately implies the decidability of first-order logic with a form of spatial conjunction over trees. The embedding into spatial conjunction also has useful consequences: because a restricted form of spatial conjunction in two-variable logic preserves decidability, we obtain that a correspondingly restricted form of second-order quantification in two-variable logic is decidable. The resulting language generalizes the first-order theory of boolean algebra over sets and is useful in reasoning about the contents of data structures in object-oriented languages.",
        "published": "2004-10-28T15:00:42Z",
        "link": "http://arxiv.org/abs/cs/0410073v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Intuitionistic computability logic",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic (CL) is a systematic formal theory of computational tasks and resources, which, in a sense, can be seen as a semantics-based alternative to (the syntactically introduced) linear logic. With its expressive and flexible language, where formulas represent computational problems and \"truth\" is understood as algorithmic solvability, CL potentially offers a comprehensive logical basis for constructive applied theories and computing systems inherently requiring constructive and computationally meaningful underlying logics.   Among the best known constructivistic logics is Heyting's intuitionistic calculus INT, whose language can be seen as a special fragment of that of CL. The constructivistic philosophy of INT, however, has never really found an intuitively convincing and mathematically strict semantical justification. CL has good claims to provide such a justification and hence a materialization of Kolmogorov's known thesis \"INT = logic of problems\". The present paper contains a soundness proof for INT with respect to the CL semantics. A comprehensive online source on CL is available at http://www.cis.upenn.edu/~giorgi/cl.html",
        "published": "2004-11-04T18:15:19Z",
        "link": "http://arxiv.org/abs/cs/0411008v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.4.1; F.1.2"
        ]
    },
    {
        "title": "Modules and Logic Programming",
        "authors": [
            "Christophe Fouquere",
            "Virgile Mogbil"
        ],
        "summary": "We study conditions for a concurrent construction of proof-nets in the framework developed by Andreoli in recent papers. We define specific correctness criteria for that purpose. We first study closed modules (i.e. validity of the execution of a logic program), then extend the criterion to open modules (i.e. validity during the execution) distinguishing criteria for acyclicity and connectability in order to allow incremental verification.",
        "published": "2004-11-10T14:01:34Z",
        "link": "http://arxiv.org/abs/cs/0411029v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Complexity of the Two-Variable Fragment with (Binary-Coded) Counting   Quantifiers",
        "authors": [
            "Ian Pratt-Hartmann"
        ],
        "summary": "We show that the satisfiability and finite satisfiability problems for the two-variable fragment of first-order logic with counting quantifiers are both in NEXPTIME, even when counting quantifiers are coded succinctly.",
        "published": "2004-11-10T18:53:54Z",
        "link": "http://arxiv.org/abs/cs/0411031v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Logic Column 10: Specifying Confidentiality",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "This article illustrates the use of a logical specification language to capture various forms of confidentiality properties used in the security literature.",
        "published": "2004-11-11T19:09:59Z",
        "link": "http://arxiv.org/abs/cs/0411032v1",
        "categories": [
            "cs.LO",
            "F,4.1; D.4.6"
        ]
    },
    {
        "title": "On the existence of truly autonomic computing systems and the link with   quantum computing",
        "authors": [
            "Radhakrishnan Srinivasan",
            "H. P. Raghunandan"
        ],
        "summary": "A theoretical model of truly autonomic computing systems (ACS), with infinitely many constraints, is proposed. An argument similar to Turing's for the unsolvability of the halting problem, which is permitted in classical logic, shows that such systems cannot exist. Turing's argument fails in the recently proposed non-Aristotelian finitary logic (NAFL), which permits the existence of ACS. NAFL also justifies quantum superposition and entanglement, which are essential ingredients of quantum algorithms, and resolves the Einstein-Podolsky-Rosen (EPR) paradox in favour of quantum mechanics and non-locality. NAFL requires that the autonomic manager (AM) must be conceptually and architecturally distinct from the managed element, in order for the ACS to exist as a non-self-referential entity. Such a scenario is possible if the AM uses quantum algorithms and is protected from all problems by (unbreakable) quantum encryption, while the managed element remains classical. NAFL supports such a link between autonomic and quantum computing, with the AM existing as a metamathematical entity. NAFL also allows quantum algorithms to access truly random elements and thereby supports non-standard models of quantum (hyper-) computation that permit infinite parallelism.",
        "published": "2004-11-25T13:16:55Z",
        "link": "http://arxiv.org/abs/cs/0411094v2",
        "categories": [
            "cs.LO",
            "math.LO",
            "quant-ph",
            "F.4.1; F.1.1"
        ]
    },
    {
        "title": "Deterministic Bayesian Logic",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "In this paper a conditional logic is defined and studied. This conditional logic, Deterministic Bayesian Logic, is constructed as a deterministic counterpart to the (probabilistic) Bayesian conditional. The logic is unrestricted, so that any logical operations are allowed. This logic is shown to be non-trivial and is not reduced to classical propositions. The Bayesian conditional of DBL implies a definition of logical independence. Interesting results are derived about the interactions between the logical independence and the proofs. A model is constructed for the logic. Completeness results are proved. It is shown that any unconditioned probability can be extended to the whole logic DBL. The Bayesian conditional is then recovered from the probabilistic DBL. At last, it is shown why DBL is compliant with Lewis triviality.",
        "published": "2004-11-29T07:06:56Z",
        "link": "http://arxiv.org/abs/cs/0411097v2",
        "categories": [
            "cs.LO",
            "math.LO",
            "math.PR"
        ]
    },
    {
        "title": "A Decidable Probability Logic for Timed Probabilistic Systems",
        "authors": [
            "Ruggero Lanotte",
            "Daniele Beauquier"
        ],
        "summary": "In this paper we extend the predicate logic introduced in [Beauquier et al. 2002] in order to deal with Semi-Markov Processes. We prove that with respect to qualitative probabilistic properties, model checking is decidable for this logic applied to Semi-Markov Processes. Furthermore we apply our logic to Probabilistic Timed Automata considering classical and urgent semantics, and considering also predicates on clock. We prove that results on Semi Markov Processes hold also for Probabilistic Timed Automata for both the two semantics considered. Moreover, we prove that results for Markov Processes shown in [Beauquier et al. 2002] are extensible to Probabilistic Timed Automata where urgent semantics is considered.",
        "published": "2004-11-30T10:42:35Z",
        "link": "http://arxiv.org/abs/cs/0411100v2",
        "categories": [
            "cs.LO",
            "C.3; D.2.4; F.3.1; F.4.1; G.3"
        ]
    },
    {
        "title": "Finite Domain Bounds Consistency Revisited",
        "authors": [
            "Chiu Wo Choi",
            "Warwick Harvey",
            "Jimmy Ho-Man Lee",
            "Peter J. Stuckey"
        ],
        "summary": "A widely adopted approach to solving constraint satisfaction problems combines systematic tree search with constraint propagation for pruning the search space. Constraint propagation is performed by propagators implementing a certain notion of consistency. Bounds consistency is the method of choice for building propagators for arithmetic constraints and several global constraints in the finite integer domain. However, there has been some confusion in the definition of bounds consistency. In this paper we clarify the differences and similarities among the three commonly used notions of bounds consistency.",
        "published": "2004-12-06T08:04:03Z",
        "link": "http://arxiv.org/abs/cs/0412021v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Removing Propagation Redundant Constraints in Redundant Modeling",
        "authors": [
            "Chiu Wo Choi",
            "Jimmy Ho-Man Lee",
            "Peter J. Stuckey"
        ],
        "summary": "A widely adopted approach to solving constraint satisfaction problems combines systematic tree search with various degrees of constraint propagation for pruning the search space. One common technique to improve the execution efficiency is to add redundant constraints, which are constraints logically implied by others in the problem model. However, some redundant constraints are propagation redundant and hence do not contribute additional propagation information to the constraint solver. Redundant constraints arise naturally in the process of redundant modeling where two models of the same problem are connected and combined through channeling constraints. In this paper, we give general theorems for proving propagation redundancy of one constraint with respect to channeling constraints and constraints in the other model. We illustrate, on problems from CSPlib (http://www.csplib.org/), how detecting and removing propagation redundant constraints in redundant modeling can significantly speed up constraint solving.",
        "published": "2004-12-07T06:31:34Z",
        "link": "http://arxiv.org/abs/cs/0412026v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.3.3; F.4.1"
        ]
    },
    {
        "title": "A feasible algorithm for typing in Elementary Affine Logic",
        "authors": [
            "Patrick Baillot",
            "Kazushige Terui"
        ],
        "summary": "We give a new type inference algorithm for typing lambda-terms in Elementary Affine Logic (EAL), which is motivated by applications to complexity and optimal reduction. Following previous references on this topic, the variant of EAL type system we consider (denoted EAL*) is a variant without sharing and without polymorphism. Our algorithm improves over the ones already known in that it offers a better complexity bound: if a simple type derivation for the term t is given our algorithm performs EAL* type inference in polynomial time.",
        "published": "2004-12-08T08:33:08Z",
        "link": "http://arxiv.org/abs/cs/0412028v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "An Efficient and Flexible Engine for Computing Fixed Points",
        "authors": [
            "Hai-Feng Guo",
            "Gopal Gupta"
        ],
        "summary": "An efficient and flexible engine for computing fixed points is critical for many practical applications. In this paper, we firstly present a goal-directed fixed point computation strategy in the logic programming paradigm. The strategy adopts a tabled resolution (or memorized resolution) to mimic the efficient semi-naive bottom-up computation. Its main idea is to dynamically identify and record those clauses that will lead to recursive variant calls, and then repetitively apply those alternatives incrementally until the fixed point is reached. Secondly, there are many situations in which a fixed point contains a large number or even infinite number of solutions. In these cases, a fixed point computation engine may not be efficient enough or feasible at all. We present a mode-declaration scheme which provides the capabilities to reduce a fixed point from a big solution set to a preferred small one, or from an infeasible infinite set to a finite one. The mode declaration scheme can be characterized as a meta-level operation over the original fixed point. We show the correctness of the mode declaration scheme. Thirdly, the mode-declaration scheme provides a new declarative method for dynamic programming, which is typically used for solving optimization problems. There is no need to define the value of an optimal solution recursively, instead, defining a general solution suffices. The optimal value as well as its corresponding concrete solution can be derived implicitly and automatically using a mode-directed fixed point computation engine. Finally, this fixed point computation engine has been successfully implemented in a commercial Prolog system. Experimental results are shown to indicate that the mode declaration improves both time and space performances in solving dynamic programming problems.",
        "published": "2004-12-09T22:59:37Z",
        "link": "http://arxiv.org/abs/cs/0412041v2",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Labelled transition systems as a Stone space",
        "authors": [
            "Michael Huth"
        ],
        "summary": "A fully abstract and universal domain model for modal transition systems and refinement is shown to be a maximal-points space model for the bisimulation quotient of labelled transition systems over a finite set of events. In this domain model we prove that this quotient is a Stone space whose compact, zero-dimensional, and ultra-metrizable Hausdorff topology measures the degree of bisimilarity such that image-finite labelled transition systems are dense. Using this compactness we show that the set of labelled transition systems that refine a modal transition system, its ''set of implementations'', is compact and derive a compactness theorem for Hennessy-Milner logic on such implementation sets. These results extend to systems that also have partially specified state propositions, unify existing denotational, operational, and metric semantics on partial processes, render robust consistency measures for modal transition systems, and yield an abstract interpretation of compact sets of labelled transition systems as Scott-closed sets of modal transition systems.",
        "published": "2004-12-15T14:34:43Z",
        "link": "http://arxiv.org/abs/cs/0412063v5",
        "categories": [
            "cs.LO",
            "F.3.2; F.4.1"
        ]
    },
    {
        "title": "The Inverse Method Implements the Automata Approach for Modal   Satisfiability",
        "authors": [
            "Franz Baader",
            "Stephan Tobies"
        ],
        "summary": "Tableaux-based decision procedures for satisfiability of modal and description logics behave quite well in practice, but it is sometimes hard to obtain exact worst-case complexity results using these approaches, especially for EXPTIME-complete logics. In contrast, automata-based approaches often yield algorithms for which optimal worst-case complexity can easily be proved. However, the algorithms obtained this way are usually not only worst-case, but also best-case exponential: they first construct an automaton that is always exponential in the size of the input, and then apply the (polynomial) emptiness test to this large automaton. To overcome this problem, one must try to construct the automaton \"on-the-fly\" while performing the emptiness test.   In this paper we will show that Voronkov's inverse method for the modal logic K can be seen as an on-the-fly realization of the emptiness test done by the automata approach for K. The benefits of this result are two-fold. First, it shows that Voronkov's implementation of the inverse method, which behaves quite well in practice, is an optimized on-the-fly implementation of the automata-based satisfiability procedure for K. Second, it can be used to give a simpler proof of the fact that Voronkov's optimizations do not destroy completeness of the procedure. We will also show that the inverse method can easily be extended to handle global axioms, and that the correspondence to the automata approach still holds in this setting. In particular, the inverse method yields an EXPTIME-algorithm for satisfiability in K w.r.t. global axioms.",
        "published": "2004-12-22T14:17:48Z",
        "link": "http://arxiv.org/abs/cs/0412101v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On the existence of stable models of non-stratified logic programs",
        "authors": [
            "Stefania Costantini"
        ],
        "summary": "This paper introduces a fundamental result, which is relevant for Answer Set programming, and planning. For the first time since the definition of the stable model semantics, the class of logic programs for which a stable model exists is given a syntactic characterization. This condition may have a practical importance both for defining new algorithms for checking consistency and computing answer sets, and for improving the existing systems. The approach of this paper is to introduce a new canonical form (to which any logic program can be reduced to), to focus the attention on cyclic dependencies. The technical result is then given in terms of programs in canonical form (canonical programs), without loss of generality. The result is based on identifying the cycles contained in the program, showing that stable models of the overall program are composed of stable models of suitable sub-programs, corresponding to the cycles, and on defining the Cycle Graph. Each vertex of this graph corresponds to one cycle, and each edge corresponds to onehandle, which is a literal containing an atom that, occurring in both cycles, actually determines a connection between them. In fact, the truth value of the handle in the cycle where it appears as the head of a rule, influences the truth value of the atoms of the cycle(s) where it occurs in the body. We can therefore introduce the concept of a handle path, connecting different cycles. If for every odd cycle we can find a handle path with certain properties, then the existence of stable model is guaranteed.",
        "published": "2004-12-23T12:02:11Z",
        "link": "http://arxiv.org/abs/cs/0412105v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "The internal logic of Bell's states",
        "authors": [
            "Giulia Battilotti",
            "Paola Zizzi"
        ],
        "summary": "We investigate the internal logic of a quantum computer with two qubits, in the two particular cases of non-entanglement (separable states) and maximal entanglement (Bell's states). To this aim, we consider an internal (reversible) measurement which preserves the probabilities by mirroring the states. We then obtain logical judgements for both cases of separable and Bell's states.",
        "published": "2004-12-26T15:16:50Z",
        "link": "http://arxiv.org/abs/quant-ph/0412199v1",
        "categories": [
            "quant-ph",
            "cs.LO",
            "gr-qc",
            "hep-th",
            "math.LO"
        ]
    },
    {
        "title": "Running C++ models undet the Swarm environment",
        "authors": [
            "Richard Leow",
            "Russell K. Standish"
        ],
        "summary": "Objective-C is still the language of choice if users want to run their simulation efficiently under the Swarm environment since the Swarm environment itself was written in Objective-C. The language is a fast, object-oriented and easy to learn. However, the language is less well known than, less expressive than, and lacks support for many important features of C++ (eg. OpenMP for high performance computing application). In this paper, we present a methodology and software tools that we have developed for auto generating an Objective-C object template (and all the necessary interfacing functions) from a given C++ model, utilising the Classdesc's object description technology, so that the C++ model can both be run and accessed under the Objective-C and C++ environments. We also present a methodology for modifying an existing Swarm application to make part of the model (eg. the heatbug's step method) run under the C++ environment.",
        "published": "2004-01-27T03:42:03Z",
        "link": "http://arxiv.org/abs/cs/0401025v1",
        "categories": [
            "cs.MA",
            "D.1.5;D.2.3;I.2.11"
        ]
    },
    {
        "title": "EcoLab: Agent Based Modeling for C++ programmers",
        "authors": [
            "Russell K. Standish",
            "Richard Leow"
        ],
        "summary": "\\EcoLab{} is an agent based modeling system for C++ programmers, strongly influenced by the design of Swarm. This paper is just a brief outline of \\EcoLab's features, more details can be found in other published articles, documentation and source code from the \\EcoLab{} website.",
        "published": "2004-01-27T03:46:30Z",
        "link": "http://arxiv.org/abs/cs/0401026v1",
        "categories": [
            "cs.MA",
            "D.1.5;D.2.3;I.2.11"
        ]
    },
    {
        "title": "Anonymity and Information Hiding in Multiagent Systems",
        "authors": [
            "Joseph Y. Halpern",
            "Kevin R. O'Neill"
        ],
        "summary": "We provide a framework for reasoning about information-hiding requirements in multiagent systems and for reasoning about anonymity in particular. Our framework employs the modal logic of knowledge within the context of the runs and systems framework, much in the spirit of our earlier work on secrecy [Halpern and O'Neill 2002]. We give several definitions of anonymity with respect to agents, actions, and observers in multiagent systems, and we relate our definitions of anonymity to other definitions of information hiding, such as secrecy. We also give probabilistic definitions of anonymity that are able to quantify an observer s uncertainty about the state of the system. Finally, we relate our definitions of anonymity to other formalizations of anonymity and information hiding, including definitions of anonymity in the process algebra CSP and definitions of information hiding using function views.",
        "published": "2004-02-18T20:46:52Z",
        "link": "http://arxiv.org/abs/cs/0402042v2",
        "categories": [
            "cs.CR",
            "cs.LO",
            "cs.MA",
            "D.4.6; D.2.1"
        ]
    },
    {
        "title": "Information Theory - The Bridge Connecting Bounded Rational Game Theory   and Statistical Physics",
        "authors": [
            "David H. Wolpert"
        ],
        "summary": "A long-running difficulty with conventional game theory has been how to modify it to accommodate the bounded rationality of all real-world players. A recurring issue in statistical physics is how best to approximate joint probability distributions with decoupled (and therefore far more tractable) distributions. This paper shows that the same information theoretic mathematical structure, known as Product Distribution (PD) theory, addresses both issues. In this, PD theory not only provides a principled formulation of bounded rationality and a set of new types of mean field theory in statistical physics. It also shows that those topics are fundamentally one and the same.",
        "published": "2004-02-19T23:20:38Z",
        "link": "http://arxiv.org/abs/cond-mat/0402508v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.GT",
            "cs.MA",
            "nlin.AO"
        ]
    },
    {
        "title": "Spontaneous Emergence of Complex Optimal Networks through Evolutionary   Adaptation",
        "authors": [
            "Venkat Venkatasubramanian",
            "Santhoji Katare",
            "Priyan R. Patkar",
            "Fangping Mu"
        ],
        "summary": "An important feature of many complex systems, both natural and artificial, is the structure and organization of their interaction networks with interesting properties. Here we present a theory of self-organization by evolutionary adaptation in which we show how the structure and organization of a network is related to the survival, or in general the performance, objectives of the system. We propose that a complex system optimizes its network structure in order to maximize its overall survival fitness which is composed of short-term and long-term survival components. These in turn depend on three critical measures of the network, namely, efficiency, robustness and cost, and the environmental selection pressure. Using a graph theoretical case study, we show that when efficiency is paramount the \"Star\" topology emerges and when robustness is important the \"Circle\" topology is found. When efficiency and robustness requirements are both important to varying degrees, other classes of networks such as the \"Hub\" emerge. Our assumptions and results are consistent with observations across a wide variety of applications.",
        "published": "2004-02-24T23:56:56Z",
        "link": "http://arxiv.org/abs/nlin/0402046v1",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.MA",
            "nlin.CG",
            "q-bio.QM"
        ]
    },
    {
        "title": "Distributed Control by Lagrangian Steepest Descent",
        "authors": [
            "David H. Wolpert",
            "Stefan Bieniawski"
        ],
        "summary": "Often adaptive, distributed control can be viewed as an iterated game between independent players. The coupling between the players' mixed strategies, arising as the system evolves from one instant to the next, is determined by the system designer. Information theory tells us that the most likely joint strategy of the players, given a value of the expectation of the overall control objective function, is the minimizer of a Lagrangian function of the joint strategy. So the goal of the system designer is to speed evolution of the joint strategy to that Lagrangian minimizing point, lower the expectated value of the control objective function, and repeat. Here we elaborate the theory of algorithms that do this using local descent procedures, and that thereby achieve efficient, adaptive, distributed control.",
        "published": "2004-03-10T03:49:25Z",
        "link": "http://arxiv.org/abs/cs/0403012v1",
        "categories": [
            "cs.MA",
            "cs.GT",
            "nlin.AO",
            "J.6; J.7; G.m"
        ]
    },
    {
        "title": "Mathematical Analysis of Multi-Agent Systems",
        "authors": [
            "Kristina Lerman",
            "Aram Galstyan",
            "Tad Hogg"
        ],
        "summary": "We review existing approaches to mathematical modeling and analysis of multi-agent systems in which complex collective behavior arises out of local interactions between many simple agents. Though the behavior of an individual agent can be considered to be stochastic and unpredictable, the collective behavior of such systems can have a simple probabilistic description. We show that a class of mathematical models that describe the dynamics of collective behavior of multi-agent systems can be written down from the details of the individual agent controller. The models are valid for Markov or memoryless agents, in which each agents future state depends only on its present state and not any of the past states. We illustrate the approach by analyzing in detail applications from the robotics domain: collaboration and foraging in groups of robots.",
        "published": "2004-04-02T02:00:00Z",
        "link": "http://arxiv.org/abs/cs/0404002v1",
        "categories": [
            "cs.RO",
            "cs.MA",
            "I.2.9; I.2.11; I.6.5"
        ]
    },
    {
        "title": "Dealing With Curious Players in Secure Networks",
        "authors": [
            "Liam Wagner"
        ],
        "summary": "In secure communications networks there are a great number of user behavioural problems, which need to be dealt with. Curious players pose a very real and serious threat to the integrity of such a network. By traversing a network a Curious player could uncover secret information, which that user has no need to know, by simply posing as a loyalty check. Loyalty checks are done simply to gauge the integrity of the network with respect to players who act in a malicious manner. We wish to propose a method, which can deal with Curious players trying to obtain \"Need to Know\" information using a combined Fault-tolerant, Cryptographic and Game Theoretic Approach.",
        "published": "2004-04-02T05:16:08Z",
        "link": "http://arxiv.org/abs/cs/0404004v1",
        "categories": [
            "cs.CR",
            "cs.GT",
            "cs.MA",
            "C.2.0; I.2.1"
        ]
    },
    {
        "title": "Tycoon: A Distributed Market-based Resource Allocation System",
        "authors": [
            "Kevin Lai",
            "Bernardo A. Huberman",
            "Leslie Fine"
        ],
        "summary": "P2P clusters like the Grid and PlanetLab enable in principle the same statistical multiplexing efficiency gains for computing as the Internet provides for networking. The key unsolved problem is resource allocation. Existing solutions are not economically efficient and require high latency to acquire resources. We designed and implemented Tycoon, a market based distributed resource allocation system based on an Auction Share scheduling algorithm. Preliminary results show that Tycoon achieves low latency and high fairness while providing incentives for truth-telling on the part of strategic users.",
        "published": "2004-04-05T19:54:30Z",
        "link": "http://arxiv.org/abs/cs/0404013v1",
        "categories": [
            "cs.DC",
            "cs.MA",
            "C.2.4; D.4.1; D.4.7; K.6.0"
        ]
    },
    {
        "title": "Multi-agent coordination using nearest neighbor rules: revisiting the   Vicsek model",
        "authors": [
            "Sanjiang Li",
            "Huaiqing Wang"
        ],
        "summary": "Recently, Jadbabaie, Lin, and Morse (IEEE TAC, 48(6)2003:988-1001) offered a mathematical analysis of the discrete time model of groups of mobile autonomous agents raised by Vicsek et al. in 1995. In their paper, Jadbabaie et al. showed that all agents shall move in the same heading, provided that these agents are periodically linked together. This paper sharpens this result by showing that coordination will be reached under a very weak condition that requires all agents are finally linked together. This condition is also strictly weaker than the one Jadbabaie et al. desired.",
        "published": "2004-07-09T02:28:02Z",
        "link": "http://arxiv.org/abs/cs/0407021v2",
        "categories": [
            "cs.MA",
            "cs.AI"
        ]
    },
    {
        "title": "An agent-based intelligent environmental monitoring system",
        "authors": [
            "Ioannis N Athanasiadis",
            "Pericles A Mitkas"
        ],
        "summary": "Fairly rapid environmental changes call for continuous surveillance and on-line decision making. There are two main areas where IT technologies can be valuable. In this paper we present a multi-agent system for monitoring and assessing air-quality attributes, which uses data coming from a meteorological station. A community of software agents is assigned to monitor and validate measurements coming from several sensors, to assess air-quality, and, finally, to fire alarms to appropriate recipients, when needed. Data mining techniques have been used for adding data-driven, customized intelligence into agents. The architecture of the developed system, its domain ontology, and typical agent interactions are presented. Finally, the deployment of a real-world test case is demonstrated.",
        "published": "2004-07-10T11:06:57Z",
        "link": "http://arxiv.org/abs/cs/0407024v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "An agent framework for dynamic agent retraining: Agent academy",
        "authors": [
            "P. Mitkas",
            "A. Symeonidis",
            "D. Kechagias",
            "I. N. Athanasiadis",
            "G. Laleci",
            "G. Kurt",
            "Y. Kabak",
            "A. Acar",
            "A. Dogac"
        ],
        "summary": "Agent Academy (AA) aims to develop a multi-agent society that can train new agents for specific or general tasks, while constantly retraining existing agents in a recursive mode. The system is based on collecting information both from the environment and the behaviors of the acting agents and their related successes/failures to generate a body of data, stored in the Agent Use Repository, which is mined by the Data Miner module, in order to generate useful knowledge about the application domain. Knowledge extracted by the Data Miner is used by the Agent Training Module as to train new agents or to enhance the behavior of agents already running. In this paper the Agent Academy framework is introduced, and its overall architecture and functionality are presented. Training issues as well as agent ontologies are discussed. Finally, a scenario, which aims to provide environmental alerts to both individuals and public authorities, is described an AA-based use case.",
        "published": "2004-07-10T11:16:45Z",
        "link": "http://arxiv.org/abs/cs/0407025v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Parallel Computing Environments and Methods for Power Distribution   System Simulation",
        "authors": [
            "Ning Lu",
            "Z. Todd Taylor",
            "David P. Chassin",
            "Ross T. Guttromson",
            "R. Scott Studham"
        ],
        "summary": "The development of cost-effective highperformance parallel computing on multi-processor supercomputers makes it attractive to port excessively time consuming simulation software from personal computers (PC) to super computes. The power distribution system simulator (PDSS) takes a bottom-up approach and simulates load at the appliance level, where detailed thermal models for appliances are used. This approach works well for a small power distribution system consisting of a few thousand appliances. When the number of appliances increases, the simulation uses up the PC memory and its runtime increases to a point where the approach is no longer feasible to model a practical large power distribution system. This paper presents an effort made to port a PC-based power distribution system simulator to a 128-processor shared-memory supercomputer. The paper offers an overview of the parallel computing environment and a description of the modification made to the PDSS model. The performance of the PDSS running on a standalone PC and on the supercomputer is compared. Future research direction of utilizing parallel computing in the power distribution system simulation is also addressed.",
        "published": "2004-09-18T17:09:26Z",
        "link": "http://arxiv.org/abs/cs/0409035v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.MA",
            "cs.PF"
        ]
    },
    {
        "title": "A dynamical model of a GRID market",
        "authors": [
            "Uli Harder",
            "Peter Harrison",
            "Maya Paczuski",
            "Tejas Shah"
        ],
        "summary": "We discuss potential market mechanisms for the GRID. A complete dynamical model of a GRID market is defined with three types of agents. Providers, middlemen and users exchange universal GRID computing units (GCUs) at varying prices. Providers and middlemen have strategies aimed at maximizing profit while users are 'satisficing' agents, and only change their behavior if the service they receive is sufficiently poor or overpriced. Preliminary results from a multi-agent numerical simulation of the market model shows that the distribution of price changes has a power law tail.",
        "published": "2004-10-02T11:02:09Z",
        "link": "http://arxiv.org/abs/cs/0410005v1",
        "categories": [
            "cs.MA",
            "cond-mat.other",
            "cs.CE"
        ]
    },
    {
        "title": "RRL: A Rich Representation Language for the Description of Agent   Behaviour in NECA",
        "authors": [
            "P. Piwek",
            "B. Krenn",
            "M. Schroeder",
            "M. Grice",
            "S. Baumann",
            "H. Pirker"
        ],
        "summary": "In this paper, we describe the Rich Representation Language (RRL) which is used in the NECA system. The NECA system generates interactions between two or more animated characters. The RRL is an XML compliant framework for representing the information that is exchanged at the interfaces between the various NECA system modules. The full XML Schemas for the RRL are available at http://www.ai.univie.ac.at/NECA/RRL",
        "published": "2004-10-11T12:34:02Z",
        "link": "http://arxiv.org/abs/cs/0410022v1",
        "categories": [
            "cs.MM",
            "cs.MA",
            "H5.1, H5.2"
        ]
    },
    {
        "title": "A model of student's dilemma",
        "authors": [
            "Adam Lipowski",
            "Antonio L. Ferreira"
        ],
        "summary": "Each year perhaps millions of young people face the following dilemma: should I continue my education or rather start working with already acquired skills. Right decision must take into account somebody's own abilities, accessibility to education institutions, competition, and potential benefits. A multi-agent, evolutionary model of this dilemma predicts a transition between stratified and homogeneous phases, evolution that diminishes fitness, fewer applicants per seat for decreased capacity of the university, and presence of poor students at \\'elite universities.",
        "published": "2004-10-22T20:30:02Z",
        "link": "http://arxiv.org/abs/cond-mat/0410594v2",
        "categories": [
            "cond-mat.other",
            "cond-mat.stat-mech",
            "cs.MA",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Robust Dialogue Understanding in HERALD",
        "authors": [
            "Vincenzo Pallotta",
            "Afzal Ballim"
        ],
        "summary": "We tackle the problem of robust dialogue processing from the perspective of language engineering. We propose an agent-oriented architecture that allows us a flexible way of composing robust processors. Our approach is based on Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP agent model can be enriched with special features and components that allow us to deal with classical problems of dialogue understanding.",
        "published": "2004-10-22T23:41:52Z",
        "link": "http://arxiv.org/abs/cs/0410058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.MA",
            "cs.SE",
            "H.5.2, I.2.7, I.2.11"
        ]
    },
    {
        "title": "Detecting synchronization in spatially extended discrete systems by   complexity measurements",
        "authors": [
            "Juan R. Sánchez",
            "Ricardo López-Ruiz"
        ],
        "summary": "The synchronization of two stochastically coupled one-dimensional cellular automata (CA) is analyzed. It is shown that the transition to synchronization is characterized by a dramatic increase of the statistical complexity of the patterns generated by the difference automaton. This singular behavior is verified to be present in several CA rules displaying complex behavior.",
        "published": "2004-11-30T12:29:12Z",
        "link": "http://arxiv.org/abs/nlin/0411063v1",
        "categories": [
            "nlin.CG",
            "cond-mat.dis-nn",
            "cs.MA",
            "math.DS",
            "nlin.PS",
            "q-bio.QM"
        ]
    },
    {
        "title": "Self-Organizing Traffic Lights",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "Steering traffic in cities is a very complex task, since improving efficiency involves the coordination of many actors. Traditional approaches attempt to optimize traffic lights for a particular density and configuration of traffic. The disadvantage of this lies in the fact that traffic densities and configurations change constantly. Traffic seems to be an adaptation problem rather than an optimization problem. We propose a simple and feasible alternative, in which traffic lights self-organize to improve traffic flow. We use a multi-agent simulation to study three self-organizing methods, which are able to outperform traditional rigid and adaptive methods. Using simple rules and no direct communication, traffic lights are able to self-organize and adapt to changing traffic conditions, reducing waiting times, number of stopped cars, and increasing average speeds.",
        "published": "2004-11-30T17:25:00Z",
        "link": "http://arxiv.org/abs/nlin/0411066v2",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Negotiating over Bundles and Prices Using Aggregate Knowledge",
        "authors": [
            "Koye Somefun",
            "Tomas Klos",
            "Han La Poutré"
        ],
        "summary": "Combining two or more items and selling them as one good, a practice called bundling, can be a very effective strategy for reducing the costs of producing, marketing, and selling goods. In this paper, we consider a form of multi-issue negotiation where a shop negotiates both the contents and the price of bundles of goods with his customers. We present some key insights about, as well as a technique for, locating mutually beneficial alternatives to the bundle currently under negotiation. The essence of our approach lies in combining historical sales data, condensed into aggregate knowledge, with current data about the ongoing negotiation process, to exploit these insights. In particular, when negotiating a given bundle of goods with a customer, the shop analyzes the sequence of the customer's offers to determine the progress in the negotiation process. In addition, it uses aggregate knowledge concerning customers' valuations of goods in general. We show how the shop can use these two sources of data to locate promising alternatives to the current bundle. When the current negotiation's progress slows down, the shop may suggest the most promising of those alternatives and, depending on the customer's response, continue negotiating about the alternative bundle, or propose another alternative. Extensive computer simulation experiments show that our approach increases the speed with which deals are reached, as well as the number and quality of the deals reached, as compared to a benchmark. In addition, we show that the performance of our system is robust to a variety of changes in the negotiation strategies employed by the customers.",
        "published": "2004-12-23T11:31:26Z",
        "link": "http://arxiv.org/abs/cs/0412104v1",
        "categories": [
            "cs.MA",
            "cs.GT"
        ]
    },
    {
        "title": "Online Learning of Aggregate Knowledge about Non-linear Preferences   Applied to Negotiating Prices and Bundles",
        "authors": [
            "Koye Somefun",
            "Tomas Klos",
            "Han La Poutré"
        ],
        "summary": "In this paper, we consider a form of multi-issue negotiation where a shop negotiates both the contents and the price of bundles of goods with his customers. We present some key insights about, as well as a procedure for, locating mutually beneficial alternatives to the bundle currently under negotiation. The essence of our approach lies in combining aggregate (anonymous) knowledge of customer preferences with current data about the ongoing negotiation process. The developed procedure either works with already obtained aggregate knowledge or, in the absence of such knowledge, learns the relevant information online. We conduct computer experiments with simulated customers that have_nonlinear_ preferences. We show how, for various types of customers, with distinct negotiation heuristics, our procedure (with and without the necessary aggregate knowledge) increases the speed with which deals are reached, as well as the number and the Pareto efficiency of the deals reached compared to a benchmark.",
        "published": "2004-12-23T15:21:40Z",
        "link": "http://arxiv.org/abs/cs/0412106v1",
        "categories": [
            "cs.MA",
            "cs.GT",
            "cs.LG"
        ]
    },
    {
        "title": "Lexical Base as a Compressed Language Model of the World (on the   material of the Ukrainian language)",
        "authors": [
            "Solomiya Buk"
        ],
        "summary": "In the article the fact is verified that the list of words selected by formal statistical methods (frequency and functional genre unrestrictedness) is not a conglomerate of non-related words. It creates a system of interrelated items and it can be named \"lexical base of language\". This selected list of words covers all the spheres of human activities. To verify this statement the invariant synoptical scheme common for ideographic dictionaries of different language was determined.",
        "published": "2004-02-24T09:34:16Z",
        "link": "http://arxiv.org/abs/cs/0402055v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Artificial Sequences and Complexity Measures",
        "authors": [
            "Andrea Baronchelli",
            "Emanuele Caglioti",
            "Vittorio Loreto"
        ],
        "summary": "In this paper we exploit concepts of information theory to address the fundamental problem of identifying and defining the most suitable tools to extract, in a automatic and agnostic way, information from a generic string of characters. We introduce in particular a class of methods which use in a crucial way data compression techniques in order to define a measure of remoteness and distance between pairs of sequences of characters (e.g. texts) based on their relative information content. We also discuss in detail how specific features of data compression techniques could be used to introduce the notion of dictionary of a given sequence and of Artificial Text and we show how these new tools can be used for information extraction purposes. We point out the versatility and generality of our method that applies to any kind of corpora of character strings independently of the type of coding behind them. We consider as a case study linguistic motivated problems and we present results for automatic language recognition, authorship attribution and self consistent-classification.",
        "published": "2004-03-09T09:56:19Z",
        "link": "http://arxiv.org/abs/cond-mat/0403233v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.IR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Flexible Rule Compiler for Speech Synthesis",
        "authors": [
            "Wojciech Skut",
            "Stefan Ulrich",
            "Kathrine Hammervold"
        ],
        "summary": "We present a flexible rule compiler developed for a text-to-speech (TTS) system. The compiler converts a set of rules into a finite-state transducer (FST). The input and output of the FST are subject to parameterization, so that the system can be applied to strings and sequences of feature-structures. The resulting transducer is guaranteed to realize a function (as opposed to a relation), and therefore can be implemented as a deterministic device (either a deterministic FST or a bimachine).",
        "published": "2004-03-23T21:06:11Z",
        "link": "http://arxiv.org/abs/cs/0403039v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.5.2; F.4.3"
        ]
    },
    {
        "title": "Delimited continuations in natural language: quantification and polarity   sensitivity",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "Making a linguistic theory is like making a programming language: one typically devises a type system to delineate the acceptable utterances and a denotational semantics to explain observations on their behavior. Via this connection, the programming language concept of delimited continuations can help analyze natural language phenomena such as quantification and polarity sensitivity. Using a logical metalanguage whose syntax includes control operators and whose semantics involves evaluation order, these analyses can be expressed in direct style rather than continuation-passing style, and these phenomena can be thought of as computational side effects.",
        "published": "2004-04-05T01:53:46Z",
        "link": "http://arxiv.org/abs/cs/0404006v1",
        "categories": [
            "cs.CL",
            "cs.PL",
            "D.3.3; J.5"
        ]
    },
    {
        "title": "Polarity sensitivity and evaluation order in type-logical grammar",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "We present a novel, type-logical analysis of_polarity sensitivity_: how negative polarity items (like \"any\" and \"ever\") or positive ones (like \"some\") are licensed or prohibited. It takes not just scopal relations but also linear order into account, using the programming-language notions of delimited continuations and evaluation order, respectively. It thus achieves greater empirical coverage than previous proposals.",
        "published": "2004-04-05T02:14:50Z",
        "link": "http://arxiv.org/abs/cs/0404007v1",
        "categories": [
            "cs.CL",
            "J.5; D.3.3"
        ]
    },
    {
        "title": "Tabular Parsing",
        "authors": [
            "Mark-Jan Nederhof",
            "Giorgio Satta"
        ],
        "summary": "This is a tutorial on tabular parsing, on the basis of tabulation of nondeterministic push-down automata. Discussed are Earley's algorithm, the Cocke-Kasami-Younger algorithm, tabular LR parsing, the construction of parse trees, and further issues.",
        "published": "2004-04-05T11:51:43Z",
        "link": "http://arxiv.org/abs/cs/0404009v1",
        "categories": [
            "cs.CL",
            "F.4.2"
        ]
    },
    {
        "title": "NLML--a Markup Language to Describe the Unlimited English Grammar",
        "authors": [
            "Jiyou Jia"
        ],
        "summary": "In this paper we present NLML (Natural Language Markup Language), a markup language to describe the syntactic and semantic structure of any grammatically correct English expression. At first the related works are analyzed to demonstrate the necessity of the NLML: simple form, easy management and direct storage. Then the description of the English grammar with NLML is introduced in details in three levels: sentences (with different complexities, voices, moods, and tenses), clause (relative clause and noun clause) and phrase (noun phrase, verb phrase, prepositional phrase, adjective phrase, adverb phrase and predicate phrase). At last the application fields of the NLML in NLP are shown with two typical examples: NLOJM (Natural Language Object Modal in Java) and NLDB (Natural Language Database).",
        "published": "2004-04-07T06:54:45Z",
        "link": "http://arxiv.org/abs/cs/0404018v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Test Collections for Patent-to-Patent Retrieval and Patent Map   Generation in NTCIR-4 Workshop",
        "authors": [
            "Atsushi Fujii",
            "Makoto Iwayama",
            "Noriko Kando"
        ],
        "summary": "This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop, and the test collections produced in this task. We perform the invalidity search task, in which each participant group searches a patent collection for the patents that can invalidate the demand in an existing claim. We also perform the automatic patent map generation task, in which the patents associated with a specific topic are organized in a multi-dimensional matrix.",
        "published": "2004-04-10T08:43:31Z",
        "link": "http://arxiv.org/abs/cs/0404025v1",
        "categories": [
            "cs.CL",
            "H.3.3; H.3.4; I.2.7"
        ]
    },
    {
        "title": "DAB Content Annotation and Receiver Hardware Control with XML",
        "authors": [
            "Darran Nathan",
            "Eva Rosdiana",
            "Chua Beng Koon"
        ],
        "summary": "The Eureka-147 Digital Audio Broadcasting (DAB) standard defines the 'dynamic labels' data field for holding information about the transmission content. However, this information does not follow a well-defined structure since it is designed to carry text for direct output to displays, for human interpretation. This poses a problem when machine interpretation of DAB content information is desired. Extensible Markup Language (XML) was developed to allow for the well-defined, structured machine-to-machine exchange of data over computer networks. This article proposes a novel technique of machine-interpretable DAB content annotation and receiver hardware control, involving the utilisation of XML as metadata in the transmitted DAB frames.",
        "published": "2004-04-11T08:36:04Z",
        "link": "http://arxiv.org/abs/cs/0404026v1",
        "categories": [
            "cs.GL",
            "cs.CL",
            "D.3.2"
        ]
    },
    {
        "title": "NLOMJ--Natural Language Object Model in Java",
        "authors": [
            "Jiyou Jia"
        ],
        "summary": "In this paper we present NLOMJ--a natural language object model in Java with English as the experiment language. This modal describes the grammar elements of any permissible expression in a natural language and their complicated relations with each other with the concept \"Object\" in OOP(Object Oriented Programming). Directly mapped to the syntax and semantics of the natural language, it can be used in information retrieval as a linguistic method. Around the UML diagram of the NLOMJ the important classes(Sentence, Clause and Phrase) and their sub classes are introduced and their syntactic and semantic meanings are explained.",
        "published": "2004-04-21T06:30:28Z",
        "link": "http://arxiv.org/abs/cs/0404041v2",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7; D.1.5"
        ]
    },
    {
        "title": "Exploiting Cross-Document Relations for Multi-document Evolving   Summarization",
        "authors": [
            "Stergos D. Afantenos",
            "Irene Doura",
            "Eleni Kapellou",
            "Vangelis Karkaletsis"
        ],
        "summary": "This paper presents a methodology for summarization from multiple documents which are about a specific topic. It is based on the specification and identification of the cross-document relations that occur among textual elements within those documents. Our methodology involves the specification of the topic-specific entities, the messages conveyed for the specific entities by certain textual elements and the specification of the relations that can hold among these messages. The above resources are necessary for setting up a specific topic for our query-based summarization approach which uses these resources to identify the query-specific messages within the documents and the query-specific relations that connect these messages across documents.",
        "published": "2004-04-23T10:44:24Z",
        "link": "http://arxiv.org/abs/cs/0404049v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "A Probabilistic Model of Machine Translation",
        "authors": [
            "G. E. Miram",
            "V. K. Petrov"
        ],
        "summary": "A probabilistic model for computer-based generation of a machine translation system on the basis of English-Russian parallel text corpora is suggested. The model is trained using parallel text corpora with pre-aligned source and target sentences. The training of the model results in a bilingual dictionary of words and \"word blocks\" with relevant translation probability.",
        "published": "2004-05-10T16:05:23Z",
        "link": "http://arxiv.org/abs/cs/0405037v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Catching the Drift: Probabilistic Content Models, with Applications to   Generation and Summarization",
        "authors": [
            "Regina Barzilay",
            "Lillian Lee"
        ],
        "summary": "We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from un-annotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.",
        "published": "2004-05-12T14:14:52Z",
        "link": "http://arxiv.org/abs/cs/0405039v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Corpus structure, language models, and ad hoc information retrieval",
        "authors": [
            "Oren Kurland",
            "Lillian Lee"
        ],
        "summary": "Most previous work on the recently developed language-modeling approach to information retrieval focuses on document-specific characteristics, and therefore does not take into account the structure of the surrounding corpus. We propose a novel algorithmic framework in which information provided by document-based language models is enhanced by the incorporation of information drawn from clusters of similar documents. Using this framework, we develop a suite of new algorithms. Even the simplest typically outperforms the standard language-modeling approach in precision and recall, and our new interpolation algorithm posts statistically significant improvements for both metrics over all three corpora tested.",
        "published": "2004-05-12T20:18:51Z",
        "link": "http://arxiv.org/abs/cs/0405044v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Algorithms for weighted multi-tape automata",
        "authors": [
            "Andre Kempe",
            "Franck Guingne",
            "Florent Nicart"
        ],
        "summary": "This report defines various operations for weighted multi-tape automata (WMTAs) and describes algorithms that have been implemented for those operations in the WFSC toolkit. Some algorithms are new, others are known or similar to known algorithms. The latter will be recalled to make this report more complete and self-standing. We present a new approach to multi-tape intersection, meaning the intersection of a number of tapes of one WMTA with the same number of tapes of another WMTA. In our approach, multi-tape intersection is not considered as an atomic operation but rather as a sequence of more elementary ones, which facilitates its implementation. We show an example of multi-tape intersection, actually transducer intersection, that can be compiled with our approach but not with several other methods that we analysed. To show the practical relavance of our work, we include an example of application: the preservation of intermediate results in transduction cascades.",
        "published": "2004-06-02T18:51:52Z",
        "link": "http://arxiv.org/abs/cs/0406003v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Zipf's law and the creation of musical context",
        "authors": [
            "Damian H. Zanette"
        ],
        "summary": "This article discusses the extension of the notion of context from linguistics to the domain of music. In language, the statistical regularity known as Zipf's law -which concerns the frequency of usage of different words- has been quantitatively related to the process of text generation. This connection is established by Simon's model, on the basis of a few assumptions regarding the accompanying creation of context. Here, it is shown that the statistics of note usage in musical compositions are compatible with the predictions of Simon's model. This result, which gives objective support to the conceptual likeness of context in language and music, is obtained through automatic analysis of the digital versions of several compositions. As a by-product, a quantitative measure of context definiteness is introduced and used to compare tonal and atonal works.",
        "published": "2004-06-07T17:52:07Z",
        "link": "http://arxiv.org/abs/cs/0406015v1",
        "categories": [
            "cs.CL",
            "cond-mat.stat-mech"
        ]
    },
    {
        "title": "A Public Reference Implementation of the RAP Anaphora Resolution   Algorithm",
        "authors": [
            "Long Qiu",
            "Min-Yen Kan",
            "Tat-Seng Chua"
        ],
        "summary": "This paper describes a standalone, publicly-available implementation of the Resolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The RAP algorithm resolves third person pronouns, lexical anaphors, and identifies pleonastic pronouns. Our implementation, JavaRAP, fills a current need in anaphora resolution research by providing a reference implementation that can be benchmarked against current algorithms. The implementation uses the standard, publicly available Charniak (2000) parser as input, and generates a list of anaphora-antecedent pairs as output. Alternately, an in-place annotation or substitution of the anaphors with their antecedents can be produced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an accuracy of 57.9%, similar to the performance given previously in the literature (e.g., Preiss 2002).",
        "published": "2004-06-17T12:29:29Z",
        "link": "http://arxiv.org/abs/cs/0406031v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Building a linguistic corpus from bee dance data",
        "authors": [
            "J. J. Paijmans"
        ],
        "summary": "This paper discusses the problems and possibility of collecting bee dance data in a linguistic \\textit{corpus} and use linguistic instruments such as Zipf's law and entropy statistics to decide on the question whether the dance carries information of any kind. We describe this against the historical background of attempts to analyse non-human communication systems.",
        "published": "2004-06-28T10:25:22Z",
        "link": "http://arxiv.org/abs/cs/0406054v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Annotating Predicate-Argument Structure for a Parallel Treebank",
        "authors": [
            "Lea Cyrus",
            "Hendrik Feddes",
            "Frank Schumacher"
        ],
        "summary": "We report on a recently initiated project which aims at building a multi-layered parallel treebank of English and German. Particular attention is devoted to a dedicated predicate-argument layer which is used for aligning translationally equivalent sentences of the two languages. We describe both our conceptual decisions and aspects of their technical realisation. We discuss some selected problems and conclude with a few remarks on how this project relates to similar projects in the field.",
        "published": "2004-07-01T16:18:52Z",
        "link": "http://arxiv.org/abs/cs/0407002v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Statistical Machine Translation by Generalized Parsing",
        "authors": [
            "I. Dan Melamed",
            "Wei Wang"
        ],
        "summary": "Designers of statistical machine translation (SMT) systems have begun to employ tree-structured translation models. Systems involving tree-structured translation models tend to be complex. This article aims to reduce the conceptual complexity of such systems, in order to make them easier to design, implement, debug, use, study, understand, explain, modify, and improve. In service of this goal, the article extends the theory of semiring parsing to arrive at a novel abstract parsing algorithm with five functional parameters: a logic, a grammar, a semiring, a search strategy, and a termination condition. The article then shows that all the common algorithms that revolve around tree-structured translation models, including hierarchical alignment, inference for parameter estimation, translation, and structured evaluation, can be derived by generalizing two of these parameters -- the grammar and the logic. The article culminates with a recipe for using such generalized parsers to train, apply, and evaluate an SMT system that is driven by tree-structured translation models.",
        "published": "2004-07-01T22:02:10Z",
        "link": "http://arxiv.org/abs/cs/0407005v3",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Summarizing Encyclopedic Term Descriptions on the Web",
        "authors": [
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "We are developing an automatic method to compile an encyclopedic corpus from the Web. In our previous work, paragraph-style descriptions for a term are extracted from Web pages and organized based on domains. However, these descriptions are independent and do not comprise a condensed text as in hand-crafted encyclopedias. To resolve this problem, we propose a summarization method, which produces a single text from multiple descriptions. The resultant summary concisely describes a term from different viewpoints. We also show the effectiveness of our method by means of experiments.",
        "published": "2004-07-10T11:18:42Z",
        "link": "http://arxiv.org/abs/cs/0407026v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.3.5"
        ]
    },
    {
        "title": "Unsupervised Topic Adaptation for Lecture Speech Retrieval",
        "authors": [
            "Atsushi Fujii",
            "Katunobu Itou",
            "Tomoyosi Akiba",
            "Tetsuya Ishikawa"
        ],
        "summary": "We are developing a cross-media information retrieval system, in which users can view specific segments of lecture videos by submitting text queries. To produce a text index, the audio track is extracted from a lecture video and a transcription is generated by automatic speech recognition. In this paper, to improve the quality of our retrieval system, we extensively investigate the effects of adapting acoustic and language models on speech recognition. We perform an MLLR-based method to adapt an acoustic model. To obtain a corpus for language model adaptation, we use the textbook for a target lecture to search a Web collection for the pages associated with the lecture topic. We show the effectiveness of our method by means of experiments.",
        "published": "2004-07-10T11:45:57Z",
        "link": "http://arxiv.org/abs/cs/0407027v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.5.1"
        ]
    },
    {
        "title": "Effects of Language Modeling on Speech-driven Question Answering",
        "authors": [
            "Tomoyosi Akiba",
            "Atsushi Fujii",
            "Katunobu Itou"
        ],
        "summary": "We integrate automatic speech recognition (ASR) and question answering (QA) to realize a speech-driven QA system, and evaluate its performance. We adapt an N-gram language model to natural language questions, so that the input of our system can be recognized with a high accuracy. We target WH-questions which consist of the topic part and fixed phrase used to ask about something. We first produce a general N-gram model intended to recognize the topic and emphasize the counts of the N-grams that correspond to the fixed phrases. Given a transcription by the ASR engine, the QA engine extracts the answer candidates from target documents. We propose a passage retrieval method robust against recognition errors in the transcription. We use the QA test collection produced in NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness of our method by means of experiments.",
        "published": "2004-07-10T11:57:17Z",
        "link": "http://arxiv.org/abs/cs/0407028v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.3.4; H.5.1"
        ]
    },
    {
        "title": "A Bimachine Compiler for Ranked Tagging Rules",
        "authors": [
            "Wojciech Skut",
            "Stefan Ulrich",
            "Kathrine Hammervold"
        ],
        "summary": "This paper describes a novel method of compiling ranked tagging rules into a deterministic finite-state device called a bimachine. The rules are formulated in the framework of regular rewrite operations and allow unrestricted regular expressions in both left and right rule contexts. The compiler is illustrated by an application within a speech synthesis system.",
        "published": "2004-07-19T11:57:42Z",
        "link": "http://arxiv.org/abs/cs/0407046v1",
        "categories": [
            "cs.CL",
            "I.2.7;F.4.2;F.4.3"
        ]
    },
    {
        "title": "Word Sense Disambiguation by Web Mining for Word Co-occurrence   Probabilities",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word \\hbox{co-occurrence} probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler.",
        "published": "2004-07-29T19:46:01Z",
        "link": "http://arxiv.org/abs/cs/0407065v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.3; I.2.6; I.2.7; J.5"
        ]
    },
    {
        "title": "Incremental Construction of Minimal Acyclic Sequential Transducers from   Unsorted Data",
        "authors": [
            "Wojciech Skut"
        ],
        "summary": "This paper presents an efficient algorithm for the incremental construction of a minimal acyclic sequential transducer (ST) for a dictionary consisting of a list of input and output strings. The algorithm generalises a known method of constructing minimal finite-state automata (Daciuk et al. 2000). Unlike the algorithm published by Mihov and Maurel (2001), it does not require the input strings to be sorted. The new method is illustrated by an application to pronunciation dictionaries.",
        "published": "2004-08-10T11:09:48Z",
        "link": "http://arxiv.org/abs/cs/0408026v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "F.4.3"
        ]
    },
    {
        "title": "CHR Grammars",
        "authors": [
            "Henning Christiansen"
        ],
        "summary": "A grammar formalism based upon CHR is proposed analogously to the way Definite Clause Grammars are defined and implemented on top of Prolog. These grammars execute as robust bottom-up parsers with an inherent treatment of ambiguity and a high flexibility to model various linguistic phenomena. The formalism extends previous logic programming based grammars with a form of context-sensitive rules and the possibility to include extra-grammatical hypotheses in both head and body of grammar rules. Among the applications are straightforward implementations of Assumption Grammars and abduction under integrity constraints for language analysis. CHR grammars appear as a powerful tool for specification and implementation of language processors and may be proposed as a new standard for bottom-up grammars in logic programming.   To appear in Theory and Practice of Logic Programming (TPLP), 2005",
        "published": "2004-08-12T11:15:17Z",
        "link": "http://arxiv.org/abs/cs/0408027v1",
        "categories": [
            "cs.CL",
            "cs.PL"
        ]
    },
    {
        "title": "Multi-dimensional Type Theory: Rules, Categories, and Combinators for   Syntax and Semantics",
        "authors": [
            "Jørgen Villadsen"
        ],
        "summary": "We investigate the possibility of modelling the syntax and semantics of natural language by constraints, or rules, imposed by the multi-dimensional type theory Nabla. The only multiplicity we explicitly consider is two, namely one dimension for the syntax and one dimension for the semantics, but the general perspective is important. For example, issues of pragmatics could be handled as additional dimensions.   One of the main problems addressed is the rather complicated repertoire of operations that exists besides the notion of categories in traditional Montague grammar. For the syntax we use a categorial grammar along the lines of Lambek. For the semantics we use so-called lexical and logical combinators inspired by work in natural logic. Nabla provides a concise interpretation and a sequent calculus as the basis for implementations.",
        "published": "2004-08-15T08:51:19Z",
        "link": "http://arxiv.org/abs/cs/0408037v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LO",
            "I.2.7"
        ]
    },
    {
        "title": "Fractal geometry of literature: first attempt to Shakespeare's works",
        "authors": [
            "Ali Eftekhari"
        ],
        "summary": "It was demonstrated that there is a geometrical order in the structure of literature. Fractal geometry as a modern mathematical approach and a new geometrical viewpoint on natural objects including both processes and structures was employed for analysis of literature. As the first study, the works of William Shakespeare were chosen as the most important items in western literature. By counting the number of letters applied in a manuscript, it is possible to study the whole manuscript statistically. A novel method based on basic assumption of fractal geometry was proposed for the calculation of fractal dimensions of the literature. The results were compared with Zipf's law. Zipf's law was successfully used for letters instead of words. Two new concepts namely Zipf's dimension and Zipf's order were also introduced. It was found that changes of both fractal dimension and Zipf's dimension are similar and dependent on the manuscript length. Interestingly, direct plotting the data obtained in semi-logarithmic and logarithmic forms also led to a power-law.",
        "published": "2004-08-17T10:14:22Z",
        "link": "http://arxiv.org/abs/cs/0408041v1",
        "categories": [
            "cs.CL",
            "cs.CC"
        ]
    },
    {
        "title": "Application of the Double Metaphone Algorithm to Amharic Orthography",
        "authors": [
            "Daniel Yacob"
        ],
        "summary": "The Metaphone algorithm applies the phonetic encoding of orthographic sequences to simplify words prior to comparison. While Metaphone has been highly successful for the English language, for which it was designed, it may not be applied directly to Ethiopian languages. The paper details how the principles of Metaphone can be applied to Ethiopic script and uses Amharic as a case study. Match results improve as specific considerations are made for Amharic writing practices. Results are shown to improve further when common errors from Amharic input methods are considered.",
        "published": "2004-08-22T19:32:48Z",
        "link": "http://arxiv.org/abs/cs/0408052v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "The role of robust semantic analysis in spoken language dialogue systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "In this paper we summarized a framework for designing grammar-based procedure for the automatic extraction of the semantic content from spoken queries. Starting with a case study and following an approach which combines the notions of fuzziness and robustness in sentence parsing, we showed we built practical domain-dependent rules which can be applied whenever it is possible to superimpose a sentence-level semantic structure to a text without relying on a previous deep syntactical analysis. This kind of procedure can be also profitably used as a pre-processing tool in order to cut out part of the sentence which have been recognized to have no relevance in the understanding process. In the case of particular dialogue applications where there is no need to build a complex semantic structure (e.g. word spotting or excerpting) the presented methodology may represent an efficient alternative solution to a sequential composition of deep linguistic analysis modules. Even if the query generation problem may not seem a critical application it should be held in mind that the sentence processing must be done on-line. Having this kind of constraints we cannot design our system without caring for efficiency and thus provide an immediate response. Another critical issue is related to whole robustness of the system. In our case study we tried to make experiences on how it is possible to deal with an unreliable and noisy input without asking the user for any repetition or clarification. This may correspond to a similar problem one may have when processing text coming from informal writing such as e-mails, news and in many cases Web pages where it is often the case to have irrelevant surrounding information.",
        "published": "2004-08-25T19:37:59Z",
        "link": "http://arxiv.org/abs/cs/0408057v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "H.5.2;H.3.1;H.3.4"
        ]
    },
    {
        "title": "Proofing Tools Technology at Neurosoft S.A.",
        "authors": [
            "Ch. Tsalidis",
            "G. Orphanos",
            "A. Iordanidou",
            "A. Vagelatos"
        ],
        "summary": "The aim of this paper is to present the R&D activities carried out at Neurosoft S.A. regarding the development of proofing tools for Modern Greek. Firstly, we focus on infrastructure issues that we faced during our initial steps. Subsequently, we describe the most important insights of three proofing tools developed by Neurosoft, i.e. the spelling checker, the hyphenator and the thesaurus, outlining their efficiencies and inefficiencies. Finally, we discuss some improvement ideas and give our future directions.",
        "published": "2004-08-26T10:50:45Z",
        "link": "http://arxiv.org/abs/cs/0408059v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Verbal chunk extraction in French using limited resources",
        "authors": [
            "Gabriel G. Bes",
            "Lionel Lamadon",
            "Francois Trouilleux"
        ],
        "summary": "A way of extracting French verbal chunks, inflected and infinitive, is explored and tested on effective corpus. Declarative morphological and local grammar rules specifying chunks and some simple contextual structures are used, relying on limited lexical information and some simple heuristic/statistic properties obtained from restricted corpora. The specific goals, the architecture and the formalism of the system, the linguistic information on which it relies and the obtained results on effective corpus are presented.",
        "published": "2004-08-26T12:44:15Z",
        "link": "http://arxiv.org/abs/cs/0408060v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "An electronic dictionary as a basis for NLP tools: The Greek case",
        "authors": [
            "Ch. Tsalidis",
            "A. Vagelatos",
            "G. Orphanos"
        ],
        "summary": "The existence of a Dictionary in electronic form for Modern Greek (MG) is mandatory if one is to process MG at the morphological and syntactic levels since MG is a highly inflectional language with marked stress and a spelling system with many characteristics carried over from Ancient Greek. Moreover, such a tool becomes necessary if one is to create efficient and sophisticated NLP applications with substantial linguistic backing and coverage. The present paper will focus on the deployment of such an electronic dictionary for Modern Greek, which was built in two phases: first it was constructed to be the basis for a spelling correction schema and then it was reconstructed in order to become the platform for the deployment of a wider spectrum of NLP tools.",
        "published": "2004-08-26T13:17:38Z",
        "link": "http://arxiv.org/abs/cs/0408061v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Model for Fine-Grained Alignment of Multilingual Texts",
        "authors": [
            "Lea Cyrus",
            "Hendrik Feddes"
        ],
        "summary": "While alignment of texts on the sentential level is often seen as being too coarse, and word alignment as being too fine-grained, bi- or multilingual texts which are aligned on a level in-between are a useful resource for many purposes. Starting from a number of examples of non-literal translations, which tend to make alignment difficult, we describe an alignment model which copes with these cases by explicitly coding them. The model is based on predicate-argument structures and thus covers the middle ground between sentence and word alignment. The model is currently used in a recently initiated project of a parallel English-German treebank (FuSe), which can in principle be extended with additional languages.",
        "published": "2004-09-07T13:46:50Z",
        "link": "http://arxiv.org/abs/cs/0409008v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A new architecture for making highly scalable applications",
        "authors": [
            "Harry Fitié"
        ],
        "summary": "An application is a logical image of the world on a computer. A scalable application is an application that allows one to update that logical image at run time. To put it in operational terms: an application is scalable if a client can change between time T1 and time T2 - the logic of the application as expressed by language L;   - the structure and volume of the stored knowledge;   - the user interface of the application; while clients working with the application at time T1 will work with the changed application at time T2 without performing any special action between T1 and T2. In order to realize such a scalable application a new architecture has been developed that fully orbits around language. In order to verify the soundness of that architecture a program has been build. Both architecture and program are called CommunSENS. The main purpose of this paper is: - to list the relevant elements of the architecture; - to give a visual presentation of how the program and its image of the world look like; - to give a visual presentation of how the image can be updated. Some relevant philosophical and practical backgrounds are included in the appendixes.",
        "published": "2004-09-23T19:29:26Z",
        "link": "http://arxiv.org/abs/cs/0409042v1",
        "categories": [
            "cs.HC",
            "cs.CL"
        ]
    },
    {
        "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity   Summarization Based on Minimum Cuts",
        "authors": [
            "Bo Pang",
            "Lillian Lee"
        ],
        "summary": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.",
        "published": "2004-09-29T20:34:04Z",
        "link": "http://arxiv.org/abs/cs/0409058v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Automated Pattern Detection--An Algorithm for Constructing Optimally   Synchronizing Multi-Regular Language Filters",
        "authors": [
            "Carl S. McTague",
            "James P. Crutchfield"
        ],
        "summary": "In the computational-mechanics structural analysis of one-dimensional cellular automata the following automata-theoretic analogue of the \\emph{change-point problem} from time series analysis arises: \\emph{Given a string $\\sigma$ and a collection $\\{\\mc{D}_i\\}$ of finite automata, identify the regions of $\\sigma$ that belong to each $\\mc{D}_i$ and, in particular, the boundaries separating them.} We present two methods for solving this \\emph{multi-regular language filtering problem}. The first, although providing the ideal solution, requires a stack, has a worst-case compute time that grows quadratically in $\\sigma$'s length and conditions its output at any point on arbitrarily long windows of future input. The second method is to algorithmically construct a transducer that approximates the first algorithm. In contrast to the stack-based algorithm, however, the transducer requires only a finite amount of memory, runs in linear time, and gives immediate output for each letter read; it is, moreover, the best possible finite-state approximation with these three features.",
        "published": "2004-10-07T17:20:56Z",
        "link": "http://arxiv.org/abs/cs/0410017v1",
        "categories": [
            "cs.CV",
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.DS",
            "cs.IR",
            "cs.LG",
            "nlin.AO",
            "nlin.CG",
            "nlin.PS",
            "physics.comp-ph",
            "q-bio.GN"
        ]
    },
    {
        "title": "Detecting User Engagement in Everyday Conversations",
        "authors": [
            "Chen Yu",
            "Paul M. Aoki",
            "Allison Woodruff"
        ],
        "summary": "This paper presents a novel application of speech emotion recognition: estimation of the level of conversational engagement between users of a voice communication system. We begin by using machine learning techniques, such as the support vector machine (SVM), to classify users' emotions as expressed in individual utterances. However, this alone fails to model the temporal and interactive aspects of conversational engagement. We therefore propose the use of a multilevel structure based on coupled hidden Markov models (HMM) to estimate engagement levels in continuous natural speech. The first level is comprised of SVM-based classifiers that recognize emotional states, which could be (e.g.) discrete emotion types or arousal/valence levels. A high-level HMM then uses these emotional states as input, estimating users' engagement in conversation by decoding the internal states of the HMM. We report experimental results obtained by applying our algorithms to the LDC Emotional Prosody and CallFriend speech corpora.",
        "published": "2004-10-13T02:28:10Z",
        "link": "http://arxiv.org/abs/cs/0410027v1",
        "categories": [
            "cs.SD",
            "cs.CL",
            "cs.HC",
            "I.5.4; I.2.7; H.5.2; H.4.3"
        ]
    },
    {
        "title": "Robust Dialogue Understanding in HERALD",
        "authors": [
            "Vincenzo Pallotta",
            "Afzal Ballim"
        ],
        "summary": "We tackle the problem of robust dialogue processing from the perspective of language engineering. We propose an agent-oriented architecture that allows us a flexible way of composing robust processors. Our approach is based on Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP agent model can be enriched with special features and components that allow us to deal with classical problems of dialogue understanding.",
        "published": "2004-10-22T23:41:52Z",
        "link": "http://arxiv.org/abs/cs/0410058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.MA",
            "cs.SE",
            "H.5.2, I.2.7, I.2.11"
        ]
    },
    {
        "title": "A knowledge-based approach to semi-automatic annotation of multimedia   documents via user adaptation",
        "authors": [
            "Afzal Ballim",
            "Nastaran Fatemi",
            "Hatem Ghorbel",
            "Vincenzo Pallotta"
        ],
        "summary": "Current approaches to the annotation process focus on annotation schemas, languages for annotation, or are very application driven. In this paper it is proposed that a more flexible architecture for annotation requires a knowledge component to allow for flexible search and navigation of the annotated material. In particular, it is claimed that a general approach must take into account the needs, competencies, and goals of the producers, annotators, and consumers of the annotated material. We propose that a user-model based approach is, therefore, necessary.",
        "published": "2004-10-23T00:06:19Z",
        "link": "http://arxiv.org/abs/cs/0410059v1",
        "categories": [
            "cs.DL",
            "cs.CL",
            "cs.IR",
            "I.7.2, H.3.7"
        ]
    },
    {
        "title": "Semantic filtering by inference on domain knowledge in spoken dialogue   systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "General natural dialogue processing requires large amounts of domain knowledge as well as linguistic knowledge in order to ensure acceptable coverage and understanding. There are several ways of integrating lexical resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at different levels of dialogue processing. We concentrate in this paper on how to exploit domain knowledge for filtering interpretation hypotheses generated by a robust semantic parser. We use domain knowledge to semantically constrain the hypothesis space. Moreover, adding an inference mechanism allows us to complete the interpretation when information is not explicitly available. Further, we discuss briefly how this can be generalized towards a predictive natural interactive system.",
        "published": "2004-10-23T00:20:06Z",
        "link": "http://arxiv.org/abs/cs/0410060v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.5.2;H.3.1;H.3.4"
        ]
    },
    {
        "title": "An argumentative annotation schema for meeting discussions",
        "authors": [
            "Vincenzo Pallotta",
            "Hatem Ghorbel",
            "Patrick Ruch",
            "Giovanni Coray"
        ],
        "summary": "In this article, we are interested in the annotation of transcriptions of human-human dialogue taken from meeting records. We first propose a meeting content model where conversational acts are interpreted with respect to their argumentative force and their role in building the argumentative structure of the meeting discussion. Argumentation in dialogue describes the way participants take part in the discussion and argue their standpoints. Then, we propose an annotation scheme based on such an argumentative dialogue model as well as the evaluation of its adequacy. The obtained higher-level semantic annotations are exploited in the conceptual indexing of the information contained in meeting discussions.",
        "published": "2004-10-25T01:38:07Z",
        "link": "http://arxiv.org/abs/cs/0410061v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "cs.IR",
            "H.3.1;I.7.2;H.5.1"
        ]
    },
    {
        "title": "Automatic Keyword Extraction from Spoken Text. A Comparison of two   Lexical Resources: the EDR and WordNet",
        "authors": [
            "Lonneke van der Plas",
            "Vincenzo Pallotta",
            "Martin Rajman",
            "Hatem Ghorbel"
        ],
        "summary": "Lexical resources such as WordNet and the EDR electronic dictionary have been used in several NLP tasks. Probably, partly due to the fact that the EDR is not freely available, WordNet has been used far more often than the EDR. We have used both resources on the same task in order to make a comparison possible. The task is automatic assignment of keywords to multi-party dialogue episodes (i.e. thematically coherent stretches of spoken text). We show that the use of lexical resources in such a task results in slightly higher performances than the use of a purely statistically based method.",
        "published": "2004-10-25T01:50:03Z",
        "link": "http://arxiv.org/abs/cs/0410062v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "cs.IR",
            "H.3.1;H.3.3;I.5.3;I.7.3"
        ]
    },
    {
        "title": "INSPIRE: Evaluation of a Smart-Home System for Infotainment Management   and Device Control",
        "authors": [
            "Sebastian Moeller",
            "Jan Krebber",
            "Alexander Raake",
            "Paula Smeele",
            "Martin Rajman",
            "Mirek Melichar",
            "Vincenzo Pallotta",
            "Gianna Tsakou",
            "Basilis Kladis",
            "Anestis Vovos",
            "Jettie Hoonhout",
            "Dietmar Schuchardt",
            "Nikos Fakotakis",
            "Todor Ganchev",
            "Ilyas Potamitis"
        ],
        "summary": "This paper gives an overview of the assessment and evaluation methods which have been used to determine the quality of the INSPIRE smart home system. The system allows different home appliances to be controlled via speech, and consists of speech and speaker recognition, speech understanding, dialogue management, and speech output components. The performance of these components is first assessed individually, and then the entire system is evaluated in an interaction experiment with test users. Initial results of the assessment and evaluation are given, in particular with respect to the transmission channel impact on speech and speaker recognition, and the assessment of speech output for different system metaphors.",
        "published": "2004-10-25T02:17:35Z",
        "link": "http://arxiv.org/abs/cs/0410063v1",
        "categories": [
            "cs.HC",
            "cs.CL",
            "H.5.2;I.2.7;H.1.2"
        ]
    },
    {
        "title": "Temporal logic with predicate abstraction",
        "authors": [
            "Alexei Lisitsa",
            "Igor Potapov"
        ],
        "summary": "A predicate linear temporal logic LTL_{\\lambda,=} without quantifiers but with predicate abstraction mechanism and equality is considered. The models of LTL_{\\lambda,=} can be naturally seen as the systems of pebbles (flexible constants) moving over the elements of some (possibly infinite) domain. This allows to use LTL_{\\lambda,=} for the specification of dynamic systems using some resources, such as processes using memory locations, mobile agents occupying some sites, etc. On the other hand we show that LTL_{\\lambda,=} is not recursively axiomatizable and, therefore, fully automated verification of LTL_{\\lambda,=} specifications is not, in general, possible.",
        "published": "2004-10-27T17:21:52Z",
        "link": "http://arxiv.org/abs/cs/0410072v1",
        "categories": [
            "cs.LO",
            "cs.CL",
            "F.1.1; F.3.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "Building Chinese Lexicons from Scratch by Unsupervised Short Document   Self-Segmentation",
        "authors": [
            "Daniel Gayo-Avello"
        ],
        "summary": "Chinese text segmentation is a well-known and difficult problem. On one side, there is not a simple notion of \"word\" in Chinese language making really hard to implement rule-based systems to segment written texts, thus lexicons and statistical information are usually employed to achieve such a task. On the other side, any piece of Chinese text usually includes segments present neither in the lexicons nor in the training data. Even worse, such unseen sequences can be segmented into a number of totally unrelated words making later processing phases difficult. For instance, using a lexicon-based system the sequence ???(Baluozuo, Barroso, current president-designate of the European Commission) can be segmented into ?(ba, to hope, to wish) and ??(luozuo, an undefined word) changing completely the meaning of the sentence. A new and extremely simple algorithm specially suited to work over short Chinese documents is introduced. This new algorithm performs text \"self-segmentation\" producing results comparable to those achieved by native speakers without using either lexicons or any statistical information beyond the obtained from the input text. Furthermore, it is really robust for finding new \"words\", especially proper nouns, and it is well suited to build lexicons from scratch. Some preliminary results are provided in addition to examples of its employment.",
        "published": "2004-11-20T00:22:30Z",
        "link": "http://arxiv.org/abs/cs/0411074v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "A Tutorial on the Expectation-Maximization Algorithm Including   Maximum-Likelihood Estimation and EM Training of Probabilistic Context-Free   Grammars",
        "authors": [
            "Detlef Prescher"
        ],
        "summary": "The paper gives a brief review of the expectation-maximization algorithm (Dempster 1977) in the comprehensible framework of discrete mathematics. In Section 2, two prominent estimation methods, the relative-frequency estimation and the maximum-likelihood estimation are presented. Section 3 is dedicated to the expectation-maximization algorithm and a simpler variant, the generalized expectation-maximization algorithm. In Section 4, two loaded dice are rolled. A more interesting example is presented in Section 5: The estimation of probabilistic context-free grammars.",
        "published": "2004-12-03T17:10:17Z",
        "link": "http://arxiv.org/abs/cs/0412015v2",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Inside-Outside Estimation Meets Dynamic EM",
        "authors": [
            "Detlef Prescher"
        ],
        "summary": "We briefly review the inside-outside and EM algorithm for probabilistic context-free grammars. As a result, we formally prove that inside-outside estimation is a dynamic-programming variant of EM. This is interesting in its own right, but even more when considered in a theoretical context since the well-known convergence behavior of inside-outside estimation has been confirmed by many experiments but apparently has never been formally proved. However, being a version of EM, inside-outside estimation also inherits the good convergence behavior of EM. Therefore, the as yet imperfect line of argumentation can be transformed into a coherent proof.",
        "published": "2004-12-03T18:10:17Z",
        "link": "http://arxiv.org/abs/cs/0412016v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Human-Level Performance on Word Analogy Questions by Latent Relational   Analysis",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, machine translation, and information retrieval. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason/stone is analogous to the pair carpenter/wood. Past work on semantic similarity measures has mainly been concerned with attributional similarity. Recently the Vector Space Model (VSM) of information retrieval has been adapted to the task of measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus (they are not predefined), (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data (it is also used this way in Latent Semantic Analysis), and (3) automatically generated synonyms are used to explore reformulations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying noun-modifier relations, LRA achieves similar gains over the VSM, while using a smaller corpus.",
        "published": "2004-12-06T21:50:18Z",
        "link": "http://arxiv.org/abs/cs/0412024v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "A Framework for Creating Natural Language User Interfaces for   Action-Based Applications",
        "authors": [
            "Stephen Chong",
            "Riccardo Pucella"
        ],
        "summary": "In this paper we present a framework for creating natural language interfaces to action-based applications. Our framework uses a number of reusable application-independent components, in order to reduce the effort of creating a natural language interface for a given application. Using a type-logical grammar, we first translate natural language sentences into expressions in an extended higher-order logic. These expressions can be seen as executable specifications corresponding to the original sentences. The executable specifications are then interpreted by invoking appropriate procedures provided by the application for which a natural language interface is being created.",
        "published": "2004-12-17T03:24:51Z",
        "link": "http://arxiv.org/abs/cs/0412065v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "H.5.2; I.2.7; F.4.2; F.4.1"
        ]
    },
    {
        "title": "The Google Similarity Distance",
        "authors": [
            "Rudi Cilibrasi",
            "Paul M. B. Vitanyi"
        ],
        "summary": "Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers the equivalent of `society' is `database,' and the equivalent of `use' is `way to search the database.' We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts we use the world-wide-web as database, and Google as search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the world-wide-web using Google page counts. The world-wide-web is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies, and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87% with the expert crafted WordNet categories.",
        "published": "2004-12-21T16:05:36Z",
        "link": "http://arxiv.org/abs/cs/0412098v3",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "State of the Art, Evaluation and Recommendations regarding \"Document   Processing and Visualization Techniques\"",
        "authors": [
            "Martin Rajman",
            "Martin Vesely",
            "Pierre Andrews"
        ],
        "summary": "Several Networks of Excellence have been set up in the framework of the European FP5 research program. Among these Networks of Excellence, the NEMIS project focuses on the field of Text Mining.   Within this field, document processing and visualization was identified as one of the key topics and the WG1 working group was created in the NEMIS project, to carry out a detailed survey of techniques associated with the text mining process and to identify the relevant research topics in related research areas.   In this document we present the results of this comprehensive survey. The report includes a description of the current state-of-the-art and practice, a roadmap for follow-up research in the identified areas, and recommendations for anticipated technological development in the domain of text mining.",
        "published": "2004-12-29T15:19:03Z",
        "link": "http://arxiv.org/abs/cs/0412114v1",
        "categories": [
            "cs.CL",
            "I.2.7;I.7"
        ]
    },
    {
        "title": "Thematic Annotation: extracting concepts out of documents",
        "authors": [
            "Pierre Andrews",
            "Martin Rajman"
        ],
        "summary": "Contrarily to standard approaches to topic annotation, the technique used in this work does not centrally rely on some sort of -- possibly statistical -- keyword extraction. In fact, the proposed annotation algorithm uses a large scale semantic database -- the EDR Electronic Dictionary -- that provides a concept hierarchy based on hyponym and hypernym relations. This concept hierarchy is used to generate a synthetic representation of the document by aggregating the words present in topically homogeneous document segments into a set of concepts best preserving the document's content.   This new extraction technique uses an unexplored approach to topic selection. Instead of using semantic similarity measures based on a semantic resource, the later is processed to extract the part of the conceptual hierarchy relevant to the document content. Then this conceptual hierarchy is searched to extract the most relevant set of concepts to represent the topics discussed in the document. Notice that this algorithm is able to extract generic concepts that are not directly present in the document.",
        "published": "2004-12-30T02:01:45Z",
        "link": "http://arxiv.org/abs/cs/0412117v1",
        "categories": [
            "cs.CL",
            "I.2.7;I.7"
        ]
    },
    {
        "title": "Mobile Re-Finding of Web Information Using a Voice Interface",
        "authors": [
            "Robert G. Capra",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "Mobile access to information is a considerable problem for many users, especially to information found on the Web. In this paper, we explore how a voice-controlled service, accessible by telephone, could support mobile users' needs for refinding specific information previously found on the Web. We outline challenges in creating such a service and describe architectural and user interfaces issues discovered in an exploratory prototype we built called WebContext.   We also present the results of a study, motivated by our experience with WebContext, to explore what people remember about information that they are trying to refind and how they express information refinding requests in a collaborative conversation. As part of the study, we examine how end-usercreated Web page annotations can be used to help support mobile information re-finding. We observed the use of URLs, page titles, and descriptions of page contents to help identify waypoints in the search process. Furthermore, we observed that the annotations were utilized extensively, indicating that explicitly added context by the user can play an important role in re-finding.",
        "published": "2004-01-31T20:09:39Z",
        "link": "http://arxiv.org/abs/cs/0402001v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.1.2; H.3.3; H.5.2"
        ]
    },
    {
        "title": "Mapping Topics and Topic Bursts in PNAS",
        "authors": [
            "Ketan Mane",
            "Katy Börner"
        ],
        "summary": "Scientific research is highly dynamic. New areas of science continually evolve;others gain or lose importance, merge or split. Due to the steady increase in the number of scientific publications it is hard to keep an overview of the structure and dynamic development of one's own field of science, much less all scientific domains. However, knowledge of hot topics, emergent research frontiers, or change of focus in certain areas is a critical component of resource allocation decisions in research labs, governmental institutions, and corporations. This paper demonstrates the utilization of Kleinberg's burst detection algorithm, co-word occurrence analysis, and graph layout techniques to generate maps that support the identification of major research topics and trends. The approach was applied to analyze and map the complete set of papers published in the Proceedings of the National Academy of Sciences (PNAS) in the years 1982-2001. Six domain experts examined and commented on the resulting maps in an attempt to reconstruct the evolution of major research areas covered by PNAS.",
        "published": "2004-02-14T03:55:53Z",
        "link": "http://arxiv.org/abs/cs/0402029v1",
        "categories": [
            "cs.IR",
            "cs.HC",
            "H.3.3; H.1.2"
        ]
    },
    {
        "title": "Dictionary based methods for information extraction",
        "authors": [
            "A. Baronchelli",
            "E. Caglioti",
            "V. Loreto",
            "E. Pizzi"
        ],
        "summary": "In this paper we present a general method for information extraction that exploits the features of data compression techniques. We first define and focus our attention on the so-called \"dictionary\" of a sequence. Dictionaries are intrinsically interesting and a study of their features can be of great usefulness to investigate the properties of the sequences they have been extracted from (e.g. DNA strings). We then describe a procedure of string comparison between dictionary-created sequences (or \"artificial texts\") that gives very good results in several contexts. We finally present some results on self-consistent classification problems.",
        "published": "2004-02-24T11:34:53Z",
        "link": "http://arxiv.org/abs/cond-mat/0402581v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.other",
            "cs.IR",
            "q-bio.GN",
            "q-bio.OT"
        ]
    },
    {
        "title": "A Correlation-Based Distance",
        "authors": [
            "Jean-Luc Falcone",
            "Paul Albuquerque"
        ],
        "summary": "In this short technical report, we define on the sample space R^D a distance between data points which depends on their correlation. We also derive an expression for the center of mass of a set of points with respect to this distance.",
        "published": "2004-02-27T14:13:01Z",
        "link": "http://arxiv.org/abs/cs/0402061v1",
        "categories": [
            "cs.IR",
            "I.5.3"
        ]
    },
    {
        "title": "Evolving a Stigmergic Self-Organized Data-Mining",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "summary": "Self-organizing complex systems typically are comprised of a large number of frequently similar components or events. Through their process, a pattern at the global-level of a system emerges solely from numerous interactions among the lower-level components of the system. Moreover, the rules specifying interactions among the system's components are executed using only local information, without reference to the global pattern, which, as in many real-world problems is not easily accessible or possible to be found. Stigmergy, a kind of indirect communication and learning by the environment found in social insects is a well know example of self-organization, providing not only vital clues in order to understand how the components can interact to produce a complex pattern, as can pinpoint simple biological non-linear rules and methods to achieve improved artificial intelligent adaptive categorization systems, critical for Data-Mining. On the present work it is our intention to show that a new type of Data-Mining can be designed based on Stigmergic paradigms, taking profit of several natural features of this phenomenon. By hybridizing bio-inspired Swarm Intelligence with Evolutionary Computation we seek for an entire distributed, adaptive, collective and cooperative self-organized Data-Mining. As a real-world, real-time test bed for our proposal, World-Wide-Web Mining will be used. Having that purpose in mind, Web usage Data was collected from the Monash University's Web site (Australia), with over 7 million hits every week. Results are compared to other recent systems, showing that the system presented is by far promising.",
        "published": "2004-02-28T23:50:45Z",
        "link": "http://arxiv.org/abs/cs/0403001v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "I.2.11"
        ]
    },
    {
        "title": "Artificial Sequences and Complexity Measures",
        "authors": [
            "Andrea Baronchelli",
            "Emanuele Caglioti",
            "Vittorio Loreto"
        ],
        "summary": "In this paper we exploit concepts of information theory to address the fundamental problem of identifying and defining the most suitable tools to extract, in a automatic and agnostic way, information from a generic string of characters. We introduce in particular a class of methods which use in a crucial way data compression techniques in order to define a measure of remoteness and distance between pairs of sequences of characters (e.g. texts) based on their relative information content. We also discuss in detail how specific features of data compression techniques could be used to introduce the notion of dictionary of a given sequence and of Artificial Text and we show how these new tools can be used for information extraction purposes. We point out the versatility and generality of our method that applies to any kind of corpora of character strings independently of the type of coding behind them. We consider as a case study linguistic motivated problems and we present results for automatic language recognition, authorship attribution and self consistent-classification.",
        "published": "2004-03-09T09:56:19Z",
        "link": "http://arxiv.org/abs/cond-mat/0403233v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.IR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Web pages search engine based on DNS",
        "authors": [
            "Wang Liang",
            "Guo Yi-Ping",
            "Fang Ming"
        ],
        "summary": "Search engine is main access to the largest information source in this world, Internet. Now Internet is changing every aspect of our life. Information retrieval service may be its most important services. But for common user, internet search service is still far from our expectation, too many unrelated search results, old information, etc. To solve these problems, a new system, search engine based on DNS is proposed. The original idea, detailed content and implementation of this system all are introduced in this paper.",
        "published": "2004-03-23T09:20:26Z",
        "link": "http://arxiv.org/abs/cs/0403035v1",
        "categories": [
            "cs.NI",
            "cs.IR",
            "H.3.3;H.3.7;C.2.2"
        ]
    },
    {
        "title": "\"In vivo\" spam filtering: A challenge problem for data mining",
        "authors": [
            "Tom Fawcett"
        ],
        "summary": "Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email communication. Many data mining researchers have addressed the problem of detecting spam, generally by treating it as a static text classification problem. True in vivo spam filtering has characteristics that make it a rich and challenging domain for data mining. Indeed, real-world datasets with these characteristics are typically difficult to acquire and to share. This paper demonstrates some of these characteristics and argues that researchers should pursue in vivo spam filtering as an accessible domain for investigating them.",
        "published": "2004-05-04T18:56:09Z",
        "link": "http://arxiv.org/abs/cs/0405007v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "Corpus structure, language models, and ad hoc information retrieval",
        "authors": [
            "Oren Kurland",
            "Lillian Lee"
        ],
        "summary": "Most previous work on the recently developed language-modeling approach to information retrieval focuses on document-specific characteristics, and therefore does not take into account the structure of the surrounding corpus. We propose a novel algorithmic framework in which information provided by document-based language models is enhanced by the incorporation of information drawn from clusters of similar documents. Using this framework, we develop a suite of new algorithms. Even the simplest typically outperforms the standard language-modeling approach in precision and recall, and our new interpolation algorithm posts statistically significant improvements for both metrics over all three corpora tested.",
        "published": "2004-05-12T20:18:51Z",
        "link": "http://arxiv.org/abs/cs/0405044v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Mining Frequent Itemsets from Secondary Memory",
        "authors": [
            "Gösta Grahne",
            "Jianfei Zhu"
        ],
        "summary": "Mining frequent itemsets is at the core of mining association rules, and is by now quite well understood algorithmically. However, most algorithms for mining frequent itemsets assume that the main memory is large enough for the data structures used in the mining, and very few efficient algorithms deal with the case when the database is very large or the minimum support is very low. Mining frequent itemsets from a very large database poses new challenges, as astronomical amounts of raw data is ubiquitously being recorded in commerce, science and government. In this paper, we discuss approaches to mining frequent itemsets when data structures are too large to fit in main memory. Several divide-and-conquer algorithms are given for mining from disks. Many novel techniques are introduced. Experimental results show that the techniques reduce the required disk accesses by orders of magnitude, and enable truly scalable data mining.",
        "published": "2004-05-20T14:33:08Z",
        "link": "http://arxiv.org/abs/cs/0405069v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.2.8"
        ]
    },
    {
        "title": "Web search engine based on DNS",
        "authors": [
            "Wang Liang",
            "Guo YiPing",
            "Fang Ming"
        ],
        "summary": "Now no web search engine can cover more than 60 percent of all the pages on Internet. The update interval of most pages database is almost one month. This condition hasn't changed for many years. Converge and recency problems have become the bottleneck problem of current web search engine. To solve these problems, a new system, search engine based on DNS is proposed in this paper. This system adopts the hierarchical distributed architecture like DNS, which is different from any current commercial search engine. In theory, this system can cover all the web pages on Internet. Its update interval could even be one day. The original idea, detailed content and implementation of this system all are introduced in this paper.",
        "published": "2004-05-27T02:36:44Z",
        "link": "http://arxiv.org/abs/cs/0405099v1",
        "categories": [
            "cs.NI",
            "cs.IR",
            "H.3.3;H.3.7;C.2.2"
        ]
    },
    {
        "title": "A Dynamic Clustering-Based Markov Model for Web Usage Mining",
        "authors": [
            "José Borges",
            "Mark Levene"
        ],
        "summary": "Markov models have been widely utilized for modelling user web navigation behaviour. In this work we propose a dynamic clustering-based method to increase a Markov model's accuracy in representing a collection of user web navigation sessions. The method makes use of the state cloning concept to duplicate states in a way that separates in-links whose corresponding second-order probabilities diverge. In addition, the new method incorporates a clustering technique which determines an effcient way to assign in-links with similar second-order probabilities to the same clone. We report on experiments conducted with both real and random data and we provide a comparison with the N-gram Markov concept. The results show that the number of additional states induced by the dynamic clustering method can be controlled through a threshold parameter, and suggest that the method's performance is linear time in the size of the model.",
        "published": "2004-06-17T13:38:17Z",
        "link": "http://arxiv.org/abs/cs/0406032v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Search Using N-gram Technique Based Statistical Analysis for Knowledge   Extraction in Case Based Reasoning Systems",
        "authors": [
            "M. N. Karthik",
            "Moshe Davis"
        ],
        "summary": "Searching techniques for Case Based Reasoning systems involve extensive methods of elimination. In this paper, we look at a new method of arriving at the right solution by performing a series of transformations upon the data. These involve N-gram based comparison and deduction of the input data with the case data, using Morphemes and Phonemes as the deciding parameters. A similar technique for eliminating possible errors using a noise removal function is performed. The error tracking and elimination is performed through a statistical analysis of obtained data, where the entire data set is analyzed as sub-categories of various etymological derivatives. A probability analysis for the closest match is then performed, which yields the final expression. This final expression is referred to the Case Base. The output is redirected through an Expert System based on best possible match. The threshold for the match is customizable, and could be set by the Knowledge-Architect.",
        "published": "2004-07-02T20:22:18Z",
        "link": "http://arxiv.org/abs/cs/0407009v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "H.3.3; I.2.1"
        ]
    },
    {
        "title": "A Framework for High-Accuracy Privacy-Preserving Mining",
        "authors": [
            "Shipra Agrawal",
            "Jayant R. Haritsa"
        ],
        "summary": "To preserve client privacy in the data mining process, a variety of techniques based on random perturbation of data records have been proposed recently. In this paper, we present a generalized matrix-theoretic model of random perturbation, which facilitates a systematic approach to the design of perturbation mechanisms for privacy-preserving mining. Specifically, we demonstrate that (a) the prior techniques differ only in their settings for the model parameters, and (b) through appropriate choice of parameter settings, we can derive new perturbation techniques that provide highly accurate mining results even under strict privacy guarantees. We also propose a novel perturbation mechanism wherein the model parameters are themselves characterized as random variables, and demonstrate that this feature provides significant improvements in privacy at a very marginal cost in accuracy.   While our model is valid for random-perturbation-based privacy-preserving mining in general, we specifically evaluate its utility here with regard to frequent-itemset mining on a variety of real datasets. The experimental results indicate that our mechanisms incur substantially lower identity and support errors as compared to the prior techniques.",
        "published": "2004-07-15T14:30:20Z",
        "link": "http://arxiv.org/abs/cs/0407035v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Design of a Parallel and Distributed Web Search Engine",
        "authors": [
            "Salvatore Orlando",
            "Raffaele Perego",
            "Fabrizio Silvestri"
        ],
        "summary": "This paper describes the architecture of MOSE (My Own Search Engine), a scalable parallel and distributed engine for searching the web. MOSE was specifically designed to efficiently exploit affordable parallel architectures, such as clusters of workstations. Its modular and scalable architecture can easily be tuned to fulfill the bandwidth requirements of the application at hand. Both task-parallel and data-parallel approaches are exploited within MOSE in order to increase the throughput and efficiently use communication, storing and computational resources. We used a collection of html documents as a benchmark, and conducted preliminary experiments on a cluster of three SMP Linux PCs.",
        "published": "2004-07-21T07:21:50Z",
        "link": "http://arxiv.org/abs/cs/0407053v1",
        "categories": [
            "cs.IR",
            "cs.DC"
        ]
    },
    {
        "title": "A measure of similarity between graph vertices",
        "authors": [
            "Vincent Blondel",
            "Anahi Gajardo",
            "Maureen Heymans",
            "Pierre Senellart",
            "Paul Van Dooren"
        ],
        "summary": "We introduce a concept of similarity between vertices of directed graphs. Let G_A and G_B be two directed graphs. We define a similarity matrix whose (i, j)-th real entry expresses how similar vertex j (in G_A) is to vertex i (in G_B. The similarity matrix can be obtained as the limit of the normalized even iterates of a linear transformation. In the special case where G_A=G_B=G, the matrix is square and the (i, j)-th entry is the similarity score between the vertices i and j of G. We point out that Kleinberg's \"hub and authority\" method to identify web-pages relevant to a given query can be viewed as a special case of our definition in the case where one of the graphs has two vertices and a unique directed edge between them. In analogy to Kleinberg, we show that our similarity scores are given by the components of a dominant eigenvector of a non-negative matrix. Potential applications of our similarity concept are numerous. We illustrate an application for the automatic extraction of synonyms in a monolingual dictionary.",
        "published": "2004-07-28T13:42:26Z",
        "link": "http://arxiv.org/abs/cs/0407061v1",
        "categories": [
            "cs.IR",
            "cond-mat.dis-nn",
            "cs.DM",
            "physics.data-an"
        ]
    },
    {
        "title": "Word Sense Disambiguation by Web Mining for Word Co-occurrence   Probabilities",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word \\hbox{co-occurrence} probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeled text, collected by a web crawler.",
        "published": "2004-07-29T19:46:01Z",
        "link": "http://arxiv.org/abs/cs/0407065v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.3; I.2.6; I.2.7; J.5"
        ]
    },
    {
        "title": "Semantic Linking - a Context-Based Approach to Interactivity in   Hypermedia",
        "authors": [
            "Michael Engelhardt",
            "Thomas C. Schmidt"
        ],
        "summary": "The semantic Web initiates new, high level access schemes to online content and applications. One area of superior need for a redefined content exploration is given by on-line educational applications and their concepts of interactivity in the framework of open hypermedia systems. In the present paper we discuss aspects and opportunities of gaining interactivity schemes from semantic notions of components. A transition from standard educational annotation to semantic statements of hyperlinks is discussed. Further on we introduce the concept of semantic link contexts as an approach to manage a coherent rhetoric of linking. A practical implementation is introduced, as well. Our semantic hyperlink implementation is based on the more general Multimedia Information Repository MIR, an open hypermedia system supporting the standards XML, Corba and JNDI.",
        "published": "2004-07-31T14:04:04Z",
        "link": "http://arxiv.org/abs/cs/0408001v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.5.4; H.2.4; H.3.4; H.5.1; C.2.4; K.3.1"
        ]
    },
    {
        "title": "Hypermedia Learning Objects System - On the Way to a Semantic   Educational Web",
        "authors": [
            "Michael Engelhardt",
            "Andreas Kárpáti",
            "Torsten Rack",
            "Ivette Schmidt",
            "Thomas C. Schmidt"
        ],
        "summary": "While eLearning systems become more and more popular in daily education, available applications lack opportunities to structure, annotate and manage their contents in a high-level fashion. General efforts to improve these deficits are taken by initiatives to define rich meta data sets and a semanticWeb layer. In the present paper we introduce Hylos, an online learning system. Hylos is based on a cellular eLearning Object (ELO) information model encapsulating meta data conforming to the LOM standard. Content management is provisioned on this semantic meta data level and allows for variable, dynamically adaptable access structures. Context aware multifunctional links permit a systematic navigation depending on the learners and didactic needs, thereby exploring the capabilities of the semantic web. Hylos is built upon the more general Multimedia Information Repository (MIR) and the MIR adaptive context linking environment (MIRaCLE), its linking extension. MIR is an open system supporting the standards XML, Corba and JNDI. Hylos benefits from manageable information structures, sophisticated access logic and high-level authoring tools like the ELO editor responsible for the semi-manual creation of meta data and WYSIWYG like content editing.",
        "published": "2004-07-31T22:16:37Z",
        "link": "http://arxiv.org/abs/cs/0408004v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.5.4; H.2.4; H.3.4; H.5.1; C.2.4; K.3.1"
        ]
    },
    {
        "title": "Educational Content Management - A Cellular Approach",
        "authors": [
            "Michael Engelhardt",
            "Arne Hildebrand",
            "Andreas Kárpáti",
            "Torsten Rack",
            "Thomas C. Schmidt"
        ],
        "summary": "In recent times online educational applications more and more are requested to provide self-consistent learning offers for students at the university level. Consequently they need to cope with the wide range of complexity and interrelations university course teaching brings along. An urgent need to overcome simplistically linked HTMLc ontent pages becomes apparent. In the present paper we discuss a schematic concept of educational content construction from information cells and introduce its implementation on the storage and runtime layer. Starting from cells content is annotated according to didactic needs, structured for dynamic arrangement, dynamically decorated with hyperlinks and, as all works are based on XML, open to any presentation layer. Data can be variably accessed through URIs built on semantic path-names and edited via an adaptive authoring toolbox. Our content management approach is based on the more general Multimedia Information Repository MIR. and allows for personalisation, as well. MIR is an open system supporting the standards XML, Corba and JNDI.",
        "published": "2004-08-01T11:34:32Z",
        "link": "http://arxiv.org/abs/cs/0408005v1",
        "categories": [
            "cs.CY",
            "cs.IR",
            "K.3.1; H.3.4; H.3.5; H.5.1; H.5.4; J.7"
        ]
    },
    {
        "title": "Analysis and Visualization of Index Words from Audio Transcripts of   Instructional Videos",
        "authors": [
            "Alexander Haubold",
            "John R. Kender"
        ],
        "summary": "We introduce new techniques for extracting, analyzing, and visualizing textual contents from instructional videos of low production quality. Using Automatic Speech Recognition, approximate transcripts (H75% Word Error Rate) are obtained from the originally highly compressed videos of university courses, each comprising between 10 to 30 lectures. Text material in the form of books or papers that accompany the course are then used to filter meaningful phrases from the seemingly incoherent transcripts. The resulting index into the transcripts is tied together and visualized in 3 experimental graphs that help in understanding the overall course structure and provide a tool for localizing certain topics for indexing. We specifically discuss a Transcript Index Map, which graphically lays out key phrases for a course, a Textbook Chapter to Transcript Match, and finally a Lecture Transcript Similarity graph, which clusters semantically similar lectures. We test our methods and tools on 7 full courses with 230 hours of video and 273 transcripts. We are able to extract up to 98 unique key terms for a given transcript and up to 347 unique key terms for an entire course. The accuracy of the Textbook Chapter to Transcript Match exceeds 70% on average. The methods used can be applied to genres of video in which there are recurrent thematic words (news, sports, meetings,...)",
        "published": "2004-08-27T20:45:32Z",
        "link": "http://arxiv.org/abs/cs/0408063v1",
        "categories": [
            "cs.IR",
            "cs.MM",
            "H.3.1;H.3.3;I.2.10"
        ]
    },
    {
        "title": "Epistemic communities: description and hierarchic categorization",
        "authors": [
            "Camille Roth",
            "Paul Bourgine"
        ],
        "summary": "Social scientists have shown an increasing interest in understanding the structure of knowledge communities, and particularly the organization of \"epistemic communities\", that is groups of agents sharing common knowledge concerns. However, most existing approaches are based only on either social relationships or semantic similarity, while there has been roughly no attempt to link social and semantic aspects. In this paper, we introduce a formal framework addressing this issue and propose a method based on Galois lattices (or concept lattices) for categorizing epistemic communities in an automated and hierarchically structured fashion. Suggesting that our process allows us to rebuild a whole community structure and taxonomy, and notably fields and subfields gathering a certain proportion of agents, we eventually apply it to empirical data to exhibit these alleged structural properties, and successfully compare our results with categories spontaneously given by domain experts.",
        "published": "2004-09-06T14:48:40Z",
        "link": "http://arxiv.org/abs/nlin/0409013v2",
        "categories": [
            "nlin.AO",
            "cs.IR"
        ]
    },
    {
        "title": "Automated Pattern Detection--An Algorithm for Constructing Optimally   Synchronizing Multi-Regular Language Filters",
        "authors": [
            "Carl S. McTague",
            "James P. Crutchfield"
        ],
        "summary": "In the computational-mechanics structural analysis of one-dimensional cellular automata the following automata-theoretic analogue of the \\emph{change-point problem} from time series analysis arises: \\emph{Given a string $\\sigma$ and a collection $\\{\\mc{D}_i\\}$ of finite automata, identify the regions of $\\sigma$ that belong to each $\\mc{D}_i$ and, in particular, the boundaries separating them.} We present two methods for solving this \\emph{multi-regular language filtering problem}. The first, although providing the ideal solution, requires a stack, has a worst-case compute time that grows quadratically in $\\sigma$'s length and conditions its output at any point on arbitrarily long windows of future input. The second method is to algorithmically construct a transducer that approximates the first algorithm. In contrast to the stack-based algorithm, however, the transducer requires only a finite amount of memory, runs in linear time, and gives immediate output for each letter read; it is, moreover, the best possible finite-state approximation with these three features.",
        "published": "2004-10-07T17:20:56Z",
        "link": "http://arxiv.org/abs/cs/0410017v1",
        "categories": [
            "cs.CV",
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.DS",
            "cs.IR",
            "cs.LG",
            "nlin.AO",
            "nlin.CG",
            "nlin.PS",
            "physics.comp-ph",
            "q-bio.GN"
        ]
    },
    {
        "title": "Mathematical knowledge management is needed",
        "authors": [
            "Michiel Hazewinkel"
        ],
        "summary": "In this lecture I discuss some aspects of MKM, Mathematical Knowledge Management, with particuar emphasis on information storage and information retrieval.",
        "published": "2004-10-21T15:51:20Z",
        "link": "http://arxiv.org/abs/cs/0410055v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "A knowledge-based approach to semi-automatic annotation of multimedia   documents via user adaptation",
        "authors": [
            "Afzal Ballim",
            "Nastaran Fatemi",
            "Hatem Ghorbel",
            "Vincenzo Pallotta"
        ],
        "summary": "Current approaches to the annotation process focus on annotation schemas, languages for annotation, or are very application driven. In this paper it is proposed that a more flexible architecture for annotation requires a knowledge component to allow for flexible search and navigation of the annotated material. In particular, it is claimed that a general approach must take into account the needs, competencies, and goals of the producers, annotators, and consumers of the annotated material. We propose that a user-model based approach is, therefore, necessary.",
        "published": "2004-10-23T00:06:19Z",
        "link": "http://arxiv.org/abs/cs/0410059v1",
        "categories": [
            "cs.DL",
            "cs.CL",
            "cs.IR",
            "I.7.2, H.3.7"
        ]
    },
    {
        "title": "Semantic filtering by inference on domain knowledge in spoken dialogue   systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "General natural dialogue processing requires large amounts of domain knowledge as well as linguistic knowledge in order to ensure acceptable coverage and understanding. There are several ways of integrating lexical resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at different levels of dialogue processing. We concentrate in this paper on how to exploit domain knowledge for filtering interpretation hypotheses generated by a robust semantic parser. We use domain knowledge to semantically constrain the hypothesis space. Moreover, adding an inference mechanism allows us to complete the interpretation when information is not explicitly available. Further, we discuss briefly how this can be generalized towards a predictive natural interactive system.",
        "published": "2004-10-23T00:20:06Z",
        "link": "http://arxiv.org/abs/cs/0410060v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.5.2;H.3.1;H.3.4"
        ]
    },
    {
        "title": "An argumentative annotation schema for meeting discussions",
        "authors": [
            "Vincenzo Pallotta",
            "Hatem Ghorbel",
            "Patrick Ruch",
            "Giovanni Coray"
        ],
        "summary": "In this article, we are interested in the annotation of transcriptions of human-human dialogue taken from meeting records. We first propose a meeting content model where conversational acts are interpreted with respect to their argumentative force and their role in building the argumentative structure of the meeting discussion. Argumentation in dialogue describes the way participants take part in the discussion and argue their standpoints. Then, we propose an annotation scheme based on such an argumentative dialogue model as well as the evaluation of its adequacy. The obtained higher-level semantic annotations are exploited in the conceptual indexing of the information contained in meeting discussions.",
        "published": "2004-10-25T01:38:07Z",
        "link": "http://arxiv.org/abs/cs/0410061v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "cs.IR",
            "H.3.1;I.7.2;H.5.1"
        ]
    },
    {
        "title": "Automatic Keyword Extraction from Spoken Text. A Comparison of two   Lexical Resources: the EDR and WordNet",
        "authors": [
            "Lonneke van der Plas",
            "Vincenzo Pallotta",
            "Martin Rajman",
            "Hatem Ghorbel"
        ],
        "summary": "Lexical resources such as WordNet and the EDR electronic dictionary have been used in several NLP tasks. Probably, partly due to the fact that the EDR is not freely available, WordNet has been used far more often than the EDR. We have used both resources on the same task in order to make a comparison possible. The task is automatic assignment of keywords to multi-party dialogue episodes (i.e. thematically coherent stretches of spoken text). We show that the use of lexical resources in such a task results in slightly higher performances than the use of a purely statistically based method.",
        "published": "2004-10-25T01:50:03Z",
        "link": "http://arxiv.org/abs/cs/0410062v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "cs.IR",
            "H.3.1;H.3.3;I.5.3;I.7.3"
        ]
    },
    {
        "title": "A Search Relevancy Tuning Method Using Expert Results Content Evaluation",
        "authors": [
            "Boris Mark Tylevich"
        ],
        "summary": "The article presents an online relevancy tuning method using explicit user feedback. The author developed and tested a method of words' weights modification based on search result evaluation by user. User decides whether the result is useful or not after inspecting the full result content. The experiment proved that the constantly accumulated words weights base leads to better search quality in a specified data domain. The author also suggested future improvements of the method.",
        "published": "2004-11-08T20:49:42Z",
        "link": "http://arxiv.org/abs/cs/0411026v1",
        "categories": [
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "CDN: Content Distribution Network",
        "authors": [
            "Gang Peng"
        ],
        "summary": "Internet evolves and operates largely without a central coordination, the lack of which was and is critically important to the rapid growth and evolution of Internet. However, the lack of management in turn makes it very difficult to guarantee proper performance and to deal systematically with performance problems. Meanwhile, the available network bandwidth and server capacity continue to be overwhelmed by the skyrocketing Internet utilization and the accelerating growth of bandwidth intensive content. As a result, Internet service quality perceived by customers is largely unpredictable and unsatisfactory. Content Distribution Network (CDN) is an effective approach to improve Internet service quality. CDN replicates the content from the place of origin to the replica servers scattered over the Internet and serves a request from a replica server close to where the request originates. In this paper, we first give an overview about CDN. We then present the critical issues involved in designing and implementing an effective CDN and survey the approaches proposed in literature to address these problems. An example of CDN is described to show how a real commercial CDN operates. After this, we present a scheme that provides fast service location for peer-to-peer systems, a special type of CDN with no infrastructure support. We conclude with a brief projection about CDN.",
        "published": "2004-11-18T23:16:08Z",
        "link": "http://arxiv.org/abs/cs/0411069v1",
        "categories": [
            "cs.NI",
            "cs.IR",
            "C.2.4 Distributed Systems;H.3.4 Systems and Software"
        ]
    },
    {
        "title": "Building Chinese Lexicons from Scratch by Unsupervised Short Document   Self-Segmentation",
        "authors": [
            "Daniel Gayo-Avello"
        ],
        "summary": "Chinese text segmentation is a well-known and difficult problem. On one side, there is not a simple notion of \"word\" in Chinese language making really hard to implement rule-based systems to segment written texts, thus lexicons and statistical information are usually employed to achieve such a task. On the other side, any piece of Chinese text usually includes segments present neither in the lexicons nor in the training data. Even worse, such unseen sequences can be segmented into a number of totally unrelated words making later processing phases difficult. For instance, using a lexicon-based system the sequence ???(Baluozuo, Barroso, current president-designate of the European Commission) can be segmented into ?(ba, to hope, to wish) and ??(luozuo, an undefined word) changing completely the meaning of the sentence. A new and extremely simple algorithm specially suited to work over short Chinese documents is introduced. This new algorithm performs text \"self-segmentation\" producing results comparable to those achieved by native speakers without using either lexicons or any statistical information beyond the obtained from the input text. Furthermore, it is really robust for finding new \"words\", especially proper nouns, and it is well suited to build lexicons from scratch. Some preliminary results are provided in addition to examples of its employment.",
        "published": "2004-11-20T00:22:30Z",
        "link": "http://arxiv.org/abs/cs/0411074v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Ranking Pages by Topology and Popularity within Web Sites",
        "authors": [
            "Jose Borges",
            "Mark Levene"
        ],
        "summary": "We compare two link analysis ranking methods of web pages in a site. The first, called Site Rank, is an adaptation of PageRank to the granularity of a web site and the second, called Popularity Rank, is based on the frequencies of user clicks on the outlinks in a page that are captured by navigation sessions of users through the web site. We ran experiments on artificially created web sites of different sizes and on two real data sets, employing the relative entropy to compare the distributions of the two ranking methods. For the real data sets we also employ a nonparametric measure, called Spearman's footrule, which we use to compare the top-ten web pages ranked by the two methods. Our main result is that the distributions of the Popularity Rank and Site Rank are surprisingly close to each other, implying that the topology of a web site is very instrumental in guiding users through the site. Thus, in practice, the Site Rank provides a reasonable first order approximation of the aggregate behaviour of users within a web site given by the Popularity Rank.",
        "published": "2004-12-01T13:22:47Z",
        "link": "http://arxiv.org/abs/cs/0412002v3",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Human-Level Performance on Word Analogy Questions by Latent Relational   Analysis",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, machine translation, and information retrieval. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason/stone is analogous to the pair carpenter/wood. Past work on semantic similarity measures has mainly been concerned with attributional similarity. Recently the Vector Space Model (VSM) of information retrieval has been adapted to the task of measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) the patterns are derived automatically from the corpus (they are not predefined), (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data (it is also used this way in Latent Semantic Analysis), and (3) automatically generated synonyms are used to explore reformulations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying noun-modifier relations, LRA achieves similar gains over the VSM, while using a smaller corpus.",
        "published": "2004-12-06T21:50:18Z",
        "link": "http://arxiv.org/abs/cs/0412024v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "The Google Similarity Distance",
        "authors": [
            "Rudi Cilibrasi",
            "Paul M. B. Vitanyi"
        ],
        "summary": "Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers the equivalent of `society' is `database,' and the equivalent of `use' is `way to search the database.' We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts we use the world-wide-web as database, and Google as search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the world-wide-web using Google page counts. The world-wide-web is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies, and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87% with the expert crafted WordNet categories.",
        "published": "2004-12-21T16:05:36Z",
        "link": "http://arxiv.org/abs/cs/0412098v3",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "Generalized Strong Preservation by Abstract Interpretation",
        "authors": [
            "Francesco Ranzato",
            "Francesco Tapparo"
        ],
        "summary": "Standard abstract model checking relies on abstract Kripke structures which approximate concrete models by gluing together indistinguishable states, namely by a partition of the concrete state space. Strong preservation for a specification language L encodes the equivalence of concrete and abstract model checking of formulas in L. We show how abstract interpretation can be used to design abstract models that are more general than abstract Kripke structures. Accordingly, strong preservation is generalized to abstract interpretation-based models and precisely related to the concept of completeness in abstract interpretation. The problem of minimally refining an abstract model in order to make it strongly preserving for some language L can be formulated as a minimal domain refinement in abstract interpretation in order to get completeness w.r.t. the logical/temporal operators of L. It turns out that this refined strongly preserving abstract model always exists and can be characterized as a greatest fixed point. As a consequence, some well-known behavioural equivalences, like bisimulation, simulation and stuttering, and their corresponding partition refinement algorithms can be elegantly characterized in abstract interpretation as completeness properties and refinements.",
        "published": "2004-01-21T14:28:32Z",
        "link": "http://arxiv.org/abs/cs/0401016v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.2.4; F.3.1; F.3.2"
        ]
    },
    {
        "title": "A correct, precise and efficient integration of set-sharing, freeness   and linearity for the analysis of finite and rational tree languages",
        "authors": [
            "Patricia M. Hill",
            "Enea Zaffanella",
            "Roberto Bagnara"
        ],
        "summary": "It is well-known that freeness and linearity information positively interact with aliasing information, allowing both the precision and the efficiency of the sharing analysis of logic programs to be improved. In this paper we present a novel combination of set-sharing with freeness and linearity information, which is characterized by an improved abstract unification operator. We provide a new abstraction function and prove the correctness of the analysis for both the finite tree and the rational tree cases. Moreover, we show that the same notion of redundant information as identified in (Bagnara et al. 2002; Zaffanella et al. 2002) also applies to this abstract domain combination: this allows for the implementation of an abstract unification operator running in polynomial time and achieving the same precision on all the considered observable properties.",
        "published": "2004-01-26T10:14:21Z",
        "link": "http://arxiv.org/abs/cs/0401021v1",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "Enhanced sharing analysis techniques: a comprehensive evaluation",
        "authors": [
            "Roberto Bagnara",
            "Enea Zaffanella",
            "Patricia M. Hill"
        ],
        "summary": "Sharing, an abstract domain developed by D. Jacobs and A. Langen for the analysis of logic programs, derives useful aliasing information. It is well-known that a commonly used core of techniques, such as the integration of Sharing with freeness and linearity information, can significantly improve the precision of the analysis. However, a number of other proposals for refined domain combinations have been circulating for years. One feature that is common to these proposals is that they do not seem to have undergone a thorough experimental evaluation even with respect to the expected precision gains. In this paper we experimentally evaluate: helping Sharing with the definitely ground variables found using Pos, the domain of positive Boolean formulas; the incorporation of explicit structural information; a full implementation of the reduced product of Sharing and Pos; the issue of reordering the bindings in the computation of the abstract mgu; an original proposal for the addition of a new mode recording the set of variables that are deemed to be ground or free; a refined way of using linearity to improve the analysis; the recovery of hidden information in the combination of Sharing with freeness information. Finally, we discuss the issue of whether tracking compoundness allows the computation of more sharing information.",
        "published": "2004-01-26T10:35:43Z",
        "link": "http://arxiv.org/abs/cs/0401022v1",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "A system for reflection in C++",
        "authors": [
            "Duraid Madina",
            "Russell K. Standish"
        ],
        "summary": "Object-oriented programming languages such as Java and Objective C have become popular for implementing agent-based and other object-based simulations since objects in those languages can {\\em reflect} (i.e. make runtime queries of an object's structure). This allows, for example, a fairly trivial {\\em serialisation} routine (conversion of an object into a binary representation that can be stored or passed over a network) to be written. However C++ does not offer this ability, as type information is thrown away at compile time. Yet C++ is often a preferred development environment, whether for performance reasons or for its expressive features such as operator overloading.   In this paper, we present the {\\em Classdesc} system which brings many of the benefits of object reflection to C++.",
        "published": "2004-01-27T03:29:16Z",
        "link": "http://arxiv.org/abs/cs/0401024v1",
        "categories": [
            "cs.PL",
            "D.1.5;D.2.3"
        ]
    },
    {
        "title": "The UPLNC Compiler: Design and Implementation",
        "authors": [
            "Evgueniy Vitchev"
        ],
        "summary": "The implementation of the compiler of the UPLNC language is presented with a full source code listing.",
        "published": "2004-02-18T06:30:12Z",
        "link": "http://arxiv.org/abs/cs/0402043v1",
        "categories": [
            "cs.PL",
            "D.3.4"
        ]
    },
    {
        "title": "Transformation Rules for Locally Stratified Constraint Logic Programs",
        "authors": [
            "Fabio Fioravanti",
            "Alberto Pettorossi",
            "Maurizio Proietti"
        ],
        "summary": "We propose a set of transformation rules for constraint logic programs with negation. We assume that every program is locally stratified and, thus, it has a unique perfect model. We give sufficient conditions which ensure that the proposed set of transformation rules preserves the perfect model of the programs. Our rules extend in some respects the rules for logic programs and constraint logic programs already considered in the literature and, in particular, they include a rule for unfolding a clause with respect to a negative literal.",
        "published": "2004-02-20T14:09:02Z",
        "link": "http://arxiv.org/abs/cs/0402048v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "A Tribute to Alain Colmerauer",
        "authors": [
            "Jacques Cohen"
        ],
        "summary": "The paper describes the contributions of Alain Colmerauer to the areas of logic programs (LP) and constraint logic programs (CLP).",
        "published": "2004-02-25T19:00:31Z",
        "link": "http://arxiv.org/abs/cs/0402058v1",
        "categories": [
            "cs.PL",
            "D 3.2"
        ]
    },
    {
        "title": "Polymorphic lemmas and definitions in Lambda Prolog and Twelf",
        "authors": [
            "Andrew W. Appel",
            "Amy P. Felty"
        ],
        "summary": "Lambda Prolog is known to be well-suited for expressing and implementing logics and inference systems. We show that lemmas and definitions in such logics can be implemented with a great economy of expression. We encode a higher-order logic using an encoding that maps both terms and types of the object logic (higher-order logic) to terms of the metalanguage (Lambda Prolog). We discuss both the Terzo and Teyjus implementations of Lambda Prolog. We also encode the same logic in Twelf and compare the features of these two metalanguages for our purposes.",
        "published": "2004-03-09T02:38:37Z",
        "link": "http://arxiv.org/abs/cs/0403010v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1; D.2.4; I.2.3; D.1.6"
        ]
    },
    {
        "title": "Specialization of Functional Logic Programs Based on Needed Narrowing",
        "authors": [
            "Maria Alpuente",
            "Michael Hanus",
            "Salvador Lucas",
            "German Vidal"
        ],
        "summary": "Many functional logic languages are based on narrowing, a unification-based goal-solving mechanism which subsumes the reduction mechanism of functional languages and the resolution principle of logic languages. Needed narrowing is an optimal evaluation strategy which constitutes the basis of modern (narrowing-based) lazy functional logic languages. In this work, we present the fundamentals of partial evaluation in such languages. We provide correctness results for partial evaluation based on needed narrowing and show that the nice properties of this strategy are essential for the specialization process. In particular, the structure of the original program is preserved by partial evaluation and, thus, the same evaluation strategy can be applied for the execution of specialized programs. This is in contrast to other partial evaluation schemes for lazy functional logic programs which may change the program structure in a negative way. Recent proposals for the partial evaluation of declarative multi-paradigm programs use (some form of) needed narrowing to perform computations at partial evaluation time. Therefore, our results constitute the basis for the correctness of such partial evaluators.",
        "published": "2004-03-09T15:43:00Z",
        "link": "http://arxiv.org/abs/cs/0403011v1",
        "categories": [
            "cs.PL",
            "D.1.1; D.1.6; D.3.4"
        ]
    },
    {
        "title": "A Comparative Study of Arithmetic Constraints on Integer Intervals",
        "authors": [
            "Krzysztof R. Apt",
            "Peter Zoeteweij"
        ],
        "summary": "We propose here a number of approaches to implement constraint propagation for arithmetic constraints on integer intervals. To this end we introduce integer interval arithmetic. Each approach is explained using appropriate proof rules that reduce the variable domains. We compare these approaches using a set of benchmarks.",
        "published": "2004-03-12T08:37:54Z",
        "link": "http://arxiv.org/abs/cs/0403016v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3"
        ]
    },
    {
        "title": "Integrating design synthesis and assembly of structured objects in a   visual design language",
        "authors": [
            "Omid Banyasad",
            "Philip T. Cox"
        ],
        "summary": "Computer Aided Design systems provide tools for building and manipulating models of solid objects. Some also provide access to programming languages so that parametrised designs can be expressed. There is a sharp distinction, therefore, between building models, a concrete graphical editing activity, and programming, an abstract, textual, algorithm-construction activity. The recently proposed Language for Structured Design (LSD) was motivated by a desire to combine the design and programming activities in one language. LSD achieves this by extending a visual logic programming language to incorporate the notions of solids and operations on solids. Here we investigate another aspect of the LSD approach; namely, that by using visual logic programming as the engine to drive the parametrised assembly of objects, we also gain the powerful symbolic problem-solving capability that is the forte of logic programming languages. This allows the designer/programmer to work at a higher level, giving declarative specifications of a design in order to obtain the design descriptions. Hence LSD integrates problem solving, design synthesis, and prototype assembly in a single homogeneous programming/design environment. We demonstrate this specification-to-final-assembly capability using the masterkeying problem for designing systems of locks and keys.",
        "published": "2004-03-21T20:30:58Z",
        "link": "http://arxiv.org/abs/cs/0403033v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; D.1.7; J.6"
        ]
    },
    {
        "title": "Phantom Types and Subtyping",
        "authors": [
            "Matthew Fluet",
            "Riccardo Pucella"
        ],
        "summary": "We investigate a technique from the literature, called the phantom-types technique, that uses parametric polymorphism, type constraints, and unification of polymorphic types to model a subtyping hierarchy. Hindley-Milner type systems, such as the one found in Standard ML, can be used to enforce the subtyping relation, at least for first-order values. We show that this technique can be used to encode any finite subtyping hierarchy (including hierarchies arising from multiple interface inheritance). We formally demonstrate the suitability of the phantom-types technique for capturing first-order subtyping by exhibiting a type-preserving translation from a simple calculus with bounded polymorphism to a calculus embodying the type system of SML.",
        "published": "2004-03-23T06:59:38Z",
        "link": "http://arxiv.org/abs/cs/0403034v3",
        "categories": [
            "cs.PL",
            "D.1.1; D.3.3; F.3.3"
        ]
    },
    {
        "title": "Schedulers and Redundancy for a Class of Constraint Propagation Rules",
        "authors": [
            "Sebastian Brand",
            "Krzysztof R. Apt"
        ],
        "summary": "We study here schedulers for a class of rules that naturally arise in the context of rule-based constraint programming. We systematically derive a scheduler for them from a generic iteration algorithm of [Apt 2000]. We apply this study to so-called membership rules of [Apt and Monfroy 2001]. This leads to an implementation that yields a considerably better performance for these rules than their execution as standard CHR rules. Finally, we show how redundant rules can be identified and how appropriately reduced sets of rules can be computed.",
        "published": "2004-03-23T11:42:45Z",
        "link": "http://arxiv.org/abs/cs/0403037v3",
        "categories": [
            "cs.DS",
            "cs.PL",
            "I.2.2; I.2.3; D.1.2; D.3.3; D.3.4"
        ]
    },
    {
        "title": "Delimited continuations in natural language: quantification and polarity   sensitivity",
        "authors": [
            "Chung-chieh Shan"
        ],
        "summary": "Making a linguistic theory is like making a programming language: one typically devises a type system to delineate the acceptable utterances and a denotational semantics to explain observations on their behavior. Via this connection, the programming language concept of delimited continuations can help analyze natural language phenomena such as quantification and polarity sensitivity. Using a logical metalanguage whose syntax includes control operators and whose semantics involves evaluation order, these analyses can be expressed in direct style rather than continuation-passing style, and these phenomena can be thought of as computational side effects.",
        "published": "2004-04-05T01:53:46Z",
        "link": "http://arxiv.org/abs/cs/0404006v1",
        "categories": [
            "cs.CL",
            "cs.PL",
            "D.3.3; J.5"
        ]
    },
    {
        "title": "A treatment of higher-order features in logic programming",
        "authors": [
            "Gopalan Nadathur"
        ],
        "summary": "The logic programming paradigm provides the basis for a new intensional view of higher-order notions. This view is realized primarily by employing the terms of a typed lambda calculus as representational devices and by using a richer form of unification for probing their structures. These additions have important meta-programming applications but they also pose non-trivial implementation problems. One issue concerns the machine representation of lambda terms suitable to their intended use: an adequate encoding must facilitate comparison operations over terms in addition to supporting the usual reduction computation. Another aspect relates to the treatment of a unification operation that has a branching character and that sometimes calls for the delaying of the solution of unification problems. A final issue concerns the execution of goals whose structures become apparent only in the course of computation. These various problems are exposed in this paper and solutions to them are described. A satisfactory representation for lambda terms is developed by exploiting the nameless notation of de Bruijn as well as explicit encodings of substitutions. Special mechanisms are molded into the structure of traditional Prolog implementations to support branching in unification and carrying of unification problems over other computation steps; a premium is placed in this context on exploiting determinism and on emulating usual first-order behaviour. An extended compilation model is presented that treats higher-order unification and also handles dynamically emergent goals. The ideas described here have been employed in the Teyjus implementation of the Lambda Prolog language, a fact that is used to obtain a preliminary assessment of their efficacy.",
        "published": "2004-04-07T17:26:09Z",
        "link": "http://arxiv.org/abs/cs/0404020v1",
        "categories": [
            "cs.PL",
            "D.3.2; D.3.3; D.3.4"
        ]
    },
    {
        "title": "NLOMJ--Natural Language Object Model in Java",
        "authors": [
            "Jiyou Jia"
        ],
        "summary": "In this paper we present NLOMJ--a natural language object model in Java with English as the experiment language. This modal describes the grammar elements of any permissible expression in a natural language and their complicated relations with each other with the concept \"Object\" in OOP(Object Oriented Programming). Directly mapped to the syntax and semantics of the natural language, it can be used in information retrieval as a linguistic method. Around the UML diagram of the NLOMJ the important classes(Sentence, Clause and Phrase) and their sub classes are introduced and their syntactic and semantic meanings are explained.",
        "published": "2004-04-21T06:30:28Z",
        "link": "http://arxiv.org/abs/cs/0404041v2",
        "categories": [
            "cs.CL",
            "cs.PL",
            "I.2.7; D.1.5"
        ]
    },
    {
        "title": "A General Framework For Lazy Functional Logic Programming With Algebraic   Polymorphic Types",
        "authors": [
            "Puri Arenas-Sanchez",
            "Mario Rodriguez-Artalejo"
        ],
        "summary": "We propose a general framework for first-order functional logic programming, supporting lazy functions, non-determinism and polymorphic datatypes whose data constructors obey a set C of equational axioms. On top of a given C, we specify a program as a set R of C-based conditional rewriting rules for defined functions. We argue that equational logic does not supply the proper semantics for such programs. Therefore, we present an alternative logic which includes C-based rewriting calculi and a notion of model. We get soundness and completeness for C-based rewriting w.r.t. models, existence of free models for all programs, and type preservation results. As operational semantics, we develop a sound and complete procedure for goal solving, which is based on the combination of lazy narrowing with unification modulo C. Our framework is quite expressive for many purposes, such as solving action and change problems, or realizing the GAMMA computation model.",
        "published": "2004-04-24T14:02:48Z",
        "link": "http://arxiv.org/abs/cs/0404050v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Multi-Threading And Message Communication In Qu-Prolog",
        "authors": [
            "Keith L. Clark",
            "Peter J. Robinson",
            "Richard Hagen"
        ],
        "summary": "This paper presents the multi-threading and internet message communication capabilities of Qu-Prolog. Message addresses are symbolic and the communications package provides high-level support that completely hides details of IP addresses and port numbers as well as the underlying TCP/IP transport layer. The combination of the multi-threads and the high level inter-thread message communications provide simple, powerful support for implementing internet distributed intelligent applications.",
        "published": "2004-04-25T03:49:44Z",
        "link": "http://arxiv.org/abs/cs/0404052v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Constraint Logic Programming with Hereditary Harrop Formula",
        "authors": [
            "Javier Leach",
            "Susana Nieva",
            "Mario Rodriguez-Artalejo"
        ],
        "summary": "Constraint Logic Programming (CLP) and Hereditary Harrop formulas (HH) are two well known ways to enhance the expressivity of Horn clauses. In this paper, we present a novel combination of these two approaches. We show how to enrich the syntax and proof theory of HH with the help of a given constraint system, in such a way that the key property of HH as a logic programming language (namely, the existence of uniform proofs) is preserved. We also present a procedure for goal solving, showing its soundness and completeness for computing answer constraints. As a consequence of this result, we obtain a new strong completeness theorem for CLP that avoids the need to build disjunctions of computed answers, as well as a more abstract formulation of a known completeness theorem for HH.",
        "published": "2004-04-26T13:22:43Z",
        "link": "http://arxiv.org/abs/cs/0404053v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Finite-Tree Analysis for Constraint Logic-Based Languages: The Complete   Unabridged Version",
        "authors": [
            "Roberto Bagnara",
            "Roberta Gori",
            "Patricia M. Hill",
            "Enea Zaffanella"
        ],
        "summary": "Logic languages based on the theory of rational, possibly infinite, trees have much appeal in that rational trees allow for faster unification (due to the safe omission of the occurs-check) and increased expressivity (cyclic terms can provide very efficient representations of grammars and other useful objects). Unfortunately, the use of infinite rational trees has problems. For instance, many of the built-in and library predicates are ill-defined for such trees and need to be supplemented by run-time checks whose cost may be significant. Moreover, some widely-used program analysis and manipulation techniques are correct only for those parts of programs working over finite trees. It is thus important to obtain, automatically, a knowledge of the program variables (the finite variables) that, at the program points of interest, will always be bound to finite terms. For these reasons, we propose here a new data-flow analysis, based on abstract interpretation, that captures such information.",
        "published": "2004-04-26T20:44:49Z",
        "link": "http://arxiv.org/abs/cs/0404055v2",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "Reduction Strategies in Lambda Term Normalization and their Effects on   Heap Usage",
        "authors": [
            "Xiaochu Qi"
        ],
        "summary": "Higher-order representations of objects such as programs, proofs, formulas and types have become important to many symbolic computation tasks. Systems that support such representations usually depend on the implementation of an intensional view of the terms of some variant of the typed lambda-calculus. Various notations have been proposed for lambda-terms to explicitly treat substitutions as basis for realizing such implementations. There are, however, several choices in the actual reduction strategies. The most common strategy utilizes such notations only implicitly via an incremental use of environments. This approach does not allow the smaller substitution steps to be intermingled with other operations of interest on lambda-terms. However, a naive strategy explicitly using such notations can also be costly: each use of the substitution propagation rules causes the creation of a new structure on the heap that is often discarded in the immediately following step. There is thus a tradeoff between these two approaches. This thesis describes the actual realization of the two approaches, discusses their tradeoffs based on this and, finally, offers an amalgamated approach that utilizes recursion in rewrite rule application but also suspends substitution operations where necessary.",
        "published": "2004-05-22T07:21:02Z",
        "link": "http://arxiv.org/abs/cs/0405075v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Higher-Order Concurrent Win32 Programming",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "We present a concurrent framework for Win32 programming based on Concurrent ML, a concurrent language with higher-order functions, static typing, lightweight threads and synchronous communication channels. The key points of the framework are the move from an event loop model to a threaded model for the processing of window messages, and the decoupling of controls notifications from the system messages. This last point allows us to derive a general way of writing controls that leads to easy composition, and can accommodate ActiveX Controls in a transparent way.",
        "published": "2004-05-23T18:56:57Z",
        "link": "http://arxiv.org/abs/cs/0405079v1",
        "categories": [
            "cs.PL",
            "D.1.1;D.3.3;H.5.2"
        ]
    },
    {
        "title": "Reactive Programming in Standard ML",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "Reactive systems are systems that maintain an ongoing interaction with their environment, activated by receiving input events from the environment and producing output events in response. Modern programming languages designed to program such systems use a paradigm based on the notions of instants and activations. We describe a library for Standard ML that provides basic primitives for programming reactive systems. The library is a low-level system upon which more sophisticated reactive behaviors can be built, which provides a convenient framework for prototyping extensions to existing reactive languages.",
        "published": "2004-05-23T19:09:11Z",
        "link": "http://arxiv.org/abs/cs/0405080v1",
        "categories": [
            "cs.PL",
            "D.1.1;D.3.3"
        ]
    },
    {
        "title": "Aspects de la Programmation d'Applications Win32 avec un Langage   Fonctionnel",
        "authors": [
            "Riccardo Pucella",
            "Erik Meijer",
            "Dino Oliva"
        ],
        "summary": "A useful programming language needs to support writing programs that take advantage of services and communication mechanisms supplied by the operating system. We examine the problem of programming native Win32 applications under Windows with Standard ML. We introduce an framework based on the IDL interface language et a minimal foreign-functions interface to explore the Win32 API et COM in the context of Standard ML.",
        "published": "2004-05-23T19:35:49Z",
        "link": "http://arxiv.org/abs/cs/0405082v1",
        "categories": [
            "cs.PL",
            "D.1.1;D.3.3;H.5.2"
        ]
    },
    {
        "title": "The Design of a COM-Oriented Module System",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "We present in this paper the preliminary design of a module system based on a notion of components such as they are found in COM. This module system is inspired from that of Standard ML, and features first-class instances of components, first-class interfaces, and interface-polymorphic functions, as well as allowing components to be both imported from the environment and exported to the environment using simple mechanisms. The module system automates the memory management of interfaces and hides the IUnknown interface and QueryInterface mechanisms from the programmer, favoring instead a higher-level approach to handling interfaces.",
        "published": "2004-05-23T19:51:01Z",
        "link": "http://arxiv.org/abs/cs/0405083v1",
        "categories": [
            "cs.PL",
            "D.2.2;D.3.3"
        ]
    },
    {
        "title": "A Framework for Interoperability",
        "authors": [
            "Kathleen Fisher",
            "Riccardo Pucella",
            "John Reppy"
        ],
        "summary": "Practical implementations of high-level languages must provide access to libraries and system services that have APIs specified in a low-level language (usually C). An important characteristic of such mechanisms is the foreign-interface policy that defines how to bridge the semantic gap between the high-level language and C. For example, IDL-based tools generate code to marshal data into and out of the high-level representation according to user annotations. The design space of foreign-interface policies is large and there are pros and cons to each approach. Rather than commit to a particular policy, we choose to focus on the problem of supporting a gamut of interoperability policies. In this paper, we describe a framework for language interoperability that is expressive enough to support very efficient implementations of a wide range of different foreign-interface policies. We describe two tools that implement substantially different policies on top of our framework and present benchmarks that demonstrate their efficiency.",
        "published": "2004-05-23T20:28:59Z",
        "link": "http://arxiv.org/abs/cs/0405084v1",
        "categories": [
            "cs.PL",
            "D.2.12;D.3.4"
        ]
    },
    {
        "title": "On the Expressive Power of First-Order Boolean Functions in PCF",
        "authors": [
            "Riccardo Pucella",
            "Prakash Panangaden"
        ],
        "summary": "Recent results of Bucciarelli show that the semilattice of degrees of parallelism of first-order boolean functions in PCF has both infinite chains and infinite antichains. By considering a simple subclass of Sieber's sequentiality relations, we identify levels in the semilattice and derive inexpressibility results concerning functions on different levels. This allows us to further explore the structure of the semilattice of degrees of parallelism: we identify semilattices characterized by simple level properties, and show the existence of new infinite hierarchies which are in a certain sense natural with respect to the levels.",
        "published": "2004-05-24T05:16:33Z",
        "link": "http://arxiv.org/abs/cs/0405085v1",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "High-Level Networking With Mobile Code And First Order AND-Continuations",
        "authors": [
            "Paul Tarau",
            "Veronica Dahl"
        ],
        "summary": "We describe a scheme for moving living code between a set of distributed processes coordinated with unification based Linda operations, and its application to building a comprehensive Logic programming based Internet programming framework. Mobile threads are implemented by capturing first order continuations in a compact data structure sent over the network. Code is fetched lazily from its original base turned into a server as the continuation executes at the remote site. Our code migration techniques, in combination with a dynamic recompilation scheme, ensure that heavily used code moves up smoothly on a speed hierarchy while volatile dynamic code is kept in a quickly updatable form. Among the examples, we describe how to build programmable client and server components (Web servers, in particular) and mobile agents.",
        "published": "2004-05-24T12:41:07Z",
        "link": "http://arxiv.org/abs/cs/0405088v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "CLAIRE: Combining Sets, Search And Rules To Better Express Algorithms",
        "authors": [
            "Yves Caseau",
            "Francois-Xavier Josset",
            "Francois Laburthe"
        ],
        "summary": "This paper presents a programming language which includes paradigms that are usually associated with declarative languages, such as sets, rules and search, into an imperative (functional) language. Although these paradigms are separately well known and are available under various programming environments, the originality of the CLAIRE language comes from the tight integration, which yields interesting run-time performances, and from the richness of this combination, which yields new ways in which to express complex algorithmic patterns with few elegant lines. To achieve the opposite goals of a high abstraction level (conciseness and readability) and run-time performance (CLAIRE is used as a C++ preprocessor), we have developed two kinds of compiler: first, a pattern pre-processor handles iterations over both concrete and abstract sets (data types and program fragments), in a completely user-extensible manner; secondly, an inference compiler transforms a set of logical rules into a set of functions (demons that are used through procedural attachment).",
        "published": "2004-05-24T17:31:12Z",
        "link": "http://arxiv.org/abs/cs/0405091v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Learning Hybrid Algorithms for Vehicle Routing Problems",
        "authors": [
            "Yves Caseau",
            "Glenn Silverstein",
            "Francois Laburthe"
        ],
        "summary": "This paper presents a generic technique for improving hybrid algorithms through the discovery of and tuning of meta-heuristics. The idea is to represent a family of push/pull heuristics that are based upon inserting and removing tasks in a current solution, with an algebra. We then let a learning algorithm search for the best possible algebraic term, which represents a hybrid algorithm for a given set of problems and an optimization criterion. In a previous paper, we described this algebra in detail and provided a set of preliminary results demonstrating the utility of this approach, using vehicle routing with time windows (VRPTW) as a domain example. In this paper we expand upon our results providing a more robust experimental framework and learning algorithms, and report on some new results using the standard Solomon benchmarks. In particular, we show that our learning algorithm is able to achieve results similar to the best-published algorithms using only a fraction of the CPU time. We also show that the automatic tuning of the best hybrid combination of such techniques yields a better solution than hand tuning, with considerably less effort.",
        "published": "2004-05-24T17:41:50Z",
        "link": "http://arxiv.org/abs/cs/0405092v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "A Coalgebraic Approach to Kleene Algebra with Tests",
        "authors": [
            "Hubie Chen",
            "Riccardo Pucella"
        ],
        "summary": "Kleene algebra with tests is an extension of Kleene algebra, the algebra of regular expressions, which can be used to reason about programs. We develop a coalgebraic theory of Kleene algebra with tests, along the lines of the coalgebraic theory of regular expressions based on deterministic automata. Since the known automata-theoretic presentation of Kleene algebra with tests does not lend itself to a coalgebraic theory, we define a new interpretation of Kleene algebra with tests expressions and a corresponding automata-theoretic presentation. One outcome of the theory is a coinductive proof principle, that can be used to establish equivalence of our Kleene algebra with tests expressions.",
        "published": "2004-05-26T16:49:44Z",
        "link": "http://arxiv.org/abs/cs/0405097v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1;I.1.1;I.1.3"
        ]
    },
    {
        "title": "Typing constraint logic programs",
        "authors": [
            "Francois Fages",
            "Emmanuel Coquery"
        ],
        "summary": "We present a prescriptive type system with parametric polymorphism and subtyping for constraint logic programs. The aim of this type system is to detect programming errors statically. It introduces a type discipline for constraint logic programs and modules, while maintaining the capabilities of performing the usual coercions between constraint domains, and of typing meta-programming predicates, thanks to the flexibility of subtyping. The property of subject reduction expresses the consistency of a prescriptive type system w.r.t. the execution model: if a program is \"well-typed\", then all derivations starting from a \"well-typed\" goal are again \"well-typed\". That property is proved w.r.t. the abstract execution model of constraint programming which proceeds by accumulation of constraints only, and w.r.t. an enriched execution model with type constraints for substitutions. We describe our implementation of the system for type checking and type inference. We report our experimental results on type checking ISO-Prolog, the (constraint) libraries of Sicstus Prolog and other Prolog programs.",
        "published": "2004-05-27T02:39:01Z",
        "link": "http://arxiv.org/abs/cs/0405100v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Worst-Case Groundness Analysis Using Definite Boolean Functions",
        "authors": [
            "Samir Genaim",
            "Michael Codish",
            "Jacob M. Howe"
        ],
        "summary": "This note illustrates theoretical worst-case scenarios for groundness analyses obtained through abstract interpretation over the abstract domains of definite (Def) and positive (Pos) Boolean functions. For Def, an example is given for which any Def-based abstract interpretation for groundness analysis follows a chain which is exponential in the number of argument positions as well as in the number of clauses but sub-exponential in the size of the program. For Pos, we strengthen a previous result by illustrating an example for which any Pos-based abstract interpretation for groundness analysis follows a chain which is exponential in the size of the program. It remains an open problem to determine if the worst case for Def is really as bad as that for Pos.",
        "published": "2004-05-27T02:56:34Z",
        "link": "http://arxiv.org/abs/cs/0405101v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "A Proof Theoretic Approach to Failure in Functional Logic Programming",
        "authors": [
            "Francisco Javier Lopez-Fraguas",
            "Jaime Sanchez-Hernandez"
        ],
        "summary": "How to extract negative information from programs is an important issue in logic programming. Here we address the problem for functional logic programs, from a proof-theoretic perspective. The starting point of our work is CRWL (Constructor based ReWriting Logic), a well established theoretical framework for functional logic programming, whose fundamental notion is that of non-strict non-deterministic function. We present a proof calculus, CRWLF, which is able to deduce negative information from CRWL-programs. In particular, CRWLF is able to prove finite failure of reduction within CRWL.",
        "published": "2004-05-27T03:08:30Z",
        "link": "http://arxiv.org/abs/cs/0405102v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Secure Prolog-Based Mobile Code",
        "authors": [
            "Seng Wai Loke",
            "Andrew Davison"
        ],
        "summary": "LogicWeb mobile code consists of Prolog-like rules embedded in Web pages, thereby adding logic programming behaviour to those pages. Since LogicWeb programs are downloaded from foreign hosts and executed locally, there is a need to protect the client from buggy or malicious code. A security model is crucial for making LogicWeb mobile code safe to execute. This paper presents such a model, which supports programs of varying trust levels by using different resource access policies. The implementation of the model derives from an extended operational semantics for the LogicWeb language, which provides a precise meaning of safety.",
        "published": "2004-06-07T07:01:53Z",
        "link": "http://arxiv.org/abs/cs/0406012v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "O(1) Reversible Tree Navigation Without Cycles",
        "authors": [
            "Richard A. O'Keefe"
        ],
        "summary": "Imperative programmers often use cyclically linked trees in order to achieve O(1) navigation time to neighbours. Some logic programmers believe that cyclic terms are necessary to achieve the same in logic-based languages. An old but little-known technique provides O(1) time and space navigation without cyclic links, in the form of reversible predicates. A small modification provides O(1) amortised time and space editing.",
        "published": "2004-06-07T14:04:48Z",
        "link": "http://arxiv.org/abs/cs/0406014v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Improving Prolog Programs: Refactoring for Prolog",
        "authors": [
            "Tom Schrijvers",
            "Alexander Serebrenik"
        ],
        "summary": "Refactoring is an established technique from the OO-community to restructure code: it aims at improving software readability, maintainability and extensibility. Although refactoring is not tied to the OO-paradigm in particular, its ideas have not been applied to Logic Programming until now.   This paper applies the ideas of refactoring to Prolog programs. A catalogue is presented listing refactorings classified according to scope. Some of the refactorings have been adapted from the OO-paradigm, while others have been specifically designed for Prolog. Also the discrepancy between intended and operational semantics in Prolog is addressed by some of the refactorings.   In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and the experience with applying \\vipress to a large Prolog legacy system is reported. Our main conclusion is that refactoring is not only a viable technique in Prolog but also a rather desirable one.",
        "published": "2004-06-16T16:55:55Z",
        "link": "http://arxiv.org/abs/cs/0406026v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.7; D.1.6"
        ]
    },
    {
        "title": "Non-Termination Inference of Logic Programs",
        "authors": [
            "Etienne Payet",
            "Fred Mesnard"
        ],
        "summary": "We present a static analysis technique for non-termination inference of logic programs. Our framework relies on an extension of the subsumption test, where some specific argument positions can be instantiated while others are generalized. We give syntactic criteria to statically identify such argument positions from the text of a program. Atomic left looping queries are generated bottom-up from selected subsets of the binary unfoldings of the program of interest. We propose a set of correct algorithms for automating the approach. Then, non-termination inference is tailored to attempt proofs of optimality of left termination conditions computed by a termination inference tool. An experimental evaluation is reported. When termination and non-termination analysis produce complementary results for a logic procedure, then with respect to the leftmost selection rule and the language used to describe sets of atomic queries, each analysis is optimal and together, they induce a characterization of the operational behavior of the logic procedure.",
        "published": "2004-06-22T11:44:07Z",
        "link": "http://arxiv.org/abs/cs/0406041v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Well-Definedness and Semantic Type-Checking in the Nested Relational   Calculus and XQuery",
        "authors": [
            "Jan Van den Bussche",
            "Dirk Van Gucht",
            "Stijn Vansummeren"
        ],
        "summary": "Two natural decision problems regarding the XML query language XQuery are well-definedness and semantic type-checking. We study these problems in the setting of a relational fragment of XQuery. We show that well-definedness and semantic type-checking are undecidable, even in the positive-existential case. Nevertheless, for a ``pure'' variant of XQuery, in which no identification is made between an item and the singleton containing that item, the problems become decidable. We also consider the analogous problems in the setting of the nested relational calculus.",
        "published": "2004-06-29T16:09:10Z",
        "link": "http://arxiv.org/abs/cs/0406060v1",
        "categories": [
            "cs.DB",
            "cs.PL"
        ]
    },
    {
        "title": "A Process Algebraic Approach to Concurrent and Distributed Quantum   Computation: Operational Semantics",
        "authors": [
            "Marie Lalire",
            "Philippe Jorrand"
        ],
        "summary": "Full formal descriptions of algorithms making use of quantum principles must take into account both quantum and classical computing components and assemble them so that they communicate and cooperate. Moreover, to model concurrent and distributed quantum computations, as well as quantum communication protocols, quantum to quantum communications which move qubits physically from one place to another must also be taken into account. Inspired by classical process algebras, which provide a framework for modeling cooperating computations, a process algebraic notation is defined, named QPAlg for Quantum Process Algebra, which provides a homogeneous style to formal descriptions of concurrent and distributed computations comprising both quantum and classical parts. On the quantum side, QPAlg provides quantum variables, operations on quantum variables (unitary operators and measurement observables), as well as new forms of communications involving the quantum world. The operational semantics makes sure that these quantum objects, operations and communications operate according to the postulates of quantum mechanics.",
        "published": "2004-07-01T08:59:53Z",
        "link": "http://arxiv.org/abs/quant-ph/0407005v1",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "Exploiting Semidefinite Relaxations in Constraint Programming",
        "authors": [
            "Willem Jan van Hoeve"
        ],
        "summary": "Constraint programming uses enumeration and search tree pruning to solve combinatorial optimization problems. In order to speed up this solution process, we investigate the use of semidefinite relaxations within constraint programming. In principle, we use the solution of a semidefinite relaxation to guide the traversal of the search tree, using a limited discrepancy search strategy. Furthermore, a semidefinite relaxation produces a bound for the solution value, which is used to prune parts of the search tree. Experimental results on stable set and maximum clique problem instances show that constraint programming can indeed greatly benefit from semidefinite relaxations.",
        "published": "2004-07-16T14:19:16Z",
        "link": "http://arxiv.org/abs/cs/0407041v1",
        "categories": [
            "cs.DM",
            "cs.PL",
            "G.1.6; G.2.2; D.3.3"
        ]
    },
    {
        "title": "A Hyper-Arc Consistency Algorithm for the Soft Alldifferent Constraint",
        "authors": [
            "Willem Jan van Hoeve"
        ],
        "summary": "This paper presents an algorithm that achieves hyper-arc consistency for the soft alldifferent constraint. To this end, we prove and exploit the equivalence with a minimum-cost flow problem. Consistency of the constraint can be checked in O(nm) time, and hyper-arc consistency is achieved in O(m) time, where n is the number of variables involved and m is the sum of the cardinalities of the domains. It improves a previous method that did not ensure hyper-arc consistency.",
        "published": "2004-07-16T14:44:21Z",
        "link": "http://arxiv.org/abs/cs/0407043v1",
        "categories": [
            "cs.PL",
            "D.3.2; D.3.3; G.2.2"
        ]
    },
    {
        "title": "The First-Order Theory of Sets with Cardinality Constraints is Decidable",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We show that the decidability of the first-order theory of the language that combines Boolean algebras of sets of uninterpreted elements with Presburger arithmetic operations. We thereby disprove a recent conjecture that this theory is undecidable. Our language allows relating the cardinalities of sets to the values of integer variables, and can distinguish finite and infinite sets. We use quantifier elimination to show the decidability and obtain an elementary upper bound on the complexity.   Precise program analyses can use our decidability result to verify representation invariants of data structures that use an integer field to represent the number of stored elements.",
        "published": "2004-07-17T04:22:39Z",
        "link": "http://arxiv.org/abs/cs/0407045v2",
        "categories": [
            "cs.LO",
            "cs.PL"
        ]
    },
    {
        "title": "Roles Are Really Great!",
        "authors": [
            "Viktor Kuncak",
            "Patrick Lam",
            "Martin Rinard"
        ],
        "summary": "We present a new role system for specifying changing referencing relationships of heap objects. The role of an object depends, in large part, on its aliasing relationships with other objects, with the role of each object changing as its aliasing relationships change. Roles therefore capture important object and data structure properties and provide useful information about how the actions of the program interact with these properties. Our role system enables the programmer to specify the legal aliasing relationships that define the set of roles that objects may play, the roles of procedure parameters and object fields, and the role changes that procedures perform while manipulating objects. We present an interprocedural, compositional, and context-sensitive role analysis algorithm that verifies that a program respects the role constraints.",
        "published": "2004-08-05T03:02:01Z",
        "link": "http://arxiv.org/abs/cs/0408013v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2"
        ]
    },
    {
        "title": "Typestate Checking and Regular Graph Constraints",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We introduce regular graph constraints and explore their decidability properties. The motivation for regular graph constraints is 1) type checking of changing types of objects in the presence of linked data structures, 2) shape analysis techniques, and 3) generalization of similar constraints over trees and grids. We define a subclass of graphs called heaps as an abstraction of the data structures that a program constructs during its execution. We prove that determining the validity of implication for regular graph constraints over the class of heaps is undecidable. We show undecidability by exhibiting a characterization of certain \"corresponder graphs\" in terms of presence and absence of homomorphisms to a finite number of fixed graphs. The undecidability of implication of regular graph constraints implies that there is no algorithm that will verify that procedure preconditions are met or that the invariants are maintained when these properties are expressed in any specification language at least as expressive as regular graph constraints.",
        "published": "2004-08-05T03:41:24Z",
        "link": "http://arxiv.org/abs/cs/0408014v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On the Theory of Structural Subtyping",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We show that the first-order theory of structural subtyping of non-recursive types is decidable. Let $\\Sigma$ be a language consisting of function symbols (representing type constructors) and $C$ a decidable structure in the relational language $L$ containing a binary relation $\\leq$. $C$ represents primitive types; $\\leq$ represents a subtype ordering. We introduce the notion of $\\Sigma$-term-power of $C$, which generalizes the structure arising in structural subtyping. The domain of the $\\Sigma$-term-power of $C$ is the set of $\\Sigma$-terms over the set of elements of $C$. We show that the decidability of the first-order theory of $C$ implies the decidability of the first-order theory of the $\\Sigma$-term-power of $C$. Our decision procedure makes use of quantifier elimination for term algebras and Feferman-Vaught theorem. Our result implies the decidability of the first-order theory of structural subtyping of non-recursive types.",
        "published": "2004-08-05T04:12:25Z",
        "link": "http://arxiv.org/abs/cs/0408015v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On Role Logic",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We present role logic, a notation for describing properties of relational structures in shape analysis, databases, and knowledge bases. We construct role logic using the ideas of de Bruijn's notation for lambda calculus, an encoding of first-order logic in lambda calculus, and a simple rule for implicit arguments of unary and binary predicates. The unrestricted version of role logic has the expressive power of first-order logic with transitive closure. Using a syntactic restriction on role logic formulas, we identify a natural fragment RL^2 of role logic. We show that the RL^2 fragment has the same expressive power as two-variable logic with counting C^2 and is therefore decidable. We present a translation of an imperative language into the decidable fragment RL^2, which allows compositional verification of programs that manipulate relational structures. In addition, we show how RL^2 encodes boolean shape analysis constraints and an expressive description logic.",
        "published": "2004-08-05T23:01:20Z",
        "link": "http://arxiv.org/abs/cs/0408018v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On Generalized Records and Spatial Conjunction in Role Logic",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We have previously introduced role logic as a notation for describing properties of relational structures in shape analysis, databases and knowledge bases. A natural fragment of role logic corresponds to two-variable logic with counting and is therefore decidable. We show how to use role logic to describe open and closed records, as well the dual of records, inverse records. We observe that the spatial conjunction operation of separation logic naturally models record concatenation. Moreover, we show how to eliminate the spatial conjunction of formulas of quantifier depth one in first-order logic with counting. As a result, allowing spatial conjunction of formulas of quantifier depth one preserves the decidability of two-variable logic with counting. This result applies to two-variable role logic fragment as well. The resulting logic smoothly integrates type system and predicate calculus notation and can be viewed as a natural generalization of the notation for constraints arising in role analysis and similar shape analysis approaches.",
        "published": "2004-08-05T23:25:20Z",
        "link": "http://arxiv.org/abs/cs/0408019v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On Global Warming (Softening Global Constraints)",
        "authors": [
            "Willem Jan van Hoeve",
            "Gilles Pesant",
            "Louis-Martin Rousseau"
        ],
        "summary": "We describe soft versions of the global cardinality constraint and the regular constraint, with efficient filtering algorithms maintaining domain consistency. For both constraints, the softening is achieved by augmenting the underlying graph. The softened constraints can be used to extend the meta-constraint framework for over-constrained problems proposed by Petit, Regin and Bessiere.",
        "published": "2004-08-09T12:06:04Z",
        "link": "http://arxiv.org/abs/cs/0408023v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "D.3.2; D.3.3; G.2.2"
        ]
    },
    {
        "title": "Optimizing compilation of constraint handling rules in HAL",
        "authors": [
            "Christian Holzbaur",
            "Maria Garcia de la Banda",
            "Peter J. Stuckey",
            "Gregory J. Duck"
        ],
        "summary": "In this paper we discuss the optimizing compilation of Constraint Handling Rules (CHRs). CHRs are a multi-headed committed choice constraint language, commonly applied for writing incremental constraint solvers. CHRs are usually implemented as a language extension that compiles to the underlying language. In this paper we show how we can use different kinds of information in the compilation of CHRs in order to obtain access efficiency, and a better translation of the CHR rules into the underlying language, which in this case is HAL. The kinds of information used include the types, modes, determinism, functional dependencies and symmetries of the CHR constraints. We also show how to analyze CHR programs to determine this information about functional dependencies, symmetries and other kinds of information supporting optimizations.",
        "published": "2004-08-10T07:55:35Z",
        "link": "http://arxiv.org/abs/cs/0408025v1",
        "categories": [
            "cs.PL",
            "D.3.2 Constraint and logic langauges; D.3.4 Optimization"
        ]
    },
    {
        "title": "CHR Grammars",
        "authors": [
            "Henning Christiansen"
        ],
        "summary": "A grammar formalism based upon CHR is proposed analogously to the way Definite Clause Grammars are defined and implemented on top of Prolog. These grammars execute as robust bottom-up parsers with an inherent treatment of ambiguity and a high flexibility to model various linguistic phenomena. The formalism extends previous logic programming based grammars with a form of context-sensitive rules and the possibility to include extra-grammatical hypotheses in both head and body of grammar rules. Among the applications are straightforward implementations of Assumption Grammars and abduction under integrity constraints for language analysis. CHR grammars appear as a powerful tool for specification and implementation of language processors and may be proposed as a new standard for bottom-up grammars in logic programming.   To appear in Theory and Practice of Logic Programming (TPLP), 2005",
        "published": "2004-08-12T11:15:17Z",
        "link": "http://arxiv.org/abs/cs/0408027v1",
        "categories": [
            "cs.CL",
            "cs.PL"
        ]
    },
    {
        "title": "On computing the fixpoint of a set of boolean equations",
        "authors": [
            "Viktor Kuncak",
            "K. Rustan M. Leino"
        ],
        "summary": "This paper presents a method for computing a least fixpoint of a system of equations over booleans. The resulting computation can be significantly shorter than the result of iteratively evaluating the entire system until a fixpoint is reached.",
        "published": "2004-08-19T17:24:49Z",
        "link": "http://arxiv.org/abs/cs/0408045v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.4; D.3.1; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "CrocoPat 2.1 Introduction and Reference Manual",
        "authors": [
            "Dirk Beyer",
            "Andreas Noack"
        ],
        "summary": "CrocoPat is an efficient, powerful and easy-to-use tool for manipulating relations of arbitrary arity, including directed graphs. This manual provides an introduction to and a reference for CrocoPat and its programming language RML. It includes several application examples, in particular from the analysis of structural models of software systems.",
        "published": "2004-09-07T09:44:18Z",
        "link": "http://arxiv.org/abs/cs/0409009v1",
        "categories": [
            "cs.PL",
            "cs.DM",
            "cs.DS",
            "cs.SE",
            "D.1.6; G.2.2.a; E.1.d; D.2.7m"
        ]
    },
    {
        "title": "Using a hierarchy of Domain Specific Languages in complex software   systems design",
        "authors": [
            "V. S. Lugovsky"
        ],
        "summary": "A new design methodology is introduced, with some examples on building Domain Specific Languages hierarchy on top of Scheme.",
        "published": "2004-09-09T01:44:05Z",
        "link": "http://arxiv.org/abs/cs/0409016v1",
        "categories": [
            "cs.PL",
            "cs.DS",
            "cs.SE",
            "D.1.1;I.2.2;D.3.2;D.2.10"
        ]
    },
    {
        "title": "Automatic Generation of CHR Constraint Solvers",
        "authors": [
            "Slim Abdennadher",
            "Christophe Rigotti"
        ],
        "summary": "In this paper, we present a framework for automatic generation of CHR solvers given the logical specification of the constraints. This approach takes advantage of the power of tabled resolution for constraint logic programming, in order to check the validity of the rules. Compared to previous works where different methods for automatic generation of constraint solvers have been proposed, our approach enables the generation of more expressive rules (even recursive and splitting rules) that can be used directly as CHR solvers.",
        "published": "2004-09-14T20:55:23Z",
        "link": "http://arxiv.org/abs/cs/0409030v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.2; I.2.2"
        ]
    },
    {
        "title": "Checking modes of HAL programs",
        "authors": [
            "Maria Garcia de la Banda",
            "Warwick Harvey",
            "Kim Marriott",
            "Peter J. Stuckey",
            "Bart Demoen"
        ],
        "summary": "Recent constraint logic programming (CLP) languages, such as HAL and Mercury, require type, mode and determinism declarations for predicates. This information allows the generation of efficient target code and the detection of many errors at compile-time. Unfortunately, mode checking in such languages is difficult. One of the main reasons is that, for each predicate mode declaration, the compiler is required to appropriately re-order literals in the predicate's definition. The task is further complicated by the need to handle complex instantiations (which interact with type declarations and higher-order predicates) and automatic initialization of solver variables. Here we define mode checking for strongly typed CLP languages which require reordering of clause body literals. In addition, we show how to handle a simple case of polymorphic modes by using the corresponding polymorphic types.",
        "published": "2004-09-21T11:48:47Z",
        "link": "http://arxiv.org/abs/cs/0409038v1",
        "categories": [
            "cs.PL",
            "D.3.2; F.3.2"
        ]
    },
    {
        "title": "On Spatial Conjunction as Second-Order Logic",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "Spatial conjunction is a powerful construct for reasoning about dynamically allocated data structures, as well as concurrent, distributed and mobile computation. While researchers have identified many uses of spatial conjunction, its precise expressive power compared to traditional logical constructs was not previously known. In this paper we establish the expressive power of spatial conjunction. We construct an embedding from first-order logic with spatial conjunction into second-order logic, and more surprisingly, an embedding from full second order logic into first-order logic with spatial conjunction. These embeddings show that the satisfiability of formulas in first-order logic with spatial conjunction is equivalent to the satisfiability of formulas in second-order logic. These results explain the great expressive power of spatial conjunction and can be used to show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidable logic. As one example, we show that adding unrestricted spatial conjunction to two-variable logic leads to undecidability. On the side of decidability, the embedding into second-order logic immediately implies the decidability of first-order logic with a form of spatial conjunction over trees. The embedding into spatial conjunction also has useful consequences: because a restricted form of spatial conjunction in two-variable logic preserves decidability, we obtain that a correspondingly restricted form of second-order quantification in two-variable logic is decidable. The resulting language generalizes the first-order theory of boolean algebra over sets and is useful in reasoning about the contents of data structures in object-oriented languages.",
        "published": "2004-10-28T15:00:42Z",
        "link": "http://arxiv.org/abs/cs/0410073v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Synchronization from a Categorical Perspective",
        "authors": [
            "Krzysztof Worytkiewicz"
        ],
        "summary": "We introduce a notion of synchronization for higher-dimensional automata, based on coskeletons of cubical sets. Categorification transports this notion to the setting of categorical transition systems. We apply the results to study the semantics of an imperative programming language with message-passing.",
        "published": "2004-11-01T16:00:07Z",
        "link": "http://arxiv.org/abs/cs/0411001v1",
        "categories": [
            "cs.PL",
            "cs.DM"
        ]
    },
    {
        "title": "Intelligent search strategies based on adaptive Constraint Handling   Rules",
        "authors": [
            "Armin Wolf"
        ],
        "summary": "The most advanced implementation of adaptive constraint processing with Constraint Handling Rules (CHR) allows the application of intelligent search strategies to solve Constraint Satisfaction Problems (CSP). This presentation compares an improved version of conflict-directed backjumping and two variants of dynamic backtracking with respect to chronological backtracking on some of the AIM instances which are a benchmark set of random 3-SAT problems. A CHR implementation of a Boolean constraint solver combined with these different search strategies in Java is thus being compared with a CHR implementation of the same Boolean constraint solver combined with chronological backtracking in SICStus Prolog. This comparison shows that the addition of ``intelligence'' to the search process may reduce the number of search steps dramatically. Furthermore, the runtime of their Java implementations is in most cases faster than the implementations of chronological backtracking. More specifically, conflict-directed backjumping is even faster than the SICStus Prolog implementation of chronological backtracking, although our Java implementation of CHR lacks the optimisations made in the SICStus Prolog system. To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2004-11-08T08:32:40Z",
        "link": "http://arxiv.org/abs/cs/0411016v2",
        "categories": [
            "cs.AI",
            "cs.PL",
            "I.1.4"
        ]
    },
    {
        "title": "A machine-independent port of the SR language run-time system to the   NetBSD operating system",
        "authors": [
            "Ignatios Souvatzis"
        ],
        "summary": "SR (synchronizing resources) is a PASCAL - style language enhanced with constructs for concurrent programming developed at the University of Arizona in the late 1980s. MPD (presented in Gregory Andrews' book about Foundations of Multithreaded, Parallel, and Distributed Programming) is its successor, providing the same language primitives with a different syntax. The run-time system (in theory, identical) of both languages provides the illusion of a multiprocessor machine on a single single- or multi- CPU Unix-like system or a (local area) network of Unix-like machines. Chair V of the Computer Science Department of the University of Bonn is operating a laboratory for a practical course in parallel programming consisting of computing nodes running NetBSD/arm, normally used via PVM, MPI etc. We are considering to offer SR and MPD for this, too. As the original language distributions are only targeted at a few commercial Unix systems, some porting effort is needed, outlined in the SR porting guide. The integrated POSIX threads support of NetBSD-2.0 should allow us to use library primitives provided for NetBSD's phtread system to implement the primitives needed by the SR run-time system, thus implementing 13 target CPUs at once and automatically making use of SMP on VAX, Alpha, PowerPC, Sparc, 32-bit Intel and 64 bit AMD CPUs.   This paper describes work in progress.",
        "published": "2004-11-10T12:39:12Z",
        "link": "http://arxiv.org/abs/cs/0411028v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.3.4"
        ]
    },
    {
        "title": "Jartege: a Tool for Random Generation of Unit Tests for Java Classes",
        "authors": [
            "Catherine Oriat"
        ],
        "summary": "This report presents Jartege, a tool which allows random generation of unit tests for Java classes specified in JML. JML (Java Modeling Language) is a specification language for Java which allows one to write invariants for classes, and pre- and postconditions for operations. As in the JML-JUnit tool, we use JML specifications on the one hand to eliminate irrelevant test cases, and on the other hand as a test oracle. Jartege randomly generates test cases, which consist of a sequence of constructor and method calls for the classes under test. The random aspect of the tool can be parameterized by associating weights to classes and operations, and by controlling the number of instances which are created for each class under test. The practical use of Jartege is illustrated by a small case study.",
        "published": "2004-12-03T12:19:16Z",
        "link": "http://arxiv.org/abs/cs/0412012v1",
        "categories": [
            "cs.PL",
            "ACM: D.2 ; D.2.5"
        ]
    },
    {
        "title": "An Efficient and Flexible Engine for Computing Fixed Points",
        "authors": [
            "Hai-Feng Guo",
            "Gopal Gupta"
        ],
        "summary": "An efficient and flexible engine for computing fixed points is critical for many practical applications. In this paper, we firstly present a goal-directed fixed point computation strategy in the logic programming paradigm. The strategy adopts a tabled resolution (or memorized resolution) to mimic the efficient semi-naive bottom-up computation. Its main idea is to dynamically identify and record those clauses that will lead to recursive variant calls, and then repetitively apply those alternatives incrementally until the fixed point is reached. Secondly, there are many situations in which a fixed point contains a large number or even infinite number of solutions. In these cases, a fixed point computation engine may not be efficient enough or feasible at all. We present a mode-declaration scheme which provides the capabilities to reduce a fixed point from a big solution set to a preferred small one, or from an infeasible infinite set to a finite one. The mode declaration scheme can be characterized as a meta-level operation over the original fixed point. We show the correctness of the mode declaration scheme. Thirdly, the mode-declaration scheme provides a new declarative method for dynamic programming, which is typically used for solving optimization problems. There is no need to define the value of an optimal solution recursively, instead, defining a general solution suffices. The optimal value as well as its corresponding concrete solution can be derived implicitly and automatically using a mode-directed fixed point computation engine. Finally, this fixed point computation engine has been successfully implemented in a commercial Prolog system. Experimental results are shown to indicate that the mode declaration improves both time and space performances in solving dynamic programming problems.",
        "published": "2004-12-09T22:59:37Z",
        "link": "http://arxiv.org/abs/cs/0412041v2",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Widening Operators for Weakly-Relational Numeric Abstractions (Extended   Abstract)",
        "authors": [
            "Roberto Bagnara",
            "Patricia M. Hill",
            "Elena Mazzi",
            "Enea Zaffanella"
        ],
        "summary": "We discuss the divergence problems recently identified in some extrapolation operators for weakly-relational numeric domains. We identify the cause of the divergences and point out that resorting to more concrete, syntactic domains can be avoided by researching suitable algorithms for the elimination of redundant constraints in the chosen representation.",
        "published": "2004-12-10T15:52:29Z",
        "link": "http://arxiv.org/abs/cs/0412043v1",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "Surface Triangulation -- The Metric Approach",
        "authors": [
            "Emil Saucan"
        ],
        "summary": "We embark in a program of studying the problem of better approximating surfaces by triangulations(triangular meshes) by considering the approximating triangulations as finite metric spaces and the target smooth surface as their Haussdorff-Gromov limit. This allows us to define in a more natural way the relevant elements, constants and invariants s.a. principal directions and principal values, Gaussian and Mean curvature, etc. By a \"natural way\" we mean an intrinsic, discrete, metric definitions as opposed to approximating or paraphrasing the differentiable notions. In this way we hope to circumvent computational errors and, indeed, conceptual ones, that are often inherent to the classical, \"numerical\" approach. In this first study we consider the problem of determining the Gaussian curvature of a polyhedral surface, by using the {\\em embedding curvature} in the sense of Wald (and Menger). We present two modalities of employing these definitions for the computation of Gaussian curvature.",
        "published": "2004-01-26T21:51:04Z",
        "link": "http://arxiv.org/abs/cs/0401023v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "math.MG",
            "G.1.2; I.4.7"
        ]
    },
    {
        "title": "A General Framework for Bounds for Higher-Dimensional Orthogonal Packing   Problems",
        "authors": [
            "Sandor P. Fekete",
            "Joerg Schepers"
        ],
        "summary": "Higher-dimensional orthogonal packing problems have a wide range of practical applications, including packing, cutting, and scheduling. In the context of a branch-and-bound framework for solving these packing problems to optimality, it is of crucial importance to have good and easy bounds for an optimal solution. Previous efforts have produced a number of special classes of such bounds. Unfortunately, some of these bounds are somewhat complicated and hard to generalize. We present a new approach for obtaining classes of lower bounds for higher-dimensional packing problems; our bounds improve and simplify several well-known bounds from previous literature. In addition, our approach provides an easy framework for proving correctness of new bounds.",
        "published": "2004-02-18T16:05:16Z",
        "link": "http://arxiv.org/abs/cs/0402044v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Polynomial-time computing over quadratic maps I: sampling in real   algebraic sets",
        "authors": [
            "Dima Grigoriev",
            "Dmitrii V. Pasechnik"
        ],
        "summary": "Given a quadratic map Q : K^n -> K^k defined over a computable subring D of a real closed field K, and a polynomial p(Y_1,...,Y_k) of degree d, we consider the zero set Z=Z(p(Q(X)),K^n) of the polynomial p(Q(X_1,...,X_n)). We present a procedure that computes, in (dn)^O(k) arithmetic operations in D, a set S of (real univariate representations of) sampling points in K^n that intersects nontrivially each connected component of Z. As soon as k=o(n), this is faster than the standard methods that all have exponential dependence on n in the complexity. In particular, our procedure is polynomial-time for constant k. In contrast, the best previously known procedure (due to A.Barvinok) is only capable of deciding in n^O(k^2) operations the nonemptiness (rather than constructing sampling points) of the set Z in the case of p(Y)=sum_i Y_i^2 and homogeneous Q.   A by-product of our procedure is a bound (dn)^O(k) on the number of connected components of Z.   The procedure consists of exact symbolic computations in D and outputs vectors of algebraic numbers. It involves extending K by infinitesimals and subsequent limit computation by a novel procedure that utilizes knowledge of an explicit isomorphism between real algebraic sets.",
        "published": "2004-03-06T08:20:38Z",
        "link": "http://arxiv.org/abs/cs/0403008v3",
        "categories": [
            "cs.SC",
            "cs.CG",
            "math.AG",
            "I.1.2; G.1.5"
        ]
    },
    {
        "title": "Visualising the structure of architectural open spaces based on shape   analysis",
        "authors": [
            "Sanjay Rana",
            "Mike Batty"
        ],
        "summary": "This paper proposes the application of some well known two-dimensional geometrical shape descriptors for the visualisation of the structure of architectural open spaces. The paper demonstrates the use of visibility measures such as distance to obstacles and amount of visible space to calculate shape descriptors such as convexity and skeleton of the open space. The aim of the paper is to indicate a simple, objective and quantifiable approach to understand the structure of open spaces otherwise impossible due to the complex construction of built structures.",
        "published": "2004-04-22T13:42:48Z",
        "link": "http://arxiv.org/abs/cs/0404046v1",
        "categories": [
            "cs.CV",
            "cs.CG",
            "cs.DS",
            "I.3.5;I.4.8;I.5.2"
        ]
    },
    {
        "title": "A New Computational Framework For 2D Shape-Enclosing Contours",
        "authors": [
            "B. R. Schlei"
        ],
        "summary": "In this paper, a new framework for one-dimensional contour extraction from discrete two-dimensional data sets is presented. Contour extraction is important in many scientific fields such as digital image processing, computer vision, pattern recognition, etc. This novel framework includes (but is not limited to) algorithms for dilated contour extraction, contour displacement, shape skeleton extraction, contour continuation, shape feature based contour refinement and contour simplification. Many of the new techniques depend strongly on the application of a Delaunay tessellation. In order to demonstrate the versatility of this novel toolbox approach, the contour extraction techniques presented here are applied to scientific problems in material science, biology and heavy ion physics.",
        "published": "2004-05-07T03:25:03Z",
        "link": "http://arxiv.org/abs/cs/0405029v2",
        "categories": [
            "cs.CV",
            "cs.CG",
            "G.1.2"
        ]
    },
    {
        "title": "Computational Geometry Column 45",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The algorithm of Edelsbrunner for surface reconstruction by ``wrapping'' a set of points in R^3 is described.",
        "published": "2004-05-07T19:22:28Z",
        "link": "http://arxiv.org/abs/cs/0405034v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Single-Strip Triangulation of Manifolds with Arbitrary Topology",
        "authors": [
            "M. Gopi",
            "David Eppstein"
        ],
        "summary": "Triangle strips have been widely used for efficient rendering. It is NP-complete to test whether a given triangulated model can be represented as a single triangle strip, so many heuristics have been proposed to partition models into few long strips. In this paper, we present a new algorithm for creating a single triangle loop or strip from a triangulated model. Our method applies a dual graph matching algorithm to partition the mesh into cycles, and then merges pairs of cycles by splitting adjacent triangles when necessary. New vertices are introduced at midpoints of edges and the new triangles thus formed are coplanar with their parent triangles, hence the visual fidelity of the geometry is not changed. We prove that the increase in the number of triangles due to this splitting is 50% in the worst case, however for all models we tested the increase was less than 2%. We also prove tight bounds on the number of triangles needed for a single-strip representation of a model with holes on its boundary. Our strips can be used not only for efficient rendering, but also for other applications including the generation of space filling curves on a manifold of any arbitrary topology.",
        "published": "2004-05-10T14:31:40Z",
        "link": "http://arxiv.org/abs/cs/0405036v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "I.3.5; G.2.2"
        ]
    },
    {
        "title": "Convex Hull of Planar H-Polyhedra",
        "authors": [
            "Axel Simon",
            "Andy King"
        ],
        "summary": "Suppose $<A_i, \\vec{c}_i>$ are planar (convex) H-polyhedra, that is, $A_i \\in \\mathbb{R}^{n_i \\times 2}$ and $\\vec{c}_i \\in \\mathbb{R}^{n_i}$. Let $P_i = \\{\\vec{x} \\in \\mathbb{R}^2 \\mid A_i\\vec{x} \\leq \\vec{c}_i \\}$ and $n = n_1 + n_2$. We present an $O(n \\log n)$ algorithm for calculating an H-polyhedron $<A, \\vec{c}>$ with the smallest $P = \\{\\vec{x} \\in \\mathbb{R}^2 \\mid A\\vec{x} \\leq \\vec{c} \\}$ such that $P_1 \\cup P_2 \\subseteq P$.",
        "published": "2004-05-24T13:19:59Z",
        "link": "http://arxiv.org/abs/cs/0405089v1",
        "categories": [
            "cs.CG",
            "I.3.5; I.3.6; F.3.1"
        ]
    },
    {
        "title": "Really Straight Graph Drawings",
        "authors": [
            "Vida Dujmovic",
            "Matthew Suderman",
            "David R. Wood"
        ],
        "summary": "This paper has been withdrawn by the authors. It has been replaced by the papers: \"Drawings of Planar Graphs with Few Slopes and Segments\" (math/0606450) and \"Graph Drawings with Few Slopes\" (math/0606446).",
        "published": "2004-05-31T16:41:10Z",
        "link": "http://arxiv.org/abs/cs/0405112v2",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "Algorithms for Drawing Media",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We describe algorithms for drawing media, systems of states, tokens and actions that have state transition graphs in the form of partial cubes. Our algorithms are based on two principles: embedding the state transition graph in a low-dimensional integer lattice and projecting the lattice onto the plane, or drawing the medium as a planar graph with centrally symmetric faces.",
        "published": "2004-06-16T00:49:40Z",
        "link": "http://arxiv.org/abs/cs/0406020v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Layout of Graphs with Bounded Tree-Width",
        "authors": [
            "Vida Dujmovic",
            "Pat Morin",
            "David R. Wood"
        ],
        "summary": "A \\emph{queue layout} of a graph consists of a total order of the vertices, and a partition of the edges into \\emph{queues}, such that no two edges in the same queue are nested. The minimum number of queues in a queue layout of a graph is its \\emph{queue-number}. A \\emph{three-dimensional (straight-line grid) drawing} of a graph represents the vertices by points in $\\mathbb{Z}^3$ and the edges by non-crossing line-segments. This paper contributes three main results:   (1) It is proved that the minimum volume of a certain type of three-dimensional drawing of a graph $G$ is closely related to the queue-number of $G$. In particular, if $G$ is an $n$-vertex member of a proper minor-closed family of graphs (such as a planar graph), then $G$ has a $O(1)\\times O(1)\\times O(n)$ drawing if and only if $G$ has O(1) queue-number.   (2) It is proved that queue-number is bounded by tree-width, thus resolving an open problem due to Ganley and Heath (2001), and disproving a conjecture of Pemmaraju (1992). This result provides renewed hope for the positive resolution of a number of open problems in the theory of queue layouts.   (3) It is proved that graphs of bounded tree-width have three-dimensional drawings with O(n) volume. This is the most general family of graphs known to admit three-dimensional drawings with O(n) volume.   The proofs depend upon our results regarding \\emph{track layouts} and \\emph{tree-partitions} of graphs, which may be of independent interest.",
        "published": "2004-06-16T15:29:07Z",
        "link": "http://arxiv.org/abs/cs/0406024v1",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "Optimal Free-Space Management and Routing-Conscious Dynamic Placement   for Reconfigurable Devices",
        "authors": [
            "Ali Ahmadinia",
            "Christophe Bobda",
            "Sandor Fekete",
            "Juergen Teich",
            "Jan van der Veen"
        ],
        "summary": "We describe algorithmic results for two crucial aspects of allocating resources on computational hardware devices with partial reconfigurability. By using methods from the field of computational geometry, we derive a method that allows correct maintainance of free and occupied space of a set of n rectangular modules in optimal time Theta(n log n); previous approaches needed a time of O(n^2) for correct results and O(n) for heuristic results. We also show that finding an optimal feasible communication-conscious placement (which minimizes the total weighted Manhattan distance between the new module and existing demand points) can be computed in Theta(n log n). Both resulting algorithms are practically easy to implement and show convincing experimental behavior.",
        "published": "2004-06-18T13:29:46Z",
        "link": "http://arxiv.org/abs/cs/0406035v3",
        "categories": [
            "cs.DS",
            "cs.CG",
            "C.1.3; F.2.2"
        ]
    },
    {
        "title": "The Computational Complexity of Orientation Search Problems in   Cryo-Electron Microscopy",
        "authors": [
            "Taneli Mielikäinen",
            "Janne Ravantti",
            "Esko Ukkonen"
        ],
        "summary": "In this report we study the problem of determining three-dimensional orientations for noisy projections of randomly oriented identical particles. The problem is of central importance in the tomographic reconstruction of the density map of macromolecular complexes from electron microscope images and it has been studied intensively for more than 30 years.   We analyze the computational complexity of the orientation problem and show that while several variants of the problem are $NP$-hard, inapproximable and fixed-parameter intractable, some restrictions are polynomial-time approximable within a constant factor or even solvable in logarithmic space. The orientation search problem is formalized as a constrained line arrangement problem that is of independent interest. The negative complexity results give a partial justification for the heuristic methods used in orientation search, and the positive complexity results on the orientation search have some positive implications also to the problem of finding functionally analogous genes.   A preliminary version ``The Computational Complexity of Orientation Search in Cryo-Electron Microscopy'' appeared in Proc. ICCS 2004, LNCS 3036, pp. 231--238. Springer-Verlag 2004.",
        "published": "2004-06-23T14:28:17Z",
        "link": "http://arxiv.org/abs/cs/0406043v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.CV",
            "F.2.2; I.4.5; J.3"
        ]
    },
    {
        "title": "An algorithm for two-dimensional mesh generation based on the pinwheel   tiling",
        "authors": [
            "Pritam Ganguly",
            "Stephen A. Vavasis",
            "Katerina D. Papoulia"
        ],
        "summary": "We propose a new two-dimensional meshing algorithm called PINW able to generate meshes that accurately approximate the distance between any two domain points by paths composed only of cell edges. This technique is based on an extension of pinwheel tilings proposed by Radin and Conway. We prove that the algorithm produces triangles of bounded aspect ratio. This kind of mesh would be useful in cohesive interface finite element modeling when the crack propagation pathis an outcome of a simulation process.",
        "published": "2004-07-07T19:06:15Z",
        "link": "http://arxiv.org/abs/cs/0407018v2",
        "categories": [
            "cs.CG",
            "cs.NA",
            "F.2.2;G.1.8;J.2"
        ]
    },
    {
        "title": "Minimum Enclosing Polytope in High Dimensions",
        "authors": [
            "Rina Panigrahy"
        ],
        "summary": "We study the problem of covering a given set of $n$ points in a high, $d$-dimensional space by the minimum enclosing polytope of a given arbitrary shape. We present algorithms that work for a large family of shapes, provided either only translations and no rotations are allowed, or only rotation about a fixed point is allowed; that is, one is allowed to only scale and translate a given shape, or scale and rotate the shape around a fixed point. Our algorithms start with a polytope guessed to be of optimal size and iteratively moves it based on a greedy principle: simply move the current polytope directly towards any outside point till it touches the surface. For computing the minimum enclosing ball, this gives a simple greedy algorithm with running time $O(nd/\\eps)$ producing a ball of radius $1+\\eps$ times the optimal. This simple principle generalizes to arbitrary convex shape when only translations are allowed, requiring at most $O(1/\\eps^2)$ iterations. Our algorithm implies that {\\em core-sets} of size $O(1/\\eps^2)$ exist not only for minimum enclosing ball but also for any convex shape with a fixed orientation. A {\\em Core-Set} is a small subset of $poly(1/\\eps)$ points whose minimum enclosing polytope is almost as large as that of the original points. Although we are unable to combine our techniques for translations and rotations for general shapes, for the min-cylinder problem, we give an algorithm similar to the one in \\cite{HV03}, but with an improved running time of $2^{O(\\frac{1}{\\eps^2}\\log \\frac{1}{\\eps})} nd$.",
        "published": "2004-07-08T19:03:32Z",
        "link": "http://arxiv.org/abs/cs/0407020v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Track Layouts of Graphs",
        "authors": [
            "Vida Dujmovic",
            "Attila Por",
            "David R. Wood"
        ],
        "summary": "A \\emph{$(k,t)$-track layout} of a graph $G$ consists of a (proper) vertex $t$-colouring of $G$, a total order of each vertex colour class, and a (non-proper) edge $k$-colouring such that between each pair of colour classes no two monochromatic edges cross. This structure has recently arisen in the study of three-dimensional graph drawings. This paper presents the beginnings of a theory of track layouts. First we determine the maximum number of edges in a $(k,t)$-track layout, and show how to colour the edges given fixed linear orderings of the vertex colour classes. We then describe methods for the manipulation of track layouts. For example, we show how to decrease the number of edge colours in a track layout at the expense of increasing the number of tracks, and vice versa. We then study the relationship between track layouts and other models of graph layout, namely stack and queue layouts, and geometric thickness. One of our principle results is that the queue-number and track-number of a graph are tied, in the sense that one is bounded by a function of the other. As corollaries we prove that acyclic chromatic number is bounded by both queue-number and stack-number. Finally we consider track layouts of planar graphs. While it is an open problem whether planar graphs have bounded track-number, we prove bounds on the track-number of outerplanar graphs, and give the best known lower bound on the track-number of planar graphs.",
        "published": "2004-07-13T15:31:02Z",
        "link": "http://arxiv.org/abs/cs/0407033v1",
        "categories": [
            "cs.DM",
            "cs.CG"
        ]
    },
    {
        "title": "Unfolding Smooth Prismatoids",
        "authors": [
            "Nadia Benbernou",
            "Patricia Cahn",
            "Joseph O'Rourke"
        ],
        "summary": "We define a notion for unfolding smooth, ruled surfaces, and prove that every smooth prismatoid (the convex hull of two smooth curves lying in parallel planes), has a nonoverlapping \"volcano unfolding.\" These unfoldings keep the base intact, unfold the sides outward, splayed around the base, and attach the top to the tip of some side rib. Our result answers a question for smooth prismatoids whose analog for polyhedral prismatoids remains unsolved.",
        "published": "2004-07-28T20:21:31Z",
        "link": "http://arxiv.org/abs/cs/0407063v3",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Perfect Delaunay Polytopes and Perfect Inhomogeneous Forms",
        "authors": [
            "Robert Erdahl",
            "Andrei Ordine",
            "Konstantin Rybnikov"
        ],
        "summary": "A lattice Delaunay polytope D is called perfect if it has the property that there is a unique circumscribing ellipsoid with interior free of lattice points, and with the surface containing only those lattice points that are the vertices of D. An inhomogeneous quadratic form is called perfect if it is determined by such a circumscribing ''empty ellipsoid'' uniquely up to a scale factor. Perfect inhomogeneous forms are associated with perfect Delaunay polytopes in much the way that perfect homogeneous forms are associated with perfect point lattices. We have been able to construct some infinite sequences of perfect Delaunay polytopes, one perfect polytope in each successive dimension starting at some initial dimension; we have been able to construct an infinite number of such infinite sequences. Perfect Delaunay polytopes are intimately related to the theory of Delaunay polytopes, and to Voronoi's theory of lattice types.",
        "published": "2004-08-09T20:34:25Z",
        "link": "http://arxiv.org/abs/math/0408122v3",
        "categories": [
            "math.NT",
            "cs.CC",
            "cs.CG",
            "math.MG",
            "quant-ph",
            "Primary: 11H50 and 11H55. Secondary: 11H06, 52C07, 52C22"
        ]
    },
    {
        "title": "Fast Construction of Nets in Low Dimensional Metrics, and Their   Applications",
        "authors": [
            "Sariel Har-Peled",
            "Manor Mendel"
        ],
        "summary": "We present a near linear time algorithm for constructing hierarchical nets in finite metric spaces with constant doubling dimension. This data-structure is then applied to obtain improved algorithms for the following problems: Approximate nearest neighbor search, well-separated pair decomposition, compact representation scheme, doubling measure, and computation of the (approximate) Lipschitz constant of a function. In all cases, the running (preprocessing) time is near-linear and the space being used is linear.",
        "published": "2004-09-29T17:44:15Z",
        "link": "http://arxiv.org/abs/cs/0409057v3",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "An Example of Clifford Algebras Calculations with GiNaC",
        "authors": [
            "Vladimir V. Kisil"
        ],
        "summary": "This example of Clifford algebras calculations uses GiNaC (http://www.ginac.de/) library, which includes a support for generic Clifford algebra starting from version~1.3.0. Both symbolic and numeric calculation are possible and can be blended with other functions of GiNaC. This calculations was made for the paper math.CV/0410399.   Described features of GiNaC are already available at PyGiNaC (http://sourceforge.net/projects/pyginac/) and due to course should propagate into other software like GNU Octave (http://www.octave.org/), gTybalt (http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as their back-end.",
        "published": "2004-10-18T17:39:51Z",
        "link": "http://arxiv.org/abs/cs/0410044v5",
        "categories": [
            "cs.MS",
            "cs.CG",
            "cs.GR",
            "cs.SC"
        ]
    },
    {
        "title": "A 2-chain can interlock with a k-chain",
        "authors": [
            "Julie Glass",
            "Stefan Langerman",
            "Joseph O'Rourke",
            "Jack Snoeyink",
            "Jianyuan K. Zhong"
        ],
        "summary": "One of the open problems posed in [3] is: what is the minimal number k such that an open, flexible k-chain can interlock with a flexible 2-chain? In this paper, we establish the assumption behind this problem, that there is indeed some k that achieves interlocking. We prove that a flexible 2-chain can interlock with a flexible, open 16-chain.",
        "published": "2004-10-20T00:55:49Z",
        "link": "http://arxiv.org/abs/cs/0410052v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "A method to discern complexity in two-dimensional patterns generated by   coupled map lattices",
        "authors": [
            "Juan Sanchez",
            "Ricardo Lopez-Ruiz"
        ],
        "summary": "Complex patterns generated by the time evolution of a one-dimensional digitalized coupled map lattice are quantitatively analyzed. A method for discerning complexity among the different patterns is implemented. The quantitative results indicate two zones in parameter space where the dynamics shows the most complex patterns. These zones are located on the two edges of an absorbent region where the system displays spatio-temporal intermittency.",
        "published": "2004-10-28T16:01:14Z",
        "link": "http://arxiv.org/abs/nlin/0410062v1",
        "categories": [
            "nlin.PS",
            "cond-mat.dis-nn",
            "cs.CG",
            "math.DS",
            "nlin.CD",
            "q-bio.QM"
        ]
    },
    {
        "title": "Minimum Dilation Stars",
        "authors": [
            "David Eppstein",
            "Kevin A. Wortman"
        ],
        "summary": "The dilation of a Euclidean graph is defined as the ratio of distance in the graph divided by distance in R^d. In this paper we consider the problem of positioning the root of a star such that the dilation of the resulting star is minimal. We present a deterministic O(n log n)-time algorithm for evaluating the dilation of a given star; a randomized O(n log n) expected-time algorithm for finding an optimal center in R^d; and for the case d=2, a randomized O(n 2^(alpha(n)) log^2 n) expected-time algorithm for finding an optimal center among the input points.",
        "published": "2004-12-07T01:39:36Z",
        "link": "http://arxiv.org/abs/cs/0412025v3",
        "categories": [
            "cs.CG",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Quasiconvex Programming",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We define quasiconvex programming, a form of generalized linear programming in which one seeks the point minimizing the pointwise maximum of a collection of quasiconvex functions. We survey algorithms for solving quasiconvex programs either numerically or via generalizations of the dual simplex method from linear programming, and describe varied applications of this geometric optimization technique in meshing, scientific computation, information visualization, automated algorithm analysis, and robust statistics.",
        "published": "2004-12-10T22:50:50Z",
        "link": "http://arxiv.org/abs/cs/0412046v1",
        "categories": [
            "cs.CG",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Partitioning Regular Polygons into Circular Pieces II:Nonconvex   Partitions",
        "authors": [
            "Mirela Damian",
            "Joseph O'Rourke"
        ],
        "summary": "We explore optimal circular nonconvex partitions of regular k-gons. The circularity of a polygon is measured by its aspect ratio: the ratio of the radii of the smallest circumscribing circle to the largest inscribed disk. An optimal circular partition minimizes the maximum ratio over all pieces in the partition. We show that the equilateral triangle has an optimal 4-piece nonconvex partition, the square an optimal 13-piece nonconvex partition, and the pentagon has an optimal nonconvex partition with more than 20 thousand pieces. For hexagons and beyond, we provide a general algorithm that approaches optimality, but does not achieve it.",
        "published": "2004-12-21T05:41:11Z",
        "link": "http://arxiv.org/abs/cs/0412095v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Surface Triangulation -- The Metric Approach",
        "authors": [
            "Emil Saucan"
        ],
        "summary": "We embark in a program of studying the problem of better approximating surfaces by triangulations(triangular meshes) by considering the approximating triangulations as finite metric spaces and the target smooth surface as their Haussdorff-Gromov limit. This allows us to define in a more natural way the relevant elements, constants and invariants s.a. principal directions and principal values, Gaussian and Mean curvature, etc. By a \"natural way\" we mean an intrinsic, discrete, metric definitions as opposed to approximating or paraphrasing the differentiable notions. In this way we hope to circumvent computational errors and, indeed, conceptual ones, that are often inherent to the classical, \"numerical\" approach. In this first study we consider the problem of determining the Gaussian curvature of a polyhedral surface, by using the {\\em embedding curvature} in the sense of Wald (and Menger). We present two modalities of employing these definitions for the computation of Gaussian curvature.",
        "published": "2004-01-26T21:51:04Z",
        "link": "http://arxiv.org/abs/cs/0401023v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "math.MG",
            "G.1.2; I.4.7"
        ]
    },
    {
        "title": "An Algorithm for Transforming Color Images into Tactile Graphics",
        "authors": [
            "Artur Rataj"
        ],
        "summary": "This paper presents an algorithm that transforms color visual images, like photographs or paintings, into tactile graphics. In the algorithm, the edges of objects are detected and colors of the objects are estimated. Then, the edges and the colors are encoded into lines and textures in the output tactile image. Design of the method is substantiated by various qualities of haptic recognizing of images. Also, means of presentation of the tactile images in printouts are discussed. Example translated images are shown.",
        "published": "2004-04-08T14:07:05Z",
        "link": "http://arxiv.org/abs/cs/0404022v1",
        "categories": [
            "cs.GR",
            "I.4.0"
        ]
    },
    {
        "title": "Single-Strip Triangulation of Manifolds with Arbitrary Topology",
        "authors": [
            "M. Gopi",
            "David Eppstein"
        ],
        "summary": "Triangle strips have been widely used for efficient rendering. It is NP-complete to test whether a given triangulated model can be represented as a single triangle strip, so many heuristics have been proposed to partition models into few long strips. In this paper, we present a new algorithm for creating a single triangle loop or strip from a triangulated model. Our method applies a dual graph matching algorithm to partition the mesh into cycles, and then merges pairs of cycles by splitting adjacent triangles when necessary. New vertices are introduced at midpoints of edges and the new triangles thus formed are coplanar with their parent triangles, hence the visual fidelity of the geometry is not changed. We prove that the increase in the number of triangles due to this splitting is 50% in the worst case, however for all models we tested the increase was less than 2%. We also prove tight bounds on the number of triangles needed for a single-strip representation of a model with holes on its boundary. Our strips can be used not only for efficient rendering, but also for other applications including the generation of space filling curves on a manifold of any arbitrary topology.",
        "published": "2004-05-10T14:31:40Z",
        "link": "http://arxiv.org/abs/cs/0405036v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "I.3.5; G.2.2"
        ]
    },
    {
        "title": "Interactive visualization of higher dimensional data in a multiview   environment",
        "authors": [
            "Stanimire Tomov",
            "Michael McGuigan"
        ],
        "summary": "We develop multiple view visualization of higher dimensional data. Our work was chiefly motivated by the need to extract insight from four dimensional Quantum Chromodynamic (QCD) data. We develop visualization where multiple views, generally views of 3D projections or slices of a higher dimensional data, are tightly coupled not only by their specific order but also by a view synchronizing interaction style, and an internally defined interaction language. The tight coupling of the different views allows a fast and well-coordinated exploration of the data. In particular, the visualization allowed us to easily make consistency checks of the 4D QCD data and to infer the correctness of particle properties calculations. The software developed was also successfully applied in material studies, in particular studies of meteorite properties. Our implementation uses the VTK API. To handle a large number of views (slices/projections) and to still maintain good resolution, we use IBM T221 display (3840 X 2400 pixels).",
        "published": "2004-05-14T18:18:04Z",
        "link": "http://arxiv.org/abs/cs/0405048v1",
        "categories": [
            "cs.GR",
            "I.3.6; I.3.8; H.5.2"
        ]
    },
    {
        "title": "An Example of Clifford Algebras Calculations with GiNaC",
        "authors": [
            "Vladimir V. Kisil"
        ],
        "summary": "This example of Clifford algebras calculations uses GiNaC (http://www.ginac.de/) library, which includes a support for generic Clifford algebra starting from version~1.3.0. Both symbolic and numeric calculation are possible and can be blended with other functions of GiNaC. This calculations was made for the paper math.CV/0410399.   Described features of GiNaC are already available at PyGiNaC (http://sourceforge.net/projects/pyginac/) and due to course should propagate into other software like GNU Octave (http://www.octave.org/), gTybalt (http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as their back-end.",
        "published": "2004-10-18T17:39:51Z",
        "link": "http://arxiv.org/abs/cs/0410044v5",
        "categories": [
            "cs.MS",
            "cs.CG",
            "cs.GR",
            "cs.SC"
        ]
    },
    {
        "title": "Mathematical Analysis of Multi-Agent Systems",
        "authors": [
            "Kristina Lerman",
            "Aram Galstyan",
            "Tad Hogg"
        ],
        "summary": "We review existing approaches to mathematical modeling and analysis of multi-agent systems in which complex collective behavior arises out of local interactions between many simple agents. Though the behavior of an individual agent can be considered to be stochastic and unpredictable, the collective behavior of such systems can have a simple probabilistic description. We show that a class of mathematical models that describe the dynamics of collective behavior of multi-agent systems can be written down from the details of the individual agent controller. The models are valid for Markov or memoryless agents, in which each agents future state depends only on its present state and not any of the past states. We illustrate the approach by analyzing in detail applications from the robotics domain: collaboration and foraging in groups of robots.",
        "published": "2004-04-02T02:00:00Z",
        "link": "http://arxiv.org/abs/cs/0404002v1",
        "categories": [
            "cs.RO",
            "cs.MA",
            "I.2.9; I.2.11; I.6.5"
        ]
    },
    {
        "title": "Online Searching with an Autonomous Robot",
        "authors": [
            "Sandor P. Fekete",
            "Rolf Klein",
            "Andreas Nuechter"
        ],
        "summary": "We discuss online strategies for visibility-based searching for an object hidden behind a corner, using Kurt3D, a real autonomous mobile robot. This task is closely related to a number of well-studied problems. Our robot uses a three-dimensional laser scanner in a stop, scan, plan, go fashion for building a virtual three-dimensional environment. Besides planning trajectories and avoiding obstacles, Kurt3D is capable of identifying objects like a chair. We derive a practically useful and asymptotically optimal strategy that guarantees a competitive ratio of 2, which differs remarkably from the well-studied scenario without the need of stopping for surveying the environment. Our strategy is used by Kurt3D, documented in a separate video.",
        "published": "2004-04-16T21:46:15Z",
        "link": "http://arxiv.org/abs/cs/0404036v1",
        "categories": [
            "cs.RO",
            "cs.DS",
            "I.2.9"
        ]
    },
    {
        "title": "Field Geology with a Wearable Computer: First Results of the Cyborg   Astrobiologist System",
        "authors": [
            "Patrick C. McGuire",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Helge Ritter",
            "Markus Oesker",
            "Robert Haschke",
            "Joerg Ontrup"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",
        "published": "2004-09-15T15:38:59Z",
        "link": "http://arxiv.org/abs/cs/0409031v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.RO",
            "I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9"
        ]
    },
    {
        "title": "Neural Architectures for Robot Intelligence",
        "authors": [
            "H. Ritter",
            "J. J. Steil",
            "C. Noelker",
            "F. Roethling",
            "P. C. McGuire"
        ],
        "summary": "We argue that the direct experimental approaches to elucidate the architecture of higher brains may benefit from insights gained from exploring the possibilities and limits of artificial control architectures for robot systems. We present some of our recent work that has been motivated by that view and that is centered around the study of various aspects of hand actions since these are intimately linked with many higher cognitive abilities. As examples, we report on the development of a modular system for the recognition of continuous hand postures based on neural nets, the use of vision and tactile sensing for guiding prehensile movements of a multifingered hand, and the recognition and use of hand gestures for robot teaching.   Regarding the issue of learning, we propose to view real-world learning from the perspective of data mining and to focus more strongly on the imitation of observed actions instead of purely reinforcement-based exploration. As a concrete example of such an effort we report on the status of an ongoing project in our lab in which a robot equipped with an attention system with a neurally inspired architecture is taught actions by using hand gestures in conjunction with speech commands. We point out some of the lessons learnt from this system, and discuss how systems of this kind can contribute to the study of issues at the junction between natural and artificial cognitive systems.",
        "published": "2004-10-18T10:50:28Z",
        "link": "http://arxiv.org/abs/cs/0410042v1",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.HC",
            "cs.LG",
            "cs.NE",
            "q-bio.NC",
            "I.2.9; I.2.10; I.2.6; H.1.2; H.2.8; I.5.4"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: First Field Experience",
        "authors": [
            "Patrick C. McGuire",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Jose Antonio Rodriguez-Manfredi",
            "Javier Gomez-Elvira",
            "Helge Ritter",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist and field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: first, detection and accounting for shadows caused by 3D jagged edges in the outcrop; second, reincorporation of more sophisticated texture-analysis algorithms into the system; third, creation of hardware and software capabilities to control the camera's zoom lens in an intelligent manner; and fourth, development of algorithms for interpretation of complex geological scenery. Nonetheless, despite these technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",
        "published": "2004-10-27T09:14:01Z",
        "link": "http://arxiv.org/abs/cs/0410071v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "q-bio.NC",
            "I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9"
        ]
    },
    {
        "title": "Topological Navigation of Simulated Robots using Occupancy Grid",
        "authors": [
            "Richard Szabo"
        ],
        "summary": "Formerly I presented a metric navigation method in the Webots mobile robot simulator. The navigating Khepera-like robot builds an occupancy grid of the environment and explores the square-shaped room around with a value iteration algorithm. Now I created a topological navigation procedure based on the occupancy grid process. The extension by a skeletonization algorithm results a graph of important places and the connecting routes among them. I also show the significant time profit gained during the process.",
        "published": "2004-11-08T20:22:52Z",
        "link": "http://arxiv.org/abs/cs/0411022v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Design and Implementation of a General Decision-making Model in RoboCup   Simulation",
        "authors": [
            "Changda Wang",
            "Xianyi Chen",
            "Xibin Zhao",
            "Shiguang Ju"
        ],
        "summary": "The study of the collaboration, coordination and negotiation among different agents in a multi-agent system (MAS) has always been the most challenging yet popular in the research of distributed artificial intelligence. In this paper, we will suggest for RoboCup simulation, a typical MAS, a general decision-making model, rather than define a different algorithm for each tactic (e.g. ball handling, pass, shoot and interception, etc.) in soccer games as most RoboCup simulation teams did. The general decision-making model is based on two critical factors in soccer games: the vertical distance to the goal line and the visual angle for the goalpost. We have used these two parameters to formalize the defensive and offensive decisions in RoboCup simulation and the results mentioned above had been applied in NOVAURO, original name is UJDB, a RoboCup simulation team of Jiangsu University, whose decision-making model, compared with that of Tsinghua University, the world champion team in 2001, is a universal model and easier to be implemented.",
        "published": "2004-11-08T20:24:32Z",
        "link": "http://arxiv.org/abs/cs/0411023v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Space Robotics Part 2: Space-based Manipulators",
        "authors": [
            "Alex Ellery"
        ],
        "summary": "In this second of three short papers, I introduce some of the basic concepts of space robotics with an emphasis on some specific challenging areas of research that are peculiar to the application of robotics to space infrastructure development. The style of these short papers is pedagogical and the concepts in this paper are developed from fundamental manipulator robotics. This second paper considers the application of space manipulators to on-orbit servicing (OOS), an application which has considerable commercial application. I provide some background to the notion of robotic on-orbit servicing and explore how manipulator control algorithms may be modified to accommodate space manipulators which operate in the micro-gravity of space.",
        "published": "2004-11-08T20:28:48Z",
        "link": "http://arxiv.org/abs/cs/0411024v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Bionic Humans Using EAP as Artificial Muscles Reality and Challenges",
        "authors": [
            "Yoseph Bar-Cohen"
        ],
        "summary": "For many years, the idea of a human with bionic muscles immediately conjures up science fiction images of a TV series superhuman character that was implanted with bionic muscles and portrayed with strength and speed far superior to any normal human. As fantastic as this idea may seem, recent developments in electroactive polymers (EAP) may one day make such bionics possible. Polymers that exhibit large displacement in response to stimulation that is other than electrical signal were known for many years. Initially, EAP received relatively little attention due to their limited actuation capability. However, in the recent years, the view of the EAP materials has changed due to the introduction of effective new materials that significantly surpassed the capability of the widely used piezoelectric polymer, PVDF. As this technology continues to evolve, novel mechanisms that are biologically inspired are expected to emerge. EAP materials can potentially provide actuation with lifelike response and more flexible configurations. While further improvements in performance and robustness are still needed, there already have been several reported successes. In recognition of the need for cooperation in this multidisciplinary field, the author initiated and organized a series of international forums that are leading to a growing number of research and development projects and to great advances in the field. In 1999, he challenged the worldwide science and engineering community of EAP experts to develop a robotic arm that is actuated by artificial muscles to win a wrestling match against a human opponent. In this paper, the field of EAP as artificial muscles will be reviewed covering the state of the art, the challenges and the vision for the progress in future years.",
        "published": "2004-11-08T20:32:11Z",
        "link": "http://arxiv.org/abs/cs/0411025v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Artificial Intelligence and Systems Theory: Applied to Cooperative   Robots",
        "authors": [
            "Pedro U. Lima",
            "Luis M. M. Custodio"
        ],
        "summary": "This paper describes an approach to the design of a population of cooperative robots based on concepts borrowed from Systems Theory and Artificial Intelligence. The research has been developed under the SocRob project, carried out by the Intelligent Systems Laboratory at the Institute for Systems and Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the project stands both for \"Society of Robots\" and \"Soccer Robots\", the case study where we are testing our population of robots. Designing soccer robots is a very challenging problem, where the robots must act not only to shoot a ball towards the goal, but also to detect and avoid static (walls, stopped robots) and dynamic (moving robots) obstacles. Furthermore, they must cooperate to defeat an opposing team. Our past and current research in soccer robotics includes cooperative sensor fusion for world modeling, object recognition and tracking, robot navigation, multi-robot distributed task planning and coordination, including cooperative reinforcement learning in cooperative and adversarial environments, and behavior-based architectures for real time task execution of cooperating robot teams.",
        "published": "2004-11-08T20:41:44Z",
        "link": "http://arxiv.org/abs/cs/0411018v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Dynamic Modelling and Adaptive Traction Control for Mobile Robots",
        "authors": [
            "A. Albagul",
            "Wahyudi"
        ],
        "summary": "Mobile robots have received a great deal of research in recent years. A significant amount of research has been published in many aspects related to mobile robots. Most of the research is devoted to design and develop some control techniques for robot motion and path planning. A large number of researchers have used kinematic models to develop motion control strategy for mobile robots. Their argument and assumption that these models are valid if the robot has low speed, low acceleration and light load. However, dynamic modelling of mobile robots is very important as they are designed to travel at higher speed and perform heavy duty work. This paper presents and discusses a new approach to develop a dynamic model and control strategy for wheeled mobile robot which I modelled as a rigid body that roles on two wheels and a castor. The motion control strategy consists of two levels. The first level is dealing with the dynamic of the system and denoted as Low level controller. The second level is developed to take care of path planning and trajectory generation.",
        "published": "2004-11-08T20:44:03Z",
        "link": "http://arxiv.org/abs/cs/0411020v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Coevolution Based Adaptive Monte Carlo Localization (CEAMCL)",
        "authors": [
            "Luo Ronghua",
            "Hong Bingrong"
        ],
        "summary": "An adaptive Monte Carlo localization algorithm based on coevolution mechanism of ecological species is proposed. Samples are clustered into species, each of which represents a hypothesis of the robots pose. Since the coevolution between the species ensures that the multiple distinct hypotheses can be tracked stably, the problem of premature convergence when using MCL in highly symmetric environments can be solved. And the sample size can be adjusted adaptively over time according to the uncertainty of the robots pose by using the population growth model. In addition, by using the crossover and mutation operators in evolutionary computation, intra-species evolution can drive the samples move towards the regions where the desired posterior density is large. So a small size of samples can represent the desired density well enough to make precise localization. The new algorithm is termed coevolution based adaptive Monte Carlo localization (CEAMCL). Experiments have been carried out to prove the efficiency of the new localization algorithm.",
        "published": "2004-11-08T20:45:18Z",
        "link": "http://arxiv.org/abs/cs/0411021v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Neural Networks in Mobile Robot Motion",
        "authors": [
            "Danica Janglova"
        ],
        "summary": "This paper deals with a path planning and intelligent control of an autonomous robot which should move safely in partially structured environment. This environment may involve any number of obstacles of arbitrary shape and size; some of them are allowed to move. We describe our approach to solving the motion-planning problem in mobile robot control using neural networks-based technique. Our method of the construction of a collision-free path for moving robot among obstacles is based on two neural networks. The first neural network is used to determine the \"free\" space using ultrasound range finder data. The second neural network \"finds\" a safe direction for the next robot section of the path in the workspace while avoiding the nearest obstacles. Simulation examples of generated path with proposed techniques will be presented.",
        "published": "2004-12-11T12:32:10Z",
        "link": "http://arxiv.org/abs/cs/0412049v1",
        "categories": [
            "cs.RO",
            "cs.AI"
        ]
    },
    {
        "title": "Gyroscopically Stabilized Robot: Balance and Tracking",
        "authors": [
            "Yongsheng Ou",
            "Yangsheng Xu"
        ],
        "summary": "The single wheel, gyroscopically stabilized robot - Gyrover, is a dynamically stable but statically unstable, underactuated system. In this paper, based on the dynamic model of the robot, we investigate two classes of nonholonomic constraints associated with the system. Then, based on the backstepping technology, we propose a control law for balance control of Gyrover. Next, through transferring the systems states from Cartesian coordinate to polar coordinate, control laws for point-to-point control and line tracking in Cartesian space are provided.",
        "published": "2004-12-11T12:38:11Z",
        "link": "http://arxiv.org/abs/cs/0412050v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Dynamic replanning in uncertain environments for a sewer inspection   robot",
        "authors": [
            "Oliver Adria",
            "Hermann Streich",
            "Joachim Hertzberg"
        ],
        "summary": "The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks will be to take water probes, analyze it onboard, and measure positions of manholes and pipes to detect polluted-loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the controller software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, not inflicting any self-damage. This paper focuses on the route planning and replanning aspect of the robot. The robots software has four different levels, of which the planning system is the highest level, and the remaining three are controller levels each with a different degree of abstraction. The planner coordinates the sequence of actions that are to be successively executed by the robot.",
        "published": "2004-12-11T12:42:10Z",
        "link": "http://arxiv.org/abs/cs/0412051v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "WebotsTM: Professional Mobile Robot Simulation",
        "authors": [
            "Olivier Michel"
        ],
        "summary": "Cyberbotics Ltd. develops WebotsTM, a mobile robotics simulation software that provides you with a rapid prototyping environment for modelling, programming and simulating mobile robots. The provided robot libraries enable you to transfer your control programs to several commercially available real mobile robots. WebotsTM lets you define and modify a complete mobile robotics setup, even several different robots sharing the same environment. For each object, you can define a number of properties, such as shape, color, texture, mass, friction, etc. You can equip each robot with a large number of available sensors and actuators. You can program these robots using your favorite development environment, simulate them and optionally transfer the resulting programs onto your real robots. WebotsTM has been developed in collaboration with the Swiss Federal Institute of Technology in Lausanne, thoroughly tested, well documented and continuously maintained for over 7 years. It is now the main commercial product available from Cyberbotics Ltd.",
        "published": "2004-12-11T12:45:08Z",
        "link": "http://arxiv.org/abs/cs/0412052v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Dynamic simulation of task constrained of a rigid-flexible manipulator",
        "authors": [
            "Atef A. Ata",
            "Habib Johar"
        ],
        "summary": "A rigid-flexible manipulator may be assigned tasks in a moving environment where the winds or vibrations affect the position and/or orientation of surface of operation. Consequently, losses of the contact and perhaps degradation of the performance may occur as references are changed. When the environment is moving, knowledge of the angle &#945; between the contact surface and the horizontal is required at every instant. In this paper, different profiles for the time varying angle &#945; are proposed to investigate the effect of this change into the contact force and the joint torques of a rigid-flexible manipulator. The coefficients of the equation of the proposed rotating surface are changing with time to determine the new X and Y coordinates of the moving surface as the surface rotates.",
        "published": "2004-12-11T12:48:08Z",
        "link": "http://arxiv.org/abs/cs/0412053v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Assembly and Disassembly Planning by using Fuzzy Logic & Genetic   Algorithms",
        "authors": [
            "L. M. Galantucci",
            "G. Percoco",
            "R. Spina"
        ],
        "summary": "The authors propose the implementation of hybrid Fuzzy Logic-Genetic Algorithm (FL-GA) methodology to plan the automatic assembly and disassembly sequence of products. The GA-Fuzzy Logic approach is implemented onto two levels. The first level of hybridization consists of the development of a Fuzzy controller for the parameters of an assembly or disassembly planner based on GAs. This controller acts on mutation probability and crossover rate in order to adapt their values dynamically while the algorithm runs. The second level consists of the identification of theoptimal assembly or disassembly sequence by a Fuzzy function, in order to obtain a closer control of the technological knowledge of the assembly/disassembly process. Two case studies were analyzed in order to test the efficiency of the Fuzzy-GA methodologies.",
        "published": "2004-12-11T12:50:36Z",
        "link": "http://arxiv.org/abs/cs/0412054v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Robotic Applications in Cardiac Surgery",
        "authors": [
            "Alan P. Kypson",
            "W. Randolph Chitwood Jr"
        ],
        "summary": "Traditionally, cardiac surgery has been performed through a median sternotomy, which allows the surgeon generous access to the heart and surrounding great vessels. As a paradigm shift in the size and location of incisions occurs in cardiac surgery, new methods have been developed to allow the surgeon the same amount of dexterity and accessibility to the heart in confined spaces and in a less invasive manner. Initially, long instruments without pivot points were used, however, more recent robotic telemanipulation systems have been applied that allow for improved dexterity, enabling the surgeon to perform cardiac surgery from a distance not previously possible. In this rapidly evolving field, we review the recent history and clinical results of using robotics in cardiac surgery.",
        "published": "2004-12-11T12:52:58Z",
        "link": "http://arxiv.org/abs/cs/0412055v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "One-Chip Solution to Intelligent Robot Control: Implementing Hexapod   Subsumption Architecture Using a Contemporary Microprocessor",
        "authors": [
            "Nikita Pashenkov",
            "Ryuichi Iwamasa"
        ],
        "summary": "This paper introduces a six-legged autonomous robot managed by a single controller and a software core modeled on subsumption architecture. We begin by discussing the features and capabilities of IsoPod, a new processor for robotics which has enabled a streamlined implementation of our project. We argue that this processor offers a unique set of hardware and software features, making it a practical development platform for robotics in general and for subsumption-based control architectures in particular. Next, we summarize original ideas on subsumption architecture implementation for a six-legged robot, as presented by its inventor Rodney Brooks in 1980s. A comparison is then made to a more recent example of a hexapod control architecture based on subsumption. The merits of both systems are analyzed and a new subsumption architecture layout is formulated as a response. We conclude with some remarks regarding the development of this project as a hint at new potentials for intelligent robot design, opened by a recent development in embedded controller market.",
        "published": "2004-12-11T12:55:30Z",
        "link": "http://arxiv.org/abs/cs/0412056v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "How to achieve various gait patterns from single nominal",
        "authors": [
            "Miomir Vukobratovic",
            "Dejan Andric",
            "Branislav Borovac"
        ],
        "summary": "In this paper is presented an approach to achieving on-line modification of nominal biped gait without recomputing entire dynamics when steady motion is performed. Straight, dynamically balanced walk was used as a nominal gait, and applied modifications were speed-up and slow-down walk and turning left and right. It is shown that the disturbances caused by these modifications jeopardize dynamic stability, but they can be simply compensated to enable walk continuation.",
        "published": "2004-12-11T12:57:33Z",
        "link": "http://arxiv.org/abs/cs/0412057v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Nested Intervals with Farey Fractions",
        "authors": [
            "Vadim Tropashko"
        ],
        "summary": "Relational Databases are universally conceived as an advance over their predecessors Network and Hierarchical models. Superior in every querying respect, they turned out to be surprisingly incomplete when modeling transitive dependencies. Almost every couple of months a question how to model a tree in the database surfaces at comp.database.theory newsgroup. This article completes a series of articles exploring Nested Intervals Model. Previous articles introduced tree encoding with Binary Rational Numbers. However, binary encoding grows exponentially, both in breadth and in depth. In this article, we'll leverage Farey fractions in order to overcome this problem. We'll also demonstrate that our implementation scales to a tree with 1M nodes.",
        "published": "2004-01-18T03:09:04Z",
        "link": "http://arxiv.org/abs/cs/0401014v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "Query Answering in Peer-to-Peer Data Exchange Systems",
        "authors": [
            "Leopoldo Bertossi",
            "Loreto Bravo"
        ],
        "summary": "The problem of answering queries posed to a peer who is a member of a peer-to-peer data exchange system is studied. The answers have to be consistent wrt to both the local semantic constraints and the data exchange constraints with other peers; and must also respect certain trust relationships between peers. A semantics for peer consistent answers under exchange constraints and trust relationships is introduced and some techniques for obtaining those answers are presented.",
        "published": "2004-01-20T19:26:35Z",
        "link": "http://arxiv.org/abs/cs/0401015v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.4;F.4.1;I.2.3"
        ]
    },
    {
        "title": "Semantic Optimization of Preference Queries",
        "authors": [
            "Jan Chomicki"
        ],
        "summary": "The notion of preference is becoming more and more ubiquitous in present-day information systems. Preferences are primarily used to filter and personalize the information reaching the users of such systems. In database systems, preferences are usually captured as preference relations that are used to build preference queries. In our approach, preference queries are relational algebra or SQL queries that contain occurrences of the winnow operator (\"find the most preferred tuples in a given relation\").   We present here a number of semantic optimization techniques applicable to preference queries. The techniques make use of integrity constraints, and make it possible to remove redundant occurrences of the winnow operator and to apply a more efficient algorithm for the computation of winnow. We also study the propagation of integrity constraints in the result of the winnow. We have identified necessary and sufficient conditions for the applicability of our techniques, and formulated those conditions as constraint satisfiability problems.",
        "published": "2004-02-02T01:42:35Z",
        "link": "http://arxiv.org/abs/cs/0402003v1",
        "categories": [
            "cs.DB",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "An Integrated Approach for Extraction of Objects from XML and   Transformation to Heterogeneous Object Oriented Databases",
        "authors": [
            "Uzair Ahmad",
            "Mohammad Waseem Hassan",
            "Arshad Ali",
            "Richard McClatchey",
            "Ian Willers"
        ],
        "summary": "CERN's (European Organization for Nuclear Research) WISDOM project uses XML for the replication of data between different data repositories in a heterogeneous operating system environment. For exchanging data from Web-resident databases, the data needs to be transformed into XML and back to the database format. Many different approaches are employed to do this transformation. This paper addresses issues that make this job more efficient and robust than existing approaches. It incorporates the World Wide Web Consortium (W3C) XML Schema specification in the database-XML relationship. Incorporation of the XML Schema exhibits significant improvements in XML content usage and reduces the limitations of DTD-based database XML services. Secondly the paper explores the possibility of database independent transformation of data between XML and different databases. It proposes a standard XML format that every serialized object should follow. This makes it possible to use objects of heterogeneous database seamlessly using XML.",
        "published": "2004-02-02T20:12:53Z",
        "link": "http://arxiv.org/abs/cs/0402007v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4"
        ]
    },
    {
        "title": "A Use-Case Driven Approach in Requirements Engineering : The Mammogrid   Project",
        "authors": [
            "Mohammed Odeh",
            "Tamas Hauer",
            "Richard McClatchey",
            "Tony Solomonides"
        ],
        "summary": "We report on the application of the use-case modeling technique to identify and specify the user requirements of the MammoGrid project in an incremental and controlled iterative approach. Modeling has been carried out in close collaboration with clinicians and radiologists with no prior experience of use cases. The study reveals the advantages and limitations of applying this technique to requirements specification in the domains of breast cancer screening and mammography research, with implications for medical imaging more generally. In addition, this research has shown a return on investment in use-case modeling in shorter gaps between phases of the requirements engineering process. The qualitative result of this analysis leads us to propose that a use-case modeling approach may result in reducing the cycle of the requirements engineering process for medical imaging.",
        "published": "2004-02-02T20:18:23Z",
        "link": "http://arxiv.org/abs/cs/0402008v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4"
        ]
    },
    {
        "title": "Resolving Clinicians Queries Across a Grids Infrastructure",
        "authors": [
            "F Estrella",
            "C del Frate",
            "T Hauer",
            "R McClatchey",
            "M Odeh",
            "D Rogulin",
            "S R Amendolia",
            "D Schottlander",
            "T Solomonides",
            "R Warren"
        ],
        "summary": "The past decade has witnessed order of magnitude increases in computing power, data storage capacity and network speed, giving birth to applications which may handle large data volumes of increased complexity, distributed over the Internet. Grids computing promises to resolve many of the difficulties in facilitating medical image analysis to allow radiologists to collaborate without having to co-locate. The EU-funded MammoGrid project aims to investigate the feasibility of developing a Grid-enabled European database of mammograms and provide an information infrastructure which federates multiple mammogram databases. This will enable clinicians to develop new common, collaborative and co-operative approaches to the analysis of mammographic data. This paper focuses on one of the key requirements for large-scale distributed mammogram analysis: resolving queries across a grid-connected federation of images.",
        "published": "2004-02-03T14:32:39Z",
        "link": "http://arxiv.org/abs/cs/0402009v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4; J.3"
        ]
    },
    {
        "title": "Perspects in astrophysical databases",
        "authors": [
            "M. Frailis",
            "A. De Angelis",
            "V. Roberto"
        ],
        "summary": "Astrophysics has become a domain extremely rich of scientific data. Data mining tools are needed for information extraction from such large datasets. This asks for an approach to data management emphasizing the efficiency and simplicity of data access; efficiency is obtained using multidimensional access methods and simplicity is achieved by properly handling metadata. Moreover, clustering and classification techniques on large datasets pose additional requirements in terms of computation and memory scalability and interpretability of results. In this study we review some possible solutions.",
        "published": "2004-02-09T19:13:17Z",
        "link": "http://arxiv.org/abs/cs/0402016v1",
        "categories": [
            "cs.DB",
            "astro-ph",
            "H.2.4; H.2.8"
        ]
    },
    {
        "title": "Pattern Reification as the Basis for Description-Driven Systems",
        "authors": [
            "Florida Estrella",
            "Zsolt Kovacs",
            "Jean-Marie Le Goff",
            "Richard McClatchey",
            "Tony Solomonides",
            "Norbert Toth"
        ],
        "summary": "One of the main factors driving object-oriented software development for information systems is the requirement for systems to be tolerant to change. To address this issue in designing systems, this paper proposes a pattern-based, object-oriented, description-driven system (DDS) architecture as an extension to the standard UML four-layer meta-model. A DDS architecture is proposed in which aspects of both static and dynamic systems behavior can be captured via descriptive models and meta-models. The proposed architecture embodies four main elements - firstly, the adoption of a multi-layered meta-modeling architecture and reflective meta-level architecture, secondly the identification of four data modeling relationships that can be made explicit such that they can be modified dynamically, thirdly the identification of five design patterns which have emerged from practice and have proved essential in providing reusable building blocks for data management, and fourthly the encoding of the structural properties of the five design patterns by means of one fundamental pattern, the Graph pattern. A practical example of this philosophy, the CRISTAL project, is used to demonstrate the use of description-driven data objects to handle system evolution.",
        "published": "2004-02-12T14:25:14Z",
        "link": "http://arxiv.org/abs/cs/0402024v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4,J.3"
        ]
    },
    {
        "title": "A perspective on the Healthgrid initiative",
        "authors": [
            "V. Breton",
            "A. E. Solomonides",
            "R. H. McClatchey"
        ],
        "summary": "This paper presents a perspective on the Healthgrid initiative which involves European projects deploying pioneering applications of grid technology in the health sector. In the last couple of years, several grid projects have been funded on health related issues at national and European levels. A crucial issue is to maximize their cross fertilization in the context of an environment where data of medical interest can be stored and made easily available to the different actors in healthcare, physicians, healthcare centres and administrations, and of course the citizens. The Healthgrid initiative, represented by the Healthgrid association (http://www.healthgrid.org), was initiated to bring the necessary long term continuity, to reinforce and promote awareness of the possibilities and advantages linked to the deployment of GRID technologies in health. Technologies to address the specific requirements for medical applications are under development. Results from the DataGrid and other projects are given as examples of early applications.",
        "published": "2004-02-12T14:36:05Z",
        "link": "http://arxiv.org/abs/cs/0402025v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4,J.3"
        ]
    },
    {
        "title": "A Service-Based Approach for Managing Mammography Data",
        "authors": [
            "Florida Estrella",
            "Richard McClatchey",
            "Dmitry Rogulina",
            "Roberto Amendolia",
            "Tony Solomonides"
        ],
        "summary": "Grid-based technologies are emerging as a potential open-source standards-based solution for managing and collabo-rating distributed resources. In view of these new computing solutions, the Mammogrid project is developing a service-based and Grid-aware application which manages a Euro-pean-wide database of mammograms. Medical conditions such as breast cancer, and mammograms as images, are ex-tremely complex with many dimensions of variability across the population. An effective solution for the management of disparate mammogram data sources is a federation of autonomous multi-centre sites which transcends national boundaries. The Mammogrid solution utilizes the Grid tech-nologies to integrate geographically distributed data sets. The Mammogrid application will explore the potential of the Grid to support effective co-working among radiologists through-out the EU. This paper outlines the Mammogrid service-based approach in managing a federation of grid-connected mam-mography databases.",
        "published": "2004-02-12T15:00:18Z",
        "link": "http://arxiv.org/abs/cs/0402023v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4; J.3"
        ]
    },
    {
        "title": "Nested Intervals Tree Encoding with Continued Fractions",
        "authors": [
            "Vadim Tropashko"
        ],
        "summary": "We introduce a new variation of Tree Encoding with Nested Intervals, find connections with Materialized Path, and suggest a method for moving parts of the hierarchy.",
        "published": "2004-02-20T19:45:56Z",
        "link": "http://arxiv.org/abs/cs/0402051v2",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "Search Efficiency in Indexing Structures for Similarity Searching",
        "authors": [
            "Girish Motwani",
            "Sandhya G. Nair"
        ],
        "summary": "Similarity searching finds application in a wide variety of domains including multilingual databases, computational biology, pattern recognition and text retrieval. Similarity is measured in terms of a distance function, edit distance, in general metric spaces, which is expensive to compute. Indexing techniques can be used reduce the number of distance computations. We present an analysis of various existing similarity indexing structures for the same. The performance obtained using the index structures studied was found to be unsatisfactory . We propose an indexing technique that combines the features of clustering with M tree(MTB) and the results indicate that this gives better performance.",
        "published": "2004-03-11T06:30:30Z",
        "link": "http://arxiv.org/abs/cs/0403014v2",
        "categories": [
            "cs.DB",
            "H.2.m"
        ]
    },
    {
        "title": "Extending the SDSS Batch Query System to the National Virtual   Observatory Grid",
        "authors": [
            "Maria A. Nieto-Santisteban",
            "William O'Mullane",
            "Jim Gray",
            "Nolan Li",
            "Tamas Budavari",
            "Alexander S. Szalay",
            "Aniruddha R. Thakar"
        ],
        "summary": "The Sloan Digital Sky Survey science database is approaching 2TB. While the vast majority of queries normally execute in seconds or minutes, this interactive execution time can be disproportionately increased by a small fraction of queries that take hours or days to run; either because they require non-index scans of the largest tables or because they request very large result sets. In response to this, we added a multi-queue job submission and tracking system. The transfer of very large result sets from queries over the network is another serious problem. Statistics suggested that much of this data transfer is unnecessary; users would prefer to store results locally in order to allow further cross matching and filtering. To allow local analysis, we implemented a system that gives users their own personal database (MyDB) at the portal site. Users may transfer data to their MyDB, and then perform further analysis before extracting it to their own machine.   We intend to extend the MyDB and asynchronous query ideas to multiple NVO nodes. This implies development, in a distributed manner, of several features, which have been demonstrated for a single node in the SDSS Batch Query System (CasJobs). The generalization of asynchronous queries necessitates some form of MyDB storage as well as workflow tracking services on each node and coordination strategies among nodes.",
        "published": "2004-03-12T09:42:04Z",
        "link": "http://arxiv.org/abs/cs/0403017v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "The World Wide Telescope: An Archetype for Online Science",
        "authors": [
            "Jim Gray",
            "Alexander S. Szalay"
        ],
        "summary": "Most scientific data will never be directly examined by scientists; rather it will be put into online databases where it will be analyzed and summarized by computer programs. Scientists increasingly see their instruments through online scientific archives and analysis tools, rather than examining the raw data. Today this analysis is primarily driven by scientists asking queries, but scientific archives are becoming active databases that self-organize and recognize interesting and anomalous facts as data arrives. In some fields, data from many different archives can be cross-correlated to produce new insights. Astronomy presents an excellent example of these trends; and, federating Astronomy archives presents interesting challenges for computer scientists.",
        "published": "2004-03-12T09:57:43Z",
        "link": "http://arxiv.org/abs/cs/0403018v1",
        "categories": [
            "cs.DB",
            "H.0"
        ]
    },
    {
        "title": "The Sloan Digital Sky Survey Science Archive: Migrating a Multi-Terabyte   Astronomical Archive from Object to Relational DBMS",
        "authors": [
            "Aniruddha R. Thakar",
            "Alexander S. Szalay",
            "Peter Z. Kunszt",
            "Jim Gray"
        ],
        "summary": "The Sloan Digital Sky Survey Science Archive is the first in a series of multi-Terabyte digital archives in Astronomy and other data-intensive sciences. To facilitate data mining in the SDSS archive, we adapted a commercial database engine and built specialized tools on top of it. Originally we chose an object-oriented database management system due to its data organization capabilities, platform independence, query performance and conceptual fit to the data. However, after using the object database for the first couple of years of the project, it soon began to fall short in terms of its query support and data mining performance. This was as much due to the inability of the database vendor to respond our demands for features and bug fixes as it was due to their failure to keep up with the rapid improvements in hardware performance, particularly faster RAID disk systems. In the end, we were forced to abandon the object database and migrate our data to a relational database. We describe below the technical issues that we faced with the object database and how and why we migrated to relational technology.",
        "published": "2004-03-12T10:20:23Z",
        "link": "http://arxiv.org/abs/cs/0403020v1",
        "categories": [
            "cs.DB",
            "H.0"
        ]
    },
    {
        "title": "A Quick Look at SATA Disk Performance",
        "authors": [
            "Tom Barclay",
            "Wyman Chong",
            "Jim Gray"
        ],
        "summary": "We have been investigating the use of low-cost, commodity components for multi-terabyte SQL Server databases. Dubbed storage bricks, these servers are white box PCs containing the largest ATA drives, value-priced AMD or Intel processors, and inexpensive ECC memory. One issue has been the wiring mess, air flow problems, length restrictions, and connector failures created by seven or more parallel ATA (PATA) ribbon cables and drives in]a tower or 3U rack-mount chassis. Large capacity Serial ATA (SATA) drives have recently become widely available for the PC environment at a reasonable price. In addition to being faster, the SATA connectors seem more reliable, have a more reasonable length restriction (1m) and allow better airflow. We tested two drive brands along with two RAID controllers to evaluate SATA drive performance and reliablility. This paper documents our results so far.",
        "published": "2004-03-12T10:42:25Z",
        "link": "http://arxiv.org/abs/cs/0403021v1",
        "categories": [
            "cs.DB",
            "cs.PF",
            "C.4"
        ]
    },
    {
        "title": "Enhancing the expressive power of the U-Datalog language",
        "authors": [
            "Elisa Bertino",
            "Barbara Catania",
            "Roberta Gori"
        ],
        "summary": "U-Datalog has been developed with the aim of providing a set-oriented logical update language, guaranteeing update parallelism in the context of a Datalog-like language. In U-Datalog, updates are expressed by introducing constraints (+p(X), to denote insertion, and [minus sign]p(X), to denote deletion) inside Datalog rules. A U-Datalog program can be interpreted as a CLP program. In this framework, a set of updates (constraints) is satisfiable if it does not represent an inconsistent theory, that is, it does not require the insertion and the deletion of the same fact. This approach resembles a very simple form of negation. However, on the other hand, U-Datalog does not provide any mechanism to explicitly deal with negative information, resulting in a language with limited expressive power. In this paper, we provide a semantics, based on stratification, handling the use of negated atoms in U-Datalog programs, and we show which problems arise in defining a compositional semantics.",
        "published": "2004-04-02T02:03:32Z",
        "link": "http://arxiv.org/abs/cs/0404003v1",
        "categories": [
            "cs.DB",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "The Persistent Buffer Tree : An I/O-efficient Index for Temporal Data",
        "authors": [
            "Saju Jude Dominic",
            "G. Sajith"
        ],
        "summary": "In a variety of applications, we need to keep track of the development of a data set over time. For maintaining and querying this multi version data I/O-efficiently, external memory data structures are required. In this paper, we present a probabilistic self-balancing persistent data structure in external memory called the persistent buffer tree, which supports insertions, updates and deletions of data items at the present version and range queries for any version, past or present. The persistent buffer tree is I/O-optimal in the sense that the expected amortized I/O performance bounds are asymptotically the same as the deterministic amortized bounds of the (single version) buffer tree in the worst case.",
        "published": "2004-04-15T03:17:56Z",
        "link": "http://arxiv.org/abs/cs/0404033v1",
        "categories": [
            "cs.GL",
            "cs.DB",
            "E.2;H.2.2;G.3"
        ]
    },
    {
        "title": "\"In vivo\" spam filtering: A challenge problem for data mining",
        "authors": [
            "Tom Fawcett"
        ],
        "summary": "Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email communication. Many data mining researchers have addressed the problem of detecting spam, generally by treating it as a static text classification problem. True in vivo spam filtering has characteristics that make it a rich and challenging domain for data mining. Indeed, real-world datasets with these characteristics are typically difficult to acquire and to share. This paper demonstrates some of these characteristics and argues that researchers should pursue in vivo spam filtering as an accessible domain for investigating them.",
        "published": "2004-05-04T18:56:09Z",
        "link": "http://arxiv.org/abs/cs/0405007v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "I.2.6;H.2.8"
        ]
    },
    {
        "title": "Mining Frequent Itemsets from Secondary Memory",
        "authors": [
            "Gösta Grahne",
            "Jianfei Zhu"
        ],
        "summary": "Mining frequent itemsets is at the core of mining association rules, and is by now quite well understood algorithmically. However, most algorithms for mining frequent itemsets assume that the main memory is large enough for the data structures used in the mining, and very few efficient algorithms deal with the case when the database is very large or the minimum support is very low. Mining frequent itemsets from a very large database poses new challenges, as astronomical amounts of raw data is ubiquitously being recorded in commerce, science and government. In this paper, we discuss approaches to mining frequent itemsets when data structures are too large to fit in main memory. Several divide-and-conquer algorithms are given for mining from disks. Many novel techniques are introduced. Experimental results show that the techniques reduce the required disk accesses by orders of magnitude, and enable truly scalable data mining.",
        "published": "2004-05-20T14:33:08Z",
        "link": "http://arxiv.org/abs/cs/0405069v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.2.8"
        ]
    },
    {
        "title": "Grid Databases for Shared Image Analysis in the MammoGrid Project",
        "authors": [
            "S. R. Amendolia",
            "F. Estrella",
            "T. Hauer",
            "D. Manset",
            "R. McClatchey",
            "M. Odeh",
            "T. Reading",
            "D. Rogulin",
            "D. Schottlander",
            "T. Solomonides"
        ],
        "summary": "The MammoGrid project aims to prove that Grid infrastructures can be used for collaborative clinical analysis of database-resident but geographically distributed medical images. This requires: a) the provision of a clinician-facing front-end workstation and b) the ability to service real-world clinician queries across a distributed and federated database. The MammoGrid project will prove the viability of the Grid by harnessing its power to enable radiologists from geographically dispersed hospitals to share standardized mammograms, to compare diagnoses (with and without computer aided detection of tumours) and to perform sophisticated epidemiological studies across national boundaries. This paper outlines the approach taken in MammoGrid to seamlessly connect radiologist workstations across a Grid using an \"information infrastructure\" and a DICOM-compliant object model residing in multiple distributed data stores in Italy and the UK",
        "published": "2004-05-21T13:24:00Z",
        "link": "http://arxiv.org/abs/cs/0405072v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "MammoGrid: A Service Oriented Architecture based Medical Grid   Application",
        "authors": [
            "S R Amendolia",
            "F Estrella",
            "W Hassan",
            "T Hauer",
            "D Manset",
            "R McClatchey",
            "D Rogulin",
            "T Solomonides"
        ],
        "summary": "The MammoGrid project has recently delivered its first proof-of-concept prototype using a Service-Oriented Architecture (SOA)-based Grid application to enable distributed computing spanning national borders. The underlying AliEn Grid infrastructure has been selected because of its practicality and because of its emergence as a potential open source standards-based solution for managing and coordinating distributed resources. The resultant prototype is expected to harness the use of huge amounts of medical image data to perform epidemiological studies, advanced image processing, radiographic education and ultimately, tele-diagnosis over communities of medical virtual organisations. The MammoGrid prototype comprises a high-quality clinician visualization workstation used for data acquisition and inspection, a DICOM-compliant interface to a set of medical services (annotation, security, image analysis, data storage and querying services) residing on a so-called Grid-box and secure access to a network of other Grid-boxes connected through Grid middleware. This paper outlines the MammoGrid approach in managing a federation of Grid-connected mammography databases in the context of the recently delivered prototype and will also describe the next phase of prototyping.",
        "published": "2004-05-21T13:41:27Z",
        "link": "http://arxiv.org/abs/cs/0405074v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H2.4;J.3;C2.4"
        ]
    },
    {
        "title": "An Abductive Framework For Computing Knowledge Base Updates",
        "authors": [
            "Chiaki Sakama",
            "Katsumi Inoue"
        ],
        "summary": "This paper introduces an abductive framework for updating knowledge bases represented by extended disjunctive programs. We first provide a simple transformation from abductive programs to update programs which are logic programs specifying changes on abductive hypotheses. Then, extended abduction, which was introduced by the same authors as a generalization of traditional abduction, is computed by the answer sets of update programs. Next, different types of updates, view updates and theory updates are characterized by abductive programs and computed by update programs. The task of consistency restoration is also realized as special cases of these updates. Each update problem is comparatively assessed from the computational complexity viewpoint. The result of this paper provides a uniform framework for different types of knowledge base updates, and each update is computed using existing procedures of logic programming.",
        "published": "2004-05-22T10:30:35Z",
        "link": "http://arxiv.org/abs/cs/0405076v1",
        "categories": [
            "cs.DB",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "A Grid Information Infrastructure for Medical Image Analysis",
        "authors": [
            "D Rogulin",
            "F Estrella",
            "T Hauer",
            "R McClatchey",
            "T Solomonides"
        ],
        "summary": "The storage and manipulation of digital images and the analysis of the information held in those images are essential requirements for next-generation medical information systems. The medical community has been exploring collaborative approaches for managing image data and exchanging knowledge and Grid technology [1] is a promising approach to enabling distributed analysis across medical institutions and for developing new collaborative and cooperative approaches for image analysis without the necessity for clinicians to co-locate. The EU-funded MammoGrid project [2] is one example of this and it aims to develop a Europe-wide database of mammograms to support effective co-working between healthcare professionals across the EU. The MammoGrid prototype comprises a high-quality clinician visualization workstation (for data acquisition and inspection), a DICOM-compliant interface to a set of medical services (annotation, security, image analysis, data storage and querying services) residing on a so-called Grid-box and secure access to a network of other Grid-boxes connected through Grid middleware. One of the main deliverables of the project is a Grid-enabled infrastructure that manages federated mammogram databases across Europe. This paper outlines the MammoGrid Information Infrastructure (MII) for meta-data analysis and knowledge discovery in the medical imaging domain.",
        "published": "2004-05-24T10:15:45Z",
        "link": "http://arxiv.org/abs/cs/0405087v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Knowledge Reduction and Discovery based on Demarcation Information",
        "authors": [
            "Yuguo He"
        ],
        "summary": "Knowledge reduction, includes attribute reduction and value reduction, is an important topic in rough set literature. It is also closely relevant to other fields, such as machine learning and data mining. In this paper, an algorithm called TWI-SQUEEZE is proposed. It can find a reduct, or an irreducible attribute subset after two scans. Its soundness and computational complexity are given, which show that it is the fastest algorithm at present. A measure of variety is brought forward, of which algorithm TWI-SQUEEZE can be regarded as an application. The author also argues the rightness of this measure as a measure of information, which can make it a unified measure for \"differentiation, a concept appeared in cognitive psychology literature. Value reduction is another important aspect of knowledge reduction. It is interesting that using the same algorithm we can execute a complete value reduction efficiently. The complete knowledge reduction, which results in an irreducible table, can therefore be accomplished after four scans of table. The byproducts of reduction are two classifiers of different styles. In this paper, various cases and models will be discussed to prove the efficiency and effectiveness of the algorithm. Some topics, such as how to integrate user preference to find a local optimal attribute subset will also be discussed.",
        "published": "2004-05-27T11:26:18Z",
        "link": "http://arxiv.org/abs/cs/0405104v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "cs.IT",
            "math.IT",
            "H.2.8;I.2.6; H.1.1;I.5.2"
        ]
    },
    {
        "title": "Application of Business Intelligence In Banks (Pakistan)",
        "authors": [
            "Muhammad Nadeem",
            "Syed Ata Hussain Jaffri"
        ],
        "summary": "The financial services industry is rapidly changing. Factors such as globalization, deregulation, mergers and acquisitions, competition from non-financial institutions, and technological innovation, have forced companies to re-think their business.Many large companies have been using Business Intelligence (BI) computer software for some years to help them gain competitive advantage. With the introduction of cheaper and more generalized products to the market place BI is now in the reach of smaller and medium sized companies. Business Intelligence is also known as knowledge management, management information systems (MIS), Executive information systems (EIS) and On-line analytical Processing (OLAP).",
        "published": "2004-06-02T12:55:04Z",
        "link": "http://arxiv.org/abs/cs/0406004v1",
        "categories": [
            "cs.DB",
            "H.2.7"
        ]
    },
    {
        "title": "Schema-based Scheduling of Event Processors and Buffer Minimization for   Queries on Structured Data Streams",
        "authors": [
            "Christoph Koch",
            "Stefanie Scherzinger",
            "Nicole Schweikardt",
            "Bernhard Stegmaier"
        ],
        "summary": "We introduce an extension of the XQuery language, FluX, that supports event-based query processing and the conscious handling of main memory buffers. Purely event-based queries of this language can be executed on streaming XML data in a very direct way. We then develop an algorithm that allows to efficiently rewrite XQueries into the event-based FluX language. This algorithm uses order constraints from a DTD to schedule event handlers and to thus minimize the amount of buffering required for evaluating a query. We discuss the various technical aspects of query optimization and query evaluation within our framework. This is complemented with an experimental evaluation of our approach.",
        "published": "2004-06-07T20:45:28Z",
        "link": "http://arxiv.org/abs/cs/0406016v1",
        "categories": [
            "cs.DB",
            "H.2.3, H.2.4"
        ]
    },
    {
        "title": "Subset Queries in Relational Databases",
        "authors": [
            "Satyanarayana R Valluri",
            "Kamalakar Karlapalem"
        ],
        "summary": "In this paper, we motivated the need for relational database systems to support subset query processing. We defined new operators in relational algebra, and new constructs in SQL for expressing subset queries. We also illustrated the applicability of subset queries through different examples expressed using extended SQL statements and relational algebra expressions. Our aim is to show the utility of subset queries for next generation applications.",
        "published": "2004-06-17T09:11:49Z",
        "link": "http://arxiv.org/abs/cs/0406029v1",
        "categories": [
            "cs.DB",
            "H2.3, H2.4"
        ]
    },
    {
        "title": "Web Services: A Process Algebra Approach",
        "authors": [
            "Andrea Ferrara"
        ],
        "summary": "It is now well-admitted that formal methods are helpful for many issues raised in the Web service area. In this paper we present a framework for the design and verification of WSs using process algebras and their tools. We define a two-way mapping between abstract specifications written using these calculi and executable Web services written in BPEL4WS. Several choices are available: design and correct errors in BPEL4WS, using process algebra verification tools, or design and correct in process algebra and automatically obtaining the corresponding BPEL4WS code. The approaches can be combined. Process algebra are not useful only for temporal logic verification: we remark the use of simulation/bisimulation both for verification and for the hierarchical refinement design method. It is worth noting that our approach allows the use of any process algebra depending on the needs of the user at different levels (expressiveness, existence of reasoning tools, user expertise).",
        "published": "2004-06-28T15:42:22Z",
        "link": "http://arxiv.org/abs/cs/0406055v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Proofs of Zero Knowledge",
        "authors": [
            "Matthias Bauer"
        ],
        "summary": "We present a protocol for verification of ``no such entry'' replies from databases. We introduce a new cryptographic primitive as the underlying structure, the keyed hash tree, which is an extension of Merkle's hash tree. We compare our scheme to Buldas et al.'s Undeniable Attesters and Micali et al.'s Zero Knowledge Sets.",
        "published": "2004-06-29T11:05:29Z",
        "link": "http://arxiv.org/abs/cs/0406058v2",
        "categories": [
            "cs.CR",
            "cs.DB",
            "E.2; H.2.0"
        ]
    },
    {
        "title": "Well-Definedness and Semantic Type-Checking in the Nested Relational   Calculus and XQuery",
        "authors": [
            "Jan Van den Bussche",
            "Dirk Van Gucht",
            "Stijn Vansummeren"
        ],
        "summary": "Two natural decision problems regarding the XML query language XQuery are well-definedness and semantic type-checking. We study these problems in the setting of a relational fragment of XQuery. We show that well-definedness and semantic type-checking are undecidable, even in the positive-existential case. Nevertheless, for a ``pure'' variant of XQuery, in which no identification is made between an item and the singleton containing that item, the problems become decidable. We also consider the analogous problems in the setting of the nested relational calculus.",
        "published": "2004-06-29T16:09:10Z",
        "link": "http://arxiv.org/abs/cs/0406060v1",
        "categories": [
            "cs.DB",
            "cs.PL"
        ]
    },
    {
        "title": "The semijoin algebra and the guarded fragment",
        "authors": [
            "Dirk Leinders",
            "Jerzy Tyszkiewicz",
            "Jan Van den Bussche"
        ],
        "summary": "The semijoin algebra is the variant of the relational algebra obtained by replacing the join operator by the semijoin operator. We discuss some interesting connections between the semijoin algebra and the guarded fragment of first-order logic. We also provide an Ehrenfeucht-Fraisse game, characterizing the discerning power of the semijoin algebra. This game gives a method for showing that certain queries are not expressible in the semijoin algebra.",
        "published": "2004-07-02T15:44:32Z",
        "link": "http://arxiv.org/abs/cs/0407007v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3;F.4.1"
        ]
    },
    {
        "title": "A Framework for High-Accuracy Privacy-Preserving Mining",
        "authors": [
            "Shipra Agrawal",
            "Jayant R. Haritsa"
        ],
        "summary": "To preserve client privacy in the data mining process, a variety of techniques based on random perturbation of data records have been proposed recently. In this paper, we present a generalized matrix-theoretic model of random perturbation, which facilitates a systematic approach to the design of perturbation mechanisms for privacy-preserving mining. Specifically, we demonstrate that (a) the prior techniques differ only in their settings for the model parameters, and (b) through appropriate choice of parameter settings, we can derive new perturbation techniques that provide highly accurate mining results even under strict privacy guarantees. We also propose a novel perturbation mechanism wherein the model parameters are themselves characterized as random variables, and demonstrate that this feature provides significant improvements in privacy at a very marginal cost in accuracy.   While our model is valid for random-perturbation-based privacy-preserving mining in general, we specifically evaluate its utility here with regard to frequent-itemset mining on a variety of real datasets. The experimental results indicate that our mechanisms incur substantially lower identity and support errors as compared to the prior techniques.",
        "published": "2004-07-15T14:30:20Z",
        "link": "http://arxiv.org/abs/cs/0407035v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "The Revolution In Database System Architecture",
        "authors": [
            "Jim Gray"
        ],
        "summary": "Database system architectures are undergoing revolutionary changes. Algorithms and data are being unified by integrating programming languages with the database system. This gives an extensible object-relational system where non-procedural relational operators manipulate object sets. Coupled with this, each DBMS is now a web service. This has huge implications for how we structure applications. DBMSs are now object containers. Queues are the first objects to be added. These queues are the basis for transaction processing and workflow applica-tions. Future workflow systems are likely to be built on this core. Data cubes and online analytic processing are now baked into most DBMSs. Beyond that, DBMSs have a framework for data mining and machine learning algorithms. Decision trees, Bayes nets, clustering, and time series analysis are built in; new algorithms can be added. Text, temporal, and spatial data access methods, along with their probabilistic reasoning have been added to database systems. Allowing approximate and probabilistic answers is essential for many applications. Many believe that XML and xQuery will be the main data structure and access pattern. Database systems must accommodate that perspective.These changes mandate a much more dynamic query optimization strategy. Intelligence is moving to the periphery of the network. Each disk and each sensor will be a competent database machine. Relational algebra is a convenient way to program these systems. Database systems are now expected to be self-managing, self-healing, and always-up.",
        "published": "2004-08-14T02:31:50Z",
        "link": "http://arxiv.org/abs/cs/0408030v1",
        "categories": [
            "cs.DB",
            "C.4"
        ]
    },
    {
        "title": "There Goes the Neighborhood: Relational Algebra for Spatial Data Search",
        "authors": [
            "Jim Gray",
            "Alexander S. Szalay",
            "Aniruddha R. Thakar",
            "Gyorgy Fekete",
            "William O'Mullane",
            "Maria A. Nieto-Santisteban",
            "Gerd Heber",
            "Arnold H. Rots"
        ],
        "summary": "We explored ways of doing spatial search within a relational database: (1) hierarchical triangular mesh (a tessellation of the sphere), (2) a zoned bucketing system, and (3) representing areas as disjunctive-normal form constraints. Each of these approaches has merits. They all allow efficient point-in-region queries. A relational representation for regions allows Boolean operations among them and allows quick tests for point-in-region, regions-containing-point, and region-overlap. The speed of these algorithms is much improved by a zone and multi-scale zone-pyramid scheme. The approach has the virtue that the zone mechanism works well on B-Trees native to all SQL systems and integrates naturally with current query optimizers - rather than requiring a new spatial access method and concomitant query optimizer extensions. Over the last 5 years, we have used these techniques extensively in our work on SkyServer.sdss.org, and SkyQuery.net.",
        "published": "2004-08-14T02:40:59Z",
        "link": "http://arxiv.org/abs/cs/0408031v1",
        "categories": [
            "cs.DB",
            "C.4"
        ]
    },
    {
        "title": "Consensus on Transaction Commit",
        "authors": [
            "Jim Gray",
            "Leslie Lamport"
        ],
        "summary": "The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. Running a Paxos consensus algorithm on the commit/abort decision of each participant yields a transaction commit protocol that uses 2F +1 coordinators and makes progress if at least F +1 of them are working. In the fault-free case, this algorithm requires one extra message delay but has the same stable-storage write delay as Two-Phase Commit. The classic Two-Phase Commit algorithm is obtained as the special F = 0 case of the general Paxos Commit algorithm.",
        "published": "2004-08-14T21:23:41Z",
        "link": "http://arxiv.org/abs/cs/0408036v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.4"
        ]
    },
    {
        "title": "Medians and Beyond: New Aggregation Techniques for Sensor Networks",
        "authors": [
            "Nisheeth Shrivastava",
            "Chiranjeeb Buragohain",
            "Divyakant Agrawal",
            "Subhash Suri"
        ],
        "summary": "Wireless sensor networks offer the potential to span and monitor large geographical areas inexpensively. Sensors, however, have significant power constraint (battery life), making communication very expensive. Another important issue in the context of sensor-based information systems is that individual sensor readings are inherently unreliable. In order to address these two aspects, sensor database systems like TinyDB and Cougar enable in-network data aggregation to reduce the communication cost and improve reliability. The existing data aggregation techniques, however, are limited to relatively simple types of queries such as SUM, COUNT, AVG, and MIN/MAX. In this paper we propose a data aggregation scheme that significantly extends the class of queries that can be answered using sensor networks. These queries include (approximate) quantiles, such as the median, the most frequent data values, such as the consensus value, a histogram of the data distribution, as well as range queries. In our scheme, each sensor aggregates the data it has received from other sensors into a fixed (user specified) size message. We provide strict theoretical guarantees on the approximation quality of the queries in terms of the message size. We evaluate the performance of our aggregation scheme by simulation and demonstrate its accuracy, scalability and low resource utilization for highly variable input data sets.",
        "published": "2004-08-17T02:21:06Z",
        "link": "http://arxiv.org/abs/cs/0408039v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Scalable XSLT Evaluation",
        "authors": [
            "Zhimao Guo",
            "Min Li",
            "Xiaoling Wang",
            "Aoying Zhou"
        ],
        "summary": "XSLT is an increasingly popular language for processing XML data. It is widely supported by application platform software. However, little optimization effort has been made inside the current XSLT processing engines. Evaluating a very simple XSLT program on a large XML document with a simple schema may result in extensive usage of memory. In this paper, we present a novel notion of \\emph{Streaming Processing Model} (\\emph{SPM}) to evaluate a subset of XSLT programs on XML documents, especially large ones. With SPM, an XSLT processor can transform an XML source document to other formats without extra memory buffers required. Therefore, our approach can not only tackle large source documents, but also produce large results. We demonstrate with a performance study the advantages of the SPM approach. Experimental results clearly confirm that SPM improves XSLT evaluation typically 2 to 10 times better than the existing approaches. Moreover, the SPM approach also features high scalability.",
        "published": "2004-08-22T03:19:19Z",
        "link": "http://arxiv.org/abs/cs/0408051v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Providing Authentic Long-term Archival Access to Complex Relational Data",
        "authors": [
            "Stephan Heuscher",
            "Stephan Jaermann",
            "Peter Keller-Marxer",
            "Frank Moehle"
        ],
        "summary": "We discuss long-term preservation of and access to relational databases. The focus is on national archives and science data archives which have to ingest and integrate data from a broad spectrum of vendor-specific relational database management systems (RDBMS). Furthermore, we present our solution SIARD which analyzes and extracts data and data logic from almost any RDBMS. It enables, to a reasonable level of authenticity, complete detachment of databases from their vendor-specific environment. The user can add archival descriptive metadata according to a customizable schema. A SIARD database archive integrates data, data logic, technical metadata, and archival descriptive information in one archival information package, independent of any specific software and hardware, based upon plain text files and the standardized languages SQL and XML. For usage purposes, a SIARD archive can be reloaded into any current or future RDBMS which supports standard SQL. In addition, SIARD contains a client that enables 'on demand' reload of archives into a target RDBMS, and multi-user remote access for querying and browsing the data together with its technical and descriptive metadata in one graphical user interface.",
        "published": "2004-08-24T07:01:29Z",
        "link": "http://arxiv.org/abs/cs/0408054v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "A Generalized Disjunctive Paraconsistent Data Model for Negative and   Disjunctive Information",
        "authors": [
            "Haibin Wang",
            "Yuanchun He",
            "Rajshekhar Sunderraman"
        ],
        "summary": "This paper presents a generalization of the disjunctive paraconsistent relational data model in which disjunctive positive and negative information can be represented explicitly and manipulated. There are situations where the closed world assumption to infer negative facts is not valid or undesirable and there is a need to represent and reason with negation explicitly. We consider explicit disjunctive negation in the context of disjunctive databases as there is an interesting interplay between these two types of information. Generalized disjunctive paraconsistent relation is introduced as the main structure in this model. The relational algebra is appropriately generalized to work on generalized disjunctive paraconsistent relations and their correctness is established.",
        "published": "2004-09-11T11:02:30Z",
        "link": "http://arxiv.org/abs/cs/0409020v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "The Infati Data",
        "authors": [
            "C. S. Jensen",
            "H. Lahrmann",
            "S. Pakalnis",
            "J. Runge"
        ],
        "summary": "The ability to perform meaningful empirical studies is of essence in research in spatio-temporal query processing. Such studies are often necessary to gain detailed insight into the functional and performance characteristics of proposals for new query processing techniques.   We present a collection of spatio-temporal data, collected during an intelligent speed adaptation project, termed INFATI, in which some two dozen cars equipped with GPS receivers and logging equipment took part. We describe how the data was collected and how it was \"modified\" to afford the drivers some degree of anonymity.   We also present the road network in which the cars were moving during data collection.   The GPS data is publicly available for non-commercial purposes. It is our hope that this resource will help the spatio-temporal research community in its efforts to develop new and better query processing techniques.",
        "published": "2004-10-01T16:55:38Z",
        "link": "http://arxiv.org/abs/cs/0410001v2",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "Frequent Knot Discovery",
        "authors": [
            "Floris Geerts"
        ],
        "summary": "We explore the possibility of applying the framework of frequent pattern mining to a class of continuous objects appearing in nature, namely knots. We introduce the frequent knot mining problem and present a solution. The key observation is that a database consisting of knots can be transformed into a transactional database. This observation is based on the Prime Decomposition Theorem of knots.",
        "published": "2004-10-16T13:14:54Z",
        "link": "http://arxiv.org/abs/cs/0410038v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "An Extended Generalized Disjunctive Paraconsistent Data Model for   Disjunctive Information",
        "authors": [
            "Haibin Wang",
            "Hao Tian",
            "Rajshekhar Sunderraman"
        ],
        "summary": "This paper presents an extension of generalized disjunctive paraconsistent relational data model in which pure disjunctive positive and negative information as well as mixed disjunctive positive and negative information can be represented explicitly and manipulated. We consider explicit mixed disjunctive information in the context of disjunctive databases as there is an interesting interplay between these two types of information. Extended generalized disjunctive paraconsistent relation is introduced as the main structure in this model. The relational algebra is appropriately generalized to work on extended generalized disjunctive paraconsistent relations and their correctness is established.",
        "published": "2004-10-20T08:50:22Z",
        "link": "http://arxiv.org/abs/cs/0410053v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Paraconsistent Intuitionistic Fuzzy Relational Data Model",
        "authors": [
            "Rajshekhar Sunderraman",
            "Haibin Wang"
        ],
        "summary": "In this paper, we present a generalization of the relational data model based on paraconsistent intuitionistic fuzzy sets. Our data model is capable of manipulating incomplete as well as inconsistent information. Fuzzy relation or intuitionistic fuzzy relation can only handle incomplete information. Associated with each relation are two membership functions one is called truth-membership function $T$ which keeps track of the extent to which we believe the tuple is in the relation, another is called false-membership function which keeps track of the extent to which we believe that it is not in the relation. A paraconsistent intuitionistic fuzzy relation is inconsistent if there exists one tuple $a$ such that $T(a) + F(a) > 1$. In order to handle inconsistent situation, we propose an operator called split to transform inconsistent paraconsistent intuitionistic fuzzy relations into pseudo-consistent paraconsistent intuitionistic fuzzy relations and do the set-theoretic and relation-theoretic operations on them and finally use another operator called combine to transform the result back to paraconsistent intuitionistic fuzzy relation. For this model, we define algebraic operators that are generalisations of the usual operators such as union, selection, join on fuzzy relations. Our data model can underlie any database and knowledge-base management system that deals with incomplete and inconsistent information.",
        "published": "2004-10-20T11:40:50Z",
        "link": "http://arxiv.org/abs/cs/0410054v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Using image partitions in 4th Dimension",
        "authors": [
            "Giovanni Gasparri"
        ],
        "summary": "I have plotted an image by using mathematical functions in the Database \"4th Dimension\". I'm going to show an alternative method to: detect which sector has been clicked; highlight it and combine it with other sectors already highlighted; store the graph information in an efficient way; load and splat image layers to reconstruct the stored graph.",
        "published": "2004-10-26T17:00:40Z",
        "link": "http://arxiv.org/abs/cs/0410070v1",
        "categories": [
            "cs.DB",
            "C.2.4"
        ]
    },
    {
        "title": "A FP-Tree Based Approach for Mining All Strongly Correlated Pairs   without Candidate Generation",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "Given a user-specified minimum correlation threshold and a transaction database, the problem of mining all-strong correlated pairs is to find all item pairs with Pearson's correlation coefficients above the threshold . Despite the use of upper bound based pruning technique in the Taper algorithm [1], when the number of items and transactions are very large, candidate pair generation and test is still costly. To avoid the costly test of a large number of candidate pairs, in this paper, we propose an efficient algorithm, called Tcp, based on the well-known FP-tree data structure, for mining the complete set of all-strong correlated item pairs. Our experimental results on both synthetic and real world datasets show that, Tcp's performance is significantly better than that of the previously developed Taper algorithm over practical ranges of correlation threshold specifications.",
        "published": "2004-11-12T12:02:17Z",
        "link": "http://arxiv.org/abs/cs/0411035v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Modeling Complex Higher Order Patterns",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The goal of this paper is to show that generalizing the notion of frequent patterns can be useful in extending association analysis to more complex higher order patterns. To that end, we describe a general framework for modeling a complex pattern based on evaluating the interestingness of its sub-patterns. A key goal of any framework is to allow people to more easily express, explore, and communicate ideas, and hence, we illustrate how our framework can be used to describe a variety of commonly used patterns, such as frequent patterns, frequent closed patterns, indirect association patterns, hub patterns and authority patterns. To further illustrate the usefulness of the framework, we also present two new kinds of patterns that derived from the framework: clique pattern and bi-clique pattern and illustrate their practical use.",
        "published": "2004-12-04T12:28:29Z",
        "link": "http://arxiv.org/abs/cs/0412018v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Deployment of a Grid-based Medical Imaging Application",
        "authors": [
            "S R Amendolia",
            "F Estrella",
            "C del Frate",
            "J Galvez",
            "W Hassan",
            "T Hauer",
            "D Manset",
            "R McClatchey",
            "M Odeh",
            "D Rogulin",
            "T Solomonides",
            "R Warren"
        ],
        "summary": "The MammoGrid project has deployed its Service-Oriented Architecture (SOA)-based Grid application in a real environment comprising actual participating hospitals. The resultant setup is currently being exploited to conduct rigorous in-house tests in the first phase before handing over the setup to the actual clinicians to get their feedback. This paper elaborates the deployment details and the experiences acquired during this phase of the project. Finally the strategy regarding migration to an upcoming middleware from EGEE project will be described. This paper concludes by highlighting some of the potential areas of future work.",
        "published": "2004-12-08T17:15:06Z",
        "link": "http://arxiv.org/abs/cs/0412035v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Reverse Engineering Ontology to Conceptual Data Models",
        "authors": [
            "Haya El-Ghalayini",
            "Mohammed Odeh",
            "Richard McClatchey",
            "Tony Solomonides"
        ],
        "summary": "Ontologies facilitate the integration of heterogeneous data sources by resolving semantic heterogeneity between them. This research aims to study the possibility of generating a domain conceptual model from a given ontology with the vision to grow this generated conceptual data model into a global conceptual model integrating a number of existing data and information sources. Based on ontologically derived semantics of the BWW model, rules are identified that map elements of the ontology language (DAML+OIL) to domain conceptual model elements. This mapping is demonstrated using TAMBIS ontology. A significant corollary of this study is that it is possible to generate a domain conceptual model from a given ontology subject to validation that needs to be performed by the domain specialist before evolving this model into a global conceptual model.",
        "published": "2004-12-08T17:23:07Z",
        "link": "http://arxiv.org/abs/cs/0412036v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Clustering Categorical Data Streams",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng",
            "Joshua Zhexue Huang"
        ],
        "summary": "The data stream model has been defined for new classes of applications involving massive data being generated at a fast pace. Web click stream analysis and detection of network intrusions are two examples. Cluster analysis on data streams becomes more difficult, because the data objects in a data stream must be accessed in order and can be read only once or few times with limited resources. Recently, a few clustering algorithms have been developed for analyzing numeric data streams. However, to our knowledge to date, no algorithm exists for clustering categorical data streams. In this paper, we propose an efficient clustering algorithm for analyzing categorical data streams. It has been proved that the proposed algorithm uses small memory footprints. We provide empirical analysis on the performance of the algorithm in clustering both synthetic and real data streams",
        "published": "2004-12-13T06:14:20Z",
        "link": "http://arxiv.org/abs/cs/0412058v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "The Google Similarity Distance",
        "authors": [
            "Rudi Cilibrasi",
            "Paul M. B. Vitanyi"
        ],
        "summary": "Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers the equivalent of `society' is `database,' and the equivalent of `use' is `way to search the database.' We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts we use the world-wide-web as database, and Google as search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the world-wide-web using Google page counts. The world-wide-web is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies, and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87% with the expert crafted WordNet categories.",
        "published": "2004-12-21T16:05:36Z",
        "link": "http://arxiv.org/abs/cs/0412098v3",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "I.2.4; I.2.7"
        ]
    },
    {
        "title": "Unitary Space Time Constellation Analysis: An Upper Bound for the   Diversity",
        "authors": [
            "Guangyue Han",
            "Joachim Rosenthal"
        ],
        "summary": "The diversity product and the diversity sum are two very important parameters for a good-performing unitary space time constellation. A basic question is what the maximal diversity product (or sum) is. In this paper we are going to derive general upper bounds on the diversity sum and the diversity product for unitary constellations of any dimension $n$ and any size $m$ using packing techniques on the compact Lie group U(n).",
        "published": "2004-01-06T16:44:05Z",
        "link": "http://arxiv.org/abs/math/0401045v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "68P30, 94A05"
        ]
    },
    {
        "title": "Generalized PSK in Space Time Coding",
        "authors": [
            "Guangyue Han"
        ],
        "summary": "A wireless communication system using multiple antennas promises reliable transmission under Rayleigh flat fading assumptions. Design criteria and practical schemes have been presented for both coherent and non-coherent communication channels. In this paper we generalize one dimensional phase shift keying (PSK) signals and introduce space time constellations from generalized phase shift keying (GPSK) signals based on the complex and real orthogonal designs. The resulting space time constellations reallocate the energy for each transmitting antenna and feature good diversity products, consequently their performances are better than some of the existing comparable codes. Moreover since the maximum likelihood (ML) decoding of our proposed codes can be decomposed to one dimensional PSK signal demodulation, the ML decoding of our codes can be implemented in a very efficient way.",
        "published": "2004-01-14T16:56:55Z",
        "link": "http://arxiv.org/abs/math/0401157v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "math.OC",
            "68P30, 94A05"
        ]
    },
    {
        "title": "Backward Optimized Orthogonal Matching Pursuit",
        "authors": [
            "M. Andrle",
            "L. Rebollo-Neira",
            "E. Sagianos"
        ],
        "summary": "A recursive approach for shrinking coefficients of an atomic decomposition is proposed. The corresponding algorithm evolves so as to provide at each iteration a) the orthogonal projection of a signal onto a reduced subspace and b) the index of the coefficient to be disregarded in order to construct a coarser approximation minimizing the norm of the residual error.",
        "published": "2004-01-21T19:01:34Z",
        "link": "http://arxiv.org/abs/math/0401279v1",
        "categories": [
            "math.GM",
            "cs.IT",
            "math.IT",
            "41A45"
        ]
    },
    {
        "title": "Artificial Sequences and Complexity Measures",
        "authors": [
            "Andrea Baronchelli",
            "Emanuele Caglioti",
            "Vittorio Loreto"
        ],
        "summary": "In this paper we exploit concepts of information theory to address the fundamental problem of identifying and defining the most suitable tools to extract, in a automatic and agnostic way, information from a generic string of characters. We introduce in particular a class of methods which use in a crucial way data compression techniques in order to define a measure of remoteness and distance between pairs of sequences of characters (e.g. texts) based on their relative information content. We also discuss in detail how specific features of data compression techniques could be used to introduce the notion of dictionary of a given sequence and of Artificial Text and we show how these new tools can be used for information extraction purposes. We point out the versatility and generality of our method that applies to any kind of corpora of character strings independently of the type of coding behind them. We consider as a case study linguistic motivated problems and we present results for automatic language recognition, authorship attribution and self consistent-classification.",
        "published": "2004-03-09T09:56:19Z",
        "link": "http://arxiv.org/abs/cond-mat/0403233v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.IR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Distribution of Mutual Information from Complete and Incomplete Data",
        "authors": [
            "Marcus Hutter",
            "Marco Zaffalon"
        ],
        "summary": "Mutual information is widely used, in a descriptive way, to measure the stochastic dependence of categorical random variables. In order to address questions such as the reliability of the descriptive value, one must consider sample-to-population inferential approaches. This paper deals with the posterior distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean, and analytical approximations for the variance, skewness and kurtosis are derived. These approximations have a guaranteed accuracy level of the order O(1/n^3), where n is the sample size. Leading order approximations for the mean and the variance are derived in the case of incomplete samples. The derived analytical expressions allow the distribution of mutual information to be approximated reliably and quickly. In fact, the derived expressions can be computed with the same order of complexity needed for descriptive mutual information. This makes the distribution of mutual information become a concrete alternative to descriptive mutual information in many applications which would benefit from moving to the inductive side. Some of these prospective applications are discussed, and one of them, namely feature selection, is shown to perform significantly better when inductive mutual information is used.",
        "published": "2004-03-15T16:33:55Z",
        "link": "http://arxiv.org/abs/cs/0403025v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2"
        ]
    },
    {
        "title": "Remarks on codes from modular curves: MAGMA applications",
        "authors": [
            "David Joyner",
            "Salahoddin Shokranian"
        ],
        "summary": "Expository paper discussing AG or Goppa codes arising from curves, first from an abstract general perspective then turning to concrete examples associated to modular curves. We will try to explain these extremely technical ideas using a special case at a level to a typical graduate student with some background in modular forms, number theory, group theory, and algebraic geometry. Many examples using MAGMA are included.",
        "published": "2004-03-31T18:19:37Z",
        "link": "http://arxiv.org/abs/math/0403548v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "14H37, 94B27,20C20,11T71,14G50,05E20,14Q05"
        ]
    },
    {
        "title": "Asymptotic Improvement of the Gilbert-Varshamov Bound on the Size of   Binary Codes",
        "authors": [
            "Tao Jiang",
            "Alexander Vardy"
        ],
        "summary": "Given positive integers $n$ and $d$, let $A_2(n,d)$ denote the maximum size of a binary code of length $n$ and minimum distance $d$. The well-known Gilbert-Varshamov bound asserts that $A_2(n,d) \\geq 2^n/V(n,d-1)$, where $V(n,d) = \\sum_{i=0}^{d} {n \\choose i}$ is the volume of a Hamming sphere of radius $d$. We show that, in fact, there exists a positive constant $c$ such that $$ A_2(n,d) \\geq c \\frac{2^n}{V(n,d-1)} \\log_2 V(n,d-1) $$ whenever $d/n \\le 0.499$. The result follows by recasting the Gilbert- Varshamov bound into a graph-theoretic framework and using the fact that the corresponding graph is locally sparse. Generalizations and extensions of this result are briefly discussed.",
        "published": "2004-04-19T02:18:47Z",
        "link": "http://arxiv.org/abs/math/0404325v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.AC",
            "math.IT",
            "05C90, 94B65 (Primary) 05A16, 05C69 (secondary)"
        ]
    },
    {
        "title": "Maximum-likelihood decoding of Reed-Solomon Codes is NP-hard",
        "authors": [
            "Venkatesan Guruswami",
            "Alexander Vardy"
        ],
        "summary": "Maximum-likelihood decoding is one of the central algorithmic problems in coding theory. It has been known for over 25 years that maximum-likelihood decoding of general linear codes is NP-hard. Nevertheless, it was so far unknown whether maximum- likelihood decoding remains hard for any specific family of codes with nontrivial algebraic structure. In this paper, we prove that maximum-likelihood decoding is NP-hard for the family of Reed-Solomon codes. We moreover show that maximum-likelihood decoding of Reed-Solomon codes remains hard even with unlimited preprocessing, thereby strengthening a result of Bruck and Naor.",
        "published": "2004-05-04T04:46:32Z",
        "link": "http://arxiv.org/abs/cs/0405005v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.IT",
            "math.IT",
            "E.4; F.1.3; F.2.1"
        ]
    },
    {
        "title": "On the List and Bounded Distance Decodibility of the Reed-Solomon Codes",
        "authors": [
            "Qi Cheng",
            "Daqing Wan"
        ],
        "summary": "In this paper show that the list and bounded-distance decoding problems of certain bounds for the Reed-Solomon code are at least as hard as the discrete logarithm problem over finite fields.",
        "published": "2004-05-05T17:36:43Z",
        "link": "http://arxiv.org/abs/math/0405082v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.IT",
            "11Y16; 68Q25"
        ]
    },
    {
        "title": "Least Dependent Component Analysis Based on Mutual Information",
        "authors": [
            "Harald Stögbauer",
            "Alexander Kraskov",
            "Sergey A. Astakhov",
            "Peter Grassberger"
        ],
        "summary": "We propose to use precise estimators of mutual information (MI) to find least dependent components in a linearly mixed signal. On the one hand this seems to lead to better blind source separation than with any other presently available algorithm. On the other hand it has the advantage, compared to other implementations of `independent' component analysis (ICA) some of which are based on crude approximations for MI, that the numerical values of the MI can be used for:   (i) estimating residual dependencies between the output components;   (ii) estimating the reliability of the output, by comparing the pairwise MIs with those of re-mixed components;   (iii) clustering the output according to the residual interdependencies.   For the MI estimator we use a recently proposed k-nearest neighbor based algorithm. For time sequences we combine this with delay embedding, in order to take into account non-trivial time correlations. After several tests with artificial data, we apply the resulting MILCA (Mutual Information based Least dependent Component Analysis) algorithm to a real-world dataset, the ECG of a pregnant woman.   The software implementation of the MILCA algorithm is freely available at http://www.fz-juelich.de/nic/cs/software",
        "published": "2004-05-10T14:58:17Z",
        "link": "http://arxiv.org/abs/physics/0405044v2",
        "categories": [
            "physics.comp-ph",
            "cs.IT",
            "math.IT",
            "physics.data-an",
            "q-bio.QM"
        ]
    },
    {
        "title": "Knowledge Reduction and Discovery based on Demarcation Information",
        "authors": [
            "Yuguo He"
        ],
        "summary": "Knowledge reduction, includes attribute reduction and value reduction, is an important topic in rough set literature. It is also closely relevant to other fields, such as machine learning and data mining. In this paper, an algorithm called TWI-SQUEEZE is proposed. It can find a reduct, or an irreducible attribute subset after two scans. Its soundness and computational complexity are given, which show that it is the fastest algorithm at present. A measure of variety is brought forward, of which algorithm TWI-SQUEEZE can be regarded as an application. The author also argues the rightness of this measure as a measure of information, which can make it a unified measure for \"differentiation, a concept appeared in cognitive psychology literature. Value reduction is another important aspect of knowledge reduction. It is interesting that using the same algorithm we can execute a complete value reduction efficiently. The complete knowledge reduction, which results in an irreducible table, can therefore be accomplished after four scans of table. The byproducts of reduction are two classifiers of different styles. In this paper, various cases and models will be discussed to prove the efficiency and effectiveness of the algorithm. Some topics, such as how to integrate user preference to find a local optimal attribute subset will also be discussed.",
        "published": "2004-05-27T11:26:18Z",
        "link": "http://arxiv.org/abs/cs/0405104v1",
        "categories": [
            "cs.LG",
            "cs.DB",
            "cs.IT",
            "math.IT",
            "H.2.8;I.2.6; H.1.1;I.5.2"
        ]
    },
    {
        "title": "Side-Information Coding with Turbo Codes and its Application to Quantum   Key Distribution",
        "authors": [
            "Kim-Chi Nguyen",
            "Gilles Van Assche",
            "Nicolas J. Cerf"
        ],
        "summary": "Turbo coding is a powerful class of forward error correcting codes, which can achieve performances close to the Shannon limit. The turbo principle can be applied to the problem of side-information source coding, and we investigate here its application to the reconciliation problem occurring in a continuous-variable quantum key distribution protocol.",
        "published": "2004-06-01T09:26:00Z",
        "link": "http://arxiv.org/abs/cs/0406001v1",
        "categories": [
            "cs.IT",
            "cs.CR",
            "math.IT",
            "quant-ph",
            "E.3"
        ]
    },
    {
        "title": "A tutorial introduction to the minimum description length principle",
        "authors": [
            "Peter Grunwald"
        ],
        "summary": "This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection \"Advances in Minimum Description Length: Theory and Application\" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).",
        "published": "2004-06-04T09:11:18Z",
        "link": "http://arxiv.org/abs/math/0406077v1",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "stat.TH",
            "6201; 6801; 68T05; 68T10; 9401"
        ]
    },
    {
        "title": "Maximum Entropy Multivariate Density Estimation: An exact   goodness-of-fit approach",
        "authors": [
            "Sabbir Rahman",
            "Mahbub Majumdar"
        ],
        "summary": "We consider the problem of estimating the population probability distribution given a finite set of multivariate samples, using the maximum entropy approach. In strict keeping with Jaynes' original definition, our precise formulation of the problem considers contributions only from the smoothness of the estimated distribution (as measured by its entropy) and the loss functional associated with its goodness-of-fit to the sample data, and in particular does not make use of any additional constraints that cannot be justified from the sample data alone. By mapping the general multivariate problem to a tractable univariate one, we are able to write down exact expressions for the goodness-of-fit of an arbitrary multivariate distribution to any given set of samples using both the traditional likelihood-based approach and a rigorous information-theoretic approach, thus solving a long-standing problem. As a corollary we also give an exact solution to the `forward problem' of determining the expected distributions of samples taken from a population with known probability distribution.",
        "published": "2004-06-06T17:03:15Z",
        "link": "http://arxiv.org/abs/physics/0406023v5",
        "categories": [
            "physics.data-an",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Information theory, multivariate dependence, and genetic network   inference",
        "authors": [
            "Ilya Nemenman"
        ],
        "summary": "We define the concept of dependence among multiple variables using maximum entropy techniques and introduce a graphical notation to denote the dependencies. Direct inference of information theoretic quantities from data uncovers dependencies even in undersampled regimes when the joint probability distribution cannot be reliably estimated. The method is tested on synthetic data. We anticipate it to be useful for inference of genetic circuits and other biological signaling networks.",
        "published": "2004-06-07T05:38:58Z",
        "link": "http://arxiv.org/abs/q-bio/0406015v1",
        "categories": [
            "q-bio.QM",
            "cs.IT",
            "math.IT",
            "math.ST",
            "physics.data-an",
            "q-bio.GN",
            "stat.TH"
        ]
    },
    {
        "title": "Suboptimal behaviour of Bayes and MDL in classification under   misspecification",
        "authors": [
            "Peter Grunwald",
            "John Langford"
        ],
        "summary": "We show that forms of Bayesian and MDL inference that are often applied to classification problems can be *inconsistent*. This means there exists a learning problem such that for all amounts of data the generalization errors of the MDL classifier and the Bayes classifier relative to the Bayesian posterior both remain bounded away from the smallest achievable generalization error.",
        "published": "2004-06-10T16:36:54Z",
        "link": "http://arxiv.org/abs/math/0406221v1",
        "categories": [
            "math.ST",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "stat.TH",
            "62A01; 68T05; 68T10"
        ]
    },
    {
        "title": "Long Nonbinary Codes Exceeding the Gilbert - Varshamov Bound for any   Fixed Distance",
        "authors": [
            "Sergey Yekhanin",
            "Ilya Dumer"
        ],
        "summary": "Let A(q,n,d) denote the maximum size of a q-ary code of length n and distance d. We study the minimum asymptotic redundancy \\rho(q,n,d)=n-log_q A(q,n,d) as n grows while q and d are fixed. For any d and q<=d-1, long algebraic codes are designed that improve on the BCH codes and have the lowest asymptotic redundancy \\rho(q,n,d) <= ((d-3)+1/(d-2)) log_q n known to date. Prior to this work, codes of fixed distance that asymptotically surpass BCH codes and the Gilbert-Varshamov bound were designed only for distances 4,5 and 6.",
        "published": "2004-06-21T17:56:14Z",
        "link": "http://arxiv.org/abs/cs/0406039v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On Expanders Graphs: Parameters and Applications",
        "authors": [
            "H. L. Janwa A. K. Lal"
        ],
        "summary": "We give a new lower bound on the expansion coefficient of an edge-vertex graph of a $d$-regular graph. As a consequence, we obtain an improvement on the lower bound on relative minimum distance of the expander codes constructed by Sipser and Spielman. We also derive some improved results on the vertex expansion of graphs that help us in improving the parameters of the expander codes of Alon, Bruck, Naor, Naor, and Roth.",
        "published": "2004-06-25T05:20:57Z",
        "link": "http://arxiv.org/abs/cs/0406048v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Finite-Length Scaling for Iteratively Decoded LDPC Ensembles",
        "authors": [
            "Abdelaziz Amraoui",
            "Andrea Montanari",
            "Tom Richardson",
            "Ruediger Urbanke"
        ],
        "summary": "In this paper we investigate the behavior of iteratively decoded low-density parity-check codes over the binary erasure channel in the so-called ``waterfall region.\" We show that the performance curves in this region follow a very basic scaling law. We conjecture that essentially the same scaling behavior applies in a much more general setting and we provide some empirical evidence to support this conjecture. The scaling law, together with the error floor expressions developed previously, can be used for fast finite-length optimization.",
        "published": "2004-06-26T11:52:17Z",
        "link": "http://arxiv.org/abs/cs/0406050v1",
        "categories": [
            "cs.IT",
            "cond-mat.dis-nn",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "Zero-error communication over networks",
        "authors": [
            "Jürg Wullschleger"
        ],
        "summary": "Zero-Error communication investigates communication without any error. By defining channels without probabilities, results from Elias can be used to completely characterize which channel can simulate which other channels. We introduce the ambiguity of a channel, which completely characterizes the possibility in principle of a channel to simulate any other channel. In the second part we will look at networks of players connected by channels, while some players may be corrupted. We will show how the ambiguity of a virtual channel connecting two arbitrary players can be calculated. This means that we can exactly specify what kind of zero-error communication is possible between two players in any network of players connected by channels.",
        "published": "2004-07-01T17:27:21Z",
        "link": "http://arxiv.org/abs/cs/0407004v1",
        "categories": [
            "cs.IT",
            "cs.CR",
            "math.IT"
        ]
    },
    {
        "title": "Improved error bounds for the erasure/list scheme: the binary and   spherical cases",
        "authors": [
            "Alexander Barg"
        ],
        "summary": "We derive improved bounds on the error and erasure rate for spherical codes and for binary linear codes under Forney's erasure/list decoding scheme and prove some related results.",
        "published": "2004-07-04T21:13:20Z",
        "link": "http://arxiv.org/abs/cs/0407010v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Distance distribution of binary codes and the error probability of   decoding",
        "authors": [
            "Alexander Barg",
            "Andrew McGregor"
        ],
        "summary": "We address the problem of bounding below the probability of error under maximum likelihood decoding of a binary code with a known distance distribution used on a binary symmetric channel. An improved upper bound is given for the maximum attainable exponent of this probability (the reliability function of the channel). In particular, we prove that the ``random coding exponent'' is the true value of the channel reliability for code rate $R$ in some interval immediately below the critical rate of the channel. An analogous result is obtained for the Gaussian channel.",
        "published": "2004-07-04T21:49:43Z",
        "link": "http://arxiv.org/abs/cs/0407011v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Convergence Speed of MDL Predictions for Bernoulli Sequences",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We consider the Minimum Description Length principle for online sequence prediction. If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure: (a) this quantity is bounded, implying convergence with probability one, and (b) it additionally specifies a `rate of convergence'. Generally, for MDL only exponential loss bounds hold, as opposed to the linear bounds for a Bayes mixture. We show that this is even the case if the model class contains only Bernoulli distributions. We derive a new upper bound on the prediction error for countable Bernoulli classes. This implies a small bound (comparable to the one for Bayes mixtures) for certain important model classes. The results apply to many Machine Learning tasks including classification and hypothesis testing. We provide arguments that our theorems generalize to countable classes of i.i.d. models.",
        "published": "2004-07-16T10:36:49Z",
        "link": "http://arxiv.org/abs/cs/0407039v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.PR",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Universal Convergence of Semimeasures on Individual Random Sequences",
        "authors": [
            "Marcus Hutter",
            "Andrej Muchnik"
        ],
        "summary": "Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.",
        "published": "2004-07-23T12:43:28Z",
        "link": "http://arxiv.org/abs/cs/0407057v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.IT",
            "math.IT",
            "math.PR",
            "I.2.6; E.4; G.3; F.1.3"
        ]
    },
    {
        "title": "Tight bounds for LDPC and LDGM codes under MAP decoding",
        "authors": [
            "Andrea Montanari"
        ],
        "summary": "A new method for analyzing low density parity check (LDPC) codes and low density generator matrix (LDGM) codes under bit maximum a posteriori probability (MAP) decoding is introduced. The method is based on a rigorous approach to spin glasses developed by Francesco Guerra. It allows to construct lower bounds on the entropy of the transmitted message conditional to the received one. Based on heuristic statistical mechanics calculations, we conjecture such bounds to be tight. The result holds for standard irregular ensembles when used over binary input output symmetric channels. The method is first developed for Tanner graph ensembles with Poisson left degree distribution. It is then generalized to `multi-Poisson' graphs, and, by a completion procedure, to arbitrary degree distribution.",
        "published": "2004-07-25T17:56:44Z",
        "link": "http://arxiv.org/abs/cs/0407060v2",
        "categories": [
            "cs.IT",
            "cond-mat.dis-nn",
            "math.IT"
        ]
    },
    {
        "title": "Iterative Quantization Using Codes On Graphs",
        "authors": [
            "Emin Martinian",
            "Jonathan S. Yedidia"
        ],
        "summary": "We study codes on graphs combined with an iterative message passing algorithm for quantization. Specifically, we consider the binary erasure quantization (BEQ) problem which is the dual of the binary erasure channel (BEC) coding problem. We show that duals of capacity achieving codes for the BEC yield codes which approach the minimum possible rate for the BEQ. In contrast, low density parity check codes cannot achieve the minimum rate unless their density grows at least logarithmically with block length. Furthermore, we show that duals of efficient iterative decoding algorithms for the BEC yield efficient encoding algorithms for the BEQ. Hence our results suggest that graphical models may yield near optimal codes in source coding as well as in channel coding and that duality plays a key role in such constructions.",
        "published": "2004-08-02T21:52:55Z",
        "link": "http://arxiv.org/abs/cs/0408008v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Entropy Maximization as a Holistic Design Principle for Complex Optimal   Networks and the Emergence of Power Laws",
        "authors": [
            "Venkat Venkatasubramanian",
            "Dimitris N. Politis",
            "Priyan R. Patkar"
        ],
        "summary": "We present a general holistic theory for the organization of complex networks, both human-engineered and naturally-evolved. Introducing concepts of value of interactions and satisfaction as generic network performance measures, we show that the underlying organizing principle is to meet an overall performance target for wide-ranging operating or environmental conditions. This design or survival requirement of reliable performance under uncertainty leads, via the maximum entropy principle, to the emergence of a power law vertex degree distribution. The theory also predicts exponential or Poisson degree distributions depending on network redundancy, thus explaining all three regimes as different manifestations of a common underlying phenomenon within a unified theoretical framework.",
        "published": "2004-08-03T10:58:02Z",
        "link": "http://arxiv.org/abs/nlin/0408007v1",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT",
            "q-bio.QM"
        ]
    },
    {
        "title": "The asymptotic number of binary codes and binary matroids",
        "authors": [
            "Marcel Wild"
        ],
        "summary": "The asyptotic number of nonequivalent binary n-codes is determined. This is also the asymptotic number of nonisomorphic binary n-matroids. The connection to a result of Lefmann, Roedl, Phelps is explored. The latter states that almost all binary n-codes have a trivial automorphism group.",
        "published": "2004-08-04T14:46:49Z",
        "link": "http://arxiv.org/abs/cs/0408011v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "Improved Upper Bound for the Redundancy of Fix-Free Codes",
        "authors": [
            "Sergey Yekhanin"
        ],
        "summary": "A variable-length code is a fix-free code if no codeword is a prefix or a suffix of any other codeword. In a fix-free code any finite sequence of codewords can be decoded in both directions, which can improve the robustness to channel noise and speed up the decoding process. In this paper we prove a new sufficient condition of the existence of fix-free codes and improve the upper bound on the redundancy of optimal fix-free codes.",
        "published": "2004-08-05T20:45:33Z",
        "link": "http://arxiv.org/abs/cs/0408017v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The Dynamics of Group Codes: Dual Abelian Group Codes and Systems",
        "authors": [
            "G. David Forney Jr.",
            "Mitchell D. Trott"
        ],
        "summary": "Fundamental results concerning the dynamics of abelian group codes (behaviors) and their duals are developed. Duals of sequence spaces over locally compact abelian groups may be defined via Pontryagin duality; dual group codes are orthogonal subgroups of dual sequence spaces. The dual of a complete code or system is finite, and the dual of a Laurent code or system is (anti-)Laurent. If C and C^\\perp are dual codes, then the state spaces of C act as the character groups of the state spaces of C^\\perp. The controllability properties of C are the observability properties of C^\\perp. In particular, C is (strongly) controllable if and only if C^\\perp is (strongly) observable, and the controller memory of C is the observer memory of C^\\perp. The controller granules of C act as the character groups of the observer granules of C^\\perp. Examples of minimal observer-form encoder and syndrome-former constructions are given. Finally, every observer granule of C is an \"end-around\" controller granule of C.",
        "published": "2004-08-16T16:05:06Z",
        "link": "http://arxiv.org/abs/cs/0408038v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Source Coding With Distortion Side Information At The Encoder",
        "authors": [
            "Emin Martinian",
            "Gregory W. Wornell",
            "Ram Zamir"
        ],
        "summary": "We consider lossy source coding when side information affecting the distortion measure may be available at the encoder, decoder, both, or neither. For example, such distortion side information can model reliabilities for noisy measurements, sensor calibration information, or perceptual effects like masking and sensitivity to context. When the distortion side information is statistically independent of the source, we show that in many cases (e.g, for additive or multiplicative distortion side information) there is no penalty for knowing the side information only at the encoder, and there is no advantage to knowing it at the decoder. Furthermore, for quadratic distortion measures scaled by the distortion side information, we evaluate the penalty for lack of encoder knowledge and show that it can be arbitrarily large. In this scenario, we also sketch transform based quantizers constructions which efficiently exploit encoder side information in the high-resolution limit.",
        "published": "2004-08-27T14:25:13Z",
        "link": "http://arxiv.org/abs/cs/0408062v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Robust Locally Testable Codes and Products of Codes",
        "authors": [
            "Eli Ben-Sasson",
            "Madhu Sudan"
        ],
        "summary": "We continue the investigation of locally testable codes, i.e., error-correcting codes for whom membership of a given word in the code can be tested probabilistically by examining it in very few locations. We give two general results on local testability: First, motivated by the recently proposed notion of {\\em robust} probabilistically checkable proofs, we introduce the notion of {\\em robust} local testability of codes. We relate this notion to a product of codes introduced by Tanner, and show a very simple composition lemma for this notion. Next, we show that codes built by tensor products can be tested robustly and somewhat locally, by applying a variant of a test and proof technique introduced by Raz and Safra in the context of testing low-degree multivariate polynomials (which are a special case of tensor codes).   Combining these two results gives us a generic construction of codes of inverse polynomial rate, that are testable with poly-logarithmically many queries. We note these locally testable tensor codes can be obtained from {\\em any} linear error correcting code with good distance. Previous results on local testability, albeit much stronger quantitatively, rely heavily on algebraic properties of the underlying codes.",
        "published": "2004-08-30T16:36:38Z",
        "link": "http://arxiv.org/abs/cs/0408066v1",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Distance properties of expander codes",
        "authors": [
            "Alexander Barg",
            "Gilles Zemor"
        ],
        "summary": "We study the minimum distance of codes defined on bipartite graphs. Weight spectrum and the minimum distance of a random ensemble of such codes are computed. It is shown that if the vertex codes have minimum distance $\\ge 3$, the overall code is asymptotically good, and sometimes meets the Gilbert-Varshamov bound.   Constructive families of expander codes are presented whose minimum distance asymptotically exceeds the product bound for all code rates between 0 and 1.",
        "published": "2004-09-07T21:02:08Z",
        "link": "http://arxiv.org/abs/cs/0409010v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "Shannon meets Wiener II: On MMSE estimation in successive decoding   schemes",
        "authors": [
            "G. David Forney Jr"
        ],
        "summary": "We continue to discuss why MMSE estimation arises in coding schemes that approach the capacity of linear Gaussian channels. Here we consider schemes that involve successive decoding, such as decision-feedback equalization or successive cancellation.",
        "published": "2004-09-07T21:02:11Z",
        "link": "http://arxiv.org/abs/cs/0409011v2",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Capacity-achieving ensembles for the binary erasure channel with bounded   complexity",
        "authors": [
            "H. Pfister",
            "I. Sason",
            "R. Urbanke"
        ],
        "summary": "We present two sequences of ensembles of non-systematic irregular repeat-accumulate codes which asymptotically (as their block length tends to infinity) achieve capacity on the binary erasure channel (BEC) with bounded complexity per information bit. This is in contrast to all previous constructions of capacity-achieving sequences of ensembles whose complexity grows at least like the log of the inverse of the gap (in rate) to capacity. The new bounded complexity result is achieved by puncturing bits, and allowing in this way a sufficient number of state nodes in the Tanner graph representing the codes. We also derive an information-theoretic lower bound on the decoding complexity of randomly punctured codes on graphs. The bound holds for every memoryless binary-input output-symmetric channel and is refined for the BEC.",
        "published": "2004-09-13T15:24:02Z",
        "link": "http://arxiv.org/abs/cs/0409026v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Bounds on the decoding complexity of punctured codes on graphs",
        "authors": [
            "H. Pfister",
            "I. Sason",
            "R. Urbanke"
        ],
        "summary": "We present two sequences of ensembles of non-systematic irregular repeat-accumulate codes which asymptotically (as their block length tends to infinity) achieve capacity on the binary erasure channel (BEC) with bounded complexity per information bit. This is in contrast to all previous constructions of capacity-achieving sequences of ensembles whose complexity grows at least like the log of the inverse of the gap (in rate) to capacity. The new bounded complexity result is achieved by puncturing bits, and allowing in this way a sufficient number of state nodes in the Tanner graph representing the codes. We also derive an information-theoretic lower bound on the decoding complexity of randomly punctured codes on graphs. The bound holds for every memoryless binary-input output-symmetric channel, and is refined for the BEC.",
        "published": "2004-09-14T10:46:54Z",
        "link": "http://arxiv.org/abs/cs/0409027v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Some Applications of Coding Theory in Computational Complexity",
        "authors": [
            "Luca Trevisan"
        ],
        "summary": "Error-correcting codes and related combinatorial constructs play an important role in several recent (and old) results in computational complexity theory. In this paper we survey results on locally-testable and locally-decodable error-correcting codes, and their applications to complexity theory and to cryptography.   Locally decodable codes are error-correcting codes with sub-linear time error-correcting algorithms. They are related to private information retrieval (a type of cryptographic protocol), and they are used in average-case complexity and to construct ``hard-core predicates'' for one-way permutations.   Locally testable codes are error-correcting codes with sub-linear time error-detection algorithms, and they are the combinatorial core of probabilistically checkable proofs.",
        "published": "2004-09-24T02:38:24Z",
        "link": "http://arxiv.org/abs/cs/0409044v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the role of MMSE estimation in approaching the information-theoretic   limits of linear Gaussian channels: Shannon meets Wiener",
        "authors": [
            "G. David Forney Jr"
        ],
        "summary": "We discuss why MMSE estimation arises in lattice-based schemes for approaching the capacity of linear Gaussian channels, and comment on its properties.",
        "published": "2004-09-26T22:05:24Z",
        "link": "http://arxiv.org/abs/cs/0409053v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "On mutual information, likelihood-ratios and estimation error for the   additive Gaussian channel",
        "authors": [
            "Moshe Zakai"
        ],
        "summary": "This paper considers the model of an arbitrary distributed signal x observed through an added independent white Gaussian noise w, y=x+w. New relations between the minimal mean square error of the non-causal estimator and the likelihood ratio between y and \\omega are derived. This is followed by an extended version of a recently derived relation between the mutual information I(x;y) and the minimal mean square error. These results are applied to derive infinite dimensional versions of the Fisher information and the de Bruijn identity. The derivation of the results is based on the Malliavin calculus.",
        "published": "2004-09-28T12:04:54Z",
        "link": "http://arxiv.org/abs/math/0409548v2",
        "categories": [
            "math.PR",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH",
            "Primary: 60G35, 60H07; Secondary: 93E10, 93E11, 94A17"
        ]
    },
    {
        "title": "Shannon Information and Kolmogorov Complexity",
        "authors": [
            "Peter Grunwald",
            "Paul Vitanyi"
        ],
        "summary": "We compare the elementary theories of Shannon information and Kolmogorov complexity, the extent to which they have a common purpose, and where they are fundamentally different. We discuss and relate the basic notions of both theories: Shannon entropy versus Kolmogorov complexity, the relation of both to universal coding, Shannon mutual information versus Kolmogorov (`algorithmic') mutual information, probabilistic sufficient statistic versus algorithmic sufficient statistic (related to lossy compression in the Shannon theory versus meaningful information in the Kolmogorov theory), and rate distortion theory versus Kolmogorov's structure function. Part of the material has appeared in print before, scattered through various publications, but this is the first comprehensive systematic comparison. The last mentioned relations are new.",
        "published": "2004-10-01T16:54:45Z",
        "link": "http://arxiv.org/abs/cs/0410002v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4, H.1.1"
        ]
    },
    {
        "title": "Capacity and Random-Coding Exponents for Channel Coding with Side   Information",
        "authors": [
            "Pierre Moulin",
            "Ying Wang"
        ],
        "summary": "Capacity formulas and random-coding exponents are derived for a generalized family of Gel'fand-Pinsker coding problems. These exponents yield asymptotic upper bounds on the achievable log probability of error. In our model, information is to be reliably transmitted through a noisy channel with finite input and output alphabets and random state sequence, and the channel is selected by a hypothetical adversary. Partial information about the state sequence is available to the encoder, adversary, and decoder. The design of the transmitter is subject to a cost constraint. Two families of channels are considered: 1) compound discrete memoryless channels (CDMC), and 2) channels with arbitrary memory, subject to an additive cost constraint, or more generally to a hard constraint on the conditional type of the channel output given the input. Both problems are closely connected. The random-coding exponent is achieved using a stacked binning scheme and a maximum penalized mutual information decoder, which may be thought of as an empirical generalized Maximum a Posteriori decoder. For channels with arbitrary memory, the random-coding exponents are larger than their CDMC counterparts. Applications of this study include watermarking, data hiding, communication in presence of partially known interferers, and problems such as broadcast channels, all of which involve the fundamental idea of binning.",
        "published": "2004-10-01T17:18:09Z",
        "link": "http://arxiv.org/abs/cs/0410003v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Source Coding with Fixed Lag Side Information",
        "authors": [
            "Emin Martinian",
            "Gregory W. Wornell"
        ],
        "summary": "We consider source coding with fixed lag side information at the decoder. We focus on the special case of perfect side information with unit lag corresponding to source coding with feedforward (the dual of channel coding with feedback) introduced by Pradhan. We use this duality to develop a linear complexity algorithm which achieves the rate-distortion bound for any memoryless finite alphabet source and distortion measure.",
        "published": "2004-10-04T20:21:31Z",
        "link": "http://arxiv.org/abs/cs/0410008v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Finite-Length Scaling and Finite-Length Shift for Low-Density   Parity-Check Codes",
        "authors": [
            "Abdelaziz Amraoui",
            "Andrea Montanari",
            "Tom Richardson",
            "Rudiger Urbanke"
        ],
        "summary": "Consider communication over the binary erasure channel BEC using random low-density parity-check codes with finite-blocklength n from `standard' ensembles. We show that large error events is conveniently described within a scaling theory, and explain how to estimate heuristically their effect. Among other quantities, we consider the finite length threshold e(n), defined by requiring a block error probability P_B = 1/2. For ensembles with minimum variable degree larger than two, the following expression is argued to hold e(n) = e -e_1 n^{-2/3} +\\Theta(n^{-1}) with a calculable shift} parameter e_1>0.",
        "published": "2004-10-10T16:28:01Z",
        "link": "http://arxiv.org/abs/cs/0410019v1",
        "categories": [
            "cs.IT",
            "cond-mat.dis-nn",
            "math.IT"
        ]
    },
    {
        "title": "On uniqueness theorems for Tsallis entropy and Tsallis relative entropy",
        "authors": [
            "Shigeru Furuichi"
        ],
        "summary": "The uniqueness theorem for Tsallis entropy was presented in {\\it H.Suyari, IEEE Trans. Inform. Theory, Vol.50, pp.1783-1787 (2004)} by introducing the generalized Shannon-Khinchin's axiom. In the present paper, this result is generalized and simplified as follows: {\\it Generalization}: The uniqueness theorem for Tsallis relative entropy is shown by means of the generalized Hobson's axiom. {\\it Simplification}: The uniqueness theorem for Tsallis entropy is shown by means of the generalized Faddeev's axiom.",
        "published": "2004-10-12T05:09:14Z",
        "link": "http://arxiv.org/abs/cond-mat/0410270v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A generalized Faddeev's axiom and the uniqueness theorem for Tsallis   entropy",
        "authors": [
            "Shigeru Furuichi"
        ],
        "summary": "The uniequness theorem for the Tsallis entropy by introducing the generalized Faddeev's axiom is proven. Our result improves the recent result, the uniqueness theorem for Tsallis entropy by the generalized Shannon-Khinchin's axiom in \\cite{Suy}, in the sence that our axiom is simpler than his one, as similar that Faddeev's axiom is simpler than Shannon-Khinchin's one.",
        "published": "2004-10-12T05:11:21Z",
        "link": "http://arxiv.org/abs/cond-mat/0410271v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Life Above Threshold: From List Decoding to Area Theorem and MSE",
        "authors": [
            "Cyril Measson",
            "Andrea Montanari",
            "Tom Richardson",
            "Rudiger Urbanke"
        ],
        "summary": "We consider communication over memoryless channels using low-density parity-check code ensembles above the iterative (belief propagation) threshold. What is the computational complexity of decoding (i.e., of reconstructing all the typical input codewords for a given channel output) in this regime? We define an algorithm accomplishing this task and analyze its typical performance. The behavior of the new algorithm can be expressed in purely information-theoretical terms. Its analysis provides an alternative proof of the area theorem for the binary erasure channel. Finally, we explain how the area theorem is generalized to arbitrary memoryless channels. We note that the recently discovered relation between mutual information and minimal square error is an instance of the area theorem in the setting of Gaussian channels.",
        "published": "2004-10-13T10:47:56Z",
        "link": "http://arxiv.org/abs/cs/0410028v2",
        "categories": [
            "cs.IT",
            "cond-mat.dis-nn",
            "math.IT"
        ]
    },
    {
        "title": "On doubly-cyclic convolutional codes",
        "authors": [
            "Heide Gluesing-Luerssen",
            "Wiland Schmale"
        ],
        "summary": "Cyclicity of a convolutional code (CC) is relying on a nontrivial automorphism of the algebra F[x]/(x^n-1), where F is a finite field. If this automorphism itself has certain specific cyclicity properties one is lead to the class of doubly-cyclic CC's. Within this large class Reed-Solomon and BCH convolutional codes can be defined. After constructing doubly-cyclic CC's, basic properties are derived on the basis of which distance properties of Reed-Solomon convolutional codes are investigated.This shows that some of them are optimal or near optimal with respect to distance and performance.",
        "published": "2004-10-13T13:42:07Z",
        "link": "http://arxiv.org/abs/math/0410317v1",
        "categories": [
            "math.RA",
            "cs.IT",
            "math.IT",
            "94B10, 94B15, 16S36"
        ]
    },
    {
        "title": "Two Methods for Decreasing the Computational Complexity of the MIMO ML   Decoder",
        "authors": [
            "Takayuki Fukatani",
            "Ryutaroh Matsumoto",
            "Tomohoko Uyematsu"
        ],
        "summary": "We propose use of QR factorization with sort and Dijkstra's algorithm for decreasing the computational complexity of the sphere decoder that is used for ML detection of signals on the multi-antenna fading channel. QR factorization with sort decreases the complexity of searching part of the decoder with small increase in the complexity required for preprocessing part of the decoder. Dijkstra's algorithm decreases the complexity of searching part of the decoder with increase in the storage complexity. The computer simulation demonstrates that the complexity of the decoder is reduced by the proposed methods significantly.",
        "published": "2004-10-18T03:20:17Z",
        "link": "http://arxiv.org/abs/cs/0410040v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Maximum Mutual Information of Space-Time Block Codes with Symbolwise   Decodability",
        "authors": [
            "Kenji Tanaka",
            "Ryutaroh Matsumoto",
            "Tomohiko Uyematsu"
        ],
        "summary": "In this paper, we analyze the performance of space-time block codes which enable symbolwise maximum likelihood decoding. We derive an upper bound of maximum mutual information (MMI) on space-time block codes that enable symbolwise maximum likelihood decoding for a frequency non-selective quasi-static fading channel. MMI is an upper bound on how much one can send information with vanishing error probability by using the target code.",
        "published": "2004-10-18T08:24:04Z",
        "link": "http://arxiv.org/abs/cs/0410041v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Strategy in Ulam's Game and Tree Code Give Error-Resistant Protocols",
        "authors": [
            "Marcin Peczarski"
        ],
        "summary": "We present a new approach to construction of protocols which are proof against communication errors. The construction is based on a generalization of the well known Ulam's game. We show equivalence between winning strategies in this game and robust protocols for multi-party computation. We do not give any complete theory. We want rather to describe a new fresh idea. We use a tree code defined by Schulman. The tree code is the most important part of the interactive version of Shannon's Coding Theorem proved by Schulman. He uses probabilistic argument for the existence of a tree code without giving any effective construction. We show another proof yielding a randomized construction which in contrary to his proof almost surely gives a good code. Moreover our construction uses much smaller alphabet.",
        "published": "2004-10-18T15:31:54Z",
        "link": "http://arxiv.org/abs/cs/0410043v1",
        "categories": [
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Applications of LDPC Codes to the Wiretap Channel",
        "authors": [
            "Andrew Thangaraj",
            "Souvik Dihidar",
            "A. R. Calderbank",
            "Steven McLaughlin",
            "Jean-Marc Merolla"
        ],
        "summary": "With the advent of quantum key distribution (QKD) systems, perfect (i.e. information-theoretic) security can now be achieved for distribution of a cryptographic key. QKD systems and similar protocols use classical error-correcting codes for both error correction (for the honest parties to correct errors) and privacy amplification (to make an eavesdropper fully ignorant). From a coding perspective, a good model that corresponds to such a setting is the wire tap channel introduced by Wyner in 1975. In this paper, we study fundamental limits and coding methods for wire tap channels. We provide an alternative view of the proof for secrecy capacity of wire tap channels and show how capacity achieving codes can be used to achieve the secrecy capacity for any wiretap channel. We also consider binary erasure channel and binary symmetric channel special cases for the wiretap channel and propose specific practical codes. In some cases our designs achieve the secrecy capacity and in others the codes provide security at rates below secrecy capacity. For the special case of a noiseless main channel and binary erasure channel, we consider encoder and decoder design for codes achieving secrecy on the wiretap channel; we show that it is possible to construct linear-time decodable secrecy codes based on LDPC codes that achieve secrecy.",
        "published": "2004-11-02T10:21:59Z",
        "link": "http://arxiv.org/abs/cs/0411003v3",
        "categories": [
            "cs.IT",
            "cs.CR",
            "math.IT"
        ]
    },
    {
        "title": "Capacity Achieving Code Constructions for Two Classes of (d,k)   Constraints",
        "authors": [
            "Yogesh Sankarasubramaniam",
            "Steven W. McLaughlin"
        ],
        "summary": "In this paper, we present two low complexity algorithms that achieve capacity for the noiseless (d,k) constrained channel when k=2d+1, or when k-d+1 is not prime. The first algorithm, called symbol sliding, is a generalized version of the bit flipping algorithm introduced by Aviran et al. [1]. In addition to achieving capacity for (d,2d+1) constraints, it comes close to capacity in other cases. The second algorithm is based on interleaving, and is a generalized version of the bit stuffing algorithm introduced by Bender and Wolf [2]. This method uses fewer than k-d biased bit streams to achieve capacity for (d,k) constraints with k-d+1 not prime. In particular, the encoder for (d,d+2^m-1) constraints, 1\\le m<\\infty, requires only m biased bit streams.",
        "published": "2004-11-03T19:20:11Z",
        "link": "http://arxiv.org/abs/cs/0411006v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Capacity Analysis for Continuous Alphabet Channels with Side   Information, Part I: A General Framework",
        "authors": [
            "Majid Fozunbal",
            "Steven W. McLaughlin",
            "Ronald W. Schafer"
        ],
        "summary": "Capacity analysis for channels with side information at the receiver has been an active area of interest. This problem is well investigated for the case of finite alphabet channels. However, the results are not easily generalizable to the case of continuous alphabet channels due to analytic difficulties inherent with continuous alphabets. In the first part of this two-part paper, we address an analytical framework for capacity analysis of continuous alphabet channels with side information at the receiver. For this purpose, we establish novel necessary and sufficient conditions for weak* continuity and strict concavity of the mutual information. These conditions are used in investigating the existence and uniqueness of the capacity-achieving measures. Furthermore, we derive necessary and sufficient conditions that characterize the capacity value and the capacity-achieving measure for continuous alphabet channels with side information at the receiver.",
        "published": "2004-11-06T20:19:28Z",
        "link": "http://arxiv.org/abs/cs/0411011v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Capacity Analysis for Continuous Alphabet Channels with Side   Information, Part II: MIMO Channels",
        "authors": [
            "Majid Fozunbal",
            "Steven W. McLaughlin",
            "Ronald W. Schafer"
        ],
        "summary": "In this part, we consider the capacity analysis for wireless mobile systems with multiple antenna architectures. We apply the results of the first part to a commonly known baseband, discrete-time multiple antenna system where both the transmitter and receiver know the channel's statistical law. We analyze the capacity for additive white Gaussian noise (AWGN) channels, fading channels with full channel state information (CSI) at the receiver, fading channels with no CSI, and fading channels with partial CSI at the receiver. For each type of channels, we study the capacity value as well as issues such as the existence, uniqueness, and characterization of the capacity-achieving measures for different types of moment constraints. The results are applicable to both Rayleigh and Rician fading channels in the presence of arbitrary line-of-sight and correlation profiles.",
        "published": "2004-11-06T20:40:39Z",
        "link": "http://arxiv.org/abs/cs/0411012v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Rate Distortion and Denoising of Individual Data Using Kolmogorov   complexity",
        "authors": [
            "Nikolai K. Vereshchagin",
            "Paul M. B. Vitanyi"
        ],
        "summary": "We examine the structure of families of distortion balls from the perspective of Kolmogorov complexity. Special attention is paid to the canonical rate-distortion function of a source word which returns the minimal Kolmogorov complexity of all distortion balls containing that word subject to a bound on their cardinality. This canonical rate-distortion function is related to the more standard algorithmic rate-distortion function for the given distortion measure. Examples are given of list distortion, Hamming distortion, and Euclidean distortion. The algorithmic rate-distortion function can behave differently from Shannon's rate-distortion function. To this end, we show that the canonical rate-distortion function can and does assume a wide class of shapes (unlike Shannon's); we relate low algorithmic mutual information to low Kolmogorov complexity (and consequently suggest that certain aspects of the mutual information formulation of Shannon's rate-distortion function behave differently than would an analogous formulation using algorithmic mutual information); we explore the notion that low Kolmogorov complexity distortion balls containing a given word capture the interesting properties of that word (which is hard to formalize in Shannon's theory) and this suggests an approach to denoising; and, finally, we show that the different behavior of the rate-distortion curves of individual source words to some extent disappears after averaging over the source words.",
        "published": "2004-11-07T04:05:25Z",
        "link": "http://arxiv.org/abs/cs/0411014v4",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Feedback Capacity of the First-Order Moving Average Gaussian Channel",
        "authors": [
            "Young-Han Kim"
        ],
        "summary": "The feedback capacity of the stationary Gaussian additive noise channel has been open, except for the case where the noise is white. Here we find the feedback capacity of the stationary first-order moving average additive Gaussian noise channel in closed form. Specifically, the channel is given by $Y_i = X_i + Z_i,$ $i = 1, 2, ...,$ where the input $\\{X_i\\}$ satisfies a power constraint and the noise $\\{Z_i\\}$ is a first-order moving average Gaussian process defined by $Z_i = \\alpha U_{i-1} + U_i,$ $|\\alpha| \\le 1,$ with white Gaussian innovations $U_i,$ $i = 0,1,....$   We show that the feedback capacity of this channel is $-\\log x_0,$ where $x_0$ is the unique positive root of the equation $ \\rho x^2 = (1-x^2) (1 - |\\alpha|x)^2,$ and $\\rho$ is the ratio of the average input power per transmission to the variance of the noise innovation $U_i$. The optimal coding scheme parallels the simple linear signalling scheme by Schalkwijk and Kailath for the additive white Gaussian noise channel -- the transmitter sends a real-valued information-bearing signal at the beginning of communication and subsequently refines the receiver's error by processing the feedback noise signal through a linear stationary first-order autoregressive filter. The resulting error probability of the maximum likelihood decoding decays doubly-exponentially in the duration of the communication. This feedback capacity of the first-order moving average Gaussian channel is very similar in form to the best known achievable rate for the first-order \\emph{autoregressive} Gaussian noise channel studied by Butman, Wolfowitz, and Tiernan, although the optimality of the latter is yet to be established.",
        "published": "2004-11-12T13:29:07Z",
        "link": "http://arxiv.org/abs/cs/0411036v2",
        "categories": [
            "cs.IT",
            "math.IT",
            "H.1.1; E.4"
        ]
    },
    {
        "title": "Geographic Routing with Limited Information in Sensor Networks",
        "authors": [
            "Sundar Subramanian",
            "Sanjay Shakkottai"
        ],
        "summary": "Geographic routing with greedy relaying strategies have been widely studied as a routing scheme in sensor networks. These schemes assume that the nodes have perfect information about the location of the destination. When the distance between the source and destination is normalized to unity, the asymptotic routing delays in these schemes are $\\Theta(\\frac{1}{M(n)}),$ where M(n) is the maximum distance traveled in a single hop (transmission range of a radio). In this paper, we consider routing scenarios where nodes have location errors (imprecise GPS), or where only coarse geographic information about the destination is available, and only a fraction of the nodes have routing information. We show that even with such imprecise or limited destination-location information, the routing delays are $\\Theta(\\frac{1}{M(n)})$. We also consider the throughput-capacity of networks with progressive routing strategies that take packets closer to the destination in every step, but not necessarily along a straight-line. We show that the throughput-capacity with progressive routing is order-wise the same as the maximum achievable throughput-capacity.",
        "published": "2004-11-20T00:20:06Z",
        "link": "http://arxiv.org/abs/cs/0411073v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the High-SNR Capacity of Non-Coherent Networks",
        "authors": [
            "Amos Lapidoth"
        ],
        "summary": "We obtain the first term in the high signal-to-noise ratio (SNR) expansion of the capacity of fading networks where the transmitters and receivers--while fully cognizant of the fading \\emph{law}--have no access to the fading \\emph{realization}. This term is an integer multiple of $\\log \\log \\textnormal{SNR}$ with the coefficient having a simple combinatorial characterization.",
        "published": "2004-11-29T09:47:06Z",
        "link": "http://arxiv.org/abs/cs/0411098v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Spectral Mixture Decomposition by Least Dependent Component Analysis",
        "authors": [
            "Sergey A. Astakhov",
            "Harald Stögbauer",
            "Alexander Kraskov",
            "Peter Grassberger"
        ],
        "summary": "A recently proposed mutual information based algorithm for decomposing data into least dependent components (MILCA) is applied to spectral analysis, namely to blind recovery of concentrations and pure spectra from their linear mixtures. The algorithm is based on precise estimates of mutual information between measured spectra, which allows to assess and make use of actual statistical dependencies between them. We show that linear filtering performed by taking second derivatives effectively reduces the dependencies caused by overlapping spectral bands and, thereby, assists resolving pure spectra. In combination with second derivative preprocessing and alternating least squares postprocessing, MILCA shows decomposition performance comparable with or superior to specialized chemometrics algorithms. The results are illustrated on a number of simulated and experimental (infrared and Raman) mixture problems, including spectroscopy of complex biological materials.   MILCA is available online at http://www.fz-juelich.de/nic/cs/software",
        "published": "2004-12-04T13:22:37Z",
        "link": "http://arxiv.org/abs/physics/0412029v2",
        "categories": [
            "physics.data-an",
            "cs.IT",
            "math.IT",
            "physics.chem-ph"
        ]
    },
    {
        "title": "Monotonicity Results for Coherent MIMO Rician Channels",
        "authors": [
            "Daniel Hoesli",
            "Young-Han Kim",
            "Amos Lapidoth"
        ],
        "summary": "The dependence of the Gaussian input information rate on the line-of-sight (LOS) matrix in multiple-input multiple-output coherent Rician fading channels is explored. It is proved that the outage probability and the mutual information induced by a multivariate circularly symmetric Gaussian input with any covariance matrix are monotonic in the LOS matrix D, or more precisely, monotonic in D'D in the sense of the Loewner partial order. Conversely, it is also demonstrated that this ordering on the LOS matrices is a necessary condition for the uniform monotonicity over all input covariance matrices. This result is subsequently applied to prove the monotonicity of the isotropic Gaussian input information rate and channel capacity in the singular values of the LOS matrix. Extensions to multiple-access channels are also discussed.",
        "published": "2004-12-13T14:57:48Z",
        "link": "http://arxiv.org/abs/cs/0412060v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Exact Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac Statistics",
        "authors": [
            "Robert K. Niven"
        ],
        "summary": "The exact Maxwell-Boltzmann (MB), Bose-Einstein (BE) and Fermi-Dirac (FD) entropies and probabilistic distributions are derived by the combinatorial method of Boltzmann, without Stirling's approximation. The new entropy measures are explicit functions of the probability and degeneracy of each state, and the total number of entities, N. By analysis of the cost of a \"binary decision\", exact BE and FD statistics are shown to have profound consequences for the behaviour of quantum mechanical systems.",
        "published": "2004-12-17T06:58:59Z",
        "link": "http://arxiv.org/abs/cond-mat/0412460v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT",
            "quant-ph"
        ]
    },
    {
        "title": "Complete Characterization of the Equivalent MIMO Channel for   Quasi-Orthogonal Space-Time Codes",
        "authors": [
            "A. Sezgin",
            "T. J. Oechtering"
        ],
        "summary": "Recently, a quasi-orthogonal space-time block code (QSTBC) capable of achieving a significant fraction of the outage mutual information of a multiple-input-multiple output (MIMO) wireless communication system for the case of four transmit and one receive antennas was proposed. We generalize these results to $n_T=2^n$ transmit and an arbitrary number of receive antennas $n_R$. Furthermore, we completely characterize the structure of the equivalent channel for the general case and show that for all $n_T=2^n$ and $n_R$ the eigenvectors of the equivalent channel are fixed and independent from the channel realization. Furthermore, the eigenvalues of the equivalent channel are independent identically distributed random variables each following a noncentral chi-square distribution with $4n_R$ degrees of freedom.   Based on these important insights into the structure of the QSTBC, we derive an analytical lower bound for the fraction of outage probability achieved with QSTBC and show that this bound is tight for low signal-to-noise-ratios (SNR) values and also for increasing number of receive antennas. We also present an upper bound, which is tight for high SNR values and derive analytical expressions for the case of four transmit antennas. Finally, by utilizing the special structure of the QSTBC we propose a new transmit strategy, which decouples the signals transmitted from different antennas in order to detect the symbols separately with a linear ML-detector rather than joint detection, an up to now only known advantage of orthogonal space-time block codes (OSTBC).",
        "published": "2004-12-17T10:36:58Z",
        "link": "http://arxiv.org/abs/cs/0412067v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A class of one-dimensional MDS convolutional codes",
        "authors": [
            "Heide Gluesing-Luerssen",
            "Barbara Langfeld"
        ],
        "summary": "A class of one-dimensional convolutional codes will be presented. They are all MDS codes, i. e., have the largest distance among all one-dimensional codes of the same length n and overall constraint length delta. Furthermore, their extended row distances are computed, and they increase with slope n-delta. In certain cases of the algebraic parameters, we will also derive parity check matrices of Vandermonde type for these codes. Finally, cyclicity in the convolutional sense will be discussed for our class of codes. It will turn out that they are cyclic if and only if the field element used in the generator matrix has order n. This can be regarded as a generalization of the block code case.",
        "published": "2004-12-17T17:16:24Z",
        "link": "http://arxiv.org/abs/cs/0412085v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "math.RA"
        ]
    },
    {
        "title": "Mutual Information and Minimum Mean-square Error in Gaussian Channels",
        "authors": [
            "Dongning Guo",
            "Shlomo Shamai",
            "Sergio Verdu"
        ],
        "summary": "This paper deals with arbitrarily distributed finite-power input signals observed through an additive Gaussian noise channel. It shows a new formula that connects the input-output mutual information and the minimum mean-square error (MMSE) achievable by optimal estimation of the input given the output. That is, the derivative of the mutual information (nats) with respect to the signal-to-noise ratio (SNR) is equal to half the MMSE, regardless of the input statistics. This relationship holds for both scalar and vector signals, as well as for discrete-time and continuous-time noncausal MMSE estimation. This fundamental information-theoretic result has an unexpected consequence in continuous-time nonlinear estimation: For any input signal with finite power, the causal filtering MMSE achieved at SNR is equal to the average value of the noncausal smoothing MMSE achieved with a channel whose signal-to-noise ratio is chosen uniformly distributed between 0 and SNR.",
        "published": "2004-12-23T20:05:17Z",
        "link": "http://arxiv.org/abs/cs/0412108v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the asymptotic accuracy of the union bound",
        "authors": [
            "Alexander Barg"
        ],
        "summary": "A new lower bound on the error probability of maximum likelihood decoding of a binary code on a binary symmetric channel was proved in Barg and McGregor (2004, cs.IT/0407011). It was observed in that paper that this bound leads to a new region of code rates in which the random coding exponent is asymptotically tight, giving a new region in which the reliability of the BSC is known exactly. The present paper explains the relation of these results to the union bound on the error probability.",
        "published": "2004-12-24T19:43:26Z",
        "link": "http://arxiv.org/abs/cs/0412111v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Source Coding With Encoder Side Information",
        "authors": [
            "Emin Martinian",
            "Gregory W. Wornell",
            "Ram Zamir"
        ],
        "summary": "We introduce the idea of distortion side information, which does not directly depend on the source but instead affects the distortion measure. We show that such distortion side information is not only useful at the encoder, but that under certain conditions, knowing it at only the encoder is as good as knowing it at both encoder and decoder, and knowing it at only the decoder is useless. Thus distortion side information is a natural complement to the signal side information studied by Wyner and Ziv, which depends on the source but does not involve the distortion measure. Furthermore, when both types of side information are present, we characterize the penalty for deviating from the configuration of encoder-only distortion side information and decoder-only signal side information, which in many cases is as good as full side information knowledge.",
        "published": "2004-12-28T03:54:41Z",
        "link": "http://arxiv.org/abs/cs/0412112v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Source-Channel Diversity for Parallel Channels",
        "authors": [
            "J. Nicholas Laneman",
            "Emin Martinian",
            "Gregory W. Wornell",
            "John G. Apostolopoulos"
        ],
        "summary": "We consider transmitting a source across a pair of independent, non-ergodic channels with random states (e.g., slow fading channels) so as to minimize the average distortion. The general problem is unsolved. Hence, we focus on comparing two commonly used source and channel encoding systems which correspond to exploiting diversity either at the physical layer through parallel channel coding or at the application layer through multiple description source coding.   For on-off channel models, source coding diversity offers better performance. For channels with a continuous range of reception quality, we show the reverse is true. Specifically, we introduce a new figure of merit called the distortion exponent which measures how fast the average distortion decays with SNR. For continuous-state models such as additive white Gaussian noise channels with multiplicative Rayleigh fading, optimal channel coding diversity at the physical layer is more efficient than source coding diversity at the application layer in that the former achieves a better distortion exponent.   Finally, we consider a third decoding architecture: multiple description encoding with a joint source-channel decoding. We show that this architecture achieves the same distortion exponent as systems with optimal channel coding diversity for continuous-state channels, and maintains the the advantages of multiple description systems for on-off channels. Thus, the multiple description system with joint decoding achieves the best performance, from among the three architectures considered, on both continuous-state and on-off channels.",
        "published": "2004-12-29T01:35:49Z",
        "link": "http://arxiv.org/abs/cs/0412113v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "c.2.1"
        ]
    },
    {
        "title": "Presynaptic modulation as fast synaptic switching: state-dependent   modulation of task performance",
        "authors": [
            "Gabriele Scheler",
            "Johann Schumann"
        ],
        "summary": "Neuromodulatory receptors in presynaptic position have the ability to suppress synaptic transmission for seconds to minutes when fully engaged. This effectively alters the synaptic strength of a connection. Much work on neuromodulation has rested on the assumption that these effects are uniform at every neuron. However, there is considerable evidence to suggest that presynaptic regulation may be in effect synapse-specific. This would define a second \"weight modulation\" matrix, which reflects presynaptic receptor efficacy at a given site. Here we explore functional consequences of this hypothesis. By analyzing and comparing the weight matrices of networks trained on different aspects of a task, we identify the potential for a low complexity \"modulation matrix\", which allows to switch between differently trained subtasks while retaining general performance characteristics for the task. This means that a given network can adapt itself to different task demands by regulating its release of neuromodulators. Specifically, we suggest that (a) a network can provide optimized responses for related classification tasks without the need to train entirely separate networks and (b) a network can blend a \"memory mode\" which aims at reproducing memorized patterns and a \"novelty mode\" which aims to facilitate classification of new patterns. We relate this work to the known effects of neuromodulators on brain-state dependent processing.",
        "published": "2004-01-25T04:22:00Z",
        "link": "http://arxiv.org/abs/cs/0401020v1",
        "categories": [
            "cs.NE",
            "q-bio.NC",
            "F.1.1;K.3.2;I.1.10"
        ]
    },
    {
        "title": "Self-Organising Networks for Classification: developing Applications to   Science Analysis for Astroparticle Physics",
        "authors": [
            "A. De Angelis",
            "P. Boinee",
            "M. Frailis",
            "E. Milotti"
        ],
        "summary": "Physics analysis in astroparticle experiments requires the capability of recognizing new phenomena; in order to establish what is new, it is important to develop tools for automatic classification, able to compare the final result with data from different detectors. A typical example is the problem of Gamma Ray Burst detection, classification, and possible association to known sources: for this task physicists will need in the next years tools to associate data from optical databases, from satellite experiments (EGRET, GLAST), and from Cherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS).",
        "published": "2004-02-09T19:44:33Z",
        "link": "http://arxiv.org/abs/cs/0402014v1",
        "categories": [
            "cs.NE",
            "astro-ph",
            "cs.AI",
            "I.5.1; I.5.3"
        ]
    },
    {
        "title": "Parameter-less hierarchical BOA",
        "authors": [
            "Martin Pelikan",
            "Tz-Kai Lin"
        ],
        "summary": "The parameter-less hierarchical Bayesian optimization algorithm (hBOA) enables the use of hBOA without the need for tuning parameters for solving each problem instance. There are three crucial parameters in hBOA: (1) the selection pressure, (2) the window size for restricted tournaments, and (3) the population size. Although both the selection pressure and the window size influence hBOA performance, performance should remain low-order polynomial with standard choices of these two parameters. However, there is no standard population size that would work for all problems of interest and the population size must thus be eliminated in a different way. To eliminate the population size, the parameter-less hBOA adopts the population-sizing technique of the parameter-less genetic algorithm. Based on the existing theory, the parameter-less hBOA should be able to solve nearly decomposable and hierarchical problems in quadratic or subquadratic number of function evaluations without the need for setting any parameters whatsoever. A number of experiments are presented to verify scalability of the parameter-less hBOA.",
        "published": "2004-02-15T06:56:45Z",
        "link": "http://arxiv.org/abs/cs/0402031v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G.1.6; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Computational complexity and simulation of rare events of Ising spin   glasses",
        "authors": [
            "Martin Pelikan",
            "Jiri Ocenasek",
            "Simon Trebst",
            "Matthias Troyer",
            "Fabien Alet"
        ],
        "summary": "We discuss the computational complexity of random 2D Ising spin glasses, which represent an interesting class of constraint satisfaction problems for black box optimization. Two extremal cases are considered: (1) the +/- J spin glass, and (2) the Gaussian spin glass. We also study a smooth transition between these two extremal cases. The computational complexity of all studied spin glass systems is found to be dominated by rare events of extremely hard spin glass samples. We show that complexity of all studied spin glass systems is closely related to Frechet extremal value distribution. In a hybrid algorithm that combines the hierarchical Bayesian optimization algorithm (hBOA) with a deterministic bit-flip hill climber, the number of steps performed by both the global searcher (hBOA) and the local searcher follow Frechet distributions. Nonetheless, unlike in methods based purely on local search, the parameters of these distributions confirm good scalability of hBOA with local search. We further argue that standard performance measures for optimization algorithms--such as the average number of evaluations until convergence--can be misleading. Finally, our results indicate that for highly multimodal constraint satisfaction problems, such as Ising spin glasses, recombination-based search can provide qualitatively better results than mutation-based search.",
        "published": "2004-02-15T06:58:09Z",
        "link": "http://arxiv.org/abs/cs/0402030v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G.1.6; I.2.8; I.2.6; J.2"
        ]
    },
    {
        "title": "Fitness inheritance in the Bayesian optimization algorithm",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry"
        ],
        "summary": "This paper describes how fitness inheritance can be used to estimate fitness for a proportion of newly sampled candidate solutions in the Bayesian optimization algorithm (BOA). The goal of estimating fitness for some candidate solutions is to reduce the number of fitness evaluations for problems where fitness evaluation is expensive. Bayesian networks used in BOA to model promising solutions and generate the new ones are extended to allow not only for modeling and sampling candidate solutions, but also for estimating their fitness. The results indicate that fitness inheritance is a promising concept in BOA, because population-sizing requirements for building appropriate models of promising solutions lead to good fitness estimates even if only a small proportion of candidate solutions is evaluated using the actual fitness function. This can lead to a reduction of the number of actual fitness evaluations by a factor of 30 or more.",
        "published": "2004-02-15T07:40:45Z",
        "link": "http://arxiv.org/abs/cs/0402032v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Parameter-less Optimization with the Extended Compact Genetic Algorithm   and Iterated Local Search",
        "authors": [
            "Claudio F. Lima",
            "Fernando G. Lobo"
        ],
        "summary": "This paper presents a parameter-less optimization framework that uses the extended compact genetic algorithm (ECGA) and iterated local search (ILS), but is not restricted to these algorithms. The presented optimization algorithm (ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA), where the parameters of a selecto-recombinative GA are eliminated. The approach that we propose is tested on several well known problems. In the absence of domain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use optimization method.",
        "published": "2004-02-19T19:37:32Z",
        "link": "http://arxiv.org/abs/cs/0402047v1",
        "categories": [
            "cs.NE",
            "G.1.6; I.2.6; I.2.8"
        ]
    },
    {
        "title": "An architecture for massive parallelization of the compact genetic   algorithm",
        "authors": [
            "Fernando G. Lobo",
            "Claudio F. Lima",
            "Hugo Martires"
        ],
        "summary": "This paper presents an architecture which is suitable for a massive parallelization of the compact genetic algorithm. The resulting scheme has three major advantages. First, it has low synchronization costs. Second, it is fault tolerant, and third, it is scalable.   The paper argues that the benefits that can be obtained with the proposed approach is potentially higher than those obtained with traditional parallel genetic algorithms. In addition, the ideas suggested in the paper may also be relevant towards parallelizing more complex probabilistic model building genetic algorithms.",
        "published": "2004-02-20T16:36:20Z",
        "link": "http://arxiv.org/abs/cs/0402049v1",
        "categories": [
            "cs.NE",
            "C.1.4; G.1.6; I.2.8"
        ]
    },
    {
        "title": "A philosophical essay on life and its connections with genetic   algorithms",
        "authors": [
            "Fernando G. Lobo"
        ],
        "summary": "This paper makes a number of connections between life and various facets of genetic and evolutionary algorithms research. Specifically, it addresses the topics of adaptation, multiobjective optimization, decision making, deception, and search operators, among others. It argues that human life, from birth to death, is an adaptive or dynamic optimization problem where people are continuously searching for happiness. More important, the paper speculates that genetic algorithms can be used as a source of inspiration for helping people make decisions in their everyday life.",
        "published": "2004-02-20T16:52:11Z",
        "link": "http://arxiv.org/abs/cs/0402050v1",
        "categories": [
            "cs.NE",
            "I.2.8; J.4; K.4.0"
        ]
    },
    {
        "title": "Evolutionary design of photometric systems and its application to Gaia",
        "authors": [
            "C. A. L. Bailer-Jones"
        ],
        "summary": "Designing a photometric system to best fulfil a set of scientific goals is a complex task, demanding a compromise between conflicting requirements and subject to various constraints. A specific example is the determination of stellar astrophysical parameters (APs) - effective temperature, metallicity etc. - across a wide range of stellar types. I present a novel approach to this problem which makes minimal assumptions about the required filter system. By considering a filter system as a set of free parameters it may be designed by optimizing some figure-of-merit (FoM) with respect to these parameters. In the example considered, the FoM is a measure of how well the filter system can `separate' stars with different APs. This separation is vectorial in nature, in the sense that the local directions of AP variance are preferably mutually orthogonal to avoid AP degeneracy. The optimization is carried out with an evolutionary algorithm, which uses principles of evolutionary biology to search the parameter space. This model, HFD (Heuristic Filter Design), is applied to the design of photometric systems for the Gaia space astrometry mission. The optimized systems show a number of interesting features, not least the persistence of broad, overlapping filters. These HFD systems perform as least as well as other proposed systems for Gaia, although inadequacies remain in all. The principles underlying HFD are quite generic and may be applied to filter design for numerous other projects, such as the search for specific types of objects or photometric redshift determination.",
        "published": "2004-02-25T14:18:14Z",
        "link": "http://arxiv.org/abs/astro-ph/0402591v1",
        "categories": [
            "astro-ph",
            "cs.NE",
            "stat.ML"
        ]
    },
    {
        "title": "Genetic Algorithms and Quantum Computation",
        "authors": [
            "Gilson A. Giraldi",
            "Renato Portugal",
            "Ricardo N. Thess"
        ],
        "summary": "Recently, researchers have applied genetic algorithms (GAs) to address some problems in quantum computation. Also, there has been some works in the designing of genetic algorithms based on quantum theoretical concepts and techniques. The so called Quantum Evolutionary Programming has two major sub-areas: Quantum Inspired Genetic Algorithms (QIGAs) and Quantum Genetic Algorithms (QGAs). The former adopts qubit chromosomes as representations and employs quantum gates for the search of the best solution. The later tries to solve a key question in this field: what GAs will look like as an implementation on quantum hardware? As we shall see, there is not a complete answer for this question. An important point for QGAs is to build a quantum algorithm that takes advantage of both the GA and quantum computing parallelism as well as true randomness provided by quantum computers. In the first part of this paper we present a survey of the main works in GAs plus quantum computing including also our works in this area. Henceforth, we review some basic concepts in quantum computation and GAs and emphasize their inherent parallelism. Next, we review the application of GAs for learning quantum operators and circuit design. Then, quantum evolutionary programming is considered. Finally, we present our current research in this field and some perspectives.",
        "published": "2004-03-04T19:24:10Z",
        "link": "http://arxiv.org/abs/cs/0403003v1",
        "categories": [
            "cs.NE",
            "D.1.0"
        ]
    },
    {
        "title": "Memorization in a neural network with adjustable transfer function and   conditional gating",
        "authors": [
            "Gabriele Scheler"
        ],
        "summary": "The main problem about replacing LTP as a memory mechanism has been to find other highly abstract, easily understandable principles for induced plasticity. In this paper we attempt to lay out such a basic mechanism, namely intrinsic plasticity. Important empirical observations with theoretical significance are time-layering of neural plasticity mediated by additional constraints to enter into later stages, various manifestations of intrinsic neural properties, and conditional gating of synaptic connections. An important consequence of the proposed mechanism is that it can explain the usually latent nature of memories.",
        "published": "2004-03-07T23:27:32Z",
        "link": "http://arxiv.org/abs/q-bio/0403011v1",
        "categories": [
            "q-bio.NC",
            "cs.NE"
        ]
    },
    {
        "title": "An approach to membrane computing under inexactitude",
        "authors": [
            "Jaume Casasnovas",
            "Joe Miro",
            "Manuel Moya",
            "Francesc Rossello"
        ],
        "summary": "In this paper we introduce a fuzzy version of symport/antiport membrane systems. Our fuzzy membrane systems handle possibly inexact copies of reactives and their rules are endowed with threshold functions that determine whether a rule can be applied or not to a given set of objects, depending of the degree of accuracy of these objects to the reactives specified in the rule. We prove that these fuzzy membrane systems generate exactly the recursively enumerable finite-valued fuzzy sets of natural numbers.",
        "published": "2004-03-16T09:02:39Z",
        "link": "http://arxiv.org/abs/cs/0403027v2",
        "categories": [
            "cs.OH",
            "cs.NE",
            "F.4.2; F.1.1"
        ]
    },
    {
        "title": "On the Practicality of Intrinsic Reconfiguration As a Fault Recovery   Method in Analog Systems",
        "authors": [
            "Garrison W. Greenwood"
        ],
        "summary": "Evolvable hardware combines the powerful search capability of evolutionary algorithms with the flexibility of reprogrammable devices, thereby providing a natural framework for reconfiguration. This framework has generated an interest in using evolvable hardware for fault-tolerant systems because reconfiguration can effectively deal with hardware faults whenever it is impossible to provide spares. But systems cannot tolerate faults indefinitely, which means reconfiguration does have a deadline. The focus of previous evolvable hardware research relating to fault-tolerance has been primarily restricted to restoring functionality, with no real consideration of time constraints. In this paper we are concerned with evolvable hardware performing reconfiguration under deadline constraints. In particular, we investigate reconfigurable hardware that undergoes intrinsic evolution. We show that fault recovery done by intrinsic reconfiguration has some restrictions, which designers cannot ignore.",
        "published": "2004-04-01T16:54:42Z",
        "link": "http://arxiv.org/abs/cs/0404001v1",
        "categories": [
            "cs.PF",
            "cs.NE",
            "B.8.1"
        ]
    },
    {
        "title": "Exploring tradeoffs in pleiotropy and redundancy using evolutionary   computing",
        "authors": [
            "Matthew J. Berryman",
            "Wei-Li Khoo",
            "Hiep Nguyen",
            "Erin O'Neill",
            "Andrew Allison",
            "Derek Abbott"
        ],
        "summary": "Evolutionary computation algorithms are increasingly being used to solve optimization problems as they have many advantages over traditional optimization algorithms. In this paper we use evolutionary computation to study the trade-off between pleiotropy and redundancy in a client-server based network. Pleiotropy is a term used to describe components that perform multiple tasks, while redundancy refers to multiple components performing one same task. Pleiotropy reduces cost but lacks robustness, while redundancy increases network reliability but is more costly, as together, pleiotropy and redundancy build flexibility and robustness into systems. Therefore it is desirable to have a network that contains a balance between pleiotropy and redundancy. We explore how factors such as link failure probability, repair rates, and the size of the network influence the design choices that we explore using genetic algorithms.",
        "published": "2004-04-07T06:23:18Z",
        "link": "http://arxiv.org/abs/cs/0404017v1",
        "categories": [
            "cs.NE",
            "cs.NI",
            "G.1.6; C.2.1"
        ]
    },
    {
        "title": "Optimizing genetic algorithm strategies for evolving networks",
        "authors": [
            "Matthew J. Berryman",
            "Andrew Allison",
            "Derek Abbott"
        ],
        "summary": "This paper explores the use of genetic algorithms for the design of networks, where the demands on the network fluctuate in time. For varying network constraints, we find the best network using the standard genetic algorithm operators such as inversion, mutation and crossover. We also examine how the choice of genetic algorithm operators affects the quality of the best network found. Such networks typically contain redundancy in servers, where several servers perform the same task and pleiotropy, where servers perform multiple tasks. We explore this trade-off between pleiotropy versus redundancy on the cost versus reliability as a measure of the quality of the network.",
        "published": "2004-04-07T07:08:09Z",
        "link": "http://arxiv.org/abs/cs/0404019v1",
        "categories": [
            "cs.NE",
            "cs.NI",
            "G.1.6; C.2.1"
        ]
    },
    {
        "title": "When Do Differences Matter? On-Line Feature Extraction Through Cognitive   Economy",
        "authors": [
            "David J. Finton"
        ],
        "summary": "For an intelligent agent to be truly autonomous, it must be able to adapt its representation to the requirements of its task as it interacts with the world. Most current approaches to on-line feature extraction are ad hoc; in contrast, this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy. The algorithm incorporates an active form of Q-learning, and partitions continuous state-spaces by merging and splitting Voronoi regions. The experiments illustrate a new methodology for testing and comparing representations by means of learning curves. Results from the puck-on-a-hill task demonstrate the algorithm's ability to learn effective representations, superior to those produced by some other, well-known, methods.",
        "published": "2004-04-15T02:59:10Z",
        "link": "http://arxiv.org/abs/cs/0404032v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "I.2.6; I.2.4; I.2.8"
        ]
    },
    {
        "title": "Extraction of topological features from communication network   topological patterns using self-organizing feature maps",
        "authors": [
            "W. Ali",
            "R. J. Mondragon",
            "F. Alavi"
        ],
        "summary": "Different classes of communication network topologies and their representation in the form of adjacency matrix and its eigenvalues are presented. A self-organizing feature map neural network is used to map different classes of communication network topological patterns. The neural network simulation results are reported.",
        "published": "2004-04-21T16:05:27Z",
        "link": "http://arxiv.org/abs/cs/0404042v2",
        "categories": [
            "cs.NE",
            "cs.CV",
            "C.2; I.5"
        ]
    },
    {
        "title": "Speculation on graph computation architectures and computing via   synchronization",
        "authors": [
            "Bayle Shanks"
        ],
        "summary": "A speculative overview of a future topic of research. The paper is a collection of ideas concerning two related areas:   1) Graph computation machines (\"computing with graphs\"). This is the class of models of computation in which the state of the computation is represented as a graph or network.   2) Arc-based neural networks, which store information not as activation in the nodes, but rather by adding and deleting arcs. Sometimes the arcs may be interpreted as synchronization.   Warnings to readers: this is not the sort of thing that one might submit to a journal or conference. No proofs are presented. The presentation is informal, and written at an introductory level. You'll probably want to wait for a more concise presentation.",
        "published": "2004-04-22T07:29:19Z",
        "link": "http://arxiv.org/abs/cs/0404045v2",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.1.1; J.3; I.2.m"
        ]
    },
    {
        "title": "Evolution of a Subsumption Architecture Neurocontroller",
        "authors": [
            "Julian Togelius"
        ],
        "summary": "An approach to robotics called layered evolution and merging features from the subsumption architecture into evolutionary robotics is presented, and its advantages are discussed. This approach is used to construct a layered controller for a simulated robot that learns which light source to approach in an environment with obstacles. The evolvability and performance of layered evolution on this task is compared to (standard) monolithic evolution, incremental and modularised evolution. To corroborate the hypothesis that a layered controller performs at least as well as an integrated one, the evolved layers are merged back into a single network. On the grounds of the test results, it is argued that layered evolution provides a superior approach for many tasks, and it is suggested that this approach may be the key to scaling up evolutionary robotics.",
        "published": "2004-05-06T19:37:07Z",
        "link": "http://arxiv.org/abs/cs/0405027v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.9; I.2.6"
        ]
    },
    {
        "title": "Efficiency Enhancement of Probabilistic Model Building Genetic   Algorithms",
        "authors": [
            "Kumara Sastry",
            "David E. Goldberg",
            "Martin Pelikan"
        ],
        "summary": "This paper presents two different efficiency-enhancement techniques for probabilistic model building genetic algorithms. The first technique proposes the use of a mutation operator which performs local search in the sub-solution neighborhood identified through the probabilistic model. The second technique proposes building and using an internal probabilistic model of the fitness along with the probabilistic model of variable interactions. The fitness values of some offspring are estimated using the probabilistic model, thereby avoiding computationally expensive function evaluations. The scalability of the aforementioned techniques are analyzed using facetwise models for convergence time and population sizing. The speed-up obtained by each of the methods is predicted and verified with empirical results. The results show that for additively separable problems the competent mutation operator requires O(k 0.5 logm)--where k is the building-block size, and m is the number of building blocks--less function evaluations than its selectorecombinative counterpart. The results also show that the use of an internal probabilistic fitness model reduces the required number of function evaluations to as low as 1-10% and yields a speed-up of 2--50.",
        "published": "2004-05-18T16:19:59Z",
        "link": "http://arxiv.org/abs/cs/0405062v1",
        "categories": [
            "cs.NE",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Let's Get Ready to Rumble: Crossover Versus Mutation Head to Head",
        "authors": [
            "Kumara Sastry",
            "David E. Goldberg"
        ],
        "summary": "This paper analyzes the relative advantages between crossover and mutation on a class of deterministic and stochastic additively separable problems. This study assumes that the recombination and mutation operators have the knowledge of the building blocks (BBs) and effectively exchange or search among competing BBs. Facetwise models of convergence time and population sizing have been used to determine the scalability of each algorithm. The analysis shows that for additively separable deterministic problems, the BB-wise mutation is more efficient than crossover, while the crossover outperforms the mutation on additively separable problems perturbed with additive Gaussian noise. The results show that the speed-up of using BB-wise mutation on deterministic problems is O(k^{0.5}logm), where k is the BB size, and m is the number of BBs. Likewise, the speed-up of using crossover on stochastic problems with fixed noise variance is O(mk^{0.5}log m).",
        "published": "2004-05-18T16:31:56Z",
        "link": "http://arxiv.org/abs/cs/0405063v1",
        "categories": [
            "cs.NE",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Designing Competent Mutation Operators via Probabilistic Model Building   of Neighborhoods",
        "authors": [
            "Kumara Sastry",
            "David E. Goldberg"
        ],
        "summary": "This paper presents a competent selectomutative genetic algorithm (GA), that adapts linkage and solves hard problems quickly, reliably, and accurately. A probabilistic model building process is used to automatically identify key building blocks (BBs) of the search problem. The mutation operator uses the probabilistic model of linkage groups to find the best among competing building blocks. The competent selectomutative GA successfully solves additively separable problems of bounded difficulty, requiring only subquadratic number of function evaluations. The results show that for additively separable problems the probabilistic model building BB-wise mutation scales as O(2^km^{1.5}), and requires O(k^{0.5}logm) less function evaluations than its selectorecombinative counterpart, confirming theoretical results reported elsewhere (Sastry & Goldberg, 2004).",
        "published": "2004-05-18T16:41:34Z",
        "link": "http://arxiv.org/abs/cs/0405064v1",
        "categories": [
            "cs.NE",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Efficiency Enhancement of Genetic Algorithms via Building-Block-Wise   Fitness Estimation",
        "authors": [
            "Kumara Sastry",
            "Martin Pelikan",
            "David E. Goldberg"
        ],
        "summary": "This paper studies fitness inheritance as an efficiency enhancement technique for a class of competent genetic algorithms called estimation distribution algorithms. Probabilistic models of important sub-solutions are developed to estimate the fitness of a proportion of individuals in the population, thereby avoiding computationally expensive function evaluations. The effect of fitness inheritance on the convergence time and population sizing are modeled and the speed-up obtained through inheritance is predicted. The results show that a fitness-inheritance mechanism which utilizes information on building-block fitnesses provides significant efficiency enhancement. For additively separable problems, fitness inheritance reduces the number of function evaluations to about half and yields a speed-up of about 1.75--2.25.",
        "published": "2004-05-18T16:55:00Z",
        "link": "http://arxiv.org/abs/cs/0405065v1",
        "categories": [
            "cs.NE",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Parallel Mixed Bayesian Optimization Algorithm: A Scaleup Analysis",
        "authors": [
            "Jiri Ocenasek",
            "Martin Pelikan"
        ],
        "summary": "Estimation of Distribution Algorithms have been proposed as a new paradigm for evolutionary optimization. This paper focuses on the parallelization of Estimation of Distribution Algorithms. More specifically, the paper discusses how to predict performance of parallel Mixed Bayesian Optimization Algorithm (MBOA) that is based on parallel construction of Bayesian networks with decision trees. We determine the time complexity of parallel Mixed Bayesian Optimization Algorithm and compare this complexity with experimental results obtained by solving the spin glass optimization problem. The empirical results fit well the theoretical time complexity, so the scalability and efficiency of parallel Mixed Bayesian Optimization Algorithm for unknown instances of spin glass benchmarks can be predicted. Furthermore, we derive the guidelines that can be used to design effective parallel Estimation of Distribution Algorithms with the speedup proportional to the number of variables in the problem.",
        "published": "2004-06-03T15:43:46Z",
        "link": "http://arxiv.org/abs/cs/0406007v1",
        "categories": [
            "cs.NE",
            "cs.DC",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Using Self-Organising Mappings to Learn the Structure of Data Manifolds",
        "authors": [
            "Stephen Luttrell"
        ],
        "summary": "In this paper it is shown how to map a data manifold into a simpler form by progressively discarding small degrees of freedom. This is the key to self-organising data fusion, where the raw data is embedded in a very high-dimensional space (e.g. the pixel values of one or more images), and the requirement is to isolate the important degrees of freedom which lie on a low-dimensional manifold. A useful advantage of the approach used in this paper is that the computations are arranged as a feed-forward processing chain, where all the details of the processing in each stage of the chain are learnt by self-organisation. This approach is demonstrated using hierarchically correlated data, which causes the processing chain to split the data into separate processing channels, and then to progressively merge these channels wherever they are correlated with each other. This is the key to self-organising data fusion.",
        "published": "2004-06-08T14:45:45Z",
        "link": "http://arxiv.org/abs/cs/0406017v2",
        "categories": [
            "cs.NE",
            "cs.CV",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Why Two Sexes?",
        "authors": [
            "Vigen A. Geodakian"
        ],
        "summary": "Evolutionary role of the separation into two sexes from a cyberneticist's point of view. [I translated this 1965 article from Russian \"Nauka i Zhizn\" (Science and Life) in 1988. In a popular form, the article puts forward several useful ideas not all of which even today are necessarily well known or widely accepted. Boris Lubachevsky, bdl@bell-labs.com ]",
        "published": "2004-08-01T15:04:53Z",
        "link": "http://arxiv.org/abs/cs/0408006v1",
        "categories": [
            "cs.NE",
            "cs.GL",
            "q-bio.PE",
            "F.1.1; J.3"
        ]
    },
    {
        "title": "Notes on information geometry and evolutionary processes",
        "authors": [
            "Marc Toussaint"
        ],
        "summary": "In order to analyze and extract different structural properties of distributions, one can introduce different coordinate systems over the manifold of distributions. In Evolutionary Computation, the Walsh bases and the Building Block Bases are often used to describe populations, which simplifies the analysis of evolutionary operators applying on populations. Quite independent from these approaches, information geometry has been developed as a geometric way to analyze different order dependencies between random variables (e.g., neural activations or genes).   In these notes I briefly review the essentials of various coordinate bases and of information geometry. The goal is to give an overview and make the approaches comparable. Besides introducing meaningful coordinate bases, information geometry also offers an explicit way to distinguish different order interactions and it offers a geometric view on the manifold and thereby also on operators that apply on the manifold. For instance, uniform crossover can be interpreted as an orthogonal projection of a population along an m-geodesic, monotonously reducing the theta-coordinates that describe interactions between genes.",
        "published": "2004-08-20T14:42:51Z",
        "link": "http://arxiv.org/abs/nlin/0408040v1",
        "categories": [
            "nlin.AO",
            "cs.NE"
        ]
    },
    {
        "title": "Using Stochastic Encoders to Discover Structure in Data",
        "authors": [
            "Stephen Luttrell"
        ],
        "summary": "In this paper a stochastic generalisation of the standard Linde-Buzo-Gray (LBG) approach to vector quantiser (VQ) design is presented, in which the encoder is implemented as the sampling of a vector of code indices from a probability distribution derived from the input vector, and the decoder is implemented as a superposition of reconstruction vectors. This stochastic VQ (SVQ) is optimised using a minimum mean Euclidean reconstruction distortion criterion, as in the LBG case. Numerical simulations are used to demonstrate how this leads to self-organisation of the SVQ, where different stochastically sampled code indices become associated with different input subspaces.",
        "published": "2004-08-21T19:40:24Z",
        "link": "http://arxiv.org/abs/cs/0408049v1",
        "categories": [
            "cs.NE",
            "cs.CV",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Invariant Stochastic Encoders",
        "authors": [
            "Stephen Luttrell"
        ],
        "summary": "The theory of stochastic vector quantisers (SVQ) has been extended to allow the quantiser to develop invariances, so that only \"large\" degrees of freedom in the input vector are represented in the code. This has been applied to the problem of encoding data vectors which are a superposition of a \"large\" jammer and a \"small\" signal, so that only the jammer is represented in the code. This allows the jammer to be subtracted from the total input vector (i.e. the jammer is nulled), leaving a residual that contains only the underlying signal. The main advantage of this approach to jammer nulling is that little prior knowledge of the jammer is assumed, because these properties are automatically discovered by the SVQ as it is trained on examples of input vectors.",
        "published": "2004-08-21T23:06:45Z",
        "link": "http://arxiv.org/abs/cs/0408050v1",
        "categories": [
            "cs.NE",
            "cs.CV",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Non-negative matrix factorization with sparseness constraints",
        "authors": [
            "Patrik O. Hoyer"
        ],
        "summary": "Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of `sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.",
        "published": "2004-08-25T20:25:43Z",
        "link": "http://arxiv.org/abs/cs/0408058v1",
        "categories": [
            "cs.LG",
            "cs.NE"
        ]
    },
    {
        "title": "The Integration of Connectionism and First-Order Knowledge   Representation and Reasoning as a Challenge for Artificial Intelligence",
        "authors": [
            "Sebastian Bader",
            "Pascal Hitzler",
            "Steffen Hoelldobler"
        ],
        "summary": "Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.",
        "published": "2004-08-31T16:16:28Z",
        "link": "http://arxiv.org/abs/cs/0408069v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.NE",
            "I.2.3,I.2.6"
        ]
    },
    {
        "title": "Applying Policy Iteration for Training Recurrent Neural Networks",
        "authors": [
            "I. Szita",
            "A. Lorincz"
        ],
        "summary": "Recurrent neural networks are often used for learning time-series data. Based on a few assumptions we model this learning task as a minimization problem of a nonlinear least-squares cost function. The special structure of the cost function allows us to build a connection to reinforcement learning. We exploit this connection and derive a convergent, policy iteration-based algorithm. Furthermore, we argue that RNN training can be fit naturally into the reinforcement learning framework.",
        "published": "2004-10-02T07:19:49Z",
        "link": "http://arxiv.org/abs/cs/0410004v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ]
    },
    {
        "title": "Adaptive Cluster Expansion (ACE): A Hierarchical Bayesian Network",
        "authors": [
            "Stephen Luttrell"
        ],
        "summary": "Using the maximum entropy method, we derive the \"adaptive cluster expansion\" (ACE), which can be trained to estimate probability density functions in high dimensional spaces. The main advantage of ACE over other Bayesian networks is its ability to capture high order statistics after short training times, which it achieves by making use of a hierarchical vector quantisation of the input data. We derive a scheme for representing the state of an ACE network as a \"probability image\", which allows us to identify statistically anomalous regions in an otherwise statistically homogeneous image, for instance. Finally, we present some probability images that we obtained after training ACE on some Brodatz texture images - these demonstrate the ability of ACE to detect subtle textural anomalies.",
        "published": "2004-10-10T18:30:03Z",
        "link": "http://arxiv.org/abs/cs/0410020v1",
        "categories": [
            "cs.NE",
            "cs.CV",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Neural Architectures for Robot Intelligence",
        "authors": [
            "H. Ritter",
            "J. J. Steil",
            "C. Noelker",
            "F. Roethling",
            "P. C. McGuire"
        ],
        "summary": "We argue that the direct experimental approaches to elucidate the architecture of higher brains may benefit from insights gained from exploring the possibilities and limits of artificial control architectures for robot systems. We present some of our recent work that has been motivated by that view and that is centered around the study of various aspects of hand actions since these are intimately linked with many higher cognitive abilities. As examples, we report on the development of a modular system for the recognition of continuous hand postures based on neural nets, the use of vision and tactile sensing for guiding prehensile movements of a multifingered hand, and the recognition and use of hand gestures for robot teaching.   Regarding the issue of learning, we propose to view real-world learning from the perspective of data mining and to focus more strongly on the imitation of observed actions instead of purely reinforcement-based exploration. As a concrete example of such an effort we report on the status of an ongoing project in our lab in which a robot equipped with an attention system with a neurally inspired architecture is taught actions by using hand gestures in conjunction with speech commands. We point out some of the lessons learnt from this system, and discuss how systems of this kind can contribute to the study of issues at the junction between natural and artificial cognitive systems.",
        "published": "2004-10-18T10:50:28Z",
        "link": "http://arxiv.org/abs/cs/0410042v1",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.HC",
            "cs.LG",
            "cs.NE",
            "q-bio.NC",
            "I.2.9; I.2.10; I.2.6; H.1.2; H.2.8; I.5.4"
        ]
    },
    {
        "title": "A Computational Study of Rotating Spiral Waves and Spatio-Temporal   Transient Chaos in a Deterministic Three-Level Active System",
        "authors": [
            "S. D. Makovetskiy",
            "D. N. Makovetskii"
        ],
        "summary": "Spatio-temporal dynamics of a deterministic three-level cellular automaton (TLCA) of Zykov-Mikhailov type (Sov. Phys. - Dokl., 1986, Vol.31, No.1, P.51) is studied numerically. Evolution of spatial structures is investigated both for the original Zykov-Mikhailov model (which is applicable to, for example, Belousov-Zhabotinskii chemical reactions) and for proposed by us TLCA, which is a generalization of Zykov-Mikhailov model for the case of two-channel diffusion. Such the TLCA is a minimal model for an excitable medium of microwave phonon laser, called phaser (D. N. Makovetskii, Tech. Phys., 2004, Vol.49, No.2, P.224; cond-mat/0402640). The most interesting observed forms of TLCA dynamics are as follows: (a) spatio-temporal transient chaos in form of highly bottlenecked collective evolution of excitations by rotating spiral waves (RSW) with variable topological charges; (b) competition of left-handed and right-handed RSW with unexpected features, including self-induced alteration of integral effective topological charge; (c) transient chimera states, i.e. coexistence of regular and chaotic domains in TLCA patterns; (d) branching of TLCA states with different symmetry which may lead to full restoring of symmetry of imperfect starting pattern. Phenomena (a) and (c) are directly related to phaser dynamics features observed earlier in real experiments at liquid helium temperatures on corundum crystals doped by iron-group ions. ACM: F.1.1, I.6, J.2; PACS:05.65.+b, 07.05.Tp, 82.20.Wt",
        "published": "2004-10-19T04:56:45Z",
        "link": "http://arxiv.org/abs/cond-mat/0410460v2",
        "categories": [
            "cond-mat.other",
            "cs.NE",
            "nlin.CG"
        ]
    },
    {
        "title": "Spontaneous Dynamics of Asymmetric Random Recurrent Spiking Neural   Networks",
        "authors": [
            "H. Soula",
            "G. Beslon",
            "O. Mazet"
        ],
        "summary": "We study in this paper the effect of an unique initial stimulation on random recurrent networks of leaky integrate and fire neurons. Indeed given a stochastic connectivity this so-called spontaneous mode exhibits various non trivial dynamics. This study brings forward a mathematical formalism that allows us to examine the variability of the afterward dynamics according to the parameters of the weight distribution. Provided independence hypothesis (e.g. in the case of very large networks) we are able to compute the average number of neurons that fire at a given time -- the spiking activity. In accordance with numerical simulations, we prove that this spiking activity reaches a steady-state, we characterize this steady-state and explore the transients.",
        "published": "2004-11-17T10:04:55Z",
        "link": "http://arxiv.org/abs/cs/0411052v1",
        "categories": [
            "cs.NE",
            "math.PR"
        ]
    },
    {
        "title": "Multidimensional data classification with artificial neural networks",
        "authors": [
            "P. Boinee",
            "F. Barbarino",
            "A. De Angelis"
        ],
        "summary": "Multi-dimensional data classification is an important and challenging problem in many astro-particle experiments. Neural networks have proved to be versatile and robust in multi-dimensional data classification. In this article we shall study the classification of gamma from the hadrons for the MAGIC Experiment. Two neural networks have been used for the classification task. One is Multi-Layer Perceptron based on supervised learning and other is Self-Organising Map (SOM), which is based on unsupervised learning technique. The results have been shown and the possible ways of combining these networks have been proposed to yield better and faster classification results.",
        "published": "2004-12-06T20:23:15Z",
        "link": "http://arxiv.org/abs/cs/0412023v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "F.1.1; K.3.2; I.2.6"
        ]
    },
    {
        "title": "Vector Symbolic Architectures answer Jackendoff's challenges for   cognitive neuroscience",
        "authors": [
            "Ross W. Gayler"
        ],
        "summary": "Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.",
        "published": "2004-12-13T08:00:55Z",
        "link": "http://arxiv.org/abs/cs/0412059v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.5.1; I.2.0, I.2.6"
        ]
    },
    {
        "title": "Web Usage Mining Using Artificial Ant Colony Clustering and Genetic   Programming",
        "authors": [
            "Ajith Abraham",
            "Vitorino Ramos"
        ],
        "summary": "The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. The study of ant colonies behavior and their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences, because it provides models of distributed adaptive organization, which are useful to solve difficult optimization, classification, and distributed control problems, among others. In this paper, we propose an ant clustering algorithm to discover Web usage patterns (data clusters) and a linear genetic programming approach to analyze the visitor trends. Empirical results clearly shows that ant colony clustering performs well when compared to a self-organizing map (for clustering Web usage patterns) even though the performance accuracy is not that efficient when comparared to evolutionary-fuzzy clustering (i-miner) approach. KEYWORDS: Web Usage Mining, Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Linear Genetic Programming.",
        "published": "2004-12-17T14:58:38Z",
        "link": "http://arxiv.org/abs/cs/0412071v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "Swarms on Continuous Data",
        "authors": [
            "Vitorino Ramos",
            "Ajith Abraham"
        ],
        "summary": "While being it extremely important, many Exploratory Data Analysis (EDA) systems have the inhability to perform classification and visualization in a continuous basis or to self-organize new data-items into the older ones (evenmore into new labels if necessary), which can be crucial in KDD - Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online forms of Web Applications are just one example). This disadvantge is also present in more recent approaches using Self-Organizing Maps. On the present work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems a robust online classifier is presented, which produces class decisions on a continuous stream data, allowing for continuous mappings. Results show that increasingly better results are achieved, as demonstraded by other authors in different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous Classification.",
        "published": "2004-12-17T15:05:40Z",
        "link": "http://arxiv.org/abs/cs/0412072v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "The Biological Concept of Neoteny in Evolutionary Colour Image   Segmentation - Simple Experiments in Simple Non-Memetic Genetic Algorithms",
        "authors": [
            "Vitorino Ramos"
        ],
        "summary": "Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of existing organisms may give rise, under certain circumstances, to wholly new organisms. Although the present work does not pretend to model or simulate the biological details of such a concept in any way, these ideas were incorporated by a rather simple abstract computational strategy, in order to allow (if possible) for faster convergence into simple non-memetic Genetic Algorithms, i.e. without using local improvement procedures (e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic Algorithm was used for colour image segmentation purposes by using K-mean unsupervised clustering methods, namely for guiding the evolutionary algorithm in his search for finding the optimal or sub-optimal data partition. Average results suggest that the use of neotonic strategies by employing juvenile genotypes into the later generations and the use of linear-dynamic mutation rates instead of constant, can increase fitness values by 58% comparing to classical Genetic Algorithms, independently from the starting population characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.",
        "published": "2004-12-17T16:39:33Z",
        "link": "http://arxiv.org/abs/cs/0412080v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2; I.5"
        ]
    },
    {
        "title": "Artificial Neoteny in Evolutionary Image Segmentation",
        "authors": [
            "Vitorino Ramos"
        ],
        "summary": "Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of existing organisms may give rise, under certain circumstances, to wholly new organisms. Although the present work does not pretend to model or simulate the biological details of such a concept in any way, these ideas were incorporated by a rather simple abstract computational strategy, in order to allow (if possible) for faster convergence into simple non-memetic Genetic Algorithms, i.e. without using local improvement procedures (e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic Algorithm was used for colour image segmentation purposes by using K-mean unsupervised clustering methods, namely for guiding the evolutionary algorithm in his search for finding the optimal or sub-optimal data partition. Average results suggest that the use of neotonic strategies by employing juvenile genotypes into the later generations and the use of linear-dynamic mutation rates instead of constant, can increase fitness values by 58% comparing to classical Genetic Algorithms, independently from the starting population characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.",
        "published": "2004-12-17T16:44:54Z",
        "link": "http://arxiv.org/abs/cs/0412081v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2; I.5"
        ]
    },
    {
        "title": "Map Segmentation by Colour Cube Genetic K-Mean Clustering",
        "authors": [
            "Vitorino Ramos",
            "Fernando Muge"
        ],
        "summary": "Segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. In this work, a method is described for evolving adaptive procedures for these problems. In many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be mapped into topological domains. We formulate the segmentation problem upon such images as an optimisation problem and adopt evolutionary strategy of Genetic Algorithms for the clustering of small regions in colour feature space. The present approach uses k-Means unsupervised clustering methods into Genetic Algorithms, namely for guiding this last Evolutionary Algorithm in his search for finding the optimal or sub-optimal data partition, task that as we know, requires a non-trivial search because of its NP-complete nature. To solve this task, the appropriate genetic coding is also discussed, since this is a key aspect in the implementation. Our purpose is to demonstrate the efficiency of Genetic Algorithms to automatic and unsupervised texture segmentation. Some examples in Colour Maps are presented and overall results discussed. KEYWORDS: Genetic Algorithms, Artificial Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation, Classification, Clustering.",
        "published": "2004-12-17T17:04:16Z",
        "link": "http://arxiv.org/abs/cs/0412084v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2; I.5"
        ]
    },
    {
        "title": "Chosen-Plaintext Cryptanalysis of a Clipped-Neural-Network-Based Chaotic   Cipher",
        "authors": [
            "Chengqing Li",
            "Shujun Li",
            "Dan Zhang",
            "Guanrong Chen"
        ],
        "summary": "In ISNN'04, a novel symmetric cipher was proposed, by combining a chaotic signal and a clipped neural network (CNN) for encryption. The present paper analyzes the security of this chaotic cipher against chosen-plaintext attacks, and points out that this cipher can be broken by a chosen-plaintext attack. Experimental analyses are given to support the feasibility of the proposed attack.",
        "published": "2004-12-23T10:33:18Z",
        "link": "http://arxiv.org/abs/cs/0412103v3",
        "categories": [
            "cs.CR",
            "cs.NE",
            "nlin.CD"
        ]
    },
    {
        "title": "Global minimization of a quadratic functional: neural network approach",
        "authors": [
            "L. B. Litinskii",
            "B. M. Magomedov"
        ],
        "summary": "The problem of finding out the global minimum of a multiextremal functional is discussed. One frequently faces with such a functional in various applications. We propose a procedure, which depends on the dimensionality of the problem polynomially. In our approach we use the eigenvalues and eigenvectors of the connection matrix.",
        "published": "2004-12-24T13:41:10Z",
        "link": "http://arxiv.org/abs/cs/0412109v1",
        "categories": [
            "cs.NE",
            "cs.DM"
        ]
    },
    {
        "title": "Q-valued neural network as a system of fast identification and pattern   recognition",
        "authors": [
            "D. I. Alieva",
            "B. V. Kryzhanovsky",
            "V. M. Kryzhanovsky",
            "A. B. Fonarev"
        ],
        "summary": "An effective neural network algorithm of the perceptron type is proposed. The algorithm allows us to identify strongly distorted input vector reliably. It is shown that its reliability and processing speed are orders of magnitude higher than that of full connected neural networks. The processing speed of our algorithm exceeds the one of the stack fast-access retrieval algorithm that is modified for working when there are noises in the input channel.",
        "published": "2004-12-24T13:48:44Z",
        "link": "http://arxiv.org/abs/cs/0412110v1",
        "categories": [
            "cs.NE",
            "cs.CV"
        ]
    },
    {
        "title": "Randomized selection with tripartitioning",
        "authors": [
            "Krzysztof C. Kiwiel"
        ],
        "summary": "We show that several versions of Floyd and Rivest's algorithm Select [Comm.\\ ACM {\\bf 18} (1975) 173] for finding the $k$th smallest of $n$ elements require at most $n+\\min\\{k,n-k\\}+o(n)$ comparisons on average, even when equal elements occur. This parallels our recent analysis of another variant due to Floyd and Rivest [Comm. ACM {\\bf 18} (1975) 165--172]. Our computational results suggest that both variants perform well in practice, and may compete with other selection methods, such as Hoare's Find or quickselect with median-of-3 pivots.",
        "published": "2004-01-04T05:29:58Z",
        "link": "http://arxiv.org/abs/cs/0401003v1",
        "categories": [
            "cs.DS",
            "F.2.2; G3"
        ]
    },
    {
        "title": "Initial Experiences Re-Exporting Duplicate and Similarity Computation   with an OAI-PMH aggregator",
        "authors": [
            "Terry L. Harrison",
            "Aravind Elango",
            "Johan Bollen",
            "Michael Nelson"
        ],
        "summary": "The proliferation of the Open Archive Initiative Protocol for Metadata Harvesting (OAI-PMH) has resulted in the creation of a large number of service providers, all harvesting from either data providers or aggregators. If data were available regarding the similarity of metadata records, service providers could track redundant records across harvests from multiple sources as well as provide additional end-user services. Due to the large number of metadata formats and the diverse mapping strategies employed by data providers, similarity calculation requirements necessitate the use of information retrieval strategies. We describe an OAI-PMH aggregator implementation that uses the optional ``<about>'' container to re-export the results of similarity calculations. Metadata records (3751) were harvested from a NASA data provider and similarities for the records were computed. The results were useful for detecting duplicates, similarities and metadata errors.",
        "published": "2004-01-05T05:41:12Z",
        "link": "http://arxiv.org/abs/cs/0401001v1",
        "categories": [
            "cs.DL",
            "cs.DS",
            "H.3.7;H.3.3"
        ]
    },
    {
        "title": "Heuristic average-case analysis of the backtrack resolution of random   3-Satisfiability instances",
        "authors": [
            "Simona Cocco",
            "Remi Monasson"
        ],
        "summary": "An analysis of the average-case complexity of solving random 3-Satisfiability (SAT) instances with backtrack algorithms is presented. We first interpret previous rigorous works in a unifying framework based on the statistical physics notions of dynamical trajectories, phase diagram and growth process. It is argued that, under the action of the Davis--Putnam--Loveland--Logemann (DPLL) algorithm, 3-SAT instances are turned into 2+p-SAT instances whose characteristic parameters (ratio alpha of clauses per variable, fraction p of 3-clauses) can be followed during the operation, and define resolution trajectories. Depending on the location of trajectories in the phase diagram of the 2+p-SAT model, easy (polynomial) or hard (exponential) resolutions are generated. Three regimes are identified, depending on the ratio alpha of the 3-SAT instance to be solved. Lower sat phase: for small ratios, DPLL almost surely finds a solution in a time growing linearly with the number N of variables. Upper sat phase: for intermediate ratios, instances are almost surely satisfiable but finding a solution requires exponential time (2 ^ (N omega) with omega>0) with high probability. Unsat phase: for large ratios, there is almost always no solution and proofs of refutation are exponential. An analysis of the growth of the search tree in both upper sat and unsat regimes is presented, and allows us to estimate omega as a function of alpha. This analysis is based on an exact relationship between the average size of the search tree and the powers of the evolution operator encoding the elementary steps of the search heuristic.",
        "published": "2004-01-14T12:47:57Z",
        "link": "http://arxiv.org/abs/cs/0401011v1",
        "categories": [
            "cs.DS",
            "cond-mat.stat-mech",
            "cs.CC",
            "A.0"
        ]
    },
    {
        "title": "Algebraic Elimination of epsilon-transitions",
        "authors": [
            "Gerard Duchamp",
            "Hatem Hadj Kacem",
            "Eric Laugerotte"
        ],
        "summary": "We present here algebraic formulas associating a k-automaton to a k-epsilon-automaton. The existence depends on the definition of the star of matrices and of elements in the semiring k. For this reason, we present the theorem which allows the transformation of k-epsilon-automata into k-automata. The two automata have the same behaviour.",
        "published": "2004-01-15T16:12:51Z",
        "link": "http://arxiv.org/abs/cs/0401012v2",
        "categories": [
            "cs.SC",
            "cs.DS",
            "H. 2"
        ]
    },
    {
        "title": "Improved randomized selection",
        "authors": [
            "Krzysztof C. Kiwiel"
        ],
        "summary": "We show that several versions of Floyd and Rivest's improved algorithm Select for finding the $k$th smallest of $n$ elements require at most $n+\\min\\{k,n-k\\}+O(n^{1/2}\\ln^{1/2}n)$ comparisons on average and with high probability. This rectifies the analysis of Floyd and Rivest, and extends it to the case of nondistinct elements. Encouraging computational results on large median-finding problems are reported.",
        "published": "2004-02-02T14:16:52Z",
        "link": "http://arxiv.org/abs/cs/0402005v1",
        "categories": [
            "cs.DS",
            "F.2.2, G3"
        ]
    },
    {
        "title": "The Munich Rent Advisor: A Success for Logic Programming on the Internet",
        "authors": [
            "Thom Fruehwirth",
            "Slim Abdennadher"
        ],
        "summary": "Most cities in Germany regularly publish a booklet called the {\\em Mietspiegel}. It basically contains a verbal description of an expert system. It allows the calculation of the estimated fair rent for a flat. By hand, one may need a weekend to do so. With our computerized version, the {\\em Munich Rent Advisor}, the user just fills in a form in a few minutes and the rent is calculated immediately. We also extended the functionality and applicability of the {\\em Mietspiegel} so that the user need not answer all questions on the form. The key to computing with partial information using high-level programming was to use constraint logic programming. We rely on the internet, and more specifically the World Wide Web, to provide this service to a broad user group. More than ten thousand people have used our service in the last three years. This article describes the experiences in implementing and using the {\\em Munich Rent Advisor}. Our results suggests that logic programming with constraints can be an important ingredient in intelligent internet systems.",
        "published": "2004-02-10T15:10:36Z",
        "link": "http://arxiv.org/abs/cs/0402019v1",
        "categories": [
            "cs.AI",
            "cs.DS",
            "D.3.3;G.1.6"
        ]
    },
    {
        "title": "The lattice dimension of a graph",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We describe a polynomial time algorithm for, given an undirected graph G, finding the minimum dimension d such that G may be isometrically embedded into the d-dimensional integer lattice Z^d.",
        "published": "2004-02-13T05:23:03Z",
        "link": "http://arxiv.org/abs/cs/0402028v1",
        "categories": [
            "cs.DS",
            "math.CO",
            "F.2.2"
        ]
    },
    {
        "title": "Coins Make Quantum Walks Faster",
        "authors": [
            "Andris Ambainis",
            "Julia Kempe",
            "Alexander Rivosh"
        ],
        "summary": "We show how to search N items arranged on a $\\sqrt{N}\\times\\sqrt{N}$ grid in time $O(\\sqrt N \\log N)$, using a discrete time quantum walk. This result for the first time exhibits a significant difference between discrete time and continuous time walks without coin degrees of freedom, since it has been shown recently that such a continuous time walk needs time $\\Omega(N)$ to perform the same task. Our result furthermore improves on a previous bound for quantum local search by Aaronson and Ambainis. We generalize our result to 3 and more dimensions where the walk yields the optimal performance of $O(\\sqrt{N})$ and give several extensions of quantum walk search algorithms for general graphs. The coin-flip operation needs to be chosen judiciously: we show that another ``natural'' choice of coin gives a walk that takes $\\Omega(N)$ steps. We also show that in 2 dimensions it is sufficient to have a two-dimensional coin-space to achieve the time $O(\\sqrt{N} \\log N)$.",
        "published": "2004-02-16T23:32:23Z",
        "link": "http://arxiv.org/abs/quant-ph/0402107v1",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "A General Framework for Bounds for Higher-Dimensional Orthogonal Packing   Problems",
        "authors": [
            "Sandor P. Fekete",
            "Joerg Schepers"
        ],
        "summary": "Higher-dimensional orthogonal packing problems have a wide range of practical applications, including packing, cutting, and scheduling. In the context of a branch-and-bound framework for solving these packing problems to optimality, it is of crucial importance to have good and easy bounds for an optimal solution. Previous efforts have produced a number of special classes of such bounds. Unfortunately, some of these bounds are somewhat complicated and hard to generalize. We present a new approach for obtaining classes of lower bounds for higher-dimensional packing problems; our bounds improve and simplify several well-known bounds from previous literature. In addition, our approach provides an easy framework for proving correctness of new bounds.",
        "published": "2004-02-18T16:05:16Z",
        "link": "http://arxiv.org/abs/cs/0402044v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "The Freeze-Tag Problem: How to Wake Up a Swarm of Robots",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Sandor P. Fekete",
            "Joseph S. B. Mitchell",
            "Martin Skutella"
        ],
        "summary": "An optimization problem that naturally arises in the study of swarm robotics is the Freeze-Tag Problem (FTP) of how to awaken a set of ``asleep'' robots, by having an awakened robot move to their locations. Once a robot is awake, it can assist in awakening other slumbering robots.The objective is to have all robots awake as early as possible. While the FTP bears some resemblance to problems from areas in combinatorial optimization such as routing, broadcasting, scheduling, and covering, its algorithmic characteristics are surprisingly different. We consider both scenarios on graphs and in geometric environments.In graphs, robots sleep at vertices and there is a length function on the edges. Awake robots travel along edges, with time depending on edge length. For most scenarios, we consider the offline version of the problem, in which each awake robot knows the position of all other robots. We prove that the problem is NP-hard, even for the special case of star graphs. We also establish hardness of approximation, showing that it is NP-hard to obtain an approximation factor better than 5/3, even for graphs of bounded degree.These lower bounds are complemented with several positive algorithmic results, including: (1) We show that the natural greedy strategy on star graphs has a tight worst-case performance of 7/3 and give a polynomial-time approximation scheme (PTAS) for star graphs. (2) We give a simple O(log D)-competitive online algorithm for graphs with maximum degree D and locally bounded edge weights. (3) We give a PTAS, running in nearly linear time, for geometrically embedded instances.",
        "published": "2004-02-18T20:49:02Z",
        "link": "http://arxiv.org/abs/cs/0402045v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Fast Multipoint-Evaluation of Bivariate Polynomials",
        "authors": [
            "Michael Nüsken",
            "Martin Ziegler"
        ],
        "summary": "We generalize univariate multipoint evaluation of polynomials of degree n at sublinear amortized cost per point. More precisely, it is shown how to evaluate a bivariate polynomial p of maximum degree less than n, specified by its n^2 coefficients, simultaneously at n^2 given points using a total of O(n^{2.667}) arithmetic operations. In terms of the input size N being quadratic in n, this amounts to an amortized cost of O(N^{0.334}) per point.",
        "published": "2004-03-12T14:31:43Z",
        "link": "http://arxiv.org/abs/cs/0403022v2",
        "categories": [
            "cs.DS",
            "F.2.1"
        ]
    },
    {
        "title": "An Application of Rational Trees in a Logic Programming Interpreter for   a Procedural Language",
        "authors": [
            "Manuel Carro"
        ],
        "summary": "We describe here a simple application of rational trees to the implementation of an interpreter for a procedural language written in a logic programming language. This is possible in languages designed to support rational trees (such as Prolog II and its descendants), but also in traditional Prolog, whose data structures are initially based on Herbrand terms, but in which implementations often omit the occurs check needed to avoid the creation of infinite data structures. We provide code implementing two interpreters, one of which needs non-occurs-check unification, which makes it faster (and more economic). We provide experimental data supporting this, and we argue that rational trees are interesting enough as to receive thorough support inside the language.",
        "published": "2004-03-16T16:48:38Z",
        "link": "http://arxiv.org/abs/cs/0403028v1",
        "categories": [
            "cs.DS",
            "cs.LO",
            "D.1.6 ; D.3.2 ; D.3.3 ; D.3.4 ; E.1 ; E.2"
        ]
    },
    {
        "title": "Quantum walks and their algorithmic applications",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "Quantum walks are quantum counterparts of Markov chains. In this article, we give a brief overview of quantum walks, with emphasis on their algorithmic applications.",
        "published": "2004-03-16T21:49:43Z",
        "link": "http://arxiv.org/abs/quant-ph/0403120v3",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Unicyclic Components in Random Graphs",
        "authors": [
            "E. Ben-Naim",
            "P. L. Krapivsky"
        ],
        "summary": "The distribution of unicyclic components in a random graph is obtained analytically. The number of unicyclic components of a given size approaches a self-similar form in the vicinity of the gelation transition. At the gelation point, this distribution decays algebraically, U_k ~ 1/(4k) for k>>1. As a result, the total number of unicyclic components grows logarithmically with the system size.",
        "published": "2004-03-18T03:19:24Z",
        "link": "http://arxiv.org/abs/cond-mat/0403453v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.DS",
            "math.PR"
        ]
    },
    {
        "title": "Schedulers and Redundancy for a Class of Constraint Propagation Rules",
        "authors": [
            "Sebastian Brand",
            "Krzysztof R. Apt"
        ],
        "summary": "We study here schedulers for a class of rules that naturally arise in the context of rule-based constraint programming. We systematically derive a scheduler for them from a generic iteration algorithm of [Apt 2000]. We apply this study to so-called membership rules of [Apt and Monfroy 2001]. This leads to an implementation that yields a considerably better performance for these rules than their execution as standard CHR rules. Finally, we show how redundant rules can be identified and how appropriately reduced sets of rules can be computed.",
        "published": "2004-03-23T11:42:45Z",
        "link": "http://arxiv.org/abs/cs/0403037v3",
        "categories": [
            "cs.DS",
            "cs.PL",
            "I.2.2; I.2.3; D.1.2; D.3.3; D.3.4"
        ]
    },
    {
        "title": "Generating connected acyclic digraphs uniformly at random",
        "authors": [
            "Guy Melancon",
            "Fabrice Philippe"
        ],
        "summary": "We describe a simple algorithm based on a Markov chain process to generate simply connected acyclic directed graphs over a fixed set of vertices. This algorithm is an extension of a previous one, designed to generate acyclic digraphs, non necessarily connected.",
        "published": "2004-03-25T08:41:38Z",
        "link": "http://arxiv.org/abs/cs/0403040v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2.2;G.2.2;G.3"
        ]
    },
    {
        "title": "A quantum Fourier transform algorithm",
        "authors": [
            "Chris Lomont"
        ],
        "summary": "Algorithms to compute the quantum Fourier transform over a cyclic group are fundamental to many quantum algorithms. This paper describes such an algorithm and gives a proof of its correctness, tightening some claimed performance bounds given earlier. Exact bounds are given for the number of qubits needed to achieve a desired tolerance, allowing simulation of the algorithm.",
        "published": "2004-04-09T17:02:50Z",
        "link": "http://arxiv.org/abs/quant-ph/0404060v2",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "The Random Buffer Tree : A Randomized Technique for I/O-efficient   Algorithms",
        "authors": [
            "Saju Jude Dominic",
            "G. Sajith"
        ],
        "summary": "In this paper, we present a probabilistic self-balancing dictionary data structure for massive data sets, and prove expected amortized I/O-optimal bounds on the dictionary operations. We show how to use the structure as an I/O-optimal priority queue. The data structure, which we call as the random buffer tree, abstracts the properties of the random treap and the buffer tree and has the same expected I/O-bounds as the buffer tree.",
        "published": "2004-04-13T13:49:11Z",
        "link": "http://arxiv.org/abs/cs/0404028v1",
        "categories": [
            "cs.DS",
            "E1;F.2.2;G.3"
        ]
    },
    {
        "title": "Online Searching with an Autonomous Robot",
        "authors": [
            "Sandor P. Fekete",
            "Rolf Klein",
            "Andreas Nuechter"
        ],
        "summary": "We discuss online strategies for visibility-based searching for an object hidden behind a corner, using Kurt3D, a real autonomous mobile robot. This task is closely related to a number of well-studied problems. Our robot uses a three-dimensional laser scanner in a stop, scan, plan, go fashion for building a virtual three-dimensional environment. Besides planning trajectories and avoiding obstacles, Kurt3D is capable of identifying objects like a chair. We derive a practically useful and asymptotically optimal strategy that guarantees a competitive ratio of 2, which differs remarkably from the well-studied scenario without the need of stopping for surveying the environment. Our strategy is used by Kurt3D, documented in a separate video.",
        "published": "2004-04-16T21:46:15Z",
        "link": "http://arxiv.org/abs/cs/0404036v1",
        "categories": [
            "cs.RO",
            "cs.DS",
            "I.2.9"
        ]
    },
    {
        "title": "Visualising the structure of architectural open spaces based on shape   analysis",
        "authors": [
            "Sanjay Rana",
            "Mike Batty"
        ],
        "summary": "This paper proposes the application of some well known two-dimensional geometrical shape descriptors for the visualisation of the structure of architectural open spaces. The paper demonstrates the use of visibility measures such as distance to obstacles and amount of visible space to calculate shape descriptors such as convexity and skeleton of the open space. The aim of the paper is to indicate a simple, objective and quantifiable approach to understand the structure of open spaces otherwise impossible due to the complex construction of built structures.",
        "published": "2004-04-22T13:42:48Z",
        "link": "http://arxiv.org/abs/cs/0404046v1",
        "categories": [
            "cs.CV",
            "cs.CG",
            "cs.DS",
            "I.3.5;I.4.8;I.5.2"
        ]
    },
    {
        "title": "Efficient coroutine generation of constrained Gray sequences",
        "authors": [
            "Donald E. Knuth",
            "Frank Ruskey"
        ],
        "summary": "We study an interesting family of cooperating coroutines, which is able to generate all patterns of bits that satisfy certain fairly general ordering constraints, changing only one bit at a time. (More precisely, the directed graph of constraints is required to be cycle-free when it is regarded as an undirected graph.) If the coroutines are implemented carefully, they yield an algorithm that needs only a bounded amount of computation per bit change, thereby solving an open problem in the field of combinatorial pattern generation.",
        "published": "2004-04-30T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/0404058v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Bi-criteria Algorithm for Scheduling Jobs on Cluster Platforms",
        "authors": [
            "Pierre-Francois Dutot",
            "Lionel Eyraud",
            "Grégory Mounié",
            "Denis Trystram"
        ],
        "summary": "We describe in this paper a new method for building an efficient algorithm for scheduling jobs in a cluster. Jobs are considered as parallel tasks (PT) which can be scheduled on any number of processors. The main feature is to consider two criteria that are optimized together. These criteria are the makespan and the weighted minimal average completion time (minsum). They are chosen for their complementarity, to be able to represent both user-oriented objectives and system administrator objectives. We propose an algorithm based on a batch policy with increasing batch sizes, with a smart selection of jobs in each batch. This algorithm is assessed by intensive simulation results, compared to a new lower bound (obtained by a relaxation of ILP) of the optimal schedules for both criteria separately. It is currently implemented in an actual real-size cluster platform.",
        "published": "2004-05-04T14:51:55Z",
        "link": "http://arxiv.org/abs/cs/0405006v3",
        "categories": [
            "cs.DC",
            "cs.DS",
            "ACM F.2.2, ACM D.4.1"
        ]
    },
    {
        "title": "The modulus in the CAD system drawings as a base of developing of the   problem-oriented extensions",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The concept of the \"modulus\" in the CAD system drawings is characterized, being a base of developing of the problem-oriented extensions. The modulus consists of visible geometric elements of the drawing and invisible parametric representation of the modelling object. The technological advantages of moduluss in a complex CAD system developing are described.",
        "published": "2004-05-12T07:55:34Z",
        "link": "http://arxiv.org/abs/cs/0405041v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "J.6"
        ]
    },
    {
        "title": "Modular technology of developing of the problem-oriented extensions of a   CAD system of reconstruction of the plant",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The modular technology of creation of the problem-oriented extensions of a CAD system is described, which was realised in a system TechnoCAD GlassX for designing of reconstruction of the plants. The modularity of the technology is expressed in storage of all parameters of the design in one element of the drawing - modulus, with automatic generation of a geometrical part of the modulus from these parameters. The common principles of the system organization of extensions developing are described: separation of the part of the design to automize in this extension, architecture of parameters in the form of the lists of objects with their properties and links to another objects, separation of common and special operations, stages of the developing, boundaries of applicability of technology.",
        "published": "2004-05-14T17:43:33Z",
        "link": "http://arxiv.org/abs/cs/0405047v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "I.2.1, J.6"
        ]
    },
    {
        "title": "Synchronous Relaxation for Parallel Ising Spin Simulations",
        "authors": [
            "Boris Lubachevsky",
            "Alan Weiss"
        ],
        "summary": "A new parallel algorithm for simulating Ising spin systems is presented. The sequential prototype is the n-fold way algorithm cite{BKL75}, which is efficient but is hard to parallelize using conservative methods. Our parallel algorithm is optimistic. Unlike other optimistic algorithms, e.g., Time Warp, our algorithm is synchronous. It also belongs to the class of simulations known as ``relaxation'' cite{CS8 hence it is named ``synchronous relaxation.'' We derive performance guarantees for this algorithm. If N is the number of PEs, then under weak assumptions we show that the number of correct events processed per unit of time is, on average, at least of order N/log(N). All communication delays, processing time, and busy waits are taken into account.",
        "published": "2004-05-17T01:32:16Z",
        "link": "http://arxiv.org/abs/cs/0405053v1",
        "categories": [
            "cs.DC",
            "cond-mat.mtrl-sci",
            "cs.DS",
            "physics.comp-ph",
            "D.1.3; D.4.1; D.4.8; I.6.8"
        ]
    },
    {
        "title": "The model of the tables in design documentation for operating with the   electronic catalogs and for specifications making in a CAD system",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The hierarchic block model of the tables in design documentation as a part of a CAD system is described, intended for automatic specifications making of elements of the drawings, with usage of the electronic catalogs. The model is created for needs of a CAD system of reconstruction of the industrial plants, where the result of designing are the drawings, which include the specifications of different types. The adequate simulation of the specification tables is ensured with technology of storing in the drawing of the visible geometric elements and invisible parametric representation, sufficient for generation of this elements.",
        "published": "2004-05-17T09:39:03Z",
        "link": "http://arxiv.org/abs/cs/0405054v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2, I.2.1, J.6"
        ]
    },
    {
        "title": "Modular technology of developing of the extensions of a CAD system.   Axonometric piping diagrams. Parametric representation",
        "authors": [
            "Vladimir V. Migunov",
            "Rustem R. Kafiatullov",
            "Ilsur T. Safin"
        ],
        "summary": "Applying the modular technology of developing of the problem-oriented extensions of a CAD system to a problem of automation of creating of the axonometric piping diagrams on an example of the program system TechnoCAD GlassX is described. The proximity of composition of the schemas is detected for special technological pipe lines, systems of a water line and water drain, heating, heat supply, ventilating, air conditioning. The structured parametric representation of the schemas, including properties of objects, their link, common settings, settings by default and the special links of compatibility is reviewed.",
        "published": "2004-05-17T09:46:02Z",
        "link": "http://arxiv.org/abs/cs/0405055v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2, I.2.1, J.6"
        ]
    },
    {
        "title": "Modular technology of developing of the extensions of a CAD system. The   axonometric piping diagrams. Common and special operations",
        "authors": [
            "Ilsur T. Safin",
            "Vladimir V. Migunov",
            "Rustem R. Kafiatullov"
        ],
        "summary": "Applying the modular technology of developing of the problem-oriented extensions of a CAD system to a problem of automation of creating of the axonometric piping diagrams on an example of the program system TechnoCAD GlassX is described. The features of realization of common operations, composition and realization of special operations of a designing of the schemas of the special technological pipe lines, systems of a water line and water drain, heating, heat supply, ventilating, air conditioning are reviewed.",
        "published": "2004-05-17T10:14:12Z",
        "link": "http://arxiv.org/abs/cs/0405056v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "I.2.1, J.6"
        ]
    },
    {
        "title": "Mathematical and programming toolkit of the computer aided design of the   axonometric piping diagrams",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The problem of the automation of the designing of the axonometric piping diagrams include, as the minimum, manipulations with the flat schemas of three-dimensional wireframe objects (with dimension of 2,5). The specialized model, methodical and mathematical approaches are required because of large bulk of calculuss. Coordinate systems, data types, common principles of realization of operation with data and composition of the basic operations are described which are realised in the complex CAD system of the reconstruction of the plants TechnoCAD GlassX.",
        "published": "2004-05-17T10:34:16Z",
        "link": "http://arxiv.org/abs/cs/0405057v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "I.2.1, J.6"
        ]
    },
    {
        "title": "Neighborhood-Based Topology Recognition in Sensor Networks",
        "authors": [
            "Sandor P. Fekete",
            "Alexander Kroeller",
            "Dennis Pfisterer",
            "Stefan Fischer",
            "Carsten Buschmann"
        ],
        "summary": "We consider a crucial aspect of self-organization of a sensor network consisting of a large set of simple sensor nodes with no location hardware and only very limited communication range. After having been distributed randomly in a given two-dimensional region, the nodes are required to develop a sense for the environment, based on a limited amount of local communication. We describe algorithmic approaches for determining the structure of boundary nodes of the region, and the topology of the region. We also develop methods for determining the outside boundary, the distance to the closest boundary for each point, the Voronoi diagram of the different boundaries, and the geometric thickness of the network. Our methods rely on a number of natural assumptions that are present in densely distributed sets of nodes, and make use of a combination of stochastics, topology, and geometry. Evaluation requires only a limited number of simple local computations.",
        "published": "2004-05-17T12:16:08Z",
        "link": "http://arxiv.org/abs/cs/0405058v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "C.2.1; F.2.2; G.3"
        ]
    },
    {
        "title": "Fast Simulation of Multicomponent Dynamic Systems",
        "authors": [
            "Boris D. Lubachevsky"
        ],
        "summary": "A computer simulation has to be fast to be helpful, if it is employed to study the behavior of a multicomponent dynamic system. This paper discusses modeling concepts and algorithmic techniques useful for creating such fast simulations. Concrete examples of simulations that range from econometric modeling to communications to material science are used to illustrate these techniques and concepts. The algorithmic and modeling methods discussed include event-driven processing, ``anticipating'' data structures, and ``lazy'' evaluation, Poisson dispenser, parallel processing by cautious advancements and by synchronous relaxations. The paper gives examples of how these techniques and models are employed in assessing efficiency of capacity management methods in wireless and wired networks, in studies of magnetization, crystalline structure, and sediment formation in material science, in studies of competition in economics.",
        "published": "2004-05-22T16:51:48Z",
        "link": "http://arxiv.org/abs/cs/0405077v2",
        "categories": [
            "cs.DS",
            "cond-mat.mtrl-sci",
            "cs.DC",
            "I.6.8; G.3; G.4; J.2; J.4"
        ]
    },
    {
        "title": "The Complexity of Maximum Matroid-Greedoid Intersection and Weighted   Greedoid Maximization",
        "authors": [
            "Taneli Mielikäinen",
            "Esko Ukkonen"
        ],
        "summary": "The maximum intersection problem for a matroid and a greedoid, given by polynomial-time oracles, is shown $NP$-hard by expressing the satisfiability of boolean formulas in 3-conjunctive normal form as such an intersection. The corresponding approximation problems are shown $NP$-hard for certain approximation performance bounds. Moreover, some natural parameterized variants of the problem are shown $W[P]$-hard. The results are in contrast with the maximum matroid-matroid intersection which is solvable in polynomial time by an old result of Edmonds. We also prove that it is $NP$-hard to approximate the weighted greedoid maximization within $2^{n^{O(1)}}$ where $n$ is the size of the domain of the greedoid.   A preliminary version ``The Complexity of Maximum Matroid-Greedoid Intersection'' appeared in Proc. FCT 2001, LNCS 2138, pp. 535--539, Springer-Verlag 2001.",
        "published": "2004-05-25T12:09:34Z",
        "link": "http://arxiv.org/abs/cs/0405094v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "An analysis of a bounded resource search puzzle",
        "authors": [
            "Gopal Ananthraman"
        ],
        "summary": "Consider the commonly known puzzle, given $k$ glass balls, find an optimal algorithm to determine the lowest floor of a building of $n$ floors from which a thrown glass ball will break. This puzzle was originally posed in its original form in \\cite{focs1980}and was later cited in the book \\cite{algthc}. There are several internet sites that presents this puzzle and its solution to the special case of $k=2$ balls. This is the first such analysis of the puzzle in its general form. Several variations of this puzzle have been studied with applications in Network Loading \\cite{cgstctl} which analyzes a case similar to a scenario where an adversary is changing the lowest floor with time. Although the algorithm specified in \\cite{algthc} solves the problem, it is not an efficient algorithm. In this paper another algorithm for the same problem is analyzed. It is shown that if $m$ is the minimum number of attempts required then for $k \\geq m$ we have $m = \\log (n+1)$ and for $k < m$ we have, $1 + \\sum_{i=1}^{k}{{m-1}\\choose{i}} < n \\leq \\sum_{i=1}^{k}{{m}\\choose{i}}$",
        "published": "2004-05-28T21:26:10Z",
        "link": "http://arxiv.org/abs/cs/0405110v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Algorithms for weighted multi-tape automata",
        "authors": [
            "Andre Kempe",
            "Franck Guingne",
            "Florent Nicart"
        ],
        "summary": "This report defines various operations for weighted multi-tape automata (WMTAs) and describes algorithms that have been implemented for those operations in the WFSC toolkit. Some algorithms are new, others are known or similar to known algorithms. The latter will be recalled to make this report more complete and self-standing. We present a new approach to multi-tape intersection, meaning the intersection of a number of tapes of one WMTA with the same number of tapes of another WMTA. In our approach, multi-tape intersection is not considered as an atomic operation but rather as a sequence of more elementary ones, which facilitates its implementation. We show an example of multi-tape intersection, actually transducer intersection, that can be compiled with our approach but not with several other methods that we analysed. To show the practical relavance of our work, we include an example of application: the preservation of intermediate results in transduction cascades.",
        "published": "2004-06-02T18:51:52Z",
        "link": "http://arxiv.org/abs/cs/0406003v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Merging costs for the additive Marcus-Lushnikov process, and Union-Find   algorithms",
        "authors": [
            "Philippe Chassaing",
            "Regine Marchand"
        ],
        "summary": "Starting with a monodisperse configuration with $n$ size-1 particles, an additive Marcus-Lushnikov process evolves until it reaches its final state (a unique particle with mass $n$). At each of the $n-1$ steps of its evolution, a merging cost is incurred, that depends on the sizes of the two particles involved, and on an independent random factor. This paper deals with the asymptotic behaviour of the cumulated costs up to the $k$th clustering, under various regimes for $(n,k)$, with applications to the study of Union--Find algorithms.",
        "published": "2004-06-05T19:20:16Z",
        "link": "http://arxiv.org/abs/math/0406094v1",
        "categories": [
            "math.PR",
            "cs.DS",
            "math.CO",
            "68P10 (Primary) 60C05, 60J65, 68R05 (Secondary)"
        ]
    },
    {
        "title": "Algorithms for Drawing Media",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We describe algorithms for drawing media, systems of states, tokens and actions that have state transition graphs in the form of partial cubes. Our algorithms are based on two principles: embedding the state transition graph in a low-dimensional integer lattice and projecting the lattice onto the plane, or drawing the medium as a planar graph with centrally symmetric faces.",
        "published": "2004-06-16T00:49:40Z",
        "link": "http://arxiv.org/abs/cs/0406020v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Ramsey-type theorems for metric spaces with applications to online   problems",
        "authors": [
            "Yair Bartal",
            "Bela Bollobas",
            "Manor Mendel"
        ],
        "summary": "A nearly logarithmic lower bound on the randomized competitive ratio for the metrical task systems problem is presented. This implies a similar lower bound for the extensively studied k-server problem. The proof is based on Ramsey-type theorems for metric spaces, that state that every metric space contains a large subspace which is approximately a hierarchically well-separated tree (and in particular an ultrametric). These Ramsey-type theorems may be of independent interest.",
        "published": "2004-06-16T21:56:48Z",
        "link": "http://arxiv.org/abs/cs/0406028v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Randomized k-server algorithms for growth-rate bounded graphs",
        "authors": [
            "Manor Mendel"
        ],
        "summary": "The paper referred to in the title is withdrawn.",
        "published": "2004-06-17T15:11:54Z",
        "link": "http://arxiv.org/abs/cs/0406033v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Better algorithms for unfair metrical task systems and applications",
        "authors": [
            "Amos Fiat",
            "Manor Mendel"
        ],
        "summary": "Unfair metrical task systems are a generalization of online metrical task systems. In this paper we introduce new techniques to combine algorithms for unfair metrical task systems and apply these techniques to obtain improved randomized online algorithms for metrical task systems on arbitrary metric spaces.",
        "published": "2004-06-17T18:49:20Z",
        "link": "http://arxiv.org/abs/cs/0406034v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "On metric Ramsey-type phenomena",
        "authors": [
            "Yair Bartal",
            "Nathan Linial",
            "Manor Mendel",
            "Assaf Naor"
        ],
        "summary": "The main question studied in this article may be viewed as a nonlinear analogue of Dvoretzky's theorem in Banach space theory or as part of Ramsey theory in combinatorics. Given a finite metric space on n points, we seek its subspace of largest cardinality which can be embedded with a given distortion in Hilbert space. We provide nearly tight upper and lower bounds on the cardinality of this subspace in terms of n and the desired distortion. Our main theorem states that for any epsilon>0, every n point metric space contains a subset of size at least n^{1-\\epsilon} which is embeddable in Hilbert space with O(\\frac{\\log(1/\\epsilon)}{\\epsilon}) distortion. The bound on the distortion is tight up to the log(1/\\epsilon) factor. We further include a comprehensive study of various other aspects of this problem.",
        "published": "2004-06-17T20:01:40Z",
        "link": "http://arxiv.org/abs/math/0406353v2",
        "categories": [
            "math.MG",
            "cs.DS",
            "52C45, 05C55, 54E40, 05C12, 54E40"
        ]
    },
    {
        "title": "Optimal Free-Space Management and Routing-Conscious Dynamic Placement   for Reconfigurable Devices",
        "authors": [
            "Ali Ahmadinia",
            "Christophe Bobda",
            "Sandor Fekete",
            "Juergen Teich",
            "Jan van der Veen"
        ],
        "summary": "We describe algorithmic results for two crucial aspects of allocating resources on computational hardware devices with partial reconfigurability. By using methods from the field of computational geometry, we derive a method that allows correct maintainance of free and occupied space of a set of n rectangular modules in optimal time Theta(n log n); previous approaches needed a time of O(n^2) for correct results and O(n) for heuristic results. We also show that finding an optimal feasible communication-conscious placement (which minimizes the total weighted Manhattan distance between the new module and existing demand points) can be computed in Theta(n log n). Both resulting algorithms are practically easy to implement and show convincing experimental behavior.",
        "published": "2004-06-18T13:29:46Z",
        "link": "http://arxiv.org/abs/cs/0406035v3",
        "categories": [
            "cs.DS",
            "cs.CG",
            "C.1.3; F.2.2"
        ]
    },
    {
        "title": "Online Companion Caching",
        "authors": [
            "Manor Mendel",
            "Steven S. Seiden"
        ],
        "summary": "This paper is concerned with online caching algorithms for the (n,k)-companion cache, defined by Brehob et. al. In this model the cache is composed of two components: a k-way set-associative cache and a companion fully-associative cache of size n. We show that the deterministic competitive ratio for this problem is (n+1)(k+1)-1, and the randomized competitive ratio is O(\\log n \\log k) and \\Omega(\\log n +\\log k).",
        "published": "2004-06-18T16:20:24Z",
        "link": "http://arxiv.org/abs/cs/0406036v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The Computational Complexity of Orientation Search Problems in   Cryo-Electron Microscopy",
        "authors": [
            "Taneli Mielikäinen",
            "Janne Ravantti",
            "Esko Ukkonen"
        ],
        "summary": "In this report we study the problem of determining three-dimensional orientations for noisy projections of randomly oriented identical particles. The problem is of central importance in the tomographic reconstruction of the density map of macromolecular complexes from electron microscope images and it has been studied intensively for more than 30 years.   We analyze the computational complexity of the orientation problem and show that while several variants of the problem are $NP$-hard, inapproximable and fixed-parameter intractable, some restrictions are polynomial-time approximable within a constant factor or even solvable in logarithmic space. The orientation search problem is formalized as a constrained line arrangement problem that is of independent interest. The negative complexity results give a partial justification for the heuristic methods used in orientation search, and the positive complexity results on the orientation search have some positive implications also to the problem of finding functionally analogous genes.   A preliminary version ``The Computational Complexity of Orientation Search in Cryo-Electron Microscopy'' appeared in Proc. ICCS 2004, LNCS 3036, pp. 231--238. Springer-Verlag 2004.",
        "published": "2004-06-23T14:28:17Z",
        "link": "http://arxiv.org/abs/cs/0406043v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "cs.CV",
            "F.2.2; I.4.5; J.3"
        ]
    },
    {
        "title": "Online Searching with Turn Cost",
        "authors": [
            "Erik D. Demaine",
            "Sandor P. Fekete",
            "Shmuel Gal"
        ],
        "summary": "We consider the problem of searching for an object on a line at an unknown distance OPT from the original position of the searcher, in the presence of a cost of d for each time the searcher changes direction. This is a generalization of the well-studied linear-search problem. We describe a strategy that is guaranteed to find the object at a cost of at most 9*OPT + 2d, which has the optimal competitive ratio 9 with respect to OPT plus the minimum corresponding additive term. Our argument for upper and lower bound uses an infinite linear program, which we solve by experimental solution of an infinite series of approximating finite linear programs, estimating the limits, and solving the resulting recurrences. We feel that this technique is interesting in its own right and should help solve other searching problems. In particular, we consider the star search or cow-path problem with turn cost, where the hidden object is placed on one of m rays emanating from the original position of the searcher. For this problem we give a tight bound of (1+(2(m^m)/((m-1)^(m-1))) OPT + m ((m/(m-1))^(m-1) - 1) d. We also discuss tradeoff between the corresponding coefficients, and briefly consider randomized strategies on the line.",
        "published": "2004-06-23T16:56:53Z",
        "link": "http://arxiv.org/abs/cs/0406045v3",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Approximation Algorithms for Minimum PCR Primer Set Selection with   Amplification Length and Uniqueness Constraints",
        "authors": [
            "K. Konwar",
            "I. Mandoiu",
            "A. Russell",
            "A. Shvartsman"
        ],
        "summary": "A critical problem in the emerging high-throughput genotyping protocols is to minimize the number of polymerase chain reaction (PCR) primers required to amplify the single nucleotide polymorphism loci of interest. In this paper we study PCR primer set selection with amplification length and uniqueness constraints from both theoretical and practical perspectives. We give a greedy algorithm that achieves a logarithmic approximation factor for the problem of minimizing the number of primers subject to a given upperbound on the length of PCR amplification products. We also give, using randomized rounding, the first non-trivial approximation algorithm for a version of the problem that requires unique amplification of each amplification target. Empirical results on randomly generated testcases as well as testcases extracted from the from the National Center for Biotechnology Information's genomic databases show that our algorithms are highly scalable and produce better results compared to previous heuristics.",
        "published": "2004-06-28T07:04:14Z",
        "link": "http://arxiv.org/abs/cs/0406053v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "q-bio.QM",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Insertion Sort is O(n log n)",
        "authors": [
            "Michael A. Bender",
            "Martin Farach-Colton",
            "Miguel Mosteiro"
        ],
        "summary": "Traditional Insertion Sort runs in O(n^2) time because each insertion takes O(n) time. When people run Insertion Sort in the physical world, they leave gaps between items to accelerate insertions. Gaps help in computers as well. This paper shows that Gapped Insertion Sort has insertion times of O(log n) with high probability, yielding a total running time of O(n log n) with high probability.",
        "published": "2004-07-01T15:50:26Z",
        "link": "http://arxiv.org/abs/cs/0407003v1",
        "categories": [
            "cs.DS",
            "E.5; F.2.2"
        ]
    },
    {
        "title": "Efficient Hashing with Lookups in two Memory Accesses",
        "authors": [
            "Rina Panigrahy"
        ],
        "summary": "The study of hashing is closely related to the analysis of balls and bins. It is well-known that instead of using a single hash function if we randomly hash a ball into two bins and place it in the smaller of the two, then this dramatically lowers the maximum load on bins. This leads to the concept of two-way hashing where the largest bucket contains $O(\\log\\log n)$ balls with high probability. The hash look up will now search in both the buckets an item hashes to. Since an item may be placed in one of two buckets, we could potentially move an item after it has been initially placed to reduce maximum load. with a maximum load of We show that by performing moves during inserts, a maximum load of 2 can be maintained on-line, with high probability, while supporting hash update operations. In fact, with $n$ buckets, even if the space for two items are pre-allocated per bucket, as may be desirable in hardware implementations, more than $n$ items can be stored giving a high memory utilization. We also analyze the trade-off between the number of moves performed during inserts and the maximum load on a bucket. By performing at most $h$ moves, we can maintain a maximum load of $O(\\frac{\\log \\log n}{h \\log(\\log\\log n/h)})$. So, even by performing one move, we achieve a better bound than by performing no moves at all.",
        "published": "2004-07-09T22:23:40Z",
        "link": "http://arxiv.org/abs/cs/0407023v1",
        "categories": [
            "cs.DS",
            "E.2"
        ]
    },
    {
        "title": "All Maximal Independent Sets and Dynamic Dominance for Sparse Graphs",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We describe algorithms, based on Avis and Fukuda's reverse search paradigm, for listing all maximal independent sets in a sparse graph in polynomial time and delay per output. For bounded degree graphs, our algorithms take constant time per set generated; for minor-closed graph families, the time is O(n) per set, and for more general sparse graph families we achieve subquadratic time per set. We also describe new data structures for maintaining a dynamic vertex set S in a sparse or minor-closed graph family, and querying the number of vertices not dominated by S; for minor-closed graph families the time per update is constant, while it is sublinear for any sparse graph family. We can also maintain a dynamic vertex set in an arbitrary m-edge graph and test the independence of the maintained set in time O(sqrt m) per update. We use the domination data structures as part of our enumeration algorithms.",
        "published": "2004-07-15T21:04:45Z",
        "link": "http://arxiv.org/abs/cs/0407036v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Communication-Aware Processor Allocation for Supercomputers",
        "authors": [
            "Michael A. Bender",
            "David P. Bunde",
            "Erik D. Demaine",
            "Sandor P. Fekete",
            "Vitus J. Leung",
            "Henk Meijer",
            "Cynthia A. Phillips"
        ],
        "summary": "This paper gives processor-allocation algorithms for minimizing the average number of communication hops between the assigned processors for grid architectures, in the presence of occupied cells. The simpler problem of assigning processors on a free grid has been studied by Karp, McKellar, and Wong who show that the solutions have nontrivial structure; they left open the complexity of the problem.   The associated clustering problem is as follows: Given n points in Re^d, find k points that minimize their average pairwise L1 distance. We present a natural approximation algorithm and show that it is a 7/4-approximation for 2D grids. For d-dimensional space, the approximation guarantee is 2-(1/2d), which is tight. We also give a polynomial-time approximation scheme (PTAS) for constant dimension d, and report on experimental results.",
        "published": "2004-07-24T13:40:26Z",
        "link": "http://arxiv.org/abs/cs/0407058v2",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; C.1.4"
        ]
    },
    {
        "title": "Multi-Embedding of Metric Spaces",
        "authors": [
            "Yair Bartal",
            "Manor Mendel"
        ],
        "summary": "Metric embedding has become a common technique in the design of algorithms. Its applicability is often dependent on how high the embedding's distortion is. For example, embedding finite metric space into trees may require linear distortion as a function of its size. Using probabilistic metric embeddings, the bound on the distortion reduces to logarithmic in the size.   We make a step in the direction of bypassing the lower bound on the distortion in terms of the size of the metric. We define \"multi-embeddings\" of metric spaces in which a point is mapped onto a set of points, while keeping the target metric of polynomial size and preserving the distortion of paths. The distortion obtained with such multi-embeddings into ultrametrics is at most O(log Delta loglog Delta) where Delta is the aspect ratio of the metric. In particular, for expander graphs, we are able to obtain constant distortion embeddings into trees in contrast with the Omega(log n) lower bound for all previous notions of embeddings.   We demonstrate the algorithmic application of the new embeddings for two optimization problems: group Steiner tree and metrical task systems.",
        "published": "2004-08-02T16:42:43Z",
        "link": "http://arxiv.org/abs/cs/0408003v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Lock-Free and Practical Deques using Single-Word Compare-And-Swap",
        "authors": [
            "Håkan Sundell",
            "Philippas Tsigas"
        ],
        "summary": "We present an efficient and practical lock-free implementation of a concurrent deque that is disjoint-parallel accessible and uses atomic primitives which are available in modern computer systems. Previously known lock-free algorithms of deques are either based on non-available atomic synchronization primitives, only implement a subset of the functionality, or are not designed for disjoint accesses. Our algorithm is based on a doubly linked list, and only requires single-word compare-and-swap atomic primitives, even for dynamic memory sizes. We have performed an empirical study using full implementations of the most efficient algorithms of lock-free deques known. For systems with low concurrency, the algorithm by Michael shows the best performance. However, as our algorithm is designed for disjoint accesses, it performs significantly better on systems with high concurrency and non-uniform memory architecture.",
        "published": "2004-08-05T14:17:01Z",
        "link": "http://arxiv.org/abs/cs/0408016v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "E.1"
        ]
    },
    {
        "title": "Incremental Construction of Minimal Acyclic Sequential Transducers from   Unsorted Data",
        "authors": [
            "Wojciech Skut"
        ],
        "summary": "This paper presents an efficient algorithm for the incremental construction of a minimal acyclic sequential transducer (ST) for a dictionary consisting of a list of input and output strings. The algorithm generalises a known method of constructing minimal finite-state automata (Daciuk et al. 2000). Unlike the algorithm published by Mihov and Maurel (2001), it does not require the input strings to be sorted. The new method is illustrated by an application to pronunciation dictionaries.",
        "published": "2004-08-10T11:09:48Z",
        "link": "http://arxiv.org/abs/cs/0408026v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "F.4.3"
        ]
    },
    {
        "title": "Medians and Beyond: New Aggregation Techniques for Sensor Networks",
        "authors": [
            "Nisheeth Shrivastava",
            "Chiranjeeb Buragohain",
            "Divyakant Agrawal",
            "Subhash Suri"
        ],
        "summary": "Wireless sensor networks offer the potential to span and monitor large geographical areas inexpensively. Sensors, however, have significant power constraint (battery life), making communication very expensive. Another important issue in the context of sensor-based information systems is that individual sensor readings are inherently unreliable. In order to address these two aspects, sensor database systems like TinyDB and Cougar enable in-network data aggregation to reduce the communication cost and improve reliability. The existing data aggregation techniques, however, are limited to relatively simple types of queries such as SUM, COUNT, AVG, and MIN/MAX. In this paper we propose a data aggregation scheme that significantly extends the class of queries that can be answered using sensor networks. These queries include (approximate) quantiles, such as the median, the most frequent data values, such as the consensus value, a histogram of the data distribution, as well as range queries. In our scheme, each sensor aggregates the data it has received from other sensors into a fixed (user specified) size message. We provide strict theoretical guarantees on the approximation quality of the queries in terms of the message size. We evaluate the performance of our aggregation scheme by simulation and demonstrate its accuracy, scalability and low resource utilization for highly variable input data sets.",
        "published": "2004-08-17T02:21:06Z",
        "link": "http://arxiv.org/abs/cs/0408039v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Hash sort: A linear time complexity multiple-dimensional sort algorithm",
        "authors": [
            "William F. Gilreath"
        ],
        "summary": "Sorting and hashing are two completely different concepts in computer science, and appear mutually exclusive to one another. Hashing is a search method using the data as a key to map to the location within memory, and is used for rapid storage and retrieval. Sorting is a process of organizing data from a random permutation into an ordered arrangement, and is a common activity performed frequently in a variety of applications.   Almost all conventional sorting algorithms work by comparison, and in doing so have a linearithmic greatest lower bound on the algorithmic time complexity. Any improvement in the theoretical time complexity of a sorting algorithm can result in overall larger gains in implementation performance.. A gain in algorithmic performance leads to much larger gains in speed for the application that uses the sort algorithm. Such a sort algorithm needs to use an alternative method for ordering the data than comparison, to exceed the linearithmic time complexity boundary on algorithmic performance.   The hash sort is a general purpose non-comparison based sorting algorithm by hashing, which has some interesting features not found in conventional sorting algorithms. The hash sort asymptotically outperforms the fastest traditional sorting algorithm, the quick sort. The hash sort algorithm has a linear time complexity factor -- even in the worst case. The hash sort opens an area for further work and investigation into alternative means of sorting.",
        "published": "2004-08-17T09:23:35Z",
        "link": "http://arxiv.org/abs/cs/0408040v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "CrocoPat 2.1 Introduction and Reference Manual",
        "authors": [
            "Dirk Beyer",
            "Andreas Noack"
        ],
        "summary": "CrocoPat is an efficient, powerful and easy-to-use tool for manipulating relations of arbitrary arity, including directed graphs. This manual provides an introduction to and a reference for CrocoPat and its programming language RML. It includes several application examples, in particular from the analysis of structural models of software systems.",
        "published": "2004-09-07T09:44:18Z",
        "link": "http://arxiv.org/abs/cs/0409009v1",
        "categories": [
            "cs.PL",
            "cs.DM",
            "cs.DS",
            "cs.SE",
            "D.1.6; G.2.2.a; E.1.d; D.2.7m"
        ]
    },
    {
        "title": "Locally connected spanning trees on graphs",
        "authors": [
            "Ching-Chi Lin",
            "Gerard J. Chang",
            "Gen-Huey Chen"
        ],
        "summary": "A locally connected spanning tree of a graph $G$ is a spanning tree $T$ of $G$ such that the set of all neighbors of $v$ in $T$ induces a connected subgraph of $G$ for every $v\\in V(G)$. The purpose of this paper is to give linear-time algorithms for finding locally connected spanning trees on strongly chordal graphs and proper circular-arc graphs, respectively.",
        "published": "2004-09-08T09:08:18Z",
        "link": "http://arxiv.org/abs/cs/0409013v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Using a hierarchy of Domain Specific Languages in complex software   systems design",
        "authors": [
            "V. S. Lugovsky"
        ],
        "summary": "A new design methodology is introduced, with some examples on building Domain Specific Languages hierarchy on top of Scheme.",
        "published": "2004-09-09T01:44:05Z",
        "link": "http://arxiv.org/abs/cs/0409016v1",
        "categories": [
            "cs.PL",
            "cs.DS",
            "cs.SE",
            "D.1.1;I.2.2;D.3.2;D.2.10"
        ]
    },
    {
        "title": "Near Optimal Routing for Small-World Networks with Augmented Local   Awareness",
        "authors": [
            "Jianyang Zeng",
            "Wen-Jing Hsu",
            "Jiangdian Wang"
        ],
        "summary": "In order to investigate the routing aspects of small-world networks, Kleinberg proposes a network model based on a $d$-dimensional lattice with long-range links chosen at random according to the $d$-harmonic distribution. Kleinberg shows that the greedy routing algorithm by using only local information performs in $O(\\log^2 n)$ expected number of hops, where $n$ denotes the number of nodes in the network. Martel and Nguyen have found that the expected diameter of Kleinberg's small-world networks is $\\Theta(\\log n)$. Thus a question arises naturally: Can we improve the routing algorithms to match the diameter of the networks while keeping the amount of information stored on each node as small as possible? We extend Kleinberg's model and add three augmented local links for each node: two of which are connected to nodes chosen randomly and uniformly within $\\log^2 n$ Mahattan distance, and the third one is connected to a node chosen randomly and uniformly within $\\log n$ Mahattan distance. We show that if each node is aware of $O(\\log n)$ number of neighbors via the augmented local links, there exist both non-oblivious and oblivious algorithms that can route messages between any pair of nodes in $O(\\log n \\log \\log n)$ expected number of hops, which is a near optimal routing complexity and outperforms the other related results for routing in Kleinberg's small-world networks. Our schemes keep only $O(\\log^2 n)$ bits of routing information on each node, thus they are scalable with the network size. Besides adding new light to the studies of social networks, our results may also find applications in the design of large-scale distributed networks, such as peer-to-peer systems, in the same spirit of Symphony.",
        "published": "2004-09-09T03:41:48Z",
        "link": "http://arxiv.org/abs/cs/0409017v3",
        "categories": [
            "cs.DM",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Fast Construction of Nets in Low Dimensional Metrics, and Their   Applications",
        "authors": [
            "Sariel Har-Peled",
            "Manor Mendel"
        ],
        "summary": "We present a near linear time algorithm for constructing hierarchical nets in finite metric spaces with constant doubling dimension. This data-structure is then applied to obtain improved algorithms for the following problems: Approximate nearest neighbor search, well-separated pair decomposition, compact representation scheme, doubling measure, and computation of the (approximate) Lipschitz constant of a function. In all cases, the running (preprocessing) time is near-linear and the space being used is linear.",
        "published": "2004-09-29T17:44:15Z",
        "link": "http://arxiv.org/abs/cs/0409057v3",
        "categories": [
            "cs.DS",
            "cs.CG"
        ]
    },
    {
        "title": "Fibonacci connection between Huffman codes and Wythoff array",
        "authors": [
            "Alex Vinokur"
        ],
        "summary": "Fibonacci connection between non-decreasing sequences of positive integers producing maximum height Huffman trees and the Wythoff array has been proved.",
        "published": "2004-10-06T11:44:02Z",
        "link": "http://arxiv.org/abs/cs/0410013v2",
        "categories": [
            "cs.DM",
            "cs.DS",
            "math.CO",
            "math.NT",
            "E.1; E.4; G.2.2; H.1.1"
        ]
    },
    {
        "title": "Automated Pattern Detection--An Algorithm for Constructing Optimally   Synchronizing Multi-Regular Language Filters",
        "authors": [
            "Carl S. McTague",
            "James P. Crutchfield"
        ],
        "summary": "In the computational-mechanics structural analysis of one-dimensional cellular automata the following automata-theoretic analogue of the \\emph{change-point problem} from time series analysis arises: \\emph{Given a string $\\sigma$ and a collection $\\{\\mc{D}_i\\}$ of finite automata, identify the regions of $\\sigma$ that belong to each $\\mc{D}_i$ and, in particular, the boundaries separating them.} We present two methods for solving this \\emph{multi-regular language filtering problem}. The first, although providing the ideal solution, requires a stack, has a worst-case compute time that grows quadratically in $\\sigma$'s length and conditions its output at any point on arbitrarily long windows of future input. The second method is to algorithmically construct a transducer that approximates the first algorithm. In contrast to the stack-based algorithm, however, the transducer requires only a finite amount of memory, runs in linear time, and gives immediate output for each letter read; it is, moreover, the best possible finite-state approximation with these three features.",
        "published": "2004-10-07T17:20:56Z",
        "link": "http://arxiv.org/abs/cs/0410017v1",
        "categories": [
            "cs.CV",
            "cond-mat.stat-mech",
            "cs.CL",
            "cs.DS",
            "cs.IR",
            "cs.LG",
            "nlin.AO",
            "nlin.CG",
            "nlin.PS",
            "physics.comp-ph",
            "q-bio.GN"
        ]
    },
    {
        "title": "Generating All Maximal Induced Subgraphs for Hereditary,   Connected-Hereditary and Rooted-Hereditary Properties",
        "authors": [
            "Sara Cohen",
            "Yehoshua Sagiv"
        ],
        "summary": "The problem of computing all maximal induced subgraphs of a graph G that have a graph property P, also called the maximal P-subgraphs problem, is considered. This problem is studied for hereditary, connected-hereditary and rooted-hereditary graph properties. The maximal P-subgraphs problem is reduced to restricted versions of this problem by providing algorithms that solve the general problem, assuming that an algorithm for a restricted version is given. The complexity of the algorithms are analyzed in terms of total polynomial time, incremental polynomial time and the complexity class P-enumerable. The general results presented allow simple proofs that the maximal P-subgraphs problem can be solved efficiently (in terms of the input and output) for many different properties.",
        "published": "2004-10-17T20:30:43Z",
        "link": "http://arxiv.org/abs/cs/0410039v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.2.2; F.2; F.1.3"
        ]
    },
    {
        "title": "A Note on Scheduling Equal-Length Jobs to Maximize Throughput",
        "authors": [
            "Marek Chrobak",
            "Christoph Durr",
            "Wojciech Jawor",
            "Lukasz Kowalik",
            "Maciej Kurowski"
        ],
        "summary": "We study the problem of scheduling equal-length jobs with release times and deadlines, where the objective is to maximize the number of completed jobs. Preemptions are not allowed. In Graham's notation, the problem is described as 1|r_j;p_j=p|\\sum U_j. We give the following results: (1) We show that the often cited algorithm by Carlier from 1981 is not correct. (2) We give an algorithm for this problem with running time O(n^5).",
        "published": "2004-10-18T22:41:30Z",
        "link": "http://arxiv.org/abs/cs/0410046v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Worst-Case Optimal Tree Layout in External Memory",
        "authors": [
            "Erik D. Demaine",
            "John Iacono",
            "Stefan Langerman"
        ],
        "summary": "Consider laying out a fixed-topology tree of N nodes into external memory with block size B so as to minimize the worst-case number of block memory transfers required to traverse a path from the root to a node of depth D. We prove that the optimal number of memory transfers is $$ \\cases{   \\displaystyle   \\Theta\\left( {D \\over \\lg (1{+}B)} \\right)   & when $D = O(\\lg N)$, \\cr   \\displaystyle   \\Theta\\left( {\\lg N \\over \\lg \\left(1{+}{B \\lg N \\over D}\\right)} \\right)   & when $D = \\Omega(\\lg N)$ and $D = O(B \\lg N)$, \\cr   \\displaystyle   \\Theta\\left( {D \\over B} \\right)   & when $D = \\Omega(B \\lg N)$.   } $$",
        "published": "2004-10-19T15:17:57Z",
        "link": "http://arxiv.org/abs/cs/0410048v4",
        "categories": [
            "cs.DS",
            "F.2.2; E.1"
        ]
    },
    {
        "title": "Understanding Search Trees via Statistical Physics",
        "authors": [
            "Satya N. Majumdar",
            "David S. Dean",
            "P. L. Krapivsky"
        ],
        "summary": "We study the random m-ary search tree model (where m stands for the number of branches of a search tree), an important problem for data storage in computer science, using a variety of statistical physics techniques that allow us to obtain exact asymptotic results. In particular, we show that the probability distributions of extreme observables associated with a random search tree such as the height and the balanced height of a tree have a traveling front structure. In addition, the variance of the number of nodes needed to store a data string of a given size N is shown to undergo a striking phase transition at a critical value of the branching ratio m_c=26. We identify the mechanism of this phase transition, show that it is generic and occurs in various other problems as well. New results are obtained when each element of the data string is a D-dimensional vector. We show that this problem also has a phase transition at a critical dimension, D_c= \\pi/\\sin^{-1}(1/\\sqrt{8})=8.69363...",
        "published": "2004-10-20T07:56:04Z",
        "link": "http://arxiv.org/abs/cond-mat/0410498v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.DS"
        ]
    },
    {
        "title": "The Schreier-Sims algorithm for matrix groups",
        "authors": [
            "Henrik Bäärnhielm"
        ],
        "summary": "This is the report of a project with the aim to make a new implementation of the Schreier-Sims algorithm in GAP, specialized for matrix groups. The standard Schreier-Sims algorithm is described in some detail, followed by descriptions of the probabilistic Schreier-Sims algorithm and the Schreier-Todd-Coxeter-Sims algorithm. Then we discuss our implementation and some optimisations, and finally we report on the performance of our implementation, as compared to the existing implementation in GAP, and we give benchmark results. The conclusion is that our implementation in some cases is faster and consumes much less memory.",
        "published": "2004-10-28T01:28:52Z",
        "link": "http://arxiv.org/abs/math/0410593v1",
        "categories": [
            "math.GR",
            "cs.DS",
            "20-04, 20G40, 20H30 (Primary) 68Q25, 68W20, 20B40 (Secondary)"
        ]
    },
    {
        "title": "Why Delannoy numbers?",
        "authors": [
            "Cyril Banderier",
            "Sylviane Schwer"
        ],
        "summary": "This article is not a research paper, but a little note on the history of combinatorics: We present here a tentative short biography of Henri Delannoy, and a survey of his most notable works. This answers to the question raised in the title, as these works are related to lattice paths enumeration, to the so-called Delannoy numbers, and were the first general way to solve Ballot-like problems. These numbers appear in probabilistic game theory, alignments of DNA sequences, tiling problems, temporal representation models, analysis of algorithms and combinatorial structures.",
        "published": "2004-11-06T07:40:07Z",
        "link": "http://arxiv.org/abs/math/0411128v1",
        "categories": [
            "math.CO",
            "cs.DS",
            "cs.GT",
            "math.HO",
            "math.PR",
            "math.ST",
            "q-bio.GN",
            "stat.TH"
        ]
    },
    {
        "title": "Generating Functions For Kernels of Digraphs (Enumeration & Asymptotics   for Nim Games)",
        "authors": [
            "Cyril Banderier",
            "Jean-Marie Le Bars",
            "Vlady Ravelomanana"
        ],
        "summary": "In this article, we study directed graphs (digraphs) with a coloring constraint due to Von Neumann and related to Nim-type games. This is equivalent to the notion of kernels of digraphs, which appears in numerous fields of research such as game theory, complexity theory, artificial intelligence (default logic, argumentation in multi-agent systems), 0-1 laws in monadic second order logic, combinatorics (perfect graphs)... Kernels of digraphs lead to numerous difficult questions (in the sense of NP-completeness, #P-completeness). However, we show here that it is possible to use a generating function approach to get new informations: we use technique of symbolic and analytic combinatorics (generating functions and their singularities) in order to get exact and asymptotic results, e.g. for the existence of a kernel in a circuit or in a unicircuit digraph. This is a first step toward a generatingfunctionology treatment of kernels, while using, e.g., an approach \"a la Wright\". Our method could be applied to more general \"local coloring constraints\" in decomposable combinatorial structures.",
        "published": "2004-11-06T20:33:39Z",
        "link": "http://arxiv.org/abs/math/0411138v1",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS",
            "cs.GT",
            "math.PR"
        ]
    },
    {
        "title": "Extremal Properties of Three Dimensional Sensor Networks with   Applications",
        "authors": [
            "Vlady Ravelomanana"
        ],
        "summary": "In this paper, we analyze various critical transmitting/sensing ranges for connectivity and coverage in three-dimensional sensor networks. As in other large-scale complex systems, many global parameters of sensor networks undergo phase transitions: For a given property of the network, there is a critical threshold, corresponding to the minimum amount of the communication effort or power expenditure by individual nodes, above (resp. below) which the property exists with high (resp. a low) probability. For sensor networks, properties of interest include simple and multiple degrees of connectivity/coverage. First, we investigate the network topology according to the region of deployment, the number of deployed sensors and their transmitting/sensing ranges. More specifically, we consider the following problems: Assume that $n$ nodes, each capable of sensing events within a radius of $r$, are randomly and uniformly distributed in a 3-dimensional region $\\mathcal{R}$ of volume $V$, how large must the sensing range be to ensure a given degree of coverage of the region to monitor? For a given transmission range, what is the minimum (resp. maximum) degree of the network? What is then the typical hop-diameter of the underlying network? Next, we show how these results affect algorithmic aspects of the network by designing specific distributed protocols for sensor networks.",
        "published": "2004-11-10T09:10:34Z",
        "link": "http://arxiv.org/abs/cs/0411027v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.DM",
            "ACM classification: C.2.1 Network architecture and design; F.2.2\n  Nonnumerical algorithms and problems; G.3 Probability and statistics"
        ]
    },
    {
        "title": "Generating functions for generating trees",
        "authors": [
            "Cyril Banderier",
            "Philippe Flajolet",
            "Daniele Gardy",
            "Mireille Bousquet-Melou",
            "Alain Denise",
            "Dominique Gouyou-Beauchamps"
        ],
        "summary": "Certain families of combinatorial objects admit recursive descriptions in terms of generating trees: each node of the tree corresponds to an object, and the branch leading to the node encodes the choices made in the construction of the object. Generating trees lead to a fast computation of enumeration sequences (sometimes, to explicit formulae as well) and provide efficient random generation algorithms. We investigate the links between the structural properties of the rewriting rules defining such trees and the rationality, algebraicity, or transcendence of the corresponding generating function.",
        "published": "2004-11-11T09:19:47Z",
        "link": "http://arxiv.org/abs/math/0411250v1",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Lower-Stretch Spanning Trees",
        "authors": [
            "Michael Elkin",
            "Yuval Emek",
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We prove that every weighted graph contains a spanning tree subgraph of average stretch O((log n log log n)^2). Moreover, we show how to construct such a tree in time O(m log^2 n).",
        "published": "2004-11-17T22:07:46Z",
        "link": "http://arxiv.org/abs/cs/0411064v5",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Forbidden Subgraphs in Connected Graphs",
        "authors": [
            "Vlady Ravelomanana",
            "Loys Thimonier"
        ],
        "summary": "Given a set $\\xi=\\{H_1,H_2,...\\}$ of connected non acyclic graphs, a $\\xi$-free graph is one which does not contain any member of $% \\xi$ as copy. Define the excess of a graph as the difference between its number of edges and its number of vertices. Let ${\\gr{W}}_{k,\\xi}$ be theexponential generating function (EGF for brief) of connected $\\xi$-free graphs of excess equal to $k$ ($k \\geq 1$). For each fixed $\\xi$, a fundamental differential recurrence satisfied by the EGFs ${\\gr{W}}_{k,\\xi}$ is derived. We give methods on how to solve this nonlinear recurrence for the first few values of $k$ by means of graph surgery. We also show that for any finite collection $\\xi$ of non-acyclic graphs, the EGFs ${\\gr{W}}_{k,\\xi}$ are always rational functions of the generating function, $T$, of Cayley's rooted (non-planar) labelled trees. From this, we prove that almost all connected graphs with $n$ nodes and $n+k$ edges are $\\xi$-free, whenever $k=o(n^{1/3})$ and $|\\xi| < \\infty$ by means of Wright's inequalities and saddle point method. Limiting distributions are derived for sparse connected $\\xi$-free components that are present when a random graph on $n$ nodes has approximately $\\frac{n}{2}$ edges. In particular, the probability distribution that it consists of trees, unicyclic components, $...$, $(q+1)$-cyclic components all $\\xi$-free is derived. Similar results are also obtained for multigraphs, which are graphs where self-loops and multiple-edges are allowed.",
        "published": "2004-11-25T09:32:25Z",
        "link": "http://arxiv.org/abs/cs/0411093v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "math.CO",
            "ACM Classification: G.2.1 Combinatorics G.2.2 Graph Theory General\n  Terms: Algorithms, Theory"
        ]
    },
    {
        "title": "Embeddings into the Pancake Interconnection Network",
        "authors": [
            "Christian Lavault"
        ],
        "summary": "Owing to its nice properties, the pancake is one of the Cayley graphs that were proposed as alternatives to the hypercube for interconnecting processors in parallel computers. In this paper, we present embeddings of rings, grids and hypercubes into the pancake with constant dilation and congestion. We also extend the results to similar efficient embeddings into the star graph.",
        "published": "2004-11-26T20:13:10Z",
        "link": "http://arxiv.org/abs/cs/0411095v1",
        "categories": [
            "cs.DC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Finding Approximate Palindromes in Strings Quickly and Simply",
        "authors": [
            "L. Allison"
        ],
        "summary": "Described are two algorithms to find long approximate palindromes in a string, for example a DNA sequence. A simple algorithm requires O(n)-space and almost always runs in $O(k.n)$-time where n is the length of the string and k is the number of ``errors'' allowed in the palindrome. Its worst-case time-complexity is $O(n^2)$ but this does not occur with real biological sequences. A more complex algorithm guarantees $O(k.n)$ worst-case time complexity.",
        "published": "2004-12-01T17:08:55Z",
        "link": "http://arxiv.org/abs/cs/0412004v1",
        "categories": [
            "cs.DS",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "The Accelerated Euclidean Algorithm",
        "authors": [
            "Sidi Mohamed Sedjelmaci"
        ],
        "summary": "We present a new GCD algorithm of two integers or polynomials. The algorithm is iterative and its time complexity is still $O(n \\\\log^2 n ~ log \\\\log n)$ for $n$-bit inputs.",
        "published": "2004-12-02T15:01:39Z",
        "link": "http://arxiv.org/abs/cs/0412006v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Measured descent: A new embedding method for finite metrics",
        "authors": [
            "Robert Krauthgamer",
            "James R. Lee",
            "Manor Mendel",
            "Assaf Naor"
        ],
        "summary": "We devise a new embedding technique, which we call measured descent, based on decomposing a metric space locally, at varying speeds, according to the density of some probability measure. This provides a refined and unified framework for the two primary methods of constructing Frechet embeddings for finite metrics, due to [Bourgain, 1985] and [Rao, 1999]. We prove that any n-point metric space (X,d) embeds in Hilbert space with distortion O(sqrt{alpha_X log n}), where alpha_X is a geometric estimate on the decomposability of X. As an immediate corollary, we obtain an O(sqrt{(log lambda_X) \\log n}) distortion embedding, where \\lambda_X is the doubling constant of X. Since \\lambda_X\\le n, this result recovers Bourgain's theorem, but when the metric X is, in a sense, ``low-dimensional,'' improved bounds are achieved.   Our embeddings are volume-respecting for subsets of arbitrary size. One consequence is the existence of (k, O(log n)) volume-respecting embeddings for all 1 \\leq k \\leq n, which is the best possible, and answers positively a question posed by U. Feige. Our techniques are also used to answer positively a question of Y. Rabinovich, showing that any weighted n-point planar graph embeds in l_\\infty^{O(log n)} with O(1) distortion. The O(log n) bound on the dimension is optimal, and improves upon the previously known bound of O((log n)^2).",
        "published": "2004-12-02T17:06:41Z",
        "link": "http://arxiv.org/abs/cs/0412008v2",
        "categories": [
            "cs.DS",
            "math.MG"
        ]
    },
    {
        "title": "Efficient Quantum Algorithms for the Hidden Subgroup Problem over a   Class of Semi-direct Product Groups",
        "authors": [
            "Yoshifumi Inui",
            "Francois Le Gall"
        ],
        "summary": "In this paper, we consider the hidden subgroup problem (HSP) over the class of semi-direct product groups $\\mathbb{Z}_{p^r}\\rtimes\\mathbb{Z}_q$, for p and q prime. We first present a classification of these groups in five classes. Then, we describe a polynomial-time quantum algorithm solving the HSP over all the groups of one of these classes: the groups of the form $\\mathbb{Z}_{p^r}\\rtimes\\mathbb{Z}_p$, where p is an odd prime. Our algorithm works even in the most general case where the group is presented as a black-box group with not necessarily unique encoding. Finally, we extend this result and present an efficient algorithm solving the HSP over the groups $\\mathbb{Z}^m_{p^r}\\rtimes\\mathbb{Z}_p$.",
        "published": "2004-12-04T13:55:28Z",
        "link": "http://arxiv.org/abs/quant-ph/0412033v3",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "The modular technology of development of the CAD expansions: profiles of   outside networks of water supply and water drain",
        "authors": [
            "Vladimir V. Migunov",
            "Rustem R. Kafiyatullov",
            "Ilsur T. Safin"
        ],
        "summary": "The modular technology of development of the problem-oriented CAD expansions is applied to a task of designing of profiles of outside networks of water supply and water drain with realization in program system TechnoCAD GlassX. The unity of structure of this profiles is revealed, the system model of the drawings of profiles of networks is developed including the structured parametric representation (properties of objects and their interdependence, general settings and default settings) and operations with it, which efficiently automate designing",
        "published": "2004-12-08T08:42:53Z",
        "link": "http://arxiv.org/abs/cs/0412029v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The modular technology of development of the CAD expansions: protection   of the buildings from the lightning",
        "authors": [
            "Vladimir V. Migunov",
            "Rustem R. Kafiyatullov",
            "Ilsur T. Safin"
        ],
        "summary": "The modular technology of development of the problem-oriented CAD expansions is applied to a task of designing of protection of the buildings from the lightning with realization in program system TechnoCAD GlassX. The system model of the drawings of lightning protection is developed including the structured parametric representation (properties of objects and their interdependence, general settings and default settings) and operations with it, which efficiently automate designing",
        "published": "2004-12-08T08:49:08Z",
        "link": "http://arxiv.org/abs/cs/0412030v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The methods of support of the requirements of the Russian standards at   development of a CAD of industrial objects",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The methods of support of the requirements of the Russian standards in a CAD of industrial objects are explained, which were implemented in the CAD system TechnoCAD GlassX with an own graphics core and own structures of data storage. It is rotined, that the binding of storage structures and program code of a CAD to the requirements of standards enable not only to fulfil these requirements in project documentation, but also to increase a degree of compactness of storage of drawings both on the disk and in the RAM",
        "published": "2004-12-08T08:57:49Z",
        "link": "http://arxiv.org/abs/cs/0412032v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "A Social Network for Societal-Scale Decision-Making Systems",
        "authors": [
            "Marko Rodriguez",
            "Daniel Steinbock"
        ],
        "summary": "In societal-scale decision-making systems the collective is faced with the problem of ensuring that the derived group decision is in accord with the collective's intention. In modern systems, political institutions have instatiated representative forms of decision-making to ensure that every individual in the society has a participatory voice in the decision-making behavior of the whole--even if only indirectly through representation. An agent-based simulation demonstrates that in modern representative systems, as the ratio of representatives increases, there exists an exponential decrease in the ability for the group to behave in accord with the desires of the whole. To remedy this issue, this paper provides a novel representative power structure for decision-making that utilizes a social network and power distribution algorithm to maintain the collective's perspective over varying degrees of participation and/or ratios of representation. This work shows promise for the future development of policy-making systems that are supported by the computer and network infrastructure of our society.",
        "published": "2004-12-11T00:32:51Z",
        "link": "http://arxiv.org/abs/cs/0412047v1",
        "categories": [
            "cs.CY",
            "cs.DS",
            "cs.HC",
            "H.4.2; J.7; K.4.m"
        ]
    },
    {
        "title": "Evolving Categories: Consistent Framework for Representation of Data and   Algorithms",
        "authors": [
            "Evgeny Yanenko"
        ],
        "summary": "A concept of \"evolving categories\" is suggested to build a simple, scalable, mathematically consistent framework for representing in uniform way both data and algorithms. A state machine for executing algorithms becomes clear, rich and powerful semantics, based on category theory, and still allows easy implementation. Moreover, it gives an original insight into the nature and semantics of algorithms.",
        "published": "2004-12-17T22:58:13Z",
        "link": "http://arxiv.org/abs/cs/0412089v1",
        "categories": [
            "cs.DS",
            "E.1; F.1.1; F.4.1; I.1.1"
        ]
    },
    {
        "title": "Preemptive Multi-Machine Scheduling of Equal-Length Jobs to Minimize the   Average Flow Time",
        "authors": [
            "Philippe Baptiste",
            "Marek Chrobak",
            "Christoph Durr",
            "Francis Sourd"
        ],
        "summary": "We study the problem of preemptive scheduling of n equal-length jobs with given release times on m identical parallel machines. The objective is to minimize the average flow time. Recently, Brucker and Kravchenko proved that the optimal schedule can be computed in polynomial time by solving a linear program with O(n^3) variables and constraints, followed by some substantial post-processing (where n is the number of jobs.) In this note we describe a simple linear program with only O(mn) variables and constraints. Our linear program produces directly the optimal schedule and does not require any post-processing.",
        "published": "2004-12-20T16:15:59Z",
        "link": "http://arxiv.org/abs/cs/0412094v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Formal Test Purposes and The Validity of Test Cases",
        "authors": [
            "Peter H. Deussen",
            "Stephan Tobies"
        ],
        "summary": "We give a formalization of the notion of test purpose based on (suitably restricted) Message Sequence Charts. We define the validity of test cases with respect to such a formal test purpose and provide a simple decision procedure for validity.",
        "published": "2004-12-22T08:53:49Z",
        "link": "http://arxiv.org/abs/cs/0412100v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A Monte Carlo algorithm for efficient large matrix inversion",
        "authors": [
            "L. A. Garcia-Cortes",
            "C. Cabrillo"
        ],
        "summary": "This paper introduces a new Monte Carlo algorithm to invert large matrices. It is based on simultaneous coupled draws from two random vectors whose covariance is the required inverse. It can be considered a generalization of a previously reported algorithm for hermitian matrices inversion based in only one draw. The use of two draws allows the inversion on non-hermitian matrices. Both the conditions for convergence and the rate of convergence are similar to the Gauss-Seidel algorithm. Results on two examples are presented, a real non-symmetric matrix related to quantitative genetics and a complex non-hermitian matrix relevant for physicists. Compared with other Monte Carlo algorithms it reveals a large reduction of the processing time showing eight times faster processing in the examples studied.",
        "published": "2004-12-23T17:01:14Z",
        "link": "http://arxiv.org/abs/cs/0412107v2",
        "categories": [
            "cs.DS",
            "cs.NA",
            "hep-lat"
        ]
    },
    {
        "title": "Initial Experiences Re-Exporting Duplicate and Similarity Computation   with an OAI-PMH aggregator",
        "authors": [
            "Terry L. Harrison",
            "Aravind Elango",
            "Johan Bollen",
            "Michael Nelson"
        ],
        "summary": "The proliferation of the Open Archive Initiative Protocol for Metadata Harvesting (OAI-PMH) has resulted in the creation of a large number of service providers, all harvesting from either data providers or aggregators. If data were available regarding the similarity of metadata records, service providers could track redundant records across harvests from multiple sources as well as provide additional end-user services. Due to the large number of metadata formats and the diverse mapping strategies employed by data providers, similarity calculation requirements necessitate the use of information retrieval strategies. We describe an OAI-PMH aggregator implementation that uses the optional ``<about>'' container to re-export the results of similarity calculations. Metadata records (3751) were harvested from a NASA data provider and similarities for the records were computed. The results were useful for detecting duplicates, similarities and metadata errors.",
        "published": "2004-01-05T05:41:12Z",
        "link": "http://arxiv.org/abs/cs/0401001v1",
        "categories": [
            "cs.DL",
            "cs.DS",
            "H.3.7;H.3.3"
        ]
    },
    {
        "title": "Design of a Community-based Translation Center",
        "authors": [
            "K. McDevitt",
            "M. A. Pérez-Quiñones",
            "O. I. Padilla-Falto"
        ],
        "summary": "Interfaces that support multi-lingual content can reach a broader community. We wish to extend the reach of CITIDEL, a digital library for computing education materials, to support multiple languages. By doing so, we hope that it will increase the number of users, and in turn the number of resources. This paper discusses three approaches to translation (automated translation, developer-based, and community-based), and a brief evaluation of these approaches. It proposes a design for an online community translation center where volunteers help translate interface components and educational materials available in CITIDEL.",
        "published": "2004-01-12T18:45:47Z",
        "link": "http://arxiv.org/abs/cs/0401007v1",
        "categories": [
            "cs.HC",
            "cs.DL",
            "H.3.7; J.5; H.1.2"
        ]
    },
    {
        "title": "Automated Resolution of Noisy Bibliographic References",
        "authors": [
            "Markus Demleitner",
            "Michael Kurtz",
            "Alberto Accomazzi",
            "Günther Eichhorn",
            "Carolyn S. Grant",
            "Steven S. Murray"
        ],
        "summary": "We describe a system used by the NASA Astrophysics Data System to identify bibliographic references obtained from scanned article pages by OCR methods with records in a bibliographic database. We analyze the process generating the noisy references and conclude that the three-step procedure of correcting the OCR results, parsing the corrected string and matching it against the database provides unsatisfactory results. Instead, we propose a method that allows a controlled merging of correction, parsing and matching, inspired by dependency grammars. We also report on the effectiveness of various heuristics that we have employed to improve recall.",
        "published": "2004-01-27T10:09:54Z",
        "link": "http://arxiv.org/abs/cs/0401028v1",
        "categories": [
            "cs.DL",
            "H.3.7; H.3.2"
        ]
    },
    {
        "title": "Dynamic Linking of Smart Digital Objects Based on User Navigation   Patterns",
        "authors": [
            "Aravind Elango",
            "Johan Bollen",
            "Michael L. Nelson"
        ],
        "summary": "We discuss a methodology to dynamically generate links among digital objects by means of an unsupervised learning mechanism which analyzes user link traversal patterns. We performed an experiment with a test bed of 150 complex data objects, referred to as buckets. Each bucket manages its own content, provides methods to interact with users and individually maintains a set of links to other buckets. We demonstrate that buckets were capable of dynamically adjusting their links to other buckets according to user link selections, thereby generating a meaningful network of bucket relations. Our results indicate such adaptive networks of linked buckets approximate the collective link preferences of a community of user",
        "published": "2004-01-28T00:45:53Z",
        "link": "http://arxiv.org/abs/cs/0401029v1",
        "categories": [
            "cs.DL",
            "H.3.7.1"
        ]
    },
    {
        "title": "Automatically Generating Interfaces for Personalized Interaction with   Digital Libraries",
        "authors": [
            "Saverio Perugini",
            "Naren Ramakrishnan",
            "Edward A. Fox"
        ],
        "summary": "We present an approach to automatically generate interfaces supporting personalized interaction with digital libraries; these interfaces augment the user-DL dialog by empowering the user to (optionally) supply out-of-turn information during an interaction, flatten or restructure the dialog, and enquire about dialog options. Interfaces generated using this approach for CITIDEL are described.",
        "published": "2004-02-12T03:00:14Z",
        "link": "http://arxiv.org/abs/cs/0402022v1",
        "categories": [
            "cs.DL",
            "cs.HC",
            "H.3.7 [Digital Libraries]: User Issues; H.5.2 [User Interfaces]:\n  Graphical user interfaces, Interaction styles; H.5.4 [Hypertext/Hypermedia]:\n  Navigation"
        ]
    },
    {
        "title": "Domain resource integration system",
        "authors": [
            "Wang Liang",
            "Guo Yi-Ping",
            "Fang Ming"
        ],
        "summary": "Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is a hierarchical distributed Internet information retrieval system. This system will solve some bottleneck problems such as long update interval, poor coverage in current web search system. DRIS will build the information retrieval infrastructure of Internet, but not a commercial search engine. The protocol series of DRIS are also detailed in this paper.",
        "published": "2004-03-23T09:30:37Z",
        "link": "http://arxiv.org/abs/cs/0403036v1",
        "categories": [
            "cs.NI",
            "cs.DL",
            "H.3.3;H.3.7;C.2.2"
        ]
    },
    {
        "title": "Numerical simulations of mixed states quantum computation",
        "authors": [
            "P. Gawron",
            "J. A. Miszczak"
        ],
        "summary": "We describe quantum-octave package of functions useful for simulations of quantum algorithms and protocols. Presented package allows to perform simulations with mixed states. We present numerical implementation of important quantum mechanical operations - partial trace and partial transpose. Those operations are used as building blocks of algorithms for analysis of entanglement and quantum error correction codes. Simulation of Shor's algorithm is presented as an example of package capabilities.",
        "published": "2004-06-29T08:23:07Z",
        "link": "http://arxiv.org/abs/quant-ph/0406211v1",
        "categories": [
            "quant-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Providing Authentic Long-term Archival Access to Complex Relational Data",
        "authors": [
            "Stephan Heuscher",
            "Stephan Jaermann",
            "Peter Keller-Marxer",
            "Frank Moehle"
        ],
        "summary": "We discuss long-term preservation of and access to relational databases. The focus is on national archives and science data archives which have to ingest and integrate data from a broad spectrum of vendor-specific relational database management systems (RDBMS). Furthermore, we present our solution SIARD which analyzes and extracts data and data logic from almost any RDBMS. It enables, to a reasonable level of authenticity, complete detachment of databases from their vendor-specific environment. The user can add archival descriptive metadata according to a customizable schema. A SIARD database archive integrates data, data logic, technical metadata, and archival descriptive information in one archival information package, independent of any specific software and hardware, based upon plain text files and the standardized languages SQL and XML. For usage purposes, a SIARD archive can be reloaded into any current or future RDBMS which supports standard SQL. In addition, SIARD contains a client that enables 'on demand' reload of archives into a target RDBMS, and multi-user remote access for querying and browsing the data together with its technical and descriptive metadata in one graphical user interface.",
        "published": "2004-08-24T07:01:29Z",
        "link": "http://arxiv.org/abs/cs/0408054v1",
        "categories": [
            "cs.DL",
            "cs.DB"
        ]
    },
    {
        "title": "The Building of Online Communities: An approach for learning   organizations, with a particular focus on the museum sector",
        "authors": [
            "Alpay Beler",
            "Ann Borda",
            "Jonathan P. Bowen",
            "Silvia Filippini-Fantoni"
        ],
        "summary": "This paper considers the move toward and potential of building online communities, with a particular focus on the museum sector. For instance, the increase in the use of `personalized' toolkits that are becoming an integral part of the online presence for learning organizations, like museums, can provide a basis for creating and sustaining communities. A set of case studies further illustrates working examples of the ways in which personalization and specific tools are developing collaborative spaces, community channels and group interactions.",
        "published": "2004-09-28T18:10:45Z",
        "link": "http://arxiv.org/abs/cs/0409055v1",
        "categories": [
            "cs.CY",
            "cs.DL",
            "H3.5; H3.7; H4.3; H5.2; H5.3; K3.1; K.4.0"
        ]
    },
    {
        "title": "A knowledge-based approach to semi-automatic annotation of multimedia   documents via user adaptation",
        "authors": [
            "Afzal Ballim",
            "Nastaran Fatemi",
            "Hatem Ghorbel",
            "Vincenzo Pallotta"
        ],
        "summary": "Current approaches to the annotation process focus on annotation schemas, languages for annotation, or are very application driven. In this paper it is proposed that a more flexible architecture for annotation requires a knowledge component to allow for flexible search and navigation of the annotated material. In particular, it is claimed that a general approach must take into account the needs, competencies, and goals of the producers, annotators, and consumers of the annotated material. We propose that a user-model based approach is, therefore, necessary.",
        "published": "2004-10-23T00:06:19Z",
        "link": "http://arxiv.org/abs/cs/0410059v1",
        "categories": [
            "cs.DL",
            "cs.CL",
            "cs.IR",
            "I.7.2, H.3.7"
        ]
    },
    {
        "title": "An argumentative annotation schema for meeting discussions",
        "authors": [
            "Vincenzo Pallotta",
            "Hatem Ghorbel",
            "Patrick Ruch",
            "Giovanni Coray"
        ],
        "summary": "In this article, we are interested in the annotation of transcriptions of human-human dialogue taken from meeting records. We first propose a meeting content model where conversational acts are interpreted with respect to their argumentative force and their role in building the argumentative structure of the meeting discussion. Argumentation in dialogue describes the way participants take part in the discussion and argue their standpoints. Then, we propose an annotation scheme based on such an argumentative dialogue model as well as the evaluation of its adequacy. The obtained higher-level semantic annotations are exploited in the conceptual indexing of the information contained in meeting discussions.",
        "published": "2004-10-25T01:38:07Z",
        "link": "http://arxiv.org/abs/cs/0410061v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "cs.IR",
            "H.3.1;I.7.2;H.5.1"
        ]
    },
    {
        "title": "Automatic Keyword Extraction from Spoken Text. A Comparison of two   Lexical Resources: the EDR and WordNet",
        "authors": [
            "Lonneke van der Plas",
            "Vincenzo Pallotta",
            "Martin Rajman",
            "Hatem Ghorbel"
        ],
        "summary": "Lexical resources such as WordNet and the EDR electronic dictionary have been used in several NLP tasks. Probably, partly due to the fact that the EDR is not freely available, WordNet has been used far more often than the EDR. We have used both resources on the same task in order to make a comparison possible. The task is automatic assignment of keywords to multi-party dialogue episodes (i.e. thematically coherent stretches of spoken text). We show that the use of lexical resources in such a task results in slightly higher performances than the use of a purely statistically based method.",
        "published": "2004-10-25T01:50:03Z",
        "link": "http://arxiv.org/abs/cs/0410062v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "cs.IR",
            "H.3.1;H.3.3;I.5.3;I.7.3"
        ]
    },
    {
        "title": "Notes On The Design Of An Internet Adversary",
        "authors": [
            "David S. H. Rosenthal",
            "Petros Maniatis",
            "Mema Roussopoulos",
            "T. J. Giuli",
            "Mary Baker"
        ],
        "summary": "The design of the defenses Internet systems can deploy against attack, especially adaptive and resilient defenses, must start from a realistic model of the threat. This requires an assessment of the capabilities of the adversary. The design typically evolves through a process of simulating both the system and the adversary. This requires the design and implementation of a simulated adversary based on the capability assessment. Consensus on the capabilities of a suitable adversary is not evident. Part of the recent redesign of the protocol used by peers in the LOCKSS digital preservation system included a conservative assessment of the adversary's capabilities. We present our assessment and the implications we drew from it as a step towards a reusable adversary specification.",
        "published": "2004-11-22T00:22:23Z",
        "link": "http://arxiv.org/abs/cs/0411078v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Transparent Format Migration of Preserved Web Content",
        "authors": [
            "David S. H. Rosenthal",
            "Thomas Lipkis",
            "Thomas Robertson",
            "Seth Morabito"
        ],
        "summary": "The LOCKSS digital preservation system collects content by crawling the web and preserves it in the format supplied by the publisher. Eventually, browsers will no longer understand that format. A process called format migration converts it to a newer format that the browsers do understand. The LOCKSS program has designed and tested an initial implementation of format migration for Web content that is transparent to readers, building on the content negotiation capabilities of HTTP.",
        "published": "2004-11-22T00:35:30Z",
        "link": "http://arxiv.org/abs/cs/0411077v1",
        "categories": [
            "cs.DL",
            "H.5.4;H.3.7"
        ]
    },
    {
        "title": "Principles for Digital Preservation",
        "authors": [
            "H. M. Gladney"
        ],
        "summary": "The immense investments in creating and disseminating digitally represented information have not been accompanied by commensurate effort to ensure the longevity of information of permanent interest. Asserted difficulties with long-term digital preservation prove to be largely underestimation of what technology can provide. We show how to clarify prominent misunderstandings and sketch a 'Trustworthy Digital Object (TDO)' method that solves all the published technical challenges.",
        "published": "2004-11-24T20:54:43Z",
        "link": "http://arxiv.org/abs/cs/0411091v2",
        "categories": [
            "cs.DL",
            "H.1 Long term digital preservation"
        ]
    },
    {
        "title": "Trustworthy 100-Year Digital Objects: Durable Encoding for When It's Too   Late to Ask",
        "authors": [
            "H. M. Gladney",
            "R. A. Lorie"
        ],
        "summary": "How can an author store digital information so that it will be reliably useful, even years later when he is no longer available to answer questions? Methods that might work are not good enough; what is preserved today should be reliably useful whenever someone wants it. Prior proposals fail because they confound saved data with irrelevant details of today's information technology--details that are difficult to define, extract, and save completely and accurately.   We use a virtual machine to represent and eventually to render any data whatsoever. We focus on a case of intermediate difficulty--an executable procedure--and identify a variant for every other data type. This solution might be more elaborate than needed to render some text, image, audio, or video data. Simple data can be preserved as representations using well-known standards. We sketch practical methods for files ranging from simple structures to those containing computer programs, treating simple cases here and deferring complex cases for future work. Enough of the complete solution is known to enable practical aggressive preservation programs today.",
        "published": "2004-11-24T21:26:13Z",
        "link": "http://arxiv.org/abs/cs/0411092v1",
        "categories": [
            "cs.DL",
            "H.1"
        ]
    },
    {
        "title": "EURYDICE : A platform for unified access to documents",
        "authors": [
            "Serge Rouveyrol",
            "Yves Chiaramella",
            "Francesca Leinardi",
            "Joanna Janik",
            "Bruno Marmol",
            "Carole Silvy",
            "Catherine Allauzun"
        ],
        "summary": "In this paper we present Eurydice, a platform dedicated to provide a unified gateway to documents. Its basic functionalities about collecting documents have been designed based on a long experience about the management of scientific documentation among large and demanding academic communities such as IMAG and INRIA. Besides the basic problem of accessing documents - which was of course the original and main motivation of the project - a great effort has been dedicated to the development of management functionalities which could help institutions to control, analyse the current situation about the use of the documentation, and finally to set a better ground for a documentation policy. Finally a great emphasis - and corresponding technical investment - has been put on the protection of property and reproduction rights both from the users' intitution side and from the editors' side.",
        "published": "2004-12-01T13:14:51Z",
        "link": "http://arxiv.org/abs/cs/0412001v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "A Link Clustering Based Approach for Clustering Categorical Data",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "Categorical data clustering (CDC) and link clustering (LC) have been considered as separate research and application areas. The main focus of this paper is to investigate the commonalities between these two problems and the uses of these commonalities for the creation of new clustering algorithms for categorical data based on cross-fertilization between the two disjoint research fields. More precisely, we formally transform the CDC problem into an LC problem, and apply LC approach for clustering categorical data. Experimental results on real datasets show that LC based clustering method is competitive with existing CDC algorithms with respect to clustering accuracy.",
        "published": "2004-12-04T12:41:08Z",
        "link": "http://arxiv.org/abs/cs/0412019v1",
        "categories": [
            "cs.DL",
            "cs.AI"
        ]
    },
    {
        "title": "MammoGrid: Large-Scale Distributed Mammogram Analysis",
        "authors": [
            "S. Roberto Amendolia",
            "Michael Brady",
            "Richard McClatchey",
            "Miguel Mulet-Parada",
            "Mohammed Odeh",
            "Tony Solomonides"
        ],
        "summary": "Breast cancer as a medical condition and mammograms as images exhibit many dimensions of variability across the population. Similarly, the way diagnostic systems are used and maintained by clinicians varies between imaging centres and breast screening programmes, and so does the appearance of the mammograms generated. A distributed database that reflects the spread of pathologies across the population is an invaluable tool for the epidemiologist and the understanding of the variation in image acquisition protocols is essential to a radiologist in a screening programme. Exploiting emerging grid technology, the aim of the MammoGrid [1] project is to develop a Europe-wide database of mammograms that will be used to investigate a set of important healthcare applications and to explore the potential of the grid to support effective co-working between healthcare professionals.",
        "published": "2004-02-02T19:34:53Z",
        "link": "http://arxiv.org/abs/cs/0402006v1",
        "categories": [
            "cs.SE",
            "J.3;H2.4"
        ]
    },
    {
        "title": "An Integrated Approach for Extraction of Objects from XML and   Transformation to Heterogeneous Object Oriented Databases",
        "authors": [
            "Uzair Ahmad",
            "Mohammad Waseem Hassan",
            "Arshad Ali",
            "Richard McClatchey",
            "Ian Willers"
        ],
        "summary": "CERN's (European Organization for Nuclear Research) WISDOM project uses XML for the replication of data between different data repositories in a heterogeneous operating system environment. For exchanging data from Web-resident databases, the data needs to be transformed into XML and back to the database format. Many different approaches are employed to do this transformation. This paper addresses issues that make this job more efficient and robust than existing approaches. It incorporates the World Wide Web Consortium (W3C) XML Schema specification in the database-XML relationship. Incorporation of the XML Schema exhibits significant improvements in XML content usage and reduces the limitations of DTD-based database XML services. Secondly the paper explores the possibility of database independent transformation of data between XML and different databases. It proposes a standard XML format that every serialized object should follow. This makes it possible to use objects of heterogeneous database seamlessly using XML.",
        "published": "2004-02-02T20:12:53Z",
        "link": "http://arxiv.org/abs/cs/0402007v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4"
        ]
    },
    {
        "title": "A Use-Case Driven Approach in Requirements Engineering : The Mammogrid   Project",
        "authors": [
            "Mohammed Odeh",
            "Tamas Hauer",
            "Richard McClatchey",
            "Tony Solomonides"
        ],
        "summary": "We report on the application of the use-case modeling technique to identify and specify the user requirements of the MammoGrid project in an incremental and controlled iterative approach. Modeling has been carried out in close collaboration with clinicians and radiologists with no prior experience of use cases. The study reveals the advantages and limitations of applying this technique to requirements specification in the domains of breast cancer screening and mammography research, with implications for medical imaging more generally. In addition, this research has shown a return on investment in use-case modeling in shorter gaps between phases of the requirements engineering process. The qualitative result of this analysis leads us to propose that a use-case modeling approach may result in reducing the cycle of the requirements engineering process for medical imaging.",
        "published": "2004-02-02T20:18:23Z",
        "link": "http://arxiv.org/abs/cs/0402008v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4"
        ]
    },
    {
        "title": "Resolving Clinicians Queries Across a Grids Infrastructure",
        "authors": [
            "F Estrella",
            "C del Frate",
            "T Hauer",
            "R McClatchey",
            "M Odeh",
            "D Rogulin",
            "S R Amendolia",
            "D Schottlander",
            "T Solomonides",
            "R Warren"
        ],
        "summary": "The past decade has witnessed order of magnitude increases in computing power, data storage capacity and network speed, giving birth to applications which may handle large data volumes of increased complexity, distributed over the Internet. Grids computing promises to resolve many of the difficulties in facilitating medical image analysis to allow radiologists to collaborate without having to co-locate. The EU-funded MammoGrid project aims to investigate the feasibility of developing a Grid-enabled European database of mammograms and provide an information infrastructure which federates multiple mammogram databases. This will enable clinicians to develop new common, collaborative and co-operative approaches to the analysis of mammographic data. This paper focuses on one of the key requirements for large-scale distributed mammogram analysis: resolving queries across a grid-connected federation of images.",
        "published": "2004-02-03T14:32:39Z",
        "link": "http://arxiv.org/abs/cs/0402009v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4; J.3"
        ]
    },
    {
        "title": "A Preliminary Study for the development of an Early Method for the   Measurement in Function Points of a Software Product",
        "authors": [
            "Ramon Asensio Monge",
            "Francisco Sanchis Marco",
            "Fernando Torre Cervigon",
            "Victor Garcia Garcia",
            "Gustavo Uria Paino"
        ],
        "summary": "The Function Points Analysis (FPA) of A.J. Albrecht is a method to determine the functional size of software products. The International Function Point Users Group, (IFPUG), establishes the FPA like a standard in the software functional size measurement. The IFPUG [3] [4] method follows the Albrecht's method and incorporates in its succesive versions modifications to the rules and hints with the intention of improving it [7]. The required documentation level to apply the method is the functional specification which corresponds to level I in the Rudolph's clasification [8]. This documentation is avalaible with some difficulty for those companies which are dedicated to develop software for third parties when they have to prepare the appropiate budget for this development. Then, we face the need of developing an early method [6] [9] for measuring the functional size of a software product that we will name to abbreviate it Early Method or EFPM (Early Function Point Method). The required documentation to apply the EFPM would be the User Requirements or some analogous documentations. This is a part of a research work now in process in Oviedo University. In this article we only show the following, results:   From the measurements of a set of projects using the IFPUG method v. 4.1 we obtain the linear correlation coefficients between the total number of Function Points for each project and the counters of the ILFs number, ILFs+EIFs number and EIs+EOs+EQs number.   Using the preliminary results we compute the regression functions. This results we will allow us to determine the factors to be considered in the development of EFPM and to estimate the function points.",
        "published": "2004-02-09T12:43:13Z",
        "link": "http://arxiv.org/abs/cs/0402015v1",
        "categories": [
            "cs.SE",
            "D.4.8"
        ]
    },
    {
        "title": "Pattern Reification as the Basis for Description-Driven Systems",
        "authors": [
            "Florida Estrella",
            "Zsolt Kovacs",
            "Jean-Marie Le Goff",
            "Richard McClatchey",
            "Tony Solomonides",
            "Norbert Toth"
        ],
        "summary": "One of the main factors driving object-oriented software development for information systems is the requirement for systems to be tolerant to change. To address this issue in designing systems, this paper proposes a pattern-based, object-oriented, description-driven system (DDS) architecture as an extension to the standard UML four-layer meta-model. A DDS architecture is proposed in which aspects of both static and dynamic systems behavior can be captured via descriptive models and meta-models. The proposed architecture embodies four main elements - firstly, the adoption of a multi-layered meta-modeling architecture and reflective meta-level architecture, secondly the identification of four data modeling relationships that can be made explicit such that they can be modified dynamically, thirdly the identification of five design patterns which have emerged from practice and have proved essential in providing reusable building blocks for data management, and fourthly the encoding of the structural properties of the five design patterns by means of one fundamental pattern, the Graph pattern. A practical example of this philosophy, the CRISTAL project, is used to demonstrate the use of description-driven data objects to handle system evolution.",
        "published": "2004-02-12T14:25:14Z",
        "link": "http://arxiv.org/abs/cs/0402024v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4,J.3"
        ]
    },
    {
        "title": "A perspective on the Healthgrid initiative",
        "authors": [
            "V. Breton",
            "A. E. Solomonides",
            "R. H. McClatchey"
        ],
        "summary": "This paper presents a perspective on the Healthgrid initiative which involves European projects deploying pioneering applications of grid technology in the health sector. In the last couple of years, several grid projects have been funded on health related issues at national and European levels. A crucial issue is to maximize their cross fertilization in the context of an environment where data of medical interest can be stored and made easily available to the different actors in healthcare, physicians, healthcare centres and administrations, and of course the citizens. The Healthgrid initiative, represented by the Healthgrid association (http://www.healthgrid.org), was initiated to bring the necessary long term continuity, to reinforce and promote awareness of the possibilities and advantages linked to the deployment of GRID technologies in health. Technologies to address the specific requirements for medical applications are under development. Results from the DataGrid and other projects are given as examples of early applications.",
        "published": "2004-02-12T14:36:05Z",
        "link": "http://arxiv.org/abs/cs/0402025v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4,J.3"
        ]
    },
    {
        "title": "A Service-Based Approach for Managing Mammography Data",
        "authors": [
            "Florida Estrella",
            "Richard McClatchey",
            "Dmitry Rogulina",
            "Roberto Amendolia",
            "Tony Solomonides"
        ],
        "summary": "Grid-based technologies are emerging as a potential open-source standards-based solution for managing and collabo-rating distributed resources. In view of these new computing solutions, the Mammogrid project is developing a service-based and Grid-aware application which manages a Euro-pean-wide database of mammograms. Medical conditions such as breast cancer, and mammograms as images, are ex-tremely complex with many dimensions of variability across the population. An effective solution for the management of disparate mammogram data sources is a federation of autonomous multi-centre sites which transcends national boundaries. The Mammogrid solution utilizes the Grid tech-nologies to integrate geographically distributed data sets. The Mammogrid application will explore the potential of the Grid to support effective co-working among radiologists through-out the EU. This paper outlines the Mammogrid service-based approach in managing a federation of grid-connected mam-mography databases.",
        "published": "2004-02-12T15:00:18Z",
        "link": "http://arxiv.org/abs/cs/0402023v1",
        "categories": [
            "cs.DB",
            "cs.SE",
            "H2.4; J.3"
        ]
    },
    {
        "title": "Model-checking Driven Black-box Testing Algorithms for Systems with   Unspecified Components",
        "authors": [
            "Gaoyan Xie",
            "Zhe Dang"
        ],
        "summary": "Component-based software development has posed a serious challenge to system verification since externally-obtained components could be a new source of system failures. This issue can not be completely solved by either model-checking or traditional software testing techniques alone due to several reasons:   1) externally obtained components are usually unspecified/partially specified; 2)it is generally difficult to establish an adequacy criteria for testing a component; 3)components may be used to dynamically upgrade a system.   This paper introduces a new approach (called {\\em model-checking driven black-box testing}) that combines model-checking with traditional black-box software testing to tackle the problem in a complete, sound, and automatic way.   The idea is to, with respect to some requirement (expressed in CTL or LTL) about the system, use model-checking techniques to derive a condition (expressed in communication graphs) for an unspecified component such that the system satisfies the requirement iff the condition is satisfied by the component, and which can be established by testing the component with test cases generated from the condition on-the-fly. In this paper, we present model-checking driven black-box testing algorithms to handle both CTL and LTL requirements.   We also illustrate the idea through some examples.",
        "published": "2004-04-19T15:02:15Z",
        "link": "http://arxiv.org/abs/cs/0404037v2",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.2.4;D.2.5;F.4.1"
        ]
    },
    {
        "title": "Benchmarking Blunders and Things That Go Bump in the Night",
        "authors": [
            "Neil J. Gunther"
        ],
        "summary": "Benchmarking; by which I mean any computer system that is driven by a controlled workload, is the ultimate in performance testing and simulation. Aside from being a form of institutionalized cheating, it also offer countless opportunities for systematic mistakes in the way the workloads are applied and the resulting measurements interpreted. Right test, wrong conclusion is a ubiquitous mistake that happens because test engineers tend to treat data as divine. Such reverence is not only misplaced, it's also a sure ticket to production hell when the application finally goes live. I demonstrate how such mistakes can be avoided by means of two war stories that are real WOPRs. (a) How to resolve benchmark flaws over the psychic hotline and (b) How benchmarks can go flat with too much Java juice. In each case I present simple performance models and show how they can be applied to correctly assess benchmark data.",
        "published": "2004-04-21T20:07:07Z",
        "link": "http://arxiv.org/abs/cs/0404043v1",
        "categories": [
            "cs.PF",
            "cs.SE",
            "B.8.1;B.8.2;D.2.5;K.7.3"
        ]
    },
    {
        "title": "Culture and International Usability Testing: The Effects of Culture in   Structured Interviews",
        "authors": [
            "Ravikiran Vatrapu",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "The global audience for software products includes members of different countries, religions, and cultures: people who speak different languages, have different life styles, and have different perceptions and expectations of any given product. A major impediment in interface development is that there is inadequate empirical evidence for the effects of culture in the usability engineering methods used for developing user interfaces. This paper presents a controlled study investigating the effects of culture on the effectiveness of structured interviews in usability testing. The experiment consisted of usability testing of a website with two independent groups of Indian participants by two interviewers; one belonging to the Indian culture and the other to the Anglo-American culture. Participants found more usability problems and made more suggestions to an interviewer who was a member of the same (Indian) culture than to the foreign (Anglo-American) interviewer. The results of the study empirically establish that culture significantly affects the efficacy of structured interviews during international user testing. The implications of this work for usability engineering are discussed.",
        "published": "2004-05-13T18:46:34Z",
        "link": "http://arxiv.org/abs/cs/0405045v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "H5.2"
        ]
    },
    {
        "title": "The ATLAS Tile Calorimeter Test Beam Monitoring Program",
        "authors": [
            "Paolo Adragna",
            "Andrea Dotti",
            "Chiara Roda"
        ],
        "summary": "During 2003 test beam session for ATLAS Tile Calorimeter a monitoring program has been developed to ease the setup of correct running condition and the assessment of data quality. The program has been built using the Online Software services provided by the ATLAS Online Software group. The first part of this note contains a brief overview of these services followed by the full description of Tile Calorimeter monitoring program architecture and features. Performances and future upgrades are discussed in the final part of this note.",
        "published": "2004-05-29T10:05:51Z",
        "link": "http://arxiv.org/abs/physics/0405154v1",
        "categories": [
            "physics.ins-det",
            "cs.PF",
            "cs.SE"
        ]
    },
    {
        "title": "Uncovering the epistemological and ontological assumptions of software   designers",
        "authors": [
            "David King",
            "Chris Kimble"
        ],
        "summary": "The ontological and epistemological positions adopted by information systems design methods are incommensur-able when pushed to their extremes. Information systems research has therefore tended to focus on the similarities between different positions, usually in search of a single, unifying position. However, by focusing on the similari-ties, the clarity of argument provided by any one philoso-phical position is necessarily diminished. Consequently, researchers often treat the philosophical foundations of design methods as being of only minor importance. In this paper, we have deliberately chosen to focus on the differences between various philosophical positions. From this focus, we believe we can offer a clearer under-standing of the empirical behaviour of software as viewed from particular philosophical positions. Since the em-pirical evidence does not favour any single position, we conclude by arguing for the validity of ad hoc approaches to software design which we believe provides a stronger and more theoretically grounded approach to software design.",
        "published": "2004-06-16T10:01:06Z",
        "link": "http://arxiv.org/abs/cs/0406022v1",
        "categories": [
            "cs.SE",
            "cs.GL",
            "K.6.3, I.0"
        ]
    },
    {
        "title": "Notions of Equivalence in Software Design",
        "authors": [
            "David King",
            "Chris Kimble"
        ],
        "summary": "Design methods in information systems frequently create software descriptions using formal languages. Nonetheless, most software designers prefer to describe software using natural languages. This distinction is not simply a matter of convenience. Natural languages are not the same as formal languages; in particular, natural languages do not follow the notions of equivalence used by formal languages. In this paper, we show both the existence and coexistence of different notions of equivalence by extending the no-tion of oracles used in formal languages. This allows distinctions to be made between the trustworthy oracles assumed by formal languages and the untrust-worthy oracles used by natural languages. By examin-ing the notion of equivalence, we hope to encourage designers of software to rethink the place of ambiguity in software design.",
        "published": "2004-06-16T10:07:16Z",
        "link": "http://arxiv.org/abs/cs/0406023v1",
        "categories": [
            "cs.SE",
            "K.6.3; I.0"
        ]
    },
    {
        "title": "Improving Prolog Programs: Refactoring for Prolog",
        "authors": [
            "Tom Schrijvers",
            "Alexander Serebrenik"
        ],
        "summary": "Refactoring is an established technique from the OO-community to restructure code: it aims at improving software readability, maintainability and extensibility. Although refactoring is not tied to the OO-paradigm in particular, its ideas have not been applied to Logic Programming until now.   This paper applies the ideas of refactoring to Prolog programs. A catalogue is presented listing refactorings classified according to scope. Some of the refactorings have been adapted from the OO-paradigm, while others have been specifically designed for Prolog. Also the discrepancy between intended and operational semantics in Prolog is addressed by some of the refactorings.   In addition, ViPReSS, a semi-automatic refactoring browser, is discussed and the experience with applying \\vipress to a large Prolog legacy system is reported. Our main conclusion is that refactoring is not only a viable technique in Prolog but also a rather desirable one.",
        "published": "2004-06-16T16:55:55Z",
        "link": "http://arxiv.org/abs/cs/0406026v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.7; D.1.6"
        ]
    },
    {
        "title": "Model Checking of Statechart Models: Survey and Research Directions",
        "authors": [
            "Purandar Bhaduri",
            "S. Ramesh"
        ],
        "summary": "We survey existing approaches to the formal verification of statecharts using model checking. Although the semantics and subset of statecharts used in each approach varies considerably, along with the model checkers and their specification languages, most approaches rely on translating the hierarchical structure into the flat representation of the input language of the model checker. This makes model checking difficult to scale to industrial models, as the state space grows exponentially with flattening. We look at current approaches to model checking hierarchical structures and find that their semantics is significantly different from statecharts. We propose to address the problem of state space explosion using a combination of techniques, which are proposed as directions for further research.",
        "published": "2004-07-16T10:18:42Z",
        "link": "http://arxiv.org/abs/cs/0407038v1",
        "categories": [
            "cs.SE",
            "D.2.4 Software/Program Verification"
        ]
    },
    {
        "title": "Bug shallowness in open-source, Macintosh software",
        "authors": [
            "G Gordon Worley III"
        ],
        "summary": "Central to the power of open-source software is bug shallowness, the relative ease of finding and fixing bugs. The open-source movement began with Unix software, so many users were also programmers capable of finding and fixing bugs given the source code. But as the open-source movement reaches the Macintosh platform, bugs may not be shallow because few Macintosh users are programmers. Based on reports from open-source developers, I, however, conclude that that bugs are as shallow in open-source, Macintosh software as in any other open-source software.",
        "published": "2004-07-20T13:04:36Z",
        "link": "http://arxiv.org/abs/cs/0407051v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Modeling and Validating Hybrid Systems Using VDM and Mathematica",
        "authors": [
            "Bernhard K. Aichernig",
            "Reinhold Kainhofer"
        ],
        "summary": "Hybrid systems are characterized by the hybrid evolution of their state: A part of the state changes discretely, the other part changes continuously over time. Typically, modern control applications belong to this class of systems, where a digital controller interacts with a physical environment. In this article we illustrate how a combination of the formal method VDM and the computer algebra system Mathematica can be used to model and simulate both aspects: the control logic and the physics involved. A new Mathematica package emulating VDM-SL has been developed that allows the integration of differential equation systems into formal specifications. The SAFER example from Kelly (1997) serves to demonstrate the new simulation capabilities Mathematica adds: After the thruster selection process, the astronaut's actual position and velocity is calculated by numerically solving Euler's and Newton's equations for rotation and translation. Furthermore, interactive validation is supported by a graphical user interface and data animation.",
        "published": "2004-07-20T15:15:48Z",
        "link": "http://arxiv.org/abs/cs/0407050v1",
        "categories": [
            "cs.SE",
            "D.2.4"
        ]
    },
    {
        "title": "Roles Are Really Great!",
        "authors": [
            "Viktor Kuncak",
            "Patrick Lam",
            "Martin Rinard"
        ],
        "summary": "We present a new role system for specifying changing referencing relationships of heap objects. The role of an object depends, in large part, on its aliasing relationships with other objects, with the role of each object changing as its aliasing relationships change. Roles therefore capture important object and data structure properties and provide useful information about how the actions of the program interact with these properties. Our role system enables the programmer to specify the legal aliasing relationships that define the set of roles that objects may play, the roles of procedure parameters and object fields, and the role changes that procedures perform while manipulating objects. We present an interprocedural, compositional, and context-sensitive role analysis algorithm that verifies that a program respects the role constraints.",
        "published": "2004-08-05T03:02:01Z",
        "link": "http://arxiv.org/abs/cs/0408013v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2"
        ]
    },
    {
        "title": "On the Theory of Structural Subtyping",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "We show that the first-order theory of structural subtyping of non-recursive types is decidable. Let $\\Sigma$ be a language consisting of function symbols (representing type constructors) and $C$ a decidable structure in the relational language $L$ containing a binary relation $\\leq$. $C$ represents primitive types; $\\leq$ represents a subtype ordering. We introduce the notion of $\\Sigma$-term-power of $C$, which generalizes the structure arising in structural subtyping. The domain of the $\\Sigma$-term-power of $C$ is the set of $\\Sigma$-terms over the set of elements of $C$. We show that the decidability of the first-order theory of $C$ implies the decidability of the first-order theory of the $\\Sigma$-term-power of $C$. Our decision procedure makes use of quantifier elimination for term algebras and Feferman-Vaught theorem. Our result implies the decidability of the first-order theory of structural subtyping of non-recursive types.",
        "published": "2004-08-05T04:12:25Z",
        "link": "http://arxiv.org/abs/cs/0408015v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE",
            "D.2.4; D.3.1; D.3.3; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "On computing the fixpoint of a set of boolean equations",
        "authors": [
            "Viktor Kuncak",
            "K. Rustan M. Leino"
        ],
        "summary": "This paper presents a method for computing a least fixpoint of a system of equations over booleans. The resulting computation can be significantly shorter than the result of iteratively evaluating the entire system until a fixpoint is reached.",
        "published": "2004-08-19T17:24:49Z",
        "link": "http://arxiv.org/abs/cs/0408045v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.4; D.3.1; F.3.1; F.3.2; F.4.1"
        ]
    },
    {
        "title": "CrocoPat 2.1 Introduction and Reference Manual",
        "authors": [
            "Dirk Beyer",
            "Andreas Noack"
        ],
        "summary": "CrocoPat is an efficient, powerful and easy-to-use tool for manipulating relations of arbitrary arity, including directed graphs. This manual provides an introduction to and a reference for CrocoPat and its programming language RML. It includes several application examples, in particular from the analysis of structural models of software systems.",
        "published": "2004-09-07T09:44:18Z",
        "link": "http://arxiv.org/abs/cs/0409009v1",
        "categories": [
            "cs.PL",
            "cs.DM",
            "cs.DS",
            "cs.SE",
            "D.1.6; G.2.2.a; E.1.d; D.2.7m"
        ]
    },
    {
        "title": "Using a hierarchy of Domain Specific Languages in complex software   systems design",
        "authors": [
            "V. S. Lugovsky"
        ],
        "summary": "A new design methodology is introduced, with some examples on building Domain Specific Languages hierarchy on top of Scheme.",
        "published": "2004-09-09T01:44:05Z",
        "link": "http://arxiv.org/abs/cs/0409016v1",
        "categories": [
            "cs.PL",
            "cs.DS",
            "cs.SE",
            "D.1.1;I.2.2;D.3.2;D.2.10"
        ]
    },
    {
        "title": "Robust Dialogue Understanding in HERALD",
        "authors": [
            "Vincenzo Pallotta",
            "Afzal Ballim"
        ],
        "summary": "We tackle the problem of robust dialogue processing from the perspective of language engineering. We propose an agent-oriented architecture that allows us a flexible way of composing robust processors. Our approach is based on Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP agent model can be enriched with special features and components that allow us to deal with classical problems of dialogue understanding.",
        "published": "2004-10-22T23:41:52Z",
        "link": "http://arxiv.org/abs/cs/0410058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.MA",
            "cs.SE",
            "H.5.2, I.2.7, I.2.11"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: First Field Experience",
        "authors": [
            "Patrick C. McGuire",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Jose Antonio Rodriguez-Manfredi",
            "Javier Gomez-Elvira",
            "Helge Ritter",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist and field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: first, detection and accounting for shadows caused by 3D jagged edges in the outcrop; second, reincorporation of more sophisticated texture-analysis algorithms into the system; third, creation of hardware and software capabilities to control the camera's zoom lens in an intelligent manner; and fourth, development of algorithms for interpretation of complex geological scenery. Nonetheless, despite these technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",
        "published": "2004-10-27T09:14:01Z",
        "link": "http://arxiv.org/abs/cs/0410071v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "q-bio.NC",
            "I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9"
        ]
    },
    {
        "title": "On Spatial Conjunction as Second-Order Logic",
        "authors": [
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "Spatial conjunction is a powerful construct for reasoning about dynamically allocated data structures, as well as concurrent, distributed and mobile computation. While researchers have identified many uses of spatial conjunction, its precise expressive power compared to traditional logical constructs was not previously known. In this paper we establish the expressive power of spatial conjunction. We construct an embedding from first-order logic with spatial conjunction into second-order logic, and more surprisingly, an embedding from full second order logic into first-order logic with spatial conjunction. These embeddings show that the satisfiability of formulas in first-order logic with spatial conjunction is equivalent to the satisfiability of formulas in second-order logic. These results explain the great expressive power of spatial conjunction and can be used to show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidable logic. As one example, we show that adding unrestricted spatial conjunction to two-variable logic leads to undecidability. On the side of decidability, the embedding into second-order logic immediately implies the decidability of first-order logic with a form of spatial conjunction over trees. The embedding into spatial conjunction also has useful consequences: because a restricted form of spatial conjunction in two-variable logic preserves decidability, we obtain that a correspondingly restricted form of second-order quantification in two-variable logic is decidable. The resulting language generalizes the first-order theory of boolean algebra over sets and is useful in reasoning about the contents of data structures in object-oriented languages.",
        "published": "2004-10-28T15:00:42Z",
        "link": "http://arxiv.org/abs/cs/0410073v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SE"
        ]
    },
    {
        "title": "Inter-Package Dependency Networks in Open-Source Software",
        "authors": [
            "Nathan LaBelle",
            "Eugene Wallingford"
        ],
        "summary": "This research analyzes complex networks in open-source software at the inter-package level, where package dependencies often span across projects and between development groups. We review complex networks identified at ``lower'' levels of abstraction, and then formulate a description of interacting software components at the package level, a relatively ``high'' level of abstraction. By mining open-source software repositories from two sources, we empirically show that the coupling of modules at this granularity creates a small-world and scale-free network in both instances.",
        "published": "2004-11-29T03:38:48Z",
        "link": "http://arxiv.org/abs/cs/0411096v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Mass Storage Management and the Grid",
        "authors": [
            "A. Earl",
            "P. Clark"
        ],
        "summary": "The University of Edinburgh has a significant interest in mass storage systems as it is one of the core groups tasked with the roll out of storage software for the UK's particle physics grid, GridPP. We present the results of a development project to provide software interfaces between the SDSC Storage Resource Broker, the EU DataGrid and the Storage Resource Manager. This project was undertaken in association with the eDikt group at the National eScience Centre, the Universities of Bristol and Glasgow, Rutherford Appleton Laboratory and the San Diego Supercomputing Center.",
        "published": "2004-12-20T11:50:44Z",
        "link": "http://arxiv.org/abs/cs/0412092v1",
        "categories": [
            "cs.DC",
            "cs.SE"
        ]
    },
    {
        "title": "The Triplet Genetic Code had a Doublet Predecessor",
        "authors": [
            "Apoorva Patel"
        ],
        "summary": "Information theoretic analysis of genetic languages indicates that the naturally occurring 20 amino acids and the triplet genetic code arose by duplication of 10 amino acids of class-II and a doublet genetic code having codons NNY and anticodons $\\overleftarrow{\\rm GNN}$. Evidence for this scenario is presented based on the properties of aminoacyl-tRNA synthetases, amino acids and nucleotide bases.",
        "published": "2004-03-25T12:30:03Z",
        "link": "http://arxiv.org/abs/q-bio/0403036v2",
        "categories": [
            "q-bio.GN",
            "cs.CE",
            "q-bio.BM",
            "quant-ph"
        ]
    },
    {
        "title": "Algorithms for Estimating Information Distance with Application to   Bioinformatics and Linguistics",
        "authors": [
            "Alexei Kaltchenko"
        ],
        "summary": "After reviewing unnormalized and normalized information distances based on incomputable notions of Kolmogorov complexity, we discuss how Kolmogorov complexity can be approximated by data compression algorithms. We argue that optimal algorithms for data compression with side information can be successfully used to approximate the normalized distance. Next, we discuss an alternative information distance, which is based on relative entropy rate (also known as Kullback-Leibler divergence), and compression-based algorithms for its estimation. Based on available biological and linguistic data, we arrive to unexpected conclusion that in Bioinformatics and Computational Linguistics this alternative distance is more relevant and important than the ones based on Kolmogorov complexity.",
        "published": "2004-04-20T15:18:43Z",
        "link": "http://arxiv.org/abs/cs/0404039v1",
        "categories": [
            "cs.CC",
            "cs.CE",
            "q-bio.GN",
            "J.3; E.4"
        ]
    },
    {
        "title": "The modulus in the CAD system drawings as a base of developing of the   problem-oriented extensions",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The concept of the \"modulus\" in the CAD system drawings is characterized, being a base of developing of the problem-oriented extensions. The modulus consists of visible geometric elements of the drawing and invisible parametric representation of the modelling object. The technological advantages of moduluss in a complex CAD system developing are described.",
        "published": "2004-05-12T07:55:34Z",
        "link": "http://arxiv.org/abs/cs/0405041v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "J.6"
        ]
    },
    {
        "title": "Modular technology of developing of the problem-oriented extensions of a   CAD system of reconstruction of the plant",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The modular technology of creation of the problem-oriented extensions of a CAD system is described, which was realised in a system TechnoCAD GlassX for designing of reconstruction of the plants. The modularity of the technology is expressed in storage of all parameters of the design in one element of the drawing - modulus, with automatic generation of a geometrical part of the modulus from these parameters. The common principles of the system organization of extensions developing are described: separation of the part of the design to automize in this extension, architecture of parameters in the form of the lists of objects with their properties and links to another objects, separation of common and special operations, stages of the developing, boundaries of applicability of technology.",
        "published": "2004-05-14T17:43:33Z",
        "link": "http://arxiv.org/abs/cs/0405047v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "I.2.1, J.6"
        ]
    },
    {
        "title": "The model of the tables in design documentation for operating with the   electronic catalogs and for specifications making in a CAD system",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The hierarchic block model of the tables in design documentation as a part of a CAD system is described, intended for automatic specifications making of elements of the drawings, with usage of the electronic catalogs. The model is created for needs of a CAD system of reconstruction of the industrial plants, where the result of designing are the drawings, which include the specifications of different types. The adequate simulation of the specification tables is ensured with technology of storing in the drawing of the visible geometric elements and invisible parametric representation, sufficient for generation of this elements.",
        "published": "2004-05-17T09:39:03Z",
        "link": "http://arxiv.org/abs/cs/0405054v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2, I.2.1, J.6"
        ]
    },
    {
        "title": "Modular technology of developing of the extensions of a CAD system.   Axonometric piping diagrams. Parametric representation",
        "authors": [
            "Vladimir V. Migunov",
            "Rustem R. Kafiatullov",
            "Ilsur T. Safin"
        ],
        "summary": "Applying the modular technology of developing of the problem-oriented extensions of a CAD system to a problem of automation of creating of the axonometric piping diagrams on an example of the program system TechnoCAD GlassX is described. The proximity of composition of the schemas is detected for special technological pipe lines, systems of a water line and water drain, heating, heat supply, ventilating, air conditioning. The structured parametric representation of the schemas, including properties of objects, their link, common settings, settings by default and the special links of compatibility is reviewed.",
        "published": "2004-05-17T09:46:02Z",
        "link": "http://arxiv.org/abs/cs/0405055v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2, I.2.1, J.6"
        ]
    },
    {
        "title": "Modular technology of developing of the extensions of a CAD system. The   axonometric piping diagrams. Common and special operations",
        "authors": [
            "Ilsur T. Safin",
            "Vladimir V. Migunov",
            "Rustem R. Kafiatullov"
        ],
        "summary": "Applying the modular technology of developing of the problem-oriented extensions of a CAD system to a problem of automation of creating of the axonometric piping diagrams on an example of the program system TechnoCAD GlassX is described. The features of realization of common operations, composition and realization of special operations of a designing of the schemas of the special technological pipe lines, systems of a water line and water drain, heating, heat supply, ventilating, air conditioning are reviewed.",
        "published": "2004-05-17T10:14:12Z",
        "link": "http://arxiv.org/abs/cs/0405056v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "I.2.1, J.6"
        ]
    },
    {
        "title": "Mathematical and programming toolkit of the computer aided design of the   axonometric piping diagrams",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The problem of the automation of the designing of the axonometric piping diagrams include, as the minimum, manipulations with the flat schemas of three-dimensional wireframe objects (with dimension of 2,5). The specialized model, methodical and mathematical approaches are required because of large bulk of calculuss. Coordinate systems, data types, common principles of realization of operation with data and composition of the basic operations are described which are realised in the complex CAD system of the reconstruction of the plants TechnoCAD GlassX.",
        "published": "2004-05-17T10:34:16Z",
        "link": "http://arxiv.org/abs/cs/0405057v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "I.2.1, J.6"
        ]
    },
    {
        "title": "A direct formulation for sparse PCA using semidefinite programming",
        "authors": [
            "Alexandre d'Aspremont",
            "Laurent El Ghaoui",
            "Michael I. Jordan",
            "Gert R. G. Lanckriet"
        ],
        "summary": "We examine the problem of approximating, in the Frobenius-norm sense, a positive, semidefinite symmetric matrix by a rank-one matrix, with an upper bound on the cardinality of its eigenvector. The problem arises in the decomposition of a covariance matrix into sparse factors, and has wide applications ranging from biology to finance. We use a modification of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semidefinite programming based relaxation for our problem. We also discuss Nesterov's smooth minimization technique applied to the SDP arising in the direct sparse PCA method.",
        "published": "2004-06-16T01:23:03Z",
        "link": "http://arxiv.org/abs/cs/0406021v3",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Business Process Measures",
        "authors": [
            "Valdis Vitolins"
        ],
        "summary": "The paper proposes a new methodology for defining business process measures and their computation. The approach is based on metamodeling according to MOF. Especially, a metamodel providing precise definitions of typical process measures for UML activity diagram-like notation is proposed, including precise definitions how measures should be aggregated for composite process elements. The proposed approach allows defining values in a natural way, and measurement of data, which are of interest to business, without deep investigation into specific technical solutions. This provides new possibilities for business process measurement, decreasing the gap between technical solutions and asset management methodologies.",
        "published": "2004-06-22T12:19:54Z",
        "link": "http://arxiv.org/abs/cs/0406042v1",
        "categories": [
            "cs.CE",
            "cs.PF",
            "J.1; J.7; C.4; I.6.5"
        ]
    },
    {
        "title": "An agent-based intelligent environmental monitoring system",
        "authors": [
            "Ioannis N Athanasiadis",
            "Pericles A Mitkas"
        ],
        "summary": "Fairly rapid environmental changes call for continuous surveillance and on-line decision making. There are two main areas where IT technologies can be valuable. In this paper we present a multi-agent system for monitoring and assessing air-quality attributes, which uses data coming from a meteorological station. A community of software agents is assigned to monitor and validate measurements coming from several sensors, to assess air-quality, and, finally, to fire alarms to appropriate recipients, when needed. Data mining techniques have been used for adding data-driven, customized intelligence into agents. The architecture of the developed system, its domain ontology, and typical agent interactions are presented. Finally, the deployment of a real-world test case is demonstrated.",
        "published": "2004-07-10T11:06:57Z",
        "link": "http://arxiv.org/abs/cs/0407024v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "Static versus Dynamic Arbitrage Bounds on Multivariate Option Prices",
        "authors": [
            "Alexandre d'Aspremont"
        ],
        "summary": "We compare static arbitrage price bounds on basket calls, i.e. bounds that only involve buy-and-hold trading strategies, with the price range obtained within a multi-variate generalization of the Black-Scholes model. While there is no gap between these two sets of prices in the univariate case, we observe here that contrary to our intuition about model risk for at-the-money calls, there is a somewhat large gap between model prices and static arbitrage prices, hence a similarly large set of prices on which a multivariate Black-Scholes model cannot be calibrated but where no conclusion can be drawn on the presence or not of a static arbitrage opportunity.",
        "published": "2004-07-10T16:17:26Z",
        "link": "http://arxiv.org/abs/cs/0407029v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "A Theoretical Study on Spin-Dependent Transport of \"Ferromagnet/Carbon   Nanotube Encapsulating Magnetic Atoms/Ferromagnet\" Junctions with 4-Valued   Conductances",
        "authors": [
            "Satoshi Kokado",
            "Kikuo Harigaya"
        ],
        "summary": "As a novel function of ferromagnet (FM)/spacer/FM junctions, we theoretically investigate multiple-valued (or multi-level) cell property, which is in principle realized by sensing conductances of four states recorded with magnetization configurations of two FMs; (up,up), (up,down), (down,up), (down,down). In order to sense all the states, 4-valued conductances corresponding to the respective states are necessary. We previously proposed that 4-valued conductances are obtained in FM1/spin-polarized spacer (SPS)/FM2 junctions, where FM1 and FM2 have different spin polarizations, and the spacer depends on spin [J. Phys.: Condens. Matter 15, 8797 (2003)]. In this paper, an ideal SPS is considered as a single-wall armchair carbon nanotube encapsulating magnetic atoms, where the nanotube shows on-resonance or off-resonance at the Fermi level according to its length. The magnitude of the obtained 4-valued conductances has an opposite order between the on-resonant nanotube and the off-resonant one, and this property can be understood by considering electronic states of the nanotube. Also, the magnetoresistance ratio between (up,up) and (down,down) can be larger than the conventional one between parallel and anti-parallel configurations.",
        "published": "2004-07-16T13:21:36Z",
        "link": "http://arxiv.org/abs/cond-mat/0407439v1",
        "categories": [
            "cond-mat.mes-hall",
            "cond-mat.mtrl-sci",
            "cs.CE",
            "physics.chem-ph"
        ]
    },
    {
        "title": "Pervasive Service Architecture for a Digital Business Ecosystem",
        "authors": [
            "Thomas Heistracher",
            "Thomas Kurz",
            "Claudius Masuch",
            "Pierfranco Ferronato",
            "Miguel Vidal",
            "Angelo Corallo",
            "Gerard Briscoe",
            "Paolo Dini"
        ],
        "summary": "In this paper we present ideas and architectural principles upon which we are basing the development of a distributed, open-source infrastructure that, in turn, will support the expression of business models, the dynamic composition of software services, and the optimisation of service chains through automatic self-organising and evolutionary algorithms derived from biology. The target users are small and medium-sized enterprises (SMEs). We call the collection of the infrastructure, the software services, and the SMEs a Digital Business Ecosystem (DBE).",
        "published": "2004-08-20T16:23:19Z",
        "link": "http://arxiv.org/abs/cs/0408047v2",
        "categories": [
            "cs.CE",
            "cs.NI"
        ]
    },
    {
        "title": "Parallel Computing Environments and Methods for Power Distribution   System Simulation",
        "authors": [
            "Ning Lu",
            "Z. Todd Taylor",
            "David P. Chassin",
            "Ross T. Guttromson",
            "R. Scott Studham"
        ],
        "summary": "The development of cost-effective highperformance parallel computing on multi-processor supercomputers makes it attractive to port excessively time consuming simulation software from personal computers (PC) to super computes. The power distribution system simulator (PDSS) takes a bottom-up approach and simulates load at the appliance level, where detailed thermal models for appliances are used. This approach works well for a small power distribution system consisting of a few thousand appliances. When the number of appliances increases, the simulation uses up the PC memory and its runtime increases to a point where the approach is no longer feasible to model a practical large power distribution system. This paper presents an effort made to port a PC-based power distribution system simulator to a 128-processor shared-memory supercomputer. The paper offers an overview of the parallel computing environment and a description of the modification made to the PDSS model. The performance of the PDSS running on a standalone PC and on the supercomputer is compared. Future research direction of utilizing parallel computing in the power distribution system simulation is also addressed.",
        "published": "2004-09-18T17:09:26Z",
        "link": "http://arxiv.org/abs/cs/0409035v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.MA",
            "cs.PF"
        ]
    },
    {
        "title": "Using sparse matrices and splines-based interpolation in computational   fluid dynamics simulations",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this relation I present a technique of construction and fast evaluation of a family of cubic polynomials for analytic smoothing and graphical rendering of particles trajectories for flows in a generic geometry. The principal result of the work was implementation and test of a method for interpolating 3D points by regular parametric curves and their fast and efficient evaluation for a good resolution of rendering. For the purpose a parallel environment using a multiprocessor cluster architecture has been used. This work has been developed for the Research and Development Department of my company for planning advanced customized models of industrial burners.",
        "published": "2004-09-29T10:34:45Z",
        "link": "http://arxiv.org/abs/cs/0409056v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "physics.comp-ph",
            "C.1.4; D.1.3; G.1.0"
        ]
    },
    {
        "title": "A dynamical model of a GRID market",
        "authors": [
            "Uli Harder",
            "Peter Harrison",
            "Maya Paczuski",
            "Tejas Shah"
        ],
        "summary": "We discuss potential market mechanisms for the GRID. A complete dynamical model of a GRID market is defined with three types of agents. Providers, middlemen and users exchange universal GRID computing units (GCUs) at varying prices. Providers and middlemen have strategies aimed at maximizing profit while users are 'satisficing' agents, and only change their behavior if the service they receive is sufficiently poor or overpriced. Preliminary results from a multi-agent numerical simulation of the market model shows that the distribution of price changes has a power law tail.",
        "published": "2004-10-02T11:02:09Z",
        "link": "http://arxiv.org/abs/cs/0410005v1",
        "categories": [
            "cs.MA",
            "cond-mat.other",
            "cs.CE"
        ]
    },
    {
        "title": "Intelligent Computer Numerical Control unit for machine tools",
        "authors": [
            "J. Balic"
        ],
        "summary": "The paper describes a new CNC control unit for machining centres with learning ability and automatic intelligent generating of NC programs on the bases of a neural network, which is built-in into a CNC unit as special device. The device performs intelligent and completely automatically the NC part programs only on the bases of 2D, 2,5D or 3D computer model of prismatic part. Intervention of the operator is not needed. The neural network for milling, drilling, reaming, threading and operations alike has learned to generate NC programs in the learning module, which is a part of intelligent CAD/CAM system.",
        "published": "2004-10-25T15:55:43Z",
        "link": "http://arxiv.org/abs/cs/0410064v1",
        "categories": [
            "cs.CE",
            "J.6.2; J.7.3"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: First Field Experience",
        "authors": [
            "Patrick C. McGuire",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Jose Antonio Rodriguez-Manfredi",
            "Javier Gomez-Elvira",
            "Helge Ritter",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist and field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: first, detection and accounting for shadows caused by 3D jagged edges in the outcrop; second, reincorporation of more sophisticated texture-analysis algorithms into the system; third, creation of hardware and software capabilities to control the camera's zoom lens in an intelligent manner; and fourth, development of algorithms for interpretation of complex geological scenery. Nonetheless, despite these technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",
        "published": "2004-10-27T09:14:01Z",
        "link": "http://arxiv.org/abs/cs/0410071v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "q-bio.NC",
            "I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9"
        ]
    },
    {
        "title": "The modular technology of development of the CAD expansions: profiles of   outside networks of water supply and water drain",
        "authors": [
            "Vladimir V. Migunov",
            "Rustem R. Kafiyatullov",
            "Ilsur T. Safin"
        ],
        "summary": "The modular technology of development of the problem-oriented CAD expansions is applied to a task of designing of profiles of outside networks of water supply and water drain with realization in program system TechnoCAD GlassX. The unity of structure of this profiles is revealed, the system model of the drawings of profiles of networks is developed including the structured parametric representation (properties of objects and their interdependence, general settings and default settings) and operations with it, which efficiently automate designing",
        "published": "2004-12-08T08:42:53Z",
        "link": "http://arxiv.org/abs/cs/0412029v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The modular technology of development of the CAD expansions: protection   of the buildings from the lightning",
        "authors": [
            "Vladimir V. Migunov",
            "Rustem R. Kafiyatullov",
            "Ilsur T. Safin"
        ],
        "summary": "The modular technology of development of the problem-oriented CAD expansions is applied to a task of designing of protection of the buildings from the lightning with realization in program system TechnoCAD GlassX. The system model of the drawings of lightning protection is developed including the structured parametric representation (properties of objects and their interdependence, general settings and default settings) and operations with it, which efficiently automate designing",
        "published": "2004-12-08T08:49:08Z",
        "link": "http://arxiv.org/abs/cs/0412030v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The Features of the Complex CAD system of Reconstruction of the   Industrial Plants",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The features of designing of reconstruction of the acting plant by its design department are considered: the results of work are drawings corresponding with the national standards; large number of the small projects for different acting objects; variety of the types of the drawings in one project; large paper archive. The models and methods of developing of the complex CAD system with friend uniform environment of designing, with setting a profile of operations, with usage of the general parts of the project, with a series of problem-oriented subsystems are described on an example of a CAD system TechnoCAD GlassX",
        "published": "2004-12-08T08:52:28Z",
        "link": "http://arxiv.org/abs/cs/0412031v1",
        "categories": [
            "cs.CE",
            "I.2.1; J.6"
        ]
    },
    {
        "title": "The methods of support of the requirements of the Russian standards at   development of a CAD of industrial objects",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The methods of support of the requirements of the Russian standards in a CAD of industrial objects are explained, which were implemented in the CAD system TechnoCAD GlassX with an own graphics core and own structures of data storage. It is rotined, that the binding of storage structures and program code of a CAD to the requirements of standards enable not only to fulfil these requirements in project documentation, but also to increase a degree of compactness of storage of drawings both on the disk and in the RAM",
        "published": "2004-12-08T08:57:49Z",
        "link": "http://arxiv.org/abs/cs/0412032v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The modelling of the build constructions in a CAD of the renovation of   the enterprises by means of units in the drawings",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The parametric model of build constructions and features of design operations are described for making drawings, which are the common component of the different parts of the projects of renovation of enterprises. The key moment of the deep design automation is the using of so-called units in the drawings, which are joining a visible graphic part and invisible parameters. The model has passed check during designing of several hundreds of drawings",
        "published": "2004-12-08T09:01:20Z",
        "link": "http://arxiv.org/abs/cs/0412033v1",
        "categories": [
            "cs.CE",
            "E.2; I.2.1; J.6"
        ]
    },
    {
        "title": "The informatization of design works at industry firm during its   renovation",
        "authors": [
            "Vladimir V. Migunov"
        ],
        "summary": "The characteristic of design works on firm at its renovation and of the common directions of their informatization is given. The implantation of a CAD is selected as the key direction, and the requirements to a complex CAD-system are stated. The methods of such a CAD-system development are featured, and the connectedness of this development with the process of integration of information space of design department of the firm is characterized. The experience of development and implantation of a complex CAD of renovation of firms TechnoCAD GlassX lies in a basis of this reviewing",
        "published": "2004-12-08T11:08:48Z",
        "link": "http://arxiv.org/abs/cs/0412034v1",
        "categories": [
            "cs.CE",
            "I.2.1; J.6"
        ]
    },
    {
        "title": "Spin dependent transport of ``nonmagnetic metal/zigzag nanotube   encapsulating magnetic atoms/nonmagnetic metal'' junctions",
        "authors": [
            "Satoshi Kokado",
            "Kikuo Harigaya"
        ],
        "summary": "Towards a novel magnetoresistance (MR) device with a carbon nanotube, we propose ``nonmagnetic metal/zigzag nanotube encapsulating magnetic atoms/nonmagnetic metal'' junctions. We theoretically investigate how spin-polarized edges of the nanotube and the encapsulated magnetic atoms influence on transport. When the on-site Coulomb energy divided by the magnitude of transfer integral, $U/|t|$, is larger than 0.8, large MR effect due to the direction of spins of magnetic atoms, which has the magnitude of the MR ratio of about 100%, appears reflecting such spin-polarized edges.",
        "published": "2004-12-21T18:53:16Z",
        "link": "http://arxiv.org/abs/cond-mat/0412587v1",
        "categories": [
            "cond-mat.mes-hall",
            "cond-mat.mtrl-sci",
            "cs.CE",
            "physics.chem-ph",
            "quant-ph"
        ]
    },
    {
        "title": "Modelling financial markets by the multiplicative sequence of trades",
        "authors": [
            "Vygintas Gontis",
            "Bronislovas Kaulakys"
        ],
        "summary": "We introduce the stochastic multiplicative point process modelling trading activity of financial markets. Such a model system exhibits power-law spectral density S(f) ~ 1/f**beta, scaled as power of frequency for various values of beta between 0.5 and 2. Furthermore, we analyze the relation between the power-law autocorrelations and the origin of the power-law probability distribution of the trading activity. The model reproduces the spectral properties of trading activity and explains the mechanism of power-law distribution in real markets.",
        "published": "2004-12-28T08:53:11Z",
        "link": "http://arxiv.org/abs/cond-mat/0412723v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CE",
            "math.SP",
            "physics.data-an",
            "q-fin.ST"
        ]
    },
    {
        "title": "Cluster computing performances using virtual processors and mathematical   software",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper I describe some results on the use of virtual processors technology for parallelize some SPMD computational programs in a cluster environment. The tested technology is the INTEL Hyper Threading on real processors, and the programs are MATLAB 6.5 Release 13 scripts for floating points computation. By the use of this technology, I tested that a cluster can run with benefit a number of concurrent processes double the amount of physical processors. The conclusions of the work concern on the utility and limits of the used approach. The main result is that using virtual processors is a good technique for improving parallel programs not only for memory-based computations, but in the case of massive disk-storage operations too.",
        "published": "2004-01-09T12:09:51Z",
        "link": "http://arxiv.org/abs/cs/0401006v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "F.1.2; D.2.8"
        ]
    },
    {
        "title": "Algorithm xxx: Modified Bessel functions of imaginary order and positive   argument",
        "authors": [
            "Amparo Gil",
            "Javier Segura",
            "Nico M. Temme"
        ],
        "summary": "Fortran 77 programs for the computation of modified Bessel functions of purely imaginary order are presented. The codes compute the functions $K_{ia}(x)$, $L_{ia}(x)$ and their derivatives for real $a$ and positive $x$; these functions are independent solutions of the differential equation $x^2 w'' +x w' +(a^2 -x^2)w=0$. The code also computes exponentially scaled functions. The range of computation is $(x,a)\\in (0,1500]\\times [-1500,1500]$ when scaled functions are considered and it is larger than $(0,500]\\times [-400,400]$ for standard IEEE double precision arithmetic. The relative accuracy is better than $10^{-13}$ in the range $(0,200]\\times [-200,200]$ and close to $10^{-12}$ in $(0,1500]\\times [-1500,1500]$.",
        "published": "2004-01-13T14:29:26Z",
        "link": "http://arxiv.org/abs/cs/0401008v1",
        "categories": [
            "cs.MS",
            "cs.NA",
            "math.NA",
            "G.4"
        ]
    },
    {
        "title": "Directional Consistency for Continuous Numerical Constraints",
        "authors": [
            "Frederic Goualard",
            "Laurent Granvilliers"
        ],
        "summary": "Bounds consistency is usually enforced on continuous constraints by first decomposing them into binary and ternary primitives. This decomposition has long been shown to drastically slow down the computation of solutions. To tackle this, Benhamou et al. have introduced an algorithm that avoids formally decomposing constraints. Its better efficiency compared to the former method has already been experimentally demonstrated. It is shown here that their algorithm implements a strategy to enforce on a continuous constraint a consistency akin to Directional Bounds Consistency as introduced by Dechter and Pearl for discrete problems. The algorithm is analyzed in this framework, and compared with algorithms that enforce bounds consistency. These theoretical results are eventually contrasted with new experimental results on standard benchmarks from the interval constraint community.",
        "published": "2004-06-16T16:33:39Z",
        "link": "http://arxiv.org/abs/cs/0406025v1",
        "categories": [
            "cs.AI",
            "cs.MS"
        ]
    },
    {
        "title": "A Fast, Vectorizable Algorithm for Producing Single-Precision   Sine-Cosine Pairs",
        "authors": [
            "Marcus H. Mendenhall"
        ],
        "summary": "This paper presents an algorithm for computing Sine-Cosine pairs to modest accuracy, but in a manner which contains no conditional tests or branching, making it highly amenable to vectorization. An exemplary implementation for PowerPC AltiVec processors is included, but the algorithm should be easily portable to other achitectures, such as Intel SSE.",
        "published": "2004-06-25T20:37:44Z",
        "link": "http://arxiv.org/abs/cs/0406049v1",
        "categories": [
            "cs.MS"
        ]
    },
    {
        "title": "M@th Desktop and MD Tools - Mathematics and Mathematica Made Easy for   Students",
        "authors": [
            "Reinhold Kainhofer",
            "Reinhard V. Simonovits"
        ],
        "summary": "We present two add-ons for Mathematica for teaching mathematics to undergraduate and high school students. These two applications, M@th Desktop (MD) and M@th Desktop Tools (MDTools), include several palettes and notebooks covering almost every field. The underlying didactic concept is so-called \"blended learning\", in which these tools are meant to be used as a complement to the professor or teacher rather than as a replacement, which other e-learning applications do. They enable students to avoid the usual problem of computer-based learning, namely that too large an amount of time is wasted struggling with computer and program errors instead of actually learning the mathematical concepts.   M@th Desktop Tools is palette-based and provides easily accessible and user-friendly templates for the most important functions in the fields of Analysis, Algebra, Linear Algebra and Statistics. M@th Desktop, in contrast, is a modern, interactive teaching and learning software package for mathematics classes. It is comprised of modules for Differentiation, Integration, and Statistics, and each module presents its topic with a combination of interactive notebooks and palettes.   Both packages can be obtained from Deltasoft's homepage at http://www.deltasoft.at/ .",
        "published": "2004-07-20T13:46:09Z",
        "link": "http://arxiv.org/abs/cs/0407052v1",
        "categories": [
            "cs.MS",
            "K.3.1"
        ]
    },
    {
        "title": "Construction of Single-valued Solutions for Nonintegrable Systems with   the Help of the Painleve Test",
        "authors": [
            "S. Yu. Vernov"
        ],
        "summary": "The Painleve test is very useful to construct not only the Laurent-series solutions but also the elliptic and trigonometric ones. Such single-valued functions are solutions of some polynomial first order differential equations. To find the elliptic solutions we transform an initial nonlinear differential equation in a nonlinear algebraic system in parameters of the Laurent-series solutions of the initial equation. The number of unknowns in the obtained nonlinear system does not depend on number of arbitrary coefficients of the used first order equation. In this paper we describe the corresponding algorithm, which has been realized in REDUCE and Maple.",
        "published": "2004-07-27T12:51:53Z",
        "link": "http://arxiv.org/abs/nlin/0407062v1",
        "categories": [
            "nlin.SI",
            "astro-ph.SR",
            "cs.MS",
            "math-ph",
            "math.DS",
            "math.MP"
        ]
    },
    {
        "title": "Tsnnls: A solver for large sparse least squares problems with   non-negative variables",
        "authors": [
            "Jason Cantarella",
            "Michael Piatek"
        ],
        "summary": "The solution of large, sparse constrained least-squares problems is a staple in scientific and engineering applications. However, currently available codes for such problems are proprietary or based on MATLAB. We announce a freely available C implementation of the fast block pivoting algorithm of Portugal, Judice, and Vicente. Our version is several times faster than Matstoms' MATLAB implementation of the same algorithm. Further, our code matches the accuracy of MATLAB's built-in lsqnonneg function.",
        "published": "2004-08-13T21:25:42Z",
        "link": "http://arxiv.org/abs/cs/0408029v1",
        "categories": [
            "cs.MS",
            "G. 1 3"
        ]
    },
    {
        "title": "Mean and Variance Estimation by Kriging",
        "authors": [
            "Tomasz Suslo"
        ],
        "summary": "The aim of the paper is to derive the numerical least-squares estimator for mean and variance of random variable. In order to do so the following questions have to be answered: (i) what is the statistical model for the estimation procedure? (ii) what are the properties of the estimator, like optimality (in which class) or asymptotic properties? (iii) how does the estimator work in practice, how compared to competing estimators?",
        "published": "2004-09-17T10:39:45Z",
        "link": "http://arxiv.org/abs/cs/0409033v4",
        "categories": [
            "cs.NA",
            "cs.MS"
        ]
    },
    {
        "title": "An Example of Clifford Algebras Calculations with GiNaC",
        "authors": [
            "Vladimir V. Kisil"
        ],
        "summary": "This example of Clifford algebras calculations uses GiNaC (http://www.ginac.de/) library, which includes a support for generic Clifford algebra starting from version~1.3.0. Both symbolic and numeric calculation are possible and can be blended with other functions of GiNaC. This calculations was made for the paper math.CV/0410399.   Described features of GiNaC are already available at PyGiNaC (http://sourceforge.net/projects/pyginac/) and due to course should propagate into other software like GNU Octave (http://www.octave.org/), gTybalt (http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as their back-end.",
        "published": "2004-10-18T17:39:51Z",
        "link": "http://arxiv.org/abs/cs/0410044v5",
        "categories": [
            "cs.MS",
            "cs.CG",
            "cs.GR",
            "cs.SC"
        ]
    },
    {
        "title": "LSJK - a C++ library for arbitrary-precision numeric evaluation of the   generalized log-sine functions",
        "authors": [
            "M. Yu. Kalmykov",
            "A. Sheplyakov"
        ],
        "summary": "Generalized log-sine functions appear in higher order epsilon-expansion of different Feynman diagrams. We present an algorithm for numerical evaluation of these functions of real argument. This algorithm is implemented as C++ library with arbitrary-precision arithmetics for integer 0 < k < 9 and j > 1. Some new relations and representations for the generalized log-sine functions are given.",
        "published": "2004-11-07T17:13:45Z",
        "link": "http://arxiv.org/abs/hep-ph/0411100v2",
        "categories": [
            "hep-ph",
            "cs.MS",
            "cs.NA",
            "math-ph",
            "math.MP",
            "math.NA"
        ]
    },
    {
        "title": "Algebraic Elimination of epsilon-transitions",
        "authors": [
            "Gerard Duchamp",
            "Hatem Hadj Kacem",
            "Eric Laugerotte"
        ],
        "summary": "We present here algebraic formulas associating a k-automaton to a k-epsilon-automaton. The existence depends on the definition of the star of matrices and of elements in the semiring k. For this reason, we present the theorem which allows the transformation of k-epsilon-automata into k-automata. The two automata have the same behaviour.",
        "published": "2004-01-15T16:12:51Z",
        "link": "http://arxiv.org/abs/cs/0401012v2",
        "categories": [
            "cs.SC",
            "cs.DS",
            "H. 2"
        ]
    },
    {
        "title": "Polynomial-time computing over quadratic maps I: sampling in real   algebraic sets",
        "authors": [
            "Dima Grigoriev",
            "Dmitrii V. Pasechnik"
        ],
        "summary": "Given a quadratic map Q : K^n -> K^k defined over a computable subring D of a real closed field K, and a polynomial p(Y_1,...,Y_k) of degree d, we consider the zero set Z=Z(p(Q(X)),K^n) of the polynomial p(Q(X_1,...,X_n)). We present a procedure that computes, in (dn)^O(k) arithmetic operations in D, a set S of (real univariate representations of) sampling points in K^n that intersects nontrivially each connected component of Z. As soon as k=o(n), this is faster than the standard methods that all have exponential dependence on n in the complexity. In particular, our procedure is polynomial-time for constant k. In contrast, the best previously known procedure (due to A.Barvinok) is only capable of deciding in n^O(k^2) operations the nonemptiness (rather than constructing sampling points) of the set Z in the case of p(Y)=sum_i Y_i^2 and homogeneous Q.   A by-product of our procedure is a bound (dn)^O(k) on the number of connected components of Z.   The procedure consists of exact symbolic computations in D and outputs vectors of algebraic numbers. It involves extending K by infinitesimals and subsequent limit computation by a novel procedure that utilizes knowledge of an explicit isomorphism between real algebraic sets.",
        "published": "2004-03-06T08:20:38Z",
        "link": "http://arxiv.org/abs/cs/0403008v3",
        "categories": [
            "cs.SC",
            "cs.CG",
            "math.AG",
            "I.1.2; G.1.5"
        ]
    },
    {
        "title": "Efficient dot product over word-size finite fields",
        "authors": [
            "Jean-Guillaume Dumas"
        ],
        "summary": "We want to achieve efficiency for the exact computation of the dot product of two vectors over word-size finite fields. We therefore compare the practical behaviors of a wide range of implementation techniques using different representations. The techniques used include oating point representations, discrete logarithms, tabulations, Montgomery reduction, delayed modulus.",
        "published": "2004-04-05T07:24:15Z",
        "link": "http://arxiv.org/abs/cs/0404008v2",
        "categories": [
            "cs.SC",
            "F.2.4; B.2.4"
        ]
    },
    {
        "title": "Computing Multi-Homogeneous Bezout Numbers is Hard",
        "authors": [
            "Gregorio Malajovich",
            "Klaus Meer"
        ],
        "summary": "The multi-homogeneous Bezout number is a bound for the number of solutions of a system of multi-homogeneous polynomial equations, in a suitable product of projective spaces.   Given an arbitrary, not necessarily multi-homogeneous system, one can ask for the optimal multi-homogenization that would minimize the Bezout number.   In this paper, it is proved that the problem of computing, or even estimating the optimal multi-homogeneous Bezout number is actually NP-hard.   In terms of approximation theory for combinatorial optimization, the problem of computing the best multi-homogeneous structure does not belong to APX, unless P = NP.   Moreover, polynomial time algorithms for estimating the minimal multi-homogeneous Bezout number up to a fixed factor cannot exist even in a randomized setting, unless BPP contains NP.",
        "published": "2004-05-05T14:33:19Z",
        "link": "http://arxiv.org/abs/cs/0405021v1",
        "categories": [
            "cs.CC",
            "cs.SC",
            "F.2.1;G.1.5"
        ]
    },
    {
        "title": "An unexpected application of minimization theory to module   decompositions",
        "authors": [
            "Gerard Duchamp",
            "Hatem Hadj Kacem",
            "Eric Laugerotte"
        ],
        "summary": "The aim of this work is to show how we can decompose a module (if decomposable) into an indecomposable module with the help of the minimization process.",
        "published": "2004-05-17T18:44:59Z",
        "link": "http://arxiv.org/abs/cs/0405060v1",
        "categories": [
            "cs.SC",
            "H.2"
        ]
    },
    {
        "title": "A Framework for Combining Defeasible Argumentation with Labeled   Deduction",
        "authors": [
            "Carlos Iván Chesñevar",
            "Guillermo Ricardo Simari"
        ],
        "summary": "In the last years, there has been an increasing demand of a variety of logical systems, prompted mostly by applications of logic in AI and other related areas. Labeled Deductive Systems (LDS) were developed as a flexible methodology to formalize such a kind of complex logical systems. Defeasible argumentation has proven to be a successful approach to formalizing commonsense reasoning, encompassing many other alternative formalisms for defeasible reasoning. Argument-based frameworks share some common notions (such as the concept of argument, defeater, etc.) along with a number of particular features which make it difficult to compare them with each other from a logical viewpoint. This paper introduces LDSar, a LDS for defeasible argumentation in which many important issues concerning defeasible argumentation are captured within a unified logical framework. We also discuss some logical properties and extensions that emerge from the proposed framework.",
        "published": "2004-05-27T18:54:31Z",
        "link": "http://arxiv.org/abs/cs/0405107v1",
        "categories": [
            "cs.AI",
            "cs.SC"
        ]
    },
    {
        "title": "A novel approach to symbolic algebra",
        "authors": [
            "Thomas Fischbacher"
        ],
        "summary": "A prototype for an extensible interactive graphical term manipulation system is presented that combines pattern matching and nondeterministic evaluation to provide a convenient framework for doing tedious algebraic manipulations that so far had to be done manually in a semi-automatic fashion.",
        "published": "2004-06-02T12:13:55Z",
        "link": "http://arxiv.org/abs/cs/0406002v1",
        "categories": [
            "cs.SC",
            "G.4; I.1.3"
        ]
    },
    {
        "title": "Abstract Canonical Inference",
        "authors": [
            "Maria Paola Bonacina",
            "Nachum Dershowitz"
        ],
        "summary": "An abstract framework of canonical inference is used to explore how different proof orderings induce different variants of saturation and completeness. Notions like completion, paramodulation, saturation, redundancy elimination, and rewrite-system reduction are connected to proof orderings. Fairness of deductive mechanisms is defined in terms of proof orderings, distinguishing between (ordinary) \"fairness,\" which yields completeness, and \"uniform fairness,\" which yields saturation.",
        "published": "2004-06-17T10:13:04Z",
        "link": "http://arxiv.org/abs/cs/0406030v2",
        "categories": [
            "cs.LO",
            "cs.SC"
        ]
    },
    {
        "title": "On rational definite summation",
        "authors": [
            "Sergey P. Tsarev"
        ],
        "summary": "We present a partial proof of van Hoeij-Abramov conjecture about the algorithmic possibility of computation of finite sums of rational functions. The theoretical results proved in this paper provide an algorithm for computation of a large class of sums $ S(n) = \\sum_{k=0}^{n-1}R(k,n)$.",
        "published": "2004-07-24T14:11:57Z",
        "link": "http://arxiv.org/abs/cs/0407059v1",
        "categories": [
            "cs.SC",
            "cs.DM",
            "I.1.2"
        ]
    },
    {
        "title": "ParFORM: Parallel Version of the Symbolic Manipulation Program FORM",
        "authors": [
            "M. Tentyukov",
            "D. Fliegner",
            "M. Frank",
            "A. Onischenko",
            "A. Retey",
            "H. M. Staudenmaier",
            "J. A. M. Vermaseren"
        ],
        "summary": "After an introduction to the sequential version of FORM and the mechanisms behind, we report on the status of our project of parallelization. We have now a parallel version of FORM running on Cluster- and SMP-architectures. This version can be used to run arbitrary FORM programs in parallel.",
        "published": "2004-07-30T10:06:16Z",
        "link": "http://arxiv.org/abs/cs/0407066v1",
        "categories": [
            "cs.SC",
            "cs.DC",
            "hep-ph",
            "I.1; I.1.2; I.1.4"
        ]
    },
    {
        "title": "Maple+GrTensorII libraries for cosmology",
        "authors": [
            "Dumitru N. Vulcanov",
            "Valentina D. Vulcanov"
        ],
        "summary": "The article mainly presents some results in using MAPLE platform for computer algebra and GrTensorII package in doing calculations for theoretical and numerical cosmology",
        "published": "2004-09-04T12:52:22Z",
        "link": "http://arxiv.org/abs/cs/0409006v1",
        "categories": [
            "cs.SC",
            "gr-qc",
            "I.1; J.2"
        ]
    },
    {
        "title": "Efficient polynomial time algorithms computing industrial-strength   primitive roots",
        "authors": [
            "Jacques Dubrois",
            "Jean-Guillaume Dumas"
        ],
        "summary": "E. Bach, following an idea of T. Itoh, has shown how to build a small set of numbers modulo a prime p such that at least one element of this set is a generator of $\\pF{p}$\\cite{Bach:1997:sppr,Itoh:2001:PPR}. E. Bach suggests also that at least half of his set should be generators. We show here that a slight variant of this set can indeed be made to contain a ratio of primitive roots as close to 1 as necessary. We thus derive several algorithms computing primitive roots correct with very high probability in polynomial time. In particular we present an asymptotically $O^{\\sim}(\\sqrt{\\frac{1}{\\epsilon}}log^1.5(p) + \\log^2(p))$ algorithm providing primitive roots of $p$ with probability of correctness greater than $1-\\epsilon$ and several $O(log^\\alpha(p))$, $\\alpha \\leq 5.23$ algorithms computing \"Industrial-strength\" primitive roots with probabilities e.g. greater than the probability of \"hardware malfunctions\".",
        "published": "2004-09-14T15:43:15Z",
        "link": "http://arxiv.org/abs/cs/0409029v2",
        "categories": [
            "cs.SC",
            "math.NT"
        ]
    },
    {
        "title": "FORM Matters: Fast Symbolic Computation under UNIX",
        "authors": [
            "Michael M. Tung"
        ],
        "summary": "We give a brief introduction to FORM, a symbolic programming language for massive batch operations, designed by J.A.M. Vermaseren. In particular, we stress various methods to efficiently use FORM under the UNIX operating system. Several scripts and examples are given, and suggestions on how to use the vim editor as development platform.",
        "published": "2004-09-27T17:25:34Z",
        "link": "http://arxiv.org/abs/cs/0409048v1",
        "categories": [
            "cs.SC",
            "I.1; H.5.2"
        ]
    },
    {
        "title": "An Example of Clifford Algebras Calculations with GiNaC",
        "authors": [
            "Vladimir V. Kisil"
        ],
        "summary": "This example of Clifford algebras calculations uses GiNaC (http://www.ginac.de/) library, which includes a support for generic Clifford algebra starting from version~1.3.0. Both symbolic and numeric calculation are possible and can be blended with other functions of GiNaC. This calculations was made for the paper math.CV/0410399.   Described features of GiNaC are already available at PyGiNaC (http://sourceforge.net/projects/pyginac/) and due to course should propagate into other software like GNU Octave (http://www.octave.org/), gTybalt (http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as their back-end.",
        "published": "2004-10-18T17:39:51Z",
        "link": "http://arxiv.org/abs/cs/0410044v5",
        "categories": [
            "cs.MS",
            "cs.CG",
            "cs.GR",
            "cs.SC"
        ]
    },
    {
        "title": "Generators of algebraic curvature tensors based on a (2,1)-symmetry",
        "authors": [
            "Bernd Fiedler"
        ],
        "summary": "We consider generators of algebraic curvature tensors R which can be constructed by a Young symmetrization of product tensors U*w or w*U, where U and w are covariant tensors of order 3 and 1. We assume that U belongs to a class of the infinite set S of irreducible symmetry classes characterized by the partition (2,1). We show that the set S contains exactly one symmetry class S_0 whose elements U can not play the role of generators of tensors R. The tensors U of all other symmetry classes from S\\{S_0} can be used as generators for tensors R. Using Computer Algebra we search for such generators whose coordinate representations are polynomials with a minimal number of summands. For a generic choice of the symmetry class of U we obtain lengths of 8 summands. In special cases these numbers can be reduced to the minimum 4. If this minimum occurs then U admits an index commutation symmetry. Furthermore minimal lengths are possible if U is formed from torsion-free covariant derivatives of alternating 2-tensor fields. We apply ideals and idempotents of group rings C[S_r] of symmetric groups S_r, Young symmetrizers, discrete Fourier transforms and Littlewood-Richardson products. For symbolic calculations we used the Mathematica packages Ricci and PERMS.",
        "published": "2004-11-02T23:05:35Z",
        "link": "http://arxiv.org/abs/math/0411056v1",
        "categories": [
            "math.DG",
            "cs.SC",
            "math.CO",
            "53B20, 15A72, 05E10, 16D60, 05-04"
        ]
    },
    {
        "title": "From Tensor Equations to Numerical Code -- Computer Algebra Tools for   Numerical Relativity",
        "authors": [
            "Christiane Lechner",
            "Dana Alic",
            "Sascha Husa"
        ],
        "summary": "In this paper we present our recent work in developing a computer-algebra tool for systems of partial differential equations (PDEs), termed \"Kranc\". Our work is motivated by the problem of finding solutions of the Einstein equations through numerical simulations. Kranc consists of Mathematica based computer-algebra packages, that facilitate the task of dealing with symbolic tensorial calculations and realize the conversion of systems of partial differential evolution equations into parallelized C or Fortran code.",
        "published": "2004-11-17T14:30:18Z",
        "link": "http://arxiv.org/abs/cs/0411063v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Jordan Normal and Rational Normal Form Algorithms",
        "authors": [
            "Bernard Parisse",
            "Morgane Vaughan"
        ],
        "summary": "In this paper, we present a determinist Jordan normal form algorithms based on the Fadeev formula: \\[(\\lambda \\cdot I-A) \\cdot B(\\lambda)=P(\\lambda) \\cdot I\\] where $B(\\lambda)$ is $(\\lambda \\cdot I-A)$'s comatrix and $P(\\lambda)$ is $A$'s characteristic polynomial. This rational Jordan normal form algorithm differs from usual algorithms since it is not based on the Frobenius/Smith normal form but rather on the idea already remarked in Gantmacher that the non-zero column vectors of $B(\\lambda_0)$ are eigenvectors of $A$ associated to $\\lambda_0$ for any root $\\lambda_0$ of the characteristical polynomial. The complexity of the algorithm is $O(n^4)$ field operations if we know the factorization of the characteristic polynomial (or $O(n^5 \\ln(n))$ operations for a matrix of integers of fixed size). This algorithm has been implemented using the Maple and Giac/Xcas computer algebra systems.",
        "published": "2004-12-02T09:39:30Z",
        "link": "http://arxiv.org/abs/cs/0412005v1",
        "categories": [
            "cs.SC",
            "MSC2000 15A21 68W30"
        ]
    },
    {
        "title": "Free quasi-symmetric functions, product actions and quantum field theory   of partitions",
        "authors": [
            "Gerard Henry Edmond Duchamp",
            "Jean-Gabriel Luque",
            "Karol A. Penson",
            "Christophe Tollu"
        ],
        "summary": "We examine two associative products over the ring of symmetric functions related to the intransitive and Cartesian products of permutation groups. As an application, we give an enumeration of some Feynman type diagrams arising in Bender's QFT of partitions. We end by exploring possibilities to construct noncommutative analogues.",
        "published": "2004-12-13T19:37:11Z",
        "link": "http://arxiv.org/abs/cs/0412061v1",
        "categories": [
            "cs.SC",
            "math.CO",
            "quant-ph"
        ]
    },
    {
        "title": "Cluster computing performances using virtual processors and mathematical   software",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper I describe some results on the use of virtual processors technology for parallelize some SPMD computational programs in a cluster environment. The tested technology is the INTEL Hyper Threading on real processors, and the programs are MATLAB 6.5 Release 13 scripts for floating points computation. By the use of this technology, I tested that a cluster can run with benefit a number of concurrent processes double the amount of physical processors. The conclusions of the work concern on the utility and limits of the used approach. The main result is that using virtual processors is a good technique for improving parallel programs not only for memory-based computations, but in the case of massive disk-storage operations too.",
        "published": "2004-01-09T12:09:51Z",
        "link": "http://arxiv.org/abs/cs/0401006v1",
        "categories": [
            "cs.DC",
            "cs.MS",
            "F.1.2; D.2.8"
        ]
    },
    {
        "title": "On the Evolution of Time Horizons in Parallel and Grid Simulations",
        "authors": [
            "L. N. Shchur",
            "M. A. Novotny"
        ],
        "summary": "We analyze the evolution of the local simulation times (LST) in Parallel Discrete Event Simulations. The new ingredients introduced are i) we associate the LST with the nodes and not with the processing elements, and 2) we propose to minimize the exchange of information between different processing elements by freezing the LST on the boundaries between processing elements for some time of processing and then releasing them by a wide-stream memory exchange between processing elements. Highlights of our approach are i) it keeps the highest level of processor time utilization during the algorithm evolution, ii) it takes a reasonable time for the memory exchange excluding the time-consuming and complicated process of message exchange between processors, and iii) the communication between processors is decoupled from the calculations performed on a processor. The effectiveness of our algorithm grows with the number of nodes (or threads). This algorithm should be applicable for any parallel simulation with short-range interactions, including parallel or grid simulations of partial differential equations.",
        "published": "2004-01-14T06:54:57Z",
        "link": "http://arxiv.org/abs/cond-mat/0401229v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DC"
        ]
    },
    {
        "title": "ClassdescMP: Easy MPI programming in C++",
        "authors": [
            "Russell K. Standish",
            "Duraid Madina"
        ],
        "summary": "ClassdescMP is a distributed memory parallel programming system for use with C++ and MPI. It uses the Classdesc reflection system to ease the task of building complicated messages to be sent between processes. It doesn't hide the underlying MPI API, so it is an augmentation of MPI capabilities. Users can still call standard MPI function calls if needed for performance reasons.",
        "published": "2004-01-27T03:55:27Z",
        "link": "http://arxiv.org/abs/cs/0401027v1",
        "categories": [
            "cs.DC",
            "D.1.3;D.1.5;D.2.3;D.2.12"
        ]
    },
    {
        "title": "A Knowledge-Theoretic Analysis of Uniform Distributed Coordination and   Failure Detectors",
        "authors": [
            "Joseph Y. Halpern",
            "Aleta Ricciardi"
        ],
        "summary": "It is shown that, in a precise sense, if there is no bound on the number of faulty processes in a system with unreliable but fair communication, Uniform Distributed Coordination (UDC) can be attained if and only if a system has perfect failure detectors. This result is generalized to the case where there is a bound t on the number of faulty processes. It is shown that a certain type of generalized failure detector is necessary and sufficient for achieving UDC in a context with at most t faulty processes. Reasoning about processes' knowledge as to which other processes are faulty plays a key role in the analysis.",
        "published": "2004-02-05T17:15:08Z",
        "link": "http://arxiv.org/abs/cs/0402012v1",
        "categories": [
            "cs.DC",
            "C.2.2, C.2.4, F.3.1"
        ]
    },
    {
        "title": "Alchemi: A .NET-based Grid Computing Framework and its Integration into   Global Grids",
        "authors": [
            "Akshay Luther",
            "Rajkumar Buyya",
            "Rajiv Ranjan",
            "Srikumar Venugopal"
        ],
        "summary": "Computational grids that couple geographically distributed resources are becoming the de-facto computing platform for solving large-scale problems in science, engineering, and commerce. Software to enable grid computing has been primarily written for Unix-class operating systems, thus severely limiting the ability to effectively utilize the computing resources of the vast majority of desktop computers i.e. those running variants of the Microsoft Windows operating system. Addressing Windows-based grid computing is particularly important from the software industry's viewpoint where interest in grids is emerging rapidly. Microsoft's .NET Framework has become near-ubiquitous for implementing commercial distributed systems for Windows-based platforms, positioning it as the ideal platform for grid computing in this context. In this paper we present Alchemi, a .NET-based grid computing framework that provides the runtime machinery and programming environment required to construct desktop grids and develop grid applications. It allows flexible application composition by supporting an object-oriented grid application programming model in addition to a grid job model. Cross-platform support is provided via a web services interface and a flexible execution model supports dedicated and non-dedicated (voluntary) execution by grid nodes.",
        "published": "2004-02-10T09:18:07Z",
        "link": "http://arxiv.org/abs/cs/0402017v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "P2P Networks for Content Sharing",
        "authors": [
            "Choon Hoong Ding",
            "Sarana Nutanong",
            "Rajkumar Buyya"
        ],
        "summary": "Peer-to-peer (P2P) technologies have been widely used for content sharing, popularly called \"file-swapping\" networks. This chapter gives a broad overview of content sharing P2P technologies. It starts with the fundamental concept of P2P computing followed by the analysis of network topologies used in peer-to-peer systems. Next, three milestone peer-to-peer technologies: Napster, Gnutella, and Fasttrack are explored in details, and they are finally concluded with the comparison table in the last section.",
        "published": "2004-02-10T14:24:48Z",
        "link": "http://arxiv.org/abs/cs/0402018v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Efficient and Scalable Barrier over Quadrics and Myrinet with a New   NIC-Based Collective Message Passing Protocol",
        "authors": [
            "Weikuan Yu",
            "Darius Buntinas",
            "Rich L. Graham",
            "Dhabaleswar K. Panda"
        ],
        "summary": "Modern interconnects often have programmable processors in the network interface that can be utilized to offload communication processing from host CPU. In this paper, we explore different schemes to support collective operations at the network interface and propose a new collective protocol. With barrier as an initial case study, we have demontrated that much of the communication processing can be greatly simplified with this collective protocol. Accordingly, %with our proposed collective processing scheme, we have designed and implemented efficient and scalable NIC-based barrier operations over two high performance interconnects, Quadrics and Myrinet.   Our evaluation shows that, over a Quadrics cluster of 8 nodes with ELan3 Network, the NIC-based barrier operation achieves a barrier latency of only 5.60$\\mu$s. This result is a 2.48 factor of improvement over the Elanlib tree-based barrier operation. Over a Myrinet cluster of 8 nodes with LANai-XP NIC cards, a barrier latency of 14.20$\\mu$s over 8 nodes is achieved. This is a 2.64 factor of improvement over the host-based barrier algorithm. Furthermore, an analytical model developed for the proposed scheme indicates that a NIC-based barrier operation on a 1024-node cluster can be performed with only 22.13$\\mu$s latency over Quadrics and with 38.94$\\mu$s latency over Myrinet. These results indicate the potential for developing high performance communication subsystems for next generation clusters.",
        "published": "2004-02-12T20:37:26Z",
        "link": "http://arxiv.org/abs/cs/0402027v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "B.4.3; C.1.4; C.2.4"
        ]
    },
    {
        "title": "Predictable Software -- A Shortcut to Dependable Computing ?",
        "authors": [
            "George Candea"
        ],
        "summary": "Many dependability techniques expect certain behaviors from the underlying subsystems and fail in chaotic ways if these expectations are not met. Under expected circumstances, however, software tends to work quite well. This paper suggests that, instead of fixing elusive bugs or rewriting software, we improve the predictability of conditions faced by our programs. This approach might be a cheaper and faster way to improve dependability of software. After identifying some of the common triggers of unpredictability, the paper describes three engineering principles that hold promise in combating unpredictability, suggests a way to benchmark predictability, and outlines a brief research agenda.",
        "published": "2004-03-11T04:01:51Z",
        "link": "http://arxiv.org/abs/cs/0403013v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "D.4.7"
        ]
    },
    {
        "title": "Belle Computing System",
        "authors": [
            "Ichiro Adachi",
            "Taisuke Hibino",
            "Luc Hinz",
            "Ryosuke Itoh",
            "Nobu Katayama",
            "Shohei Nishida",
            "Frederic Ronga",
            "Toshifumi Tsukamoto",
            "Masahiko Yokoyama"
        ],
        "summary": "We describe the present status of the computing system in the Belle experiment at the KEKB $e^+e^-$ asymmetric-energy collider. So far, we have logged more than 160 fb$^{-1}$ of data, corresponding to the world's largest data sample of 170M $B\\bar{B}$ pairs at the $\\Upsilon(4S)$ energy region. A large amount of event data has to be processed to produce an analysis event sample in a timely fashion. In addition, Monte Carlo events have to be created to control systematic errors accurately. This requires stable and efficient usage of computing resources. Here we review our computing model and then describe how we efficiently proceed DST/MC productions in our system.",
        "published": "2004-03-11T09:20:20Z",
        "link": "http://arxiv.org/abs/cs/0403015v2",
        "categories": [
            "cs.DC",
            "B.8.2;C.4"
        ]
    },
    {
        "title": "Distributed Computing Economics",
        "authors": [
            "Jim Gray"
        ],
        "summary": "Computing economics are changing. Today there is rough price parity between (1) one database access, (2) ten bytes of network traffic, (3) 100,000 instructions, (4) 10 bytes of disk storage, and (5) a megabyte of disk bandwidth. This has implications for how one structures Internet-scale distributed computing: one puts computing as close to the data as possible in order to avoid expensive network traffic.",
        "published": "2004-03-12T10:05:00Z",
        "link": "http://arxiv.org/abs/cs/0403019v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "K.6.0"
        ]
    },
    {
        "title": "Roughening of the (1+1) interfaces in two-component surface growth with   an admixture of random deposition",
        "authors": [
            "A. Kolakowska",
            "M. A. Novotny",
            "P. S. Verma"
        ],
        "summary": "We simulate competitive two-component growth on a one dimensional substrate of $L$ sites. One component is a Poisson-type deposition that generates Kardar-Parisi-Zhang (KPZ) correlations. The other is random deposition (RD). We derive the universal scaling function of the interface width for this model and show that the RD admixture acts as a dilatation mechanism to the fundamental time and height scales, but leaves the KPZ correlations intact. This observation is generalized to other growth models. It is shown that the flat-substrate initial condition is responsible for the existence of an early non-scaling phase in the interface evolution. The length of this initial phase is a non-universal parameter, but its presence is universal. In application to parallel and distributed computations, the important consequence of the derived scaling is the existence of the upper bound for the desynchronization in a conservative update algorithm for parallel discrete-event simulations. It is shown that such algorithms are generally scalable in a ring communication topology.",
        "published": "2004-03-13T21:05:04Z",
        "link": "http://arxiv.org/abs/cond-mat/0403341v2",
        "categories": [
            "cond-mat.mtrl-sci",
            "cs.DC",
            "cs.OH",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Tycoon: A Distributed Market-based Resource Allocation System",
        "authors": [
            "Kevin Lai",
            "Bernardo A. Huberman",
            "Leslie Fine"
        ],
        "summary": "P2P clusters like the Grid and PlanetLab enable in principle the same statistical multiplexing efficiency gains for computing as the Internet provides for networking. The key unsolved problem is resource allocation. Existing solutions are not economically efficient and require high latency to acquire resources. We designed and implemented Tycoon, a market based distributed resource allocation system based on an Auction Share scheduling algorithm. Preliminary results show that Tycoon achieves low latency and high fairness while providing incentives for truth-telling on the part of strategic users.",
        "published": "2004-04-05T19:54:30Z",
        "link": "http://arxiv.org/abs/cs/0404013v1",
        "categories": [
            "cs.DC",
            "cs.MA",
            "C.2.4; D.4.1; D.4.7; K.6.0"
        ]
    },
    {
        "title": "A Modular and Fault-Tolerant Data Transport Framework",
        "authors": [
            "Timm M. Steinbeck"
        ],
        "summary": "The High Level Trigger (HLT) of the future ALICE heavy-ion experiment has to reduce its input data rate of up to 25 GB/s to at most 1.25 GB/s for output before the data is written to permanent storage. To cope with these data rates a large PC cluster system is being designed to scale to several 1000 nodes, connected by a fast network. For the software that will run on these nodes a flexible data transport and distribution software framework, described in this thesis, has been developed. The framework consists of a set of separate components, that can be connected via a common interface. This allows to construct different configurations for the HLT, that are even changeable at runtime. To ensure a fault-tolerant operation of the HLT, the framework includes a basic fail-over mechanism that allows to replace whole nodes after a failure. The mechanism will be further expanded in the future, utilizing the runtime reconnection feature of the framework's component interface. To connect cluster nodes a communication class library is used that abstracts from the actual network technology and protocol used to retain flexibility in the hardware choice. It contains already two working prototype versions for the TCP protocol as well as SCI network adapters. Extensions can be added to the library without modifications to other parts of the framework. Extensive tests and measurements have been performed with the framework. Their results as well as conclusions drawn from them are also presented in this thesis. Performance tests show very promising results for the system, indicating that it can fulfill ALICE's requirements concerning the data transport.",
        "published": "2004-04-06T13:35:55Z",
        "link": "http://arxiv.org/abs/cs/0404014v1",
        "categories": [
            "cs.DC",
            "D.1.3; C.2.4; C.4; J.2"
        ]
    },
    {
        "title": "The study of distributed computing algorithms by multithread   applications",
        "authors": [
            "Ahmet A. Husainov"
        ],
        "summary": "The material in this note is used as an introduction to distributed algorithms in a four year course on software and automatic control system in the computer technology department of the Komsomolsk-on-Amur state technical university. All our the program examples are written in Borland C/C++ 5.02 for Windows 95/98/2000/NT/XP, and hence suit to compile and execute by Visual C/C++. We consider the following approaches of the distributed computing: the conversion of recursive algorithms to multithread applications, a realization of the pairing algorithm, the building of wave systems by Petri nets and object oriented programming.",
        "published": "2004-04-07T01:27:32Z",
        "link": "http://arxiv.org/abs/cs/0404015v2",
        "categories": [
            "cs.DC",
            "68M14, 68Q85",
            "C.1.4; D.1.3"
        ]
    },
    {
        "title": "The Gridbus Toolkit for Service Oriented Grid and Utility Computing: An   Overview and Status Report",
        "authors": [
            "Rajkumar Buyya",
            "Srikumar Venugopal"
        ],
        "summary": "Grids aim at exploiting synergies that result from cooperation of autonomous distributed entities. The synergies that result from grid cooperation include the sharing, exchange, selection, and aggregation of geographically distributed resources such as computers, data bases, software, and scientific instruments for solving large-scale problems in science, engineering, and commerce. For this cooperation to be sustainable, participants need to have economic incentive. Therefore, \"incentive\" mechanisms should be considered as one of key design parameters of Grid architectures. In this article, we present an overview and status of an open source Grid toolkit, called Gridbus, whose architecture is fundamentally driven by the requirements of Grid economy. Gridbus technologies provide services for both computational and data grids that power the emerging eScience and eBusiness applications.",
        "published": "2004-04-13T09:16:54Z",
        "link": "http://arxiv.org/abs/cs/0404027v1",
        "categories": [
            "cs.DC",
            "C.2.4, C.1.4, C.2.1"
        ]
    },
    {
        "title": "The Effect of Faults on Network Expansion",
        "authors": [
            "Amitabha Bagchi",
            "Ankur Bhargava",
            "Amitabh Chaudhary",
            "David Eppstein",
            "Christian Scheideler"
        ],
        "summary": "In this paper we study the problem of how resilient networks are to node faults. Specifically, we investigate the question of how many faults a network can sustain so that it still contains a large (i.e. linear-sized) connected component that still has approximately the same expansion as the original fault-free network. For this we apply a pruning technique which culls away parts of the faulty network which have poor expansion. This technique can be applied to both adversarial faults and to random faults. For adversarial faults we prove that for every network with expansion alpha, a large connected component with basically the same expansion as the original network exists for up to a constant times alpha n faults. This result is tight in the sense that every graph G of size n and uniform expansion alpha(.), i.e. G has an expansion of alpha(n) and every subgraph G' of size m of G has an expansion of O(alpha(m)), can be broken into sublinear components with omega(alpha(n) n) faults.   For random faults we observe that the situation is significantly different, because in this case the expansion of a graph only gives a very weak bound on its resilience to random faults. More specifically, there are networks of uniform expansion O(sqrt{n}) that are resilient against a constant fault probability but there are also networks of uniform expansion Omega(1/log n) that are not resilient against a O(1/log n) fault probability. Thus, a different parameter is needed. For this we introduce the span of a graph which allows us to determine the maximum fault probability in a much better way than the expansion can. We use the span to show the first known results for the effect of random faults on the expansion of d-dimensional meshes.",
        "published": "2004-04-13T18:33:36Z",
        "link": "http://arxiv.org/abs/cs/0404029v1",
        "categories": [
            "cs.DC",
            "cs.DM",
            "C.2; G.2.2"
        ]
    },
    {
        "title": "Using matrices in post-processing phase of CFD simulations",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this work I present a technique of construction and fast evaluation of a family of cubic polynomials for analytic smoothing and graphical rendering of particles trajectories for flows in a generic geometry. The principal result of the work was implementation and test of a method for interpolating 3D points by regular parametric curves and their fast and efficient evaluation for a good resolution of rendering. For the purpose I have used a parallel environment using a multiprocessor cluster architecture. The efficiency of the used method is good, mainly reducing the number of floating-points computations by caching the numerical values of some line-parameter's powers, and reducing the necessity of communication among processes. This work has been developed for the Research and Development Department of my company for planning advanced customized models of industrial burners.",
        "published": "2004-04-22T15:33:52Z",
        "link": "http://arxiv.org/abs/cs/0404047v1",
        "categories": [
            "cs.NA",
            "cs.DC",
            "physics.comp-ph",
            "C.1.4; D.1.3; G.1.0"
        ]
    },
    {
        "title": "Bi-criteria Algorithm for Scheduling Jobs on Cluster Platforms",
        "authors": [
            "Pierre-Francois Dutot",
            "Lionel Eyraud",
            "Grégory Mounié",
            "Denis Trystram"
        ],
        "summary": "We describe in this paper a new method for building an efficient algorithm for scheduling jobs in a cluster. Jobs are considered as parallel tasks (PT) which can be scheduled on any number of processors. The main feature is to consider two criteria that are optimized together. These criteria are the makespan and the weighted minimal average completion time (minsum). They are chosen for their complementarity, to be able to represent both user-oriented objectives and system administrator objectives. We propose an algorithm based on a batch policy with increasing batch sizes, with a smart selection of jobs in each batch. This algorithm is assessed by intensive simulation results, compared to a new lower bound (obtained by a relaxation of ILP) of the optimal schedules for both criteria separately. It is currently implemented in an actual real-size cluster platform.",
        "published": "2004-05-04T14:51:55Z",
        "link": "http://arxiv.org/abs/cs/0405006v3",
        "categories": [
            "cs.DC",
            "cs.DS",
            "ACM F.2.2, ACM D.4.1"
        ]
    },
    {
        "title": "A Grid Service Broker for Scheduling Distributed Data-Oriented   Applications on Global Grids",
        "authors": [
            "Srikumar Venugopal",
            "Rajkumar Buyya",
            "Lyle Winton"
        ],
        "summary": "The next generation of scientific experiments and studies, popularly called as e-Science, is carried out by large collaborations of researchers distributed around the world engaged in analysis of huge collections of data generated by scientific instruments. Grid computing has emerged as an enabler for e-Science as it permits the creation of virtual organizations that bring together communities with common objectives. Within a community, data collections are stored or replicated on distributed resources to enhance storage capability or efficiency of access. In such an environment, scientists need to have the ability to carry out their studies by transparently accessing distributed data and computational resources. In this paper, we propose and develop a Grid broker that mediates access to distributed resources by (a) discovering suitable data sources for a given analysis scenario, (b) suitable computational resources, (c) optimally mapping analysis jobs to resources, (d) deploying and monitoring job execution on selected resources, (e) accessing data from local or remote data source during job execution and (f) collating and presenting results. The broker supports a declarative and dynamic parametric programming model for creating grid applications. We have used this model in grid-enabling a high energy physics analysis application (Belle Analysis Software Framework). The broker has been used in deploying Belle experiment data analysis jobs on a grid testbed, called Belle Analysis Data Grid, having resources distributed across Australia interconnected through GrangeNet.",
        "published": "2004-05-06T10:15:26Z",
        "link": "http://arxiv.org/abs/cs/0405023v1",
        "categories": [
            "cs.DC",
            "C.1.4"
        ]
    },
    {
        "title": "Supervisory Control of Fuzzy Discrete Event Systems",
        "authors": [
            "Yongzhi Cao",
            "Mingsheng Ying"
        ],
        "summary": "In order to cope with situations in which a plant's dynamics are not precisely known, we consider the problem of supervisory control for a class of discrete event systems modelled by fuzzy automata. The behavior of such discrete event systems is described by fuzzy languages; the supervisors are event feedback and can disable only controllable events with any degree. The concept of discrete event system controllability is thus extended by incorporating fuzziness. In this new sense, we present a necessary and sufficient condition for a fuzzy language to be controllable. We also study the supremal controllable fuzzy sublanguage and the infimal controllable fuzzy superlanguage when a given pre-specified desired fuzzy language is uncontrollable. Our framework generalizes that of Ramadge-Wonham and reduces to Ramadge-Wonham framework when membership grades in all fuzzy languages must be either 0 or 1. The theoretical development is accompanied by illustrative numerical examples.",
        "published": "2004-05-12T03:35:35Z",
        "link": "http://arxiv.org/abs/cs/0405040v1",
        "categories": [
            "cs.DM",
            "cs.DC",
            "G.3;I.6.8"
        ]
    },
    {
        "title": "A Distributed TDMA Slot Assignment Algorithm for Wireless Sensor   Networks",
        "authors": [
            "T. Herman",
            "S. Tixeuil"
        ],
        "summary": "Wireless sensor networks benefit from communication protocols that reduce power requirements by avoiding frame collision. Time Division Media Access methods schedule transmission in slots to avoid collision, however these methods often lack scalability when implemented in \\emph{ad hoc} networks subject to node failures and dynamic topology. This paper reports a distributed algorithm for TDMA slot assignment that is self-stabilizing to transient faults and dynamic topology change. The expected local convergence time is O(1) for any size network satisfying a constant bound on the size of a node neighborhood.",
        "published": "2004-05-12T14:53:29Z",
        "link": "http://arxiv.org/abs/cs/0405042v2",
        "categories": [
            "cs.DC",
            "cs.NI",
            "C.2.1; C.2.5; H.3.4"
        ]
    },
    {
        "title": "Synchronous Relaxation for Parallel Ising Spin Simulations",
        "authors": [
            "Boris Lubachevsky",
            "Alan Weiss"
        ],
        "summary": "A new parallel algorithm for simulating Ising spin systems is presented. The sequential prototype is the n-fold way algorithm cite{BKL75}, which is efficient but is hard to parallelize using conservative methods. Our parallel algorithm is optimistic. Unlike other optimistic algorithms, e.g., Time Warp, our algorithm is synchronous. It also belongs to the class of simulations known as ``relaxation'' cite{CS8 hence it is named ``synchronous relaxation.'' We derive performance guarantees for this algorithm. If N is the number of PEs, then under weak assumptions we show that the number of correct events processed per unit of time is, on average, at least of order N/log(N). All communication delays, processing time, and busy waits are taken into account.",
        "published": "2004-05-17T01:32:16Z",
        "link": "http://arxiv.org/abs/cs/0405053v1",
        "categories": [
            "cs.DC",
            "cond-mat.mtrl-sci",
            "cs.DS",
            "physics.comp-ph",
            "D.1.3; D.4.1; D.4.8; I.6.8"
        ]
    },
    {
        "title": "Neighborhood-Based Topology Recognition in Sensor Networks",
        "authors": [
            "Sandor P. Fekete",
            "Alexander Kroeller",
            "Dennis Pfisterer",
            "Stefan Fischer",
            "Carsten Buschmann"
        ],
        "summary": "We consider a crucial aspect of self-organization of a sensor network consisting of a large set of simple sensor nodes with no location hardware and only very limited communication range. After having been distributed randomly in a given two-dimensional region, the nodes are required to develop a sense for the environment, based on a limited amount of local communication. We describe algorithmic approaches for determining the structure of boundary nodes of the region, and the topology of the region. We also develop methods for determining the outside boundary, the distance to the closest boundary for each point, the Voronoi diagram of the different boundaries, and the geometric thickness of the network. Our methods rely on a number of natural assumptions that are present in densely distributed sets of nodes, and make use of a combination of stochastics, topology, and geometry. Evaluation requires only a limited number of simple local computations.",
        "published": "2004-05-17T12:16:08Z",
        "link": "http://arxiv.org/abs/cs/0405058v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "C.2.1; F.2.2; G.3"
        ]
    },
    {
        "title": "Observability and Decentralized Control of Fuzzy Discrete Event Systems",
        "authors": [
            "Yongzhi Cao",
            "Mingsheng Ying"
        ],
        "summary": "Fuzzy discrete event systems as a generalization of (crisp) discrete event systems have been introduced in order that it is possible to effectively represent uncertainty, imprecision, and vagueness arising from the dynamic of systems. A fuzzy discrete event system has been modelled by a fuzzy automaton; its behavior is described in terms of the fuzzy language generated by the automaton. In this paper, we are concerned with the supervisory control problem for fuzzy discrete event systems with partial observation. Observability, normality, and co-observability of crisp languages are extended to fuzzy languages. It is shown that the observability, together with controllability, of the desired fuzzy language is a necessary and sufficient condition for the existence of a partially observable fuzzy supervisor. When a decentralized solution is desired, it is proved that there exist local fuzzy supervisors if and only if the fuzzy language to be synthesized is controllable and co-observable. Moreover, the infimal controllable and observable fuzzy superlanguage, and the supremal controllable and normal fuzzy sublanguage are also discussed. Simple examples are provided to illustrate the theoretical development.",
        "published": "2004-05-20T08:19:26Z",
        "link": "http://arxiv.org/abs/cs/0405068v2",
        "categories": [
            "cs.DM",
            "cs.DC",
            "G.3; I.6.8"
        ]
    },
    {
        "title": "Grid Databases for Shared Image Analysis in the MammoGrid Project",
        "authors": [
            "S. R. Amendolia",
            "F. Estrella",
            "T. Hauer",
            "D. Manset",
            "R. McClatchey",
            "M. Odeh",
            "T. Reading",
            "D. Rogulin",
            "D. Schottlander",
            "T. Solomonides"
        ],
        "summary": "The MammoGrid project aims to prove that Grid infrastructures can be used for collaborative clinical analysis of database-resident but geographically distributed medical images. This requires: a) the provision of a clinician-facing front-end workstation and b) the ability to service real-world clinician queries across a distributed and federated database. The MammoGrid project will prove the viability of the Grid by harnessing its power to enable radiologists from geographically dispersed hospitals to share standardized mammograms, to compare diagnoses (with and without computer aided detection of tumours) and to perform sophisticated epidemiological studies across national boundaries. This paper outlines the approach taken in MammoGrid to seamlessly connect radiologist workstations across a Grid using an \"information infrastructure\" and a DICOM-compliant object model residing in multiple distributed data stores in Italy and the UK",
        "published": "2004-05-21T13:24:00Z",
        "link": "http://arxiv.org/abs/cs/0405072v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "MammoGrid: A Service Oriented Architecture based Medical Grid   Application",
        "authors": [
            "S R Amendolia",
            "F Estrella",
            "W Hassan",
            "T Hauer",
            "D Manset",
            "R McClatchey",
            "D Rogulin",
            "T Solomonides"
        ],
        "summary": "The MammoGrid project has recently delivered its first proof-of-concept prototype using a Service-Oriented Architecture (SOA)-based Grid application to enable distributed computing spanning national borders. The underlying AliEn Grid infrastructure has been selected because of its practicality and because of its emergence as a potential open source standards-based solution for managing and coordinating distributed resources. The resultant prototype is expected to harness the use of huge amounts of medical image data to perform epidemiological studies, advanced image processing, radiographic education and ultimately, tele-diagnosis over communities of medical virtual organisations. The MammoGrid prototype comprises a high-quality clinician visualization workstation used for data acquisition and inspection, a DICOM-compliant interface to a set of medical services (annotation, security, image analysis, data storage and querying services) residing on a so-called Grid-box and secure access to a network of other Grid-boxes connected through Grid middleware. This paper outlines the MammoGrid approach in managing a federation of Grid-connected mammography databases in the context of the recently delivered prototype and will also describe the next phase of prototyping.",
        "published": "2004-05-21T13:41:27Z",
        "link": "http://arxiv.org/abs/cs/0405074v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H2.4;J.3;C2.4"
        ]
    },
    {
        "title": "Fast Simulation of Multicomponent Dynamic Systems",
        "authors": [
            "Boris D. Lubachevsky"
        ],
        "summary": "A computer simulation has to be fast to be helpful, if it is employed to study the behavior of a multicomponent dynamic system. This paper discusses modeling concepts and algorithmic techniques useful for creating such fast simulations. Concrete examples of simulations that range from econometric modeling to communications to material science are used to illustrate these techniques and concepts. The algorithmic and modeling methods discussed include event-driven processing, ``anticipating'' data structures, and ``lazy'' evaluation, Poisson dispenser, parallel processing by cautious advancements and by synchronous relaxations. The paper gives examples of how these techniques and models are employed in assessing efficiency of capacity management methods in wireless and wired networks, in studies of magnetization, crystalline structure, and sediment formation in material science, in studies of competition in economics.",
        "published": "2004-05-22T16:51:48Z",
        "link": "http://arxiv.org/abs/cs/0405077v2",
        "categories": [
            "cs.DS",
            "cond-mat.mtrl-sci",
            "cs.DC",
            "I.6.8; G.3; G.4; J.2; J.4"
        ]
    },
    {
        "title": "A New Dynamical Domain Decomposition Method for Parallel Molecular   Dynamics Simulation on Grid",
        "authors": [
            "Vasilii Zhakhovskii",
            "Katsunobu Nishihara",
            "Yuko Fukuda",
            "Shinji Shimojo"
        ],
        "summary": "We develop a new Lagrangian material particle -- dynamical domain decomposition method (MPD^3) for large scale parallel molecular dynamics (MD) simulation of nonstationary heterogeneous systems on a heterogeneous computing net. MPD^3 is based on Voronoi decomposition of simulated matter. The map of Voronoi polygons is known as the Dirichlet tessellation and used for grid generation in computational fluid dynamics. From the hydrodynamics point of view the moving Voronoi polygon looks as a material particle (MP). MPs can exchange particles and information. To balance heterogeneous computing conditions the MP centers should be dependent on timing data. We propose a simple and efficient iterative algorithm which based on definition of the timing-dependent balancing displacement of MP center for next simulation step.   The MPD^3 program was tested in various computing environments and physical problems. We have demonstrated that MPD^3 is a high-adaptive decomposition algorithm for MD simulation. It was shown that the well-balanced decomposition can result from dynamical Voronoi polygon tessellation. One would expect the similar approach can be successfully applied for other particle methods like Monte Carlo, particle-in-cell, and smooth-particle-hydrodynamics.",
        "published": "2004-05-24T07:29:13Z",
        "link": "http://arxiv.org/abs/cs/0405086v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Grid Information Infrastructure for Medical Image Analysis",
        "authors": [
            "D Rogulin",
            "F Estrella",
            "T Hauer",
            "R McClatchey",
            "T Solomonides"
        ],
        "summary": "The storage and manipulation of digital images and the analysis of the information held in those images are essential requirements for next-generation medical information systems. The medical community has been exploring collaborative approaches for managing image data and exchanging knowledge and Grid technology [1] is a promising approach to enabling distributed analysis across medical institutions and for developing new collaborative and cooperative approaches for image analysis without the necessity for clinicians to co-locate. The EU-funded MammoGrid project [2] is one example of this and it aims to develop a Europe-wide database of mammograms to support effective co-working between healthcare professionals across the EU. The MammoGrid prototype comprises a high-quality clinician visualization workstation (for data acquisition and inspection), a DICOM-compliant interface to a set of medical services (annotation, security, image analysis, data storage and querying services) residing on a so-called Grid-box and secure access to a network of other Grid-boxes connected through Grid middleware. One of the main deliverables of the project is a Grid-enabled infrastructure that manages federated mammogram databases across Europe. This paper outlines the MammoGrid Information Infrastructure (MII) for meta-data analysis and knowledge discovery in the medical imaging domain.",
        "published": "2004-05-24T10:15:45Z",
        "link": "http://arxiv.org/abs/cs/0405087v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Microreboot -- A Technique for Cheap Recovery",
        "authors": [
            "George Candea",
            "Shinichi Kawamoto",
            "Yuichi Fujiki",
            "Greg Friedman",
            "Armando Fox"
        ],
        "summary": "A significant fraction of software failures in large-scale Internet systems are cured by rebooting, even when the exact failure causes are unknown. However, rebooting can be expensive, causing nontrivial service disruption or downtime even when clusters and failover are employed. In this work we separate process recovery from data recovery to enable microrebooting -- a fine-grain technique for surgically recovering faulty application components, without disturbing the rest of the application.   We evaluate microrebooting in an Internet auction system running on an application server. Microreboots recover most of the same failures as full reboots, but do so an order of magnitude faster and result in an order of magnitude savings in lost work. This cheap form of recovery engenders a new approach to high availability: microreboots can be employed at the slightest hint of failure, prior to node failover in multi-node clusters, even when mistakes in failure detection are likely; failure and recovery can be masked from end users through transparent call-level retries; and systems can be rejuvenated by parts, without ever being shut down.",
        "published": "2004-06-02T21:54:50Z",
        "link": "http://arxiv.org/abs/cs/0406005v3",
        "categories": [
            "cs.OS",
            "cs.DC",
            "D.4.0"
        ]
    },
    {
        "title": "Parallel Mixed Bayesian Optimization Algorithm: A Scaleup Analysis",
        "authors": [
            "Jiri Ocenasek",
            "Martin Pelikan"
        ],
        "summary": "Estimation of Distribution Algorithms have been proposed as a new paradigm for evolutionary optimization. This paper focuses on the parallelization of Estimation of Distribution Algorithms. More specifically, the paper discusses how to predict performance of parallel Mixed Bayesian Optimization Algorithm (MBOA) that is based on parallel construction of Bayesian networks with decision trees. We determine the time complexity of parallel Mixed Bayesian Optimization Algorithm and compare this complexity with experimental results obtained by solving the spin glass optimization problem. The empirical results fit well the theoretical time complexity, so the scalability and efficiency of parallel Mixed Bayesian Optimization Algorithm for unknown instances of spin glass benchmarks can be predicted. Furthermore, we derive the guidelines that can be used to design effective parallel Estimation of Distribution Algorithms with the speedup proportional to the number of variables in the problem.",
        "published": "2004-06-03T15:43:46Z",
        "link": "http://arxiv.org/abs/cs/0406007v1",
        "categories": [
            "cs.NE",
            "cs.DC",
            "G.1.6; G.3; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Fluctuation in Peer-to-Peer Networks: Mitigating Its Effect on DHT   Performance",
        "authors": [
            "Dietrich Fahrenholtz",
            "Volker Turau"
        ],
        "summary": "Due to the transient nature of peers, any Peer-to-Peer network is in peril to falling apart if peers do not receive routing table updates periodically. To this end, maintenance, which affects every peer, ensures connectedness and sustained data operation performance. However, a high rate of change in peer population usually incurs lots of network maintenance messages and can severely degrade overall performance. We discuss three methods how to tackle and mitigate the effect of peer fluctuation on a tree-based distributed hash table.",
        "published": "2004-06-16T17:32:57Z",
        "link": "http://arxiv.org/abs/cs/0406027v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Cheap Recovery: A Key to Self-Managing State",
        "authors": [
            "Andrew C. Huang",
            "Armando Fox"
        ],
        "summary": "Cluster hash tables (CHTs) are a key persistent-storage component of many large-scale Internet services due to their high performance and scalability. We show that a correctly-designed CHT can also be as easy to manage as a farm of stateless servers. Specifically, we trade away some consistency to obtain reboot-based recovery that is simple, maintains full data availability, and only has modest impact on performance. This simplifies management in two ways. First, it simplifies failure detection by lowering the cost of acting on false positives, allowing us to use simple but aggressive statistical techniques to quickly detect potential failures and node degradations; even when a false alarm is raised or when rebooting will not fix the problem, attempting recovery by rebooting is relatively non-intrusive to system availability and performance. Second, it allows us to re-cast online repartitioning as failure plus recovery, simplifying dynamic scaling and capacity planning. These properties make it possible for the system to be continuously self-adjusting, a key property of self-managing, autonomic systems.",
        "published": "2004-06-23T18:08:50Z",
        "link": "http://arxiv.org/abs/cs/0406046v1",
        "categories": [
            "cs.NI",
            "cs.DC"
        ]
    },
    {
        "title": "Global Grids and Software Toolkits: A Study of Four Grid Middleware   Technologies",
        "authors": [
            "Parvin Asadzadeh",
            "Rajkumar Buyya",
            "Chun Ling Kei",
            "Deepa Nayar",
            "Srikumar Venugopal"
        ],
        "summary": "Grid is an infrastructure that involves the integrated and collaborative use of computers, networks, databases and scientific instruments owned and managed by multiple organizations. Grid applications often involve large amounts of data and/or computing resources that require secure resource sharing across organizational boundaries. This makes Grid application management and deployment a complex undertaking. Grid middlewares provide users with seamless computing ability and uniform access to resources in the heterogeneous Grid environment. Several software toolkits and systems have been developed, most of which are results of academic research projects, all over the world. This chapter will focus on four of these middlewares--UNICORE, Globus, Legion and Gridbus. It also presents our implementation of a resource broker for UNICORE as this functionality was not supported in it. A comparison of these systems on the basis of the architecture, implementation model and several other features is included.",
        "published": "2004-07-01T11:54:06Z",
        "link": "http://arxiv.org/abs/cs/0407001v1",
        "categories": [
            "cs.DC",
            "C.1.4"
        ]
    },
    {
        "title": "A Taxonomy and Survey of Grid Resource Planning and Reservation Systems   for Grid Enabled Analysis Environment",
        "authors": [
            "Arshad Ali",
            "Ashiq Anjum",
            "Atif Mehmood",
            "Richard McClatchey",
            "Ian Willers",
            "Julian Bunn",
            "Harvey Newman",
            "Michael Thomas",
            "Conrad Steenberg"
        ],
        "summary": "The concept of coupling geographically distributed resources for solving large scale problems is becoming increasingly popular forming what is popularly called grid computing. Management of resources in the Grid environment becomes complex as the resources are geographically distributed, heterogeneous in nature and owned by different individuals and organizations each having their own resource management policies and different access and cost models. There have been many projects that have designed and implemented the resource management systems with a variety of architectures and services. In this paper we have presented the general requirements that a Resource Management system should satisfy. The taxonomy has also been defined based on which survey of resource management systems in different existing Grid projects has been conducted to identify the key areas where these systems lack the desired functionality.",
        "published": "2004-07-05T15:48:43Z",
        "link": "http://arxiv.org/abs/cs/0407012v2",
        "categories": [
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Distributed Analysis and Load Balancing System for Grid Enabled Analysis   on Hand-held devices using Multi-Agents Systems",
        "authors": [
            "Naveed Ahmad",
            "Arshad Ali",
            "Ashiq Anjum",
            "Tahir Azim",
            "Julian Bunn",
            "Ali Hassan",
            "Ahsan Ikram",
            "Frank van Lingen",
            "Richard McClatchey",
            "Harvey Newman",
            "Conrad Steenberg",
            "Michael Thomas",
            "Ian Willers"
        ],
        "summary": "Handheld devices, while growing rapidly, are inherently constrained and lack the capability of executing resource hungry applications. This paper presents the design and implementation of distributed analysis and load-balancing system for hand-held devices using multi-agents system. This system enables low resource mobile handheld devices to act as potential clients for Grid enabled applications and analysis environments. We propose a system, in which mobile agents will transport, schedule, execute and return results for heavy computational jobs submitted by handheld devices. Moreover, in this way, our system provides high throughput computing environment for hand-held devices.",
        "published": "2004-07-05T16:08:36Z",
        "link": "http://arxiv.org/abs/cs/0407013v1",
        "categories": [
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "A Grid-enabled Interface to Condor for Interactive Analysis on Handheld   and Resource-limited Devices",
        "authors": [
            "Arshad Ali",
            "Ashiq Anjum",
            "Tahir Azim",
            "Julian Bunn",
            "Ahsan Ikram",
            "Richard McClatchey",
            "Harvey Newman",
            "Conrad Steenberg",
            "Michael Thomas",
            "Ian Willers"
        ],
        "summary": "This paper was withdrawn by the authors.",
        "published": "2004-07-05T19:40:00Z",
        "link": "http://arxiv.org/abs/cs/0407014v2",
        "categories": [
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "A Low Cost Distributed Computing Approach to Pulsar Searches at a Small   College",
        "authors": [
            "Andrew Cantino",
            "Fronefield Crawford",
            "Saurav Dhital",
            "John P. Dougherty",
            "Reid Sherman"
        ],
        "summary": "We describe a distributed processing cluster of inexpensive Linux machines developed jointly by the Astronomy and Computer Science departments at Haverford College which has been successfully used to search a large volume of data from a recent radio pulsar survey. Analysis of radio pulsar surveys requires significant computational resources to handle the demanding data storage and processing needs. One goal of this project was to explore issues encountered when processing a large amount of pulsar survey data with limited computational resources. This cluster, which was developed and activated in only a few weeks by supervised undergraduate summer research students, used existing decommissioned computers, the campus network, and a script-based, client-oriented, self-scheduled data distribution approach to process the data. This setup provided simplicity, efficiency, and \"on-the-fly\" scalability at low cost. The entire 570 GB data set from the pulsar survey was processed at Haverford over the course of a ten-week summer period using this cluster. We conclude that this cluster can serve as a useful computational model in cases where data processing must be carried out on a limited budget. We have also constructed a DVD archive of the raw survey data in order to investigate the feasibility of using DVD as an inexpensive and easily accessible raw data storage format for pulsar surveys. DVD-based storage has not been widely explored in the pulsar community, but it has several advantages. The DVD archive we have constructed is reliable, portable, inexpensive, and can be easily read by any standard modern machine.",
        "published": "2004-07-07T18:55:20Z",
        "link": "http://arxiv.org/abs/cs/0407017v1",
        "categories": [
            "cs.DC",
            "J.2"
        ]
    },
    {
        "title": "Design of a Parallel and Distributed Web Search Engine",
        "authors": [
            "Salvatore Orlando",
            "Raffaele Perego",
            "Fabrizio Silvestri"
        ],
        "summary": "This paper describes the architecture of MOSE (My Own Search Engine), a scalable parallel and distributed engine for searching the web. MOSE was specifically designed to efficiently exploit affordable parallel architectures, such as clusters of workstations. Its modular and scalable architecture can easily be tuned to fulfill the bandwidth requirements of the application at hand. Both task-parallel and data-parallel approaches are exploited within MOSE in order to increase the throughput and efficiently use communication, storing and computational resources. We used a collection of html documents as a benchmark, and conducted preliminary experiments on a cluster of three SMP Linux PCs.",
        "published": "2004-07-21T07:21:50Z",
        "link": "http://arxiv.org/abs/cs/0407053v1",
        "categories": [
            "cs.IR",
            "cs.DC"
        ]
    },
    {
        "title": "PELCR: Parallel Environment for Optimal Lambda-Calculus Reduction",
        "authors": [
            "M. Pedicini",
            "F. Quaglia"
        ],
        "summary": "In this article we present the implementation of an environment supporting L\\'evy's \\emph{optimal reduction} for the $\\lambda$-calculus \\cite{Lev78} on parallel (or distributed) computing systems. In a similar approach to Lamping's one in \\cite{Lamping90}, we base our work on a graph reduction technique known as \\emph{directed virtual reduction} \\cite{DPR97} which is actually a restriction of Danos-Regnier virtual reduction \\cite{DanosRegnier93}.   The environment, which we refer to as PELCR (Parallel Environment for optimal Lambda-Calculus Reduction) relies on a strategy for directed virtual reduction, namely {\\em half combustion}, which we introduce in this article. While developing PELCR we have adopted both a message aggregation technique, allowing a reduction of the communication overhead, and a fair policy for distributing dynamically originated load among processors.   We also present an experimental study demonstrating the ability of PELCR to definitely exploit parallelism intrinsic to $\\lambda$-terms while performing the reduction. By the results we show how PELCR allows achieving up to 70/80% of the ideal speedup on last generation multiprocessor computing systems. As a last note, the software modules have been developed with the {\\tt C} language and using a standard interface for message passing, i.e. MPI, thus making PELCR itself a highly portable software package.",
        "published": "2004-07-22T17:52:39Z",
        "link": "http://arxiv.org/abs/cs/0407055v2",
        "categories": [
            "cs.LO",
            "cs.DC",
            "F.4.1"
        ]
    },
    {
        "title": "Communication-Aware Processor Allocation for Supercomputers",
        "authors": [
            "Michael A. Bender",
            "David P. Bunde",
            "Erik D. Demaine",
            "Sandor P. Fekete",
            "Vitus J. Leung",
            "Henk Meijer",
            "Cynthia A. Phillips"
        ],
        "summary": "This paper gives processor-allocation algorithms for minimizing the average number of communication hops between the assigned processors for grid architectures, in the presence of occupied cells. The simpler problem of assigning processors on a free grid has been studied by Karp, McKellar, and Wong who show that the solutions have nontrivial structure; they left open the complexity of the problem.   The associated clustering problem is as follows: Given n points in Re^d, find k points that minimize their average pairwise L1 distance. We present a natural approximation algorithm and show that it is a 7/4-approximation for 2D grids. For d-dimensional space, the approximation guarantee is 2-(1/2d), which is tight. We also give a polynomial-time approximation scheme (PTAS) for constant dimension d, and report on experimental results.",
        "published": "2004-07-24T13:40:26Z",
        "link": "http://arxiv.org/abs/cs/0407058v2",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; C.1.4"
        ]
    },
    {
        "title": "Performance Analysis of the Globus Toolkit Monitoring and Discovery   Service, MDS2",
        "authors": [
            "Xuehai Zhang",
            "Jennifer M. Schopf"
        ],
        "summary": "Monitoring and information services form a key component of a distributed system, or Grid. A quantitative study of such services can aid in understanding the performance limitations, advise in the deployment of the monitoring system, and help evaluate future development work. To this end, we examined the performance of the Globus Toolkit(reg. trdmrk) Monitoring and Discovery Service (MDS2) by instrumenting its main services using NetLogger. Our study shows a strong advantage to caching or prefetching the data, as well as the need to have primary components at well-connected sites.",
        "published": "2004-07-28T14:07:58Z",
        "link": "http://arxiv.org/abs/cs/0407062v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "H.3.4; H.5.3"
        ]
    },
    {
        "title": "ParFORM: Parallel Version of the Symbolic Manipulation Program FORM",
        "authors": [
            "M. Tentyukov",
            "D. Fliegner",
            "M. Frank",
            "A. Onischenko",
            "A. Retey",
            "H. M. Staudenmaier",
            "J. A. M. Vermaseren"
        ],
        "summary": "After an introduction to the sequential version of FORM and the mechanisms behind, we report on the status of our project of parallelization. We have now a parallel version of FORM running on Cluster- and SMP-architectures. This version can be used to run arbitrary FORM programs in parallel.",
        "published": "2004-07-30T10:06:16Z",
        "link": "http://arxiv.org/abs/cs/0407066v1",
        "categories": [
            "cs.SC",
            "cs.DC",
            "hep-ph",
            "I.1; I.1.2; I.1.4"
        ]
    },
    {
        "title": "Lock-Free and Practical Deques using Single-Word Compare-And-Swap",
        "authors": [
            "Håkan Sundell",
            "Philippas Tsigas"
        ],
        "summary": "We present an efficient and practical lock-free implementation of a concurrent deque that is disjoint-parallel accessible and uses atomic primitives which are available in modern computer systems. Previously known lock-free algorithms of deques are either based on non-available atomic synchronization primitives, only implement a subset of the functionality, or are not designed for disjoint accesses. Our algorithm is based on a doubly linked list, and only requires single-word compare-and-swap atomic primitives, even for dynamic memory sizes. We have performed an empirical study using full implementations of the most efficient algorithms of lock-free deques known. For systems with low concurrency, the algorithm by Michael shows the best performance. However, as our algorithm is designed for disjoint accesses, it performs significantly better on systems with high concurrency and non-uniform memory architecture.",
        "published": "2004-08-05T14:17:01Z",
        "link": "http://arxiv.org/abs/cs/0408016v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "E.1"
        ]
    },
    {
        "title": "Performance Characterisation of Intra-Cluster Collective Communications",
        "authors": [
            "Luiz Angelo Barchet-Estefanel",
            "Gregory Mounie"
        ],
        "summary": "Although recent works try to improve collective communication in grid systems by separating intra and inter-cluster communication, the optimisation of communications focus only on inter-cluster communications. We believe, instead, that the overall performance of the application may be improved if intra-cluster collective communications performance is known in advance. Hence, it is important to have an accurate model of the intra-cluster collective communications, which provides the necessary evidences to tune and to predict their performance correctly. In this paper we present our experience on modelling such communication strategies. We describe and compare different implementation strategies with their communication models, evaluating the models' accuracy and describing the practical challenges that can be found when modelling collective communications.",
        "published": "2004-08-14T06:31:10Z",
        "link": "http://arxiv.org/abs/cs/0408032v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Identifying Logical Homogeneous Clusters for Efficient Wide-area   Communications",
        "authors": [
            "Luiz Angelo Barchet-Estefanel",
            "Gregory Mounie"
        ],
        "summary": "Recently, many works focus on the implementation of collective communication operations adapted to wide area computational systems, like computational Grids or global-computing. Due to the inherently heterogeneity of such environments, most works separate \"clusters\" in different hierarchy levels. to better model the communication. However, in our opinion, such works do not give enough attention to the delimitation of such clusters, as they normally use the locality or the IP subnet from the machines to delimit a cluster without verifying the \"homogeneity\" of such clusters. In this paper, we describe a strategy to gather network information from different local-area networks and to construct \"logical homogeneous clusters\", better suited to the performance modelling.",
        "published": "2004-08-14T07:39:57Z",
        "link": "http://arxiv.org/abs/cs/0408033v3",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Fast Tuning of Intra-Cluster Collective Communications",
        "authors": [
            "Luiz Angelo Barchet-Estefanel",
            "Gregory Mounie"
        ],
        "summary": "Recent works try to optimise collective communication in grid systems focusing mostly on the optimisation of communications among different clusters. We believe that intra-cluster collective communications should also be optimised, as a way to improve the overall efficiency and to allow the construction of multi-level collective operations. Indeed, inside homogeneous clusters, a simple optimisation approach rely on the comparison from different implementation strategies, through their communication models. In this paper we evaluate this approach, comparing different implementation strategies with their predicted performances. As a result, we are able to choose the communication strategy that better adapts to each network environment.",
        "published": "2004-08-14T07:40:33Z",
        "link": "http://arxiv.org/abs/cs/0408034v3",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Monitoring, Analyzing, and Controlling Internet-scale Systems with ACME",
        "authors": [
            "David Oppenheimer",
            "Vitaliy Vatkovskiy",
            "Hakim Weatherspoon",
            "Jason Lee",
            "David A. Patterson",
            "John Kubiatowicz"
        ],
        "summary": "Analyzing and controlling large distributed services under a wide range of conditions is difficult. Yet these capabilities are essential to a number of important development and operational tasks such as benchmarking, testing, and system management. To facilitate these tasks, we have built the Application Control and Monitoring Environment (ACME), a scalable, flexible infrastructure for monitoring, analyzing, and controlling Internet-scale systems. ACME consists of two parts. ISING, the Internet Sensor In-Network agGregator, queries sensors and aggregates the results as they are routed through an overlay network. ENTRIE, the ENgine for TRiggering Internet Events, uses the data streams supplied by ISING, in combination with a user's XML configuration file, to trigger actuators such as killing processes during a robustness benchmark or paging a system administrator when predefined anomalous conditions are observed. In this paper we describe the design, implementation, and evaluation of ACME and its constituent parts. We find that for a 512-node system running atop an emulated Internet topology, ISING's use of in-network aggregation can reduce end-to-end query-response latency by more than 50% compared to using either direct network connections or the same overlay network without aggregation. We also find that an untuned implementation of ACME can invoke an actuator on one or all nodes in response to a discrete or aggregate event in less than four seconds, and we illustrate ACME's applicability to concrete benchmarking and monitoring scenarios.",
        "published": "2004-08-14T16:05:47Z",
        "link": "http://arxiv.org/abs/cs/0408035v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Consensus on Transaction Commit",
        "authors": [
            "Jim Gray",
            "Leslie Lamport"
        ],
        "summary": "The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. Running a Paxos consensus algorithm on the commit/abort decision of each participant yields a transaction commit protocol that uses 2F +1 coordinators and makes progress if at least F +1 of them are working. In the fault-free case, this algorithm requires one extra message delay but has the same stable-storage write delay as Two-Phase Commit. The classic Two-Phase Commit algorithm is obtained as the special F = 0 case of the general Paxos Commit algorithm.",
        "published": "2004-08-14T21:23:41Z",
        "link": "http://arxiv.org/abs/cs/0408036v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.4"
        ]
    },
    {
        "title": "Medians and Beyond: New Aggregation Techniques for Sensor Networks",
        "authors": [
            "Nisheeth Shrivastava",
            "Chiranjeeb Buragohain",
            "Divyakant Agrawal",
            "Subhash Suri"
        ],
        "summary": "Wireless sensor networks offer the potential to span and monitor large geographical areas inexpensively. Sensors, however, have significant power constraint (battery life), making communication very expensive. Another important issue in the context of sensor-based information systems is that individual sensor readings are inherently unreliable. In order to address these two aspects, sensor database systems like TinyDB and Cougar enable in-network data aggregation to reduce the communication cost and improve reliability. The existing data aggregation techniques, however, are limited to relatively simple types of queries such as SUM, COUNT, AVG, and MIN/MAX. In this paper we propose a data aggregation scheme that significantly extends the class of queries that can be answered using sensor networks. These queries include (approximate) quantiles, such as the median, the most frequent data values, such as the consensus value, a histogram of the data distribution, as well as range queries. In our scheme, each sensor aggregates the data it has received from other sensors into a fixed (user specified) size message. We provide strict theoretical guarantees on the approximation quality of the queries in terms of the message size. We evaluate the performance of our aggregation scheme by simulation and demonstrate its accuracy, scalability and low resource utilization for highly variable input data sets.",
        "published": "2004-08-17T02:21:06Z",
        "link": "http://arxiv.org/abs/cs/0408039v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Probabilistic heuristics for disseminating information in networks",
        "authors": [
            "A. O. Stauffer",
            "V. C. Barbosa"
        ],
        "summary": "We study the problem of disseminating a piece of information through all the nodes of a network, given that it is known originally only to a single node. In the absence of any structural knowledge on the network other than the nodes' neighborhoods, this problem is traditionally solved by flooding all the network's edges. We analyze a recently introduced probabilistic algorithm for flooding and give an alternative probabilistic heuristic that can lead to some cost-effective improvements, like better trade-offs between the message and time complexities involved. We analyze the two algorithms both mathematically and by means of simulations, always within a random-graph framework and considering relevant node-degree distributions.",
        "published": "2004-09-01T16:42:11Z",
        "link": "http://arxiv.org/abs/cs/0409001v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.2; F.2.2"
        ]
    },
    {
        "title": "Near Optimal Routing for Small-World Networks with Augmented Local   Awareness",
        "authors": [
            "Jianyang Zeng",
            "Wen-Jing Hsu",
            "Jiangdian Wang"
        ],
        "summary": "In order to investigate the routing aspects of small-world networks, Kleinberg proposes a network model based on a $d$-dimensional lattice with long-range links chosen at random according to the $d$-harmonic distribution. Kleinberg shows that the greedy routing algorithm by using only local information performs in $O(\\log^2 n)$ expected number of hops, where $n$ denotes the number of nodes in the network. Martel and Nguyen have found that the expected diameter of Kleinberg's small-world networks is $\\Theta(\\log n)$. Thus a question arises naturally: Can we improve the routing algorithms to match the diameter of the networks while keeping the amount of information stored on each node as small as possible? We extend Kleinberg's model and add three augmented local links for each node: two of which are connected to nodes chosen randomly and uniformly within $\\log^2 n$ Mahattan distance, and the third one is connected to a node chosen randomly and uniformly within $\\log n$ Mahattan distance. We show that if each node is aware of $O(\\log n)$ number of neighbors via the augmented local links, there exist both non-oblivious and oblivious algorithms that can route messages between any pair of nodes in $O(\\log n \\log \\log n)$ expected number of hops, which is a near optimal routing complexity and outperforms the other related results for routing in Kleinberg's small-world networks. Our schemes keep only $O(\\log^2 n)$ bits of routing information on each node, thus they are scalable with the network size. Besides adding new light to the studies of social networks, our results may also find applications in the design of large-scale distributed networks, such as peer-to-peer systems, in the same spirit of Symphony.",
        "published": "2004-09-09T03:41:48Z",
        "link": "http://arxiv.org/abs/cs/0409017v3",
        "categories": [
            "cs.DM",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Aperiodic Tilings: Breaking Translational Symmetry",
        "authors": [
            "Leonid A. Levin"
        ],
        "summary": "Classical results on aperiodic tilings are rather complicated and not widely understood. Below, an alternative approach is discussed in hope to provide additional intuition not apparent in classical works.",
        "published": "2004-09-13T19:46:50Z",
        "link": "http://arxiv.org/abs/cs/0409024v5",
        "categories": [
            "cs.DM",
            "cs.DC",
            "F.1.1; G.2.1"
        ]
    },
    {
        "title": "Desynchronization and Speedup in an Asynchronous Conservative Parallel   Update Protocol",
        "authors": [
            "A. Kolakowska",
            "M. A. Novotny"
        ],
        "summary": "In a state-update protocol for a system of $L$ asynchronous parallel processes that communicate only with nearest neighbors, global desynchronization in operation times can be deduced from kinetic roughening of the corresponding virtual-time horizon (VTH). The utilization of the parallel processing environment can be deduced by analyzing the microscopic structure of the VTH. We give an overview of how the methods of non-equilibrium surface growth (physics of complex systems) can be applied to uncover some properties of state update algorithms used in distributed parallel discrete-event simulations (PDES). In particular, we focus on the asynchronous conservative PDES algorithm in a ring communication topology. The time evolution of its VTH is simulated numerically as asynchronous cellular automaton whose update rule corresponds to the update rule followed by this algorithm. We give theoretical estimates of the performance as a function of $L$ and the load per processor, i.e., approximate formulas for the mean speedup and for the desynchronization. It is established that, for a given simulation size, there is a theoretical upper bound for the desynchronization and a theoretical non-zero lower bound for the utilization. The new approach to performance studies, outlined in this chapter, is particularly useful in the search for the design of a new-generation of algorithms that would efficiently carry out an autonomous or tunable synchronization.",
        "published": "2004-09-16T06:40:54Z",
        "link": "http://arxiv.org/abs/cs/0409032v1",
        "categories": [
            "cs.DC",
            "cond-mat.mtrl-sci",
            "physics.comp-ph",
            "D.1.3; F.2.0"
        ]
    },
    {
        "title": "Parallel Computing Environments and Methods for Power Distribution   System Simulation",
        "authors": [
            "Ning Lu",
            "Z. Todd Taylor",
            "David P. Chassin",
            "Ross T. Guttromson",
            "R. Scott Studham"
        ],
        "summary": "The development of cost-effective highperformance parallel computing on multi-processor supercomputers makes it attractive to port excessively time consuming simulation software from personal computers (PC) to super computes. The power distribution system simulator (PDSS) takes a bottom-up approach and simulates load at the appliance level, where detailed thermal models for appliances are used. This approach works well for a small power distribution system consisting of a few thousand appliances. When the number of appliances increases, the simulation uses up the PC memory and its runtime increases to a point where the approach is no longer feasible to model a practical large power distribution system. This paper presents an effort made to port a PC-based power distribution system simulator to a 128-processor shared-memory supercomputer. The paper offers an overview of the parallel computing environment and a description of the modification made to the PDSS model. The performance of the PDSS running on a standalone PC and on the supercomputer is compared. Future research direction of utilizing parallel computing in the power distribution system simulation is also addressed.",
        "published": "2004-09-18T17:09:26Z",
        "link": "http://arxiv.org/abs/cs/0409035v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.MA",
            "cs.PF"
        ]
    },
    {
        "title": "Topology of biological networks and reliability of information   processing",
        "authors": [
            "Konstantin Klemm",
            "Stefan Bornholdt"
        ],
        "summary": "Biological systems rely on robust internal information processing: Survival depends on highly reproducible dynamics of regulatory processes. Biological information processing elements, however, are intrinsically noisy (genetic switches, neurons, etc.). Such noise poses severe stability problems to system behavior as it tends to desynchronize system dynamics (e.g. via fluctuating response or transmission time of the elements). Synchronicity in parallel information processing is not readily sustained in the absence of a central clock. Here we analyze the influence of topology on synchronicity in networks of autonomous noisy elements. In numerical and analytical studies we find a clear distinction between non-reliable and reliable dynamical attractors, depending on the topology of the circuit. In the reliable cases, synchronicity is sustained, while in the unreliable scenario, fluctuating responses of single elements can gradually desynchronize the system, leading to non-reproducible behavior. We find that the fraction of reliable dynamical attractors strongly correlates with the underlying circuitry. Our model suggests that the observed motif structure of biological signaling networks is shaped by the biological requirement for reproducibility of attractors.",
        "published": "2004-09-20T13:46:08Z",
        "link": "http://arxiv.org/abs/q-bio/0409022v1",
        "categories": [
            "q-bio.MN",
            "cond-mat.dis-nn",
            "cs.DC"
        ]
    },
    {
        "title": "A Shared Write-protected Root Filesystem for a Group of Networked   Clients",
        "authors": [
            "Ignatios Souvatzis"
        ],
        "summary": "A method to boot a cluster of diskless network clients from a single write-protected NFS root file system is shown. The problems encountered when first implementing the setup and their solution are discussed. Finally, the setup is briefly compared to using a kernel-embedded root file system.",
        "published": "2004-10-04T14:49:00Z",
        "link": "http://arxiv.org/abs/cs/0410007v2",
        "categories": [
            "cs.OS",
            "cs.DC",
            "C.2.4; D.4.7"
        ]
    },
    {
        "title": "Diffusive Load Balancing of Loosely-Synchronous Parallel Programs over   Peer-to-Peer Networks",
        "authors": [
            "Scott Douglas",
            "Aaron Harwood"
        ],
        "summary": "The use of under-utilized Internet resources is widely recognized as a viable form of high performance computing. Sustained processing power of roughly 40T FLOPS using 4 million volunteered Internet hosts has been reported for embarrassingly parallel problems. At the same time, peer-to-peer (P2P) file sharing networks, with more than 50 million participants, have demonstrated the capacity for scale in distributed systems. This paper contributes a study of load balancing techniques for a general class of loosely-synchronous parallel algorithms when executed over a P2P network. We show that decentralized, diffusive load balancing can be effective at balancing load and is facilitated by the dynamic properties of P2P. While a moderate degree of dynamicity can benefit load balancing, significant dynamicity hinders the parallel program performance due to the need for increased load migration. To the best of our knowledge this study provides new insight into the performance of loosely-synchronous parallel programs over the Internet.",
        "published": "2004-10-05T08:23:45Z",
        "link": "http://arxiv.org/abs/cs/0410009v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "DiPerF: an automated DIstributed PERformance testing Framework",
        "authors": [
            "Catalin Dumitrescu",
            "Ioan Raicu",
            "Matei Ripeanu",
            "Ian Foster"
        ],
        "summary": "We present DiPerF, a distributed performance testing framework, aimed at simplifying and automating service performance evaluation. DiPerF coordinates a pool of machines that test a target service, collects and aggregates performance metrics, and generates performance statistics. The aggregate data collected provide information on service throughput, on service \"fairness\" when serving multiple clients concurrently, and on the impact of network latency on service performance. Furthermore, using this data, it is possible to build predictive models that estimate a service performance given the service load. We have tested DiPerF on 100+ machines on two testbeds, Grid3 and PlanetLab, and explored the performance of job submission services (pre WS GRAM and WS GRAM) included with Globus Toolkit 3.2.",
        "published": "2004-10-05T22:38:14Z",
        "link": "http://arxiv.org/abs/cs/0410012v1",
        "categories": [
            "cs.PF",
            "cs.DC"
        ]
    },
    {
        "title": "HEP@Home - A distributed computing system based on BOINC",
        "authors": [
            "Antonio Amorim",
            "Jaime Villate",
            "Pedro Andrade"
        ],
        "summary": "Project SETI@HOME has proven to be one of the biggest successes of distributed computing during the last years. With a quite simple approach SETI manages to process large volumes of data using a vast amount of distributed computer power.   To extend the generic usage of this kind of distributed computing tools, BOINC is being developed. In this paper we propose HEP@HOME, a BOINC version tailored to the specific requirements of the High Energy Physics (HEP) community.   The HEP@HOME will be able to process large amounts of data using virtually unlimited computing power, as BOINC does, and it should be able to work according to HEP specifications. In HEP the amounts of data to be analyzed or reconstructed are of central importance. Therefore, one of the design principles of this tool is to avoid data transfer. This will allow scientists to run their analysis applications and taking advantage of a large number of CPUs. This tool also satisfies other important requirements in HEP, namely, security, fault-tolerance and monitoring.",
        "published": "2004-10-07T12:42:10Z",
        "link": "http://arxiv.org/abs/cs/0410016v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Lattice QCD Data and Metadata Archives at Fermilab and the International   Lattice Data Grid",
        "authors": [
            "Eric H. Neilsen Jr",
            "James Simone"
        ],
        "summary": "The lattice gauge theory community produces large volumes of data. Because the data produced by completed computations form the basis for future work, the maintenance of archives of existing data and metadata describing the provenance, generation parameters, and derived characteristics of that data is essential not only as a reference, but also as a basis for future work. Development of these archives according to uniform standards both in the data and metadata formats provided and in the software interfaces to the component services could greatly simplify collaborations between institutions and enable the dissemination of meaningful results.   This paper describes the progress made in the development of a set of such archives at the Fermilab lattice QCD facility. We are coordinating the development of the interfaces to these facilities and the formats of the data and metadata they provide with the efforts of the international lattice data grid (ILDG) metadata and middleware working groups, whose goals are to develop standard formats for lattice QCD data and metadata and a uniform interface to archive facilities that store them. Services under development include those commonly associate with data grids: a service registry, a metadata database, a replica catalog, and an interface to a mass storage system. All services provide GSI authenticated web service interfaces following modern standards, including WSDL and SOAP, and accept and provide data and metadata following recent XML based formats proposed by the ILDG metadata working group.",
        "published": "2004-10-12T15:34:51Z",
        "link": "http://arxiv.org/abs/cs/0410026v1",
        "categories": [
            "cs.DC",
            "hep-lat"
        ]
    },
    {
        "title": "Strategy in Ulam's Game and Tree Code Give Error-Resistant Protocols",
        "authors": [
            "Marcin Peczarski"
        ],
        "summary": "We present a new approach to construction of protocols which are proof against communication errors. The construction is based on a generalization of the well known Ulam's game. We show equivalence between winning strategies in this game and robust protocols for multi-party computation. We do not give any complete theory. We want rather to describe a new fresh idea. We use a tree code defined by Schulman. The tree code is the most important part of the interactive version of Shannon's Coding Theorem proved by Schulman. He uses probabilistic argument for the existence of a tree code without giving any effective construction. We show another proof yielding a randomized construction which in contrary to his proof almost surely gives a good code. Moreover our construction uses much smaller alphabet.",
        "published": "2004-10-18T15:31:54Z",
        "link": "http://arxiv.org/abs/cs/0410043v1",
        "categories": [
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Simple Distributed Weighted Matchings",
        "authors": [
            "Jaap-Henk Hoepman"
        ],
        "summary": "Wattenhofer [WW04] derive a complicated distributed algorithm to compute a weighted matching of an arbitrary weighted graph, that is at most a factor 5 away from the maximum weighted matching of that graph. We show that a variant of the obvious sequential greedy algorithm [Pre99], that computes a weighted matching at most a factor 2 away from the maximum, is easily distributed. This yields the best known distributed approximation algorithm for this problem so far.",
        "published": "2004-10-19T09:00:06Z",
        "link": "http://arxiv.org/abs/cs/0410047v1",
        "categories": [
            "cs.DC",
            "cs.DM"
        ]
    },
    {
        "title": "THE CAVES Project - Collaborative Analysis Versioning Environment   System; THE CODESH Project - Collaborative Development Shell",
        "authors": [
            "Dimitri Bourilkov"
        ],
        "summary": "A key feature of collaboration in science and software development is to have a {\\em log} of what and how is being done - for private use and reuse and for sharing selected parts with collaborators, which most often today are distributed geographically on an ever larger scale. Even better if this log is {\\em automatic}, created on the fly while a scientist or software developer is working in a habitual way, without the need for extra efforts. The {\\tt CAVES} and {\\tt CODESH} projects address this problem in a novel way, building on the concepts of {\\em virtual state} and {\\em virtual transition} to provide an automatic persistent logbook for sessions of data analysis or software development in a collaborating group. A repository of sessions can be configured dynamically to record and make available the knowledge accumulated in the course of a scientific or software endeavor. Access can be controlled to define logbooks of private sessions and sessions shared within or between collaborating groups.",
        "published": "2004-10-22T20:25:11Z",
        "link": "http://arxiv.org/abs/physics/0410226v1",
        "categories": [
            "physics.data-an",
            "cs.DC",
            "hep-ex",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Fast Query Processing by Distributing an Index over CPU Caches",
        "authors": [
            "Xiaoqin Ma",
            "Gene Cooperman"
        ],
        "summary": "Data intensive applications on clusters often require requests quickly be sent to the node managing the desired data. In many applications, one must look through a sorted tree structure to determine the responsible node for accessing or storing the data.   Examples include object tracking in sensor networks, packet routing over the internet, request processing in publish-subscribe middleware, and query processing in database systems. When the tree structure is larger than the CPU cache, the standard implementation potentially incurs many cache misses for each lookup; one cache miss at each successive level of the tree. As the CPU-RAM gap grows, this performance degradation will only become worse in the future.   We propose a solution that takes advantage of the growing speed of local area networks for clusters. We split the sorted tree structure among the nodes of the cluster. We assume that the structure will fit inside the aggregation of the CPU caches of the entire cluster. We then send a word over the network (as part of a larger packet containing other words) in order to examine the tree structure in another node's CPU cache. We show that this is often faster than the standard solution, which locally incurs multiple cache misses while accessing each successive level of the tree.",
        "published": "2004-10-25T22:23:17Z",
        "link": "http://arxiv.org/abs/cs/0410066v2",
        "categories": [
            "cs.DC",
            "cs.PF",
            "C.4"
        ]
    },
    {
        "title": "Computational Unification: a Vision for Connecting Researchers",
        "authors": [
            "Richard M. Troy III"
        ],
        "summary": "The extent to which the benefits of science can be fully realized depends critically upon the quality of the connection between researchers themselves and between researchers and members of the public. We believe that it is now possible to improve these connections on a community-wide and even world-wide basis through the use of an appropriate information management system. In this paper we explore the concepts and challenges, and propose an architecture for the implementation of such a system.   \"One of the greatest visions for science is a computational unification in which every researcher can interact with all other researchers through use of their own research system.\"   \"These features not only enable research collaboration on a scale never previously envisaged, they also enable sharing and dissemination of scientific knowledge to the public at large with a sophistication unparalleled in history.\"",
        "published": "2004-10-26T18:21:54Z",
        "link": "http://arxiv.org/abs/cs/0410067v1",
        "categories": [
            "cs.DC",
            "cs.CY",
            "C.2.4; D.2.12; D.2.13; E.1; F.1.2; H.1.1; H.2.4; H.2.5; H.3.4;\n  H.3.7; H.5.3; K.6.1"
        ]
    },
    {
        "title": "ReCord: A Distributed Hash Table with Recursive Structure",
        "authors": [
            "Jianyang Zeng",
            "Wen-Jing Hsu"
        ],
        "summary": "We propose a simple distributed hash table called ReCord, which is a generalized version of Randomized-Chord and offers improved tradeoffs in performance and topology maintenance over existing P2P systems. ReCord is scalable and can be easily implemented as an overlay network, and offers a good tradeoff between the node degree and query latency. For instance, an $n$-node ReCord with $O(\\log n)$ node degree has an expected latency of $\\Theta(\\log n)$ hops. Alternatively, it can also offer $\\Theta(\\frac{\\log n}{\\log \\log n})$ hops latency at a higher cost of $O(\\frac{\\log^2 n}{\\log   \\log n})$ node degree. Meanwhile, simulations of the dynamic behaviors of ReCord are studied.",
        "published": "2004-10-29T02:56:21Z",
        "link": "http://arxiv.org/abs/cs/0410074v2",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Extremal Properties of Three Dimensional Sensor Networks with   Applications",
        "authors": [
            "Vlady Ravelomanana"
        ],
        "summary": "In this paper, we analyze various critical transmitting/sensing ranges for connectivity and coverage in three-dimensional sensor networks. As in other large-scale complex systems, many global parameters of sensor networks undergo phase transitions: For a given property of the network, there is a critical threshold, corresponding to the minimum amount of the communication effort or power expenditure by individual nodes, above (resp. below) which the property exists with high (resp. a low) probability. For sensor networks, properties of interest include simple and multiple degrees of connectivity/coverage. First, we investigate the network topology according to the region of deployment, the number of deployed sensors and their transmitting/sensing ranges. More specifically, we consider the following problems: Assume that $n$ nodes, each capable of sensing events within a radius of $r$, are randomly and uniformly distributed in a 3-dimensional region $\\mathcal{R}$ of volume $V$, how large must the sensing range be to ensure a given degree of coverage of the region to monitor? For a given transmission range, what is the minimum (resp. maximum) degree of the network? What is then the typical hop-diameter of the underlying network? Next, we show how these results affect algorithmic aspects of the network by designing specific distributed protocols for sensor networks.",
        "published": "2004-11-10T09:10:34Z",
        "link": "http://arxiv.org/abs/cs/0411027v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.DM",
            "ACM classification: C.2.1 Network architecture and design; F.2.2\n  Nonnumerical algorithms and problems; G.3 Probability and statistics"
        ]
    },
    {
        "title": "A machine-independent port of the SR language run-time system to the   NetBSD operating system",
        "authors": [
            "Ignatios Souvatzis"
        ],
        "summary": "SR (synchronizing resources) is a PASCAL - style language enhanced with constructs for concurrent programming developed at the University of Arizona in the late 1980s. MPD (presented in Gregory Andrews' book about Foundations of Multithreaded, Parallel, and Distributed Programming) is its successor, providing the same language primitives with a different syntax. The run-time system (in theory, identical) of both languages provides the illusion of a multiprocessor machine on a single single- or multi- CPU Unix-like system or a (local area) network of Unix-like machines. Chair V of the Computer Science Department of the University of Bonn is operating a laboratory for a practical course in parallel programming consisting of computing nodes running NetBSD/arm, normally used via PVM, MPI etc. We are considering to offer SR and MPD for this, too. As the original language distributions are only targeted at a few commercial Unix systems, some porting effort is needed, outlined in the SR porting guide. The integrated POSIX threads support of NetBSD-2.0 should allow us to use library primitives provided for NetBSD's phtread system to implement the primitives needed by the SR run-time system, thus implementing 13 target CPUs at once and automatically making use of SMP on VAX, Alpha, PowerPC, Sparc, 32-bit Intel and 64 bit AMD CPUs.   This paper describes work in progress.",
        "published": "2004-11-10T12:39:12Z",
        "link": "http://arxiv.org/abs/cs/0411028v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.3.4"
        ]
    },
    {
        "title": "Usage Policy-based CPU Sharing in VOs",
        "authors": [
            "Catalin Dumitrescu",
            "Ian Foster"
        ],
        "summary": "Resource sharing within Grid collaborations usually implies specific sharing mechanisms at participating sites. Challenging policy issues can arise within virtual organizations (VOs) that integrate participants and resources spanning multiple physical institutions. Resource owners may wish to grant to one or more VOs the right to use certain resources subject to local policy and service level agreements, and each VO may then wish to use those resources subject to VO policy. Thus, we must address the question of what usage policies (UPs) should be considered for resource sharing in VOs. As a first step in addressing this question, we develop and evaluate different UP scenarios within a specialized context that mimics scientific Grids within which the resources to be shared are computers. We also present a UP architecture and define roles and functions for scheduling resources in such grid environments while satisfying resource owner policies.",
        "published": "2004-11-12T22:48:00Z",
        "link": "http://arxiv.org/abs/cs/0411045v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Balanced Overlay Networks (BON): Decentralized Load Balancing via   Self-Organized Random Networks",
        "authors": [
            "Jesse S. A. Bridgewater",
            "P. Oscar Boykin",
            "Vwani P. Roychowdhury"
        ],
        "summary": "We present a novel framework, called balanced overlay networks (BON), that provides scalable, decentralized load balancing for distributed computing using large-scale pools of heterogeneous computers. Fundamentally, BON encodes the information about each node's available computational resources in the structure of the links connecting the nodes in the network. This distributed encoding is self-organized, with each node managing its in-degree and local connectivity via random-walk sampling. Assignment of incoming jobs to nodes with the most free resources is also accomplished by sampling the nodes via short random walks. Extensive simulations show that the resulting highly dynamic and self-organized graph structure can efficiently balance computational load throughout large-scale networks. These simulations cover a wide spectrum of cases, including significant heterogeneity in available computing resources and high burstiness in incoming load. We provide analytical results that prove BON's scalability for truly large-scale networks: in particular we show that under certain ideal conditions, the network structure converges to Erdos-Renyi (ER) random graphs; our simulation results, however, show that the algorithm does much better, and the structures seem to approach the ideal case of d-regular random graphs. We also make a connection between highly-loaded BONs and the well-known ball-bin randomized load balancing framework.",
        "published": "2004-11-15T20:05:00Z",
        "link": "http://arxiv.org/abs/cs/0411046v2",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Utilizing Reconfigurable Hardware Processors via Grid Services",
        "authors": [
            "Darran Nathan",
            "Ralf Clemens"
        ],
        "summary": "Computational grids typically consist of nodes utilizing ordinary processors such as the Intel Pentium. Field Programmable Gate Arrays (FPGAs) are able to perform certain compute-intensive tasks very well due to their inherent parallel architecture, often resulting in orders of magnitude speedups. This paper explores how FPGAs can be transparently exposed for remote use via grid services, by integrating the Proteus Software Platform with the Globus Toolkit 3.0.",
        "published": "2004-11-16T01:49:22Z",
        "link": "http://arxiv.org/abs/cs/0411050v1",
        "categories": [
            "cs.DC",
            "cs.AR"
        ]
    },
    {
        "title": "A Self-Reconfigurable Computing Platform Hardware Architecture",
        "authors": [
            "Andreas Weisensee",
            "Darran Nathan"
        ],
        "summary": "Field Programmable Gate Arrays (FPGAs) have recently been increasingly used for highly-parallel processing of compute intensive tasks. This paper introduces an FPGA hardware platform architecture that is PC-based, allows for fast reconfiguration over the PCI bus, and retains a simple physical hardware design. The design considerations are first discussed, then the resulting system architecture designed is illustrated. Finally, experimental results on the FPGA resources utilized for this design are presented.",
        "published": "2004-11-20T06:31:32Z",
        "link": "http://arxiv.org/abs/cs/0411075v1",
        "categories": [
            "cs.AR",
            "cs.DC"
        ]
    },
    {
        "title": "Embeddings into the Pancake Interconnection Network",
        "authors": [
            "Christian Lavault"
        ],
        "summary": "Owing to its nice properties, the pancake is one of the Cayley graphs that were proposed as alternatives to the hypercube for interconnecting processors in parallel computers. In this paper, we present embeddings of rings, grids and hypercubes into the pancake with constant dilation and congestion. We also extend the results to similar efficient embeddings into the star graph.",
        "published": "2004-11-26T20:13:10Z",
        "link": "http://arxiv.org/abs/cs/0411095v1",
        "categories": [
            "cs.DC",
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "Signals for Cellular Automata in dimension 2 or higher",
        "authors": [
            "Jean-Christophe Dubacq",
            "Veronique Terrier"
        ],
        "summary": "We investigate how increasing the dimension of the array can help to draw signals on cellular automata.We show the existence of a gap of constructible signals in any dimension. We exhibit two cellular automata in dimension 2 to show that increasing the dimension allows to reduce the number of states required for some constructions.",
        "published": "2004-12-03T16:35:17Z",
        "link": "http://arxiv.org/abs/cs/0412013v1",
        "categories": [
            "cs.CC",
            "cs.DC",
            "cs.DM",
            "math.CO"
        ]
    },
    {
        "title": "Randomized Initialization of a Wireless Multihop Network",
        "authors": [
            "Vlady Ravelomanana"
        ],
        "summary": "Address autoconfiguration is an important mechanism required to set the IP address of a node automatically in a wireless network. The address autoconfiguration, also known as initialization or naming, consists to give a unique identifier ranging from 1 to $n$ for a set of $n$ indistinguishable nodes. We consider a wireless network where $n$ nodes (processors) are randomly thrown in a square $X$, uniformly and independently. We assume that the network is synchronous and two nodes are able to communicate if they are within distance at most of $r$ of each other ($r$ is the transmitting/receiving range). The model of this paper concerns nodes without the collision detection ability: if two or more neighbors of a processor $u$ transmit concurrently at the same time, then $u$ would not receive either messages. We suppose also that nodes know neither the topology of the network nor the number of nodes in the network. Moreover, they start indistinguishable, anonymous and unnamed. Under this extremal scenario, we design and analyze a fully distributed protocol to achieve the initialization task for a wireless multihop network of $n$ nodes uniformly scattered in a square $X$. We show how the transmitting range of the deployed stations can affect the typical characteristics such as the degrees and the diameter of the network. By allowing the nodes to transmit at a range $r= \\sqrt{\\frac{(1+\\ell) \\ln{n} \\SIZE}{\\pi n}}$ (slightly greater than the one required to have a connected network), we show how to design a randomized protocol running in expected time $O(n^{3/2} \\log^2{n})$ in order to assign a unique number ranging from 1 to $n$ to each of the $n$ participating nodes.",
        "published": "2004-12-03T16:39:58Z",
        "link": "http://arxiv.org/abs/cs/0412014v1",
        "categories": [
            "cs.DC",
            "cs.DM",
            "ACM Classification: C.2.1 Network architecture and design; F.2.2\n  Nonnumerical algorithms and problems; G.3 Probability and statistics"
        ]
    },
    {
        "title": "Deployment of a Grid-based Medical Imaging Application",
        "authors": [
            "S R Amendolia",
            "F Estrella",
            "C del Frate",
            "J Galvez",
            "W Hassan",
            "T Hauer",
            "D Manset",
            "R McClatchey",
            "M Odeh",
            "D Rogulin",
            "T Solomonides",
            "R Warren"
        ],
        "summary": "The MammoGrid project has deployed its Service-Oriented Architecture (SOA)-based Grid application in a real environment comprising actual participating hospitals. The resultant setup is currently being exploited to conduct rigorous in-house tests in the first phase before handing over the setup to the actual clinicians to get their feedback. This paper elaborates the deployment details and the experiences acquired during this phase of the project. Finally the strategy regarding migration to an upcoming middleware from EGEE project will be described. This paper concludes by highlighting some of the potential areas of future work.",
        "published": "2004-12-08T17:15:06Z",
        "link": "http://arxiv.org/abs/cs/0412035v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Reverse Engineering Ontology to Conceptual Data Models",
        "authors": [
            "Haya El-Ghalayini",
            "Mohammed Odeh",
            "Richard McClatchey",
            "Tony Solomonides"
        ],
        "summary": "Ontologies facilitate the integration of heterogeneous data sources by resolving semantic heterogeneity between them. This research aims to study the possibility of generating a domain conceptual model from a given ontology with the vision to grow this generated conceptual data model into a global conceptual model integrating a number of existing data and information sources. Based on ontologically derived semantics of the BWW model, rules are identified that map elements of the ontology language (DAML+OIL) to domain conceptual model elements. This mapping is demonstrated using TAMBIS ontology. A significant corollary of this study is that it is possible to generate a domain conceptual model from a given ontology subject to validation that needs to be performed by the domain specialist before evolving this model into a global conceptual model.",
        "published": "2004-12-08T17:23:07Z",
        "link": "http://arxiv.org/abs/cs/0412036v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Tycoon: an Implementation of a Distributed, Market-based Resource   Allocation System",
        "authors": [
            "Kevin Lai",
            "Lars Rasmusson",
            "Eytan Adar",
            "Stephen Sorkin",
            "Li Zhang",
            "Bernardo A. Huberman"
        ],
        "summary": "Distributed clusters like the Grid and PlanetLab enable the same statistical multiplexing efficiency gains for computing as the Internet provides for networking. One major challenge is allocating resources in an economically efficient and low-latency way. A common solution is proportional share, where users each get resources in proportion to their pre-defined weight. However, this does not allow users to differentiate the value of their jobs. This leads to economic inefficiency. In contrast, systems that require reservations impose a high latency (typically minutes to hours) to acquire resources.   We present Tycoon, a market based distributed resource allocation system based on proportional share. The key advantages of Tycoon are that it allows users to differentiate the value of their jobs, its resource acquisition latency is limited only by communication delays, and it imposes no manual bidding overhead on users. We present experimental results using a prototype implementation of our design.",
        "published": "2004-12-09T01:20:39Z",
        "link": "http://arxiv.org/abs/cs/0412038v1",
        "categories": [
            "cs.DC",
            "cs.OS",
            "C.2.4; D.4.1; K.6.4"
        ]
    },
    {
        "title": "Self-Organized Stigmergic Document Maps: Environment as a Mechanism for   Context Learning",
        "authors": [
            "Vitorino Ramos",
            "Juan J. Merelo"
        ],
        "summary": "Social insect societies and more specifically ant colonies, are distributed systems that, in spite of the simplicity of their individuals, present a highly structured social organization. As a result of this organization, ant colonies can accomplish complex tasks that in some cases exceed the individual capabilities of a single ant. The study of ant colonies behavior and of their self-organizing capabilities is of interest to knowledge retrieval/management and decision support systems sciences, because it provides models of distributed adaptive organization which are useful to solve difficult optimization, classification, and distributed control problems, among others. In the present work we overview some models derived from the observation of real ants, emphasizing the role played by stigmergy as distributed communication paradigm, and we present a novel strategy to tackle unsupervised clustering as well as data retrieval problems. The present ant clustering system (ACLUSTER) avoids not only short-term memory based strategies, as well as the use of several artificial ant types (using different speeds), present in some recent approaches. Moreover and according to our knowledge, this is also the first application of ant systems into textual document clustering. KEYWORDS: Swarm Intelligence, Ant Systems, Unsupervised Clustering, Data Retrieval, Data Mining, Distributed Computing, Document Maps, Textual Document Clustering.",
        "published": "2004-12-17T15:47:44Z",
        "link": "http://arxiv.org/abs/cs/0412075v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "I.5; I.5.3; I.4; I.2.11"
        ]
    },
    {
        "title": "Mass Storage Management and the Grid",
        "authors": [
            "A. Earl",
            "P. Clark"
        ],
        "summary": "The University of Edinburgh has a significant interest in mass storage systems as it is one of the core groups tasked with the roll out of storage software for the UK's particle physics grid, GridPP. We present the results of a development project to provide software interfaces between the SDSC Storage Resource Broker, the EU DataGrid and the Storage Resource Manager. This project was undertaken in association with the eDikt group at the National eScience Centre, the Universities of Bristol and Glasgow, Rutherford Appleton Laboratory and the San Diego Supercomputing Center.",
        "published": "2004-12-20T11:50:44Z",
        "link": "http://arxiv.org/abs/cs/0412092v1",
        "categories": [
            "cs.DC",
            "cs.SE"
        ]
    },
    {
        "title": "ScotGrid: A Prototype Tier 2 Centre",
        "authors": [
            "A. Earl",
            "P. Clark",
            "S. Thorn"
        ],
        "summary": "ScotGrid is a prototype regional computing centre formed as a collaboration between the universities of Durham, Edinburgh and Glasgow as part of the UK's national particle physics grid, GridPP. We outline the resources available at the three core sites and our optimisation efforts for our user communities. We discuss the work which has been conducted in extending the centre to embrace new projects both from particle physics and new user communities and explain our methodology for doing this.",
        "published": "2004-12-20T11:55:30Z",
        "link": "http://arxiv.org/abs/cs/0412093v1",
        "categories": [
            "cs.AR",
            "cs.DC"
        ]
    },
    {
        "title": "Reductions in Distributed Computing Part I: Consensus and Atomic   Commitment Tasks",
        "authors": [
            "Bernadette Charron-Bost"
        ],
        "summary": "We introduce several notions of reduction in distributed computing, and investigate reduction properties of two fundamental agreement tasks, namely Consensus and Atomic Commitment.   We first propose the notion of reduction \"a la Karp'', an analog for distributed computing of the classical Karp reduction. We then define a weaker reduction which is the analog of Cook reduction. These two reductions are called K-reduction and C-reduction, respectively.   We also introduce the notion of C*-reduction which has no counterpart in classical (namely, non distributed) systems, and which naturally arises when dealing with symmetric tasks.   We establish various reducibility and irreducibility theorems with respect to these three reductions. Our main result is an incomparability statement for Consensus and Atomic Commitment tasks: we show that they are incomparable with respect to the C-reduction, except when the resiliency degree is 1, in which case Atomic Commitment is strictly harder than Consensus. A side consequence of these results is that our notion of C-reduction is strictly weaker than the one of K-reduction, even for unsolvable tasks.",
        "published": "2004-12-29T19:50:21Z",
        "link": "http://arxiv.org/abs/cs/0412115v1",
        "categories": [
            "cs.DC",
            "C.4; C.2.4; F.1.3"
        ]
    },
    {
        "title": "Reductions in Distributed Computing Part II: k-Threshold Agreement Tasks",
        "authors": [
            "Bernadette Charron-Bost"
        ],
        "summary": "We extend the results of Part I by considering a new class of agreement tasks, the so-called k-Threshold Agreement tasks (previously introduced by Charron-Bost and Le Fessant). These tasks naturally interpolate between Atomic Commitment and Consensus. Moreover, they constitute a valuable tool to derive irreducibility results between Consensus tasks only. In particular, they allow us to show that (A) for a fixed set of processes, the higher the resiliency degree is, the harder the Consensus task is, and (B) for a fixed resiliency degree, the smaller the set of processes is, the harder the Consensus task is.   The proofs of these results lead us to consider new oracle-based reductions, involving a weaker variant of the C-reduction introduced in Part I. We also discuss the relationship between our results and previous ones relating f-resiliency and wait-freedom.",
        "published": "2004-12-29T20:51:58Z",
        "link": "http://arxiv.org/abs/cs/0412116v1",
        "categories": [
            "cs.DC",
            "C.4; C.2.4; F.1.3"
        ]
    },
    {
        "title": "Power Aware Routing for Sensor Databases",
        "authors": [
            "Chiranjeeb Buragohain",
            "Divyakant Agrawal",
            "Subhash Suri"
        ],
        "summary": "Wireless sensor networks offer the potential to span and monitor large geographical areas inexpensively. Sensor network databases like TinyDB are the dominant architectures to extract and manage data in such networks. Since sensors have significant power constraints (battery life), and high communication costs, design of energy efficient communication algorithms is of great importance. The data flow in a sensor database is very different from data flow in an ordinary network and poses novel challenges in designing efficient routing algorithms. In this work we explore the problem of energy efficient routing for various different types of database queries and show that in general, this problem is NP-complete. We give a constant factor approximation algorithm for one class of query, and for other queries give heuristic algorithms. We evaluate the efficiency of the proposed algorithms by simulation and demonstrate their near optimal performance for various network sizes.",
        "published": "2004-12-30T02:02:35Z",
        "link": "http://arxiv.org/abs/cs/0412118v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "C.2.1, C.2.2, F.1.3"
        ]
    },
    {
        "title": "A Distributed Economics-based Infrastructure for Utility Computing",
        "authors": [
            "Michael Treaster",
            "Nadir Kiyanclar",
            "Gregory A. Koenig",
            "William Yurcik"
        ],
        "summary": "Existing attempts at utility computing revolve around two approaches. The first consists of proprietary solutions involving renting time on dedicated utility computing machines. The second requires the use of heavy, monolithic applications that are difficult to deploy, maintain, and use.   We propose a distributed, community-oriented approach to utility computing. Our approach provides an infrastructure built on Web Services in which modular components are combined to create a seemingly simple, yet powerful system. The community-oriented nature generates an economic environment which results in fair transactions between consumers and providers of computing cycles while simultaneously encouraging improvements in the infrastructure of the computational grid itself.",
        "published": "2004-12-31T19:16:38Z",
        "link": "http://arxiv.org/abs/cs/0412121v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Design of a Community-based Translation Center",
        "authors": [
            "K. McDevitt",
            "M. A. Pérez-Quiñones",
            "O. I. Padilla-Falto"
        ],
        "summary": "Interfaces that support multi-lingual content can reach a broader community. We wish to extend the reach of CITIDEL, a digital library for computing education materials, to support multiple languages. By doing so, we hope that it will increase the number of users, and in turn the number of resources. This paper discusses three approaches to translation (automated translation, developer-based, and community-based), and a brief evaluation of these approaches. It proposes a design for an online community translation center where volunteers help translate interface components and educational materials available in CITIDEL.",
        "published": "2004-01-12T18:45:47Z",
        "link": "http://arxiv.org/abs/cs/0401007v1",
        "categories": [
            "cs.HC",
            "cs.DL",
            "H.3.7; J.5; H.1.2"
        ]
    },
    {
        "title": "Mobile Re-Finding of Web Information Using a Voice Interface",
        "authors": [
            "Robert G. Capra",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "Mobile access to information is a considerable problem for many users, especially to information found on the Web. In this paper, we explore how a voice-controlled service, accessible by telephone, could support mobile users' needs for refinding specific information previously found on the Web. We outline challenges in creating such a service and describe architectural and user interfaces issues discovered in an exploratory prototype we built called WebContext.   We also present the results of a study, motivated by our experience with WebContext, to explore what people remember about information that they are trying to refind and how they express information refinding requests in a collaborative conversation. As part of the study, we examine how end-usercreated Web page annotations can be used to help support mobile information re-finding. We observed the use of URLs, page titles, and descriptions of page contents to help identify waypoints in the search process. Furthermore, we observed that the annotations were utilized extensively, indicating that explicitly added context by the user can play an important role in re-finding.",
        "published": "2004-01-31T20:09:39Z",
        "link": "http://arxiv.org/abs/cs/0402001v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.1.2; H.3.3; H.5.2"
        ]
    },
    {
        "title": "Automatically Generating Interfaces for Personalized Interaction with   Digital Libraries",
        "authors": [
            "Saverio Perugini",
            "Naren Ramakrishnan",
            "Edward A. Fox"
        ],
        "summary": "We present an approach to automatically generate interfaces supporting personalized interaction with digital libraries; these interfaces augment the user-DL dialog by empowering the user to (optionally) supply out-of-turn information during an interaction, flatten or restructure the dialog, and enquire about dialog options. Interfaces generated using this approach for CITIDEL are described.",
        "published": "2004-02-12T03:00:14Z",
        "link": "http://arxiv.org/abs/cs/0402022v1",
        "categories": [
            "cs.DL",
            "cs.HC",
            "H.3.7 [Digital Libraries]: User Issues; H.5.2 [User Interfaces]:\n  Graphical user interfaces, Interaction styles; H.5.4 [Hypertext/Hypermedia]:\n  Navigation"
        ]
    },
    {
        "title": "Mapping Topics and Topic Bursts in PNAS",
        "authors": [
            "Ketan Mane",
            "Katy Börner"
        ],
        "summary": "Scientific research is highly dynamic. New areas of science continually evolve;others gain or lose importance, merge or split. Due to the steady increase in the number of scientific publications it is hard to keep an overview of the structure and dynamic development of one's own field of science, much less all scientific domains. However, knowledge of hot topics, emergent research frontiers, or change of focus in certain areas is a critical component of resource allocation decisions in research labs, governmental institutions, and corporations. This paper demonstrates the utilization of Kleinberg's burst detection algorithm, co-word occurrence analysis, and graph layout techniques to generate maps that support the identification of major research topics and trends. The approach was applied to analyze and map the complete set of papers published in the Proceedings of the National Academy of Sciences (PNAS) in the years 1982-2001. Six domain experts examined and commented on the resulting maps in an attempt to reconstruct the evolution of major research areas covered by PNAS.",
        "published": "2004-02-14T03:55:53Z",
        "link": "http://arxiv.org/abs/cs/0402029v1",
        "categories": [
            "cs.IR",
            "cs.HC",
            "H.3.3; H.1.2"
        ]
    },
    {
        "title": "Towards a Model-Based Framework for Integrating Usability and Software   Engineering Life Cycles",
        "authors": [
            "Pardha S. Pyla",
            "Manuel A. Perez-Quinones",
            "James D. Arthur",
            "H. Rex Hartson"
        ],
        "summary": "In this position paper we propose a process model that provides a development infrastructure in which the usability engineering and software engineering life cycles co-exist in complementary roles. We describe the motivation, hurdles, rationale, arguments, and implementation plan for the need, specification, and the usefulness of such a model.",
        "published": "2004-02-16T21:56:03Z",
        "link": "http://arxiv.org/abs/cs/0402036v1",
        "categories": [
            "cs.HC",
            "H.5.0; K.6.1; K.6.3; D.2.9"
        ]
    },
    {
        "title": "Culture and International Usability Testing: The Effects of Culture in   Structured Interviews",
        "authors": [
            "Ravikiran Vatrapu",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "The global audience for software products includes members of different countries, religions, and cultures: people who speak different languages, have different life styles, and have different perceptions and expectations of any given product. A major impediment in interface development is that there is inadequate empirical evidence for the effects of culture in the usability engineering methods used for developing user interfaces. This paper presents a controlled study investigating the effects of culture on the effectiveness of structured interviews in usability testing. The experiment consisted of usability testing of a website with two independent groups of Indian participants by two interviewers; one belonging to the Indian culture and the other to the Anglo-American culture. Participants found more usability problems and made more suggestions to an interviewer who was a member of the same (Indian) culture than to the foreign (Anglo-American) interviewer. The results of the study empirically establish that culture significantly affects the efficacy of structured interviews during international user testing. The implications of this work for usability engineering are discussed.",
        "published": "2004-05-13T18:46:34Z",
        "link": "http://arxiv.org/abs/cs/0405045v1",
        "categories": [
            "cs.HC",
            "cs.SE",
            "H5.2"
        ]
    },
    {
        "title": "Generative Programming of Graphical User Interfaces",
        "authors": [
            "Max Schlee",
            "Jean Vanderdonckt"
        ],
        "summary": "Generative Programming (GP) is a computing paradigm allowing automatic creation of entire software families utilizing the configuration of elementary and reusable components. GP can be projected on different technologies, e.g. C++-templates, Java-Beans, Aspect-Oriented Programming (AOP), or Frame technology. This paper focuses on Frame Technology, which aids the possible implementation and completion of software components. The purpose of this paper is to introduce the GP paradigm in the area of GUI application generation. It demonstrates how automatically customized executable applications with GUI parts can be generated from an abstract specification.",
        "published": "2004-05-23T09:53:12Z",
        "link": "http://arxiv.org/abs/cs/0405078v1",
        "categories": [
            "cs.HC",
            "D.2.1, D.2.2, H.2.4, I.3.6"
        ]
    },
    {
        "title": "\"User Interfaces\" and the Social Negotiation of Availability",
        "authors": [
            "Paul M. Aoki",
            "Allison Woodruff"
        ],
        "summary": "In current presence or availability systems, the method of presenting a user's state often supposes an instantaneous notion of that state - for example, a visualization is rendered or an inference is made about the potential actions that might be consistent with a user's state. Drawing on observational research on the use of existing communication technology, we argue (as have others in the past) that determination of availability is often a joint process, and often one that takes the form of a negotiation (whether implicit or explicit). We briefly describe our current research on applying machine learning to infer degrees of conversational engagement from observed conversational behavior. Such inferences can be applied to facilitate the implicit negotiation of conversational engagement - in effect, helping users to weave together the act of contact with the act of determining availability.",
        "published": "2004-05-28T02:53:46Z",
        "link": "http://arxiv.org/abs/cs/0405108v1",
        "categories": [
            "cs.HC",
            "H.5.2; H.4.3; H.1.2"
        ]
    },
    {
        "title": "Conversation Analysis and the User Experience",
        "authors": [
            "Allison Woodruff",
            "Paul M. Aoki"
        ],
        "summary": "We provide two case studies in the application of ideas drawn from conversation analysis to the design of technologies that enhance the experience of human conversation. We first present a case study of the design of an electronic guidebook, focusing on how conversation analytic principles played a role in the design process. We then discuss how the guidebook project has inspired our continuing work in social, mobile audio spaces. In particular, we describe some as yet unrealized concepts for adaptive audio spaces.",
        "published": "2004-05-28T02:59:54Z",
        "link": "http://arxiv.org/abs/cs/0405109v1",
        "categories": [
            "cs.HC",
            "H.5.2; H.1.2"
        ]
    },
    {
        "title": "Three-Dimensional Face Orientation and Gaze Detection from a Single   Image",
        "authors": [
            "J. Y. Kaminski",
            "M. Teicher",
            "D. Knaan",
            "A. Shavit"
        ],
        "summary": "Gaze detection and head orientation are an important part of many advanced human-machine interaction applications. Many systems have been proposed for gaze detection. Typically, they require some form of user cooperation and calibration. Additionally, they may require multiple cameras and/or restricted head positions. We present a new approach for inference of both face orientation and gaze direction from a single image with no restrictions on the head position. Our algorithm is based on a face and eye model, deduced from anthropometric data. This approach allows us to use a single camera and requires no cooperation from the user. Using a single image avoids the complexities associated with of a multi-camera system. Evaluation tests show that our system is accurate, fast and can be used in a variety of applications, including ones where the user is unaware of the system.",
        "published": "2004-08-04T18:12:52Z",
        "link": "http://arxiv.org/abs/cs/0408012v1",
        "categories": [
            "cs.CV",
            "cs.HC"
        ]
    },
    {
        "title": "The role of robust semantic analysis in spoken language dialogue systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "In this paper we summarized a framework for designing grammar-based procedure for the automatic extraction of the semantic content from spoken queries. Starting with a case study and following an approach which combines the notions of fuzziness and robustness in sentence parsing, we showed we built practical domain-dependent rules which can be applied whenever it is possible to superimpose a sentence-level semantic structure to a text without relying on a previous deep syntactical analysis. This kind of procedure can be also profitably used as a pre-processing tool in order to cut out part of the sentence which have been recognized to have no relevance in the understanding process. In the case of particular dialogue applications where there is no need to build a complex semantic structure (e.g. word spotting or excerpting) the presented methodology may represent an efficient alternative solution to a sequential composition of deep linguistic analysis modules. Even if the query generation problem may not seem a critical application it should be held in mind that the sentence processing must be done on-line. Having this kind of constraints we cannot design our system without caring for efficiency and thus provide an immediate response. Another critical issue is related to whole robustness of the system. In our case study we tried to make experiences on how it is possible to deal with an unreliable and noisy input without asking the user for any repetition or clarification. This may correspond to a similar problem one may have when processing text coming from informal writing such as e-mails, news and in many cases Web pages where it is often the case to have irrelevant surrounding information.",
        "published": "2004-08-25T19:37:59Z",
        "link": "http://arxiv.org/abs/cs/0408057v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "H.5.2;H.3.1;H.3.4"
        ]
    },
    {
        "title": "ScheduleNanny: Using GPS to Learn the User's Significant Locations,   Travel Times and Schedule",
        "authors": [
            "Parth Bhawalkar",
            "Victor Bigio",
            "Adam Davis",
            "Karthik Narayanaswami",
            "Femi Olumoko"
        ],
        "summary": "As computing technology becomes more pervasive, personal devices such as the PDA, cell-phone, and notebook should use context to determine how to act. Location is one form of context that can be used in many ways. We present a multiple-device system that collects and clusters GPS data into significant locations. These locations are then used to determine travel times and a probabilistic model of the user's schedule, which is used to intelligently alert the user. We evaluate our system and suggest how it should be integrated with a variety of applications.",
        "published": "2004-09-02T15:28:53Z",
        "link": "http://arxiv.org/abs/cs/0409003v1",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.HC",
            "F.2.2; I.5.3; H.5.3; H.5.m"
        ]
    },
    {
        "title": "Four Principles Fundamental to Design Practice for Human Centred Systems",
        "authors": [
            "Vita Hinze-Hoare"
        ],
        "summary": "A Survey of the principal literature on Human Centred Design reveals the four most referenced principles. These are discussed with reference to the application of a particular website, and a user survey is constructed based upon the four principles.",
        "published": "2004-09-23T19:08:17Z",
        "link": "http://arxiv.org/abs/cs/0409041v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "A new architecture for making highly scalable applications",
        "authors": [
            "Harry Fitié"
        ],
        "summary": "An application is a logical image of the world on a computer. A scalable application is an application that allows one to update that logical image at run time. To put it in operational terms: an application is scalable if a client can change between time T1 and time T2 - the logic of the application as expressed by language L;   - the structure and volume of the stored knowledge;   - the user interface of the application; while clients working with the application at time T1 will work with the changed application at time T2 without performing any special action between T1 and T2. In order to realize such a scalable application a new architecture has been developed that fully orbits around language. In order to verify the soundness of that architecture a program has been build. Both architecture and program are called CommunSENS. The main purpose of this paper is: - to list the relevant elements of the architecture; - to give a visual presentation of how the program and its image of the world look like; - to give a visual presentation of how the image can be updated. Some relevant philosophical and practical backgrounds are included in the appendixes.",
        "published": "2004-09-23T19:29:26Z",
        "link": "http://arxiv.org/abs/cs/0409042v1",
        "categories": [
            "cs.HC",
            "cs.CL"
        ]
    },
    {
        "title": "Detecting User Engagement in Everyday Conversations",
        "authors": [
            "Chen Yu",
            "Paul M. Aoki",
            "Allison Woodruff"
        ],
        "summary": "This paper presents a novel application of speech emotion recognition: estimation of the level of conversational engagement between users of a voice communication system. We begin by using machine learning techniques, such as the support vector machine (SVM), to classify users' emotions as expressed in individual utterances. However, this alone fails to model the temporal and interactive aspects of conversational engagement. We therefore propose the use of a multilevel structure based on coupled hidden Markov models (HMM) to estimate engagement levels in continuous natural speech. The first level is comprised of SVM-based classifiers that recognize emotional states, which could be (e.g.) discrete emotion types or arousal/valence levels. A high-level HMM then uses these emotional states as input, estimating users' engagement in conversation by decoding the internal states of the HMM. We report experimental results obtained by applying our algorithms to the LDC Emotional Prosody and CallFriend speech corpora.",
        "published": "2004-10-13T02:28:10Z",
        "link": "http://arxiv.org/abs/cs/0410027v1",
        "categories": [
            "cs.SD",
            "cs.CL",
            "cs.HC",
            "I.5.4; I.2.7; H.5.2; H.4.3"
        ]
    },
    {
        "title": "Neural Architectures for Robot Intelligence",
        "authors": [
            "H. Ritter",
            "J. J. Steil",
            "C. Noelker",
            "F. Roethling",
            "P. C. McGuire"
        ],
        "summary": "We argue that the direct experimental approaches to elucidate the architecture of higher brains may benefit from insights gained from exploring the possibilities and limits of artificial control architectures for robot systems. We present some of our recent work that has been motivated by that view and that is centered around the study of various aspects of hand actions since these are intimately linked with many higher cognitive abilities. As examples, we report on the development of a modular system for the recognition of continuous hand postures based on neural nets, the use of vision and tactile sensing for guiding prehensile movements of a multifingered hand, and the recognition and use of hand gestures for robot teaching.   Regarding the issue of learning, we propose to view real-world learning from the perspective of data mining and to focus more strongly on the imitation of observed actions instead of purely reinforcement-based exploration. As a concrete example of such an effort we report on the status of an ongoing project in our lab in which a robot equipped with an attention system with a neurally inspired architecture is taught actions by using hand gestures in conjunction with speech commands. We point out some of the lessons learnt from this system, and discuss how systems of this kind can contribute to the study of issues at the junction between natural and artificial cognitive systems.",
        "published": "2004-10-18T10:50:28Z",
        "link": "http://arxiv.org/abs/cs/0410042v1",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.HC",
            "cs.LG",
            "cs.NE",
            "q-bio.NC",
            "I.2.9; I.2.10; I.2.6; H.1.2; H.2.8; I.5.4"
        ]
    },
    {
        "title": "Robust Dialogue Understanding in HERALD",
        "authors": [
            "Vincenzo Pallotta",
            "Afzal Ballim"
        ],
        "summary": "We tackle the problem of robust dialogue processing from the perspective of language engineering. We propose an agent-oriented architecture that allows us a flexible way of composing robust processors. Our approach is based on Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP agent model can be enriched with special features and components that allow us to deal with classical problems of dialogue understanding.",
        "published": "2004-10-22T23:41:52Z",
        "link": "http://arxiv.org/abs/cs/0410058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.MA",
            "cs.SE",
            "H.5.2, I.2.7, I.2.11"
        ]
    },
    {
        "title": "Semantic filtering by inference on domain knowledge in spoken dialogue   systems",
        "authors": [
            "Afzal Ballim",
            "Vincenzo Pallotta"
        ],
        "summary": "General natural dialogue processing requires large amounts of domain knowledge as well as linguistic knowledge in order to ensure acceptable coverage and understanding. There are several ways of integrating lexical resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at different levels of dialogue processing. We concentrate in this paper on how to exploit domain knowledge for filtering interpretation hypotheses generated by a robust semantic parser. We use domain knowledge to semantically constrain the hypothesis space. Moreover, adding an inference mechanism allows us to complete the interpretation when information is not explicitly available. Further, we discuss briefly how this can be generalized towards a predictive natural interactive system.",
        "published": "2004-10-23T00:20:06Z",
        "link": "http://arxiv.org/abs/cs/0410060v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.IR",
            "H.5.2;H.3.1;H.3.4"
        ]
    },
    {
        "title": "INSPIRE: Evaluation of a Smart-Home System for Infotainment Management   and Device Control",
        "authors": [
            "Sebastian Moeller",
            "Jan Krebber",
            "Alexander Raake",
            "Paula Smeele",
            "Martin Rajman",
            "Mirek Melichar",
            "Vincenzo Pallotta",
            "Gianna Tsakou",
            "Basilis Kladis",
            "Anestis Vovos",
            "Jettie Hoonhout",
            "Dietmar Schuchardt",
            "Nikos Fakotakis",
            "Todor Ganchev",
            "Ilyas Potamitis"
        ],
        "summary": "This paper gives an overview of the assessment and evaluation methods which have been used to determine the quality of the INSPIRE smart home system. The system allows different home appliances to be controlled via speech, and consists of speech and speaker recognition, speech understanding, dialogue management, and speech output components. The performance of these components is first assessed individually, and then the entire system is evaluated in an interaction experiment with test users. Initial results of the assessment and evaluation are given, in particular with respect to the transmission channel impact on speech and speaker recognition, and the assessment of speech output for different system metaphors.",
        "published": "2004-10-25T02:17:35Z",
        "link": "http://arxiv.org/abs/cs/0410063v1",
        "categories": [
            "cs.HC",
            "cs.CL",
            "H.5.2;I.2.7;H.1.2"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: First Field Experience",
        "authors": [
            "Patrick C. McGuire",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Jose Antonio Rodriguez-Manfredi",
            "Javier Gomez-Elvira",
            "Helge Ritter",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist and field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. The hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: first, detection and accounting for shadows caused by 3D jagged edges in the outcrop; second, reincorporation of more sophisticated texture-analysis algorithms into the system; third, creation of hardware and software capabilities to control the camera's zoom lens in an intelligent manner; and fourth, development of algorithms for interpretation of complex geological scenery. Nonetheless, despite these technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner.",
        "published": "2004-10-27T09:14:01Z",
        "link": "http://arxiv.org/abs/cs/0410071v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "q-bio.NC",
            "I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9"
        ]
    },
    {
        "title": "A Social Network for Societal-Scale Decision-Making Systems",
        "authors": [
            "Marko Rodriguez",
            "Daniel Steinbock"
        ],
        "summary": "In societal-scale decision-making systems the collective is faced with the problem of ensuring that the derived group decision is in accord with the collective's intention. In modern systems, political institutions have instatiated representative forms of decision-making to ensure that every individual in the society has a participatory voice in the decision-making behavior of the whole--even if only indirectly through representation. An agent-based simulation demonstrates that in modern representative systems, as the ratio of representatives increases, there exists an exponential decrease in the ability for the group to behave in accord with the desires of the whole. To remedy this issue, this paper provides a novel representative power structure for decision-making that utilizes a social network and power distribution algorithm to maintain the collective's perspective over varying degrees of participation and/or ratios of representation. This work shows promise for the future development of policy-making systems that are supported by the computer and network infrastructure of our society.",
        "published": "2004-12-11T00:32:51Z",
        "link": "http://arxiv.org/abs/cs/0412047v1",
        "categories": [
            "cs.CY",
            "cs.DS",
            "cs.HC",
            "H.4.2; J.7; K.4.m"
        ]
    },
    {
        "title": "Collective Intelligence Quanitifed for Computer-Mediated Group Problem   Solving",
        "authors": [
            "Dan Steinbock",
            "Craig Kaplan",
            "Marko Rodriguez",
            "Juana Diaz",
            "Newton Der",
            "Suzanne Garcia"
        ],
        "summary": "Collective Intelligence (CI) is the ability of a group to exhibit greater intelligence than its individual members. Expressed by the common saying that \"two minds are better than one,\" CI has been a topic of interest for social psychology and the information sciences. Computer mediation adds a new element in the form of distributed networks and group support systems. These facilitate highly organized group activities that were all but impossible before computer mediation. This paper presents experimental findings on group problem solving where a distributed software system automatically integrates input from many humans. In order to quantify Collective Intelligence, we compare the performance of groups to individuals when solving a mathematically formalized problem. This study shows that groups can outperform individuals on difficult but not easy problems, though groups are slower to produce solutions. The subjects are 57 university students. The task is the 8-Puzzle sliding tile game.",
        "published": "2004-12-15T17:13:41Z",
        "link": "http://arxiv.org/abs/cs/0412064v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.OH",
            "H.1.2; H.4.2"
        ]
    },
    {
        "title": "A Framework for Creating Natural Language User Interfaces for   Action-Based Applications",
        "authors": [
            "Stephen Chong",
            "Riccardo Pucella"
        ],
        "summary": "In this paper we present a framework for creating natural language interfaces to action-based applications. Our framework uses a number of reusable application-independent components, in order to reduce the effort of creating a natural language interface for a given application. Using a type-logical grammar, we first translate natural language sentences into expressions in an extended higher-order logic. These expressions can be seen as executable specifications corresponding to the original sentences. The executable specifications are then interpreted by invoking appropriate procedures provided by the application for which a natural language interface is being created.",
        "published": "2004-12-17T03:24:51Z",
        "link": "http://arxiv.org/abs/cs/0412065v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "H.5.2; I.2.7; F.4.2; F.4.1"
        ]
    },
    {
        "title": "Threats of Human Error in a High-Performance Storage System: Problem   Statement and Case Study",
        "authors": [
            "Elizabeth Haubert"
        ],
        "summary": "System administration is a difficult, often tedious, job requiring many skilled laborers. The data that is protected by system administrators is often valued at or above the value of the institution maintaining that data. A number of ethnographic studies have confirmed the skill of these operators, and the difficulty of providing adequate tools. In an effort to minimize the maintenance costs, an increasing portion of system administration is subject to automation - particularly simple, routine tasks such as data backup. While such tools reduce the risk of errors from carelessness, the same tools may result in reduced skill and system familiarity in experienced workers. Care should be taken to ensure that operators maintain system awareness without placing the operator in a passive, monitoring role.",
        "published": "2004-12-17T15:38:19Z",
        "link": "http://arxiv.org/abs/cs/0412074v1",
        "categories": [
            "cs.HC",
            "cs.OS",
            "H.1.2"
        ]
    }
]