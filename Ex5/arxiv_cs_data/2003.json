[
    {
        "title": "Temporal plannability by variance of the episode length",
        "authors": [
            "Balint Takacs",
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "Optimization of decision problems in stochastic environments is usually concerned with maximizing the probability of achieving the goal and minimizing the expected episode length. For interacting agents in time-critical applications, learning of the possibility of scheduling of subtasks (events) or the full task is an additional relevant issue. Besides, there exist highly stochastic problems where the actual trajectories show great variety from episode to episode, but completing the task takes almost the same amount of time. The identification of sub-problems of this nature may promote e.g., planning, scheduling and segmenting Markov decision processes. In this work, formulae for the average duration as well as the standard deviation of the duration of events are derived. The emerging Bellman-type equation is a simple extension of Sobel's work (1982). Methods of dynamic programming as well as methods of reinforcement learning can be applied for our extension. Computer demonstration on a toy problem serve to highlight the principle.",
        "published": "2003-01-09T12:39:03Z",
        "link": "http://arxiv.org/abs/cs/0301006v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Kalman filter control in the reinforcement learning framework",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "There is a growing interest in using Kalman-filter models in brain modelling. In turn, it is of considerable importance to make Kalman-filters amenable for reinforcement learning. In the usual formulation of optimal control it is computed off-line by solving a backward recursion. In this technical note we show that slight modification of the linear-quadratic-Gaussian Kalman-filter model allows the on-line estimation of optimal control and makes the bridge to reinforcement learning. Moreover, the learning rule for value estimation assumes a Hebbian form weighted by the error of the value estimation.",
        "published": "2003-01-09T15:08:47Z",
        "link": "http://arxiv.org/abs/cs/0301007v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Formal Concept Analysis and Resolution in Algebraic Domains",
        "authors": [
            "Pascal Hitzler",
            "Matthias Wendt"
        ],
        "summary": "We relate two formerly independent areas: Formal concept analysis and logic of domains. We will establish a correspondene between contextual attribute logic on formal contexts resp. concept lattices and a clausal logic on coherent algebraic cpos. We show how to identify the notion of formal concept in the domain theoretic setting. In particular, we show that a special instance of the resolution rule from the domain logic coincides with the concept closure operator from formal concept analysis. The results shed light on the use of contexts and domains for knowledge representation and reasoning purposes.",
        "published": "2003-01-09T23:37:57Z",
        "link": "http://arxiv.org/abs/cs/0301008v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3;I.2.4"
        ]
    },
    {
        "title": "Comparisons and Computation of Well-founded Semantics for Disjunctive   Logic Programs",
        "authors": [
            "Kewen Wang",
            "Lizhu Zhou"
        ],
        "summary": "Much work has been done on extending the well-founded semantics to general disjunctive logic programs and various approaches have been proposed. However, these semantics are different from each other and no consensus is reached about which semantics is the most intended. In this paper we look at disjunctive well-founded reasoning from different angles. We show that there is an intuitive form of the well-founded reasoning in disjunctive logic programming which can be characterized by slightly modifying some exisitng approaches to defining disjunctive well-founded semantics, including program transformations, argumentation, unfounded sets (and resolution-like procedure). We also provide a bottom-up procedure for this semantics. The significance of our work is not only in clarifying the relationship among different approaches, but also shed some light on what is an intended well-founded semantics for disjunctive logic programs.",
        "published": "2003-01-14T08:14:43Z",
        "link": "http://arxiv.org/abs/cs/0301010v2",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Convergence and Loss Bounds for Bayesian Sequence Prediction",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The probability of observing $x_t$ at time $t$, given past observations $x_1...x_{t-1}$ can be computed with Bayes' rule if the true generating distribution $\\mu$ of the sequences $x_1x_2x_3...$ is known. If $\\mu$ is unknown, but known to belong to a class $M$ one can base ones prediction on the Bayes mix $\\xi$ defined as a weighted sum of distributions $\\nu\\in M$. Various convergence results of the mixture posterior $\\xi_t$ to the true posterior $\\mu_t$ are presented. In particular a new (elementary) derivation of the convergence $\\xi_t/\\mu_t\\to 1$ is provided, which additionally gives the rate of convergence. A general sequence predictor is allowed to choose an action $y_t$ based on $x_1...x_{t-1}$ and receives loss $\\ell_{x_t y_t}$ if $x_t$ is the next symbol of the sequence. No assumptions are made on the structure of $\\ell$ (apart from being bounded) and $M$. The Bayes-optimal prediction scheme $\\Lambda_\\xi$ based on mixture $\\xi$ and the Bayes-optimal informed prediction scheme $\\Lambda_\\mu$ are defined and the total loss $L_\\xi$ of $\\Lambda_\\xi$ is bounded in terms of the total loss $L_\\mu$ of $\\Lambda_\\mu$. It is shown that $L_\\xi$ is bounded for bounded $L_\\mu$ and $L_\\xi/L_\\mu\\to 1$ for $L_\\mu\\to \\infty$. Convergence of the instantaneous losses are also proven.",
        "published": "2003-01-16T16:36:15Z",
        "link": "http://arxiv.org/abs/cs/0301014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.PR",
            "E.4;I.2.6;G.3"
        ]
    },
    {
        "title": "A semantic framework for preference handling in answer set programming",
        "authors": [
            "Torsten Schaub",
            "Kewen Wang"
        ],
        "summary": "We provide a semantic framework for preference handling in answer set programming. To this end, we introduce preference preserving consequence operators. The resulting fixpoint characterizations provide us with a uniform semantic framework for characterizing preference handling in existing approaches. Although our approach is extensible to other semantics by means of an alternating fixpoint theory, we focus here on the elaboration of preferences under answer set semantics. Alternatively, we show how these approaches can be characterized by the concept of order preservation. These uniform semantic characterizations provide us with new insights about interrelationships and moreover about ways of implementation.",
        "published": "2003-01-23T09:09:31Z",
        "link": "http://arxiv.org/abs/cs/0301023v1",
        "categories": [
            "cs.AI",
            "I.2.3; D.1.6"
        ]
    },
    {
        "title": "Many Hard Examples in Exact Phase Transitions with Application to   Generating Hard Satisfiable Instances",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "This paper first analyzes the resolution complexity of two random CSP models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CNF formulas hard for resolution, which is a central task of Proof-Complexity theory, but also propose models with both many hard instances and exact phase transitions. Then, the implications of such models are addressed. It is shown both theoretically and experimentally that an application of Model RB/RD might be in the generation of hard satisfiable instances, which is not only of practical importance but also related to some open problems in cryptography such as generating one-way functions. Subsequently, a further theoretical support for the generation method is shown by establishing exponential lower bounds on the complexity of solving random satisfiable and forced satisfiable instances of RB/RD near the threshold. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three different kinds of phase transition behavior in NP-complete problems.",
        "published": "2003-02-01T15:58:16Z",
        "link": "http://arxiv.org/abs/cs/0302001v5",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.DM",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "The New AI: General & Sound & Relevant for Physics",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "Most traditional artificial intelligence (AI) systems of the past 50 years are either very limited, or based on heuristics, or both. The new millennium, however, has brought substantial progress in the field of theoretically optimal and practically feasible algorithms for prediction, search, inductive inference based on Occam's razor, problem solving, decision making, and reinforcement learning in environments of a very general type. Since inductive inference is at the heart of all inductive sciences, some of the results are relevant not only for AI and computer science but also for physics, provoking nontraditional predictions based on Zuse's thesis of the computer-generated universe.",
        "published": "2003-02-10T14:17:33Z",
        "link": "http://arxiv.org/abs/cs/0302012v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "quant-ph",
            "I.2"
        ]
    },
    {
        "title": "Unsupervised Learning in a Framework of Information Compression by   Multiple Alignment, Unification and Search",
        "authors": [
            "J. G. Wolff"
        ],
        "summary": "This paper describes a novel approach to unsupervised learning that has been developed within a framework of \"information compression by multiple alignment, unification and search\" (ICMAUS), designed to integrate learning with other AI functions such as parsing and production of language, fuzzy pattern recognition, probabilistic and exact forms of reasoning, and others.",
        "published": "2003-02-12T09:39:00Z",
        "link": "http://arxiv.org/abs/cs/0302015v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.4; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Defeasible Logic Programming: An Argumentative Approach",
        "authors": [
            "Alejandro Javier Garcia",
            "Guillermo Ricardo Simari"
        ],
        "summary": "The work reported here introduces Defeasible Logic Programming (DeLP), a formalism that combines results of Logic Programming and Defeasible Argumentation. DeLP provides the possibility of representing information in the form of weak rules in a declarative manner, and a defeasible argumentation inference mechanism for warranting the entailed conclusions.   In DeLP an argumentation formalism will be used for deciding between contradictory goals. Queries will be supported by arguments that could be defeated by other arguments. A query q will succeed when there is an argument A for q that is warranted, ie, the argument A that supports q is found undefeated by a warrant procedure that implements a dialectical analysis.   The defeasible argumentation basis of DeLP allows to build applications that deal with incomplete and contradictory information in dynamic domains. Thus, the resulting approach is suitable for representing agent's knowledge and for providing an argumentation based reasoning mechanism to agents.",
        "published": "2003-02-20T00:48:06Z",
        "link": "http://arxiv.org/abs/cs/0302029v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Constraint-based analysis of composite solvers",
        "authors": [
            "Evgueni Petrov",
            "Eric Monfroy"
        ],
        "summary": "Cooperative constraint solving is an area of constraint programming that studies the interaction between constraint solvers with the aim of discovering the interaction patterns that amplify the positive qualities of individual solvers. Automatisation and formalisation of such studies is an important issue of cooperative constraint solving.   In this paper we present a constraint-based analysis of composite solvers that integrates reasoning about the individual solvers and the processed data. The idea is to approximate this reasoning by resolution of set constraints on the finite sets representing the predicates that express all the necessary properties. We illustrate application of our analysis to two important cooperation patterns: deterministic choice and loop.",
        "published": "2003-02-25T14:33:08Z",
        "link": "http://arxiv.org/abs/cs/0302036v2",
        "categories": [
            "cs.AI",
            "I.2"
        ]
    },
    {
        "title": "Tight Logic Programs",
        "authors": [
            "Esra Erdem",
            "Vladimir Lifschitz"
        ],
        "summary": "This note is about the relationship between two theories of negation as failure -- one based on program completion, the other based on stable models, or answer sets. Francois Fages showed that if a logic program satisfies a certain syntactic condition, which is now called ``tightness,'' then its stable models can be characterized as the models of its completion. We extend the definition of tightness and Fages' theorem to programs with nested expressions in the bodies of rules, and study tight logic programs containing the definition of the transitive closure of a predicate.",
        "published": "2003-02-28T01:28:22Z",
        "link": "http://arxiv.org/abs/cs/0302038v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Kalman-filtering using local interactions",
        "authors": [
            "Barnabas Poczos",
            "Andras Lorincz"
        ],
        "summary": "There is a growing interest in using Kalman-filter models for brain modelling. In turn, it is of considerable importance to represent Kalman-filter in connectionist forms with local Hebbian learning rules. To our best knowledge, Kalman-filter has not been given such local representation. It seems that the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a connectionist representation is presented, which is derived by means of the recursive prediction error method. We show that this method gives rise to attractive local learning rules and can adapt the Kalman-gain.",
        "published": "2003-02-28T18:32:26Z",
        "link": "http://arxiv.org/abs/cs/0302039v1",
        "categories": [
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "On the Notion of Cognition",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "We discuss philosophical issues concerning the notion of cognition basing ourselves in experimental results in cognitive sciences, especially in computer simulations of cognitive systems. There have been debates on the \"proper\" approach for studying cognition, but we have realized that all approaches can be in theory equivalent. Different approaches model different properties of cognitive systems from different perspectives, so we can only learn from all of them. We also integrate ideas from several perspectives for enhancing the notion of cognition, such that it can contain other definitions of cognition as special cases. This allows us to propose a simple classification of different types of cognition.",
        "published": "2003-03-10T18:20:28Z",
        "link": "http://arxiv.org/abs/cs/0303006v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Unfolding Partiality and Disjunctions in Stable Model Semantics",
        "authors": [
            "T. Janhunen",
            "I. Niemela",
            "D. Seipel",
            "P. Simons",
            "J. You"
        ],
        "summary": "The paper studies an implementation methodology for partial and disjunctive stable models where partiality and disjunctions are unfolded from a logic program so that an implementation of stable models for normal (disjunction-free) programs can be used as the core inference engine. The unfolding is done in two separate steps. Firstly, it is shown that partial stable models can be captured by total stable models using a simple linear and modular program transformation. Hence, reasoning tasks concerning partial stable models can be solved using an implementation of total stable models. Disjunctive partial stable models have been lacking implementations which now become available as the translation handles also the disjunctive case. Secondly, it is shown how total stable models of disjunctive programs can be determined by computing stable models for normal programs. Hence, an implementation of stable models of normal programs can be used as a core engine for implementing disjunctive programs. The feasibility of the approach is demonstrated by constructing a system for computing stable models of disjunctive programs using the smodels system as the core engine. The performance of the resulting system is compared to that of dlv which is a state-of-the-art special purpose system for disjunctive programs.",
        "published": "2003-03-14T14:29:32Z",
        "link": "http://arxiv.org/abs/cs/0303009v2",
        "categories": [
            "cs.AI",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "A Neural Network Assembly Memory Model with Maximum-Likelihood Recall   and Recognition Properties",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "It has been shown that a neural network model recently proposed to describe basic memory performance is based on a ternary/binary coding/decoding algorithm which leads to a new neural network assembly memory model (NNAMM) providing maximum-likelihood recall/recognition properties and implying a new memory unit architecture with Hopfield two-layer network, N-channel time gate, auxiliary reference memory, and two nested feedback loops. For the data coding used, conditions are found under which a version of Hopfied network implements maximum-likelihood convolutional decoding algorithm and, simultaneously, linear statistical classifier of arbitrary binary vectors with respect to Hamming distance between vector analyzed and reference vector given. In addition to basic memory performance and etc, the model explicitly describes the dependence on time of memory trace retrieval, gives a possibility of one-trial learning, metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. It has been shown that an assembly memory unit may be viewed as a model of a smallest inseparable part or an 'atom' of consciousness. Some nontraditional neurobiological backgrounds (dynamic spatiotemporal synchrony, properties of time dependent and error detector neurons, early precise spike firing, etc) and the model's application to solve some interdisciplinary problems from different scientific fields are discussed.",
        "published": "2003-03-19T23:17:16Z",
        "link": "http://arxiv.org/abs/cs/0303017v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "Multi-target particle filtering for the probability hypothesis density",
        "authors": [
            "Hedvig Sidenbladh"
        ],
        "summary": "When tracking a large number of targets, it is often computationally expensive to represent the full joint distribution over target states. In cases where the targets move independently, each target can instead be tracked with a separate filter. However, this leads to a model-data association problem. Another approach to solve the problem with computational complexity is to track only the first moment of the joint distribution, the probability hypothesis density (PHD). The integral of this distribution over any area S is the expected number of targets within S. Since no record of object identity is kept, the model-data association problem is avoided.   The contribution of this paper is a particle filter implementation of the PHD filter mentioned above. This PHD particle filter is applied to tracking of multiple vehicles in terrain, a non-linear tracking problem. Experiments show that the filter can track a changing number of vehicles robustly, achieving near-real-time performance.",
        "published": "2003-03-20T13:48:04Z",
        "link": "http://arxiv.org/abs/cs/0303018v1",
        "categories": [
            "cs.AI",
            "G.3"
        ]
    },
    {
        "title": "Conferences with Internet Web-Casting as Binding Events in a Global   Brain: Example Data From Complexity Digest",
        "authors": [
            "A. Das",
            "G. Mayer-Kress",
            "C. Gershenson",
            "P. Das"
        ],
        "summary": "There is likeness of the Internet to human brains which has led to the metaphor of the world-wide computer network as a `Global Brain'. We consider conferences as 'binding events' in the Global Brain that can lead to metacognitive structures on a global scale. One of the critical factors for that phenomenon to happen (similar to the biological brain) are the time-scales characteristic for the information exchange. In an electronic newsletter- the Complexity Digest (ComDig) we include webcasting of audio (mp3) and video (asf) files from international conferences in the weekly ComDig issues. Here we present the time variation of the weekly rate of accesses to the conference files. From those empirical data it appears that the characteristic time-scales related to access of web-casting files is of the order of a few weeks. This is at least an order of magnitude shorter than the characteristic time-scales of peer reviewed publications and conference proceedings. We predict that this observation will have profound implications on the nature of future conference proceedings, presumably in electronic form.",
        "published": "2003-03-22T13:13:20Z",
        "link": "http://arxiv.org/abs/cs/0303023v1",
        "categories": [
            "cs.NI",
            "cs.AI",
            "A.m"
        ]
    },
    {
        "title": "A Method for Clustering Web Attacks Using Edit Distance",
        "authors": [
            "Slobodan Petrovic",
            "Gonzalo Alvarez"
        ],
        "summary": "Cluster analysis often serves as the initial step in the process of data classification. In this paper, the problem of clustering different length input data is considered. The edit distance as the minimum number of elementary edit operations needed to transform one vector into another is used. A heuristic for clustering unequal length vectors, analogue to the well known k-means algorithm is described and analyzed. This heuristic determines cluster centroids expanding shorter vectors to the lengths of the longest ones in each cluster in a specific way. It is shown that the time and space complexities of the heuristic are linear in the number of input vectors. Experimental results on real data originating from a system for classification of Web attacks are given.",
        "published": "2003-04-03T10:17:02Z",
        "link": "http://arxiv.org/abs/cs/0304007v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CR",
            "H.3.3;K.6.5"
        ]
    },
    {
        "title": "Determining possible avenues of approach using ANTS",
        "authors": [
            "Pontus Svenson",
            "Hedvig Sidenbladh"
        ],
        "summary": "Threat assessment is an important part of level 3 data fusion. Here we study a subproblem of this, worst-case risk assessment. Inspired by agent-based models used for simulation of trail formation for urban planning, we use ant colony optimization (ANTS) to determine possible avenues of approach for the enemy, given a situation picture.   One way of determining such avenues would be to calculate the ``potential field'' caused by placing sources at possible goals for the enemy. This requires postulating a functional form for the potential, and also takes long time. Here we instead seek a method for quickly obtaining an effective potential. ANTS, which has previously been used to obtain approximate solutions to various optimization problems, is well suited for this. The output of our method describes possible avenues of approach for the enemy, i.e, areas where we should be prepared for attack. (The algorithm can also be run ``reversed'' to instead get areas of opportunity for our forces to exploit.)   Using real geographical data, we found that our method gives a fast and reliable way of determining such avenues. Our method can be used in a computer-based command and control system to replace the first step of human intelligence analysis.",
        "published": "2003-04-04T15:25:51Z",
        "link": "http://arxiv.org/abs/nlin/0304006v1",
        "categories": [
            "nlin.AO",
            "cs.AI"
        ]
    },
    {
        "title": "A Framework for Searching AND/OR Graphs with Cycles",
        "authors": [
            "Ambuj Mahanti",
            "Supriyo Ghose",
            "Samir K. Sadhukhan"
        ],
        "summary": "Search in cyclic AND/OR graphs was traditionally known to be an unsolved problem. In the recent past several important studies have been reported in this domain. In this paper, we have taken a fresh look at the problem. First, a new and comprehensive theoretical framework for cyclic AND/OR graphs has been presented, which was found missing in the recent literature. Based on this framework, two best-first search algorithms, S1 and S2, have been developed. S1 does uninformed search and is a simple modification of the Bottom-up algorithm by Martelli and Montanari. S2 performs a heuristically guided search and replicates the modification in Bottom-up's successors, namely HS and AO*. Both S1 and S2 solve the problem of searching AND/OR graphs in presence of cycles. We then present a detailed analysis for the correctness and complexity results of S1 and S2, using the proposed framework. We have observed through experiments that S1 and S2 output correct results in all cases.",
        "published": "2003-05-01T04:48:29Z",
        "link": "http://arxiv.org/abs/cs/0305001v1",
        "categories": [
            "cs.AI",
            "I.2.8"
        ]
    },
    {
        "title": "Approximate Grammar for Information Extraction",
        "authors": [
            "V. Sriram",
            "B. Ravi Sekar Reddy",
            "R. Sangal"
        ],
        "summary": "In this paper, we present the concept of Approximate grammar and how it can be used to extract information from a documemt. As the structure of informational strings cannot be defined well in a document, we cannot use the conventional grammar rules to represent the information. Hence, the need arises to design an approximate grammar that can be used effectively to accomplish the task of Information extraction. Approximate grammars are a novel step in this direction. The rules of an approximate grammar can be given by a user or the machine can learn the rules from an annotated document. We have performed our experiments in both the above areas and the results have been impressive.",
        "published": "2003-05-06T14:06:48Z",
        "link": "http://arxiv.org/abs/cs/0305004v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Time-scales, Meaning, and Availability of Information in a Global Brain",
        "authors": [
            "Carlos Gershenson",
            "Gottfried Mayer-Kress",
            "Atin Das",
            "Pritha Das",
            "Matus Marko"
        ],
        "summary": "We note the importance of time-scales, meaning, and availability of information for the emergence of novel information meta-structures at a global scale. We discuss previous work in this area and develop future perspectives. We focus on the transmission of scientific articles and the integration of traditional conferences with their virtual extensions on the Internet, their time-scales, and availability. We mention the Semantic Web as an effort for integrating meaningful information.",
        "published": "2003-05-15T16:30:49Z",
        "link": "http://arxiv.org/abs/cs/0305012v2",
        "categories": [
            "cs.AI",
            "cs.CY",
            "cs.NI",
            "C.2.m; H.0; K.4.0"
        ]
    },
    {
        "title": "On Nonspecific Evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "When simultaneously reasoning with evidences about several different events it is necessary to separate the evidence according to event. These events should then be handled independently. However, when propositions of evidences are weakly specified in the sense that it may not be certain to which event they are referring, this may not be directly possible. In this paper a criterion for partitioning evidences into subsets representing events is established. This criterion, derived from the conflict within each subset, involves minimising a criterion function for the overall conflict of the partition. An algorithm based on characteristics of the criterion function and an iterative optimisation among partitionings of evidences is proposed.",
        "published": "2003-05-16T13:39:19Z",
        "link": "http://arxiv.org/abs/cs/0305013v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Dempster's Rule for Evidence Ordered in a Complete Directed Acyclic   Graph",
        "authors": [
            "Ulla Bergsten",
            "Johan Schubert"
        ],
        "summary": "For the case of evidence ordered in a complete directed acyclic graph this paper presents a new algorithm with lower computational complexity for Dempster's rule than that of step-by-step application of Dempster's rule. In this problem, every original pair of evidences, has a corresponding evidence against the simultaneous belief in both propositions. In this case, it is uncertain whether the propositions of any two evidences are in logical conflict. The original evidences are associated with the vertices and the additional evidences are associated with the edges. The original evidences are ordered, i.e., for every pair of evidences it is determinable which of the two evidences is the earlier one. We are interested in finding the most probable completely specified path through the graph, where transitions are possible only from lower- to higher-ranked vertices. The path is here a representation for a sequence of states, for instance a sequence of snapshots of a physical object's track. A completely specified path means that the path includes no other vertices than those stated in the path representation, as opposed to an incompletely specified path that may also include other vertices than those stated. In a hierarchical network of all subsets of the frame, i.e., of all incompletely specified paths, the original and additional evidences support subsets that are not disjoint, thus it is not possible to prune the network to a tree. Instead of propagating belief, the new algorithm reasons about the logical conditions of a completely specified path through the graph. The new algorithm is O(|THETA| log |THETA|), compared to O(|THETA| ** log |THETA|) of the classic brute force algorithm.",
        "published": "2003-05-16T14:23:00Z",
        "link": "http://arxiv.org/abs/cs/0305014v1",
        "categories": [
            "cs.AI",
            "cs.DM",
            "G.2.2; I.2.3; I.4.8"
        ]
    },
    {
        "title": "Finding a Posterior Domain Probability Distribution by Specifying   Nonspecific Evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "This article is an extension of the results of two earlier articles. In [J. Schubert, On nonspecific evidence, Int. J. Intell. Syst. 8 (1993) 711-725] we established within Dempster-Shafer theory a criterion function called the metaconflict function. With this criterion we can partition into subsets a set of several pieces of evidence with propositions that are weakly specified in the sense that it may be uncertain to which event a proposition is referring. In a second article [J. Schubert, Specifying nonspecific evidence, in Cluster-based specification techniques in Dempster-Shafer theory for an evidential intelligence analysis of multiple target tracks, Ph.D. Thesis, TRITA-NA-9410, Royal Institute of Technology, Stockholm, 1994, ISBN 91-7170-801-4] we not only found the most plausible subset for each piece of evidence, we also found the plausibility for every subset that this piece of evidence belongs to the subset. In this article we aim to find a posterior probability distribution regarding the number of subsets. We use the idea that each piece of evidence in a subset supports the existence of that subset to the degree that this piece of evidence supports anything at all. From this we can derive a bpa that is concerned with the question of how many subsets we have. That bpa can then be combined with a given prior domain probability distribution in order to obtain the sought-after posterior domain distribution.",
        "published": "2003-05-16T14:36:03Z",
        "link": "http://arxiv.org/abs/cs/0305015v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Cluster-based Specification Techniques in Dempster-Shafer Theory",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "When reasoning with uncertainty there are many situations where evidences are not only uncertain but their propositions may also be weakly specified in the sense that it may not be certain to which event a proposition is referring. It is then crucial not to combine such evidences in the mistaken belief that they are referring to the same event. This situation would become manageable if the evidences could be clustered into subsets representing events that should be handled separately. In an earlier article we established within Dempster-Shafer theory a criterion function called the metaconflict function. With this criterion we can partition a set of evidences into subsets. Each subset representing a separate event. In this article we will not only find the most plausible subset for each piece of evidence, we will also find the plausibility for every subset that the evidence belongs to the subset. Also, when the number of subsets are uncertain we aim to find a posterior probability distribution regarding the number of subsets.",
        "published": "2003-05-16T14:46:37Z",
        "link": "http://arxiv.org/abs/cs/0305017v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Cluster-based Specification Techniques in Dempster-Shafer Theory for an   Evidential Intelligence Analysis of MultipleTarget Tracks (Thesis Abstract)",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In Intelligence Analysis it is of vital importance to manage uncertainty. Intelligence data is almost always uncertain and incomplete, making it necessary to reason and taking decisions under uncertainty. One way to manage the uncertainty in Intelligence Analysis is Dempster-Shafer Theory. This thesis contains five results regarding multiple target tracks and intelligence specification.",
        "published": "2003-05-16T14:59:45Z",
        "link": "http://arxiv.org/abs/cs/0305018v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "On rho in a Decision-Theoretic Apparatus of Dempster-Shafer Theory",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "Thomas M. Strat has developed a decision-theoretic apparatus for Dempster-Shafer theory (Decision analysis using belief functions, Intern. J. Approx. Reason. 4(5/6), 391-417, 1990). In this apparatus, expected utility intervals are constructed for different choices. The choice with the highest expected utility is preferable to others. However, to find the preferred choice when the expected utility interval of one choice is included in that of another, it is necessary to interpolate a discerning point in the intervals. This is done by the parameter rho, defined as the probability that the ambiguity about the utility of every nonsingleton focal element will turn out as favorable as possible. If there are several different decision makers, we might sometimes be more interested in having the highest expected utility among the decision makers rather than only trying to maximize our own expected utility regardless of choices made by other decision makers. The preference of each choice is then determined by the probability of yielding the highest expected utility. This probability is equal to the maximal interval length of rho under which an alternative is preferred. We must here take into account not only the choices already made by other decision makers but also the rational choices we can assume to be made by later decision makers. In Strats apparatus, an assumption, unwarranted by the evidence at hand, has to be made about the value of rho. We demonstrate that no such assumption is necessary. It is sufficient to assume a uniform probability distribution for rho to be able to discern the most preferable choice. We discuss when this approach is justifiable.",
        "published": "2003-05-16T15:07:09Z",
        "link": "http://arxiv.org/abs/cs/0305019v1",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "Specifying nonspecific evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In an earlier article [J. Schubert, On nonspecific evidence, Int. J. Intell. Syst. 8(6), 711-725 (1993)] we established within Dempster-Shafer theory a criterion function called the metaconflict function. With this criterion we can partition into subsets a set of several pieces of evidence with propositions that are weakly specified in the sense that it may be uncertain to which event a proposition is referring. Each subset in the partitioning is representing a separate event. The metaconflict function was derived as the plausibility that the partitioning is correct when viewing the conflict in Dempster's rule within each subset as a newly constructed piece of metalevel evidence with a proposition giving support against the entire partitioning. In this article we extend the results of the previous article. We will not only find the most plausible subset for each piece of evidence as was done in the earlier article. In addition we will specify each piece of nonspecific evidence, in the sense that we find to which events the proposition might be referring, by finding the plausibility for every subset that this piece of evidence belong to the subset. In doing this we will automatically receive indication that some evidence might be false. We will then develop a new methodology to exploit these newly specified pieces of evidence in a subsequent reasoning process. This will include methods to discount evidence based on their degree of falsity and on their degree of credibility due to a partial specification of affiliation, as well as a refined method to infer the event of each subset.",
        "published": "2003-05-16T15:13:22Z",
        "link": "http://arxiv.org/abs/cs/0305020v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Creating Prototypes for Fast Classification in Dempster-Shafer   Clustering",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "We develop a classification method for incoming pieces of evidence in Dempster-Shafer theory. This methodology is based on previous work with clustering and specification of originally nonspecific evidence. This methodology is here put in order for fast classification of future incoming pieces of evidence by comparing them with prototypes representing the clusters, instead of making a full clustering of all evidence. This method has a computational complexity of O(M * N) for each new piece of evidence, where M is the maximum number of subsets and N is the number of prototypes chosen for each subset. That is, a computational complexity independent of the total number of previously arrived pieces of evidence. The parameters M and N are typically fixed and domain dependent in any application.",
        "published": "2003-05-16T15:32:14Z",
        "link": "http://arxiv.org/abs/cs/0305021v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Applying Data Mining and Machine Learning Techniques to Submarine   Intelligence Analysis",
        "authors": [
            "Ulla Bergsten",
            "Johan Schubert",
            "Per Svensson"
        ],
        "summary": "We describe how specialized database technology and data analysis methods were applied by the Swedish defense to help deal with the violation of Swedish marine territory by foreign submarine intruders during the Eighties and early Nineties. Among several approaches tried some yielded interesting information, although most of the key questions remain unanswered. We conclude with a survey of belief-function- and genetic-algorithm-based methods which were proposed to support interpretation of intelligence reports and prediction of future submarine positions, respectively.",
        "published": "2003-05-16T15:41:58Z",
        "link": "http://arxiv.org/abs/cs/0305022v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.NE",
            "H.2.8; H.4.2; I.2.3; I.5.3"
        ]
    },
    {
        "title": "Fast Dempster-Shafer clustering using a neural network structure",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we study a problem within Dempster-Shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. However, for large scale problems we need a method with lower computational complexity. The neural structure was found to be effective and much faster than iterative optimization for larger problems. While the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems, the metaconflict per cluster and evidence was moderate. The neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters.",
        "published": "2003-05-16T15:48:24Z",
        "link": "http://arxiv.org/abs/cs/0305023v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "A neural network and iterative optimization hybrid for Dempster-Shafer   clustering",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we extend an earlier result within Dempster-Shafer theory [\"Fast Dempster-Shafer Clustering Using a Neural Network Structure,\" in Proc. Seventh Int. Conf. Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU 98)] where a large number of pieces of evidence are clustered into subsets by a neural network structure. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. While the neural method had a much lower computation time than iterative optimization its average clustering performance was not as good. Here, we develop a hybrid of the two methods. We let the neural structure do the initial clustering in order to achieve a high computational performance. Its solution is fed as the initial state to the iterative optimization in order to improve the clustering performance.",
        "published": "2003-05-16T15:54:46Z",
        "link": "http://arxiv.org/abs/cs/0305024v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Simultaneous Dempster-Shafer clustering and gradual determination of   number of clusters using a neural network structure",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we extend an earlier result within Dempster-Shafer theory [\"Fast Dempster-Shafer Clustering Using a Neural Network Structure,\" in Proc. Seventh Int. Conf. Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU'98)] where several pieces of evidence were clustered into a fixed number of clusters using a neural structure. This was done by minimizing a metaconflict function. We now develop a method for simultaneous clustering and determination of number of clusters during iteration in the neural structure. We let the output signals of neurons represent the degree to which a pieces of evidence belong to a corresponding cluster. From these we derive a probability distribution regarding the number of clusters, which gradually during the iteration is transformed into a determination of number of clusters. This gradual determination is fed back into the neural structure at each iteration to influence the clustering process.",
        "published": "2003-05-16T16:00:23Z",
        "link": "http://arxiv.org/abs/cs/0305025v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Fast Dempster-Shafer clustering using a neural network structure",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this article we study a problem within Dempster-Shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. However, for large scale problems we need a method with lower computational complexity. The neural structure was found to be effective and much faster than iterative optimization for larger problems. While the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems, the metaconflict per cluster and evidence was moderate. The neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters.",
        "published": "2003-05-16T16:06:22Z",
        "link": "http://arxiv.org/abs/cs/0305026v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Managing Inconsistent Intelligence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we demonstrate that it is possible to manage intelligence in constant time as a pre-process to information fusion through a series of processes dealing with issues such as clustering reports, ranking reports with respect to importance, extraction of prototypes from clusters and immediate classification of newly arriving intelligence reports. These methods are used when intelligence reports arrive which concerns different events which should be handled independently, when it is not known a priori to which event each intelligence report is related. We use clustering that runs as a back-end process to partition the intelligence into subsets representing the events, and in parallel, a fast classification that runs as a front-end process in order to put the newly arriving intelligence into its correct information fusion process.",
        "published": "2003-05-16T16:16:17Z",
        "link": "http://arxiv.org/abs/cs/0305027v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Dempster-Shafer clustering using Potts spin mean field theory",
        "authors": [
            "Mats Bengtsson",
            "Johan Schubert"
        ],
        "summary": "In this article we investigate a problem within Dempster-Shafer theory where 2**q - 1 pieces of evidence are clustered into q clusters by minimizing a metaconflict function, or equivalently, by minimizing the sum of weight of conflict over all clusters. Previously one of us developed a method based on a Hopfield and Tank model. However, for very large problems we need a method with lower computational complexity. We demonstrate that the weight of conflict of evidence can, as an approximation, be linearized and mapped to an antiferromagnetic Potts Spin model. This facilitates efficient numerical solution, even for large problem sizes. Optimal or nearly optimal solutions are found for Dempster-Shafer clustering benchmark tests with a time complexity of approximately O(N**2 log**2 N). Furthermore, an isomorphism between the antiferromagnetic Potts spin model and a graph optimization problem is shown. The graph model has dynamic variables living on the links, which have a priori probabilities that are directly related to the pairwise conflict between pieces of evidence. Hence, the relations between three different models are shown.",
        "published": "2003-05-16T16:28:20Z",
        "link": "http://arxiv.org/abs/cs/0305028v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Conflict-based Force Aggregation",
        "authors": [
            "John Cantwell",
            "Johan Schubert",
            "Johan Walter"
        ],
        "summary": "In this paper we present an application where we put together two methods for clustering and classification into a force aggregation method. Both methods are based on conflicts between elements. These methods work with different type of elements (intelligence reports, vehicles, military units) on different hierarchical levels using specific conflict assessment methods on each level. We use Dempster-Shafer theory for conflict calculation between elements, Dempster-Shafer clustering for clustering these elements, and templates for classification. The result of these processes is a complete force aggregation on all levels handled.",
        "published": "2003-05-16T16:37:01Z",
        "link": "http://arxiv.org/abs/cs/0305029v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "H.4.2; I.2.3; I.2.6; I.5.3; J.7"
        ]
    },
    {
        "title": "Reliable Force Aggregation Using a Refined Evidence Specification from   Dempster-Shafer Clustering",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we develop methods for selection of templates and use these templates to recluster an already performed Dempster-Shafer clustering taking into account intelligence to template fit during the reclustering phase. By this process the risk of erroneous force aggregation based on some misplace pieces of evidence from the first clustering process is greatly reduced. Finally, a more reliable force aggregation is performed using the result of the second clustering. These steps are taken in order to maintain most of the excellent computational performance of Dempster-Shafer clustering, while at the same time improve on the clustering result by including some higher relations among intelligence reports described by the templates. The new improved algorithm has a computational complexity of O(n**3 log**2 n) compared to O(n**2 log**2 n) of standard Dempster-Shafer clustering using Potts spin mean field theory.",
        "published": "2003-05-16T16:45:38Z",
        "link": "http://arxiv.org/abs/cs/0305030v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Clustering belief functions based on attracting and conflicting   metalevel evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we develop a method for clustering belief functions based on attracting and conflicting metalevel evidence. Such clustering is done when the belief functions concern multiple events, and all belief functions are mixed up. The clustering process is used as the means for separating the belief functions into subsets that should be handled independently. While the conflicting metalevel evidence is generated internally from pairwise conflicts of all belief functions, the attracting metalevel evidence is assumed given by some external source.",
        "published": "2003-05-16T16:52:48Z",
        "link": "http://arxiv.org/abs/cs/0305031v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Robust Report Level Cluster-to-Track Fusion",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we develop a method for report level tracking based on Dempster-Shafer clustering using Potts spin neural networks where clusters of incoming reports are gradually fused into existing tracks, one cluster for each track. Incoming reports are put into a cluster and continuous reclustering of older reports is made in order to obtain maximum association fit within the cluster and towards the track. Over time, the oldest reports of the cluster leave the cluster for the fixed track at the same rate as new incoming reports are put into it. Fusing reports to existing tracks in this fashion allows us to take account of both existing tracks and the probable future of each track, as represented by younger reports within the corresponding cluster. This gives us a robust report-to-track association. Compared to clustering of all available reports this approach is computationally faster and has a better report-to-track association than simple step-by-step association.",
        "published": "2003-05-16T16:58:38Z",
        "link": "http://arxiv.org/abs/cs/0305032v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.4.8; I.5.3"
        ]
    },
    {
        "title": "Beslutstödssystemet Dezzy - en översikt",
        "authors": [
            "Ulla Bergsten",
            "Johan Schubert",
            "Per Svensson"
        ],
        "summary": "Within the scope of the three-year ANTI-SUBMARINE WARFARE project of the National Defence Research Establishment, the INFORMATION SYSTEMS subproject has developed the demonstration prototype Dezzy for handling and analysis of intelligence reports concerning foreign underwater activities.   -----   Inom ramen f\\\"or FOA:s tre{\\aa}riga huvudprojekt UB{\\AA}TSSKYDD har delprojekt INFORMATIONSSYSTEM utvecklat demonstrationsprototypen Dezzy till ett beslutsst\\\"odsystem f\\\"or hantering och analys av underr\\\"attelser om fr\\\"ammande undervattensverksamhet.",
        "published": "2003-05-16T18:26:22Z",
        "link": "http://arxiv.org/abs/cs/0305033v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "H.4.2; I.2.3"
        ]
    },
    {
        "title": "Bounded LTL Model Checking with Stable Models",
        "authors": [
            "Keijo Heljanko",
            "Ilkka Niemelä"
        ],
        "summary": "In this paper bounded model checking of asynchronous concurrent systems is introduced as a promising application area for answer set programming. As the model of asynchronous systems a generalisation of communicating automata, 1-safe Petri nets, are used. It is shown how a 1-safe Petri net and a requirement on the behaviour of the net can be translated into a logic program such that the bounded model checking problem for the net can be solved by computing stable models of the corresponding program. The use of the stable model semantics leads to compact encodings of bounded reachability and deadlock detection tasks as well as the more general problem of bounded model checking of linear temporal logic. Correctness proofs of the devised translations are given, and some experimental results using the translation and the Smodels system are presented.",
        "published": "2003-05-23T17:16:24Z",
        "link": "http://arxiv.org/abs/cs/0305040v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.2.4;D.2.2;F.4.1;I.2.4;F.3.1"
        ]
    },
    {
        "title": "Updating beliefs with incomplete observations",
        "authors": [
            "Gert de Cooman",
            "Marco Zaffalon"
        ],
        "summary": "Currently, there is renewed interest in the problem, raised by Shafer in 1985, of updating probabilities when observations are incomplete. This is a fundamental problem in general, and of particular interest for Bayesian networks. Recently, Grunwald and Halpern have shown that commonly used updating strategies fail in this case, except under very special assumptions. In this paper we propose a new method for updating probabilities with incomplete observations. Our approach is deliberately conservative: we make no assumptions about the so-called incompleteness mechanism that associates complete with incomplete observations. We model our ignorance about this mechanism by a vacuous lower prevision, a tool from the theory of imprecise probabilities, and we use only coherence arguments to turn prior into posterior probabilities. In general, this new approach to updating produces lower and upper posterior probabilities and expectations, as well as partially determinate decisions. This is a logical consequence of the existing ignorance about the incompleteness mechanism. We apply the new approach to the problem of classification of new evidence in probabilistic expert systems, where it leads to a new, so-called conservative updating rule. In the special case of Bayesian networks constructed using expert knowledge, we provide an exact algorithm for classification based on our updating rule, which has linear-time complexity for a class of networks wider than polytrees. This result is then extended to the more general framework of credal networks, where computations are often much harder than with Bayesian nets. Using an example, we show that our rule appears to provide a solid basis for reliable updating with incomplete observations, when no strong assumptions about the incompleteness mechanism are justified.",
        "published": "2003-05-27T11:05:52Z",
        "link": "http://arxiv.org/abs/cs/0305044v2",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "On the Existence and Convergence Computable Universal Priors",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff unified Occam's razor and Epicurus' principle of multiple explanations to one elegant, formal, universal theory of inductive inference, which initiated the field of algorithmic information theory. His central result is that the posterior of his universal semimeasure M converges rapidly to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal predictor in case of unknown mu. We investigate the existence and convergence of computable universal (semi)measures for a hierarchy of computability classes: finitely computable, estimable, enumerable, and approximable. For instance, M is known to be enumerable, but not finitely computable, and to dominate all enumerable semimeasures. We define seven classes of (semi)measures based on these four computability concepts. Each class may or may not contain a (semi)measure which dominates all elements of another class. The analysis of these 49 cases can be reduced to four basic cases, two of them being new. The results hold for discrete and continuous semimeasures. We also investigate more closely the types of convergence, possibly implied by universality: in difference and in ratio, with probability 1, in mean sum, and for Martin-Loef random sequences. We introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues.",
        "published": "2003-05-29T11:11:01Z",
        "link": "http://arxiv.org/abs/cs/0305052v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "math.ST",
            "stat.TH",
            "G.3; I.2"
        ]
    },
    {
        "title": "Minimum Model Semantics for Logic Programs with Negation-as-Failure",
        "authors": [
            "Panos Rondogiannis",
            "William W. Wadge"
        ],
        "summary": "We give a purely model-theoretic characterization of the semantics of logic programs with negation-as-failure allowed in clause bodies. In our semantics the meaning of a program is, as in the classical case, the unique minimum model in a program-independent ordering. We use an expanded truth domain that has an uncountable linearly ordered set of truth values between False (the minimum element) and True (the maximum), with a Zero element in the middle. The truth values below Zero are ordered like the countable ordinals. The values above Zero have exactly the reverse order. Negation is interpreted as reflection about Zero followed by a step towards Zero; the only truth value that remains unaffected by negation is Zero. We show that every program has a unique minimum model M_P, and that this model can be constructed with a T_P iteration which proceeds through the countable ordinals. Furthermore, we demonstrate that M_P can also be obtained through a model intersection construction which generalizes the well-known model intersection theorem for classical logic programming. Finally, we show that by collapsing the true and false values of the infinite-valued model M_P to (the classical) True and False, we obtain a three-valued model identical to the well-founded one.",
        "published": "2003-06-03T12:12:40Z",
        "link": "http://arxiv.org/abs/cs/0306017v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "F.3.2;F.4.1;D.1.6;I.2.3"
        ]
    },
    {
        "title": "Techniques for effective vocabulary selection",
        "authors": [
            "Anand Venkataraman",
            "Wen Wang"
        ],
        "summary": "The vocabulary of a continuous speech recognition (CSR) system is a significant factor in determining its performance. In this paper, we present three principled approaches to select the target vocabulary for a particular domain by trading off between the target out-of-vocabulary (OOV) rate and vocabulary size. We evaluate these approaches against an ad-hoc baseline strategy. Results are presented in the form of OOV rate graphs plotted against increasing vocabulary size for each technique.",
        "published": "2003-06-04T23:08:03Z",
        "link": "http://arxiv.org/abs/cs/0306022v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "Sequence Prediction based on Monotone Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "This paper studies sequence prediction based on the monotone Kolmogorov complexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is extremely close to Solomonoff's prior M, the latter being an excellent predictor in deterministic as well as probabilistic environments, where performance is measured in terms of convergence of posteriors or losses. Despite this closeness to M, it is difficult to assess the prediction quality of m, since little is known about the closeness of their posteriors, which are the important quantities for prediction. We show that for deterministic computable environments, the \"posterior\" and losses of m converge, but rapid convergence could only be shown on-sequence; the off-sequence behavior is unclear. In probabilistic environments, neither the posterior nor the losses converge, in general.",
        "published": "2003-06-07T19:21:20Z",
        "link": "http://arxiv.org/abs/cs/0306036v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2"
        ]
    },
    {
        "title": "Bayesian Information Extraction Network",
        "authors": [
            "Leonid Peshkin",
            "Avi Pfeffer"
        ],
        "summary": "Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various aspects of language in one model. Many existing algorithms developed for learning and inference in DBNs are applicable to probabilistic language modeling. To demonstrate the potential of DBNs for natural language processing, we employ a DBN in an information extraction task. We show how to assemble wealth of emerging linguistic instruments for shallow parsing, syntactic and semantic tagging, morphological decomposition, named entity recognition etc. in order to incrementally build a robust information extraction system. Our method outperforms previously published results on an established benchmark domain.",
        "published": "2003-06-10T04:40:45Z",
        "link": "http://arxiv.org/abs/cs/0306039v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "C.1.3; I.5.1; I.7.2; I.2.7"
        ]
    },
    {
        "title": "Universal Sequential Decisions in Unknown Environments",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "We give a brief introduction to the AIXI model, which unifies and overcomes the limitations of sequential decision theory and universal Solomonoff induction. While the former theory is suited for active agents in known environments, the latter is suited for passive prediction of unknown environments.",
        "published": "2003-06-16T13:15:29Z",
        "link": "http://arxiv.org/abs/cs/0306091v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "I.2; G.3"
        ]
    },
    {
        "title": "Lexicographic probability, conditional probability, and nonstandard   probability",
        "authors": [
            "Joseph Y. Halpern"
        ],
        "summary": "The relationship between Popper spaces (conditional probability spaces that satisfy some regularity conditions), lexicographic probability systems (LPS's), and nonstandard probability spaces (NPS's) is considered. If countable additivity is assumed, Popper spaces and a subclass of LPS's are equivalent; without the assumption of countable additivity, the equivalence no longer holds. If the state space is finite, LPS's are equivalent to NPS's. However, if the state space is infinite, NPS's are shown to be more general than LPS's.",
        "published": "2003-06-17T22:11:36Z",
        "link": "http://arxiv.org/abs/cs/0306106v2",
        "categories": [
            "cs.GT",
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "D0 Data Handling Operational Experience",
        "authors": [
            "A. Baranovski",
            "C. Brock",
            "D. Bonham",
            "L. Carpenter",
            "L. Lueking",
            "W. Merritt",
            "C. Moore",
            "I. Terekhov",
            "J. Trumbo",
            "S. Veseli",
            "J. Weigand",
            "S. White",
            "K. Yip"
        ],
        "summary": "We report on the production experience of the D0 experiment at the Fermilab Tevatron, using the SAM data handling system with a variety of computing hardware configurations, batch systems, and mass storage strategies. We have stored more than 300 TB of data in the Fermilab Enstore mass storage system. We deliver data through this system at an average rate of more than 2 TB/day to analysis programs, with a substantial multiplication factor in the consumed data through intelligent cache management. We handle more than 1.7 Million files in this system and provide data delivery to user jobs at Fermilab on four types of systems: a reconstruction farm, a large SMP system, a Linux batch cluster, and a Linux desktop cluster. In addition, we import simulation data generated at 6 sites worldwide, and deliver data to jobs at many more sites. We describe the scope of the data handling deployment worldwide, the operational experience with this system, and the feedback of that experience.",
        "published": "2003-06-19T18:13:23Z",
        "link": "http://arxiv.org/abs/cs/0306114v1",
        "categories": [
            "cs.DC",
            "cs.AI",
            "H.3"
        ]
    },
    {
        "title": "Reinforcement Learning with Linear Function Approximation and LQ control   Converges",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "Reinforcement learning is commonly used with function approximation. However, very few positive results are known about the convergence of function approximation based RL control algorithms. In this paper we show that TD(0) and Sarsa(0) with linear function approximation is convergent for a simple class of problems, where the system is linear and the costs are quadratic (the LQ control problem). Furthermore, we show that for systems with Gaussian noise and non-completely observable states (the LQG problem), the mentioned RL algorithms are still convergent, if they are combined with Kalman filtering.",
        "published": "2003-06-22T08:00:09Z",
        "link": "http://arxiv.org/abs/cs/0306120v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Updating Probabilities",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "summary": "As examples such as the Monty Hall puzzle show, applying conditioning to update a probability distribution on a ``naive space'', which does not take into account the protocol used, can often lead to counterintuitive results. Here we examine why. A criterion known as CAR (``coarsening at random'') in the statistical literature characterizes when ``naive'' conditioning in a naive space works. We show that the CAR condition holds rather infrequently, and we provide a procedural characterization of it, by giving a randomized algorithm that generates all and only distributions for which CAR holds. This substantially extends previous characterizations of CAR. We also consider more generalized notions of update such as Jeffrey conditioning and minimizing relative entropy (MRE). We give a generalization of the CAR condition that characterizes when Jeffrey conditioning leads to appropriate answers, and show that there exist some very simple settings in which MRE essentially never gives the right results. This generalizes and interconnects previous results obtained in the literature on CAR and MRE.",
        "published": "2003-06-23T22:24:05Z",
        "link": "http://arxiv.org/abs/cs/0306124v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Bayesian Treatment of Incomplete Discrete Data applied to Mutual   Information and Feature Selection",
        "authors": [
            "Marcus Hutter",
            "Marco Zaffalon"
        ],
        "summary": "Given the joint chances of a pair of random variables one can compute quantities of interest, like the mutual information. The Bayesian treatment of unknown chances involves computing, from a second order prior distribution and the data likelihood, a posterior distribution of the chances. A common treatment of incomplete data is to assume ignorability and determine the chances by the expectation maximization (EM) algorithm. The two different methods above are well established but typically separated. This paper joins the two approaches in the case of Dirichlet priors, and derives efficient approximations for the mean, mode and the (co)variance of the chances and the mutual information. Furthermore, we prove the unimodality of the posterior distribution, whence the important property of convergence of EM to the global maximum in the chosen framework. These results are applied to the problem of selecting features for incremental learning and naive Bayes classification. A fast filter based on the distribution of mutual information is shown to outperform the traditional filter based on empirical mutual information on a number of incomplete real data sets.",
        "published": "2003-06-24T09:50:29Z",
        "link": "http://arxiv.org/abs/cs/0306126v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.PR",
            "G.3; G.1.2; I.2"
        ]
    },
    {
        "title": "Anusaaraka: Machine Translation in Stages",
        "authors": [
            "Akshar Bharati",
            "Vineet Chaitanya",
            "Amba P. Kulkarni",
            "Rajeev Sangal"
        ],
        "summary": "Fully-automatic general-purpose high-quality machine translation systems (FGH-MT) are extremely difficult to build. In fact, there is no system in the world for any pair of languages which qualifies to be called FGH-MT. The reasons are not far to seek. Translation is a creative process which involves interpretation of the given text by the translator. Translation would also vary depending on the audience and the purpose for which it is meant. This would explain the difficulty of building a machine translation system. Since, the machine is not capable of interpreting a general text with sufficient accuracy automatically at present - let alone re-expressing it for a given audience, it fails to perform as FGH-MT. FOOTNOTE{The major difficulty that the machine faces in interpreting a given text is the lack of general world knowledge or common sense knowledge.}",
        "published": "2003-06-25T10:26:29Z",
        "link": "http://arxiv.org/abs/cs/0306130v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Pruning Isomorphic Structural Sub-problems in Configuration",
        "authors": [
            "Stephane Grandcolas",
            "Laurent Henocque",
            "Nicolas Prcovic"
        ],
        "summary": "Configuring consists in simulating the realization of a complex product from a catalog of component parts, using known relations between types, and picking values for object attributes. This highly combinatorial problem in the field of constraint programming has been addressed with a variety of approaches since the foundation system R1(McDermott82). An inherent difficulty in solving configuration problems is the existence of many isomorphisms among interpretations. We describe a formalism independent approach to improve the detection of isomorphisms by configurators, which does not require to adapt the problem model. To achieve this, we exploit the properties of a characteristic subset of configuration problems, called the structural sub-problem, which canonical solutions can be produced or tested at a limited cost. In this paper we present an algorithm for testing the canonicity of configurations, that can be added as a symmetry breaking constraint to any configurator. The cost and efficiency of this canonicity test are given.",
        "published": "2003-06-27T11:25:17Z",
        "link": "http://arxiv.org/abs/cs/0306135v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.2.4; I.2.8; F.4.1"
        ]
    },
    {
        "title": "Probabilistic Reasoning as Information Compression by Multiple   Alignment, Unification and Search: An Introduction and Overview",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This article introduces the idea that probabilistic reasoning (PR) may be understood as \"information compression by multiple alignment, unification and search\" (ICMAUS). In this context, multiple alignment has a meaning which is similar to but distinct from its meaning in bio-informatics, while unification means a simple merging of matching patterns, a meaning which is related to but simpler than the meaning of that term in logic.   A software model, SP61, has been developed for the discovery and formation of 'good' multiple alignments, evaluated in terms of information compression. The model is described in outline.   Using examples from the SP61 model, this article describes in outline how the ICMAUS framework can model various kinds of PR including: PR in best-match pattern recognition and information retrieval; one-step 'deductive' and 'abductive' PR; inheritance of attributes in a class hierarchy; chains of reasoning (probabilistic decision networks and decision trees, and PR with 'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with default values; modelling the function of a Bayesian network.",
        "published": "2003-07-04T16:34:45Z",
        "link": "http://arxiv.org/abs/cs/0307010v2",
        "categories": [
            "cs.AI",
            "I.2.3"
        ]
    },
    {
        "title": "'Computing' as Information Compression by Multiple Alignment,   Unification and Search",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This paper argues that the operations of a 'Universal Turing Machine' (UTM) and equivalent mechanisms such as the 'Post Canonical System' (PCS) - which are widely accepted as definitions of the concept of `computing' - may be interpreted as *information compression by multiple alignment, unification and search* (ICMAUS).   The motivation for this interpretation is that it suggests ways in which the UTM/PCS model may be augmented in a proposed new computing system designed to exploit the ICMAUS principles as fully as possible. The provision of a relatively sophisticated search mechanism in the proposed 'SP' system appears to open the door to the integration and simplification of a range of functions including unsupervised inductive learning, best-match pattern recognition and information retrieval, probabilistic reasoning, planning and problem solving, and others. Detailed consideration of how the ICMAUS principles may be applied to these functions is outside the scope of this article but relevant sources are cited in this article.",
        "published": "2003-07-05T18:52:20Z",
        "link": "http://arxiv.org/abs/cs/0307013v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "F.0"
        ]
    },
    {
        "title": "Syntax, Parsing and Production of Natural Language in a Framework of   Information Compression by Multiple Alignment, Unification and Search",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This article introduces the idea that \"information compression by multiple alignment, unification and search\" (ICMAUS) provides a framework within which natural language syntax may be represented in a simple format and the parsing and production of natural language may be performed in a transparent manner.   The ICMAUS concepts are embodied in a software model, SP61. The organisation and operation of the model are described and a simple example is presented showing how the model can achieve parsing of natural language.   Notwithstanding the apparent paradox of 'decompression by compression', the ICMAUS framework, without any modification, can produce a sentence by decoding a compressed code for the sentence. This is illustrated with output from the SP61 model.   The article includes four other examples - one of the parsing of a sentence in French and three from the domain of English auxiliary verbs. These examples show how the ICMAUS framework and the SP61 model can accommodate 'context sensitive' features of syntax in a relatively simple and direct manner.",
        "published": "2003-07-07T08:47:53Z",
        "link": "http://arxiv.org/abs/cs/0307014v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Definition and Complexity of Some Basic Metareasoning Problems",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "In most real-world settings, due to limited time or other resources, an agent cannot perform all potentially useful deliberation and information gathering actions. This leads to the metareasoning problem of selecting such actions. Decision-theoretic methods for metareasoning have been studied in AI, but there are few theoretical results on the complexity of metareasoning.   We derive hardness results for three settings which most real metareasoning systems would have to encompass as special cases. In the first, the agent has to decide how to allocate its deliberation time across anytime algorithms running on different problem instances. We show this to be $\\mathcal{NP}$-complete. In the second, the agent has to (dynamically) allocate its deliberation or information gathering resources across multiple actions that it has to choose among. We show this to be $\\mathcal{NP}$-hard even when evaluating each individual action is extremely simple. In the third, the agent has to (dynamically) choose a limited number of deliberation or information gathering actions to disambiguate the state of the world. We show that this is $\\mathcal{NP}$-hard under a natural restriction, and $\\mathcal{PSPACE}$-hard in general.",
        "published": "2003-07-07T20:32:20Z",
        "link": "http://arxiv.org/abs/cs/0307017v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.11"
        ]
    },
    {
        "title": "Information Compression by Multiple Alignment, Unification and Search as   a Unifying Principle in Computing and Cognition",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This article presents an overview of the idea that \"information compression by multiple alignment, unification and search\" (ICMAUS) may serve as a unifying principle in computing (including mathematics and logic) and in such aspects of human cognition as the analysis and production of natural language, fuzzy pattern recognition and best-match information retrieval, concept hierarchies with inheritance of attributes, probabilistic reasoning, and unsupervised inductive learning. The ICMAUS concepts are described together with an outline of the SP61 software model in which the ICMAUS concepts are currently realised. A range of examples is presented, illustrated with output from the SP61 model.",
        "published": "2003-07-10T15:32:31Z",
        "link": "http://arxiv.org/abs/cs/0307025v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Automatic Classification using Self-Organising Neural Networks in   Astrophysical Experiments",
        "authors": [
            "P. Boinee",
            "A. De Angelis",
            "E. Milotti"
        ],
        "summary": "Self-Organising Maps (SOMs) are effective tools in classification problems, and in recent years the even more powerful Dynamic Growing Neural Networks, a variant of SOMs, have been developed. Automatic Classification (also called clustering) is an important and difficult problem in many Astrophysical experiments, for instance, Gamma Ray Burst classification, or gamma-hadron separation. After a brief introduction to classification problem, we discuss Self-Organising Maps in section 2. Section 3 discusses with various models of growing neural networks and finally in section 4 we discuss the research perspectives in growing neural networks for efficient classification in astrophysical problems.",
        "published": "2003-07-12T12:04:33Z",
        "link": "http://arxiv.org/abs/cs/0307031v2",
        "categories": [
            "cs.NE",
            "astro-ph",
            "cs.AI",
            "I.5.1; I.5.3"
        ]
    },
    {
        "title": "Supporting Dynamic Ad hoc Collaboration Capabilities",
        "authors": [
            "D. Agarwal",
            "K. Berket"
        ],
        "summary": "Modern HENP experiments such as CMS and Atlas involve as many as 2000 collaborators around the world. Collaborations this large will be unable to meet often enough to support working closely together. Many of the tools currently available for collaboration focus on heavy-weight applications such as videoconferencing tools. While these are important, there is a more basic need for tools that support connecting physicists to work together on an ad hoc or continuous basis. Tools that support the day-to-day connectivity and underlying needs of a group of collaborators are important for providing light-weight, non-intrusive, and flexible ways to work collaboratively. Some example tools include messaging, file-sharing, and shared plot viewers. An important component of the environment is a scalable underlying communication framework. In this paper we will describe our current progress on building a dynamic and ad hoc collaboration environment and our vision for its evolution into a HENP collaboration environment.",
        "published": "2003-07-14T17:49:35Z",
        "link": "http://arxiv.org/abs/cs/0307037v1",
        "categories": [
            "cs.OH",
            "cs.AI",
            "H.5.3"
        ]
    },
    {
        "title": "Bridging the gap between modal temporal logics and constraint-based QSR   as an ALC(D) spatio-temporalisation with weakly cyclic TBoxes",
        "authors": [
            "Amar Isli"
        ],
        "summary": "The aim of this work is to provide a family of qualitative theories for spatial change in general, and for motion of spatial scenes in particular. To achieve this, we consider a spatio-temporalisation MTALC(D_x), of the well-known ALC(D) family of Description Logics (DLs) with a concrete domainan. In particular, the concrete domain D_x is generated by a qualitative spatial Relation Algebra (RA) x. We show the important result that satisfiability of an MTALC(D_x) concept with respect to a weakly cyclic TBox is decidable in nondeterministic exponential time, by reducing it to the emptiness problem of a weak alternating automaton augmented with spatial constraints, which we show to remain decidable, although the accepting condition of a run involves, additionally to the standard case, consistency of a CSP (Constraint Satisfaction Problem) potentially infinite. The result provides an effective tableaux-like satisfiability procedure which is discussed.",
        "published": "2003-07-17T14:46:19Z",
        "link": "http://arxiv.org/abs/cs/0307040v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.4)"
        ]
    },
    {
        "title": "Integrating cardinal direction relations and other orientation relations   in Qualitative Spatial Reasoning",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We propose a calculus integrating two calculi well-known in Qualitative Spatial Reasoning (QSR): Frank's projection-based cardinal direction calculus, and a coarser version of Freksa's relative orientation calculus. An original constraint propagation procedure is presented, which implements the interaction between the two integrated calculi. The importance of taking into account the interaction is shown with a real example providing an inconsistent knowledge base, whose inconsistency (a) cannot be detected by reasoning separately about each of the two components of the knowledge, just because, taken separately, each is consistent, but (b) is detected by the proposed algorithm, thanks to the interaction knowledge propagated from each of the two compnents to the other.",
        "published": "2003-07-21T13:03:19Z",
        "link": "http://arxiv.org/abs/cs/0307048v2",
        "categories": [
            "cs.AI",
            "I.2 (I.2.4)"
        ]
    },
    {
        "title": "A ternary Relation Algebra of directed lines",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We define a ternary Relation Algebra (RA) of relative position relations on two-dimensional directed lines (d-lines for short). A d-line has two degrees of freedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The representation of the RDF of a d-line will be handled by an RA of 2D orientations, CYC_t, known in the literature. A second algebra, TA_t, which will handle the TDF of a d-line, will be defined. The two algebras, CYC_t and TA_t, will constitute, respectively, the translational and the rotational components of the RA, PA_t, of relative position relations on d-lines: the PA_t atoms will consist of those pairs <t,r> of a TA_t atom and a CYC_t atom that are compatible. We present in detail the RA PA_t, with its converse table, its rotation table and its composition tables. We show that a (polynomial) constraint propagation algorithm, known in the literature, is complete for a subset of PA_t relations including almost all of the atomic relations. We will discuss the application scope of the RA, which includes incidence geometry, GIS (Geographic Information Systems), shape representation, localisation in (multi-)robot navigation, and the representation of motion prepositions in NLP (Natural Language Processing). We then compare the RA to existing ones, such as an algebra for reasoning about rectangles parallel to the axes of an (orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval algebra, and an algebra for reasoning about 2D segments.",
        "published": "2003-07-21T16:01:11Z",
        "link": "http://arxiv.org/abs/cs/0307050v1",
        "categories": [
            "cs.AI",
            "I.2 (I.2.4)"
        ]
    },
    {
        "title": "From Statistical Knowledge Bases to Degrees of Belief",
        "authors": [
            "Fahiem Bacchus",
            "Adam Grove",
            "Joseph Y. Halpern",
            "Daphne Koller"
        ],
        "summary": "An intelligent agent will often be uncertain about various properties of its environment, and when acting in that environment it will frequently need to quantify its uncertainty. For example, if the agent wishes to employ the expected-utility paradigm of decision theory to guide its actions, it will need to assign degrees of belief (subjective probabilities) to various assertions. Of course, these degrees of belief should not be arbitrary, but rather should be based on the information available to the agent. This paper describes one approach for inducing degrees of belief from very rich knowledge bases, that can include information about particular individuals, statistical correlations, physical laws, and default rules. We call our approach the random-worlds method. The method is based on the principle of indifference: it treats all of the worlds the agent considers possible as being equally likely. It is able to integrate qualitative default reasoning with quantitative probabilistic reasoning by providing a language in which both types of information can be easily expressed. Our results show that a number of desiderata that arise in direct inference (reasoning from statistical information to conclusions about individuals) and default reasoning follow directly {from} the semantics of random worlds. For example, random worlds captures important patterns of reasoning such as specificity, inheritance, indifference to irrelevant information, and default assumptions of independence. Furthermore, the expressive power of the language used and the intuitive semantics of random worlds allow the method to deal with problems that are beyond the scope of many other non-deductive reasoning systems.",
        "published": "2003-07-24T21:32:09Z",
        "link": "http://arxiv.org/abs/cs/0307056v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Neural realisation of the SP theory: cell assemblies revisited",
        "authors": [
            "J. Gerard Wolff"
        ],
        "summary": "This paper describes how the elements of the SP theory (Wolff, 2003a) may be realised with neural structures and processes. To the extent that this is successful, the insights that have been achieved in the SP theory - the integration and simplification of a range of phenomena in perception and cognition - may be incorporated in a neural view of brain function.   These proposals may be seen as a development of Hebb's (1949) concept of a 'cell assembly'. By contrast with that concept and variants of it, the version described in this paper proposes that any one neuron can belong in one assembly and only one assembly. A distinctive feature of the present proposals is that any neuron or cluster of neurons within a cell assembly may serve as a proxy or reference for another cell assembly or class of cell assemblies. This device provides solutions to many of the problems associated with cell assemblies, it allows information to be stored in a compressed form, and it provides a robust mechanism by which assemblies may be connected to form hierarchies, grammars and other kinds of knowledge structure.   Drawing on insights derived from the SP theory, the paper also describes how unsupervised learning may be achieved with neural structures and processes. This theory of learning overcomes weaknesses in the Hebbian concept of learning and it is, at the same time, compatible with the observations that Hebb's theory was designed to explain.",
        "published": "2003-07-27T22:53:12Z",
        "link": "http://arxiv.org/abs/cs/0307060v2",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.0"
        ]
    },
    {
        "title": "An Alternative to RDF-Based Languages for the Representation and   Processing of Ontologies in the Semantic Web",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This paper describes an approach to the representation and processing of ontologies in the Semantic Web, based on the ICMAUS theory of computation and AI. This approach has strengths that complement those of languages based on the Resource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main benefits of the ICMAUS approach are simplicity and comprehensibility in the representation of ontologies, an ability to cope with errors and uncertainties in knowledge, and a versatile reasoning system with capabilities in the kinds of probabilistic reasoning that seem to be required in the Semantic Web.",
        "published": "2003-07-29T11:13:41Z",
        "link": "http://arxiv.org/abs/cs/0307063v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "A logic for reasoning about upper probabilities",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "We present a propositional logic %which can be used to reason about the uncertainty of events, where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event. We give a sound and complete axiomatization for the logic, and show that the satisfiability problem is NP-complete, no harder than satisfiability for propositional logic.",
        "published": "2003-07-30T21:08:54Z",
        "link": "http://arxiv.org/abs/cs/0307069v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "Modeling Belief in Dynamic Systems, Part I: Foundations",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "Belief change is a fundamental problem in AI: Agents constantly have to update their beliefs to accommodate new observations. In recent years, there has been much work on axiomatic characterizations of belief change. We claim that a better understanding of belief change can be gained from examining appropriate semantic models. In this paper we propose a general framework in which to model belief change. We begin by defining belief in terms of knowledge and plausibility: an agent believes p if he knows that p is more plausible than its negation. We then consider some properties defining the interaction between knowledge and plausibility, and show how these properties affect the properties of belief. In particular, we show that by assuming two of the most natural properties, belief becomes a KD45 operator. Finally, we add time to the picture. This gives us a framework in which we can talk about knowledge, plausibility (and hence belief), and time, which extends the framework of Halpern and Fagin for modeling knowledge in multi-agent systems. We then examine the problem of ``minimal change''. This notion can be captured by using prior plausibilities, an analogue to prior probabilities, which can be updated by ``conditioning''. We show by example that conditioning on a plausibility measure can capture many scenarios of interest. In a companion paper, we show how the two best-studied scenarios of belief change, belief revisionand belief update, fit into our framework.",
        "published": "2003-07-30T21:36:03Z",
        "link": "http://arxiv.org/abs/cs/0307070v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "Modeling Belief in Dynamic Systems, Part II: Revisions and Update",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper, we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.",
        "published": "2003-07-30T21:51:56Z",
        "link": "http://arxiv.org/abs/cs/0307071v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "Quantifying and Visualizing Attribute Interactions",
        "authors": [
            "Aleks Jakulin",
            "Ivan Bratko"
        ],
        "summary": "Interactions are patterns between several attributes in data that cannot be inferred from any subset of these attributes. While mutual information is a well-established approach to evaluating the interactions between two attributes, we surveyed its generalizations as to quantify interactions between several attributes. We have chosen McGill's interaction information, which has been independently rediscovered a number of times under various names in various disciplines, because of its many intuitively appealing properties. We apply interaction information to visually present the most important interactions of the data. Visualization of interactions has provided insight into the structure of data on a number of domains, identifying redundant attributes and opportunities for constructing new features, discovering unexpected regularities in data, and have helped during construction of predictive models; we illustrate the methods on numerous examples. A machine learning method that disregards interactions may get caught in two traps: myopia is caused by learning algorithms assuming independence in spite of interactions, whereas fragmentation arises from assuming an interaction in spite of independence.",
        "published": "2003-08-01T10:50:07Z",
        "link": "http://arxiv.org/abs/cs/0308002v3",
        "categories": [
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Ensembles of Protein Molecules as Statistical Analog Computers",
        "authors": [
            "Victor Eliashberg"
        ],
        "summary": "A class of analog computers built from large numbers of microscopic probabilistic machines is discussed. It is postulated that such computers are implemented in biological systems as ensembles of protein molecules. The formalism is based on an abstract computational model referred to as Protein Molecule Machine (PMM). A PMM is a continuous-time first-order Markov system with real input and output vectors, a finite set of discrete states, and the input-dependent conditional probability densities of state transitions. The output of a PMM is a function of its input and state. The components of input vector, called generalized potentials, can be interpreted as membrane potential, and concentrations of neurotransmitters. The components of output vector, called generalized currents, can represent ion currents, and the flows of second messengers. An Ensemble of PMMs (EPMM) is a set of independent identical PMMs with the same input vector, and the output vector equal to the sum of output vectors of individual PMMs. The paper suggests that biological neurons have much more sophisticated computational resources than the presently popular models of artificial neurons.",
        "published": "2003-08-11T16:32:47Z",
        "link": "http://arxiv.org/abs/physics/0308041v2",
        "categories": [
            "physics.bio-ph",
            "cs.AI",
            "cs.NE",
            "physics.comp-ph",
            "physics.data-an",
            "q-bio.NC"
        ]
    },
    {
        "title": "Mathematics and Logic as Information Compression by Multiple Alignment,   Unification and Search",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This article introduces the conjecture that \"mathematics, logic and related disciplines may usefully be understood as information compression (IC) by 'multiple alignment', 'unification' and 'search' (ICMAUS)\".   As a preparation for the two main sections of the article, concepts of information and information compression are reviewed. Related areas of research are also described including IC in brains and nervous systems, and IC in relation to inductive inference, Minimum Length Encoding and probabilistic reasoning. The ICMAUS concepts and a computer model in which they are embodied are briefly described.   The first of the two main sections describes how many of the commonly-used forms and structures in mathematics, logic and related disciplines (such as theoretical linguistics and computer programming) may be seen as devices for IC. In some cases, these forms and structures may be interpreted in terms of the ICMAUS framework.   The second main section describes a selection of examples where processes of calculation and inference in mathematics, logic and related disciplines may be understood as IC. In many cases, these examples may be understood more specifically in terms of the ICMAUS concepts.",
        "published": "2003-08-15T16:24:53Z",
        "link": "http://arxiv.org/abs/math/0308153v1",
        "categories": [
            "math.GM",
            "cs.AI",
            "math.LO",
            "00A30; 03A05"
        ]
    },
    {
        "title": "Controlled hierarchical filtering: Model of neocortical sensory   processing",
        "authors": [
            "Andras Lorincz"
        ],
        "summary": "A model of sensory information processing is presented. The model assumes that learning of internal (hidden) generative models, which can predict the future and evaluate the precision of that prediction, is of central importance for information extraction. Furthermore, the model makes a bridge to goal-oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippocampal entorhinal loop. This generative control architecture is mapped to the neocortex and to the hippocampal entorhinal loop. Implicit memory phenomena; priming and prototype learning are emerging features of the model. Mathematical theorems ensure stability and attractive learning properties of the architecture. Connections to reinforcement learning are also established: both the control network, and the network with a hidden model converge to (near) optimal policy under suitable conditions. Falsifying predictions, including the role of the feedback connections between neocortical areas are made.",
        "published": "2003-08-16T07:31:57Z",
        "link": "http://arxiv.org/abs/cs/0308025v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "q-bio.NC",
            "C.1.3; F.1.1.; I.2.0; I.2.6; I.2.10; I.4.3.; I.4.10; I.5.1"
        ]
    },
    {
        "title": "Learning in Multiagent Systems: An Introduction from a Game-Theoretic   Perspective",
        "authors": [
            "Jose M. Vidal"
        ],
        "summary": "We introduce the topic of learning in multiagent systems. We first provide a quick introduction to the field of game theory, focusing on the equilibrium concepts of iterated dominance, and Nash equilibrium. We show some of the most relevant findings in the theory of learning in games, including theorems on fictitious play, replicator dynamics, and evolutionary stable strategies. The CLRI theory and n-level learning agents are introduced as attempts to apply some of these findings to the problem of engineering multiagent systems with learning agents. Finally, we summarize some of the remaining challenges in the field of learning in multiagent systems.",
        "published": "2003-08-19T15:45:16Z",
        "link": "http://arxiv.org/abs/cs/0308030v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Artificial Neural Networks for Beginners",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "The scope of this teaching package is to make a brief induction to Artificial Neural Networks (ANNs) for people who have no previous knowledge of them. We first make a brief introduction to models of networks, for then describing in general terms ANNs. As an application, we explain the backpropagation algorithm, since it is widely used and many other algorithms are derived from it. The user should know algebra and the handling of functions and vectors. Differential calculus is recommendable, but not necessary. The contents of this package should be understood by people with high school education. It would be useful for people who are just curious about what are ANNs, or for people who want to become familiar with them, so when they study them more fully, they will already have clear notions of ANNs. Also, people who only want to apply the backpropagation algorithm without a detailed and formal explanation of it will find this material useful. This work should not be seen as \"Nets for dummies\", but of course it is not a treatise. Much of the formality is skipped for the sake of simplicity. Detailed explanations and demonstrations can be found in the referred readings. The included exercises complement the understanding of the theory. The on-line resources are highly recommended for extending this brief induction.",
        "published": "2003-08-20T09:40:25Z",
        "link": "http://arxiv.org/abs/cs/0308031v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "C.1.3; I.5.1"
        ]
    },
    {
        "title": "ROC Curves Within the Framework of Neural Network Assembly Memory Model:   Some Analytic Results",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "On the basis of convolutional (Hamming) version of recent Neural Network Assembly Memory Model (NNAMM) for intact two-layer autoassociative Hopfield network optimal receiver operating characteristics (ROCs) have been derived analytically. A method of taking into account explicitly a priori probabilities of alternative hypotheses on the structure of information initiating memory trace retrieval and modified ROCs (mROCs, a posteriori probabilities of correct recall vs. false alarm probability) are introduced. The comparison of empirical and calculated ROCs (or mROCs) demonstrates that they coincide quantitatively and in this way intensities of cues used in appropriate experiments may be estimated. It has been found that basic ROC properties which are one of experimental findings underpinning dual-process models of recognition memory can be explained within our one-factor NNAMM.",
        "published": "2003-09-07T20:11:10Z",
        "link": "http://arxiv.org/abs/cs/0309007v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "What Is Working Memory and Mental Imagery? A Robot that Learns to   Perform Mental Computations",
        "authors": [
            "Victor Eliashberg"
        ],
        "summary": "This paper goes back to Turing (1936) and treats his machine as a cognitive model (W,D,B), where W is an \"external world\" represented by memory device (the tape divided into squares), and (D,B) is a simple robot that consists of the sensory-motor devices, D, and the brain, B. The robot's sensory-motor devices (the \"eye\", the \"hand\", and the \"organ of speech\") allow the robot to simulate the work of any Turing machine. The robot simulates the internal states of a Turing machine by \"talking to itself.\" At the stage of training, the teacher forces the robot (by acting directly on its motor centers) to perform several examples of an algorithm with different input data presented on tape. Two effects are achieved: 1) The robot learns to perform the shown algorithm with any input data using the tape. 2) The robot learns to perform the algorithm \"mentally\" using an \"imaginary tape.\" The model illustrates the simplest concept of a universal learning neurocomputer, demonstrates universality of associative learning as the mechanism of programming, and provides a simplified, but nontrivial neurobiologically plausible explanation of the phenomena of working memory and mental imagery. The model is implemented as a user-friendly program for Windows called EROBOT. The program is available at www.brain0.com/software.html.",
        "published": "2003-09-08T07:31:55Z",
        "link": "http://arxiv.org/abs/cs/0309009v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.0"
        ]
    },
    {
        "title": "Exploration of RNA Editing and Design of Robust Genetic Algorithms",
        "authors": [
            "C. Huang",
            "L. M. Rocha"
        ],
        "summary": "This paper presents our computational methodology using Genetic Algorithms (GA) for exploring the nature of RNA editing. These models are constructed using several genetic editing characteristics that are gleaned from the RNA editing system as observed in several organisms. We have expanded the traditional Genetic Algorithm with artificial editing mechanisms as proposed by (Rocha, 1997). The incorporation of editing mechanisms provides a means for artificial agents with genetic descriptions to gain greater phenotypic plasticity, which may be environmentally regulated. Our first implementations of these ideas have shed some light into the evolutionary implications of RNA editing. Based on these understandings, we demonstrate how to select proper RNA editors for designing more robust GAs, and the results will show promising applications to real-world problems. We expect that the framework proposed will both facilitate determining the evolutionary role of RNA editing in biology, and advance the current state of research in Genetic Algorithms.",
        "published": "2003-09-09T04:25:26Z",
        "link": "http://arxiv.org/abs/cs/0309012v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "nlin.AO",
            "q-bio.GN",
            "I.2"
        ]
    },
    {
        "title": "Semi-metric Behavior in Document Networks and its Application to   Recommendation Systems",
        "authors": [
            "L. M. Rocha"
        ],
        "summary": "Recommendation systems for different Document Networks (DN) such as the World Wide Web (WWW) and Digital Libraries, often use distance functions extracted from relationships among documents and keywords. For instance, documents in the WWW are related via a hyperlink network, while documents in bibliographic databases are related by citation and collaboration networks. Furthermore, documents are related to keyterms. The distance functions computed from these relations establish associative networks among items of the DN, referred to as Distance Graphs, which allow recommendation systems to identify relevant associations for individual users. However, modern recommendation systems need to integrate associative data from multiple sources such as different databases, web sites, and even other users. Thus, we are presented with a problem of combining evidence (about associations between items) from different sources characterized by distance functions. In this paper we describe our work on (1) inferring relevant associations from, as well as characterizing, semi-metric distance graphs and (2) combining evidence from different distance graphs in a recommendation system. Regarding (1), we present the idea of semi-metric distance graphs, and introduce ratios to measure semi-metric behavior. We compute these ratios for several DN such as digital libraries and web sites and show that they are useful to identify implicit associations. Regarding (2), we describe an algorithm to combine evidence from distance graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed for a recommendation system named TalkMine.",
        "published": "2003-09-09T05:24:03Z",
        "link": "http://arxiv.org/abs/cs/0309013v1",
        "categories": [
            "cs.IR",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.MA",
            "H.3.0; H.3.3, H.3.4; H.3.6; H.3.7; I.2.11; H.3.5"
        ]
    },
    {
        "title": "Evidential Force Aggregation",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we develop an evidential force aggregation method intended for classification of evidential intelligence into recognized force structures. We assume that the intelligence has already been partitioned into clusters and use the classification method individually in each cluster. The classification is based on a measure of fitness between template and fused intelligence that makes it possible to handle intelligence reports with multiple nonspecific and uncertain propositions. With this measure we can aggregate on a level-by-level basis, starting from general intelligence to achieve a complete force structure with recognized units on all hierarchical levels.",
        "published": "2003-09-15T07:20:48Z",
        "link": "http://arxiv.org/abs/cs/0309025v1",
        "categories": [
            "cs.AI",
            "I.2.3; I.5.2"
        ]
    },
    {
        "title": "Model-Based Debugging using Multiple Abstract Models",
        "authors": [
            "Wolfgang Mayer",
            "Markus Stumptner"
        ],
        "summary": "This paper introduces an automatic debugging framework that relies on model-based reasoning techniques to locate faults in programs. In particular, model-based diagnosis, together with an abstract interpretation based conflict detection mechanism is used to derive diagnoses, which correspond to possible faults in programs. Design information and partial specifications are applied to guide a model revision process, which allows for automatic detection and correction of structural faults.",
        "published": "2003-09-17T03:49:05Z",
        "link": "http://arxiv.org/abs/cs/0309030v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "D.2.5"
        ]
    },
    {
        "title": "A Neural Network Assembly Memory Model Based on an Optimal Binary Signal   Detection Theory",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "A ternary/binary data coding algorithm and conditions under which Hopfield networks implement optimal convolutional or Hamming decoding algorithms has been described. Using the coding/decoding approach (an optimal Binary Signal Detection Theory, BSDT) introduced a Neural Network Assembly Memory Model (NNAMM) is built. The model provides optimal (the best) basic memory performance and demands the use of a new memory unit architecture with two-layer Hopfield network, N-channel time gate, auxiliary reference memory, and two nested feedback loops. NNAMM explicitly describes the dependence on time of a memory trace retrieval, gives a possibility of metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. A model of smallest inseparable part or an \"atom\" of consciousness is also defined. The NNAMM's neurobiological backgrounds and its applications to solving some interdisciplinary problems are shortly discussed. BSDT could implement the \"best neural code\" used in nervous tissues of animals and humans.",
        "published": "2003-09-21T17:11:11Z",
        "link": "http://arxiv.org/abs/cs/0309036v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "Goedel Machines: Self-Referential Universal Problem Solvers Making   Provably Optimal Self-Improvements",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.",
        "published": "2003-09-25T15:59:46Z",
        "link": "http://arxiv.org/abs/cs/0309048v5",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "A Hierarchical Situation Calculus",
        "authors": [
            "David A. Plaisted"
        ],
        "summary": "A situation calculus is presented that provides a solution to the frame problem for hierarchical situations, that is, situations that have a modular structure in which parts of the situation behave in a relatively independent manner. This situation calculus is given in a relational, functional, and modal logic form. Each form permits both a single level hierarchy or a multiple level hierarchy, giving six versions of the formalism in all, and a number of sub-versions of these. For multiple level hierarchies, it is possible to give equations between parts of the situation to impose additional structure on the problem. This approach is compared to others in the literature.",
        "published": "2003-09-29T17:13:58Z",
        "link": "http://arxiv.org/abs/cs/0309053v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Using Artificial Intelligence for Model Selection",
        "authors": [
            "Darin Goldstein",
            "William Murray",
            "Binh Yang"
        ],
        "summary": "We apply the optimization algorithm Adaptive Simulated Annealing (ASA) to the problem of analyzing data on a large population and selecting the best model to predict that an individual with various traits will have a particular disease. We compare ASA with traditional forward and backward regression on computer simulated data. We find that the traditional methods of modeling are better for smaller data sets whereas a numerically stable ASA seems to perform better on larger and more complicated data sets.",
        "published": "2003-10-05T18:59:12Z",
        "link": "http://arxiv.org/abs/cs/0310005v1",
        "categories": [
            "cs.AI",
            "q-bio.QM",
            "H.2.8; J.3"
        ]
    },
    {
        "title": "Transient Diversity in Multi-Agent Systems",
        "authors": [
            "David Lyback"
        ],
        "summary": "Diversity is an important aspect of highly efficient multi-agent teams. We introduce the main factors that drive a multi-agent system in either direction along the diversity scale. A metric for diversity is described, and we speculate on the concept of transient diversity. Finally, an experiment on social entropy using a RoboCup simulated soccer team is presented.",
        "published": "2003-10-06T16:13:13Z",
        "link": "http://arxiv.org/abs/cs/0310010v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Fuzzy Relational Modeling of Cost and Affordability for Advanced   Technology Manufacturing Environment",
        "authors": [
            "Ladislav J. Kohout",
            "Eunjin Kim",
            "Gary Zenz"
        ],
        "summary": "Relational representation of knowledge makes it possible to perform all the computations and decision making in a uniform relational way by means of special relational compositions called triangle and square products. In this paper some applications in manufacturing related to cost analysis are described. Testing fuzzy relational structures for various relational properties allows us to discover dependencies, hierarchies, similarities, and equivalences of the attributes characterizing technological processes and manufactured artifacts in their relationship to costs and performance.   A brief overview of mathematical aspects of BK-relational products is given in Appendix 1 together with further references in the literature.",
        "published": "2003-10-11T14:39:00Z",
        "link": "http://arxiv.org/abs/cs/0310021v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "math.OC",
            "J2; J1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Application of Kullback-Leibler Metric to Speech Recognition",
        "authors": [
            "Igor Bocharov",
            "Pavel Lukin"
        ],
        "summary": "Article discusses the application of Kullback-Leibler divergence to the recognition of speech signals and suggests three algorithms implementing this divergence criterion: correlation algorithm, spectral algorithm and filter algorithm. Discussion covers an approach to the problem of speech variability and is illustrated with the results of experimental modeling of speech signals. The article gives a number of recommendations on the choice of appropriate model parameters and provides a comparison to some other methods of speech recognition.",
        "published": "2003-10-13T16:17:51Z",
        "link": "http://arxiv.org/abs/cs/0310023v1",
        "categories": [
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "The Algebra of Utility Inference",
        "authors": [
            "Ali E. Abbas"
        ],
        "summary": "Richard Cox [1] set the axiomatic foundations of probable inference and the algebra of propositions. He showed that consistency within these axioms requires certain rules for updating belief. In this paper we use the analogy between probability and utility introduced in [2] to propose an axiomatic foundation for utility inference and the algebra of preferences. We show that consistency within these axioms requires certain rules for updating preference. We discuss a class of utility functions that stems from the axioms of utility inference and show that this class is the basic building block for any general multiattribute utility function. We use this class of utility functions together with the algebra of preferences to construct utility functions represented by logical operations on the attributes.",
        "published": "2003-10-23T01:13:20Z",
        "link": "http://arxiv.org/abs/cs/0310044v1",
        "categories": [
            "cs.AI",
            "J.4"
        ]
    },
    {
        "title": "An information theory for preferences",
        "authors": [
            "Ali E. Abbas"
        ],
        "summary": "Recent literature in the last Maximum Entropy workshop introduced an analogy between cumulative probability distributions and normalized utility functions. Based on this analogy, a utility density function can de defined as the derivative of a normalized utility function. A utility density function is non-negative and integrates to unity. These two properties form the basis of a correspondence between utility and probability. A natural application of this analogy is a maximum entropy principle to assign maximum entropy utility values. Maximum entropy utility interprets many of the common utility functions based on the preference information needed for their assignment, and helps assign utility values based on partial preference information. This paper reviews maximum entropy utility and introduces further results that stem from the duality between probability and utility.",
        "published": "2003-10-23T01:34:44Z",
        "link": "http://arxiv.org/abs/cs/0310045v1",
        "categories": [
            "cs.AI",
            "J.4"
        ]
    },
    {
        "title": "Abductive Logic Programs with Penalization: Semantics, Complexity and   Implementation",
        "authors": [
            "Simona Perri",
            "Francesco Scarcello",
            "Nicola Leone"
        ],
        "summary": "Abduction, first proposed in the setting of classical logics, has been studied with growing interest in the logic programming area during the last years.   In this paper we study abduction with penalization in the logic programming framework. This form of abductive reasoning, which has not been previously analyzed in logic programming, turns out to represent several relevant problems, including optimization problems, very naturally. We define a formal model for abduction with penalization over logic programs, which extends the abductive framework proposed by Kakas and Mancarella. We address knowledge representation issues, encoding a number of problems in our abductive framework. In particular, we consider some relevant problems, taken from different domains, ranging from optimization theory to diagnosis and planning; their encodings turn out to be simple and elegant in our formalism. We thoroughly analyze the computational complexity of the main problems arising in the context of abduction with penalization from logic programs. Finally, we implement a system supporting the proposed abductive framework on top of the DLV engine. To this end, we design a translation from abduction problems with penalties into logic programs with weak constraints. We prove that this approach is sound and complete.",
        "published": "2003-10-24T18:03:06Z",
        "link": "http://arxiv.org/abs/cs/0310047v1",
        "categories": [
            "cs.AI",
            "D.1.6"
        ]
    },
    {
        "title": "Local-search techniques for propositional logic extended with   cardinality constraints",
        "authors": [
            "Lengning Liu",
            "Miroslaw Truszczynski"
        ],
        "summary": "We study local-search satisfiability solvers for propositional logic extended with cardinality atoms, that is, expressions that provide explicit ways to model constraints on cardinalities of sets. Adding cardinality atoms to the language of propositional logic facilitates modeling search problems and often results in concise encodings. We propose two ``native'' local-search solvers for theories in the extended language. We also describe techniques to reduce the problem to standard propositional satisfiability and allow us to use off-the-shelf SAT solvers. We study these methods experimentally. Our general finding is that native solvers designed specifically for the extended language perform better than indirect methods relying on SAT solvers.",
        "published": "2003-10-31T16:29:02Z",
        "link": "http://arxiv.org/abs/cs/0310061v1",
        "categories": [
            "cs.AI",
            "I.2.8; F.4.1"
        ]
    },
    {
        "title": "WSAT(cc) - a fast local-search ASP solver",
        "authors": [
            "Lengning Liu",
            "Miroslaw Truszczynski"
        ],
        "summary": "We describe WSAT(cc), a local-search solver for computing models of theories in the language of propositional logic extended by cardinality atoms. WSAT(cc) is a processing back-end for the logic PS+, a recently proposed formalism for answer-set programming.",
        "published": "2003-10-31T16:46:07Z",
        "link": "http://arxiv.org/abs/cs/0310062v1",
        "categories": [
            "cs.AI",
            "I.2.8; F.4.1"
        ]
    },
    {
        "title": "Modeling State in Software Debugging of VHDL-RTL Designs -- A   Model-Based Diagnosis Approach",
        "authors": [
            "Bernhard Peischl",
            "Franz Wotawa"
        ],
        "summary": "In this paper we outline an approach of applying model-based diagnosis to the field of automatic software debugging of hardware designs. We present our value-level model for debugging VHDL-RTL designs and show how to localize the erroneous component responsible for an observed misbehavior. Furthermore, we discuss an extension of our model that supports the debugging of sequential circuits, not only at a given point in time, but also allows for considering the temporal behavior of VHDL-RTL designs. The introduced model is capable of handling state inherently present in every sequential circuit. The principal applicability of the new model is outlined briefly and we use industrial-sized real world examples from the ISCAS'85 benchmark suite to discuss the scalability of our approach.",
        "published": "2003-11-03T19:29:31Z",
        "link": "http://arxiv.org/abs/cs/0311001v1",
        "categories": [
            "cs.AI",
            "cs.SE",
            "D. 2.5"
        ]
    },
    {
        "title": "Enhancing a Search Algorithm to Perform Intelligent Backtracking",
        "authors": [
            "Maurice Bruynooghe"
        ],
        "summary": "This paper illustrates how a Prolog program, using chronological backtracking to find a solution in some search space, can be enhanced to perform intelligent backtracking. The enhancement crucially relies on the impurity of Prolog that allows a program to store information when a dead end is reached. To illustrate the technique, a simple search program is enhanced.   To appear in Theory and Practice of Logic Programming.   Keywords: intelligent backtracking, dependency-directed backtracking, backjumping, conflict-directed backjumping, nogood sets, look-back.",
        "published": "2003-11-05T09:56:42Z",
        "link": "http://arxiv.org/abs/cs/0311003v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.8; I.2.3; F.3.3; F.4.1; D.1.6; D3.3"
        ]
    },
    {
        "title": "Utility-Probability Duality",
        "authors": [
            "Ali Abbas",
            "Jim Matheson"
        ],
        "summary": "This paper presents duality between probability distributions and utility functions.",
        "published": "2003-11-06T07:33:23Z",
        "link": "http://arxiv.org/abs/cs/0311004v1",
        "categories": [
            "cs.AI",
            "G.3.3"
        ]
    },
    {
        "title": "Parametric Connectives in Disjunctive Logic Programming",
        "authors": [
            "Simona Perri",
            "Nicola Leone"
        ],
        "summary": "Disjunctive Logic Programming (\\DLP) is an advanced formalism for Knowledge Representation and Reasoning (KRR). \\DLP is very expressive in a precise mathematical sense: it allows to express every property of finite structures that is decidable in the complexity class $\\SigmaP{2}$ ($\\NP^{\\NP}$). Importantly, the \\DLP encodings are often simple and natural.   In this paper, we single out some limitations of \\DLP for KRR, which cannot naturally express problems where the size of the disjunction is not known ``a priori'' (like N-Coloring), but it is part of the input. To overcome these limitations, we further enhance the knowledge modelling abilities of \\DLP, by extending this language by {\\em Parametric Connectives (OR and AND)}. These connectives allow us to represent compactly the disjunction/conjunction of a set of atoms having a given property. We formally define the semantics of the new language, named $DLP^{\\bigvee,\\bigwedge}$ and we show the usefulness of the new constructs on relevant knowledge-based problems. We address implementation issues and discuss related works.",
        "published": "2003-11-07T15:57:07Z",
        "link": "http://arxiv.org/abs/cs/0311007v1",
        "categories": [
            "cs.AI",
            "D.1.6; D.3.1"
        ]
    },
    {
        "title": "A Parameterised Hierarchy of Argumentation Semantics for Extended Logic   Programming and its Application to the Well-founded Semantics",
        "authors": [
            "Ralf Schweimeier",
            "Michael Schroeder"
        ],
        "summary": "Argumentation has proved a useful tool in defining formal semantics for assumption-based reasoning by viewing a proof as a process in which proponents and opponents attack each others arguments by undercuts (attack to an argument's premise) and rebuts (attack to an argument's conclusion). In this paper, we formulate a variety of notions of attack for extended logic programs from combinations of undercuts and rebuts and define a general hierarchy of argumentation semantics parameterised by the notions of attack chosen by proponent and opponent. We prove the equivalence and subset relationships between the semantics and examine some essential properties concerning consistency and the coherence principle, which relates default negation and explicit negation. Most significantly, we place existing semantics put forward in the literature in our hierarchy and identify a particular argumentation semantics for which we prove equivalence to the paraconsistent well-founded semantics with explicit negation, WFSX$_p$. Finally, we present a general proof theory, based on dialogue trees, and show that it is sound and complete with respect to the argumentation semantics.",
        "published": "2003-11-08T14:02:47Z",
        "link": "http://arxiv.org/abs/cs/0311008v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6.; F.3.2.; F.4.1; I.2.3.; I.2.4"
        ]
    },
    {
        "title": "Optimality of Universal Bayesian Sequence Prediction for General Loss   and Alphabet",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Various optimality properties of universal sequence predictors based on Bayes-mixtures in general, and Solomonoff's prediction scheme in particular, will be studied. The probability of observing $x_t$ at time $t$, given past observations $x_1...x_{t-1}$ can be computed with the chain rule if the true generating distribution $\\mu$ of the sequences $x_1x_2x_3...$ is known. If $\\mu$ is unknown, but known to belong to a countable or continuous class $\\M$ one can base ones prediction on the Bayes-mixture $\\xi$ defined as a $w_\\nu$-weighted sum or integral of distributions $\\nu\\in\\M$. The cumulative expected loss of the Bayes-optimal universal prediction scheme based on $\\xi$ is shown to be close to the loss of the Bayes-optimal, but infeasible prediction scheme based on $\\mu$. We show that the bounds are tight and that no other predictor can lead to significantly smaller bounds. Furthermore, for various performance measures, we show Pareto-optimality of $\\xi$ and give an Occam's razor argument that the choice $w_\\nu\\sim 2^{-K(\\nu)}$ for the weights is optimal, where $K(\\nu)$ is the length of the shortest program describing $\\nu$. The results are applied to games of chance, defined as a sequence of bets, observations, and rewards. The prediction schemes (and bounds) are compared to the popular predictors based on expert advice. Extensions to infinite alphabets, partial, delayed and probabilistic prediction, classification, and more active systems are briefly discussed.",
        "published": "2003-11-13T12:02:04Z",
        "link": "http://arxiv.org/abs/cs/0311014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.PR",
            "E.4;I.2.6;G.3"
        ]
    },
    {
        "title": "Logic-Based Specification Languages for Intelligent Software Agents",
        "authors": [
            "Viviana Mascardi",
            "Maurizio Martelli",
            "Leon Sterling"
        ],
        "summary": "The research field of Agent-Oriented Software Engineering (AOSE) aims to find abstractions, languages, methodologies and toolkits for modeling, verifying, validating and prototyping complex applications conceptualized as Multiagent Systems (MASs). A very lively research sub-field studies how formal methods can be used for AOSE. This paper presents a detailed survey of six logic-based executable agent specification languages that have been chosen for their potential to be integrated in our ARPEGGIO project, an open framework for specifying and prototyping a MAS. The six languages are ConGoLog, Agent-0, the IMPACT agent programming language, DyLog, Concurrent METATEM and Ehhf. For each executable language, the logic foundations are described and an example of use is shown. A comparison of the six languages and a survey of similar approaches complete the paper, together with considerations of the advantages of using logic-based languages in MAS modeling and prototyping.",
        "published": "2003-11-20T10:10:25Z",
        "link": "http://arxiv.org/abs/cs/0311024v1",
        "categories": [
            "cs.AI",
            "A.1; F.4.1; I.2.11"
        ]
    },
    {
        "title": "Using Counterfactuals in Knowledge-Based Programming",
        "authors": [
            "Joseph Y. Halpern",
            "Yoram Moses"
        ],
        "summary": "This paper adds counterfactuals to the framework of knowledge-based programs of Fagin, Halpern, Moses, and Vardi. The use of counterfactuals is illustrated by designing a protocol in which an agent stops sending messages once it knows that it is safe to do so. Such behavior is difficult to capture in the original framework because it involves reasoning about counterfactual executions, including ones that are not consistent with the protocol. Attempts to formalize these notions without counterfactuals are shown to lead to rather counterintuitive behavior.",
        "published": "2003-11-20T17:26:27Z",
        "link": "http://arxiv.org/abs/cs/0311028v1",
        "categories": [
            "cs.DC",
            "cs.AI",
            "F.4.1, F.3.1, I.2.4, C.2.2, C.2.4"
        ]
    },
    {
        "title": "Great Expectations. Part I: On the Customizability of Generalized   Expected Utility",
        "authors": [
            "Francis C. Chu",
            "Joseph Y. Halpern"
        ],
        "summary": "We propose a generalization of expected utility that we call generalized EU (GEU), where a decision maker's beliefs are represented by plausibility measures, and the decision maker's tastes are represented by general (i.e.,not necessarily real-valued) utility functions. We show that every agent, ``rational'' or not, can be modeled as a GEU maximizer. We then show that we can customize GEU by selectively imposing just the constraints we want. In particular, we show how each of Savage's postulates corresponds to constraints on GEU.",
        "published": "2003-11-20T17:36:53Z",
        "link": "http://arxiv.org/abs/cs/0311026v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Great Expectations. Part II: Generalized Expected Utility as a Universal   Decision Rule",
        "authors": [
            "Francis C. Chu",
            "Joseph Y. Halpern"
        ],
        "summary": "Many different rules for decision making have been introduced in the literature. We show that a notion of generalized expected utility proposed in Part I of this paper is a universal decision rule, in the sense that it can represent essentially all other decision rules.",
        "published": "2003-11-20T17:39:22Z",
        "link": "http://arxiv.org/abs/cs/0311027v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Towards an Intelligent Database System Founded on the SP Theory of   Computing and Cognition",
        "authors": [
            "J. Gerard Wolff"
        ],
        "summary": "The SP theory of computing and cognition, described in previous publications, is an attractive model for intelligent databases because it provides a simple but versatile format for different kinds of knowledge, it has capabilities in artificial intelligence, and it can also function like established database models when that is required.   This paper describes how the SP model can emulate other models used in database applications and compares the SP model with those other models. The artificial intelligence capabilities of the SP model are reviewed and its relationship with other artificial intelligence systems is described. Also considered are ways in which current prototypes may be translated into an 'industrial strength' working system.",
        "published": "2003-11-21T14:44:22Z",
        "link": "http://arxiv.org/abs/cs/0311031v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.1"
        ]
    },
    {
        "title": "Unsupervised Grammar Induction in a Framework of Information Compression   by Multiple Alignment, Unification and Search",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This paper describes a novel approach to grammar induction that has been developed within a framework designed to integrate learning with other aspects of computing, AI, mathematics and logic. This framework, called \"information compression by multiple alignment, unification and search\" (ICMAUS), is founded on principles of Minimum Length Encoding pioneered by Solomonoff and others. Most of the paper describes SP70, a computer model of the ICMAUS framework that incorporates processes for unsupervised learning of grammars. An example is presented to show how the model can infer a plausible grammar from appropriate input. Limitations of the current model and how they may be overcome are briefly discussed.",
        "published": "2003-11-27T11:18:59Z",
        "link": "http://arxiv.org/abs/cs/0311045v1",
        "categories": [
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Turning CARTwheels: An Alternating Algorithm for Mining Redescriptions",
        "authors": [
            "Deept Kumar",
            "Naren Ramakrishnan",
            "Malcolm Potts",
            "Richard F. Helm"
        ],
        "summary": "We present an unusual algorithm involving classification trees where two trees are grown in opposite directions so that they are matched at their leaves. This approach finds application in a new data mining task we formulate, called \"redescription mining\". A redescription is a shift-of-vocabulary, or a different way of communicating information about a given subset of data; the goal of redescription mining is to find subsets of data that afford multiple descriptions. We highlight the importance of this problem in domains such as bioinformatics, which exhibit an underlying richness and diversity of data descriptors (e.g., genes can be studied in a variety of ways). Our approach helps integrate multiple forms of characterizing datasets, situates the knowledge gained from one dataset in the context of others, and harnesses high-level abstractions for uncovering cryptic and subtle features of data. Algorithm design decisions, implementation details, and experimental results are presented.",
        "published": "2003-11-27T18:13:38Z",
        "link": "http://arxiv.org/abs/cs/0311048v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Data mining and Privacy in Public Sector using Intelligent Agents   (discussion paper)",
        "authors": [
            "Max Voskob",
            "Nuck Punin"
        ],
        "summary": "The public sector comprises government agencies, ministries, education institutions, health providers and other types of government, commercial and not-for-profit organisations. Unlike commercial enterprises, this environment is highly heterogeneous in all aspects. This forms a complex network which is not always optimised. A lack of optimisation and communication hinders information sharing between the network nodes limiting the flow of information. Another limiting aspect is privacy of personal information and security of operations of some nodes or segments of the network. Attempts to reorganise the network or improve communications to make more information available for sharing and analysis may be hindered or completely halted by public concerns over privacy, political agendas, social and technological barriers. This paper discusses a technical solution for information sharing while addressing the privacy concerns with no need for reorganisation of the existing public sector infrastructure . The solution is based on imposing an additional layer of Intelligent Software Agents and Knowledge Bases for data mining and analysis.",
        "published": "2003-11-28T00:06:32Z",
        "link": "http://arxiv.org/abs/cs/0311050v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.IR",
            "cs.MA",
            "C.2.4; D.2.11; H.1.1; H.1.2; H.3.5; I.2.11; K.4.1"
        ]
    },
    {
        "title": "Integrating existing cone-shaped and projection-based cardinal direction   relations and a TCSP-like decidable generalisation",
        "authors": [
            "Amar Isli"
        ],
        "summary": "We consider the integration of existing cone-shaped and projection-based calculi of cardinal direction relations, well-known in QSR. The more general, integrating language we consider is based on convex constraints of the qualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal direction atomic relation, or of the quantitative form $(\\alpha ,\\beta)(x,y)$, with $\\alpha ,\\beta\\in [0,2\\pi)$ and $(\\beta -\\alpha)\\in [0,\\pi ]$: the meaning of the quantitative constraint, in particular, is that point $x$ belongs to the (convex) cone-shaped area rooted at $y$, and bounded by angles $\\alpha$ and $\\beta$. The general form of a constraint is a disjunction of the form $[r_1\\vee...\\vee r_{n_1}\\vee (\\alpha_1,\\beta_1)\\vee...\\vee (\\alpha _{n_2},\\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\\alpha _i,\\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above: the meaning of such a general constraint is that, for some $i=1... n_1$, $r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\\alpha_i,\\beta_i)(x,y)$ holds. A conjunction of such general constraints is a $\\tcsp$-like CSP, which we will refer to as an $\\scsp$ (Spatial Constraint Satisfaction Problem). An effective solution search algorithm for an $\\scsp$ will be described, which uses (1) constraint propagation, based on a composition operation to be defined, as the filtering method during the search, and (2) the Simplex algorithm, guaranteeing completeness, at the leaves of the search tree. The approach is particularly suited for large-scale high-level vision, such as, e.g., satellite-like surveillance of a geographic area.",
        "published": "2003-11-28T04:06:56Z",
        "link": "http://arxiv.org/abs/cs/0311051v1",
        "categories": [
            "cs.AI",
            "I.2 (I.2.4)"
        ]
    },
    {
        "title": "A Situation Calculus-based Approach To Model Ubiquitous Information   Services",
        "authors": [
            "Dong Wen-Yu",
            "Xu Ke",
            "Lin Meng-Xiang"
        ],
        "summary": "This paper presents an augmented situation calculus-based approach to model autonomous computing paradigm in ubiquitous information services. To make it practical for commercial development and easier to support autonomous paradigm imposed by ubiquitous information services, we made improvements based on Reiter's standard situation calculus. First we explore the inherent relationship between fluents and evolution: since not all fluents contribute to systems' evolution and some fluents can be derived from some others, we define those fluents that are sufficient and necessary to determine evolutional potential as decisive fluents, and then we prove that their successor states wrt to deterministic complex actions satisfy Markov property. Then, within the calculus framework we build, we introduce validity theory to model the autonomous services with application-specific validity requirements, including: validity fluents to axiomatize validity requirements, heuristic multiple alternative service choices ranging from complete acceptance, partial acceptance, to complete rejection, and validity-ensured policy to comprise such alternative service choices into organic, autonomously-computable services. Our approach is demonstrated by a ubiquitous calendaring service, ACS, throughout the paper.",
        "published": "2003-11-28T09:29:35Z",
        "link": "http://arxiv.org/abs/cs/0311052v2",
        "categories": [
            "cs.AI",
            "cs.HC",
            "I.2.0;H.1.2"
        ]
    },
    {
        "title": "Modeling Object Oriented Constraint Programs in Z",
        "authors": [
            "Laurent Henocque"
        ],
        "summary": "Object oriented constraint programs (OOCPs) emerge as a leading evolution of constraint programming and artificial intelligence, first applied to a range of industrial applications called configuration problems. The rich variety of technical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP, Terminological systems, constraint programs with set variables ...) is a source of difficulty. No universally accepted formal language exists for communicating about OOCPs, which makes the comparison of systems difficult. We present here a Z based specification of OOCPs which avoids the falltrap of hidden object semantics. The object system is part of the specification, and captures all of the most advanced notions from the object oriented modeling standard UML. The paper illustrates these issues and the conciseness and precision of Z by the specification of a working OOCP that solves an historical AI problem : parsing a context free grammar. Being written in Z, an OOCP specification also supports formal proofs. The whole builds the foundation of an adaptative and evolving framework for communicating about constrained object models and programs.",
        "published": "2003-12-12T10:15:38Z",
        "link": "http://arxiv.org/abs/cs/0312020v1",
        "categories": [
            "cs.AI",
            "I.2.4;F.4.1"
        ]
    },
    {
        "title": "Soft Constraint Programming to Analysing Security Protocols",
        "authors": [
            "Giampaolo Bella",
            "Stefano Bistarelli"
        ],
        "summary": "Security protocols stipulate how the remote principals of a computer network should interact in order to obtain specific security goals. The crucial goals of confidentiality and authentication may be achieved in various forms, each of different strength. Using soft (rather than crisp) constraints, we develop a uniform formal notion for the two goals. They are no longer formalised as mere yes/no properties as in the existing literature, but gain an extra parameter, the security level. For example, different messages can enjoy different levels of confidentiality, or a principal can achieve different levels of authentication with different principals.   The goals are formalised within a general framework for protocol analysis that is amenable to mechanisation by model checking. Following the application of the framework to analysing the asymmetric Needham-Schroeder protocol, we have recently discovered a new attack on that protocol as a form of retaliation by principals who have been attacked previously. Having commented on that attack, we then demonstrate the framework on a bigger, largely deployed protocol consisting of three phases, Kerberos.",
        "published": "2003-12-14T21:17:08Z",
        "link": "http://arxiv.org/abs/cs/0312025v1",
        "categories": [
            "cs.CR",
            "cs.AI",
            "D.3.3; K.6.5; D.4.6"
        ]
    },
    {
        "title": "Speedup of Logic Programs by Binarization and Partial Deduction",
        "authors": [
            "Jan Hruza",
            "Petr Stepanek"
        ],
        "summary": "Binary logic programs can be obtained from ordinary logic programs by a binarizing transformation. In most cases, binary programs obtained this way are less efficient than the original programs. (Demoen, 1992) showed an interesting example of a logic program whose computational behaviour was improved when it was transformed to a binary program and then specialized by partial deduction. The class of B-stratifiable logic programs is defined. It is shown that for every B-stratifiable logic program, binarization and subsequent partial deduction produce a binary program which does not contain variables for continuations introduced by binarization. Such programs usually have a better computational behaviour than the original ones. Both binarization and partial deduction can be easily automated. A comparison with other related approaches to program transformation is given.",
        "published": "2003-12-15T12:23:42Z",
        "link": "http://arxiv.org/abs/cs/0312026v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6;I.2.2;I.2.3;F.4.1"
        ]
    },
    {
        "title": "Minimal founded semantics for disjunctive logic programs and deductive   databases",
        "authors": [
            "Filippo Furfaro",
            "Gianluigi Greco",
            "Sergio Greco"
        ],
        "summary": "In this paper, we propose a variant of stable model semantics for disjunctive logic programming and deductive databases. The semantics, called minimal founded, generalizes stable model semantics for normal (i.e. non disjunctive) programs but differs from disjunctive stable model semantics (the extension of stable model semantics for disjunctive programs). Compared with disjunctive stable model semantics, minimal founded semantics seems to be more intuitive, it gives meaning to programs which are meaningless under stable model semantics and is no harder to compute. More specifically, minimal founded semantics differs from stable model semantics only for disjunctive programs having constraint rules or rules working as constraints. We study the expressive power of the semantics and show that for general disjunctive datalog programs it has the same power as disjunctive stable model semantics.",
        "published": "2003-12-15T18:29:41Z",
        "link": "http://arxiv.org/abs/cs/0312028v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Strong Equivalence Made Easy: Nested Expressions and Weight Constraints",
        "authors": [
            "Hudson Turner"
        ],
        "summary": "Logic programs P and Q are strongly equivalent if, given any program R, programs P union R and Q union R are equivalent (that is, have the same answer sets). Strong equivalence is convenient for the study of equivalent transformations of logic programs: one can prove that a local change is correct without considering the whole program. Lifschitz, Pearce and Valverde showed that Heyting's logic of here-and-there can be used to characterize strong equivalence for logic programs with nested expressions (which subsume the better-known extended disjunctive programs). This note considers a simpler, more direct characterization of strong equivalence for such programs, and shows that it can also be applied without modification to the weight constraint programs of Niemela and Simons. Thus, this characterization of strong equivalence is convenient for the study of equivalent transformations of logic programs written in the input languages of answer set programming systems dlv and smodels. The note concludes with a brief discussion of results that can be used to automate reasoning about strong equivalence, including a novel encoding that reduces the problem of deciding the strong equivalence of a pair of weight constraint programs to that of deciding the inconsistency of a weight constraint program.",
        "published": "2003-12-15T20:58:14Z",
        "link": "http://arxiv.org/abs/cs/0312029v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6"
        ]
    },
    {
        "title": "What Causes a System to Satisfy a Specification?",
        "authors": [
            "Hana Chockler",
            "Joseph Y. Halpern",
            "Orna Kupferman"
        ],
        "summary": "Even when a system is proven to be correct with respect to a specification, there is still a question of how complete the specification is, and whether it really covers all the behaviors of the system. Coverage metrics attempt to check which parts of a system are actually relevant for the verification process to succeed. Recent work on coverage in model checking suggests several coverage metrics and algorithms for finding parts of the system that are not covered by the specification. The work has already proven to be effective in practice, detecting design errors that escape early verification efforts in industrial settings. In this paper, we relate a formal definition of causality given by Halpern and Pearl [2001] to coverage. We show that it gives significant insight into unresolved issues regarding the definition of coverage and leads to potentially useful extensions of coverage. In particular, we introduce the notion of responsibility, which assigns to components of a system a quantitative measure of their relevance to the satisfaction of the specification.",
        "published": "2003-12-17T16:42:41Z",
        "link": "http://arxiv.org/abs/cs/0312036v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; F.3.1; I.2.4"
        ]
    },
    {
        "title": "Characterizing and Reasoning about Probabilistic and Non-Probabilistic   Expectation",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "Expectation is a central notion in probability theory. The notion of expectation also makes sense for other notions of uncertainty. We introduce a propositional logic for reasoning about expectation, where the semantics depends on the underlying representation of uncertainty. We give sound and complete axiomatizations for the logic in the case that the underlying representation is (a) probability, (b) sets of probability measures, (c) belief functions, and (d) possibility measures. We show that this logic is more expressive than the corresponding logic for reasoning about likelihood in the case of sets of probability measures, but equi-expressive in the case of probability, belief, and possibility. Finally, we show that satisfiability for these logics is NP-complete, no harder than satisfiability for propositional logic.",
        "published": "2003-12-17T16:50:39Z",
        "link": "http://arxiv.org/abs/cs/0312037v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.4; G.3"
        ]
    },
    {
        "title": "Responsibility and blame: a structural-model approach",
        "authors": [
            "Hana Chockler",
            "Joseph Y. Halpern"
        ],
        "summary": "Causality is typically treated an all-or-nothing concept; either A is a cause of B or it is not. We extend the definition of causality introduced by Halpern and Pearl [2001] to take into account the degree of responsibility of A for B. For example, if someone wins an election 11--0, then each person who votes for him is less responsible for the victory than if he had won 6--5. We then define a notion of degree of blame, which takes into account an agent's epistemic state. Roughly speaking, the degree of blame of A for B is the expected degree of responsibility of A for B, taken over the epistemic state of an agent.",
        "published": "2003-12-17T16:59:50Z",
        "link": "http://arxiv.org/abs/cs/0312038v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.1"
        ]
    },
    {
        "title": "Diagnostic reasoning with A-Prolog",
        "authors": [
            "Marcello Balduccini",
            "Michael Gelfond"
        ],
        "summary": "In this paper we suggest an architecture for a software agent which operates a physical device and is capable of making observations and of testing and repairing the device's components. We present simplified definitions of the notions of symptom, candidate diagnosis, and diagnosis which are based on the theory of action language ${\\cal AL}$. The definitions allow one to give a simple account of the agent's behavior in which many of the agent's tasks are reduced to computing stable models of logic programs.",
        "published": "2003-12-18T13:38:49Z",
        "link": "http://arxiv.org/abs/cs/0312040v1",
        "categories": [
            "cs.AI",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Greedy Algorithms in Datalog",
        "authors": [
            "Sergio Greco",
            "Carlo Zaniolo"
        ],
        "summary": "In the design of algorithms, the greedy paradigm provides a powerful tool for solving efficiently classical computational problems, within the framework of procedural languages. However, expressing these algorithms within the declarative framework of logic-based languages has proven a difficult research challenge. In this paper, we extend the framework of Datalog-like languages to obtain simple declarative formulations for such problems, and propose effective implementation techniques to ensure computational complexities comparable to those of procedural formulations. These advances are achieved through the use of the \"choice\" construct, extended with preference annotations to effect the selection of alternative stable-models and nondeterministic fixpoints. We show that, with suitable storage structures, the differential fixpoint computation of our programs matches the complexity of procedural algorithms in classical search and optimization problems.",
        "published": "2003-12-18T17:29:05Z",
        "link": "http://arxiv.org/abs/cs/0312041v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "D.1.6; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Weight Constraints as Nested Expressions",
        "authors": [
            "Paolo Ferraris",
            "Vladimir Lifschitz"
        ],
        "summary": "We compare two recent extensions of the answer set (stable model) semantics of logic programs. One of them, due to Lifschitz, Tang and Turner, allows the bodies and heads of rules to contain nested expressions. The other, due to Niemela and Simons, uses weight constraints. We show that there is a simple, modular translation from the language of weight constraints into the language of nested expressions that preserves the program's answer sets. Nested expressions can be eliminated from the result of this translation in favor of additional atoms. The translation makes it possible to compute answer sets for some programs with weight constraints using satisfiability solvers, and to prove the strong equivalence of programs with weight constraints using the logic of here-and there.",
        "published": "2003-12-19T16:00:43Z",
        "link": "http://arxiv.org/abs/cs/0312045v1",
        "categories": [
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Clustering by compression",
        "authors": [
            "Rudi Cilibrasi",
            "Paul Vitanyi"
        ],
        "summary": "We present a new method for clustering based on compression. The method doesn't use subject-specific features or background knowledge, and works as follows: First, we determine a universal similarity distance, the normalized compression distance or NCD, computed from the lengths of compressed data files (singly and in pairwise concatenation). Second, we apply a hierarchical clustering method. The NCD is universal in that it is not restricted to a specific application area, and works across application area boundaries. A theoretical precursor, the normalized information distance, co-developed by one of the authors, is provably optimal but uses the non-computable notion of Kolmogorov complexity. We propose precise notions of similarity metric, normal compressor, and show that the NCD based on a normal compressor is a similarity metric that approximates universality. To extract a hierarchy of clusters from the distance matrix, we determine a dendrogram (binary tree) by a new quartet method and a fast heuristic to implement it. The method is implemented and available as public software, and is robust under choice of different compressors. To substantiate our claims of universality and robustness, we report evidence of successful application in areas as diverse as genomics, virology, languages, literature, music, handwritten digits, astronomy, and combinations of objects from completely different domains, using statistical, dictionary, and block sorting compressors. In genomics we presented new evidence for major questions in Mammalian evolution, based on whole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta hypothesis against the Theria hypothesis.",
        "published": "2003-12-19T18:41:29Z",
        "link": "http://arxiv.org/abs/cs/0312044v2",
        "categories": [
            "cs.CV",
            "cond-mat.stat-mech",
            "cs.AI",
            "physics.data-an",
            "q-bio.GN",
            "q-bio.QM",
            "E4, H.3.3, H.5.5, I.2.6, I.2.10, I.5.3, J.3,J.5"
        ]
    },
    {
        "title": "Representation Dependence in Probabilistic Inference",
        "authors": [
            "Joseph Y. Halpern",
            "Daphne Koller"
        ],
        "summary": "Non-deductive reasoning systems are often {\\em representation dependent}: representing the same situation in two different ways may cause such a system to return two different answers. Some have viewed this as a significant problem. For example, the principle of maximum entropy has been subjected to much criticism due to its representation dependence. There has, however, been almost no work investigating representation dependence. In this paper, we formalize this notion and show that it is not a problem specific to maximum entropy. In fact, we show that any representation-independent probabilistic inference procedure that ignores irrelevant information is essentially entailment, in a precise sense. Moreover, we show that representation independence is incompatible with even a weak default assumption of independence. We then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata, and provide a construction of a family of inference procedures that provides such restricted representation independence, using relative entropy.",
        "published": "2003-12-20T16:30:20Z",
        "link": "http://arxiv.org/abs/cs/0312048v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.q"
        ]
    },
    {
        "title": "Towards Automated Generation of Scripted Dialogue: Some Time-Honoured   Strategies",
        "authors": [
            "Paul Piwek",
            "Kees van Deemter"
        ],
        "summary": "The main aim of this paper is to introduce automated generation of scripted dialogue as a worthwhile topic of investigation. In particular the fact that scripted dialogue involves two layers of communication, i.e., uni-directional communication between the author and the audience of a scripted dialogue and bi-directional pretended communication between the characters featuring in the dialogue, is argued to raise some interesting issues. Our hope is that the combined study of the two layers will forge links between research in text generation and dialogue processing. The paper presents a first attempt at creating such links by studying three types of strategies for the automated generation of scripted dialogue. The strategies are derived from examples of human-authored and naturally occurring dialogue.",
        "published": "2003-12-22T16:51:53Z",
        "link": "http://arxiv.org/abs/cs/0312051v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Dialogue as Discourse: Controlling Global Properties of Scripted   Dialogue",
        "authors": [
            "Paul Piwek",
            "Kees van Deemter"
        ],
        "summary": "This paper explains why scripted dialogue shares some crucial properties with discourse. In particular, when scripted dialogues are generated by a Natural Language Generation system, the generator can apply revision strategies that cannot normally be used when the dialogue results from an interaction between autonomous agents (i.e., when the dialogue is not scripted). The paper explains that the relevant revision operators are best applied at the level of a dialogue plan and discusses how the generator may decide when to apply a given revision operator.",
        "published": "2003-12-22T17:07:31Z",
        "link": "http://arxiv.org/abs/cs/0312052v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "On the Expressibility of Stable Logic Programming",
        "authors": [
            "Victor W. Marek",
            "Jeffrey B. Remmel"
        ],
        "summary": "(We apologize for pidgin LaTeX) Schlipf \\cite{sch91} proved that Stable Logic Programming (SLP) solves all $\\mathit{NP}$ decision problems. We extend Schlipf's result to prove that SLP solves all search problems in the class $\\mathit{NP}$. Moreover, we do this in a uniform way as defined in \\cite{mt99}. Specifically, we show that there is a single $\\mathrm{DATALOG}^{\\neg}$ program $P_{\\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$ with non-negative integer coefficients and any input $\\sigma$ of size $n$ over a fixed alphabet $\\Sigma$, there is an extensional database $\\mathit{edb}_{M,p,\\sigma}$ such that there is a one-to-one correspondence between the stable models of $\\mathit{edb}_{M,p,\\sigma} \\cup P_{\\mathit{Trg}}$ and the accepting computations of the machine $M$ that reach the final state in at most $p(n)$ steps. Moreover, $\\mathit{edb}_{M,p,\\sigma}$ can be computed in polynomial time from $p$, $\\sigma$ and the description of $M$ and the decoding of such accepting computations from its corresponding stable model of $\\mathit{edb}_{M,p,\\sigma} \\cup P_{\\mathit{Trg}}$ can be computed in linear time. A similar statement holds for Default Logic with respect to $\\Sigma_2^\\mathrm{P}$-search problems\\footnote{The proof of this result involves additional technical complications and will be a subject of another publication.}.",
        "published": "2003-12-22T18:34:21Z",
        "link": "http://arxiv.org/abs/cs/0312053v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Abduction in Well-Founded Semantics and Generalized Stable Models",
        "authors": [
            "José Júlio Alferes",
            "Luís Moniz Pereira",
            "Terrance Swift"
        ],
        "summary": "Abductive logic programming offers a formalism to declaratively express and solve problems in areas such as diagnosis, planning, belief revision and hypothetical reasoning. Tabled logic programming offers a computational mechanism that provides a level of declarativity superior to that of Prolog, and which has supported successful applications in fields such as parsing, program analysis, and model checking. In this paper we show how to use tabled logic programming to evaluate queries to abductive frameworks with integrity constraints when these frameworks contain both default and explicit negation. The result is the ability to compute abduction over well-founded semantics with explicit negation and answer sets. Our approach consists of a transformation and an evaluation method. The transformation adjoins to each objective literal $O$ in a program, an objective literal $not(O)$ along with rules that ensure that $not(O)$ will be true if and only if $O$ is false. We call the resulting program a {\\em dual} program. The evaluation method, \\wfsmeth, then operates on the dual program. \\wfsmeth{} is sound and complete for evaluating queries to abductive frameworks whose entailment method is based on either the well-founded semantics with explicit negation, or on answer sets. Further, \\wfsmeth{} is asymptotically as efficient as any known method for either class of problems. In addition, when abduction is not desired, \\wfsmeth{} operating on a dual program provides a novel tabling method for evaluating queries to ground extended programs whose complexity and termination properties are similar to those of the best tabling methods for the well-founded semantics. A publicly available meta-interpreter has been developed for \\wfsmeth{} using the XSB system.",
        "published": "2003-12-24T15:45:40Z",
        "link": "http://arxiv.org/abs/cs/0312057v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; I.2.4"
        ]
    },
    {
        "title": "Acquiring Lexical Paraphrases from a Single Corpus",
        "authors": [
            "Oren Glickman",
            "Ido Dagan"
        ],
        "summary": "This paper studies the potential of identifying lexical paraphrases within a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information. A comparison suggests that an instance-based approach may be combined with a vector based approach in order to assess better the paraphrase likelihood for many verb pairs.",
        "published": "2003-12-25T16:45:20Z",
        "link": "http://arxiv.org/abs/cs/0312058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "I.7"
        ]
    },
    {
        "title": "Polyhierarchical Classifications Induced by Criteria Polyhierarchies,   and Taxonomy Algebra",
        "authors": [
            "Pavel Babikov",
            "Oleg Gontcharov",
            "Maria Babikova"
        ],
        "summary": "A new approach to the construction of general persistent polyhierarchical classifications is proposed. It is based on implicit description of category polyhierarchy by a generating polyhierarchy of classification criteria. Similarly to existing approaches, the classification categories are defined by logical functions encoded by attributive expressions. However, the generating hierarchy explicitly predefines domains of criteria applicability, and the semantics of relations between categories is invariant to changes in the universe composition, extending variety of criteria, and increasing their cardinalities. The generating polyhierarchy is an independent, compact, portable, and re-usable information structure serving as a template classification. It can be associated with one or more particular sets of objects, included in more general classifications as a standard component, or used as a prototype for more comprehensive classifications. The approach dramatically simplifies development and unplanned modifications of persistent hierarchical classifications compared with tree, DAG, and faceted schemes. It can be efficiently implemented in common DBMS, while considerably reducing amount of computer resources required for storage, maintenance, and use of complex polyhierarchies.",
        "published": "2003-12-26T05:30:24Z",
        "link": "http://arxiv.org/abs/cs/0312059v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "I.2.4; H.3.3"
        ]
    },
    {
        "title": "Kalman filter control in the reinforcement learning framework",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "There is a growing interest in using Kalman-filter models in brain modelling. In turn, it is of considerable importance to make Kalman-filters amenable for reinforcement learning. In the usual formulation of optimal control it is computed off-line by solving a backward recursion. In this technical note we show that slight modification of the linear-quadratic-Gaussian Kalman-filter model allows the on-line estimation of optimal control and makes the bridge to reinforcement learning. Moreover, the learning rule for value estimation assumes a Hebbian form weighted by the error of the value estimation.",
        "published": "2003-01-09T15:08:47Z",
        "link": "http://arxiv.org/abs/cs/0301007v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Convergence and Loss Bounds for Bayesian Sequence Prediction",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The probability of observing $x_t$ at time $t$, given past observations $x_1...x_{t-1}$ can be computed with Bayes' rule if the true generating distribution $\\mu$ of the sequences $x_1x_2x_3...$ is known. If $\\mu$ is unknown, but known to belong to a class $M$ one can base ones prediction on the Bayes mix $\\xi$ defined as a weighted sum of distributions $\\nu\\in M$. Various convergence results of the mixture posterior $\\xi_t$ to the true posterior $\\mu_t$ are presented. In particular a new (elementary) derivation of the convergence $\\xi_t/\\mu_t\\to 1$ is provided, which additionally gives the rate of convergence. A general sequence predictor is allowed to choose an action $y_t$ based on $x_1...x_{t-1}$ and receives loss $\\ell_{x_t y_t}$ if $x_t$ is the next symbol of the sequence. No assumptions are made on the structure of $\\ell$ (apart from being bounded) and $M$. The Bayes-optimal prediction scheme $\\Lambda_\\xi$ based on mixture $\\xi$ and the Bayes-optimal informed prediction scheme $\\Lambda_\\mu$ are defined and the total loss $L_\\xi$ of $\\Lambda_\\xi$ is bounded in terms of the total loss $L_\\mu$ of $\\Lambda_\\mu$. It is shown that $L_\\xi$ is bounded for bounded $L_\\mu$ and $L_\\xi/L_\\mu\\to 1$ for $L_\\mu\\to \\infty$. Convergence of the instantaneous losses are also proven.",
        "published": "2003-01-16T16:36:15Z",
        "link": "http://arxiv.org/abs/cs/0301014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.PR",
            "E.4;I.2.6;G.3"
        ]
    },
    {
        "title": "The New AI: General & Sound & Relevant for Physics",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "Most traditional artificial intelligence (AI) systems of the past 50 years are either very limited, or based on heuristics, or both. The new millennium, however, has brought substantial progress in the field of theoretically optimal and practically feasible algorithms for prediction, search, inductive inference based on Occam's razor, problem solving, decision making, and reinforcement learning in environments of a very general type. Since inductive inference is at the heart of all inductive sciences, some of the results are relevant not only for AI and computer science but also for physics, provoking nontraditional predictions based on Zuse's thesis of the computer-generated universe.",
        "published": "2003-02-10T14:17:33Z",
        "link": "http://arxiv.org/abs/cs/0302012v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "quant-ph",
            "I.2"
        ]
    },
    {
        "title": "Unsupervised Learning in a Framework of Information Compression by   Multiple Alignment, Unification and Search",
        "authors": [
            "J. G. Wolff"
        ],
        "summary": "This paper describes a novel approach to unsupervised learning that has been developed within a framework of \"information compression by multiple alignment, unification and search\" (ICMAUS), designed to integrate learning with other AI functions such as parsing and production of language, fuzzy pattern recognition, probabilistic and exact forms of reasoning, and others.",
        "published": "2003-02-12T09:39:00Z",
        "link": "http://arxiv.org/abs/cs/0302015v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.4; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Algorithmic Clustering of Music",
        "authors": [
            "Rudi Cilibrasi",
            "Paul Vitanyi",
            "Ronald de Wolf"
        ],
        "summary": "We present a fully automatic method for music classification, based only on compression of strings that represent the music pieces. The method uses no background knowledge about music whatsoever: it is completely general and can, without change, be used in different areas like linguistic classification and genomics. It is based on an ideal theory of the information content in individual objects (Kolmogorov complexity), information distance, and a universal similarity metric. Experiments show that the method distinguishes reasonably well between various musical genres and can even cluster pieces by composer.",
        "published": "2003-03-24T16:01:46Z",
        "link": "http://arxiv.org/abs/cs/0303025v1",
        "categories": [
            "cs.SD",
            "cs.LG",
            "physics.data-an",
            "E.4, H.3.1, I.5.3, F.1.3, J.5"
        ]
    },
    {
        "title": "Robust Estimators under the Imprecise Dirichlet Model",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Walley's Imprecise Dirichlet Model (IDM) for categorical data overcomes several fundamental problems which other approaches to uncertainty suffer from. Yet, to be useful in practice, one needs efficient ways for computing the imprecise=robust sets or intervals. The main objective of this work is to derive exact, conservative, and approximate, robust and credible interval estimates under the IDM for a large class of statistical estimators, including the entropy and mutual information.",
        "published": "2003-05-08T17:11:45Z",
        "link": "http://arxiv.org/abs/math/0305121v1",
        "categories": [
            "math.PR",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "On the Existence and Convergence Computable Universal Priors",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff unified Occam's razor and Epicurus' principle of multiple explanations to one elegant, formal, universal theory of inductive inference, which initiated the field of algorithmic information theory. His central result is that the posterior of his universal semimeasure M converges rapidly to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal predictor in case of unknown mu. We investigate the existence and convergence of computable universal (semi)measures for a hierarchy of computability classes: finitely computable, estimable, enumerable, and approximable. For instance, M is known to be enumerable, but not finitely computable, and to dominate all enumerable semimeasures. We define seven classes of (semi)measures based on these four computability concepts. Each class may or may not contain a (semi)measure which dominates all elements of another class. The analysis of these 49 cases can be reduced to four basic cases, two of them being new. The results hold for discrete and continuous semimeasures. We also investigate more closely the types of convergence, possibly implied by universality: in difference and in ratio, with probability 1, in mean sum, and for Martin-Loef random sequences. We introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues.",
        "published": "2003-05-29T11:11:01Z",
        "link": "http://arxiv.org/abs/cs/0305052v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "math.ST",
            "stat.TH",
            "G.3; I.2"
        ]
    },
    {
        "title": "Sequence Prediction based on Monotone Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "This paper studies sequence prediction based on the monotone Kolmogorov complexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is extremely close to Solomonoff's prior M, the latter being an excellent predictor in deterministic as well as probabilistic environments, where performance is measured in terms of convergence of posteriors or losses. Despite this closeness to M, it is difficult to assess the prediction quality of m, since little is known about the closeness of their posteriors, which are the important quantities for prediction. We show that for deterministic computable environments, the \"posterior\" and losses of m converge, but rapid convergence could only be shown on-sequence; the off-sequence behavior is unclear. In probabilistic environments, neither the posterior nor the losses converge, in general.",
        "published": "2003-06-07T19:21:20Z",
        "link": "http://arxiv.org/abs/cs/0306036v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2"
        ]
    },
    {
        "title": "Universal Sequential Decisions in Unknown Environments",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "We give a brief introduction to the AIXI model, which unifies and overcomes the limitations of sequential decision theory and universal Solomonoff induction. While the former theory is suited for active agents in known environments, the latter is suited for passive prediction of unknown environments.",
        "published": "2003-06-16T13:15:29Z",
        "link": "http://arxiv.org/abs/cs/0306091v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "I.2; G.3"
        ]
    },
    {
        "title": "Reinforcement Learning with Linear Function Approximation and LQ control   Converges",
        "authors": [
            "Istvan Szita",
            "Andras Lorincz"
        ],
        "summary": "Reinforcement learning is commonly used with function approximation. However, very few positive results are known about the convergence of function approximation based RL control algorithms. In this paper we show that TD(0) and Sarsa(0) with linear function approximation is convergent for a simple class of problems, where the system is linear and the costs are quadratic (the LQ control problem). Furthermore, we show that for systems with Gaussian noise and non-completely observable states (the LQG problem), the mentioned RL algorithms are still convergent, if they are combined with Kalman filtering.",
        "published": "2003-06-22T08:00:09Z",
        "link": "http://arxiv.org/abs/cs/0306120v2",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.2.8"
        ]
    },
    {
        "title": "Bayesian Treatment of Incomplete Discrete Data applied to Mutual   Information and Feature Selection",
        "authors": [
            "Marcus Hutter",
            "Marco Zaffalon"
        ],
        "summary": "Given the joint chances of a pair of random variables one can compute quantities of interest, like the mutual information. The Bayesian treatment of unknown chances involves computing, from a second order prior distribution and the data likelihood, a posterior distribution of the chances. A common treatment of incomplete data is to assume ignorability and determine the chances by the expectation maximization (EM) algorithm. The two different methods above are well established but typically separated. This paper joins the two approaches in the case of Dirichlet priors, and derives efficient approximations for the mean, mode and the (co)variance of the chances and the mutual information. Furthermore, we prove the unimodality of the posterior distribution, whence the important property of convergence of EM to the global maximum in the chosen framework. These results are applied to the problem of selecting features for incremental learning and naive Bayes classification. A fast filter based on the distribution of mutual information is shown to outperform the traditional filter based on empirical mutual information on a number of incomplete real data sets.",
        "published": "2003-06-24T09:50:29Z",
        "link": "http://arxiv.org/abs/cs/0306126v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.PR",
            "G.3; G.1.2; I.2"
        ]
    },
    {
        "title": "A Model for Prejudiced Learning in Noisy Environments",
        "authors": [
            "Andreas U. Schmidt"
        ],
        "summary": "Based on the heuristics that maintaining presumptions can be beneficial in uncertain environments, we propose a set of basic axioms for learning systems to incorporate the concept of prejudice. The simplest, memoryless model of a deterministic learning rule obeying the axioms is constructed, and shown to be equivalent to the logistic map. The system's performance is analysed in an environment in which it is subject to external randomness, weighing learning defectiveness against stability gained. The corresponding random dynamical system with inhomogeneous, additive noise is studied, and shown to exhibit the phenomena of noise induced stability and stochastic bifurcations. The overall results allow for the interpretation that prejudice in uncertain environments entails a considerable portion of stubbornness as a secondary phenomenon.",
        "published": "2003-06-26T10:12:58Z",
        "link": "http://arxiv.org/abs/nlin/0306055v2",
        "categories": [
            "nlin.AO",
            "cs.LG"
        ]
    },
    {
        "title": "AWESOME: A General Multiagent Learning Algorithm that Converges in   Self-Play and Learns a Best Response Against Stationary Opponents",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "A satisfactory multiagent learning algorithm should, {\\em at a minimum}, learn to play optimally against stationary opponents and converge to a Nash equilibrium in self-play. The algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action repeated games--assuming that the opponent's (mixed) strategy is observable. In this paper we present AWESOME, the first algorithm that is guaranteed to have these two properties in {\\em all} repeated (finite) games. It requires only that the other players' actual actions (not their strategies) can be observed at each step. It also learns to play optimally against opponents that {\\em eventually become} stationary. The basic idea behind AWESOME ({\\em Adapt When Everybody is Stationary, Otherwise Move to Equilibrium}) is to try to adapt to the others' strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. The techniques used to prove the properties of AWESOME are fundamentally different from those used for previous algorithms, and may help in analyzing other multiagent learning algorithms also.",
        "published": "2003-07-01T23:22:44Z",
        "link": "http://arxiv.org/abs/cs/0307002v1",
        "categories": [
            "cs.GT",
            "cs.LG",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "BL-WoLF: A Framework For Loss-Bounded Learnability In Zero-Sum Games",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "We present BL-WoLF, a framework for learnability in repeated zero-sum games where the cost of learning is measured by the losses the learning agent accrues (rather than the number of rounds). The game is adversarially chosen from some family that the learner knows. The opponent knows the game and the learner's learning strategy. The learner tries to either not accrue losses, or to quickly learn about the game so as to avoid future losses (this is consistent with the Win or Learn Fast (WoLF) principle; BL stands for ``bounded loss''). Our framework allows for both probabilistic and approximate learning. The resultant notion of {\\em BL-WoLF}-learnability can be applied to any class of games, and allows us to measure the inherent disadvantage to a player that does not know which game in the class it is in. We present {\\em guaranteed BL-WoLF-learnability} results for families of games with deterministic payoffs and families of games with stochastic payoffs. We demonstrate that these families are {\\em guaranteed approximately BL-WoLF-learnable} with lower cost. We then demonstrate families of games (both stochastic and deterministic) that are not guaranteed BL-WoLF-learnable. We show that those families, nevertheless, are {\\em BL-WoLF-learnable}. To prove these results, we use a key lemma which we derive.",
        "published": "2003-07-03T15:44:36Z",
        "link": "http://arxiv.org/abs/cs/0307006v1",
        "categories": [
            "cs.GT",
            "cs.LG",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Manifold Learning with Geodesic Minimal Spanning Trees",
        "authors": [
            "Jose Costa",
            "Alfred Hero"
        ],
        "summary": "In the manifold learning problem one seeks to discover a smooth low dimensional surface, i.e., a manifold embedded in a higher dimensional linear vector space, based on a set of measured sample points on the surface. In this paper we consider the closely related problem of estimating the manifold's intrinsic dimension and the intrinsic entropy of the sample points. Specifically, we view the sample points as realizations of an unknown multivariate density supported on an unknown smooth manifold. We present a novel geometrical probability approach, called the geodesic-minimal-spanning-tree (GMST), to obtaining asymptotically consistent estimates of the manifold dimension and the R\\'{e}nyi $\\alpha$-entropy of the sample density on the manifold. The GMST approach is striking in its simplicity and does not require reconstructing the manifold or estimating the multivariate density of the samples. The GMST method simply constructs a minimal spanning tree (MST) sequence using a geodesic edge matrix and uses the overall lengths of the MSTs to simultaneously estimate manifold dimension and entropy. We illustrate the GMST approach for dimension and entropy estimation of a human face dataset.",
        "published": "2003-07-16T23:50:53Z",
        "link": "http://arxiv.org/abs/cs/0307038v1",
        "categories": [
            "cs.CV",
            "cs.LG",
            "G.3;F.2.2"
        ]
    },
    {
        "title": "Learning Analogies and Semantic Relations",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal analogy has the form A:B::C:D, meaning \"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct). We motivate this research by relating it to work in cognitive science and linguistics, and by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \"laser printer\", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for these challenging problems.",
        "published": "2003-07-24T21:09:43Z",
        "link": "http://arxiv.org/abs/cs/0307055v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Controlled hierarchical filtering: Model of neocortical sensory   processing",
        "authors": [
            "Andras Lorincz"
        ],
        "summary": "A model of sensory information processing is presented. The model assumes that learning of internal (hidden) generative models, which can predict the future and evaluate the precision of that prediction, is of central importance for information extraction. Furthermore, the model makes a bridge to goal-oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippocampal entorhinal loop. This generative control architecture is mapped to the neocortex and to the hippocampal entorhinal loop. Implicit memory phenomena; priming and prototype learning are emerging features of the model. Mathematical theorems ensure stability and attractive learning properties of the architecture. Connections to reinforcement learning are also established: both the control network, and the network with a hidden model converge to (near) optimal policy under suitable conditions. Falsifying predictions, including the role of the feedback connections between neocortical areas are made.",
        "published": "2003-08-16T07:31:57Z",
        "link": "http://arxiv.org/abs/cs/0308025v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "q-bio.NC",
            "C.1.3; F.1.1.; I.2.0; I.2.6; I.2.10; I.4.3.; I.4.10; I.5.1"
        ]
    },
    {
        "title": "Coherent Keyphrase Extraction via Web Mining",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. A limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent. That is, the majority of the output keyphrases may fit together well, but there may be a minority that appear to be outliers, with no clear semantic relation to the majority or to each other. This paper presents enhancements to the Kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. The approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related. The statistical association is measured using web mining. Experiments demonstrate that the enhancements improve the quality of the extracted keyphrases. Furthermore, the enhancements are not domain-specific: the algorithm generalizes well when it is trained on one domain (computer science documents) and tested on another (physics documents).",
        "published": "2003-08-20T20:42:19Z",
        "link": "http://arxiv.org/abs/cs/0308033v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Reliable and Efficient Inference of Bayesian Networks from Sparse Data   by Statistical Learning Theory",
        "authors": [
            "Dominik Janzing",
            "Daniel Herrmann"
        ],
        "summary": "To learn (statistical) dependencies among random variables requires exponentially large sample size in the number of observed random variables if any arbitrary joint probability distribution can occur.   We consider the case that sparse data strongly suggest that the probabilities can be described by a simple Bayesian network, i.e., by a graph with small in-degree \\Delta. Then this simple law will also explain further data with high confidence. This is shown by calculating bounds on the VC dimension of the set of those probability measures that correspond to simple graphs. This allows to select networks by structural risk minimization and gives reliability bounds on the error of the estimated joint measure without (in contrast to a previous paper) any prior assumptions on the set of possible joint measures.   The complexity for searching the optimal Bayesian networks of in-degree \\Delta increases only polynomially in the number of random varibales for constant \\Delta and the optimal joint measure associated with a given graph can be found by convex optimization.",
        "published": "2003-09-10T13:56:41Z",
        "link": "http://arxiv.org/abs/cs/0309015v1",
        "categories": [
            "cs.LG",
            "K.3.2"
        ]
    },
    {
        "title": "Using Simulated Annealing to Calculate the Trembles of Trembling Hand   Perfection",
        "authors": [
            "Stuart McDonald",
            "Liam Wagner"
        ],
        "summary": "Within the literature on non-cooperative game theory, there have been a number of attempts to propose logorithms which will compute Nash equilibria. Rather than derive a new algorithm, this paper shows that the family of algorithms known as Markov chain Monte Carlo (MCMC) can be used to calculate Nash equilibria. MCMC is a type of Monte Carlo simulation that relies on Markov chains to ensure its regularity conditions. MCMC has been widely used throughout the statistics and optimization literature, where variants of this algorithm are known as simulated annealing. This paper shows that there is interesting connection between the trembles that underlie the functioning of this algorithm and the type of Nash refinement known as trembling hand perfection.",
        "published": "2003-09-10T15:11:44Z",
        "link": "http://arxiv.org/abs/cs/0309016v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.NE",
            "q-bio.PE",
            "F.1.1;F.2.2;G.3;I.2.1;J.4"
        ]
    },
    {
        "title": "Measuring Praise and Criticism: Inference of Semantic Orientation from   Association",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., \"honest\", \"intrepid\") and negative semantic orientation indicates criticism (e.g., \"disturbing\", \"superfluous\"). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots). This paper introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words. Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA). The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words). The method attains an accuracy of 82.8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words.",
        "published": "2003-09-19T16:30:55Z",
        "link": "http://arxiv.org/abs/cs/0309034v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Combining Independent Modules to Solve Multiple-choice Synonym and   Analogy Problems",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman",
            "Jeffrey Bigham",
            "Victor Shnayder"
        ],
        "summary": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the well known mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",
        "published": "2003-09-19T20:13:07Z",
        "link": "http://arxiv.org/abs/cs/0309035v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "I.2.6; I.2.7; H.3.1; J.5"
        ]
    },
    {
        "title": "Optimality of Universal Bayesian Sequence Prediction for General Loss   and Alphabet",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Various optimality properties of universal sequence predictors based on Bayes-mixtures in general, and Solomonoff's prediction scheme in particular, will be studied. The probability of observing $x_t$ at time $t$, given past observations $x_1...x_{t-1}$ can be computed with the chain rule if the true generating distribution $\\mu$ of the sequences $x_1x_2x_3...$ is known. If $\\mu$ is unknown, but known to belong to a countable or continuous class $\\M$ one can base ones prediction on the Bayes-mixture $\\xi$ defined as a $w_\\nu$-weighted sum or integral of distributions $\\nu\\in\\M$. The cumulative expected loss of the Bayes-optimal universal prediction scheme based on $\\xi$ is shown to be close to the loss of the Bayes-optimal, but infeasible prediction scheme based on $\\mu$. We show that the bounds are tight and that no other predictor can lead to significantly smaller bounds. Furthermore, for various performance measures, we show Pareto-optimality of $\\xi$ and give an Occam's razor argument that the choice $w_\\nu\\sim 2^{-K(\\nu)}$ for the weights is optimal, where $K(\\nu)$ is the length of the shortest program describing $\\nu$. The results are applied to games of chance, defined as a sequence of bets, observations, and rewards. The prediction schemes (and bounds) are compared to the popular predictors based on expert advice. Extensions to infinite alphabets, partial, delayed and probabilistic prediction, classification, and more active systems are briefly discussed.",
        "published": "2003-11-13T12:02:04Z",
        "link": "http://arxiv.org/abs/cs/0311014v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.PR",
            "E.4;I.2.6;G.3"
        ]
    },
    {
        "title": "Toward Attribute Efficient Learning Algorithms",
        "authors": [
            "Adam R. Klivans",
            "Rocco A. Servedio"
        ],
        "summary": "We make progress on two important problems regarding attribute efficient learnability.   First, we give an algorithm for learning decision lists of length $k$ over $n$ variables using $2^{\\tilde{O}(k^{1/3})} \\log n$ examples and time $n^{\\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives an essentially optimal tradeoff between polynomial threshold function degree and weight.   Second, we give an algorithm for learning an unknown parity function on $k$ out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$. For $k=o(\\log n)$ this yields a polynomial time algorithm with sample complexity $o(n)$. This is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity.",
        "published": "2003-11-27T05:34:04Z",
        "link": "http://arxiv.org/abs/cs/0311042v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Hybrid LQG-Neural Controller for Inverted Pendulum System",
        "authors": [
            "E. S. Sazonov",
            "P. Klinkhachorn",
            "R. L. Klein"
        ],
        "summary": "The paper presents a hybrid system controller, incorporating a neural and an LQG controller. The neural controller has been optimized by genetic algorithms directly on the inverted pendulum system. The failure free optimization process stipulated a relatively small region of the asymptotic stability of the neural controller, which is concentrated around the regulation point. The presented hybrid controller combines benefits of a genetically optimized neural controller and an LQG controller in a single system controller. High quality of the regulation process is achieved through utilization of the neural controller, while stability of the system during transient processes and a wide range of operation are assured through application of the LQG controller. The hybrid controller has been validated by applying it to a simulation model of an inherently unstable system of inverted pendulum.",
        "published": "2003-11-30T00:19:19Z",
        "link": "http://arxiv.org/abs/cs/0312003v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6;C.1.3;I.5.1"
        ]
    },
    {
        "title": "Improving spam filtering by combining Naive Bayes with simple k-nearest   neighbor searches",
        "authors": [
            "Daniel Etzold"
        ],
        "summary": "Using naive Bayes for email classification has become very popular within the last few months. They are quite easy to implement and very efficient. In this paper we want to present empirical results of email classification using a combination of naive Bayes and k-nearest neighbor searches. Using this technique we show that the accuracy of a Bayes filter can be improved slightly for a high number of features and significantly for a small number of features.",
        "published": "2003-11-30T20:41:18Z",
        "link": "http://arxiv.org/abs/cs/0312004v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Failure-Free Genetic Algorithm Optimization of a System Controller Using   SAFE/LEARNING Controllers in Tandem",
        "authors": [
            "E. S. Sazonov",
            "D. Del Gobbo",
            "P. Klinkhachorn",
            "R. L. Klein"
        ],
        "summary": "The paper presents a method for failure free genetic algorithm optimization of a system controller. Genetic algorithms present a powerful tool that facilitates producing near-optimal system controllers. Applied to such methods of computational intelligence as neural networks or fuzzy logic, these methods are capable of combining the non-linear mapping capabilities of the latter with learning the system behavior directly, that is, without a prior model. At the same time, genetic algorithms routinely produce solutions that lead to the failure of the controlled system. Such solutions are generally unacceptable for applications where safe operation must be guaranteed. We present here a method of design, which allows failure-free application of genetic algorithms through utilization of SAFE and LEARNING controllers in tandem, where the SAFE controller recovers the system from dangerous states while the LEARNING controller learns its behavior. The method has been validated by applying it to an inherently unstable system of inverted pendulum.",
        "published": "2003-12-03T22:29:01Z",
        "link": "http://arxiv.org/abs/cs/0312009v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6;C.1.3;I.5.1"
        ]
    },
    {
        "title": "Mapping Subsets of Scholarly Information",
        "authors": [
            "Paul Ginsparg",
            "Paul Houle",
            "Thorsten Joachims",
            "Jae-Hoon Sul"
        ],
        "summary": "We illustrate the use of machine learning techniques to analyze, structure, maintain, and evolve a large online corpus of academic literature. An emerging field of research can be identified as part of an existing corpus, permitting the implementation of a more coherent community structure for its practitioners.",
        "published": "2003-12-11T20:07:39Z",
        "link": "http://arxiv.org/abs/cs/0312018v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.6; I.2.6"
        ]
    },
    {
        "title": "Acquiring Lexical Paraphrases from a Single Corpus",
        "authors": [
            "Oren Glickman",
            "Ido Dagan"
        ],
        "summary": "This paper studies the potential of identifying lexical paraphrases within a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information. A comparison suggests that an instance-based approach may be combined with a vector based approach in order to assess better the paraphrase likelihood for many verb pairs.",
        "published": "2003-12-25T16:45:20Z",
        "link": "http://arxiv.org/abs/cs/0312058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "I.7"
        ]
    },
    {
        "title": "Part-of-Speech Tagging with Minimal Lexicalization",
        "authors": [
            "Virginia Savova",
            "Leonid Peshkin"
        ],
        "summary": "We use a Dynamic Bayesian Network to represent compactly a variety of sublexical and contextual features relevant to Part-of-Speech (PoS) tagging. The outcome is a flexible tagger (LegoTag) with state-of-the-art performance (3.6% error on a benchmark corpus). We explore the effect of eliminating redundancy and radically reducing the size of feature vocabularies. We find that a small but linguistically motivated set of suffixes results in improved cross-corpora generalization. We also show that a minimal lexicon limited to function words is sufficient to ensure reasonable performance.",
        "published": "2003-12-27T21:21:48Z",
        "link": "http://arxiv.org/abs/cs/0312060v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.7"
        ]
    },
    {
        "title": "Near Quadratic Matrix Multiplication Modulo Composites",
        "authors": [
            "Vince Grolmusz"
        ],
        "summary": "We show how one can use non-prime-power, composite moduli for computing representations of the product of two $n\\times n$ matrices using only $n^{2+o(1)}$ multiplications.",
        "published": "2003-01-08T14:35:25Z",
        "link": "http://arxiv.org/abs/cs/0301004v3",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; F.2.1; G.1.3"
        ]
    },
    {
        "title": "Hard satisfiable formulas for DPLL-type algorithms",
        "authors": [
            "Sergey I. Nikolenko"
        ],
        "summary": "We address lower bounds on the time complexity of algorithms solving the propositional satisfiability problem. Namely, we consider two DPLL-type algorithms, enhanced with the unit clause and pure literal heuristics. Exponential lower bounds for solving satisfiability on provably satisfiable formulas are proven.",
        "published": "2003-01-15T08:44:21Z",
        "link": "http://arxiv.org/abs/cs/0301012v1",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Solving satisfiability problems by fluctuations: The dynamics of   stochastic local search algorithms",
        "authors": [
            "Wolfgang Barthel",
            "Alexander K. Hartmann",
            "Martin Weigt"
        ],
        "summary": "Stochastic local search algorithms are frequently used to numerically solve hard combinatorial optimization or decision problems. We give numerical and approximate analytical descriptions of the dynamics of such algorithms applied to random satisfiability problems. We find two different dynamical regimes, depending on the number of constraints per variable: For low constraintness, the problems are solved efficiently, i.e. in linear time. For higher constraintness, the solution times become exponential. We observe that the dynamical behavior is characterized by a fast equilibration and fluctuations around this equilibrium. If the algorithm runs long enough, an exponentially rare fluctuation towards a solution appears.",
        "published": "2003-01-15T16:24:14Z",
        "link": "http://arxiv.org/abs/cond-mat/0301271v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Relaxation and Metastability in the RandomWalkSAT search procedure",
        "authors": [
            "Guilhem Semerjian",
            "Remi Monasson"
        ],
        "summary": "An analysis of the average properties of a local search resolution procedure for the satisfaction of random Boolean constraints is presented. Depending on the ratio alpha of constraints per variable, resolution takes a time T_res growing linearly (T_res \\sim tau(alpha) N, alpha < alpha_d) or exponentially (T_res \\sim exp(N zeta(alpha)), alpha > alpha_d) with the size N of the instance. The relaxation time tau(alpha) in the linear phase is calculated through a systematic expansion scheme based on a quantum formulation of the evolution operator. For alpha > alpha_d, the system is trapped in some metastable state, and resolution occurs from escape from this state through crossing of a large barrier. An annealed calculation of the height zeta(alpha) of this barrier is proposed. The polynomial/exponentiel cross-over alpha_d is not related to the onset of clustering among solutions.",
        "published": "2003-01-15T16:27:11Z",
        "link": "http://arxiv.org/abs/cond-mat/0301272v2",
        "categories": [
            "cond-mat",
            "cs.CC"
        ]
    },
    {
        "title": "Independence Properties of Algorithmically Random Sequences",
        "authors": [
            "S. M. Kautz"
        ],
        "summary": "A bounded Kolmogorov-Loveland selection rule is an adaptive strategy for recursively selecting a subsequence of an infinite binary sequence; such a subsequence may be interpreted as the query sequence of a time-bounded Turing machine. In this paper we show that if A is an algorithmically random sequence, A_0 is selected from A via a bounded Kolmogorov-Loveland selection rule, and A_1 denotes the sequence of nonselected bits of A, then A_1 is independent of A_0; that is, A_1 is algorithmically random relative to A_0. This result has been used by Kautz and Miltersen [1] to show that relative to a random oracle, NP does not have p-measure zero (in the sense of Lutz [2]).   [1] S. M. Kautz and P. B. Miltersen. Relative to a random oracle, NP is not small. Journal of Computer and System Sciences, 53:235-250, 1996.   [2] J. H. Lutz. Almost everywhere high nonuniform complexity. Journal of Computer and System Sciences, 44:220-258, 1992.",
        "published": "2003-01-16T02:56:59Z",
        "link": "http://arxiv.org/abs/cs/0301013v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Some remarks on the survey decimation algorithm for K-satisfiability",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In this note we study the convergence of the survey decimation algorithm. An analytic formula for the reduction of the complexity during the decimation is derived. The limit of the converge of the algorithm are estimated in the random case: interesting phenomena appear near the boundary of convergence.",
        "published": "2003-01-16T10:38:36Z",
        "link": "http://arxiv.org/abs/cs/0301015v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DS",
            "G.3, G.2.1"
        ]
    },
    {
        "title": "Lower Bounds on the Bounded Coefficient Complexity of Bilinear Maps",
        "authors": [
            "Peter Buergisser",
            "Martin Lotz"
        ],
        "summary": "We prove lower bounds of order $n\\log n$ for both the problem to multiply polynomials of degree $n$, and to divide polynomials with remainder, in the model of bounded coefficient arithmetic circuits over the complex numbers. These lower bounds are optimal up to order of magnitude. The proof uses a recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix multiplication. It reduces the linear problem to multiply a random circulant matrix with a vector to the bilinear problem of cyclic convolution. We treat the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp. 305-306, 1973] in a unitarily invariant way. This establishes a new lower bound on the bounded coefficient complexity of linear forms in terms of the singular values of the corresponding matrix. In addition, we extend these lower bounds for linear and bilinear maps to a model of circuits that allows a restricted number of unbounded scalar multiplications.",
        "published": "2003-01-16T15:36:00Z",
        "link": "http://arxiv.org/abs/cs/0301016v1",
        "categories": [
            "cs.CC",
            "F1.1; F2.1; I.1.2"
        ]
    },
    {
        "title": "Complexity and Completeness of Immanants",
        "authors": [
            "Jean-Luc Brylinski",
            "Ranee Brylinski"
        ],
        "summary": "Immanants are polynomial functions of n by n matrices attached to irreducible characters of the symmetric group S_n, or equivalently to Young diagrams of size n. Immanants include determinants and permanents as extreme cases. Valiant proved that computation of permanents is a complete problem in his algebraic model of NP theory, i.e., it is VNP-complete. We prove that computation of immanants is VNP-complete if the immanants are attached to a family of diagrams whose separation is $\\Omega(n^\\delta)$ for some $\\delta>0$. We define the separation of a diagram to be the largest number of overhanging boxes contained in a single row. Our theorem proves a conjecture of Buergisser for a large variety of families, and in particular we recover with new proofs his VNP-completeness results for hooks and rectangles.",
        "published": "2003-01-23T20:31:46Z",
        "link": "http://arxiv.org/abs/cs/0301024v2",
        "categories": [
            "cs.CC",
            "math.CO",
            "F.1.3 ; F.2.3"
        ]
    },
    {
        "title": "On the existence of a new family of Diophantine equations for $\\bf   Ω$",
        "authors": [
            "Toby Ord",
            "Tien D. Kieu"
        ],
        "summary": "We show how to determine the $k$-th bit of Chaitin's algorithmically random real number $\\Omega$ by solving $k$ instances of the halting problem. From this we then reduce the problem of determining the $k$-th bit of $\\Omega$ to determining whether a certain Diophantine equation with two parameters, $k$ and $N$, has solutions for an odd or an even number of values of $N$. We also demonstrate two further examples of $\\Omega$ in number theory: an exponential Diophantine equation with a parameter $k$ which has an odd number of solutions iff the $k$-th bit of $\\Omega$ is 1, and a polynomial of positive integer variables and a parameter $k$ that takes on an odd number of positive values iff the $k$-th bit of $\\Omega$ is 1.",
        "published": "2003-01-24T06:27:50Z",
        "link": "http://arxiv.org/abs/math/0301274v3",
        "categories": [
            "math.NT",
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "Many Hard Examples in Exact Phase Transitions with Application to   Generating Hard Satisfiable Instances",
        "authors": [
            "Ke Xu",
            "Wei Li"
        ],
        "summary": "This paper first analyzes the resolution complexity of two random CSP models (i.e. Model RB/RD) for which we can establish the existence of phase transitions and identify the threshold points exactly. By encoding CSPs into CNF formulas, it is proved that almost all instances of Model RB/RD have no tree-like resolution proofs of less than exponential size. Thus, we not only introduce new families of CNF formulas hard for resolution, which is a central task of Proof-Complexity theory, but also propose models with both many hard instances and exact phase transitions. Then, the implications of such models are addressed. It is shown both theoretically and experimentally that an application of Model RB/RD might be in the generation of hard satisfiable instances, which is not only of practical importance but also related to some open problems in cryptography such as generating one-way functions. Subsequently, a further theoretical support for the generation method is shown by establishing exponential lower bounds on the complexity of solving random satisfiable and forced satisfiable instances of RB/RD near the threshold. Finally, conclusions are presented, as well as a detailed comparison of Model RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively, exhibit three different kinds of phase transition behavior in NP-complete problems.",
        "published": "2003-02-01T15:58:16Z",
        "link": "http://arxiv.org/abs/cs/0302001v5",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.DM",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Approximate analysis of search algorithms with \"physical\" methods",
        "authors": [
            "Simona Cocco",
            "Remi Monasson",
            "Andrea Montanari",
            "Guilhem Semerjian"
        ],
        "summary": "An overview of some methods of statistical physics applied to the analysis of algorithms for optimization problems (satisfiability of Boolean constraints, vertex cover of graphs, decoding, ...) with distributions of random inputs is proposed. Two types of algorithms are analyzed: complete procedures with backtracking (Davis-Putnam-Loveland-Logeman algorithm) and incomplete, local search procedures (gradient descent, random walksat, ...). The study of complete algorithms makes use of physical concepts such as phase transitions, dynamical renormalization flow, growth processes, ... As for local search procedures, the connection between computational complexity and the structure of the cost function landscape is questioned, with emphasis on the notion of metastability.",
        "published": "2003-02-03T11:04:24Z",
        "link": "http://arxiv.org/abs/cs/0302003v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "F.2.2"
        ]
    },
    {
        "title": "3-Local Hamiltonian is QMA-complete",
        "authors": [
            "Julia Kempe",
            "Oded Regev"
        ],
        "summary": "It has been shown by Kitaev that the 5-local Hamiltonian problem is QMA-complete. Here we reduce the locality of the problem by showing that 3-local Hamiltonian is already QMA-complete.",
        "published": "2003-02-10T23:47:12Z",
        "link": "http://arxiv.org/abs/quant-ph/0302079v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Cryptographic Randomized Response Techniques",
        "authors": [
            "Andris Ambainis",
            "Markus Jakobsson",
            "Helger Lipmaa"
        ],
        "summary": "We develop cryptographically secure techniques to guarantee unconditional privacy for respondents to polls. Our constructions are efficient and practical, and are shown not to allow cheating respondents to affect the ``tally'' by more than their own vote -- which will be given the exact same weight as that of other respondents. We demonstrate solutions to this problem based on both traditional cryptographic techniques and quantum cryptography.",
        "published": "2003-02-18T11:23:23Z",
        "link": "http://arxiv.org/abs/cs/0302025v2",
        "categories": [
            "cs.CC",
            "cs.CR",
            "cs.CY",
            "quant-ph",
            "D.4.6"
        ]
    },
    {
        "title": "The Boolean Functions Computed by Random Boolean Formulas OR How to Grow   the Right Function",
        "authors": [
            "Alex Brodsky",
            "Nicholas Pippenger"
        ],
        "summary": "Among their many uses, growth processes (probabilistic amplification), were used for constructing reliable networks from unreliable components, and deriving complexity bounds of various classes of functions. Hence, determining the initial conditions for such processes is an important and challenging problem. In this paper we characterize growth processes by their initial conditions and derive conditions under which results such as Valiant's (1984) hold. First, we completely characterize growth processes that use linear connectives. Second, by extending Savick\\'y's (1990) analysis, via ``Restriction Lemmas'', we characterize growth processes that use monotone connectives, and show that our technique is applicable to growth processes that use other connectives as well. Additionally, we obtain explicit bounds on the convergence rates of several growth processes, including the growth process studied by Savick\\'y (1990).",
        "published": "2003-02-19T21:57:06Z",
        "link": "http://arxiv.org/abs/cs/0302028v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "F.1.1;F.1.2;G.2.1;G.3"
        ]
    },
    {
        "title": "Phase Diagram for the Constrained Integer Partitioning Problem",
        "authors": [
            "C. Borgs",
            "J. T. Chayes",
            "S. Mertens",
            "B. Pittel"
        ],
        "summary": "We consider the problem of partitioning $n$ integers into two subsets of given cardinalities such that the discrepancy, the absolute value of the difference of their sums, is minimized. The integers are i.i.d. random variables chosen uniformly from the set $\\{1,...,M\\}$. We study how the typical behavior of the optimal partition depends on $n,M$ and the bias $s$, the difference between the cardinalities of the two subsets in the partition. In particular, we rigorously establish this typical behavior as a function of the two parameters $\\kappa:=n^{-1}\\log_2M$ and $b:=|s|/n$ by proving the existence of three distinct ``phases'' in the $\\kappa b$-plane, characterized by the value of the discrepancy and the number of optimal solutions: a ``perfect phase'' with exponentially many optimal solutions with discrepancy 0 or 1; a ``hard phase'' with minimal discrepancy of order $Me^{-\\Theta(n)}$; and a ``sorted phase'' with an unique optimal partition of order $Mn$, obtained by putting the $(s+n)/2$ smallest integers in one subset. Our phase diagram covers all but a relatively small region in the $\\kappa b$-plane. We also show that the three phases can be alternatively characterized by the number of basis solutions of the associated linear programming problem, and by the fraction of these basis solutions whose $\\pm 1$-valued components form optimal integer partitions of the subproblem with the corresponding weights. We show in particular that this fraction is one in the sorted phase, and exponentially small in both the perfect and hard phases, and strictly exponentially smaller in the hard phase than in the perfect phase. Open problems are discussed, and numerical experiments are presented.",
        "published": "2003-02-26T09:05:18Z",
        "link": "http://arxiv.org/abs/cond-mat/0302536v1",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "A first approach for a possible cellular automaton model of fluids   dynamics",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper I present a first attempt for a possible description of fluids dynamics by mean of a cellular automata technique. With the use of simple and elementary rules, based on random behaviour either, the model permits to obtain the evolution in time for a two-dimensional grid, where one molecule of the material fluid can ideally place itself on a single geometric square. By mean of computational simulations, some realistic effects, here showed by use of digital pictures, have been obtained. In a subsequent step of this work I think to use a parallel program for a high performances computational simulation, for increasing the degree of realism of the digital rendering by mean of a three-dimensional grid too. For the execution of the simulations, numerical methods of resolution for differential equations have not been used.",
        "published": "2003-03-06T00:02:11Z",
        "link": "http://arxiv.org/abs/cs/0303003v1",
        "categories": [
            "cs.CC",
            "cs.DC",
            "nlin.CG",
            "physics.comp-ph",
            "F.1.1"
        ]
    },
    {
        "title": "Algorithmic Chaos",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "Many physical theories like chaos theory are fundamentally concerned with the conceptual tension between determinism and randomness. Kolmogorov complexity can express randomness in determinism and gives an approach to formulate chaotic behavior.",
        "published": "2003-03-07T18:37:26Z",
        "link": "http://arxiv.org/abs/nlin/0303016v1",
        "categories": [
            "nlin.CD",
            "cs.CC",
            "math.DS"
        ]
    },
    {
        "title": "When Can we Call a System Self-organizing?",
        "authors": [
            "Carlos Gershenson",
            "Francis Heylighen"
        ],
        "summary": "We do not attempt to provide yet another definition of selforganization, but explore the conditions under which we can model a system as self-organizing. These involve the dynamics of entropy, and the purpose, aspects, and description level chosen by an observer. We show how, changing the level or \"graining\" of description, the same system can appear selforganizing or self-disorganizing. We discuss ontological issues we face when studying self-organizing systems, and analyse when designing and controlling artificial self-organizing systems is useful. We conclude that self-organization is a way of observing systems, not an absolute class of systems.",
        "published": "2003-03-10T18:24:04Z",
        "link": "http://arxiv.org/abs/nlin/0303020v2",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Contextual Random Boolean Networks",
        "authors": [
            "Carlos Gershenson",
            "Jan Broekaert",
            "Diederik Aerts"
        ],
        "summary": "We propose the use of Deterministic Generalized Asynchronous Random Boolean Networks [Gershenson, 2002] as models of contextual deterministic discrete dynamical systems. We show that changes in the context have drastic effects on the global properties of the same networks, namely the average number of attractors and the average percentage of states in attractors. We introduce the situation where we lack knowledge on the context as a more realistic model for contextual dynamical systems. We notice that this makes the network non-deterministic in a specific way, namely introducing a non-Kolmogorovian quantum-like structure for the modelling of the network [Aerts, 1986]. In this case, for example, a state of the network has the potentiality (probability) of collapsing into different attractors, depending on the specific form of lack of knowledge on the context.",
        "published": "2003-03-10T18:42:57Z",
        "link": "http://arxiv.org/abs/nlin/0303021v2",
        "categories": [
            "nlin.AO",
            "cond-mat.dis-nn",
            "cs.CC",
            "nlin.CG"
        ]
    },
    {
        "title": "Solution of the Linear Ordering Problem (NP=P)",
        "authors": [
            "Givi Bolotashvili"
        ],
        "summary": "A polynomial algorithm is obtained for the NP-complete linear ordering problem.",
        "published": "2003-03-14T13:53:21Z",
        "link": "http://arxiv.org/abs/cs/0303008v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.0"
        ]
    },
    {
        "title": "Generic properties of Whitehead's Algorithm and isomorphism rigidity of   random one-relator groups",
        "authors": [
            "Ilya Kapovich",
            "Paul Schupp",
            "Vladimir Shpilrain"
        ],
        "summary": "We prove that Whitehead's algorithm for solving the automorphism problem in a fixed free group $F_k$ has strongly linear time generic-case complexity. This is done by showing that the ``hard'' part of the algorithm terminates in linear time on an exponentially generic set of input pairs. We then apply these results to one-relator groups. We obtain a Mostow-type isomorphism rigidity result for random one-relator groups: If two such groups are isomorphic then their Cayley graphs on the \\emph{given generating sets} are isometric. Although no nontrivial examples were previously known, we prove that one-relator groups are generically \\emph{complete} groups, that is, they have trivial center and trivial outer automorphism group. We also prove that the stabilizers of generic elements of $F_k$ in $Aut(F_k)$ are cyclic groups generated by inner automorphisms and that $Aut(F_k)$-orbits are uniformly small in the sense of their growth entropy. We further prove that the number $I_k(n)$ of \\emph{isomorphism types} of $k$-generator one-relator groups with defining relators of length $n$ satisfies \\[ \\frac{c_1}{n} (2k-1)^n \\le I_k(n)\\le \\frac{c_2}{n} (2k-1)^n, \\] where $c_1=c_1(k)>0, c_2=c_2(k)>0$ are some constants independent of $n$. Thus $I_k(n)$ grows in essentially the same manner as the number of cyclic words of length $n$.",
        "published": "2003-03-31T13:59:44Z",
        "link": "http://arxiv.org/abs/math/0303386v4",
        "categories": [
            "math.GR",
            "cs.CC",
            "math.GT",
            "20F36"
        ]
    },
    {
        "title": "A Physics-Free Introduction to the Quantum Computation Model",
        "authors": [
            "Stephen A. Fenner"
        ],
        "summary": "This article defines and proves basic properties of the standard quantum circuit model of computation. The model is developed abstractly in close analogy with (classical) deterministic and probabilistic circuits, without recourse to any physical concepts or principles. It is intended as a primer for theoretical computer scientists who do not know--and perhaps do not care to know--any physics.",
        "published": "2003-04-04T20:01:08Z",
        "link": "http://arxiv.org/abs/cs/0304008v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.1; F.1.2; F.1.3"
        ]
    },
    {
        "title": "Quantum Search on Bounded-Error Inputs",
        "authors": [
            "Peter Hoyer",
            "Michele Mosca",
            "Ronald de Wolf"
        ],
        "summary": "Suppose we have n algorithms, quantum or classical, each computing some bit-value with bounded error probability. We describe a quantum algorithm that uses O(sqrt{n}) repetitions of the base algorithms and with high probability finds the index of a 1-bit among these n bits (if there is such an index). This shows that it is not necessary to first significantly reduce the error probability in the base algorithms to O(1/poly(n)) (which would require O(sqrt{n}log n) repetitions in total). Our technique is a recursive interleaving of amplitude amplification and error-reduction, and may be of more general interest. Essentially, it shows that quantum amplitude amplification can be made to work also with a bounded-error verifier. As a corollary we obtain optimal quantum upper bounds of O(sqrt{N}) queries for all constant-depth AND-OR trees on N variables, improving upon earlier upper bounds of O(sqrt{N}polylog(N)).",
        "published": "2003-04-07T16:11:22Z",
        "link": "http://arxiv.org/abs/quant-ph/0304052v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A Direct Ultrametric Approach to Additive Complexity and the Shub-Smale   Tau Conjecture",
        "authors": [
            "J. Maurice Rojas"
        ],
        "summary": "The Shub-Smale Tau Conjecture is a hypothesis relating the number of integral roots of a polynomial f in one variable and the Straight-Line Program (SLP) complexity of f. A consequence of the truth of this conjecture is that, for the Blum-Shub-Smale model over the complex numbers, P differs from NP. We prove two weak versions of the Tau Conjecture and in so doing show that the Tau Conjecture follows from an even more plausible hypothesis.   Our results follow from a new p-adic analogue of earlier work relating real algebraic geometry to additive complexity. For instance, we can show that a nonzero univariate polynomial of additive complexity s can have no more than 15+s^3(s+1)(7.5)^s s! =O(e^{s\\log s}) roots in the 2-adic rational numbers Q_2, thus dramatically improving an earlier result of the author. This immediately implies the same bound on the number of ordinary rational roots, whereas the best previous upper bound via earlier techniques from real algebraic geometry was a quantity in Omega((22.6)^{s^2}).   This paper presents another step in the author's program of establishing an algorithmic arithmetic version of fewnomial theory.",
        "published": "2003-04-07T23:10:53Z",
        "link": "http://arxiv.org/abs/math/0304100v2",
        "categories": [
            "math.NT",
            "cs.CC"
        ]
    },
    {
        "title": "Individual Communication Complexity",
        "authors": [
            "Harry Buhrman",
            "Hartmut Klauck",
            "Nikolai Vereshchagin",
            "Paul Vitanyi"
        ],
        "summary": "We initiate the theory of communication complexity of individual inputs held by the agents, rather than worst-case or average-case. We consider total, partial, and partially correct protocols, one-way versus two-way, with and without help bits. The results are expressed in trems of Kolmogorov complexity.",
        "published": "2003-04-08T15:41:27Z",
        "link": "http://arxiv.org/abs/cs/0304012v1",
        "categories": [
            "cs.CC",
            "cs.DC",
            "F.1; F.2"
        ]
    },
    {
        "title": "A direct sum theorem in communication complexity via message compression",
        "authors": [
            "Rahul Jain",
            "Jaikumar Radhakrishnan",
            "Pranab Sen"
        ],
        "summary": "We prove lower bounds for the direct sum problem for two-party bounded error randomised multiple-round communication protocols. Our proofs use the notion of information cost of a protocol, as defined by Chakrabarti, Shi, Wirth and Yao and refined further by Bar-Yossef, Jayram, Kumar and Sivakumar. Our main technical result is a `compression' theorem saying that, for any probability distribution $\\mu$ over the inputs, a $k$-round private coin bounded error protocol for a function $f$ with information cost $c$ can be converted into a $k$-round deterministic protocol for $f$ with bounded distributional error and communication cost $O(kc)$. We prove this result using a substate theorem about relative entropy and a rejection sampling argument. Our direct sum result follows from this `compression' result via elementary information theoretic arguments.   We also consider the direct sum problem in quantum communication. Using a probabilistic argument, we show that messages cannot be compressed in this manner even if they carry small information. Hence, new techniques may be necessary to tackle the direct sum problem in quantum communication.",
        "published": "2003-04-12T02:26:26Z",
        "link": "http://arxiv.org/abs/cs/0304020v2",
        "categories": [
            "cs.CC",
            "F.1.2; E.4"
        ]
    },
    {
        "title": "A New Multilayered PCP and the Hardness of Hypergraph Vertex Cover",
        "authors": [
            "Irit Dinur",
            "Venkatesan Guruswami",
            "Subhash Khot",
            "Oded Regev"
        ],
        "summary": "Given a $k$-uniform hyper-graph, the E$k$-Vertex-Cover problem is to find the smallest subset of vertices that intersects every hyper-edge. We present a new multilayered PCP construction that extends the Raz verifier. This enables us to prove that E$k$-Vertex-Cover is NP-hard to approximate within factor $(k-1-\\epsilon)$ for any $k \\geq 3$ and any $\\epsilon>0$. The result is essentially tight as this problem can be easily approximated within factor $k$. Our construction makes use of the biased Long-Code and is analyzed using combinatorial properties of $s$-wise $t$-intersecting families of subsets.",
        "published": "2003-04-19T17:59:33Z",
        "link": "http://arxiv.org/abs/cs/0304026v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Small Spans in Scaled Dimension",
        "authors": [
            "John M. Hitchcock"
        ],
        "summary": "Juedes and Lutz (1995) proved a small span theorem for polynomial-time many-one reductions in exponential time. This result says that for language A decidable in exponential time, either the class of languages reducible to A (the lower span) or the class of problems to which A can be reduced (the upper span) is small in the sense of resource-bounded measure and, in particular, that the degree of A is small. Small span theorems have been proven for increasingly stronger polynomial-time reductions, and a small span theorem for polynomial-time Turing reductions would imply BPP != EXP. In contrast to the progress in resource-bounded measure, Ambos-Spies, Merkle, Reimann, and Stephan (2001) showed that there is no small span theorem for the resource-bounded dimension of Lutz (2000), even for polynomial-time many-one reductions.   Resource-bounded scaled dimension, recently introduced by Hitchcock, Lutz, and Mayordomo (2003), provides rescalings of resource-bounded dimension. We use scaled dimension to further understand the contrast between measure and dimension regarding polynomial-time spans and degrees. We strengthen prior results by showing that the small span theorem holds for polynomial-time many-one reductions in the -3rd-order scaled dimension, but fails to hold in the -2nd-order scaled dimension. Our results also hold in exponential space.   As an application, we show that determining the -2nd- or -1st-order scaled dimension in ESPACE of the many-one complete languages for E would yield a proof of P = BPP or P != PSPACE. On the other hand, it is shown unconditionally that the complete languages for E have -3rd-order scaled dimension 0 in ESPACE and -2nd- and -1st-order scaled dimension 1 in E.",
        "published": "2003-04-22T19:43:35Z",
        "link": "http://arxiv.org/abs/cs/0304030v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "How NP got a new definition: a survey of probabilistically checkable   proofs",
        "authors": [
            "Sanjeev Arora"
        ],
        "summary": "We survey a collective achievement of a group of researchers: the PCP Theorems. They give new definitions of the class \\np, and imply that computing approximate solutions to many \\np-hard problems is itself \\np-hard. Techniques developed to prove them have had many other consequences.",
        "published": "2003-04-28T22:49:52Z",
        "link": "http://arxiv.org/abs/cs/0304038v1",
        "categories": [
            "cs.CC",
            "68Q10, 68Q15, 68Q17, 68Q25",
            "F.1"
        ]
    },
    {
        "title": "Approximation thresholds for combinatorial optimization problems",
        "authors": [
            "Uriel Feige"
        ],
        "summary": "An NP-hard combinatorial optimization problem $\\Pi$ is said to have an {\\em approximation threshold} if there is some $t$ such that the optimal value of $\\Pi$ can be approximated in polynomial time within a ratio of $t$, and it is NP-hard to approximate it within a ratio better than $t$. We survey some of the known approximation threshold results, and discuss the pattern that emerges from the known results.",
        "published": "2003-04-28T22:50:21Z",
        "link": "http://arxiv.org/abs/cs/0304039v1",
        "categories": [
            "cs.CC",
            "68Q17, 68W25",
            "F.1"
        ]
    },
    {
        "title": "Hardness as randomness: a survey of universal derandomization",
        "authors": [
            "Russell Impagliazzo"
        ],
        "summary": "We survey recent developments in the study of probabilistic complexity classes. While the evidence seems to support the conjecture that probabilism can be deterministically simulated with relatively low overhead, i.e., that $P=BPP$, it also indicates that this may be a difficult question to resolve. In fact, proving that probabilistic algorithms have non-trivial deterministic simulations is basically equivalent to proving circuit lower bounds, either in the algebraic or Boolean models.",
        "published": "2003-04-28T22:50:34Z",
        "link": "http://arxiv.org/abs/cs/0304040v1",
        "categories": [
            "cs.CC",
            "68Q15, 68Q10, 68Q17, 68W20",
            "F.1"
        ]
    },
    {
        "title": "$P \\ne NP$, propositional proof complexity, and resolution lower bounds   for the weak pigeonhole principle",
        "authors": [
            "Ran Raz"
        ],
        "summary": "Recent results established exponential lower bounds for the length of any Resolution proof for the weak pigeonhole principle. More formally, it was proved that any Resolution proof for the weak pigeonhole principle, with $n$ holes and any number of pigeons, is of length $\\Omega(2^{n^{\\epsilon}})$, (for a constant $\\epsilon = 1/3$). One corollary is that certain propositional formulations of the statement $P \\ne NP$ do not have short Resolution proofs. After a short introduction to the problem of $P \\ne NP$ and to the research area of propositional proof complexity, I will discuss the above mentioned lower bounds for the weak pigeonhole principle and the connections to the hardness of proving $P \\ne NP$.",
        "published": "2003-04-28T22:51:57Z",
        "link": "http://arxiv.org/abs/cs/0304041v1",
        "categories": [
            "cs.CC",
            "68Q15, 68Q17, 03F20, 03D15",
            "F.1, F.4"
        ]
    },
    {
        "title": "Hardness of approximating the weight enumerator of a binary linear code",
        "authors": [
            "M. N. Vyalyi"
        ],
        "summary": "We consider the problem of evaluation of the weight enumerator of a binary linear code. We show that the exact evaluation is hard for polynomial hierarchy. More exactly, if WE is an oracle answering the solution of the evaluation problem then P^WE=P^GapP. Also we consider the approximative evaluation of the weight enumerator. In the case of approximation with additive accuracy $2^{\\alpha n}$, $\\alpha$ is constant the problem is hard in the above sense. We also prove that approximate evaluation at a single point $e^{\\pi i/4}$ is hard for $0<\\al<\\al_0\\approx0.88$.",
        "published": "2003-04-30T03:13:23Z",
        "link": "http://arxiv.org/abs/cs/0304044v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Hybrid Rounding Techniques for Knapsack Problems",
        "authors": [
            "Monaldo Mastrolilli",
            "Marcus Hutter"
        ],
        "summary": "We address the classical knapsack problem and a variant in which an upper bound is imposed on the number of items that can be selected. We show that appropriate combinations of rounding techniques yield novel and powerful ways of rounding. As an application of these techniques, we present a linear-storage Polynomial Time Approximation Scheme (PTAS) and a Fully Polynomial Time Approximation Scheme (FPTAS) that compute an approximate solution, of any fixed accuracy, in linear time. This linear complexity bound gives a substantial improvement of the best previously known polynomial bounds.",
        "published": "2003-05-02T20:40:20Z",
        "link": "http://arxiv.org/abs/cs/0305002v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "Polynomial degree vs. quantum query complexity",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "The degree of a polynomial representing (or approximating) a function f is a lower bound for the number of quantum queries needed to compute f. This observation has been a source of many lower bounds on quantum algorithms. It has been an open problem whether this lower bound is tight.   We exhibit a function with polynomial degree M and quantum query complexity \\Omega(M^{1.321...}). This is the first superlinear separation between polynomial degree and quantum query complexity. The lower bound is shown by a new, more general version of quantum adversary method.",
        "published": "2003-05-06T11:53:23Z",
        "link": "http://arxiv.org/abs/quant-ph/0305028v4",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "An In-Place Sorting with O(n log n) Comparisons and O(n) Moves",
        "authors": [
            "Gianni Franceschini",
            "Viliam Geffert"
        ],
        "summary": "We present the first in-place algorithm for sorting an array of size n that performs, in the worst case, at most O(n log n) element comparisons and O(n) element transports.   This solves a long-standing open problem, stated explicitly, e.g., in [J.I. Munro and V. Raman, Sorting with minimum data movement, J. Algorithms, 13, 374-93, 1992], of whether there exists a sorting algorithm that matches the asymptotic lower bounds on all computational resources simultaneously.",
        "published": "2003-05-09T14:56:07Z",
        "link": "http://arxiv.org/abs/cs/0305005v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "A Representation of Changes of Images and its Application for   Developmental Biolology",
        "authors": [
            "Gene Kim",
            "MyungHo Kim"
        ],
        "summary": "In this paper, we consider a series of events observed at spaced time intervals and present a method of representation of the series. To explain an idea, by dealing with a set of gene expression data, which could be obtained from developmental biology, the procedures are sketched with comments in some details. We mean representation by choosing a proper function, which fits well with observed data of a series, and turning its characteristics into numbers, which extract the intrinsic properties of fluctuating data. With help of a machine learning techniques, this method will give a classification of developmental biological data as well as any varying data during a certain period and the classification can be applied for diagnosis of a disease.",
        "published": "2003-05-13T17:48:35Z",
        "link": "http://arxiv.org/abs/cs/0305008v1",
        "categories": [
            "cs.CC",
            "cs.MS",
            "q-bio",
            "I.1.2; H.1.1;I.5.0"
        ]
    },
    {
        "title": "The Threshold for Random k-SAT is 2^k ln2 - O(k)",
        "authors": [
            "Dimitris Achlioptas",
            "Yuval Peres"
        ],
        "summary": "Let F be a random k-SAT formula on n variables, formed by selecting uniformly and independently m = rn out of all possible k-clauses. It is well-known that if r>2^k ln 2, then the formula F is unsatisfiable with probability that tends to 1 as n tends to infinity. We prove that there exists a sequence t_k = O(k) such that if r < 2^k ln 2 - t_k, then the formula F is satisfiable with probability that tends to 1 as n tends to infinity.   Our technique yields an explicit lower bound for the random k-SAT threshold for every k. For k>3 this improves upon all previously known lower bounds. For example, when k=10 our lower bound is 704.94 while the upper bound is 708.94.",
        "published": "2003-05-14T01:30:10Z",
        "link": "http://arxiv.org/abs/cs/0305009v2",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "cs.DM",
            "math.PR",
            "F.2.2"
        ]
    },
    {
        "title": "Polynomial time quantum computation with advice",
        "authors": [
            "Harumichi Nishimura",
            "Tomoyuki Yamakami"
        ],
        "summary": "Advice is supplementary information that enhances the computational power of an underlying computation. This paper focuses on advice that is given in the form of a pure quantum state and examines the influence of such advice on the behaviors of an underlying polynomial-time quantum computation with bounded-error probability.",
        "published": "2003-05-18T20:54:28Z",
        "link": "http://arxiv.org/abs/quant-ph/0305100v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A mathematical definition of \"simplify\"",
        "authors": [
            "Craig Alan Feinstein"
        ],
        "summary": "Even though every mathematician knows intuitively what it means to \"simplify\" a mathematical expression, there is still no universally accepted rigorous mathematical definition of \"simplify\". In this paper, we shall give a simple and plausible definition of \"simplify\" in terms of the computational complexity of integer functions. We shall also use this definition to show that there is no deterministic and exact algorithm which can compute the permanent of an $n \\times n$ matrix in $o(2^n)$ time.",
        "published": "2003-05-19T16:02:54Z",
        "link": "http://arxiv.org/abs/cs/0305035v27",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Fast n-point correlation functions and three-point lensing application",
        "authors": [
            "Lucy Liuxuan Zhang",
            "Ue-Li Pen"
        ],
        "summary": "We present a new algorithm to rapidly compute the two-point (2PCF), three-point (3PCF) and n-point (n-PCF) correlation functions in roughly O(N log N) time for N particles, instead of O(N^n) as required by brute force approaches. The algorithm enables an estimate of the full 3PCF for as many as 10^6 galaxies. This technique exploits node-to-node correlations of a recursive bisectional binary tree. A balanced tree construction minimizes the depth of the tree and the worst case error at each node. The algorithm presented in this paper can be applied to problems with arbitrary geometry.   We describe the detailed implementation to compute the two point function and all eight components of the 3PCF for a two-component field, with attention to shear fields generated by gravitational lensing. We also generalize the algorithm to compute the n-point correlation function for a scalar field in k dimensions where n and k are arbitrary positive integers.",
        "published": "2003-05-23T05:01:28Z",
        "link": "http://arxiv.org/abs/astro-ph/0305447v5",
        "categories": [
            "astro-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "2D Electrophoresis Gel Image and Diagnosis of a Disease",
        "authors": [
            "Gene Kim",
            "MyungHo Kim"
        ],
        "summary": "The process of diagnosing a disease from the 2D gel electrophoresis image is a challenging problem. This is due to technical difficulties of generating reproducible images with a normalized form and the effect of negative stain. In this paper, we will discuss a new concept of interpreting the 2D images and overcoming the aforementioned technical difficulties using mathematical transformation. The method makes use of 2D gel images of proteins in serums and we explain a way of representing the images into vectors in order to apply machine-learning methods, such as the support vector machine.",
        "published": "2003-05-28T13:48:34Z",
        "link": "http://arxiv.org/abs/cs/0305048v1",
        "categories": [
            "cs.CC",
            "cs.CV",
            "q-bio.QM",
            "I.5; J.3; I.4.1; I.4.3"
        ]
    },
    {
        "title": "On the Existence and Convergence Computable Universal Priors",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff unified Occam's razor and Epicurus' principle of multiple explanations to one elegant, formal, universal theory of inductive inference, which initiated the field of algorithmic information theory. His central result is that the posterior of his universal semimeasure M converges rapidly to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal predictor in case of unknown mu. We investigate the existence and convergence of computable universal (semi)measures for a hierarchy of computability classes: finitely computable, estimable, enumerable, and approximable. For instance, M is known to be enumerable, but not finitely computable, and to dominate all enumerable semimeasures. We define seven classes of (semi)measures based on these four computability concepts. Each class may or may not contain a (semi)measure which dominates all elements of another class. The analysis of these 49 cases can be reduced to four basic cases, two of them being new. The results hold for discrete and continuous semimeasures. We also investigate more closely the types of convergence, possibly implied by universality: in difference and in ratio, with probability 1, in mean sum, and for Martin-Loef random sequences. We introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues.",
        "published": "2003-05-29T11:11:01Z",
        "link": "http://arxiv.org/abs/cs/0305052v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "math.ST",
            "stat.TH",
            "G.3; I.2"
        ]
    },
    {
        "title": "Polynomial Degree and Lower Bounds in Quantum Complexity: Collision and   Element Distinctness with Small Range",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We give a general method for proving quantum lower bounds for problems with small range. Namely, we show that, for any symmetric problem defined on functions $f:\\{1, ..., N\\}\\to\\{1, ..., M\\}$, its polynomial degree is the same for all $M\\geq N$. Therefore, if we have a quantum lower bound for some (possibly, quite large) range $M$ which is shown using polynomials method, we immediately get the same lower bound for all ranges $M\\geq N$. In particular, we get $\\Omega(N^{1/3})$ and $\\Omega(N^{2/3})$ quantum lower bounds for collision and element distinctness with small range.",
        "published": "2003-05-29T16:20:35Z",
        "link": "http://arxiv.org/abs/quant-ph/0305179v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Permutation Generation: Two New Permutation Algorithms",
        "authors": [
            "Jie Gao",
            "Dianjun Wang"
        ],
        "summary": "Two completely new algorithms for generating permutations, shift-cursor algorithm and level algorithm, and their efficient implementations are presented in this paper. One implementation of the shift cursor algorithm gives an optimal solution of the permutation generation problem, and one implementation of the level algorithm can be used to generate random permutations.",
        "published": "2003-06-05T11:49:22Z",
        "link": "http://arxiv.org/abs/cs/0306025v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "A Transformational Decision Procedure for Non-Clausal Propositional   Formulas",
        "authors": [
            "Alexander Sakharov"
        ],
        "summary": "A decision procedure for detecting valid propositional formulas is presented. It is based on the Davis-Putnam method and deals with propositional formulas that are initially converted to negational normal form. This procedure splits variables but, in contrast to other decision procedures based on the Davis-Putnam method, it does not branch. Instead, this procedure iteratively makes validity-preserving transformations of fragments of the formula. The transformations involve only a minimal formula part containing occurrences of the selected variable. Selection of the best variable for splitting is crucial in this decision procedure - it may shorten the decision process dramatically. A variable whose splitting leads to a minimal size of the transformed formula is selected. Also, the decision procedure performs plenty of optimizations based on calculation of delta-sets. Some optimizations lead to removing fragments of the formula. Others detect variables for which a single truth value assignment is sufficient. The latest information about this research can be found at http://www.sakharov.net/valid.html",
        "published": "2003-06-07T16:13:19Z",
        "link": "http://arxiv.org/abs/cs/0306035v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Hyperdense Coding Modulo 6 with Filter-Machines",
        "authors": [
            "Vince Grolmusz"
        ],
        "summary": "We show how one can encode $n$ bits with $n^{o(1)}$ ``wave-bits'' using still hypothetical filter-machines (here $o(1)$ denotes a positive quantity which goes to 0 as $n$ goes to infity). Our present result - in a completely different computational model - significantly improves on the quantum superdense-coding breakthrough of Bennet and Wiesner (1992) which encoded $n$ bits by $\\lceil{n/2}\\rceil$ quantum-bits. We also show that our earlier algorithm (Tech. Rep. TR03-001, ECCC, See ftp://ftp.eccc.uni-trier.de/pub/eccc/reports/2003/TR03-001/index.html) which used $n^{o(1)}$ muliplication for computing a representation of the dot-product of two $n$-bit sequences modulo 6, and, similarly, an algorithm for computing a representation of the multiplication of two $n\\times n$ matrices with $n^{2+o(1)}$ multiplications can be turned to algorithms computing the exact dot-product or the exact matrix-product with the same number of multiplications with filter-machines. With classical computation, computing the dot-product needs $\\Omega(n)$ multiplications and the best known algorithm for matrix multiplication (D. Coppersmith and S. Winograd, Matrix multiplication via arithmetic progressions, J. Symbolic Comput., 9(3):251--280, 1990) uses $n^{2.376}$ multiplications.",
        "published": "2003-06-11T20:31:46Z",
        "link": "http://arxiv.org/abs/cs/0306049v1",
        "categories": [
            "cs.CC",
            "cs.DB",
            "F.1.1"
        ]
    },
    {
        "title": "Universal Sequential Decisions in Unknown Environments",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "We give a brief introduction to the AIXI model, which unifies and overcomes the limitations of sequential decision theory and universal Solomonoff induction. While the former theory is suited for active agents in known environments, the latter is suited for passive prediction of unknown environments.",
        "published": "2003-06-16T13:15:29Z",
        "link": "http://arxiv.org/abs/cs/0306091v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LG",
            "I.2; G.3"
        ]
    },
    {
        "title": "Complexity of Cycle Length Modularity Problems in Graphs",
        "authors": [
            "Edith Hemaspaandra",
            "Holger Spakowski",
            "Mayur Thakur"
        ],
        "summary": "The even cycle problem for both undirected and directed graphs has been the topic of intense research in the last decade. In this paper, we study the computational complexity of \\emph{cycle length modularity problems}. Roughly speaking, in a cycle length modularity problem, given an input (undirected or directed) graph, one has to determine whether the graph has a cycle $C$ of a specific length (or one of several different lengths), modulo a fixed integer. We denote the two families (one for undirected graphs and one for directed graphs) of problems by $(S,m)\\hbox{-}{\\rm UC}$ and $(S,m)\\hbox{-}{\\rm DC}$, where $m \\in \\mathcal{N}$ and $S \\subseteq \\{0,1, ..., m-1\\}$. $(S,m)\\hbox{-}{\\rm UC}$ (respectively, $(S,m)\\hbox{-}{\\rm DC}$) is defined as follows: Given an undirected (respectively, directed) graph $G$, is there a cycle in $G$ whose length, modulo $m$, is a member of $S$? In this paper, we fully classify (i.e., as either polynomial-time solvable or as ${\\rm NP}$-complete) each problem $(S,m)\\hbox{-}{\\rm UC}$ such that $0 \\in S$ and each problem $(S,m)\\hbox{-}{\\rm DC}$ such that $0 \\notin S$. We also give a sufficient condition on $S$ and $m$ for the following problem to be polynomial-time computable: $(S,m)\\hbox{-}{\\rm UC}$ such that $0 \\notin S$.",
        "published": "2003-06-25T15:37:43Z",
        "link": "http://arxiv.org/abs/cs/0306131v1",
        "categories": [
            "cs.CC",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Quantum Computing Without Entanglement",
        "authors": [
            "Eli Biham",
            "Gilles Brassard",
            "Dan Kenigsberg",
            "Tal Mor"
        ],
        "summary": "It is generally believed that entanglement is essential for quantum computing. We present here a few simple examples in which quantum computing without entanglement is better than anything classically achievable, in terms of the reliability of the outcome after a xed number of oracle calls. Using a separable (that is, unentangled) n-qubit state, we show that the Deutsch-Jozsa problem and the Simon problem can be solved more reliably by a quantum computer than by the best possible classical algorithm, even probabilistic. We conclude that: (a) entanglement is not essential for quantum computing; and (b) some advantage of quantum algorithms over classical algorithms persists even when the quantum state contains an arbitrarily small amount of information|that is, even when the state is arbitrarily close to being totally mixed.",
        "published": "2003-06-26T15:55:58Z",
        "link": "http://arxiv.org/abs/quant-ph/0306182v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of Boolean Constraint Isomorphism",
        "authors": [
            "Elmar Böhler",
            "Edith Hemaspaandra",
            "Steffen Reith",
            "Heribert Vollmer"
        ],
        "summary": "In 1978, Schaefer proved his famous dichotomy theorem for generalized satisfiability problems. He defined an infinite number of propositional satisfiability problems (nowadays usually called Boolean constraint satisfaction problems) and showed that all these satisfiability problems are either in P or NP-complete. In recent years, similar results have been obtained for quite a few other problems for Boolean constraints.Almost all of these problems are variations of the satisfiability problem.   In this paper, we address a problem that is not a variation of satisfiability, namely, the isomorphism problem for Boolean constraints. Previous work by B\\\"ohler et al. showed that the isomorphism problem is either coNP-hard or reducible to the graph isomorphism problem (a problem that is in NP, but not known to be NP-hard), thus distinguishing a hard case and an easier case. However, they did not classify which cases are truly easy, i.e., in P. This paper accomplishes exactly that. It shows that Boolean constraint isomorphism is coNP-hard (and GI-hard), or equivalent to graph isomorphism, or in P, and it gives simple criteria to determine which case holds.",
        "published": "2003-06-27T11:08:50Z",
        "link": "http://arxiv.org/abs/cs/0306134v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3;F.4.1"
        ]
    },
    {
        "title": "How many candidates are needed to make elections hard to manipulate?",
        "authors": [
            "Vincent Conitzer",
            "Jerome Lang",
            "Tuomas Sandholm"
        ],
        "summary": "In multiagent settings where the agents have different preferences, preference aggregation is a central issue. Voting is a general method for preference aggregation, but seminal results have shown that all general voting protocols are manipulable. One could try to avoid manipulation by using voting protocols where determining a beneficial manipulation is hard computationally. The complexity of manipulating realistic elections where the number of candidates is a small constant was recently studied (Conitzer 2002), but the emphasis was on the question of whether or not a protocol becomes hard to manipulate for some constant number of candidates. That work, in many cases, left open the question: How many candidates are needed to make elections hard to manipulate? This is a crucial question when comparing the relative manipulability of different voting protocols. In this paper we answer that question for the voting protocols of the earlier study: plurality, Borda, STV, Copeland, maximin, regular cup, and randomized cup. We also answer that question for two voting protocols for which no results on the complexity of manipulation have been derived before: veto and plurality with runoff. It turns out that the voting protocols under study become hard to manipulate at 3 candidates, 4 candidates, 7 candidates, or never.",
        "published": "2003-07-02T18:33:50Z",
        "link": "http://arxiv.org/abs/cs/0307003v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Quaternionic Computing",
        "authors": [
            "Jose M. Fernandez",
            "William A. Schneeberger"
        ],
        "summary": "We introduce a model of computation based on quaternions, which is inspired on the quantum computing model. Pure states are vectors of a suitable linear space over the quaternions. Other aspects of the theory are the same as in quantum computing: superposition and linearity of the state space, unitarity of the transformations, and projective measurements. However, one notable exception is the fact that quaternionic circuits do not have a uniquely defined behaviour, unless a total ordering of evaluation of the gates is defined. Given such an ordering a unique unitary operator can be associated with the quaternionic circuit and a proper semantics of computation can be associated with it.   The main result of this paper consists in showing that this model is no more powerful than quantum computing, as long as such an ordering of gates can be defined. More concretely we show, that for all quaternionic computation using n quaterbits, the behaviour of the circuit for each possible gate ordering can be simulated with n+1 qubits, and this with little or no overhead in circuit size. The proof of this result is inspired of a new simplified and improved proof of the equivalence of a similar model based on real amplitudes to quantum computing, which states that any quantum computation using n qubits can be simulated with n+1 rebits, and in this with no circuit size overhead.   Beyond this potential computational equivalence, however, we propose this model as a simpler framework in which to discuss the possibility of a quaternionic quantum mechanics or information theory. In particular, it already allows us to illustrate that the introduction of quaternions might violate some of the ``natural'' properties that we have come to expect from physical models.",
        "published": "2003-07-02T19:14:58Z",
        "link": "http://arxiv.org/abs/quant-ph/0307017v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "'Computing' as Information Compression by Multiple Alignment,   Unification and Search",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This paper argues that the operations of a 'Universal Turing Machine' (UTM) and equivalent mechanisms such as the 'Post Canonical System' (PCS) - which are widely accepted as definitions of the concept of `computing' - may be interpreted as *information compression by multiple alignment, unification and search* (ICMAUS).   The motivation for this interpretation is that it suggests ways in which the UTM/PCS model may be augmented in a proposed new computing system designed to exploit the ICMAUS principles as fully as possible. The provision of a relatively sophisticated search mechanism in the proposed 'SP' system appears to open the door to the integration and simplification of a range of functions including unsupervised inductive learning, best-match pattern recognition and information retrieval, probabilistic reasoning, planning and problem solving, and others. Detailed consideration of how the ICMAUS principles may be applied to these functions is outside the scope of this article but relevant sources are cited in this article.",
        "published": "2003-07-05T18:52:20Z",
        "link": "http://arxiv.org/abs/cs/0307013v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "F.0"
        ]
    },
    {
        "title": "Complexity of Determining Nonemptiness of the Core",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Coalition formation is a key problem in automated negotiation among self-interested agents, and other multiagent applications. A coalition of agents can sometimes accomplish things that the individual agents cannot, or can do things more efficiently. However, motivating the agents to abide to a solution requires careful analysis: only some of the solutions are stable in the sense that no group of agents is motivated to break off and form a new coalition. This constraint has been studied extensively in cooperative game theory. However, the computational questions around this constraint have received less attention. When it comes to coalition formation among software agents (that represent real-world parties), these questions become increasingly explicit.   In this paper we define a concise general representation for games in characteristic form that relies on superadditivity, and show that it allows for efficient checking of whether a given outcome is in the core. We then show that determining whether the core is nonempty is $\\mathcal{NP}$-complete both with and without transferable utility. We demonstrate that what makes the problem hard in both cases is determining the collaborative possibilities (the set of outcomes possible for the grand coalition), by showing that if these are given, the problem becomes tractable in both cases. However, we then demonstrate that for a hybrid version of the problem, where utility transfer is possible only within the grand coalition, the problem remains $\\mathcal{NP}$-complete even when the collaborative possibilities are given.",
        "published": "2003-07-07T20:02:23Z",
        "link": "http://arxiv.org/abs/cs/0307016v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Definition and Complexity of Some Basic Metareasoning Problems",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "In most real-world settings, due to limited time or other resources, an agent cannot perform all potentially useful deliberation and information gathering actions. This leads to the metareasoning problem of selecting such actions. Decision-theoretic methods for metareasoning have been studied in AI, but there are few theoretical results on the complexity of metareasoning.   We derive hardness results for three settings which most real metareasoning systems would have to encompass as special cases. In the first, the agent has to decide how to allocate its deliberation time across anytime algorithms running on different problem instances. We show this to be $\\mathcal{NP}$-complete. In the second, the agent has to (dynamically) allocate its deliberation or information gathering resources across multiple actions that it has to choose among. We show this to be $\\mathcal{NP}$-hard even when evaluating each individual action is extremely simple. In the third, the agent has to (dynamically) choose a limited number of deliberation or information gathering actions to disambiguate the state of the world. We show that this is $\\mathcal{NP}$-hard under a natural restriction, and $\\mathcal{PSPACE}$-hard in general.",
        "published": "2003-07-07T20:32:20Z",
        "link": "http://arxiv.org/abs/cs/0307017v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.11"
        ]
    },
    {
        "title": "Universal Voting Protocol Tweaks to Make Manipulation Hard",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Voting is a general method for preference aggregation in multiagent settings, but seminal results have shown that all (nondictatorial) voting protocols are manipulable. One could try to avoid manipulation by using voting protocols where determining a beneficial manipulation is hard computationally. A number of recent papers study the complexity of manipulating existing protocols. This paper is the first work to take the next step of designing new protocols that are especially hard to manipulate. Rather than designing these new protocols from scratch, we instead show how to tweak existing protocols to make manipulation hard, while leaving much of the original nature of the protocol intact. The tweak studied consists of adding one elimination preround to the election. Surprisingly, this extremely simple and universal tweak makes typical protocols hard to manipulate! The protocols become NP-hard, #P-hard, or PSPACE-hard to manipulate, depending on whether the schedule of the preround is determined before the votes are collected, after the votes are collected, or the scheduling and the vote collecting are interleaved, respectively. We prove general sufficient conditions on the protocols for this tweak to introduce the hardness, and show that the most common voting protocols satisfy those conditions. These are the first results in voting settings where manipulation is in a higher complexity class than NP (presuming PSPACE $\\neq$ NP).",
        "published": "2003-07-07T20:41:26Z",
        "link": "http://arxiv.org/abs/cs/0307018v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Defying Dimensions Mod 6",
        "authors": [
            "Vince Grolmusz"
        ],
        "summary": "We show that a certain representation of the matrix-product can be computed with $n^{o(1)}$ multiplications. We also show, that siumilar representations of matrices can be compressed enormously.",
        "published": "2003-07-08T15:10:37Z",
        "link": "http://arxiv.org/abs/cs/0307020v1",
        "categories": [
            "cs.CC",
            "F.1"
        ]
    },
    {
        "title": "High-density and Secure Data Transmission via Linear Combinations",
        "authors": [
            "Vince Grolmusz"
        ],
        "summary": "Suppose that there are $n$ Senders and $n$ Receivers. Our goal is to send long messages from Sender $i$ to Receiver $i$ such that no other receiver can retrieve the message intended for Receiver $i$. The task can easily be completed using $n$ private channels between the pairs. Solutions, using one channel needs either encryption or switching elements for routing the messages to their addressee.   The main result of the present work is a description of a network in which The Senders and the Receivers are connected with only $n^{o(1)}$ channels; the encoding and de-coding is nothing else just very fast linear combinations of the message-bits; and there are no switching or routing-elements in the network, just linear combinations are computed, with fixed connections (channels or wires).   In the proofs we do not use {\\em any} unproven cryptographical or complexity theoretical assumptions.",
        "published": "2003-07-17T12:10:21Z",
        "link": "http://arxiv.org/abs/cs/0307041v1",
        "categories": [
            "cs.CC",
            "cs.AR",
            "F.1"
        ]
    },
    {
        "title": "Lower Bounds for Local Search by Quantum Arguments",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "The problem of finding a local minimum of a black-box function is central for understanding local search as well as quantum adiabatic algorithms. For functions on the Boolean hypercube {0,1}^n, we show a lower bound of Omega(2^{n/4}/n) on the number of queries needed by a quantum computer to solve this problem. More surprisingly, our approach, based on Ambainis' quantum adversary method, also yields a lower bound of Omega(2^{n/2}/n^2) on the problem's classical randomized query complexity. This improves and simplifies a 1983 result of Aldous. Finally, in both the randomized and quantum cases, we give the first nontrivial lower bounds for finding local minima on grids of constant dimension greater than 2.",
        "published": "2003-07-21T15:24:17Z",
        "link": "http://arxiv.org/abs/quant-ph/0307149v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Euclidean algorithms are Gaussian",
        "authors": [
            "Viviane Baladi",
            "Brigitte Vallee"
        ],
        "summary": "This study provides new results about the probabilistic behaviour of a class of Euclidean algorithms: the asymptotic distribution of a whole class of cost-parameters associated to these algorithms is normal. For the cost corresponding to the number of steps Hensley already has proved a Local Limit Theorem; we give a new proof, and extend his result to other euclidean algorithms and to a large class of digit costs, obtaining a faster, optimal, rate of convergence. The paper is based on the dynamical systems methodology, and the main tool is the transfer operator. In particular, we use recent results of Dolgopyat.",
        "published": "2003-07-28T10:31:53Z",
        "link": "http://arxiv.org/abs/cs/0307062v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.1, I.1.2"
        ]
    },
    {
        "title": "On the probabilistic approach to the random satisfiability problem",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In this note I will review some of the recent results that have been obtained in the probabilistic approach to the random satisfiability problem. At the present moment the results are only heuristic. In the case of the random 3-satisfiability problem a phase transition from the satisfiable to the unsatisfiable phase is found at $\\alpha=4.267$. There are other values of $\\alpha$ that separates different regimes and they will be described in details. In this context the properties of the survey decimation algorithm will also be discussed.",
        "published": "2003-08-05T10:34:36Z",
        "link": "http://arxiv.org/abs/cs/0308010v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "G.3, G.2.1 G.3, G.2.1 G.3, G.2.1"
        ]
    },
    {
        "title": "Constant-Depth Frege Systems with Counting Axioms Polynomially Simulate   Nullstellensatz Refutations",
        "authors": [
            "Russell Impagliazzo",
            "Nathan Segerlind"
        ],
        "summary": "We show that constant-depth Frege systems with counting axioms modulo $m$ polynomially simulate Nullstellensatz refutations modulo $m$. Central to this is a new definition of reducibility from formulas to systems of polynomials with the property that, for most previously studied translations of formulas to systems of polynomials, a formula reduces to its translation. When combined with a previous result of the authors, this establishes the first size separation between Nullstellensatz and polynomial calculus refutations. We also obtain new, small refutations for certain CNFs by constant-depth Frege systems with counting axioms.",
        "published": "2003-08-05T19:25:21Z",
        "link": "http://arxiv.org/abs/cs/0308012v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Instability of one-step replica-symmetry-broken phase in satisfiability   problems",
        "authors": [
            "Andrea Montanari",
            "Giorgio Parisi",
            "Federico Ricci-Tersenghi"
        ],
        "summary": "We reconsider the one-step replica-symmetry-breaking (1RSB) solutions of two random combinatorial problems: k-XORSAT and k-SAT. We present a general method for establishing the stability of these solutions with respect to further steps of replica-symmetry breaking. Our approach extends the ideas of [A.Montanari and F. Ricci-Tersenghi, Eur.Phys.J. B 33, 339 (2003)] to more general combinatorial problems.   It turns out that 1RSB is always unstable at sufficiently small clauses density alpha or high energy. In particular, the recent 1RSB solution to 3-SAT is unstable at zero energy for alpha< alpha_m, with alpha_m\\approx 4.153. On the other hand, the SAT-UNSAT phase transition seems to be correctly described within 1RSB.",
        "published": "2003-08-07T18:33:47Z",
        "link": "http://arxiv.org/abs/cond-mat/0308147v1",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Computational Complexity Measures of Multipartite Quantum Entanglement",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "We shed new light on entanglement measures in multipartite quantum systems by taking a computational-complexity approach toward quantifying quantum entanglement with two familiar notions--approximability and distinguishability. Built upon the formal treatment of partial separability, we measure the complexity of an entangled quantum state by determining (i) how hard to approximate it from a fixed classical state and (ii) how hard to distinguish it from all partially separable states. We further consider the Kolmogorovian-style descriptive complexity of approximation and distinction of partial entanglement.",
        "published": "2003-08-13T06:26:48Z",
        "link": "http://arxiv.org/abs/quant-ph/0308072v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "On the complexity of curve fitting algorithms",
        "authors": [
            "N. Chernov",
            "C. Lesort",
            "N. Simanyi"
        ],
        "summary": "We study a popular algorithm for fitting polynomial curves to scattered data based on the least squares with gradient weights. We show that sometimes this algorithm admits a substantial reduction of complexity, and, furthermore, find precise conditions under which this is possible. It turns out that this is, indeed, possible when one fits circles but not ellipses or hyperbolas.",
        "published": "2003-08-15T15:37:43Z",
        "link": "http://arxiv.org/abs/cs/0308023v1",
        "categories": [
            "cs.CC",
            "cs.CV",
            "I.4.8; I.5.1; G.3"
        ]
    },
    {
        "title": "Quantum NP and a Quantum Hierarchy",
        "authors": [
            "Tomoyuki Yamakami"
        ],
        "summary": "The complexity class NP is quintessential and ubiquitous in theoretical computer science. Two different approaches have been made to define \"Quantum NP,\" the quantum analogue of NP: NQP by Adleman, DeMarrais, and Huang, and QMA by Knill, Kitaev, and Watrous. From an operator point of view, NP can be viewed as the result of the exists-operator applied to P. Recently, Green, Homer, Moore, and Pollett proposed its quantum version, called the N-operator, which is an abstraction of NQP. This paper introduces the exists^{Q}-operator, which is an abstraction of QMA, and its complement, the forall^{Q}-operator. These operators not only define Quantum NP but also build a quantum hierarchy, similar to the Meyer-Stockmeyer polynomial hierarchy, based on two-sided bounded-error quantum computation.",
        "published": "2003-08-23T20:39:56Z",
        "link": "http://arxiv.org/abs/quant-ph/0308125v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The KR-Benes Network: A Control-Optimal Rearrangeable Permutation   Network",
        "authors": [
            "Rajgopal Kannan"
        ],
        "summary": "The Benes network has been used as a rearrangeable network for over 40 years, yet the uniform $N(2 \\log N-1)$ control complexity of the $N \\times N$ Benes is not optimal for many permutations. In this paper, we present a novel $O(\\log N)$ depth rearrangeable network called KR-Benes that is {\\it permutation-specific control-optimal}. The KR-Benes routes {\\it every} permutation with the minimal control complexity {\\it specific} to that permutation and its worst-case complexity for arbitrary permutations is bounded by the Benes; thus it replaces the Benes when considering control complexity/latency. We design the KR-Benes by first constructing a restricted $2 \\log K +2$ depth rearrangeable network called $K$-Benes for routing $K$-bounded permutations with control $2N \\log K$, $0 \\leq K \\leq N/4$. We then show that the $N \\times N$ Benes network itself (with one additional stage) contains every $K$-Benes network as a subgraph and use this property to construct the KR-Benes network. With regard to the control-optimality of the KR-Benes, we show that any optimal network for rearrangeably routing $K$-bounded permutations must have depth $2 \\log K + 2$, and therefore the $K$-Benes (and hence the KR-Benes) is optimal.",
        "published": "2003-09-06T06:19:30Z",
        "link": "http://arxiv.org/abs/cs/0309006v4",
        "categories": [
            "cs.NI",
            "cs.CC",
            "C.2.1"
        ]
    },
    {
        "title": "Using Simulated Annealing to Calculate the Trembles of Trembling Hand   Perfection",
        "authors": [
            "Stuart McDonald",
            "Liam Wagner"
        ],
        "summary": "Within the literature on non-cooperative game theory, there have been a number of attempts to propose logorithms which will compute Nash equilibria. Rather than derive a new algorithm, this paper shows that the family of algorithms known as Markov chain Monte Carlo (MCMC) can be used to calculate Nash equilibria. MCMC is a type of Monte Carlo simulation that relies on Markov chains to ensure its regularity conditions. MCMC has been widely used throughout the statistics and optimization literature, where variants of this algorithm are known as simulated annealing. This paper shows that there is interesting connection between the trembles that underlie the functioning of this algorithm and the type of Nash refinement known as trembling hand perfection.",
        "published": "2003-09-10T15:11:44Z",
        "link": "http://arxiv.org/abs/cs/0309016v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.NE",
            "q-bio.PE",
            "F.1.1;F.2.2;G.3;I.2.1;J.4"
        ]
    },
    {
        "title": "Using Propagation for Solving Complex Arithmetic Constraints",
        "authors": [
            "M. H. van Emden",
            "B. Moa"
        ],
        "summary": "Solving a system of nonlinear inequalities is an important problem for which conventional numerical analysis has no satisfactory method. With a box-consistency algorithm one can compute a cover for the solution set to arbitrarily close approximation. Because of difficulties in the use of propagation for complex arithmetic expressions, box consistency is computed with interval arithmetic. In this paper we present theorems that support a simple modification of propagation that allows complex arithmetic expressions to be handled efficiently. The version of box consistency that is obtained in this way is stronger than when interval arithmetic is used.",
        "published": "2003-09-11T18:37:09Z",
        "link": "http://arxiv.org/abs/cs/0309018v1",
        "categories": [
            "math.NA",
            "cs.AR",
            "cs.CC",
            "cs.NA",
            "cs.PF",
            "cs.RO",
            "B.8; G.1.5;G.1.6;I.2.9;I.3.1;C.1.4;D.2.4;F.2"
        ]
    },
    {
        "title": "Threshold values of Random K-SAT from the cavity method",
        "authors": [
            "Stephan Mertens",
            "Marc Mezard",
            "Riccardo Zecchina"
        ],
        "summary": "Using the cavity equations of \\cite{mezard:parisi:zecchina:02,mezard:zecchina:02}, we derive the various threshold values for the number of clauses per variable of the random $K$-satisfiability problem, generalizing the previous results to $K \\ge 4$. We also give an analytic solution of the equations, and some closed expressions for these thresholds, in an expansion around large $K$. The stability of the solution is also computed. For any $K$, the satisfiability threshold is found to be in the stable region of the solution, which adds further credit to the conjecture that this computation gives the exact satisfiability threshold.",
        "published": "2003-09-12T19:59:10Z",
        "link": "http://arxiv.org/abs/cs/0309020v2",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DM",
            "F.2.0; G.2.0"
        ]
    },
    {
        "title": "Lower bounds for predecessor searching in the cell probe model",
        "authors": [
            "Pranab Sen",
            "S. Venkatesh"
        ],
        "summary": "We consider a fundamental problem in data structures, static predecessor searching: Given a subset S of size n from the universe [m], store S so that queries of the form \"What is the predecessor of x in S?\" can be answered efficiently. We study this problem in the cell probe model introduced by Yao. Recently, Beame and Fich obtained optimal bounds on the number of probes needed by any deterministic query scheme if the associated storage scheme uses only n^{O(1)} cells of word size (\\log m)^{O(1)} bits. We give a new lower bound proof for this problem that matches the bounds of Beame and Fich. Our lower bound proof has the following advantages: it works for randomised query schemes too, while Beame and Fich's proof works for deterministic query schemes only. It also extends to `quantum address-only' query schemes that we define in this paper, and is simpler than Beame and Fich's proof. We prove our lower bound using the round elimination approach of Miltersen, Nisan, Safra and Wigderson. Using tools from information theory, we prove a strong round elimination lemma for communication complexity that enables us to obtain a tight lower bound for the predecessor problem. Our strong round elimination lemma also extends to quantum communication complexity. We also use our round elimination lemma to obtain a rounds versus communication tradeoff for the `greater-than' problem, improving on the tradeoff in Miltersen et al. We believe that our round elimination lemma is of independent interest and should have other applications.",
        "published": "2003-09-17T19:14:05Z",
        "link": "http://arxiv.org/abs/cs/0309033v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "quant-ph",
            "E.1; E.4"
        ]
    },
    {
        "title": "Minimal DFAs for Testing Divisibility",
        "authors": [
            "Boris Alexeev"
        ],
        "summary": "We present and prove a theorem answering the question \"how many states does a minimal deterministic finite automaton (DFA) that recognizes the set of base-b numbers divisible by k have?\"",
        "published": "2003-09-29T19:34:36Z",
        "link": "http://arxiv.org/abs/cs/0309052v1",
        "categories": [
            "cs.CC",
            "F.1.1; F.4.3"
        ]
    },
    {
        "title": "Robust Polynomials and Quantum Algorithms",
        "authors": [
            "Harry Buhrman",
            "Ilan Newman",
            "Hein Roehrig",
            "Ronald de Wolf"
        ],
        "summary": "We define and study the complexity of robust polynomials for Boolean functions and the related fault-tolerant quantum decision trees, where input bits are perturbed by noise. We compare several different possible definitions. Our main results are   * For every n-bit Boolean function f there is an n-variate polynomial p of degree O(n) that robustly approximates it, in the sense that p(x) remains close to f(x) if we slightly vary each of the n inputs of the polynomial.   * There is an O(n)-query quantum algorithm that robustly recovers n noisy input bits. Hence every n-bit function can be quantum computed with O(n) queries in the presence of noise. This contrasts with the classical model of Feige et al., where functions such as parity need Theta(n*log n) queries.   We give several extensions and applications of these results.",
        "published": "2003-09-30T14:59:51Z",
        "link": "http://arxiv.org/abs/quant-ph/0309220v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Random k-SAT: Two Moments Suffice to Cross a Sharp Threshold",
        "authors": [
            "Dimitris Achlioptas",
            "Cristopher Moore"
        ],
        "summary": "Many NP-complete constraint satisfaction problems appear to undergo a \"phase transition'' from solubility to insolubility when the constraint density passes through a critical threshold. In all such cases it is easy to derive upper bounds on the location of the threshold by showing that above a certain density the first moment (expectation) of the number of solutions tends to zero. We show that in the case of certain symmetric constraints, considering the second moment of the number of solutions yields nearly matching lower bounds for the location of the threshold. Specifically, we prove that the threshold for both random hypergraph 2-colorability (Property B) and random Not-All-Equal k-SAT is 2^{k-1} ln 2 -O(1). As a corollary, we establish that the threshold for random k-SAT is of order Theta(2^k), resolving a long-standing open problem.",
        "published": "2003-10-09T21:52:43Z",
        "link": "http://arxiv.org/abs/cond-mat/0310227v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC",
            "math.CO",
            "math.PR"
        ]
    },
    {
        "title": "Theory of One Tape Linear Time Turing Machines",
        "authors": [
            "Kohtaro Tadaki",
            "Tomoyuki Yamakami",
            "Jack C. H. Lin"
        ],
        "summary": "A theory of one-tape (one-head) linear-time Turing machines is essentially different from its polynomial-time counterpart since these machines are closely related to finite state automata. This paper discusses structural-complexity issues of one-tape Turing machines of various types (deterministic, nondeterministic, reversible, alternating, probabilistic, counting, and quantum Turing machines) that halt in linear time, where the running time of a machine is defined as the length of any longest computation path. We explore structural properties of one-tape linear-time Turing machines and clarify how the machines' resources affect their computational patterns and power.",
        "published": "2003-10-23T21:08:22Z",
        "link": "http://arxiv.org/abs/cs/0310046v3",
        "categories": [
            "cs.CC",
            "F.1.1; F.1.2; F.4.3"
        ]
    },
    {
        "title": "Puzzle: Zermelo-Fraenkel set theory is inconsistent",
        "authors": [
            "Craig Alan Feinstein"
        ],
        "summary": "In this note, we present a puzzle. We prove that Zermelo-Fraenkel set theory is inconsistent by proving, using Zermelo-Fraenkel set theory, the false statement that any algorithm that determines whether any $n \\times n$ matrix over $\\mathbb F_2$, the finite field of order 2, is nonsingular must run in exponential time in the worst-case scenario. The object of the puzzle is to find the error in the proof.",
        "published": "2003-10-31T18:32:00Z",
        "link": "http://arxiv.org/abs/cs/0310060v19",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Assessing security of some group based cryptosystems",
        "authors": [
            "Vladimir Shpilrain"
        ],
        "summary": "One of the possible generalizations of the discrete logarithm problem to arbitrary groups is the so-called conjugacy search problem (sometimes erroneously called just the conjugacy problem): given two elements a, b of a group G and the information that a^x=b for some x \\in G, find at least one particular element x like that. Here a^x stands for xax^{-1}. The computational difficulty of this problem in some particular groups has been used in several group based cryptosystems. Recently, a few preprints have been in circulation that suggested various \"neighbourhood search\" type heuristic attacks on the conjugacy search problem. The goal of the present survey is to stress a (probably well known) fact that these heuristic attacks alone are not a threat to the security of a cryptosystem, and, more importantly, to suggest a more credible approach to assessing security of group based cryptosystems. Such an approach should be necessarily based on the concept of the average case complexity (or expected running time) of an algorithm.   These arguments support the following conclusion: although it is generally feasible to base the security of a cryptosystem on the difficulty of the conjugacy search problem, the group G itself (the \"platform\") has to be chosen very carefully. In particular, experimental as well as theoretical evidence collected so far makes it appear likely that braid groups are not a good choice for the platform. We also reflect on possible replacements.",
        "published": "2003-11-04T20:23:19Z",
        "link": "http://arxiv.org/abs/math/0311047v1",
        "categories": [
            "math.GR",
            "cs.CC",
            "cs.CR",
            "20F36; 68Q17"
        ]
    },
    {
        "title": "Phase Transitions in Random Boolean Networks with Different Updating   Schemes",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "In this paper we study the phase transitions of different types of Random Boolean networks. These differ in their updating scheme: synchronous, semi-synchronous, or asynchronous, and deterministic or non-deterministic. It has been shown that the statistical properties of Random Boolean networks change considerable according to the updating scheme. We study with computer simulations sensitivity to initial conditions as a measure of order/chaos. We find that independently of their updating scheme, all network types have very similar phase transitions, namely when the average number of connections of nodes is between one and three. This critical value depends more on the size of the network than on the updating scheme.",
        "published": "2003-11-05T19:09:38Z",
        "link": "http://arxiv.org/abs/nlin/0311008v1",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.CC",
            "nlin.CG",
            "q-bio.QM"
        ]
    },
    {
        "title": "Multilinear Formulas and Skepticism of Quantum Computing",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "Several researchers, including Leonid Levin, Gerard 't Hooft, and Stephen Wolfram, have argued that quantum mechanics will break down before the factoring of large numbers becomes possible. If this is true, then there should be a natural set of quantum states that can account for all experiments performed to date, but not for Shor's factoring algorithm. We investigate as a candidate the set of states expressible by a polynomial number of additions and tensor products. Using a recent lower bound on multilinear formula size due to Raz, we then show that states arising in quantum error-correction require n^{Omega(log n)} additions and tensor products even to approximate, which incidentally yields the first superpolynomial gap between general and multilinear formula size of functions. More broadly, we introduce a complexity classification of pure quantum states, and prove many basic facts about this classification. Our goal is to refine vague ideas about a breakdown of quantum mechanics into specific hypotheses that might be experimentally testable in the near future.",
        "published": "2003-11-07T00:53:17Z",
        "link": "http://arxiv.org/abs/quant-ph/0311039v4",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Hierarchical Clustering Using Mutual Information",
        "authors": [
            "Alexander Kraskov",
            "Harald Stoegbauer",
            "Ralph G. Andrzejak",
            "Peter Grassberger"
        ],
        "summary": "We present a method for hierarchical clustering of data called {\\it mutual information clustering} (MIC) algorithm. It uses mutual information (MI) as a similarity measure and exploits its grouping property: The MI between three objects $X, Y,$ and $Z$ is equal to the sum of the MI between $X$ and $Y$, plus the MI between $Z$ and the combined object $(XY)$. We use this both in the Shannon (probabilistic) version of information theory and in the Kolmogorov (algorithmic) version. We apply our method to the construction of phylogenetic trees from mitochondrial DNA sequences and to the output of independent components analysis (ICA) as illustrated with the ECG of a pregnant woman.",
        "published": "2003-11-27T07:58:52Z",
        "link": "http://arxiv.org/abs/q-bio/0311037v1",
        "categories": [
            "q-bio.QM",
            "cs.CC",
            "physics.data-an"
        ]
    },
    {
        "title": "Weak Bezout inequality for D-modules",
        "authors": [
            "Dima Grigoriev"
        ],
        "summary": "Let $\\{w_{i,j}\\}_{1\\leq i\\leq n, 1\\leq j\\leq s} \\subset L_m=F(X_1,...,X_m)[{\\partial \\over \\partial X_1},..., {\\partial \\over \\partial X_m}]$ be linear partial differential operators of orders with respect to ${\\partial \\over \\partial X_1},..., {\\partial \\over \\partial X_m}$ at most $d$. We prove an upper bound n(4m^2d\\min\\{n,s\\})^{4^{m-t-1}(2(m-t))} on the leading coefficient of the Hilbert-Kolchin polynomial of the left $L_m$-module $<\\{w_{1,j}, ..., w_{n,j}\\}_{1\\leq j \\leq s} > \\subset L_m^n$ having the differential type $t$ (also being equal to the degree of the Hilbert-Kolchin polynomial). The main technical tool is the complexity bound on solving systems of linear equations over {\\it algebras of fractions} of the form $$L_m(F[X_1,..., X_m, {\\partial \\over \\partial X_1},..., {\\partial \\over \\partial X_k}])^{-1}.$$",
        "published": "2003-11-28T15:28:04Z",
        "link": "http://arxiv.org/abs/cs/0311053v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1.2"
        ]
    },
    {
        "title": "Hierarchical Clustering Based on Mutual Information",
        "authors": [
            "Alexander Kraskov",
            "Harald Stögbauer",
            "Ralph G. Andrzejak",
            "Peter Grassberger"
        ],
        "summary": "Motivation: Clustering is a frequently used concept in variety of bioinformatical applications. We present a new method for hierarchical clustering of data called mutual information clustering (MIC) algorithm. It uses mutual information (MI) as a similarity measure and exploits its grouping property: The MI between three objects X, Y, and Z is equal to the sum of the MI between X and Y, plus the MI between Z and the combined object (XY).   Results: We use this both in the Shannon (probabilistic) version of information theory, where the \"objects\" are probability distributions represented by random samples, and in the Kolmogorov (algorithmic) version, where the \"objects\" are symbol sequences. We apply our method to the construction of mammal phylogenetic trees from mitochondrial DNA sequences and we reconstruct the fetal ECG from the output of independent components analysis (ICA) applied to the ECG of a pregnant woman.   Availability: The programs for estimation of MI and for clustering (probabilistic version) are available at http://www.fz-juelich.de/nic/cs/software",
        "published": "2003-11-28T17:04:26Z",
        "link": "http://arxiv.org/abs/q-bio/0311039v2",
        "categories": [
            "q-bio.QM",
            "cs.CC",
            "physics.bio-ph"
        ]
    },
    {
        "title": "An Algorithmic Argument for Nonadaptive Query Complexity Lower Bounds on   Advised Quantum Computation",
        "authors": [
            "Harumichi Nishimura",
            "Tomoyuki Yamakami"
        ],
        "summary": "This paper employs a powerful argument, called an algorithmic argument, to prove lower bounds of the quantum query complexity of a multiple-block ordered search problem in which, given a block number i, we are to find a location of a target keyword in an ordered list of the i-th block. Apart from much studied polynomial and adversary methods for quantum query complexity lower bounds, our argument shows that the multiple-block ordered search needs a large number of nonadaptive oracle queries on a black-box model of quantum computation that is also supplemented with advice. Our argument is also applied to the notions of computational complexity theory: quantum truth-table reducibility and quantum truth-table autoreducibility.",
        "published": "2003-11-29T07:40:22Z",
        "link": "http://arxiv.org/abs/quant-ph/0312003v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Counting complexity classes for numeric computations II: algebraic and   semialgebraic sets",
        "authors": [
            "Peter Buergisser",
            "Felipe Cucker"
        ],
        "summary": "We define counting classes #P_R and #P_C in the Blum-Shub-Smale setting of computations over the real or complex numbers, respectively. The problems of counting the number of solutions of systems of polynomial inequalities over R, or of systems of polynomial equalities over C, respectively, turn out to be natural complete problems in these classes. We investigate to what extent the new counting classes capture the complexity of computing basic topological invariants of semialgebraic sets (over R) and algebraic sets (over C). We prove that the problem of computing the (modified) Euler characteristic of semialgebraic sets is FP_R^{#P_R}-complete, and that the problem of computing the geometric degree of complex algebraic sets is FP_C^{#P_C}-complete. We also define new counting complexity classes in the classical Turing model via taking Boolean parts of the classes above, and show that the problems to compute the Euler characteristic and the geometric degree of (semi)algebraic sets given by integer polynomials are complete in these classes. We complement the results in the Turing model by proving, for all k in N, the FPSPACE-hardness of the problem of computing the k-th Betti number of the zet of real zeros of a given integer polynomial. This holds with respect to the singular homology as well as for the Borel-Moore homology.",
        "published": "2003-12-02T16:17:16Z",
        "link": "http://arxiv.org/abs/cs/0312007v1",
        "categories": [
            "cs.CC",
            "math.AT",
            "F.1.3; I1.2"
        ]
    },
    {
        "title": "Constraint Optimization and Statistical Mechanics",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In these lectures I will present an introduction to the results that have been recently obtained in constraint optimization of random problems using statistical mechanics techniques. After presenting the general results, in order to simplify the presentation I will describe in details only the problems related to the coloring of a random graph.",
        "published": "2003-12-05T16:46:59Z",
        "link": "http://arxiv.org/abs/cs/0312011v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DS",
            "G.3; G.2.1 G.3, G.2.1 G.3, G.2.1"
        ]
    },
    {
        "title": "Soft lambda-calculus: a language for polynomial time computation",
        "authors": [
            "Patrick Baillot",
            "Virgile Mogbil"
        ],
        "summary": "Soft linear logic ([Lafont02]) is a subsystem of linear logic characterizing the class PTIME. We introduce Soft lambda-calculus as a calculus typable in the intuitionistic and affine variant of this logic. We prove that the (untyped) terms of this calculus are reducible in polynomial time. We then extend the type system of Soft logic with recursive types. This allows us to consider non-standard types for representing lists. Using these datatypes we examine the concrete expressivity of Soft lambda-calculus with the example of the insertion sort algorithm.",
        "published": "2003-12-07T23:42:06Z",
        "link": "http://arxiv.org/abs/cs/0312015v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4; F.4.1; F.4.2"
        ]
    },
    {
        "title": "Uniform test of algorithmic randomness over a general space",
        "authors": [
            "Peter Gacs"
        ],
        "summary": "The algorithmic theory of randomness is well developed when the underlying space is the set of finite or infinite sequences and the underlying probability distribution is the uniform distribution or a computable distribution. These restrictions seem artificial. Some progress has been made to extend the theory to arbitrary Bernoulli distributions (by Martin-Loef), and to arbitrary distributions (by Levin). We recall the main ideas and problems of Levin's theory, and report further progress in the same framework.   - We allow non-compact spaces (like the space of continuous functions, underlying the Brownian motion).   - The uniform test (deficiency of randomness) d_P(x) (depending both on the outcome x and the measure P should be defined in a general and natural way.   - We see which of the old results survive: existence of universal tests, conservation of randomness, expression of tests in terms of description complexity, existence of a universal measure, expression of mutual information as \"deficiency of independence.   - The negative of the new randomness test is shown to be a generalization of complexity in continuous spaces; we show that the addition theorem survives.   The paper's main contribution is introducing an appropriate framework for studying these questions and related ones (like statistics for a general family of distributions).",
        "published": "2003-12-17T19:25:30Z",
        "link": "http://arxiv.org/abs/cs/0312039v3",
        "categories": [
            "cs.CC",
            "E.4; G.3; H.1.1"
        ]
    },
    {
        "title": "Survey Propagation as local equilibrium equations",
        "authors": [
            "A. Braunstein",
            "R. Zecchina"
        ],
        "summary": "It has been shown experimentally that a decimation algorithm based on Survey Propagation (SP) equations allows to solve efficiently some combinatorial problems over random graphs. We show that these equations can be derived as sum-product equations for the computation of marginals in an extended space where the variables are allowed to take an additional value -- $*$ -- when they are not forced by the combinatorial constraints. An appropriate ``local equilibrium condition'' cost/energy function is introduced and its entropy is shown to coincide with the expected logarithm of the number of clusters of solutions as computed by SP. These results may help to clarify the geometrical notion of clusters assumed by SP for the random K-SAT or random graph coloring (where it is conjectured to be exact) and helps to explain which kind of clustering operation or approximation is enforced in general/small sized models in which it is known to be inexact.",
        "published": "2003-12-18T15:32:46Z",
        "link": "http://arxiv.org/abs/cond-mat/0312483v5",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Formal Concept Analysis and Resolution in Algebraic Domains",
        "authors": [
            "Pascal Hitzler",
            "Matthias Wendt"
        ],
        "summary": "We relate two formerly independent areas: Formal concept analysis and logic of domains. We will establish a correspondene between contextual attribute logic on formal contexts resp. concept lattices and a clausal logic on coherent algebraic cpos. We show how to identify the notion of formal concept in the domain theoretic setting. In particular, we show that a special instance of the resolution rule from the domain logic coincides with the concept closure operator from formal concept analysis. The results shed light on the use of contexts and domains for knowledge representation and reasoning purposes.",
        "published": "2003-01-09T23:37:57Z",
        "link": "http://arxiv.org/abs/cs/0301008v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3;I.2.4"
        ]
    },
    {
        "title": "Double-Negation Elimination in Some Propositional Logics",
        "authors": [
            "Michael Beeson",
            "Robert Veroff",
            "Larry Wos"
        ],
        "summary": "This article answers two questions (posed in the literature), each concerning the guaranteed existence of proofs free of double negation. A proof is free of double negation if none of its deduced steps contains a term of the form n(n(t)) for some term t, where n denotes negation. The first question asks for conditions on the hypotheses that, if satisfied, guarantee the existence of a double-negation-free proof when the conclusion is free of double negation. The second question asks about the existence of an axiom system for classical propositional calculus whose use, for theorems with a conclusion free of double negation, guarantees the existence of a double-negation-free proof. After giving conditions that answer the first question, we answer the second question by focusing on the Lukasiewicz three-axiom system. We then extend our studies to infinite-valued sentential calculus and to intuitionistic logic and generalize the notion of being double-negation free. The double-negation proofs of interest rely exclusively on the inference rule condensed detachment, a rule that combines modus ponens with an appropriately general rule of substitution. The automated reasoning program OTTER played an indispensable role in this study.",
        "published": "2003-01-24T20:25:50Z",
        "link": "http://arxiv.org/abs/cs/0301026v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Tight Logic Programs",
        "authors": [
            "Esra Erdem",
            "Vladimir Lifschitz"
        ],
        "summary": "This note is about the relationship between two theories of negation as failure -- one based on program completion, the other based on stable models, or answer sets. Francois Fages showed that if a logic program satisfies a certain syntactic condition, which is now called ``tightness,'' then its stable models can be characterized as the models of its completion. We extend the definition of tightness and Fages' theorem to programs with nested expressions in the bodies of rules, and study tight logic programs containing the definition of the transitive closure of a predicate.",
        "published": "2003-02-28T01:28:22Z",
        "link": "http://arxiv.org/abs/cs/0302038v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "An Effective Decision Procedure for Linear Arithmetic with Integer and   Real Variables",
        "authors": [
            "Bernard Boigelot",
            "Sebastien Jodogne",
            "Pierre Wolper"
        ],
        "summary": "This paper considers finite-automata based algorithms for handling linear arithmetic with both real and integer variables. Previous work has shown that this theory can be dealt with by using finite automata on infinite words, but this involves some difficult and delicate to implement algorithms. The contribution of this paper is to show, using topological arguments, that only a restricted class of automata on infinite words are necessary for handling real and integer linear arithmetic. This allows the use of substantially simpler algorithms, which have been successfully implemented.",
        "published": "2003-03-20T17:05:24Z",
        "link": "http://arxiv.org/abs/cs/0303019v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.1.1; F.4.1; F.4.3"
        ]
    },
    {
        "title": "A Development Calculus for Specifications",
        "authors": [
            "Wei Li"
        ],
        "summary": "A first order inference system, called R-calculus, is defined to develop the specifications. It is used to eliminate the laws which is not consistent with the user's requirements. The R-calculus consists of the structural rules, an axiom, a cut rule, and the rules for logical connectives. Some examples are given to demonstrate the usage of the R-calculus. The properties about reachability and completeness of the R-calculus are formally defined and are proved.",
        "published": "2003-03-21T08:23:33Z",
        "link": "http://arxiv.org/abs/cs/0303021v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1"
        ]
    },
    {
        "title": "Ground Canonicity",
        "authors": [
            "Nachum Dershowitz"
        ],
        "summary": "We explore how different proof orderings induce different notions of saturation. We relate completion, paramodulation, saturation, redundancy elimination, and rewrite system reduction to proof orderings.",
        "published": "2003-04-10T20:08:18Z",
        "link": "http://arxiv.org/abs/cs/0304017v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Model Checking for a Class of Weighted Automata",
        "authors": [
            "Peter Buchholz",
            "Peter Kemper"
        ],
        "summary": "A large number of different model checking approaches has been proposed during the last decade. The different approaches are applicable to different model types including untimed, timed, probabilistic and stochastic models. This paper presents a new framework for model checking techniques which includes some of the known approaches, but enlarges the class of models for which model checking can be applied to the general class of weighted automata. The approach allows an easy adaption of model checking to models which have not been considered yet for this purpose. Examples for those new model types for which model checking can be applied are max/plus or min/plus automata which are well established models to describe different forms of dynamic systems and optimization problems. In this context, model checking can be used to verify temporal or quantitative properties of a system. The paper first presents briefly our class of weighted automata, as a very general model type. Then Valued Computational Tree Logic (CTL$) is introduced as a natural extension of the well known branching time logic CTL. Afterwards, algorithms to check a weighted automaton according to a CTL$ formula are presented. As a last result, a bisimulation is presented for weighted automata and for CTL$.",
        "published": "2003-04-15T19:08:00Z",
        "link": "http://arxiv.org/abs/cs/0304021v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1"
        ]
    },
    {
        "title": "Numerical simulations of a quantum algorithm for Hilbert's tenth problem",
        "authors": [
            "Tien D Kieu"
        ],
        "summary": "We employ quantum mechanical principles in the computability exploration of the class of classically noncomputable Hilbert's tenth problem which is equivalent to the Turing halting problem in Computer Science. The Quantum Adiabatic Theorem enables us to establish a connection between the solution for this class of problems and the asymptotic behaviour of solutions of a particular type of time-dependent Schr\\\"odinger equations. We then present some preliminary numerical simulation results for the quantum adiabatic processes corresponding to various Diophantine equations.",
        "published": "2003-04-16T17:23:14Z",
        "link": "http://arxiv.org/abs/quant-ph/0304114v1",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math.LO",
            "math.NT"
        ]
    },
    {
        "title": "Distributed States Temporal Logic",
        "authors": [
            "Carlo Montangero",
            "Laura Semini"
        ],
        "summary": "We introduce a temporal logic to reason on global applications in an asynchronous setting. First, we define the Distributed States Logic (DSL), a modal logic for localities that embeds the local theories of each component into a theory of the distributed states of the system. We provide the logic with a sound and complete axiomatization. The contribution is that it is possible to reason about properties that involve several components, even in the absence of a global clock. Then, we define the Distributed States Temporal Logic (DSTL) by introducing temporal operators a' la Unity. We support our proposal by working out a pair of examples: a simple secure communication system, and an algorithm for distributed leader election.   The motivation for this work is that the existing logics for distributed systems do not have the right expressive power to reason on the systems behaviour, when the communication is based on asynchronous message passing. On the other side, asynchronous communication is the most used abstraction when modelling global applications.",
        "published": "2003-04-30T18:54:59Z",
        "link": "http://arxiv.org/abs/cs/0304046v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Computing only minimal answers in disjunctive deductive databases",
        "authors": [
            "C. A. Johnson"
        ],
        "summary": "A method is presented for computing minimal answers in disjunctive deductive databases under the disjunctive stable model semantics. Such answers are constructed by repeatedly extending partial answers. Our method is complete (in that every minimal answer can be computed) and does not admit redundancy (in the sense that every partial answer generated can be extended to a minimal answer), whence no non-minimal answer is generated. For stratified databases, the method does not (necessarily) require the computation of models of the database in their entirety. Compilation is proposed as a tool by which problems relating to computational efficiency and the non-existence of disjunctive stable models can be overcome. The extension of our method to other semantics is also considered.",
        "published": "2003-05-13T08:27:45Z",
        "link": "http://arxiv.org/abs/cs/0305007v1",
        "categories": [
            "cs.LO",
            "F4.1"
        ]
    },
    {
        "title": "Optimizing Optimal Reduction: A Type Inference Algorithm for Elementary   Affine Logic",
        "authors": [
            "Paolo Coppola",
            "Simone Martini"
        ],
        "summary": "We present a type inference algorithm for lambda-terms in Elementary Affine Logic using linear constraints. We prove that the algorithm is correct and complete.",
        "published": "2003-05-15T10:46:00Z",
        "link": "http://arxiv.org/abs/cs/0305011v1",
        "categories": [
            "cs.LO",
            "F.4.1: D.3"
        ]
    },
    {
        "title": "Bounded LTL Model Checking with Stable Models",
        "authors": [
            "Keijo Heljanko",
            "Ilkka Niemelä"
        ],
        "summary": "In this paper bounded model checking of asynchronous concurrent systems is introduced as a promising application area for answer set programming. As the model of asynchronous systems a generalisation of communicating automata, 1-safe Petri nets, are used. It is shown how a 1-safe Petri net and a requirement on the behaviour of the net can be translated into a logic program such that the bounded model checking problem for the net can be solved by computing stable models of the corresponding program. The use of the stable model semantics leads to compact encodings of bounded reachability and deadlock detection tasks as well as the more general problem of bounded model checking of linear temporal logic. Correctness proofs of the devised translations are given, and some experimental results using the translation and the Smodels system are presented.",
        "published": "2003-05-23T17:16:24Z",
        "link": "http://arxiv.org/abs/cs/0305040v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.2.4;D.2.2;F.4.1;I.2.4;F.3.1"
        ]
    },
    {
        "title": "Applications of Intuitionistic Logic in Answer Set Programming",
        "authors": [
            "Mauricio Osorio",
            "Juan Antonio Navarro",
            "Jose Arrazola"
        ],
        "summary": "We present some applications of intermediate logics in the field of Answer Set Programming (ASP). A brief, but comprehensive introduction to the answer set semantics, intuitionistic and other intermediate logics is given. Some equivalence notions and their applications are discussed. Some results on intermediate logics are shown, and applied later to prove properties of answer sets. A characterization of answer sets for logic programs with nested expressions is provided in terms of intuitionistic provability, generalizing a recent result given by Pearce.   It is known that the answer set semantics for logic programs with nested expressions may select non-minimal models. Minimal models can be very important in some applications, therefore we studied them; in particular we obtain a characterization, in terms of intuitionistic logic, of answer sets which are also minimal models. We show that the logic G3 characterizes the notion of strong equivalence between programs under the semantic induced by these models. Finally we discuss possible applications and consequences of our results. They clearly state interesting links between ASP and intermediate logics, which might bring research in these two areas together.",
        "published": "2003-05-27T16:40:30Z",
        "link": "http://arxiv.org/abs/cs/0305046v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Minimum Model Semantics for Logic Programs with Negation-as-Failure",
        "authors": [
            "Panos Rondogiannis",
            "William W. Wadge"
        ],
        "summary": "We give a purely model-theoretic characterization of the semantics of logic programs with negation-as-failure allowed in clause bodies. In our semantics the meaning of a program is, as in the classical case, the unique minimum model in a program-independent ordering. We use an expanded truth domain that has an uncountable linearly ordered set of truth values between False (the minimum element) and True (the maximum), with a Zero element in the middle. The truth values below Zero are ordered like the countable ordinals. The values above Zero have exactly the reverse order. Negation is interpreted as reflection about Zero followed by a step towards Zero; the only truth value that remains unaffected by negation is Zero. We show that every program has a unique minimum model M_P, and that this model can be constructed with a T_P iteration which proceeds through the countable ordinals. Furthermore, we demonstrate that M_P can also be obtained through a model intersection construction which generalizes the well-known model intersection theorem for classical logic programming. Finally, we show that by collapsing the true and false values of the infinite-valued model M_P to (the classical) True and False, we obtain a three-valued model identical to the well-founded one.",
        "published": "2003-06-03T12:12:40Z",
        "link": "http://arxiv.org/abs/cs/0306017v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "F.3.2;F.4.1;D.1.6;I.2.3"
        ]
    },
    {
        "title": "An Abstract Programming System",
        "authors": [
            "David A. Plaisted"
        ],
        "summary": "The system PL permits the translation of abstract proofs of program correctness into programs in a variety of programming languages. A programming language satisfying certain axioms may be the target of such a translation. The system PL also permits the construction and proof of correctness of programs in an abstract programming language, and permits the translation of these programs into correct programs in a variety of languages. The abstract programming language has an imperative style of programming with assignment statements and side-effects, to allow the efficient generation of code. The abstract programs may be written by humans and then translated, avoiding the need to write the same program repeatedly in different languages or even the same language. This system uses classical logic, is conceptually simple, and permits reasoning about nonterminating programs using Scott-Strachey style denotational semantics.",
        "published": "2003-06-05T18:31:14Z",
        "link": "http://arxiv.org/abs/cs/0306028v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.1.2"
        ]
    },
    {
        "title": "A Transformational Decision Procedure for Non-Clausal Propositional   Formulas",
        "authors": [
            "Alexander Sakharov"
        ],
        "summary": "A decision procedure for detecting valid propositional formulas is presented. It is based on the Davis-Putnam method and deals with propositional formulas that are initially converted to negational normal form. This procedure splits variables but, in contrast to other decision procedures based on the Davis-Putnam method, it does not branch. Instead, this procedure iteratively makes validity-preserving transformations of fragments of the formula. The transformations involve only a minimal formula part containing occurrences of the selected variable. Selection of the best variable for splitting is crucial in this decision procedure - it may shorten the decision process dramatically. A variable whose splitting leads to a minimal size of the transformed formula is selected. Also, the decision procedure performs plenty of optimizations based on calculation of delta-sets. Some optimizations lead to removing fragments of the formula. Others detect variables for which a single truth value assignment is sufficient. The latest information about this research can be found at http://www.sakharov.net/valid.html",
        "published": "2003-06-07T16:13:19Z",
        "link": "http://arxiv.org/abs/cs/0306035v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.2.2"
        ]
    },
    {
        "title": "Quanta: a Language for Modeling and Manipulating Information Structures",
        "authors": [
            "Bruce Long"
        ],
        "summary": "We present a theory for modeling the structure of information and a language (Quanta) expressing the theory. Unlike Shannon's information theory, which focuses on the amount of information in an information system, we focus on the structure of the information in the system. For example, we can model the information structure corresponding to an algorithm or a physical process such as the structure of a quantum interaction. After a brief discussion of the relation between an evolving state-system and an information structure, we develop an algebra of information pieces (infons) to represent the structure of systems where descriptions of complex systems are constructed from expressions involving descriptions of simpler information systems. We map the theory to the Von Neumann computing model of sequences/conditionals/repetitions, and to the class/object theory of object-oriented programming (OOP).",
        "published": "2003-06-09T20:13:42Z",
        "link": "http://arxiv.org/abs/cs/0306038v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "I.1.3; D.3.2; F.4.1"
        ]
    },
    {
        "title": "Monodic temporal resolution",
        "authors": [
            "Anatoly Degtyarev",
            "Michael Fisher",
            "Boris Konev"
        ],
        "summary": "Until recently, First-Order Temporal Logic (FOTL) has been little understood. While it is well known that the full logic has no finite axiomatisation, a more detailed analysis of fragments of the logic was not previously available. However, a breakthrough by Hodkinson et.al., identifying a finitely axiomatisable fragment, termed the monodic fragment, has led to improved understanding of FOTL. Yet, in order to utilise these theoretical advances, it is important to have appropriate proof techniques for the monodic fragment.   In this paper, we modify and extend the clausal temporal resolution technique, originally developed for propositional temporal logics, to enable its use in such monodic fragments. We develop a specific normal form for formulae in FOTL, and provide a complete resolution calculus for formulae in this form. Not only is this clausal resolution technique useful as a practical proof technique for certain monodic classes, but the use of this approach provides us with increased understanding of the monodic fragment. In particular, we here show how several features of monodic FOTL are established as corollaries of the completeness result for the clausal temporal resolution method. These include definitions of new decidable monodic classes, simplification of existing monodic classes by reductions, and completeness of clausal temporal resolution in the case of monodic logics with expanding domains, a case with much significance in both theory and practice.",
        "published": "2003-06-10T10:02:03Z",
        "link": "http://arxiv.org/abs/cs/0306041v1",
        "categories": [
            "cs.LO",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Symbolic Parametric Analysis of Embedded Systems with BDD-like   Data-Structures",
        "authors": [
            "Farn Wang"
        ],
        "summary": "We use dense variable-ordering to define HRD (Hybrid-Restriction Diagram), a new BDD-like data-structure for the representation and manipulation of state-spaces of linear hybrid automata. We present and discuss various manipulation algorithms for HRD, including the basic set-oriented operations, weakest precondition calculation, and normalization. We implemented the ideas and experimented to see their performance. Finally, we have also developed a pruning technique for state-space exploration based on parameter valuation space characterization. The technique showed good promise in our experiment.",
        "published": "2003-06-19T09:57:09Z",
        "link": "http://arxiv.org/abs/cs/0306113v2",
        "categories": [
            "cs.DS",
            "cs.LO",
            "B.2.2; B.4.4; B.5.2; B.6.3; D.2.4; F.1.1; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Deciding regular grammar logics with converse through first-order logic",
        "authors": [
            "Stephane Demri",
            "Hans de Nivelle"
        ],
        "summary": "We provide a simple translation of the satisfiability problem for regular grammar logics with converse into GF2, which is the intersection of the guarded fragment and the 2-variable fragment of first-order logic. This translation is theoretically interesting because it translates modal logics with certain frame conditions into first-order logic, without explicitly expressing the frame conditions.   A consequence of the translation is that the general satisfiability problem for regular grammar logics with converse is in EXPTIME. This extends a previous result of the first author for grammar logics without converse. Using the same method, we show how some other modal logics can be naturally translated into GF2, including nominal tense logics and intuitionistic logic.   In our view, the results in this paper show that the natural first-order fragment corresponding to regular grammar logics is simply GF2 without extra machinery such as fixed point-operators.",
        "published": "2003-06-20T08:26:57Z",
        "link": "http://arxiv.org/abs/cs/0306117v2",
        "categories": [
            "cs.LO",
            "F.4.1;I.2.4;I.2.3"
        ]
    },
    {
        "title": "On coalgebra based on classes",
        "authors": [
            "J. Adamek",
            "S. Milius",
            "J. Velebil"
        ],
        "summary": "Every endofunctor of the category of classes is proved to be set-based in the sense of Aczel and Mendler, therefore, it has a final coalgebra. Other basic properties of these endofunctors are proved, e.g. the existence of a free completely iterative theory.",
        "published": "2003-06-20T09:43:58Z",
        "link": "http://arxiv.org/abs/cs/0306118v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Reachability problems for communicating finite state machines",
        "authors": [
            "Jan Pachl"
        ],
        "summary": "The paper deals with the verification of reachability properties in a commonly used state transition model of communication protocols, which consists of finite state machines connected by potentially unbounded FIFO channels. Although simple reachability problems are undecidable for general protocols with unbounded channels, they are decidable for the protocols with the recognizable channel property. The decidability question is open for the protocols with the rational channel property.",
        "published": "2003-06-22T19:18:55Z",
        "link": "http://arxiv.org/abs/cs/0306121v2",
        "categories": [
            "cs.LO",
            "cs.NI",
            "F.1.1; C.2.2; F.3.1; D.2.4"
        ]
    },
    {
        "title": "The Complexity of Boolean Constraint Isomorphism",
        "authors": [
            "Elmar Böhler",
            "Edith Hemaspaandra",
            "Steffen Reith",
            "Heribert Vollmer"
        ],
        "summary": "In 1978, Schaefer proved his famous dichotomy theorem for generalized satisfiability problems. He defined an infinite number of propositional satisfiability problems (nowadays usually called Boolean constraint satisfaction problems) and showed that all these satisfiability problems are either in P or NP-complete. In recent years, similar results have been obtained for quite a few other problems for Boolean constraints.Almost all of these problems are variations of the satisfiability problem.   In this paper, we address a problem that is not a variation of satisfiability, namely, the isomorphism problem for Boolean constraints. Previous work by B\\\"ohler et al. showed that the isomorphism problem is either coNP-hard or reducible to the graph isomorphism problem (a problem that is in NP, but not known to be NP-hard), thus distinguishing a hard case and an easier case. However, they did not classify which cases are truly easy, i.e., in P. This paper accomplishes exactly that. It shows that Boolean constraint isomorphism is coNP-hard (and GI-hard), or equivalent to graph isomorphism, or in P, and it gives simple criteria to determine which case holds.",
        "published": "2003-06-27T11:08:50Z",
        "link": "http://arxiv.org/abs/cs/0306134v2",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3;F.4.1"
        ]
    },
    {
        "title": "Distributive Computability",
        "authors": [
            "Sebastiano Vigna"
        ],
        "summary": "This thesis presents a series of theoretical results and practical realisations about the theory of computation in distributive categories. Distributive categories have been proposed as a foundational tool for Computer Science in the last years, starting from the papers of R.F.C. Walters. We shall focus on two major topics: distributive computability, i.e., a generalized theory of computability based on distributive categories, and the Imp(G) language, which is a language based on the syntax of distributive categories. The link between the former and the latter is that the functions computed by Imp(G) programs are exactly the distributively computable functions.",
        "published": "2003-06-27T14:06:59Z",
        "link": "http://arxiv.org/abs/cs/0306136v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.1.1"
        ]
    },
    {
        "title": "Transformations of Logic Programs with Goals as Arguments",
        "authors": [
            "Alberto Pettorossi",
            "Maurizio Proietti"
        ],
        "summary": "We consider a simple extension of logic programming where variables may range over goals and goals may be arguments of predicates. In this language we can write logic programs which use goals as data. We give practical evidence that, by exploiting this capability when transforming programs, we can improve program efficiency.   We propose a set of program transformation rules which extend the familiar unfolding and folding rules and allow us to manipulate clauses with goals which occur as arguments of predicates. In order to prove the correctness of these transformation rules, we formally define the operational semantics of our extended logic programming language. This semantics is a simple variant of LD-resolution. When suitable conditions are satisfied this semantics agrees with LD-resolution and, thus, the programs written in our extended language can be run by ordinary Prolog systems.   Our transformation rules are shown to preserve the operational semantics and termination.",
        "published": "2003-07-09T16:54:44Z",
        "link": "http://arxiv.org/abs/cs/0307022v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "Bridging the gap between modal temporal logics and constraint-based QSR   as an ALC(D) spatio-temporalisation with weakly cyclic TBoxes",
        "authors": [
            "Amar Isli"
        ],
        "summary": "The aim of this work is to provide a family of qualitative theories for spatial change in general, and for motion of spatial scenes in particular. To achieve this, we consider a spatio-temporalisation MTALC(D_x), of the well-known ALC(D) family of Description Logics (DLs) with a concrete domainan. In particular, the concrete domain D_x is generated by a qualitative spatial Relation Algebra (RA) x. We show the important result that satisfiability of an MTALC(D_x) concept with respect to a weakly cyclic TBox is decidable in nondeterministic exponential time, by reducing it to the emptiness problem of a weak alternating automaton augmented with spatial constraints, which we show to remain decidable, although the accepting condition of a run involves, additionally to the standard case, consistency of a CSP (Constraint Satisfaction Problem) potentially infinite. The result provides an effective tableaux-like satisfiability procedure which is discussed.",
        "published": "2003-07-17T14:46:19Z",
        "link": "http://arxiv.org/abs/cs/0307040v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2 (I.2.4)"
        ]
    },
    {
        "title": "A Lambda Calculus for Quantum Computation",
        "authors": [
            "Andre van Tonder"
        ],
        "summary": "The classical lambda calculus may be regarded both as a programming language and as a formal algebraic system for reasoning about computation. It provides a computational model equivalent to the Turing machine, and continues to be of enormous benefit in the classical theory of computation. We propose that quantum computation, like its classical counterpart, may benefit from a version of the lambda calculus suitable for expressing and reasoning about quantum algorithms. In this paper we develop a quantum lambda calculus as an alternative model of quantum computation, which combines some of the benefits of both the quantum Turing machine and the quantum circuit models. The calculus turns out to be closely related to the linear lambda calculi used in the study of Linear Logic. We set up a computational model and an equational proof system for this calculus, and we argue that it is equivalent to the quantum Turing machine.",
        "published": "2003-07-21T17:48:49Z",
        "link": "http://arxiv.org/abs/quant-ph/0307150v5",
        "categories": [
            "quant-ph",
            "cs.LO",
            "hep-th"
        ]
    },
    {
        "title": "Secrecy in Multiagent Systems",
        "authors": [
            "Kevin R. O'Neill",
            "Joseph Y. Halpern"
        ],
        "summary": "We introduce a general framework for reasoning about secrecy and privacy requirements in multiagent systems. Our definitions extend earlier definitions of secrecy and nondeducibility given by Shannon and Sutherland. Roughly speaking, one agent maintains secrecy with respect to another if the second agent cannot rule out any possibilities for the behavior or state of the first agent. We show that the framework can handle probability and nondeterminism in a clean way, is useful for reasoning about asynchronous systems as well as synchronous systems, and suggests generalizations of secrecy that may be useful for dealing with issues such as resource-bounded reasoning. We also show that a number of well-known attempts to characterize the absence of information flow are special cases of our definitions of secrecy.",
        "published": "2003-07-24T22:22:47Z",
        "link": "http://arxiv.org/abs/cs/0307057v2",
        "categories": [
            "cs.CR",
            "cs.LO",
            "D.4.6, F.4.1"
        ]
    },
    {
        "title": "Sound search in a denotational semantics for first order logic",
        "authors": [
            "C. F. M. Vermeulen"
        ],
        "summary": "In this paper we adapt the definitions and results from Apt and Vermeulen on `First order logic as a constraint programming language' (in: Proceedings of LPAR2001, Baaz and Voronkov (eds.), Springer LNAI 2514) to include important ideas about search and choice into the system. We give motivating examples. Then we set up denotational semantics for first order logic as follows: the semantic universe includes states that consist of two components: a substitution, which can be seen as the computed answer; and a constraint satisfaction problem, which can be seen as the residue of the original problem, yet to be handled by constraint programming. The interaction between these components is regulated by an operator called: infer. In this paper we regard infer as an operator on sets of states to enable us to analyze ideas about search among states and choice between states.   The precise adaptations of definitions and results are able to deal with the examples and we show that, given several reasonable conditions, the new definitions ensure soundness of the system with respect to the standard interpretation of first order logic. In this way the `reasonable conditions' can be read as conditions for sound search.   We indicate briefly how to investigate efficiency of search in future research.",
        "published": "2003-07-30T13:57:55Z",
        "link": "http://arxiv.org/abs/cs/0307067v1",
        "categories": [
            "cs.LO",
            "D.1.6;D.3.1;F.3.2"
        ]
    },
    {
        "title": "A logic for reasoning about upper probabilities",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "We present a propositional logic %which can be used to reason about the uncertainty of events, where the uncertainty is modeled by a set of probability measures assigning an interval of probability to each event. We give a sound and complete axiomatization for the logic, and show that the satisfiability problem is NP-complete, no harder than satisfiability for propositional logic.",
        "published": "2003-07-30T21:08:54Z",
        "link": "http://arxiv.org/abs/cs/0307069v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "Modeling Belief in Dynamic Systems, Part I: Foundations",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "Belief change is a fundamental problem in AI: Agents constantly have to update their beliefs to accommodate new observations. In recent years, there has been much work on axiomatic characterizations of belief change. We claim that a better understanding of belief change can be gained from examining appropriate semantic models. In this paper we propose a general framework in which to model belief change. We begin by defining belief in terms of knowledge and plausibility: an agent believes p if he knows that p is more plausible than its negation. We then consider some properties defining the interaction between knowledge and plausibility, and show how these properties affect the properties of belief. In particular, we show that by assuming two of the most natural properties, belief becomes a KD45 operator. Finally, we add time to the picture. This gives us a framework in which we can talk about knowledge, plausibility (and hence belief), and time, which extends the framework of Halpern and Fagin for modeling knowledge in multi-agent systems. We then examine the problem of ``minimal change''. This notion can be captured by using prior plausibilities, an analogue to prior probabilities, which can be updated by ``conditioning''. We show by example that conditioning on a plausibility measure can capture many scenarios of interest. In a companion paper, we show how the two best-studied scenarios of belief change, belief revisionand belief update, fit into our framework.",
        "published": "2003-07-30T21:36:03Z",
        "link": "http://arxiv.org/abs/cs/0307070v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "Modeling Belief in Dynamic Systems, Part II: Revisions and Update",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper, we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.",
        "published": "2003-07-30T21:51:56Z",
        "link": "http://arxiv.org/abs/cs/0307071v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.2.1"
        ]
    },
    {
        "title": "Two- versus three-dimensional connectivity testing of first-order   queries to semi-algebraic sets",
        "authors": [
            "Floris Geerts",
            "Lieven Smits",
            "Jan Van den Bussche"
        ],
        "summary": "This paper addresses the question whether one can determine the connectivity of a semi-algebraic set in three dimensions by testing the connectivity of a finite number of two-dimensional ``samples'' of the set, where these samples are defined by first-order queries. The question is answered negatively for two classes of first-order queries: cartesian-product-free, and positive one-pass.",
        "published": "2003-08-01T00:59:41Z",
        "link": "http://arxiv.org/abs/cs/0308001v2",
        "categories": [
            "cs.LO",
            "cs.CG",
            "cs.DB",
            "F.4.1; F.2.2; H.2.8"
        ]
    },
    {
        "title": "Constant-Depth Frege Systems with Counting Axioms Polynomially Simulate   Nullstellensatz Refutations",
        "authors": [
            "Russell Impagliazzo",
            "Nathan Segerlind"
        ],
        "summary": "We show that constant-depth Frege systems with counting axioms modulo $m$ polynomially simulate Nullstellensatz refutations modulo $m$. Central to this is a new definition of reducibility from formulas to systems of polynomials with the property that, for most previously studied translations of formulas to systems of polynomials, a formula reduces to its translation. When combined with a previous result of the authors, this establishes the first size separation between Nullstellensatz and polynomial calculus refutations. We also obtain new, small refutations for certain CNFs by constant-depth Frege systems with counting axioms.",
        "published": "2003-08-05T19:25:21Z",
        "link": "http://arxiv.org/abs/cs/0308012v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "On the expressive power of semijoin queries",
        "authors": [
            "Dirk Leinders",
            "Jerzy Tyszkiewicz",
            "Jan Van den Bussche"
        ],
        "summary": "The semijoin algebra is the variant of the relational algebra obtained by replacing the join operator by the semijoin operator. We provide an Ehrenfeucht-Fraiss\\'{e} game, characterizing the discerning power of the semijoin algebra. This game gives a method for showing that queries are not expressible in the semijoin algebra.",
        "published": "2003-08-06T14:12:11Z",
        "link": "http://arxiv.org/abs/cs/0308014v2",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3; F.4.1"
        ]
    },
    {
        "title": "On Decidability of Expressive Description Logics with Composition of   Roles in Number Restrictions",
        "authors": [
            "Fabio Grandi"
        ],
        "summary": "Description Logics are knowledge representation formalisms which have been used in a wide range of application domains. Owing to their appealing expressiveness, we consider in this paper extensions of the well-known concept language ALC allowing for number restrictions on complex role expressions. These have been first introduced by Baader and Sattler as ALCN(M) languages, with the adoption of role constructors M subset-of {o,-,And,Or}. In particular, they showed in 1999 that, although ALCN(o) is decidable, the addition of other operators may easily lead to undecidability: in fact, ALCN(o,And) and ALCN(o,-,Or) were proved undecidable.   In this work, we further investigate the computational properties of the ALCN family, aiming at narrowing the decidability gap left open by Baader and Sattler's results. In particular, we will show that ALCN(o) extended with inverse roles both in number and in value restrictions becomes undecidable, whereas it can be safely extended with qualified number restrictions without losing decidability.",
        "published": "2003-08-19T12:50:02Z",
        "link": "http://arxiv.org/abs/cs/0308029v1",
        "categories": [
            "cs.LO",
            "F.2.2; F.4.1; I.2.4"
        ]
    },
    {
        "title": "The Structure of Information",
        "authors": [
            "Bruce Long"
        ],
        "summary": "A formal model of the structure of information is presented in five axioms which define identity, containment, and joins of infons. Joins are shown to be commutative, associative, provide inverses of infons, and, potentially, have many identity elements, two of which are multiplicative and additive. Those two types of join are distributive. The other identity elements are for operators on entwined states. Multiplicative joins correspond to adding or removing new bits to a system while additive joins correspond to a change of state. The order or size of an infon is defined. This groundwork is intended to be used to model continuous and discreet information structures through time, especially in closed systems.",
        "published": "2003-09-02T10:10:51Z",
        "link": "http://arxiv.org/abs/cs/0309004v1",
        "categories": [
            "cs.LO",
            "F.0;H.1.1"
        ]
    },
    {
        "title": "Results on the quantitative mu-calculus qMu",
        "authors": [
            "Annabelle McIver",
            "Carroll Morgan"
        ],
        "summary": "The mu-calculus is a powerful tool for specifying and verifying transition systems, including those with both demonic and angelic choice; its quantitative generalisation qMu extends that to probabilistic choice.   We show that for a finite-state system the logical interpretation of qMu, via fixed-points in a domain of real-valued functions into [0,1], is equivalent to an operational interpretation given as a turn-based gambling game between two players.   The logical interpretation provides direct access to axioms, laws and meta-theorems. The operational, game- based interpretation aids the intuition and continues in the more general context to provide a surprisingly practical specification tool.   A corollary of our proofs is an extension of Everett's singly-nested games result in the finite turn-based case: we prove well-definedness of the minimax value, and existence of fixed memoriless strategies, for all qMu games/formulae, of arbitrary (including alternating) nesting structure.",
        "published": "2003-09-15T03:35:56Z",
        "link": "http://arxiv.org/abs/cs/0309024v1",
        "categories": [
            "cs.LO",
            "cs.GT",
            "D.2.4;F.1.2;F.3.1;F.4.1;G.3"
        ]
    },
    {
        "title": "A uniform approach to constraint-solving for lists, multisets, compact   lists, and sets",
        "authors": [
            "Agostino Dovier",
            "Carla Piazza",
            "Gianfranco Rossi"
        ],
        "summary": "Lists, multisets, and sets are well-known data structures whose usefulness is widely recognized in various areas of Computer Science. These data structures have been analyzed from an axiomatic point of view with a parametric approach in (*) where the relevant unification algorithms have been developed. In this paper we extend these results considering more general constraints including not only equality but also membership constraints as well as their negative counterparts.   (*) A. Dovier, A. Policriti, and G. Rossi. A uniform axiomatic view of lists, multisets, and sets, and the relevant unification algorithms. Fundamenta Informaticae, 36(2/3):201--234, 1998.",
        "published": "2003-09-24T10:08:00Z",
        "link": "http://arxiv.org/abs/cs/0309045v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SC",
            "D.3.3; F.4.1; F.2.2; I.1.2; I.2.3"
        ]
    },
    {
        "title": "The Liar and Related Paradoxes: Fuzzy Truth Value Assignment for   Collections of Self-Referential Sentences",
        "authors": [
            "K. Vezerides",
            "Ath. Kehagias"
        ],
        "summary": "We study self-referential sentences of the type related to the Liar paradox. In particular, we consider the problem of assigning consistent fuzzy truth values to collections of self-referential sentences. We show that the problem can be reduced to the solution of a system of nonlinear equations. Furthermore, we prove that, under mild conditions, such a system always has a solution (i.e. a consistent truth value assignment) and that, for a particular implementation of logical ``and'', ``or'' and ``negation'', the ``mid-point'' solution is always consistent. Next we turn to computational issues and present several truth-value assignment algorithms; we argue that these algorithms can be understood as generalized sequential reasoning. In an Appendix we present a large number of examples of self-referential collections (including the Liar and the Strengthened Liar), we formulate the corresponding truth value equations and solve them analytically and/ or numerically.",
        "published": "2003-09-24T11:50:00Z",
        "link": "http://arxiv.org/abs/cs/0309046v1",
        "categories": [
            "cs.LO",
            "F.4.1; G.1.5; I.2.3"
        ]
    },
    {
        "title": "Goedel Machines: Self-Referential Universal Problem Solvers Making   Provably Optimal Self-Improvements",
        "authors": [
            "Juergen Schmidhuber"
        ],
        "summary": "We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.",
        "published": "2003-09-25T15:59:46Z",
        "link": "http://arxiv.org/abs/cs/0309048v5",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "A Hierarchical Situation Calculus",
        "authors": [
            "David A. Plaisted"
        ],
        "summary": "A situation calculus is presented that provides a solution to the frame problem for hierarchical situations, that is, situations that have a modular structure in which parts of the situation behave in a relatively independent manner. This situation calculus is given in a relational, functional, and modal logic form. Each form permits both a single level hierarchy or a multiple level hierarchy, giving six versions of the formalism in all, and a number of sub-versions of these. For multiple level hierarchies, it is possible to give equations between parts of the situation to impose additional structure on the problem. This approach is compared to others in the literature.",
        "published": "2003-09-29T17:13:58Z",
        "link": "http://arxiv.org/abs/cs/0309053v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; I.2.4"
        ]
    },
    {
        "title": "Pure Prolog Execution in 21 Rules",
        "authors": [
            "Marija Kulas"
        ],
        "summary": "A simple mathematical definition of the 4-port model for pure Prolog is given. The model combines the intuition of ports with a compact representation of execution state. Forward and backward derivation steps are possible. The model satisfies a modularity claim, making it suitable for formal reasoning.",
        "published": "2003-10-10T18:46:15Z",
        "link": "http://arxiv.org/abs/cs/0310020v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2; F.3.1; D.2.5; I.2.3"
        ]
    },
    {
        "title": "Defining Homomorphisms and Other Generalized Morphisms of Fuzzy   Relations in Monoidal Fuzzy Logics by Means of BK-Products",
        "authors": [
            "Ladislav J. Kohout"
        ],
        "summary": "The present paper extends generalized morphisms of relations into the realm of Monoidal Fuzzy Logics by first proving and then using relational inequalities over pseudo-associative BK-products (compositions) of relations in these logics.   In 1977 Bandler and Kohout introduced generalized homomorphism, proteromorphism, amphimorphism, forward and backward compatibility of relations, and non-associative and pseudo-associative products (compositions) of relations into crisp (non-fuzzy Boolean) theory of relations. This was generalized later by Kohout to relations based on fuzzy Basic Logic systems (BL) of H\\'ajek and also for relational systems based on left-continuous t-norms.   The present paper is based on monoidal logics, hence it subsumes as special cases the theories of generalized morphisms (etc.) based on the following systems of logics: BL systems (which include the well known Goedel, product logic systems; Lukasiewicz logic and its extension to MV-algebras related to quantum logics), intuitionistic logics and linear logics.",
        "published": "2003-10-12T22:18:46Z",
        "link": "http://arxiv.org/abs/math/0310175v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "math-ph",
            "math.MP",
            "math.QA",
            "04A72; 08A02; 37F05"
        ]
    },
    {
        "title": "Kleene algebra with domain",
        "authors": [
            "J. Desharnais",
            "B. Möller",
            "G. Struth"
        ],
        "summary": "We propose Kleene algebra with domain (KAD), an extension of Kleene algebra with two equational axioms for a domain and a codomain operation, respectively. KAD considerably augments the expressiveness of Kleene algebra, in particular for the specification and analysis of state transition systems. We develop the basic calculus, discuss some related theories and present the most important models of KAD. We demonstrate applicability by two examples: First, an algebraic reconstruction of Noethericity and well-foundedness; second, an algebraic reconstruction of propositional Hoare logic.",
        "published": "2003-10-28T16:09:48Z",
        "link": "http://arxiv.org/abs/cs/0310054v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1; F.3.2; I.1.3"
        ]
    },
    {
        "title": "Logic programs with monotone cardinality atoms",
        "authors": [
            "Victor W. Marek",
            "Ilkka Niemela",
            "Miroslaw Truszczynski"
        ],
        "summary": "We investigate mca-programs, that is, logic programs with clauses built of monotone cardinality atoms of the form kX, where k is a non-negative integer and X is a finite set of propositional atoms. We develop a theory of mca-programs. We demonstrate that the operational concept of the one-step provability operator generalizes to mca-programs, but the generalization involves nondeterminism. Our main results show that the formalism of mca-programs is a common generalization of (1) normal logic programming with its semantics of models, supported models and stable models, (2) logic programming with cardinality atoms and with the semantics of stable models, as defined by Niemela, Simons and Soininen, and (3) of disjunctive logic programming with the possible-model semantics of Sakama and Inoue.",
        "published": "2003-10-31T16:56:18Z",
        "link": "http://arxiv.org/abs/cs/0310063v1",
        "categories": [
            "cs.LO",
            "D.1.6; I.2.3"
        ]
    },
    {
        "title": "Satisfiability and computing van der Waerden numbers",
        "authors": [
            "Michael R. Dransfield",
            "Victor W. Marek",
            "Miroslaw Truszczynski"
        ],
        "summary": "In this paper we bring together the areas of combinatorics and propositional satisfiability. Many combinatorial theorems establish, often constructively, the existence of positive integer functions, without actually providing their closed algebraic form or tight lower and upper bounds. The area of Ramsey theory is especially rich in such results. Using the problem of computing van der Waerden numbers as an example, we show that these problems can be represented by parameterized propositional theories in such a way that decisions concerning their satisfiability determine the numbers (function) in question. We show that by using general-purpose complete and local-search techniques for testing propositional satisfiability, this approach becomes effective -- competitive with specialized approaches. By following it, we were able to obtain several new results pertaining to the problem of computing van der Waerden numbers. We also note that due to their properties, especially their structural simplicity and computational hardness, propositional theories that arise in this research can be of use in development, testing and benchmarking of SAT solvers.",
        "published": "2003-10-31T17:05:58Z",
        "link": "http://arxiv.org/abs/cs/0310064v1",
        "categories": [
            "cs.LO",
            "I.2.8; F.4.1"
        ]
    },
    {
        "title": "Enhancing a Search Algorithm to Perform Intelligent Backtracking",
        "authors": [
            "Maurice Bruynooghe"
        ],
        "summary": "This paper illustrates how a Prolog program, using chronological backtracking to find a solution in some search space, can be enhanced to perform intelligent backtracking. The enhancement crucially relies on the impurity of Prolog that allows a program to store information when a dead end is reached. To illustrate the technique, a simple search program is enhanced.   To appear in Theory and Practice of Logic Programming.   Keywords: intelligent backtracking, dependency-directed backtracking, backjumping, conflict-directed backjumping, nogood sets, look-back.",
        "published": "2003-11-05T09:56:42Z",
        "link": "http://arxiv.org/abs/cs/0311003v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.8; I.2.3; F.3.3; F.4.1; D.1.6; D3.3"
        ]
    },
    {
        "title": "A Parameterised Hierarchy of Argumentation Semantics for Extended Logic   Programming and its Application to the Well-founded Semantics",
        "authors": [
            "Ralf Schweimeier",
            "Michael Schroeder"
        ],
        "summary": "Argumentation has proved a useful tool in defining formal semantics for assumption-based reasoning by viewing a proof as a process in which proponents and opponents attack each others arguments by undercuts (attack to an argument's premise) and rebuts (attack to an argument's conclusion). In this paper, we formulate a variety of notions of attack for extended logic programs from combinations of undercuts and rebuts and define a general hierarchy of argumentation semantics parameterised by the notions of attack chosen by proponent and opponent. We prove the equivalence and subset relationships between the semantics and examine some essential properties concerning consistency and the coherence principle, which relates default negation and explicit negation. Most significantly, we place existing semantics put forward in the literature in our hierarchy and identify a particular argumentation semantics for which we prove equivalence to the paraconsistent well-founded semantics with explicit negation, WFSX$_p$. Finally, we present a general proof theory, based on dialogue trees, and show that it is sound and complete with respect to the argumentation semantics.",
        "published": "2003-11-08T14:02:47Z",
        "link": "http://arxiv.org/abs/cs/0311008v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6.; F.3.2.; F.4.1; I.2.3.; I.2.4"
        ]
    },
    {
        "title": "Ackermann Encoding, Bisimulations, and OBDDs",
        "authors": [
            "Carla Piazza",
            "Alberto Policriti"
        ],
        "summary": "We propose an alternative way to represent graphs via OBDDs based on the observation that a partition of the graph nodes allows sharing among the employed OBDDs. In the second part of the paper we present a method to compute at the same time the quotient w.r.t. the maximum bisimulation and the OBDD representation of a given graph. The proposed computation is based on an OBDD-rewriting of the notion of Ackermann encoding of hereditarily finite sets into natural numbers.",
        "published": "2003-11-16T19:30:28Z",
        "link": "http://arxiv.org/abs/cs/0311018v1",
        "categories": [
            "cs.LO",
            "cs.DS",
            "E.1; F.2.2; D.2.4"
        ]
    },
    {
        "title": "Temporalized logics and automata for time granularity",
        "authors": [
            "M. Franceschet",
            "A. Montanari"
        ],
        "summary": "Suitable extensions of the monadic second-order theory of k successors have been proposed in the literature to capture the notion of time granularity. In this paper, we provide the monadic second-order theories of downward unbounded layered structures, which are infinitely refinable structures consisting of a coarsest domain and an infinite number of finer and finer domains, and of upward unbounded layered structures, which consist of a finest domain and an infinite number of coarser and coarser domains, with expressively complete and elementarily decidable temporal logic counterparts.   We obtain such a result in two steps. First, we define a new class of combined automata, called temporalized automata, which can be proved to be the automata-theoretic counterpart of temporalized logics, and show that relevant properties, such as closure under Boolean operations, decidability, and expressive equivalence with respect to temporal logics, transfer from component automata to temporalized ones. Then, we exploit the correspondence between temporalized logics and automata to reduce the task of finding the temporal logic counterparts of the given theories of time granularity to the easier one of finding temporalized automata counterparts of them.",
        "published": "2003-11-17T14:11:20Z",
        "link": "http://arxiv.org/abs/cs/0311022v1",
        "categories": [
            "cs.LO",
            "F.3.1"
        ]
    },
    {
        "title": "Combining Logic Programs and Monadic Second Order Logics by Program   Transformation",
        "authors": [
            "F. Fioravanti",
            "A. Pettorossi",
            "M. Proietti"
        ],
        "summary": "We present a program synthesis method based on unfold/fold transformation rules which can be used for deriving terminating definite logic programs from formulas of the Weak Monadic Second Order theory of one successor (WS1S). This synthesis method can also be used as a proof method which is a decision procedure for closed formulas of WS1S. We apply our synthesis method for translating CLP(WS1S) programs into logic programs and we use it also as a proof method for verifying safety properties of infinite state systems.",
        "published": "2003-11-27T09:47:28Z",
        "link": "http://arxiv.org/abs/cs/0311043v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "Derivation of Efficient Logic Programs by Specialization and Reduction   of Nondeterminism",
        "authors": [
            "Alberto Pettorossi",
            "Maurizio Proietti",
            "Sophie Renault"
        ],
        "summary": "Program specialization is a program transformation methodology which improves program efficiency by exploiting the information about the input data which are available at compile time. We show that current techniques for program specialization based on partial evaluation do not perform well on nondeterministic logic programs. We then consider a set of transformation rules which extend the ones used for partial evaluation, and we propose a strategy for guiding the application of these extended rules so to derive very efficient specialized programs. The efficiency improvements which sometimes are exponential, are due to the reduction of nondeterminism and to the fact that the computations which are performed by the initial programs in different branches of the computation trees, are performed by the specialized programs within single branches. In order to reduce nondeterminism we also make use of mode information for guiding the unfolding process. To exemplify our technique, we show that we can automatically derive very efficient matching programs and parsers for regular languages. The derivations we have performed could not have been done by previously known partial evaluation techniques.",
        "published": "2003-11-27T10:04:53Z",
        "link": "http://arxiv.org/abs/cs/0311044v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "Algebras for Agent Norm-Regulation",
        "authors": [
            "Jan Odelstad",
            "Magnus Boman"
        ],
        "summary": "An abstract architecture for idealized multi-agent systems whose behaviour is regulated by normative systems is developed and discussed. Agent choices are determined partially by the preference ordering of possible states and partially by normative considerations: The agent chooses that act which leads to the best outcome of all permissible actions. If an action is non-permissible depends on if the result of performing that action leads to a state satisfying a condition which is forbidden, according to the norms regulating the multi-agent system. This idea is formalized by defining set-theoretic predicates characterizing multi-agent systems. The definition of the predicate uses decision theory, the Kanger-Lindahl theory of normative positions, and an algebraic representation of normative systems.",
        "published": "2003-11-27T13:03:55Z",
        "link": "http://arxiv.org/abs/cs/0311046v1",
        "categories": [
            "cs.LO",
            "I.2.11"
        ]
    },
    {
        "title": "The concept of strong and weak virtual reality",
        "authors": [
            "A. M. Lisewski"
        ],
        "summary": "We approach the virtual reality phenomenon by studying its relationship to set theory, and we investigate the case where this is done using the wellfoundedness property of sets. Our hypothesis is that non-wellfounded sets (hypersets) give rise to a different quality of virtual reality than do familiar wellfounded sets. We initially provide an alternative approach to virtual reality based on Sommerhoff's idea of first and second order self-awareness; both categories of self-awareness are considered as necessary conditions for consciousness in terms of higher cognitive functions. We then introduce a representation of first and second order self-awareness through sets, and assume that these sets, which we call events, originally form a collection of wellfounded sets. Strong virtual reality characterizes virtual reality environments which have the limited capacity to create only events associated with wellfounded sets. In contrast, the more general concept of weak virtual reality characterizes collections of virtual reality mediated events altogether forming an entirety larger than any collection of wellfounded sets. By giving reference to Aczel's hyperset theory we indicate that this definition is not empty, because hypersets encompass wellfounded sets already. Moreover, we argue that weak virtual reality could be realized in human history through continued progress in computer technology. Finally, we reformulate our characterization into a more general framework, and use Baltag's Structural Theory of Sets (STS) to show that within this general hyperset theory Sommerhoff's first and second order self-awareness as well as both concepts of virtual reality admit a consistent mathematical representation.",
        "published": "2003-11-29T14:08:56Z",
        "link": "http://arxiv.org/abs/cs/0312001v3",
        "categories": [
            "cs.LO",
            "nlin.AO",
            "physics.comp-ph",
            "F.4.1; I.2.4; H.1.2; H.5.1"
        ]
    },
    {
        "title": "On Structuring Proof Search for First Order Linear Logic",
        "authors": [
            "Paola Bruscoli",
            "Alessio Guglielmi"
        ],
        "summary": "Full first order linear logic can be presented as an abstract logic programming language in Miller's system Forum, which yields a sensible operational interpretation in the 'proof search as computation' paradigm. However, Forum still has to deal with syntactic details that would normally be ignored by a reasonable operational semantics. In this respect, Forum improves on Gentzen systems for linear logic by restricting the language and the form of inference rules. We further improve on Forum by restricting the class of formulae allowed, in a system we call G-Forum, which is still equivalent to full first order linear logic. The only formulae allowed in G-Forum have the same shape as Forum sequents: the restriction does not diminish expressiveness and makes G-Forum amenable to proof theoretic analysis. G-Forum consists of two (big) inference rules, for which we show a cut elimination procedure. This does not need to appeal to finer detail in formulae and sequents than is provided by G-Forum, thus successfully testing the internal symmetries of our system.",
        "published": "2003-12-01T14:33:06Z",
        "link": "http://arxiv.org/abs/cs/0312002v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Partiality in physics",
        "authors": [
            "Bob Coecke",
            "Keye Martin"
        ],
        "summary": "We revisit the standard axioms of domain theory with emphasis on their relation to the concept of partiality, explain how this idea arises naturally in probability theory and quantum mechanics, and then search for a mathematical setting capable of providing a satisfactory unification of the two.",
        "published": "2003-12-04T16:03:39Z",
        "link": "http://arxiv.org/abs/quant-ph/0312044v1",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math.PR"
        ]
    },
    {
        "title": "Methods to Model-Check Parallel Systems Software",
        "authors": [
            "Olga Shumsky Matlin",
            "William McCune",
            "Ewing Lusk"
        ],
        "summary": "We report on an effort to develop methodologies for formal verification of parts of the Multi-Purpose Daemon (MPD) parallel process management system. MPD is a distributed collection of communicating processes. While the individual components of the collection execute simple algorithms, their interaction leads to unexpected errors that are difficult to uncover by conventional means. Two verification approaches are discussed here: the standard model checking approach using the software model checker SPIN and the nonstandard use of a general-purpose first-order resolution-style theorem prover OTTER to conduct the traditional state space exploration. We compare modeling methodology and analyze performance and scalability of the two methods with respect to verification of MPD.",
        "published": "2003-12-05T16:57:41Z",
        "link": "http://arxiv.org/abs/cs/0312012v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "D.2.4; D.1.3"
        ]
    },
    {
        "title": "Fuzziness versus probability again",
        "authors": [
            "F. Jurkovic"
        ],
        "summary": "A construction of a fuzzy logic controller based on an analogy between fuzzy conditional rule of inference and marginal probability in terms of the conditional probability function has been proposed.",
        "published": "2003-12-06T18:37:06Z",
        "link": "http://arxiv.org/abs/cs/0312013v2",
        "categories": [
            "cs.LO",
            "G.3;I2.3"
        ]
    },
    {
        "title": "Logical Characterizations of Heap Abstractions",
        "authors": [
            "G. Yorsh",
            "T. Reps",
            "M. Sagiv",
            "R. Wilhelm"
        ],
        "summary": "Shape analysis concerns the problem of determining \"shape invariants\" for programs that perform destructive updating on dynamically allocated storage. In recent work, we have shown how shape analysis can be performed, using an abstract interpretation based on 3-valued first-order logic. In that work, concrete stores are finite 2-valued logical structures, and the sets of stores that can possibly arise during execution are represented (conservatively) using a certain family of finite 3-valued logical structures. In this paper, we show how 3-valued structures that arise in shape analysis can be characterized using formulas in first-order logic with transitive closure.   We also define a non-standard (\"supervaluational\") semantics for 3-valued first-order logic that is more precise than a conventional 3-valued semantics, and demonstrate that the supervaluational semantics can be effectively implemented using existing theorem provers.",
        "published": "2003-12-07T20:31:23Z",
        "link": "http://arxiv.org/abs/cs/0312014v3",
        "categories": [
            "cs.LO",
            "D.2.4"
        ]
    },
    {
        "title": "Soft lambda-calculus: a language for polynomial time computation",
        "authors": [
            "Patrick Baillot",
            "Virgile Mogbil"
        ],
        "summary": "Soft linear logic ([Lafont02]) is a subsystem of linear logic characterizing the class PTIME. We introduce Soft lambda-calculus as a calculus typable in the intuitionistic and affine variant of this logic. We prove that the (untyped) terms of this calculus are reducible in polynomial time. We then extend the type system of Soft logic with recursive types. This allows us to consider non-standard types for representing lists. Using these datatypes we examine the concrete expressivity of Soft lambda-calculus with the example of the insertion sort algorithm.",
        "published": "2003-12-07T23:42:06Z",
        "link": "http://arxiv.org/abs/cs/0312015v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4; F.4.1; F.4.2"
        ]
    },
    {
        "title": "Verification of recursive parallel systems",
        "authors": [
            "Laura Bozzelli",
            "Massimo Benerecetti",
            "Adriano Peron"
        ],
        "summary": "In this paper we consider the problem of proving properties of infinite behaviour of formalisms suitable to describe (infinite state) systems with recursion and parallelism. As a formal setting, we consider the framework of Process Rewriting Systems (PRSs). For a meaningfull fragment of PRSs, allowing to accommodate both Pushdown Automata and Petri Nets, we state decidability results for a class of properties about infinite derivations (infinite term rewritings). The given results can be exploited for the automatic verification of some classes of linear time properties of infinite state systems described by PRSs. In order to exemplify the assessed results, we introduce a meaningful automaton based formalism which allows to express both recursion and multi--treading.",
        "published": "2003-12-11T14:54:06Z",
        "link": "http://arxiv.org/abs/cs/0312019v1",
        "categories": [
            "cs.LO",
            "68Q60"
        ]
    },
    {
        "title": "Minimal founded semantics for disjunctive logic programs and deductive   databases",
        "authors": [
            "Filippo Furfaro",
            "Gianluigi Greco",
            "Sergio Greco"
        ],
        "summary": "In this paper, we propose a variant of stable model semantics for disjunctive logic programming and deductive databases. The semantics, called minimal founded, generalizes stable model semantics for normal (i.e. non disjunctive) programs but differs from disjunctive stable model semantics (the extension of stable model semantics for disjunctive programs). Compared with disjunctive stable model semantics, minimal founded semantics seems to be more intuitive, it gives meaning to programs which are meaningless under stable model semantics and is no harder to compute. More specifically, minimal founded semantics differs from stable model semantics only for disjunctive programs having constraint rules or rules working as constraints. We study the expressive power of the semantics and show that for general disjunctive datalog programs it has the same power as disjunctive stable model semantics.",
        "published": "2003-12-15T18:29:41Z",
        "link": "http://arxiv.org/abs/cs/0312028v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Strong Equivalence Made Easy: Nested Expressions and Weight Constraints",
        "authors": [
            "Hudson Turner"
        ],
        "summary": "Logic programs P and Q are strongly equivalent if, given any program R, programs P union R and Q union R are equivalent (that is, have the same answer sets). Strong equivalence is convenient for the study of equivalent transformations of logic programs: one can prove that a local change is correct without considering the whole program. Lifschitz, Pearce and Valverde showed that Heyting's logic of here-and-there can be used to characterize strong equivalence for logic programs with nested expressions (which subsume the better-known extended disjunctive programs). This note considers a simpler, more direct characterization of strong equivalence for such programs, and shows that it can also be applied without modification to the weight constraint programs of Niemela and Simons. Thus, this characterization of strong equivalence is convenient for the study of equivalent transformations of logic programs written in the input languages of answer set programming systems dlv and smodels. The note concludes with a brief discussion of results that can be used to automate reasoning about strong equivalence, including a novel encoding that reduces the problem of deciding the strong equivalence of a pair of weight constraint programs to that of deciding the inconsistency of a weight constraint program.",
        "published": "2003-12-15T20:58:14Z",
        "link": "http://arxiv.org/abs/cs/0312029v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6"
        ]
    },
    {
        "title": "Learning in a Compiler for MINSAT Algorithms",
        "authors": [
            "Anja Remshagen",
            "Klaus Truemper"
        ],
        "summary": "This paper describes learning in a compiler for algorithms solving classes of the logic minimization problem MINSAT, where the underlying propositional formula is in conjunctive normal form (CNF) and where costs are associated with the True/False values of the variables. Each class consists of all instances that may be derived from a given propositional formula and costs for True/False values by fixing or deleting variables, and by deleting clauses. The learning step begins once the compiler has constructed a solution algorithm for a given class. The step applies that algorithm to comparatively few instances of the class, analyses the performance of the algorithm on these instances, and modifies the underlying propositional formula, with the goal that the algorithm will perform much better on all instances of the class.",
        "published": "2003-12-16T17:24:18Z",
        "link": "http://arxiv.org/abs/cs/0312032v1",
        "categories": [
            "cs.LO",
            "F.4.1, I.2.3"
        ]
    },
    {
        "title": "What Causes a System to Satisfy a Specification?",
        "authors": [
            "Hana Chockler",
            "Joseph Y. Halpern",
            "Orna Kupferman"
        ],
        "summary": "Even when a system is proven to be correct with respect to a specification, there is still a question of how complete the specification is, and whether it really covers all the behaviors of the system. Coverage metrics attempt to check which parts of a system are actually relevant for the verification process to succeed. Recent work on coverage in model checking suggests several coverage metrics and algorithms for finding parts of the system that are not covered by the specification. The work has already proven to be effective in practice, detecting design errors that escape early verification efforts in industrial settings. In this paper, we relate a formal definition of causality given by Halpern and Pearl [2001] to coverage. We show that it gives significant insight into unresolved issues regarding the definition of coverage and leads to potentially useful extensions of coverage. In particular, we introduce the notion of responsibility, which assigns to components of a system a quantitative measure of their relevance to the satisfaction of the specification.",
        "published": "2003-12-17T16:42:41Z",
        "link": "http://arxiv.org/abs/cs/0312036v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; F.3.1; I.2.4"
        ]
    },
    {
        "title": "Characterizing and Reasoning about Probabilistic and Non-Probabilistic   Expectation",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "Expectation is a central notion in probability theory. The notion of expectation also makes sense for other notions of uncertainty. We introduce a propositional logic for reasoning about expectation, where the semantics depends on the underlying representation of uncertainty. We give sound and complete axiomatizations for the logic in the case that the underlying representation is (a) probability, (b) sets of probability measures, (c) belief functions, and (d) possibility measures. We show that this logic is more expressive than the corresponding logic for reasoning about likelihood in the case of sets of probability measures, but equi-expressive in the case of probability, belief, and possibility. Finally, we show that satisfiability for these logics is NP-complete, no harder than satisfiability for propositional logic.",
        "published": "2003-12-17T16:50:39Z",
        "link": "http://arxiv.org/abs/cs/0312037v2",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1; I.2.3; I.2.4; G.3"
        ]
    },
    {
        "title": "Responsibility and blame: a structural-model approach",
        "authors": [
            "Hana Chockler",
            "Joseph Y. Halpern"
        ],
        "summary": "Causality is typically treated an all-or-nothing concept; either A is a cause of B or it is not. We extend the definition of causality introduced by Halpern and Pearl [2001] to take into account the degree of responsibility of A for B. For example, if someone wins an election 11--0, then each person who votes for him is less responsible for the victory than if he had won 6--5. We then define a notion of degree of blame, which takes into account an agent's epistemic state. Roughly speaking, the degree of blame of A for B is the expected degree of responsibility of A for B, taken over the epistemic state of an agent.",
        "published": "2003-12-17T16:59:50Z",
        "link": "http://arxiv.org/abs/cs/0312038v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.1"
        ]
    },
    {
        "title": "On the Abductive or Deductive Nature of Database Schema Validation and   Update Processing Problems",
        "authors": [
            "Ernest Teniente",
            "Toni Urpi"
        ],
        "summary": "We show that database schema validation and update processing problems such as view updating, materialized view maintenance, integrity constraint checking, integrity constraint maintenance or condition monitoring can be classified as problems of either abductive or deductive nature, according to the reasoning paradigm that inherently suites them. This is done by performing abductive and deductive reasoning on the event rules [Oli91], a set of rules that define the difference between consecutive database states In this way, we show that it is possible to provide methods able to deal with all these problems as a whole. We also show how some existing general deductive and abductive procedures may be used to reason on the event rules. In this way, we show that these procedures can deal with all database schema validation and update processing problems considered in this paper.",
        "published": "2003-12-19T15:25:53Z",
        "link": "http://arxiv.org/abs/cs/0312046v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.1;H.2.4; H.2.3"
        ]
    },
    {
        "title": "Representation Dependence in Probabilistic Inference",
        "authors": [
            "Joseph Y. Halpern",
            "Daphne Koller"
        ],
        "summary": "Non-deductive reasoning systems are often {\\em representation dependent}: representing the same situation in two different ways may cause such a system to return two different answers. Some have viewed this as a significant problem. For example, the principle of maximum entropy has been subjected to much criticism due to its representation dependence. There has, however, been almost no work investigating representation dependence. In this paper, we formalize this notion and show that it is not a problem specific to maximum entropy. In fact, we show that any representation-independent probabilistic inference procedure that ignores irrelevant information is essentially entailment, in a precise sense. Moreover, we show that representation independence is incompatible with even a weak default assumption of independence. We then show that invariance under a restricted class of representation changes can form a reasonable compromise between representation independence and other desiderata, and provide a construction of a family of inference procedures that provides such restricted representation independence, using relative entropy.",
        "published": "2003-12-20T16:30:20Z",
        "link": "http://arxiv.org/abs/cs/0312048v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; F.4.q"
        ]
    },
    {
        "title": "Quantum Computation, Categorical Semantics and Linear Logic",
        "authors": [
            "André van Tonder",
            "Miquel Dorca"
        ],
        "summary": "This preprint has been withdrawn.",
        "published": "2003-12-20T17:47:33Z",
        "link": "http://arxiv.org/abs/quant-ph/0312174v5",
        "categories": [
            "quant-ph",
            "cs.LO",
            "hep-th"
        ]
    },
    {
        "title": "ΣΠ-polycategories, additive linear logic, and process semantics",
        "authors": [
            "C. A. Pastro"
        ],
        "summary": "We present a process semantics for the purely additive fragment of linear logic in which formulas denote protocols and (equivalence classes of) proofs denote multi-channel concurrent processes. The polycategorical model induced by this process semantics is shown to be equivalent to the free polycategory based on the syntax (i.e., it is full and faithfully complete). This establishes that the additive fragment of linear logic provides a semantics of concurrent processes. Another property of this semantics is that it gives a canonical representation of proofs in additive linear logic.   This arXived version omits Section 1.7.1: \"Circuit diagrams for polycategories\" as the Xy-pic diagrams would not compile due to lack of memory. For a complete version see \"http://www.cpsc.ucalgary.ca/~pastroc/\".",
        "published": "2003-12-23T03:29:19Z",
        "link": "http://arxiv.org/abs/math/0312422v2",
        "categories": [
            "math.CT",
            "cs.LO",
            "math.LO",
            "18A15, 03F52, 68Q85"
        ]
    },
    {
        "title": "Abduction in Well-Founded Semantics and Generalized Stable Models",
        "authors": [
            "José Júlio Alferes",
            "Luís Moniz Pereira",
            "Terrance Swift"
        ],
        "summary": "Abductive logic programming offers a formalism to declaratively express and solve problems in areas such as diagnosis, planning, belief revision and hypothetical reasoning. Tabled logic programming offers a computational mechanism that provides a level of declarativity superior to that of Prolog, and which has supported successful applications in fields such as parsing, program analysis, and model checking. In this paper we show how to use tabled logic programming to evaluate queries to abductive frameworks with integrity constraints when these frameworks contain both default and explicit negation. The result is the ability to compute abduction over well-founded semantics with explicit negation and answer sets. Our approach consists of a transformation and an evaluation method. The transformation adjoins to each objective literal $O$ in a program, an objective literal $not(O)$ along with rules that ensure that $not(O)$ will be true if and only if $O$ is false. We call the resulting program a {\\em dual} program. The evaluation method, \\wfsmeth, then operates on the dual program. \\wfsmeth{} is sound and complete for evaluating queries to abductive frameworks whose entailment method is based on either the well-founded semantics with explicit negation, or on answer sets. Further, \\wfsmeth{} is asymptotically as efficient as any known method for either class of problems. In addition, when abduction is not desired, \\wfsmeth{} operating on a dual program provides a novel tabling method for evaluating queries to ground extended programs whose complexity and termination properties are similar to those of the best tabling methods for the well-founded semantics. A publicly available meta-interpreter has been developed for \\wfsmeth{} using the XSB system.",
        "published": "2003-12-24T15:45:40Z",
        "link": "http://arxiv.org/abs/cs/0312057v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; I.2.4"
        ]
    },
    {
        "title": "Collectives for the Optimal Combination of Imperfect Objects",
        "authors": [
            "Kagan Tumer",
            "David Wolpert"
        ],
        "summary": "In this letter we summarize some recent theoretical work on the design of collectives, i.e., of systems containing many agents, each of which can be viewed as trying to maximize an associated private utility, where there is also a world utility rating the behavior of that overall system that the designer of the collective wishes to optimize. We then apply algorithms based on that work on a recently suggested testbed for such optimization problems (Challet & Johnson, PRL, vol 89, 028701 2002). This is the problem of finding the combination of imperfect nano-scale objects that results in the best aggregate object. We present experimental results showing that these algorithms outperform conventional methods by more than an order of magnitude in this domain.",
        "published": "2003-01-23T20:02:26Z",
        "link": "http://arxiv.org/abs/cond-mat/0301459v1",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.MA",
            "nlin.AO"
        ]
    },
    {
        "title": "Improving Search Algorithms by Using Intelligent Coordinates",
        "authors": [
            "David Wolpert",
            "Kagan Tumer",
            "Esfandiar Bandari"
        ],
        "summary": "We consider the problem of designing a set of computational agents so that as they all pursue their self-interests a global function G of the collective system is optimized. Three factors govern the quality of such design. The first relates to conventional exploration-exploitation search algorithms for finding the maxima of such a global function, e.g., simulated annealing. Game-theoretic algorithms instead are related to the second of those factors, and the third is related to techniques from the field of machine learning. Here we demonstrate how to exploit all three factors by modifying the search algorithm's exploration stage so that rather than by random sampling, each coordinate of the underlying search space is controlled by an associated machine-learning-based ``player'' engaged in a non-cooperative game. Experiments demonstrate that this modification improves SA by up to an order of magnitude for bin-packing and for a model of an economic process run over an underlying network. These experiments also reveal novel small worlds phenomena.",
        "published": "2003-01-23T20:22:02Z",
        "link": "http://arxiv.org/abs/math/0301268v1",
        "categories": [
            "math.OC",
            "cond-mat.stat-mech",
            "cs.MA",
            "nlin.AO"
        ]
    },
    {
        "title": "A Method for Solving Distributed Service Allocation Problems",
        "authors": [
            "Jose M Vidal"
        ],
        "summary": "We present a method for solving service allocation problems in which a set of services must be allocated to a set of agents so as to maximize a global utility. The method is completely distributed so it can scale to any number of services without degradation. We first formalize the service allocation problem and then present a simple hill-climbing, a global hill-climbing, and a bidding-protocol algorithm for solving it. We analyze the expected performance of these algorithms as a function of various problem parameters such as the branching factor and the number of agents. Finally, we use the sensor allocation problem, an instance of a service allocation problem, to show the bidding protocol at work. The simulations also show that phase transition on the expected quality of the solution exists as the amount of communication between agents increases.",
        "published": "2003-06-20T15:12:22Z",
        "link": "http://arxiv.org/abs/cs/0306119v1",
        "categories": [
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "AWESOME: A General Multiagent Learning Algorithm that Converges in   Self-Play and Learns a Best Response Against Stationary Opponents",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "A satisfactory multiagent learning algorithm should, {\\em at a minimum}, learn to play optimally against stationary opponents and converge to a Nash equilibrium in self-play. The algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action repeated games--assuming that the opponent's (mixed) strategy is observable. In this paper we present AWESOME, the first algorithm that is guaranteed to have these two properties in {\\em all} repeated (finite) games. It requires only that the other players' actual actions (not their strategies) can be observed at each step. It also learns to play optimally against opponents that {\\em eventually become} stationary. The basic idea behind AWESOME ({\\em Adapt When Everybody is Stationary, Otherwise Move to Equilibrium}) is to try to adapt to the others' strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. The techniques used to prove the properties of AWESOME are fundamentally different from those used for previous algorithms, and may help in analyzing other multiagent learning algorithms also.",
        "published": "2003-07-01T23:22:44Z",
        "link": "http://arxiv.org/abs/cs/0307002v1",
        "categories": [
            "cs.GT",
            "cs.LG",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "How many candidates are needed to make elections hard to manipulate?",
        "authors": [
            "Vincent Conitzer",
            "Jerome Lang",
            "Tuomas Sandholm"
        ],
        "summary": "In multiagent settings where the agents have different preferences, preference aggregation is a central issue. Voting is a general method for preference aggregation, but seminal results have shown that all general voting protocols are manipulable. One could try to avoid manipulation by using voting protocols where determining a beneficial manipulation is hard computationally. The complexity of manipulating realistic elections where the number of candidates is a small constant was recently studied (Conitzer 2002), but the emphasis was on the question of whether or not a protocol becomes hard to manipulate for some constant number of candidates. That work, in many cases, left open the question: How many candidates are needed to make elections hard to manipulate? This is a crucial question when comparing the relative manipulability of different voting protocols. In this paper we answer that question for the voting protocols of the earlier study: plurality, Borda, STV, Copeland, maximin, regular cup, and randomized cup. We also answer that question for two voting protocols for which no results on the complexity of manipulation have been derived before: veto and plurality with runoff. It turns out that the voting protocols under study become hard to manipulate at 3 candidates, 4 candidates, 7 candidates, or never.",
        "published": "2003-07-02T18:33:50Z",
        "link": "http://arxiv.org/abs/cs/0307003v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "BL-WoLF: A Framework For Loss-Bounded Learnability In Zero-Sum Games",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "We present BL-WoLF, a framework for learnability in repeated zero-sum games where the cost of learning is measured by the losses the learning agent accrues (rather than the number of rounds). The game is adversarially chosen from some family that the learner knows. The opponent knows the game and the learner's learning strategy. The learner tries to either not accrue losses, or to quickly learn about the game so as to avoid future losses (this is consistent with the Win or Learn Fast (WoLF) principle; BL stands for ``bounded loss''). Our framework allows for both probabilistic and approximate learning. The resultant notion of {\\em BL-WoLF}-learnability can be applied to any class of games, and allows us to measure the inherent disadvantage to a player that does not know which game in the class it is in. We present {\\em guaranteed BL-WoLF-learnability} results for families of games with deterministic payoffs and families of games with stochastic payoffs. We demonstrate that these families are {\\em guaranteed approximately BL-WoLF-learnable} with lower cost. We then demonstrate families of games (both stochastic and deterministic) that are not guaranteed BL-WoLF-learnable. We show that those families, nevertheless, are {\\em BL-WoLF-learnable}. To prove these results, we use a key lemma which we derive.",
        "published": "2003-07-03T15:44:36Z",
        "link": "http://arxiv.org/abs/cs/0307006v1",
        "categories": [
            "cs.GT",
            "cs.LG",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Complexity of Determining Nonemptiness of the Core",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Coalition formation is a key problem in automated negotiation among self-interested agents, and other multiagent applications. A coalition of agents can sometimes accomplish things that the individual agents cannot, or can do things more efficiently. However, motivating the agents to abide to a solution requires careful analysis: only some of the solutions are stable in the sense that no group of agents is motivated to break off and form a new coalition. This constraint has been studied extensively in cooperative game theory. However, the computational questions around this constraint have received less attention. When it comes to coalition formation among software agents (that represent real-world parties), these questions become increasingly explicit.   In this paper we define a concise general representation for games in characteristic form that relies on superadditivity, and show that it allows for efficient checking of whether a given outcome is in the core. We then show that determining whether the core is nonempty is $\\mathcal{NP}$-complete both with and without transferable utility. We demonstrate that what makes the problem hard in both cases is determining the collaborative possibilities (the set of outcomes possible for the grand coalition), by showing that if these are given, the problem becomes tractable in both cases. However, we then demonstrate that for a hybrid version of the problem, where utility transfer is possible only within the grand coalition, the problem remains $\\mathcal{NP}$-complete even when the collaborative possibilities are given.",
        "published": "2003-07-07T20:02:23Z",
        "link": "http://arxiv.org/abs/cs/0307016v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Universal Voting Protocol Tweaks to Make Manipulation Hard",
        "authors": [
            "Vincent Conitzer",
            "Tuomas Sandholm"
        ],
        "summary": "Voting is a general method for preference aggregation in multiagent settings, but seminal results have shown that all (nondictatorial) voting protocols are manipulable. One could try to avoid manipulation by using voting protocols where determining a beneficial manipulation is hard computationally. A number of recent papers study the complexity of manipulating existing protocols. This paper is the first work to take the next step of designing new protocols that are especially hard to manipulate. Rather than designing these new protocols from scratch, we instead show how to tweak existing protocols to make manipulation hard, while leaving much of the original nature of the protocol intact. The tweak studied consists of adding one elimination preround to the election. Surprisingly, this extremely simple and universal tweak makes typical protocols hard to manipulate! The protocols become NP-hard, #P-hard, or PSPACE-hard to manipulate, depending on whether the schedule of the preround is determined before the votes are collected, after the votes are collected, or the scheduling and the vote collecting are interleaved, respectively. We prove general sufficient conditions on the protocols for this tweak to introduce the hardness, and show that the most common voting protocols satisfy those conditions. These are the first results in voting settings where manipulation is in a higher complexity class than NP (presuming PSPACE $\\neq$ NP).",
        "published": "2003-07-07T20:41:26Z",
        "link": "http://arxiv.org/abs/cs/0307018v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Product Distribution Field Theory",
        "authors": [
            "David H. Wolpert"
        ],
        "summary": "This paper presents a novel way to approximate a distribution governing a system of coupled particles with a product of independent distributions. The approach is an extension of mean field theory that allows the independent distributions to live in a different space from the system, and thereby capture statistical dependencies in that system. It also allows different Hamiltonians for each independent distribution, to facilitate Monte Carlo estimation of those distributions. The approach leads to a novel energy-minimization algorithm in which each coordinate Monte Carlo estimates an associated spectrum, and then independently sets its state by sampling a Boltzmann distribution across that spectrum. It can also be used for high-dimensional numerical integration, (constrained) combinatorial optimization, and adaptive distributed control. This approach also provides a simple, physics-based derivation of the powerful approximate energy-minimization algorithms semi-formally derived in \\cite{wowh00, wotu02c, wolp03a}. In addition it suggests many improvements to those algorithms, and motivates a new (bounded rationality) game theory equilibrium concept.",
        "published": "2003-07-25T02:59:49Z",
        "link": "http://arxiv.org/abs/cond-mat/0307630v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.MA",
            "nlin.AO"
        ]
    },
    {
        "title": "Finding Traitors in Secure Networks Using Byzantine Agreements",
        "authors": [
            "Liam Wagner",
            "Stuart McDonald"
        ],
        "summary": "Secure networks rely upon players to maintain security and reliability. However not every player can be assumed to have total loyalty and one must use methods to uncover traitors in such networks. We use the original concept of the Byzantine Generals Problem by Lamport, and the more formal Byzantine Agreement describe by Linial, to nd traitors in secure networks. By applying general fault-tolerance methods to develop a more formal design of secure networks we are able to uncover traitors amongst a group of players. We also propose methods to integrate this system with insecure channels. This new resiliency can be applied to broadcast and peer-to-peer secure communication systems where agents may be traitors or become unreliable due to faults.",
        "published": "2003-08-19T08:42:43Z",
        "link": "http://arxiv.org/abs/cs/0308028v5",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.GT",
            "cs.MA",
            "B.1.3; C.2.0; C.4.0; D.4.5; H.2.0; H.2.7"
        ]
    },
    {
        "title": "Learning in Multiagent Systems: An Introduction from a Game-Theoretic   Perspective",
        "authors": [
            "Jose M. Vidal"
        ],
        "summary": "We introduce the topic of learning in multiagent systems. We first provide a quick introduction to the field of game theory, focusing on the equilibrium concepts of iterated dominance, and Nash equilibrium. We show some of the most relevant findings in the theory of learning in games, including theorems on fictitious play, replicator dynamics, and evolutionary stable strategies. The CLRI theory and n-level learning agents are introduced as attempts to apply some of these findings to the problem of engineering multiagent systems with learning agents. Finally, we summarize some of the remaining challenges in the field of learning in multiagent systems.",
        "published": "2003-08-19T15:45:16Z",
        "link": "http://arxiv.org/abs/cs/0308030v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Semi-metric Behavior in Document Networks and its Application to   Recommendation Systems",
        "authors": [
            "L. M. Rocha"
        ],
        "summary": "Recommendation systems for different Document Networks (DN) such as the World Wide Web (WWW) and Digital Libraries, often use distance functions extracted from relationships among documents and keywords. For instance, documents in the WWW are related via a hyperlink network, while documents in bibliographic databases are related by citation and collaboration networks. Furthermore, documents are related to keyterms. The distance functions computed from these relations establish associative networks among items of the DN, referred to as Distance Graphs, which allow recommendation systems to identify relevant associations for individual users. However, modern recommendation systems need to integrate associative data from multiple sources such as different databases, web sites, and even other users. Thus, we are presented with a problem of combining evidence (about associations between items) from different sources characterized by distance functions. In this paper we describe our work on (1) inferring relevant associations from, as well as characterizing, semi-metric distance graphs and (2) combining evidence from different distance graphs in a recommendation system. Regarding (1), we present the idea of semi-metric distance graphs, and introduce ratios to measure semi-metric behavior. We compute these ratios for several DN such as digital libraries and web sites and show that they are useful to identify implicit associations. Regarding (2), we describe an algorithm to combine evidence from distance graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed for a recommendation system named TalkMine.",
        "published": "2003-09-09T05:24:03Z",
        "link": "http://arxiv.org/abs/cs/0309013v1",
        "categories": [
            "cs.IR",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.MA",
            "H.3.0; H.3.3, H.3.4; H.3.6; H.3.7; I.2.11; H.3.5"
        ]
    },
    {
        "title": "Self-organizing Traffic Control: First Results",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "We developed a virtual laboratory for traffic control where agents use different strategies in order to self-organize on the road. We present our first results where we compare the performance and behaviour promoted by environmental constrains and five different simple strategies: three inspired in flocking behaviour, one selfish, and one inspired in the minority game. Experiments are presented for comparing the strategies. Different issues are discussed, such as the important role of environmental constrains and the emergence of traffic lanes.",
        "published": "2003-09-12T20:39:35Z",
        "link": "http://arxiv.org/abs/nlin/0309039v1",
        "categories": [
            "nlin.AO",
            "cs.MA"
        ]
    },
    {
        "title": "Transient Diversity in Multi-Agent Systems",
        "authors": [
            "David Lyback"
        ],
        "summary": "Diversity is an important aspect of highly efficient multi-agent teams. We introduce the main factors that drive a multi-agent system in either direction along the diversity scale. A metric for diversity is described, and we speculate on the concept of transient diversity. Finally, an experiment on social entropy using a RoboCup simulated soccer team is presented.",
        "published": "2003-10-06T16:13:13Z",
        "link": "http://arxiv.org/abs/cs/0310010v1",
        "categories": [
            "cs.AI",
            "cs.MA",
            "I.2.11"
        ]
    },
    {
        "title": "Data mining and Privacy in Public Sector using Intelligent Agents   (discussion paper)",
        "authors": [
            "Max Voskob",
            "Nuck Punin"
        ],
        "summary": "The public sector comprises government agencies, ministries, education institutions, health providers and other types of government, commercial and not-for-profit organisations. Unlike commercial enterprises, this environment is highly heterogeneous in all aspects. This forms a complex network which is not always optimised. A lack of optimisation and communication hinders information sharing between the network nodes limiting the flow of information. Another limiting aspect is privacy of personal information and security of operations of some nodes or segments of the network. Attempts to reorganise the network or improve communications to make more information available for sharing and analysis may be hindered or completely halted by public concerns over privacy, political agendas, social and technological barriers. This paper discusses a technical solution for information sharing while addressing the privacy concerns with no need for reorganisation of the existing public sector infrastructure . The solution is based on imposing an additional layer of Intelligent Software Agents and Knowledge Bases for data mining and analysis.",
        "published": "2003-11-28T00:06:32Z",
        "link": "http://arxiv.org/abs/cs/0311050v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.IR",
            "cs.MA",
            "C.2.4; D.2.11; H.1.1; H.1.2; H.3.5; I.2.11; K.4.1"
        ]
    },
    {
        "title": "An Algorithm for Aligning Sentences in Bilingual Corpora Using Lexical   Information",
        "authors": [
            "Akshar Bharati",
            "V. Sriram",
            "A. Vamshi Krishna",
            "Rajeev Sangal",
            "S. M. Bendre"
        ],
        "summary": "In this paper we describe an algorithm for aligning sentences with their translations in a bilingual corpus using lexical information of the languages. Existing efficient algorithms ignore word identities and consider only the sentence lengths (Brown, 1991; Gale and Church, 1993). For a sentence in the source language text, the proposed algorithm picks the most likely translation from the target language text using lexical information and certain heuristics. It does not do statistical analysis using sentence lengths. The algorithm is language independent. It also aids in detecting addition and deletion of text in translations. The algorithm gives comparable results with the existing algorithms in most of the cases while it does better in cases where statistical algorithms do not give good results.",
        "published": "2003-02-12T06:31:54Z",
        "link": "http://arxiv.org/abs/cs/0302014v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Building an Open Language Archives Community on the OAI Foundation",
        "authors": [
            "Gary Simons",
            "Steven Bird"
        ],
        "summary": "The Open Language Archives Community (OLAC) is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources. The Dublin Core (DC) Element Set and the OAI Protocol have provided a solid foundation for the OLAC framework. However, we need more precision in community-specific aspects of resource description than is offered by DC. Furthermore, many of the institutions and individuals who might participate in OLAC do not have the technical resources to support the OAI protocol. This paper presents our solutions to these two problems. To address the first, we have developed an extensible application profile for language resource metadata. To address the second, we have implemented Vida (the virtual data provider) and Viser (the virtual service provider), which permit community members to provide data and services without having to implement the OAI protocol. These solutions are generic and could be adopted by other specialized subcommunities.",
        "published": "2003-02-14T07:11:50Z",
        "link": "http://arxiv.org/abs/cs/0302021v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Empirical Methods for Compound Splitting",
        "authors": [
            "Philipp Koehn",
            "Kevin Knight"
        ],
        "summary": "Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.",
        "published": "2003-02-22T23:37:26Z",
        "link": "http://arxiv.org/abs/cs/0302032v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "About compression of vocabulary in computer oriented languages",
        "authors": [
            "V. P. Maslov"
        ],
        "summary": "The author uses the entropy of the ideal Bose-Einstein gas to minimize losses in computer-oriented languages.",
        "published": "2003-03-05T08:50:42Z",
        "link": "http://arxiv.org/abs/cs/0303002v2",
        "categories": [
            "cs.CL",
            "B.1.4; H.3.1"
        ]
    },
    {
        "title": "Glottochronology and problems of protolanguage reconstruction",
        "authors": [
            "Kromer Victor"
        ],
        "summary": "A method of languages genealogical trees construction is proposed.",
        "published": "2003-03-14T05:15:20Z",
        "link": "http://arxiv.org/abs/cs/0303007v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence   Alignment",
        "authors": [
            "Regina Barzilay",
            "Lillian Lee"
        ],
        "summary": "We address the text-to-text generation problem of sentence-level paraphrasing -- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.",
        "published": "2003-04-02T23:02:44Z",
        "link": "http://arxiv.org/abs/cs/0304006v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Blind Normalization of Speech From Different Channels",
        "authors": [
            "David N. Levin"
        ],
        "summary": "We show how to construct a channel-independent representation of speech that has propagated through a noisy reverberant channel. This is done by blindly rescaling the cepstral time series by a non-linear function, with the form of this scale function being determined by previously encountered cepstra from that channel. The rescaled form of the time series is an invariant property of it in the following sense: it is unaffected if the time series is transformed by any time-independent invertible distortion. Because a linear channel with stationary noise and impulse response transforms cepstra in this way, the new technique can be used to remove the channel dependence of a cepstral time series. In experiments, the method achieved greater channel-independence than cepstral mean normalization, and it was comparable to the combination of cepstral mean normalization and spectral subtraction, despite the fact that no measurements of channel noise or reverberations were required (unlike spectral subtraction).",
        "published": "2003-04-10T22:25:01Z",
        "link": "http://arxiv.org/abs/cs/0304019v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Glottochronologic Retrognostic of Language System",
        "authors": [
            "Kromer Victor"
        ],
        "summary": "A glottochronologic retrognostic of language system is proposed",
        "published": "2003-04-17T02:22:06Z",
        "link": "http://arxiv.org/abs/cs/0304024v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "\"I'm sorry Dave, I'm afraid I can't do that\": Linguistics, Statistics,   and Natural Language Processing circa 2001",
        "authors": [
            "Lillian Lee"
        ],
        "summary": "A brief, general-audience overview of the history of natural language processing, focusing on data-driven approaches.Topics include \"Ambiguity and language analysis\", \"Firth things first\", \"A 'C' change\", and \"The empiricists strike back\".",
        "published": "2003-04-21T22:10:21Z",
        "link": "http://arxiv.org/abs/cs/0304027v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Grid-Enabling Natural Language Engineering By Stealth",
        "authors": [
            "Baden Hughes",
            "Steven Bird"
        ],
        "summary": "We describe a proposal for an extensible, component-based software architecture for natural language engineering applications. Our model leverages existing linguistic resource description and discovery mechanisms based on extended Dublin Core metadata. In addition, the application design is flexible, allowing disparate components to be combined to suit the overall application functionality. An application specification language provides abstraction from the programming environment and allows ease of interface with computational grids via a broker.",
        "published": "2003-04-22T02:51:51Z",
        "link": "http://arxiv.org/abs/cs/0304028v1",
        "categories": [
            "cs.DC",
            "cs.CL",
            "J.5; D.1; C.2"
        ]
    },
    {
        "title": "An XML based Document Suite",
        "authors": [
            "Dietmar Roesner",
            "Manuela Kunze"
        ],
        "summary": "We report about the current state of development of a document suite and its applications. This collection of tools for the flexible and robust processing of documents in German is based on the use of XML as unifying formalism for encoding input and output data as well as process information. It is organized in modules with limited responsibilities that can easily be combined into pipelines to solve complex tasks. Strong emphasis is laid on a number of techniques to deal with lexical and conceptual gaps that are typical when starting a new application.",
        "published": "2003-04-22T13:45:37Z",
        "link": "http://arxiv.org/abs/cs/0304029v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.1"
        ]
    },
    {
        "title": "Exploiting Sublanguage and Domain Characteristics in a Bootstrapping   Approach to Lexicon and Ontology Creation",
        "authors": [
            "Dietmar Roesner",
            "Manuela Kunze"
        ],
        "summary": "It is very costly to build up lexical resources and domain ontologies. Especially when confronted with a new application domain lexical gaps and a poor coverage of domain concepts are a problem for the successful exploitation of natural language document analysis systems that need and exploit such knowledge sources. In this paper we report about ongoing experiments with `bootstrapping techniques' for lexicon and ontology creation.",
        "published": "2003-04-23T08:02:53Z",
        "link": "http://arxiv.org/abs/cs/0304035v1",
        "categories": [
            "cs.CL",
            "H.3.1; I.2.7"
        ]
    },
    {
        "title": "An Approach for Resource Sharing in Multilingual NLP",
        "authors": [
            "Manuela Kunze",
            "Chun Xiao"
        ],
        "summary": "In this paper we describe an approach for the analysis of documents in German and English with a shared pool of resources. For the analysis of German documents we use a document suite, which supports the user in tasks like information retrieval and information extraction. The core of the document suite is based on our tool XDOC. Now we want to exploit these methods for the analysis of English documents as well. For this aim we need a multilingual presentation format of the resources. These resources must be transformed into an unified format, in which we can set additional information about linguistic characteristics of the language depending on the analyzed documents. In this paper we describe our approach for such an exchange model for multilingual resources based on XML.",
        "published": "2003-04-23T08:32:19Z",
        "link": "http://arxiv.org/abs/cs/0304036v1",
        "categories": [
            "cs.CL",
            "H3.1; I.2.7"
        ]
    },
    {
        "title": "Approximate Grammar for Information Extraction",
        "authors": [
            "V. Sriram",
            "B. Ravi Sekar Reddy",
            "R. Sangal"
        ],
        "summary": "In this paper, we present the concept of Approximate grammar and how it can be used to extract information from a documemt. As the structure of informational strings cannot be defined well in a document, we cannot use the conventional grammar rules to represent the information. Hence, the need arises to design an approximate grammar that can be used effectively to accomplish the task of Information extraction. Approximate grammars are a novel step in this direction. The rules of an approximate grammar can be given by a user or the machine can learn the rules from an annotated document. We have performed our experiments in both the above areas and the results have been impressive.",
        "published": "2003-05-06T14:06:48Z",
        "link": "http://arxiv.org/abs/cs/0305004v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Factorization of Language Models through Backing-Off Lattices",
        "authors": [
            "Wei Wang"
        ],
        "summary": "Factorization of statistical language models is the task that we resolve the most discriminative model into factored models and determine a new model by combining them so as to provide better estimate. Most of previous works mainly focus on factorizing models of sequential events, each of which allows only one factorization manner. To enable parallel factorization, which allows a model event to be resolved in more than one ways at the same time, we propose a general framework, where we adopt a backing-off lattice to reflect parallel factorizations and to define the paths along which a model is resolved into factored models, we use a mixture model to combine parallel paths in the lattice, and generalize Katz's backing-off method to integrate all the mixture models got by traversing the entire lattice. Based on this framework, we formulate two types of model factorizations that are used in natural language modeling.",
        "published": "2003-05-23T19:38:22Z",
        "link": "http://arxiv.org/abs/cs/0305041v2",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Developing Open Data Models for Linguistic Field Data",
        "authors": [
            "Baden Hughes"
        ],
        "summary": "The UQ Flint Archive houses the field notes and elicitation recordings made by Elwyn Flint in the 1950's and 1960's during extensive linguistic survey work across Queensland, Australia.   The process of digitizing the contents of the UQ Flint Archive provides a number of interesting challenges in the context of EMELD. Firstly, all of the linguistic data is for languages which are either endangered or extinct, and as such forms a valuable ethnographic repository. Secondly, the physical format of the data is itself in danger of decline, and as such digitization is an important preservation task in the short to medium term. Thirdly, the adoption of open standards for the encoding and presentation of text and audio data for linguistic field data, whilst enabling preservation, represents a new field of research in itself where best practice has yet to be formalised. Fourthly, the provision of this linguistic data online as a new data source for future research introduces concerns of data portability and longevity.   This paper will outline the origins of the data model, the content creation components, presentation forms based on the data model, data capture tools and media conversion components. It will also address some of the larger questions regarding the digitization and annotation of linguistic field work based on experience gained through work with the Flint Archive contents.",
        "published": "2003-05-29T12:12:19Z",
        "link": "http://arxiv.org/abs/cs/0305053v1",
        "categories": [
            "cs.DL",
            "cs.CL",
            "H.3.7; H.2.4; H.2.1; E.2; D.2.11"
        ]
    },
    {
        "title": "Techniques for effective vocabulary selection",
        "authors": [
            "Anand Venkataraman",
            "Wen Wang"
        ],
        "summary": "The vocabulary of a continuous speech recognition (CSR) system is a significant factor in determining its performance. In this paper, we present three principled approaches to select the target vocabulary for a particular domain by trading off between the target out-of-vocabulary (OOV) rate and vocabulary size. We evaluate these approaches against an ad-hoc baseline strategy. Results are presented in the form of OOV rate graphs plotted against increasing vocabulary size for each technique.",
        "published": "2003-06-04T23:08:03Z",
        "link": "http://arxiv.org/abs/cs/0306022v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "Bayesian Information Extraction Network",
        "authors": [
            "Leonid Peshkin",
            "Avi Pfeffer"
        ],
        "summary": "Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various aspects of language in one model. Many existing algorithms developed for learning and inference in DBNs are applicable to probabilistic language modeling. To demonstrate the potential of DBNs for natural language processing, we employ a DBN in an information extraction task. We show how to assemble wealth of emerging linguistic instruments for shallow parsing, syntactic and semantic tagging, morphological decomposition, named entity recognition etc. in order to incrementally build a robust information extraction system. Our method outperforms previously published results on an established benchmark domain.",
        "published": "2003-06-10T04:40:45Z",
        "link": "http://arxiv.org/abs/cs/0306039v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "C.1.3; I.5.1; I.7.2; I.2.7"
        ]
    },
    {
        "title": "The Open Language Archives Community: An infrastructure for distributed   archiving of language resources",
        "authors": [
            "Gary Simons",
            "Steven Bird"
        ],
        "summary": "New ways of documenting and describing language via electronic media coupled with new ways of distributing the results via the World-Wide Web offer a degree of access to language resources that is unparalleled in history. At the same time, the proliferation of approaches to using these new technologies is causing serious problems relating to resource discovery and resource creation. This article describes the infrastructure that the Open Language Archives Community (OLAC) has built in order to address these problems. Its technical and usage infrastructures address problems of resource discovery by constructing a single virtual library of distributed resources. Its governance infrastructure addresses problems of resource creation by providing a mechanism through which the language-resource community can express its consensus on recommended best practices.",
        "published": "2003-06-10T07:33:32Z",
        "link": "http://arxiv.org/abs/cs/0306040v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named   Entity Recognition",
        "authors": [
            "Erik F. Tjong Kim Sang",
            "Fien De Meulder"
        ],
        "summary": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.",
        "published": "2003-06-12T12:35:00Z",
        "link": "http://arxiv.org/abs/cs/0306050v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Learning to Order Facts for Discourse Planning in Natural Language   Generation",
        "authors": [
            "Aggeliki Dimitromanolaki",
            "Ion Androutsopoulos"
        ],
        "summary": "This paper presents a machine learning approach to discourse planning in natural language generation. More specifically, we address the problem of learning the most natural ordering of facts in discourse plans for a specific domain. We discuss our methodology and how it was instantiated using two different machine learning algorithms. A quantitative evaluation performed in the domain of museum exhibit descriptions indicates that our approach performs significantly better than manually constructed ordering rules. Being retrainable, the resulting planners can be ported easily to other similar domains, without requiring language technology expertise.",
        "published": "2003-06-13T09:05:10Z",
        "link": "http://arxiv.org/abs/cs/0306062v1",
        "categories": [
            "cs.CL",
            "H.5.2"
        ]
    },
    {
        "title": "An Improved k-Nearest Neighbor Algorithm for Text Categorization",
        "authors": [
            "Baoli Li",
            "Shiwen Yu",
            "Qin Lu"
        ],
        "summary": "k is the most important parameter in a text categorization system based on k-Nearest Neighbor algorithm (kNN).In the classification process, k nearest documents to the test one in the training set are determined firstly. Then, the predication can be made according to the category distribution among these k nearest neighbors. Generally speaking, the class distribution in the training set is uneven. Some classes may have more samples than others. Therefore, the system performance is very sensitive to the choice of the parameter k. And it is very likely that a fixed k value will result in a bias on large categories. To deal with these problems, we propose an improved kNN algorithm, which uses different numbers of nearest neighbors for different categories, rather than a fixed number across all categories. More samples (nearest neighbors) will be used for deciding whether a test document should be classified to a category, which has more samples in the training set. Preliminary experiments on Chinese text categorization show that our method is less sensitive to the parameter k than the traditional one, and it can properly classify documents belonging to smaller classes with a large k. The method is promising for some cases, where estimating the parameter k via cross-validation is not allowed.",
        "published": "2003-06-16T13:54:03Z",
        "link": "http://arxiv.org/abs/cs/0306099v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3"
        ]
    },
    {
        "title": "Anusaaraka: Machine Translation in Stages",
        "authors": [
            "Akshar Bharati",
            "Vineet Chaitanya",
            "Amba P. Kulkarni",
            "Rajeev Sangal"
        ],
        "summary": "Fully-automatic general-purpose high-quality machine translation systems (FGH-MT) are extremely difficult to build. In fact, there is no system in the world for any pair of languages which qualifies to be called FGH-MT. The reasons are not far to seek. Translation is a creative process which involves interpretation of the given text by the translator. Translation would also vary depending on the audience and the purpose for which it is meant. This would explain the difficulty of building a machine translation system. Since, the machine is not capable of interpreting a general text with sufficient accuracy automatically at present - let alone re-expressing it for a given audience, it fails to perform as FGH-MT. FOOTNOTE{The major difficulty that the machine faces in interpreting a given text is the lack of general world knowledge or common sense knowledge.}",
        "published": "2003-06-25T10:26:29Z",
        "link": "http://arxiv.org/abs/cs/0306130v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Syntax, Parsing and Production of Natural Language in a Framework of   Information Compression by Multiple Alignment, Unification and Search",
        "authors": [
            "J Gerard Wolff"
        ],
        "summary": "This article introduces the idea that \"information compression by multiple alignment, unification and search\" (ICMAUS) provides a framework within which natural language syntax may be represented in a simple format and the parsing and production of natural language may be performed in a transparent manner.   The ICMAUS concepts are embodied in a software model, SP61. The organisation and operation of the model are described and a simple example is presented showing how the model can achieve parsing of natural language.   Notwithstanding the apparent paradox of 'decompression by compression', the ICMAUS framework, without any modification, can produce a sentence by decoding a compressed code for the sentence. This is illustrated with output from the SP61 model.   The article includes four other examples - one of the parsing of a sentence in French and three from the domain of English auxiliary verbs. These examples show how the ICMAUS framework and the SP61 model can accommodate 'context sensitive' features of syntax in a relatively simple and direct manner.",
        "published": "2003-07-07T08:47:53Z",
        "link": "http://arxiv.org/abs/cs/0307014v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Issues in Communication Game",
        "authors": [
            "Koiti Hasida"
        ],
        "summary": "As interaction between autonomous agents, communication can be analyzed in game-theoretic terms. Meaning game is proposed to formalize the core of intended communication in which the sender sends a message and the receiver attempts to infer its meaning intended by the sender. Basic issues involved in the game of natural language communication are discussed, such as salience, grammaticality, common sense, and common belief, together with some demonstration of the feasibility of game-theoretic account of language.",
        "published": "2003-07-11T14:43:51Z",
        "link": "http://arxiv.org/abs/cs/0307028v1",
        "categories": [
            "cs.CL",
            "J.5; I.2.7"
        ]
    },
    {
        "title": "Parsing and Generation with Tabulation and Compilation",
        "authors": [
            "Koiti Hasida",
            "Takashi Miyata"
        ],
        "summary": "The standard tabulation techniques for logic programming presuppose fixed order of computation. Some data-driven control should be introduced in order to deal with diverse contexts. The present paper describes a data-driven method of constraint transformation with a sort of compilation which subsumes accessibility check and last-call optimization, which characterize standard natural-language parsing techniques, semantic-head-driven generation, etc.",
        "published": "2003-07-11T15:42:51Z",
        "link": "http://arxiv.org/abs/cs/0307030v1",
        "categories": [
            "cs.CL",
            "I.2.7; D.1.6"
        ]
    },
    {
        "title": "The Linguistic DS: Linguisitic Description in MPEG-7",
        "authors": [
            "Koiti Hasida"
        ],
        "summary": "MPEG-7 (Moving Picture Experts Group Phase 7) is an XML-based international standard on semantic description of multimedia content. This document discusses the Linguistic DS and related tools. The linguistic DS is a tool, based on the GDA tag set (http://i-content.org/GDA/tagset.html), for semantic annotation of linguistic data in or associated with multimedia content. The current document text reflects `Study of FPDAM - MPEG-7 MDS Extensions' issued in March 2003, and not most part of MPEG-7 MDS, for which the readers are referred to the first version of MPEG-7 MDS document available from ISO (http://www.iso.org). Without that reference, however, this document should be mostly intelligible to those who are familiar with XML and linguistic theories. Comments are welcome and will be considered in the standardization process.",
        "published": "2003-07-19T12:24:33Z",
        "link": "http://arxiv.org/abs/cs/0307044v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Symbolic stochastic dynamical systems viewed as binary N-step Markov   chains",
        "authors": [
            "O. V. Usatenko",
            "V. A. Yampol'skii",
            "K. E. Kechedzhy",
            "S. S. Mel'nyk"
        ],
        "summary": "A theory of systems with long-range correlations based on the consideration of binary N-step Markov chains is developed. In the model, the conditional probability that the i-th symbol in the chain equals zero (or unity) is a linear function of the number of unities among the preceding N symbols. The correlation and distribution functions as well as the variance of number of symbols in the words of arbitrary length L are obtained analytically and numerically. A self-similarity of the studied stochastic process is revealed and the similarity group transformation of the chain parameters is presented. The diffusion Fokker-Planck equation governing the distribution function of the L-words is explored. If the persistent correlations are not extremely strong, the distribution function is shown to be the Gaussian with the variance being nonlinearly dependent on L. The applicability of the developed theory to the coarse-grained written and DNA texts is discussed.",
        "published": "2003-07-23T15:51:34Z",
        "link": "http://arxiv.org/abs/physics/0307117v1",
        "categories": [
            "physics.data-an",
            "cond-mat.stat-mech",
            "cs.CL",
            "math-ph",
            "math.MP",
            "nlin.AO",
            "physics.class-ph"
        ]
    },
    {
        "title": "Learning Analogies and Semantic Relations",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal analogy has the form A:B::C:D, meaning \"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct). We motivate this research by relating it to work in cognitive science and linguistics, and by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \"laser printer\", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for these challenging problems.",
        "published": "2003-07-24T21:09:43Z",
        "link": "http://arxiv.org/abs/cs/0307055v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "A Grid Based Architecture for High-Performance NLP",
        "authors": [
            "Baden Hughes",
            "Steven Bird"
        ],
        "summary": "We describe the design and early implementation of an extensible, component-based software architecture for natural language engineering applications which interfaces with high performance distributed computing services. The architecture leverages existing linguistic resource description and discovery mechanisms based on metadata descriptions, combining these in a compatible fashion with other software definition abstractions. Within this architecture, application design is highly flexible, allowing disparate components to be combined to suit the overall application functionality, and formally described independently of processing concerns. An application specification language provides abstraction from the programming environment and allows ease of interface with high performance computational grids via a broker.",
        "published": "2003-08-05T00:46:08Z",
        "link": "http://arxiv.org/abs/cs/0308008v1",
        "categories": [
            "cs.DC",
            "cs.CL",
            "J.5; D.1; C.2"
        ]
    },
    {
        "title": "Collaborative Creation of Digital Content in Indian Languages",
        "authors": [
            "Akshar Bharati",
            "Rajeev Sangal"
        ],
        "summary": "The world is passing through a major revolution called the information revolution, in which information and knowledge is becoming available to people in unprecedented amounts wherever and whenever they need it. Those societies which fail to take advantage of the new technology will be left behind, just like in the industrial revolution.   The information revolution is based on two major technologies: computers and communication. These technologies have to be delivered in a COST EFFECTIVE manner, and in LANGUAGES accessible to people.   One way to deliver them in cost effective manner is to make suitable technology choices, and to allow people to access through shared resources. This could be done throuch street corner shops (for computer usage, e-mail etc.), schools, community centres and local library centres.",
        "published": "2003-08-07T09:01:56Z",
        "link": "http://arxiv.org/abs/cs/0308016v1",
        "categories": [
            "cs.CL",
            "I,2,7"
        ]
    },
    {
        "title": "Information Revolution",
        "authors": [
            "Akshar Bharati",
            "Vineet Chaitanya",
            "Rajeev Sangal"
        ],
        "summary": "The world is passing through a major revolution called the information revolution, in which information and knowledge is becoming available to people in unprecedented amounts wherever and whenever they need it. Those societies which fail to take advantage of the new technology will be left behind, just like in the industrial revolution.   The information revolution is based on two major technologies: computers and communication. These technologies have to be delivered in a COST EFFECTIVE manner, and in LANGUAGES accessible to people.   One way to deliver them in cost effective manner is to make suitable technology choices (discussed later), and to allow people to access through shared resources. This could be done throuch street corner shops (for computer usage, e-mail etc.), schools, community centers and local library centres.",
        "published": "2003-08-07T09:17:16Z",
        "link": "http://arxiv.org/abs/cs/0308017v1",
        "categories": [
            "cs.CL",
            "I,2,7"
        ]
    },
    {
        "title": "Anusaaraka: Overcoming the Language Barrier in India",
        "authors": [
            "Akshar Bharati",
            "Vineet Chaitanya",
            "Amba P. Kulkarni",
            "Rajeev Sangal",
            "G Umamaheshwara Rao"
        ],
        "summary": "The anusaaraka system makes text in one Indian language accessible in another Indian language. In the anusaaraka approach, the load is so divided between man and computer that the language load is taken by the machine, and the interpretation of the text is left to the man. The machine presents an image of the source text in a language close to the target language.In the image, some constructions of the source language (which do not have equivalents) spill over to the output. Some special notation is also devised. The user after some training learns to read and understand the output. Because the Indian languages are close, the learning time of the output language is short, and is expected to be around 2 weeks.   The output can also be post-edited by a trained user to make it grammatically correct in the target language. Style can also be changed, if necessary. Thus, in this scenario, it can function as a human assisted translation system.   Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali and Punjabi to Hindi. They can be built for all Indian languages in the near future. Everybody must pitch in to build such systems connecting all Indian languages, using the free software model.",
        "published": "2003-08-07T09:24:46Z",
        "link": "http://arxiv.org/abs/cs/0308018v1",
        "categories": [
            "cs.CL",
            "I,2,7"
        ]
    },
    {
        "title": "Language Access: An Information Based Approach",
        "authors": [
            "Akshar Bharati",
            "Vineet Chaitanya",
            "Amba P. Kulkarni",
            "Rajeev Sangal"
        ],
        "summary": "The anusaaraka system (a kind of machine translation system) makes text in one Indian language accessible through another Indian language. The machine presents an image of the source text in a language close to the target language. In the image, some constructions of the source language (which do not have equivalents in the target language) spill over to the output. Some special notation is also devised.   Anusaarakas have been built from five pairs of languages: Telugu,Kannada, Marathi, Bengali and Punjabi to Hindi. They are available for use through Email servers.   Anusaarkas follows the principle of substitutibility and reversibility of strings produced. This implies preservation of information while going from a source language to a target language.   For narrow subject areas, specialized modules can be built by putting subject domain knowledge into the system, which produce good quality grammatical output. However, it should be remembered, that such modules will work only in narrow areas, and will sometimes go wrong. In such a situation, anusaaraka output will still remain useful.",
        "published": "2003-08-07T09:40:04Z",
        "link": "http://arxiv.org/abs/cs/0308019v1",
        "categories": [
            "cs.CL",
            "I,2,7"
        ]
    },
    {
        "title": "LERIL : Collaborative Effort for Creating Lexical Resources",
        "authors": [
            "Akshar Bharati",
            "Dipti M Sharma",
            "Vineet Chaitanya",
            "Amba P Kulkarni",
            "Rajeev Sangal",
            "Durgesh D Rao"
        ],
        "summary": "The paper reports on efforts taken to create lexical resources pertaining to Indian languages, using the collaborative model. The lexical resources being developed are: (1) Transfer lexicon and grammar from English to several Indian languages. (2) Dependencey tree bank of annotated corpora for several Indian languages. The dependency trees are based on the Paninian model. (3) Bilingual dictionary of 'core meanings'.",
        "published": "2003-08-07T10:08:43Z",
        "link": "http://arxiv.org/abs/cs/0308020v1",
        "categories": [
            "cs.CL",
            "I,2,7"
        ]
    },
    {
        "title": "Extending Dublin Core Metadata to Support the Description and Discovery   of Language Resources",
        "authors": [
            "Steven Bird",
            "Gary Simons"
        ],
        "summary": "As language data and associated technologies proliferate and as the language resources community expands, it is becoming increasingly difficult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool works with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to find language resources. This paper reports on a new digital infrastructure for discovering language resources being developed by the Open Language Archives Community (OLAC). At the core of OLAC is its metadata format, which is designed to facilitate description and discovery of all kinds of language resources, including data, tools, or advice. The paper describes OLAC metadata, its relationship to Dublin Core metadata, and its dissemination using the metadata harvesting protocol of the Open Archives Initiative.",
        "published": "2003-08-14T23:28:21Z",
        "link": "http://arxiv.org/abs/cs/0308022v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Evaluation of text data mining for database curation: lessons learned   from the KDD Challenge Cup",
        "authors": [
            "Alexander S. Yeh",
            "Lynette Hirschman",
            "Alexander A. Morgan"
        ],
        "summary": "MOTIVATION: The biological literature is a major repository of knowledge. Many biological databases draw much of their content from a careful curation of this literature. However, as the volume of literature increases, the burden of curation increases. Text mining may provide useful tools to assist in the curation process. To date, the lack of standards has made it impossible to determine whether text mining techniques are sufficiently mature to be useful.   RESULTS: We report on a Challenge Evaluation task that we created for the Knowledge Discovery and Data Mining (KDD) Challenge Cup. We provided a training corpus of 862 articles consisting of journal articles curated in FlyBase, along with the associated lists of genes and gene products, as well as the relevant data fields from FlyBase. For the test, we provided a corpus of 213 new (`blind') articles; the 18 participating groups provided systems that flagged articles for curation, based on whether the article contained experimental evidence for gene expression products. We report on the the evaluation results and describe the techniques used by the top performing groups.   CONTACT: asy@mitre.org   KEYWORDS: text mining, evaluation, curation, genomics, data management",
        "published": "2003-08-20T18:40:39Z",
        "link": "http://arxiv.org/abs/cs/0308032v1",
        "categories": [
            "cs.CL",
            "q-bio.OT",
            "I.2.7; J.3"
        ]
    },
    {
        "title": "Coherent Keyphrase Extraction via Web Mining",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. A limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent. That is, the majority of the output keyphrases may fit together well, but there may be a minority that appear to be outliers, with no clear semantic relation to the majority or to each other. This paper presents enhancements to the Kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. The approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related. The statistical association is measured using web mining. Experiments demonstrate that the enhancements improve the quality of the extracted keyphrases. Furthermore, the enhancements are not domain-specific: the algorithm generalizes well when it is trained on one domain (computer science documents) and tested on another (physics documents).",
        "published": "2003-08-20T20:42:19Z",
        "link": "http://arxiv.org/abs/cs/0308033v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Quantum Aspects of Semantic Analysis and Symbolic Artificial   Intelligence",
        "authors": [
            "Diederik Aerts",
            "Marek Czachor"
        ],
        "summary": "Modern approaches to semanic analysis if reformulated as Hilbert-space problems reveal formal structures known from quantum mechanics. Similar situation is found in distributed representations of cognitive structures developed for the purposes of neural networks. We take a closer look at similarites and differences between the above two fields and quantum information theory.",
        "published": "2003-09-01T19:10:55Z",
        "link": "http://arxiv.org/abs/quant-ph/0309022v4",
        "categories": [
            "quant-ph",
            "cs.CL"
        ]
    },
    {
        "title": "Building a Test Collection for Speech-Driven Web Retrieval",
        "authors": [
            "Atsushi Fujii",
            "Katunobu Itou"
        ],
        "summary": "This paper describes a test collection (benchmark data) for retrieval systems driven by spoken queries. This collection was produced in the subtask of the NTCIR-3 Web retrieval task, which was performed in a TREC-style evaluation workshop. The search topics and document collection for the Web retrieval task were used to produce spoken queries and language models for speech recognition, respectively. We used this collection to evaluate the performance of our retrieval system. Experimental results showed that (a) the use of target documents for language modeling and (b) enhancement of the vocabulary size in speech recognition were effective in improving the system performance.",
        "published": "2003-09-12T12:43:00Z",
        "link": "http://arxiv.org/abs/cs/0309019v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.5.1"
        ]
    },
    {
        "title": "A Cross-media Retrieval System for Lecture Videos",
        "authors": [
            "Atsushi Fujii",
            "Katunobu Itou",
            "Tomoyosi Akiba",
            "Tetsuya Ishikawa"
        ],
        "summary": "We propose a cross-media lecture-on-demand system, in which users can selectively view specific segments of lecture videos by submitting text queries. Users can easily formulate queries by using the textbook associated with a target lecture, even if they cannot come up with effective keywords. Our system extracts the audio track from a target lecture video, generates a transcription by large vocabulary continuous speech recognition, and produces a text index. Experimental results showed that by adapting speech recognition to the topic of the lecture, the recognition accuracy increased and the retrieval accuracy was comparable with that obtained by human transcription.",
        "published": "2003-09-13T06:54:58Z",
        "link": "http://arxiv.org/abs/cs/0309021v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.5.1"
        ]
    },
    {
        "title": "Measuring Praise and Criticism: Inference of Semantic Orientation from   Association",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., \"honest\", \"intrepid\") and negative semantic orientation indicates criticism (e.g., \"disturbing\", \"superfluous\"). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots). This paper introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words. Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA). The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words). The method attains an accuracy of 82.8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words.",
        "published": "2003-09-19T16:30:55Z",
        "link": "http://arxiv.org/abs/cs/0309034v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Combining Independent Modules to Solve Multiple-choice Synonym and   Analogy Problems",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman",
            "Jeffrey Bigham",
            "Victor Shnayder"
        ],
        "summary": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the well known mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",
        "published": "2003-09-19T20:13:07Z",
        "link": "http://arxiv.org/abs/cs/0309035v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "I.2.6; I.2.7; H.3.1; J.5"
        ]
    },
    {
        "title": "Effective XML Representation for Spoken Language in Organisations",
        "authors": [
            "Rodney J. Clarke",
            "Philip C. Windridge",
            "Dali Dong"
        ],
        "summary": "Spoken Language can be used to provide insights into organisational processes, unfortunately transcription and coding stages are very time consuming and expensive. The concept of partial transcription and coding is proposed in which spoken language is indexed prior to any subsequent processing. The functional linguistic theory of texture is used to describe the effects of partial transcription on observational records. The standard used to encode transcript context and metadata is called CHAT, but a previous XML schema developed to implement it contains design assumptions that make it difficult to support partial transcription for example. This paper describes a more effective XML schema that overcomes many of these problems and is intended for use in applications that support the rapid development of spoken language deliverables.",
        "published": "2003-10-08T16:40:33Z",
        "link": "http://arxiv.org/abs/cs/0310014v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "The Study of the Application of a Keywords-based Chatbot System on the   Teaching of Foreign Languages",
        "authors": [
            "Jiyou Jia"
        ],
        "summary": "This paper reports the findings of a study conducted on the application of an on-line human-computer dialog system with natural language (chatbot) on the teaching of foreign languages. A keywords-based human-computer dialog system makes it possible that the user could chat with the computer using a natural language, i.e. in English or in German to some extent. So an experiment has been made using this system online to work as a chat partner with the users learning the foreign languages. Dialogs between the users and the chatbot are collected. Findings indicate that the dialogs between the human and the computer are mostly very short because the user finds the responses from the computer are mostly repeated and irrelevant with the topics and context and the program does not understand the language at all. With analysis of the keywords or pattern-matching mechanism used in this chatbot it can be concluded that this kind of system can not work as a teaching assistant program in foreign language learning.",
        "published": "2003-10-10T15:25:57Z",
        "link": "http://arxiv.org/abs/cs/0310018v1",
        "categories": [
            "cs.CY",
            "cs.CL",
            "K.3.1;I.2.7"
        ]
    },
    {
        "title": "A Dynamic Programming Algorithm for the Segmentation of Greek Texts",
        "authors": [
            "Pavlina Fragkou"
        ],
        "summary": "In this paper we introduce a dynamic programming algorithm to perform linear text segmentation by global minimization of a segmentation cost function which consists of: (a) within-segment word similarity and (b) prior information about segment length. The evaluation of the segmentation accuracy of the algorithm on a text collection consisting of Greek texts showed that the algorithm achieves high segmentation accuracy and appears to be very innovating and promissing.",
        "published": "2003-10-21T18:19:52Z",
        "link": "http://arxiv.org/abs/cs/0310041v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.3.1; H.3.3; H.3.7"
        ]
    },
    {
        "title": "Application Architecture for Spoken Language Resources in Organisational   Settings",
        "authors": [
            "Rodney J. Clarke",
            "Dali Dong",
            "Philip C. Windridge"
        ],
        "summary": "Special technologies need to be used to take advantage of, and overcome, the challenges associated with acquiring, transforming, storing, processing, and distributing spoken language resources in organisations. This paper introduces an application architecture consisting of tools and supporting utilities for indexing and transcription, and describes how these tools, together with downstream processing and distribution systems, can be integrated into a workflow. Two sample applications for this architecture are outlined- the analysis of decision-making processes in organisations and the deployment of systems development methods by designers in the field.",
        "published": "2003-10-29T20:13:30Z",
        "link": "http://arxiv.org/abs/cs/0310058v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "The Rank-Frequency Analysis for the Functional Style Corpora in the   Ukrainian Language",
        "authors": [
            "Solomija N. Buk",
            "Andrij A. Rovenchak"
        ],
        "summary": "We use the rank-frequency analysis for the estimation of Kernel Vocabulary size within specific corpora of Ukrainian. The extrapolation of high-rank behaviour is utilized for estimation of the total vocabulary size.",
        "published": "2003-11-21T19:48:17Z",
        "link": "http://arxiv.org/abs/cs/0311033v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Measuring the Functional Load of Phonological Contrasts",
        "authors": [
            "Dinoj Surendran",
            "Partha Niyogi"
        ],
        "summary": "Frequency counts are a measure of how much use a language makes of a linguistic unit, such as a phoneme or word. However, what is often important is not the units themselves, but the contrasts between them. A measure is therefore needed for how much use a language makes of a contrast, i.e. the functional load (FL) of the contrast. We generalize previous work in linguistics and speech recognition and propose a family of measures for the FL of several phonological contrasts, including phonemic oppositions, distinctive features, suprasegmentals, and phonological rules. We then test it for robustness to changes of corpora. Finally, we provide examples in Cantonese, Dutch, English, German and Mandarin, in the context of historical linguistics, language acquisition and speech recognition. More information can be found at http://dinoj.info/research/fload",
        "published": "2003-11-24T18:05:40Z",
        "link": "http://arxiv.org/abs/cs/0311036v1",
        "categories": [
            "cs.CL",
            "I.2.6;I.2.7;J.5"
        ]
    },
    {
        "title": "Embedding Web-based Statistical Translation Models in Cross-Language   Information Retrieval",
        "authors": [
            "Wessel Kraaij",
            "Jian-Yun Nie",
            "Michel Simard"
        ],
        "summary": "Although more and more language pairs are covered by machine translation services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application which needs translation functionality of a relatively low level of sophistication since current models for information retrieval (IR) are still based on a bag-of-words. The Web provides a vast resource for the automatic construction of parallel corpora which can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this paper, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.",
        "published": "2003-12-03T13:09:28Z",
        "link": "http://arxiv.org/abs/cs/0312008v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.7"
        ]
    },
    {
        "title": "A Flexible Pragmatics-driven Language Generator for Animated Agents",
        "authors": [
            "Paul Piwek"
        ],
        "summary": "This paper describes the NECA MNLG; a fully implemented Multimodal Natural Language Generation module. The MNLG is deployed as part of the NECA system which generates dialogues between animated agents. The generation module supports the seamless integration of full grammar rules, templates and canned text. The generator takes input which allows for the specification of syntactic, semantic and pragmatic constraints on the output.",
        "published": "2003-12-22T16:23:34Z",
        "link": "http://arxiv.org/abs/cs/0312050v1",
        "categories": [
            "cs.CL",
            "cs.MM",
            "I.2.7"
        ]
    },
    {
        "title": "Towards Automated Generation of Scripted Dialogue: Some Time-Honoured   Strategies",
        "authors": [
            "Paul Piwek",
            "Kees van Deemter"
        ],
        "summary": "The main aim of this paper is to introduce automated generation of scripted dialogue as a worthwhile topic of investigation. In particular the fact that scripted dialogue involves two layers of communication, i.e., uni-directional communication between the author and the audience of a scripted dialogue and bi-directional pretended communication between the characters featuring in the dialogue, is argued to raise some interesting issues. Our hope is that the combined study of the two layers will forge links between research in text generation and dialogue processing. The paper presents a first attempt at creating such links by studying three types of strategies for the automated generation of scripted dialogue. The strategies are derived from examples of human-authored and naturally occurring dialogue.",
        "published": "2003-12-22T16:51:53Z",
        "link": "http://arxiv.org/abs/cs/0312051v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Dialogue as Discourse: Controlling Global Properties of Scripted   Dialogue",
        "authors": [
            "Paul Piwek",
            "Kees van Deemter"
        ],
        "summary": "This paper explains why scripted dialogue shares some crucial properties with discourse. In particular, when scripted dialogues are generated by a Natural Language Generation system, the generator can apply revision strategies that cannot normally be used when the dialogue results from an interaction between autonomous agents (i.e., when the dialogue is not scripted). The paper explains that the relevant revision operators are best applied at the level of a dialogue plan and discusses how the generator may decide when to apply a given revision operator.",
        "published": "2003-12-22T17:07:31Z",
        "link": "http://arxiv.org/abs/cs/0312052v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Acquiring Lexical Paraphrases from a Single Corpus",
        "authors": [
            "Oren Glickman",
            "Ido Dagan"
        ],
        "summary": "This paper studies the potential of identifying lexical paraphrases within a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information. A comparison suggests that an instance-based approach may be combined with a vector based approach in order to assess better the paraphrase likelihood for many verb pairs.",
        "published": "2003-12-25T16:45:20Z",
        "link": "http://arxiv.org/abs/cs/0312058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "I.7"
        ]
    },
    {
        "title": "Part-of-Speech Tagging with Minimal Lexicalization",
        "authors": [
            "Virginia Savova",
            "Leonid Peshkin"
        ],
        "summary": "We use a Dynamic Bayesian Network to represent compactly a variety of sublexical and contextual features relevant to Part-of-Speech (PoS) tagging. The outcome is a flexible tagger (LegoTag) with state-of-the-art performance (3.6% error on a benchmark corpus). We explore the effect of eliminating redundancy and radically reducing the size of feature vocabularies. We find that a small but linguistically motivated set of suffixes results in improved cross-corpora generalization. We also show that a minimal lexicon limited to function words is sufficient to ensure reasonable performance.",
        "published": "2003-12-27T21:21:48Z",
        "link": "http://arxiv.org/abs/cs/0312060v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.7"
        ]
    },
    {
        "title": "Segmentation, Indexing, and Visualization of Extended Instructional   Videos",
        "authors": [
            "Alexander Haubold",
            "John R. Kender"
        ],
        "summary": "We present a new method for segmenting, and a new user interface for indexing and visualizing, the semantic content of extended instructional videos. Given a series of key frames from the video, we generate a condensed view of the data by clustering frames according to media type and visual similarities. Using various visual filters, key frames are first assigned a media type (board, class, computer, illustration, podium, and sheet). Key frames of media type board and sheet are then clustered based on contents via an algorithm with near-linear cost. A novel user interface, the result of two user studies, displays related topics using icons linked topologically, allowing users to quickly locate semantically related portions of the video. We analyze the accuracy of the segmentation tool on 17 instructional videos, each of which is from 75 to 150 minutes in duration (a total of 40 hours); the classification accuracy exceeds 96%.",
        "published": "2003-02-16T22:08:01Z",
        "link": "http://arxiv.org/abs/cs/0302023v1",
        "categories": [
            "cs.IR",
            "cs.CV",
            "H.3.1;H.3.3;I.4.8;I.5.3"
        ]
    },
    {
        "title": "Analysis and Interface for Instructional Video",
        "authors": [
            "Alexander Haubold",
            "John R. Kender"
        ],
        "summary": "We present a new method for segmenting, and a new user interface for indexing and visualizing, the semantic content of extended instructional videos. Using various visual filters, key frames are first assigned a media type (board, class, computer, illustration, podium, and sheet). Key frames of media type board and sheet are then clustered based on contents via an algorithm with near-linear cost. A novel user interface, the result of two user studies, displays related topics using icons linked topologically, allowing users to quickly locate semantically related portions of the video. We analyze the accuracy of the segmentation tool on 17 instructional videos, each of which is from 75 to 150 minutes in duration (a total of 40 hours); it exceeds 96%.",
        "published": "2003-02-16T22:13:42Z",
        "link": "http://arxiv.org/abs/cs/0302024v2",
        "categories": [
            "cs.IR",
            "cs.CV",
            "H.3.1;H.3.3;I.4.8;I.5.3"
        ]
    },
    {
        "title": "A Neural Network Assembly Memory Model with Maximum-Likelihood Recall   and Recognition Properties",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "It has been shown that a neural network model recently proposed to describe basic memory performance is based on a ternary/binary coding/decoding algorithm which leads to a new neural network assembly memory model (NNAMM) providing maximum-likelihood recall/recognition properties and implying a new memory unit architecture with Hopfield two-layer network, N-channel time gate, auxiliary reference memory, and two nested feedback loops. For the data coding used, conditions are found under which a version of Hopfied network implements maximum-likelihood convolutional decoding algorithm and, simultaneously, linear statistical classifier of arbitrary binary vectors with respect to Hamming distance between vector analyzed and reference vector given. In addition to basic memory performance and etc, the model explicitly describes the dependence on time of memory trace retrieval, gives a possibility of one-trial learning, metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. It has been shown that an assembly memory unit may be viewed as a model of a smallest inseparable part or an 'atom' of consciousness. Some nontraditional neurobiological backgrounds (dynamic spatiotemporal synchrony, properties of time dependent and error detector neurons, early precise spike firing, etc) and the model's application to solve some interdisciplinary problems from different scientific fields are discussed.",
        "published": "2003-03-19T23:17:16Z",
        "link": "http://arxiv.org/abs/cs/0303017v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "A Method for Clustering Web Attacks Using Edit Distance",
        "authors": [
            "Slobodan Petrovic",
            "Gonzalo Alvarez"
        ],
        "summary": "Cluster analysis often serves as the initial step in the process of data classification. In this paper, the problem of clustering different length input data is considered. The edit distance as the minimum number of elementary edit operations needed to transform one vector into another is used. A heuristic for clustering unequal length vectors, analogue to the well known k-means algorithm is described and analyzed. This heuristic determines cluster centroids expanding shorter vectors to the lengths of the longest ones in each cluster in a specific way. It is shown that the time and space complexities of the heuristic are linear in the number of input vectors. Experimental results on real data originating from a system for classification of Web attacks are given.",
        "published": "2003-04-03T10:17:02Z",
        "link": "http://arxiv.org/abs/cs/0304007v1",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CR",
            "H.3.3;K.6.5"
        ]
    },
    {
        "title": "Configuration Database for BaBar On-line",
        "authors": [
            "R. Bartoldus",
            "G. Dubois-Felsmann",
            "Y. Kolomensky",
            "A. Salnikov"
        ],
        "summary": "The configuration database is one of the vital systems in the BaBar on-line system. It provides services for the different parts of the data acquisition system and control system, which require run-time parameters. The original design and implementation of the configuration database played a significant role in the successful BaBar operations since the beginning of experiment. Recent additions to the design of the configuration database provide better means for the management of data and add new tools to simplify main configuration tasks. We describe the design of the configuration database, its implementation with the Objectivity/DB object-oriented database, and our experience collected during the years of operation.",
        "published": "2003-05-29T21:37:47Z",
        "link": "http://arxiv.org/abs/cs/0305056v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.2.4; H.2.8"
        ]
    },
    {
        "title": "Visualization for Periodic Population Movement between Distinct   Localities",
        "authors": [
            "Alexander Haubold"
        ],
        "summary": "We present a new visualization method to summarize and present periodic population movement between distinct locations, such as floors, buildings, cities, or the like. In the specific case of this paper, we have chosen to focus on student movement between college dormitories on the Columbia University campus. The visual information is presented to the information analyst in the form of an interactive geographical map, in which specific temporal periods as well as individual buildings can be singled out for detailed data exploration. The navigational interface has been designed to specifically meet a geographical setting.",
        "published": "2003-06-04T19:40:04Z",
        "link": "http://arxiv.org/abs/cs/0306021v2",
        "categories": [
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "BdbServer++: A User Driven Data Location and Retrieval Tool",
        "authors": [
            "A. D. Earl",
            "A. Hasan",
            "D. Boutigany"
        ],
        "summary": "The adoption of Grid technology has the potential to greatly aid the BaBar experiment. BdbServer was originally designed to extract copies of data from the Objectivity/DB database at SLAC and IN2P3. With data now stored in multiple locations in a variety of data formats, we are enhancing this tool. This will enable users to extract selected deep copies of event collections and ship them to the requested site using the facilities offered by the existing Grid infrastructure. By building on the work done by various groups in BaBar, and the European DataGrid, we have successfully expanded the capabilities of the BdbServer software. This should provide a framework for future work in data distribution.",
        "published": "2003-06-05T12:20:32Z",
        "link": "http://arxiv.org/abs/cs/0306026v1",
        "categories": [
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "Bayesian Information Extraction Network",
        "authors": [
            "Leonid Peshkin",
            "Avi Pfeffer"
        ],
        "summary": "Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various aspects of language in one model. Many existing algorithms developed for learning and inference in DBNs are applicable to probabilistic language modeling. To demonstrate the potential of DBNs for natural language processing, we employ a DBN in an information extraction task. We show how to assemble wealth of emerging linguistic instruments for shallow parsing, syntactic and semantic tagging, morphological decomposition, named entity recognition etc. in order to incrementally build a robust information extraction system. Our method outperforms previously published results on an established benchmark domain.",
        "published": "2003-06-10T04:40:45Z",
        "link": "http://arxiv.org/abs/cs/0306039v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "C.1.3; I.5.1; I.7.2; I.2.7"
        ]
    },
    {
        "title": "GMA Instrumentation of the Athena Framework using NetLogger",
        "authors": [
            "Craig E. Tull",
            "Dan Gunter",
            "Wim Lavrijsen",
            "David Quarrie",
            "Brian Tierney"
        ],
        "summary": "Grid applications are, by their nature, wide-area distributed applications. This WAN aspect of Grid applications makes the use of conventional monitoring and instrumentation tools (such as top, gprof, LSF Monitor, etc) impractical for verification that the application is running correctly and efficiently. To be effective, monitoring data must be \"end-to-end\", meaning that all components between the Grid application endpoints must be monitored. Instrumented applications can generate a large amount of monitoring data, so typically the instrumentation is off by default. For jobs running on a Grid, there needs to be a general mechanism to remotely activate the instrumentation in running jobs. The NetLogger Toolkit Activation Service provides this mechanism.   To demonstrate this, we have instrumented the ATLAS Athena Framework with NetLogger to generate monitoring events. We then use a GMA-based activation service to control NetLogger's trigger mechanism. The NetLogger trigger mechanism allows one to easily start, stop, or change the logging level of a running program by modifying a trigger file. We present here details of the design of the NetLogger implementation of the GMA-based activation service and the instrumentation service for Athena. We also describe how this activation service allows us to non-intrusively collect and visualize the ATLAS Athena Framework monitoring data.",
        "published": "2003-06-14T05:40:27Z",
        "link": "http://arxiv.org/abs/cs/0306086v1",
        "categories": [
            "cs.DC",
            "cs.IR",
            "C.2.2"
        ]
    },
    {
        "title": "BaBar - A Community Web Site in an Organizational Setting",
        "authors": [
            "Ray Cowan",
            "Yogesh Deshpande",
            "Bebo White"
        ],
        "summary": "The BABAR Web site was established in 1993 at the Stanford Linear Accelerator Center (SLAC) to support the BABAR experiment, to report its results, and to facilitate communication among its scientific and engineering collaborators, currently numbering about 600 individuals from 75 collaborating institutions in 10 countries. The BABAR Web site is, therefore, a community Web site. At the same time it is hosted at SLAC and funded by agencies that demand adherence to policies decided under different priorities. Additionally, the BABAR Web administrators deal with the problems that arise during the course of managing users, content, policies, standards, and changing technologies. Desired solutions to some of these problems may be incompatible with the overall administration of the SLAC Web sites and/or the SLAC policies and concerns. There are thus different perspectives of the same Web site and differing expectations in segments of the SLAC population which act as constraints and challenges in any review or re-engineering activities. Web Engineering, which post-dates the BABAR Web, has aimed to provide a comprehensive understanding of all aspects of Web development. This paper reports on the first part of a recent review of application of Web Engineering methods to the BABAR Web site, which has led to explicit user and information models of the BABAR community and how SLAC and the BABAR community relate and react to each other. The paper identifies the issues of a community Web site in a hierarchical, semi-governmental sector and formulates a strategy for periodic reviews of BABAR and similar sites.",
        "published": "2003-06-16T06:12:04Z",
        "link": "http://arxiv.org/abs/cs/0306094v1",
        "categories": [
            "cs.IR",
            "H.4.m"
        ]
    },
    {
        "title": "The Best Trail Algorithm for Assisted Navigation of Web Sites",
        "authors": [
            "Richard Wheeldon",
            "Mark Levene"
        ],
        "summary": "We present an algorithm called the Best Trail Algorithm, which helps solve the hypertext navigation problem by automating the construction of memex-like trails through the corpus. The algorithm performs a probabilistic best-first expansion of a set of navigation trees to find relevant and compact trails. We describe the implementation of the algorithm, scoring methods for trails, filtering algorithms and a new metric called \\emph{potential gain} which measures the potential of a page for future navigation opportunities.",
        "published": "2003-06-22T17:38:13Z",
        "link": "http://arxiv.org/abs/cs/0306122v1",
        "categories": [
            "cs.DS",
            "cs.IR",
            "H.3.3;H.5.4;G.2.2;F.2.2"
        ]
    },
    {
        "title": "Supporting Out-of-turn Interactions in a Multimodal Web Interface",
        "authors": [
            "Atul Shenoy",
            "Naren Ramakrishnan",
            "Manuel A. Perez-Quinones",
            "Srinidhi Varadarajan"
        ],
        "summary": "Multimodal interfaces are becoming increasingly important with the advent of mobile devices, accessibility considerations, and novel software technologies that combine diverse interaction media. This article investigates systems support for web browsing in a multimodal interface. Specifically, we outline the design and implementation of a software framework that integrates hyperlink and speech modes of interaction. Instead of viewing speech as merely an alternative interaction medium, the framework uses it to support out-of-turn interaction, providing a flexibility of information access not possible with hyperlinks alone. This approach enables the creation of websites that adapt to the needs of users, yet permits the designer fine-grained control over what interactions to support. Design methodology, implementation details, and two case studies are presented.",
        "published": "2003-07-04T13:44:04Z",
        "link": "http://arxiv.org/abs/cs/0307011v1",
        "categories": [
            "cs.IR",
            "cs.HC",
            "H.5"
        ]
    },
    {
        "title": "Learning Analogies and Semantic Relations",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the Scholastic Aptitude Test (SAT). A verbal analogy has the form A:B::C:D, meaning \"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct). We motivate this research by relating it to work in cognitive science and linguistics, and by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \"laser printer\", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for these challenging problems.",
        "published": "2003-07-24T21:09:43Z",
        "link": "http://arxiv.org/abs/cs/0307055v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Web Access to Cultural Heritage for the Disabled",
        "authors": [
            "Jonathan P. Bowen"
        ],
        "summary": "Physical disabled access is something that most cultural institutions such as museums consider very seriously. Indeed, there are normally legal requirements to do so. However, online disabled access is still a relatively novel and developing field. Many cultural organizations have not yet considered the issues in depth and web developers are not necessarily experts either. The interface for websites is normally tested with major browsers, but not with specialist software like text to audio converters for the blind or against the relevant accessibility and validation standards. We consider the current state of the art in this area, especially with respect to aspects of particular importance to the access of cultural heritage.",
        "published": "2003-07-30T18:37:14Z",
        "link": "http://arxiv.org/abs/cs/0307068v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.IR",
            "H.1.2;H.3.5;H.3.7;H.5.2;H.5.3;H.5.4;I.7.2;K.5.0"
        ]
    },
    {
        "title": "Disabled Access for Museum Websites",
        "authors": [
            "Jonathan P. Bowen"
        ],
        "summary": "Physical disabled access is something that most museums consider very seriously. Indeed, there are normally legal requirements to do so. However, online disabled access is still a relatively novel field. Most museums have not yet considered the issues in depth. The Human-Computer Interface for their websites is normally tested with major browsers, but not with specialist browsers or against the relevant accessibility and validation standards. We consider the current state of the art in this area and mention an accessibility survey of some museum websites.",
        "published": "2003-08-03T17:57:24Z",
        "link": "http://arxiv.org/abs/cs/0308005v2",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.IR",
            "H.1.2;H.3.5;H.3.7;H.5.1;H.5.2;H.5.3;H.5.4;I.7.2;K.5.0"
        ]
    },
    {
        "title": "Coherent Keyphrase Extraction via Web Mining",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within the text of a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. A limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent. That is, the majority of the output keyphrases may fit together well, but there may be a minority that appear to be outliers, with no clear semantic relation to the majority or to each other. This paper presents enhancements to the Kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. The approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related. The statistical association is measured using web mining. Experiments demonstrate that the enhancements improve the quality of the extracted keyphrases. Furthermore, the enhancements are not domain-specific: the algorithm generalizes well when it is trained on one domain (computer science documents) and tested on another (physics documents).",
        "published": "2003-08-20T20:42:19Z",
        "link": "http://arxiv.org/abs/cs/0308033v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "A new approach to relevancy in Internet searching - the \"Vox Populi   Algorithm\"",
        "authors": [
            "Andreas Schaale",
            "Carsten Wulf-Mathies",
            "Soenke Lieberam-Schmidt"
        ],
        "summary": "In this paper we will derive a new algorithm for Internet searching. The main idea of this algorithm is to extend the existing algorithms by a component, which reflects the interests of the users more than existing methods. The \"Vox Populi Algorithm\" (VPA) creates a feedback from the users to the content of the search index. The information derived from the users query analysis is used to modify the existing crawling algorithms. The VPA controls the distribution of the resources of the crawler. Finally, we also discuss methods of suppressing unwanted content (spam).",
        "published": "2003-08-23T13:34:25Z",
        "link": "http://arxiv.org/abs/cs/0308039v1",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.IR",
            "H.3.1; H.3.2; H.3.3"
        ]
    },
    {
        "title": "Centralized reward system gives rise to fast and efficient work sharing   for intelligent Internet agents lacking direct communication",
        "authors": [
            "Zsolt Palotai",
            "Sandor Mandusitz",
            "Andras Lorincz"
        ],
        "summary": "WWW has a scale-free structure where novel information is often difficult to locate. Moreover, Intelligent agents easily get trapped in this structure. Here a novel method is put forth, which turns these traps into information repositories, supplies: We populated an Internet environment with intelligent news foragers. Foraging has its associated cost whereas foragers are rewarded if they detect not yet discovered novel information. The intelligent news foragers crawl by using the estimated long-term cumulated reward, and also have a finite sized memory: the list of most promising supplies. Foragers form an artificial life community: the most successful ones are allowed to multiply, while unsuccessful ones die out. The specific property of this community is that there is no direct communication amongst foragers but the centralized rewarding system. Still, fast division of work is achieved.",
        "published": "2003-08-27T13:32:29Z",
        "link": "http://arxiv.org/abs/cs/0308042v1",
        "categories": [
            "cs.IR",
            "H.3.3; H.3.4"
        ]
    },
    {
        "title": "ROC Curves Within the Framework of Neural Network Assembly Memory Model:   Some Analytic Results",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "On the basis of convolutional (Hamming) version of recent Neural Network Assembly Memory Model (NNAMM) for intact two-layer autoassociative Hopfield network optimal receiver operating characteristics (ROCs) have been derived analytically. A method of taking into account explicitly a priori probabilities of alternative hypotheses on the structure of information initiating memory trace retrieval and modified ROCs (mROCs, a posteriori probabilities of correct recall vs. false alarm probability) are introduced. The comparison of empirical and calculated ROCs (or mROCs) demonstrates that they coincide quantitatively and in this way intensities of cues used in appropriate experiments may be estimated. It has been found that basic ROC properties which are one of experimental findings underpinning dual-process models of recognition memory can be explained within our one-factor NNAMM.",
        "published": "2003-09-07T20:11:10Z",
        "link": "http://arxiv.org/abs/cs/0309007v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "Semi-metric Behavior in Document Networks and its Application to   Recommendation Systems",
        "authors": [
            "L. M. Rocha"
        ],
        "summary": "Recommendation systems for different Document Networks (DN) such as the World Wide Web (WWW) and Digital Libraries, often use distance functions extracted from relationships among documents and keywords. For instance, documents in the WWW are related via a hyperlink network, while documents in bibliographic databases are related by citation and collaboration networks. Furthermore, documents are related to keyterms. The distance functions computed from these relations establish associative networks among items of the DN, referred to as Distance Graphs, which allow recommendation systems to identify relevant associations for individual users. However, modern recommendation systems need to integrate associative data from multiple sources such as different databases, web sites, and even other users. Thus, we are presented with a problem of combining evidence (about associations between items) from different sources characterized by distance functions. In this paper we describe our work on (1) inferring relevant associations from, as well as characterizing, semi-metric distance graphs and (2) combining evidence from different distance graphs in a recommendation system. Regarding (1), we present the idea of semi-metric distance graphs, and introduce ratios to measure semi-metric behavior. We compute these ratios for several DN such as digital libraries and web sites and show that they are useful to identify implicit associations. Regarding (2), we describe an algorithm to combine evidence from distance graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed for a recommendation system named TalkMine.",
        "published": "2003-09-09T05:24:03Z",
        "link": "http://arxiv.org/abs/cs/0309013v1",
        "categories": [
            "cs.IR",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.MA",
            "H.3.0; H.3.3, H.3.4; H.3.6; H.3.7; I.2.11; H.3.5"
        ]
    },
    {
        "title": "Proposed Specification of a Distributed XML-Query Network",
        "authors": [
            "Christian Thiemann",
            "Michael Schlenker",
            "Thomas Severiens"
        ],
        "summary": "W3C's XML-Query language offers a powerful instrument for information retrieval on XML repositories. This article describes an implementation of this retrieval in a real world's scenario. Distributed XML-Query processing reduces load on every single attending node to an acceptable level. The network allows every participant to control their computing load themselves. Furthermore XML-repositories may stay at the rights holder, so every Data-Provider can decide, whether to process critical queries or not. If Data-Providers keep redundant information, this distributed network improves reliability of information with duplicates removed.",
        "published": "2003-09-13T19:24:56Z",
        "link": "http://arxiv.org/abs/cs/0309022v2",
        "categories": [
            "cs.DC",
            "cs.IR",
            "H.3.4"
        ]
    },
    {
        "title": "Measuring Praise and Criticism: Inference of Semantic Orientation from   Association",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "The evaluative character of a word is called its semantic orientation. Positive semantic orientation indicates praise (e.g., \"honest\", \"intrepid\") and negative semantic orientation indicates criticism (e.g., \"disturbing\", \"superfluous\"). Semantic orientation varies in both direction (positive or negative) and degree (mild to strong). An automated system for measuring semantic orientation would have application in text classification, text filtering, tracking opinions in online discussions, analysis of survey responses, and automated chat systems (chatbots). This paper introduces a method for inferring the semantic orientation of a word from its statistical association with a set of positive and negative paradigm words. Two instances of this approach are evaluated, based on two different statistical measures of word association: pointwise mutual information (PMI) and latent semantic analysis (LSA). The method is experimentally tested with 3,596 words (including adjectives, adverbs, nouns, and verbs) that have been manually labeled positive (1,614 words) and negative (1,982 words). The method attains an accuracy of 82.8% on the full test set, but the accuracy rises above 95% when the algorithm is allowed to abstain from classifying mild words.",
        "published": "2003-09-19T16:30:55Z",
        "link": "http://arxiv.org/abs/cs/0309034v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Combining Independent Modules to Solve Multiple-choice Synonym and   Analogy Problems",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman",
            "Jeffrey Bigham",
            "Victor Shnayder"
        ],
        "summary": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of successful, separately developed modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the well known mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems commonly used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",
        "published": "2003-09-19T20:13:07Z",
        "link": "http://arxiv.org/abs/cs/0309035v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "I.2.6; I.2.7; H.3.1; J.5"
        ]
    },
    {
        "title": "A Neural Network Assembly Memory Model Based on an Optimal Binary Signal   Detection Theory",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "A ternary/binary data coding algorithm and conditions under which Hopfield networks implement optimal convolutional or Hamming decoding algorithms has been described. Using the coding/decoding approach (an optimal Binary Signal Detection Theory, BSDT) introduced a Neural Network Assembly Memory Model (NNAMM) is built. The model provides optimal (the best) basic memory performance and demands the use of a new memory unit architecture with two-layer Hopfield network, N-channel time gate, auxiliary reference memory, and two nested feedback loops. NNAMM explicitly describes the dependence on time of a memory trace retrieval, gives a possibility of metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. A model of smallest inseparable part or an \"atom\" of consciousness is also defined. The NNAMM's neurobiological backgrounds and its applications to solving some interdisciplinary problems are shortly discussed. BSDT could implement the \"best neural code\" used in nervous tissues of animals and humans.",
        "published": "2003-09-21T17:11:11Z",
        "link": "http://arxiv.org/abs/cs/0309036v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "Re-Finding Found Things: An Exploratory Study of How Users Re-Find   Information",
        "authors": [
            "Robert G. Capra",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "The problem of how people find information is studied extensively; however, the problem of how people organize, re-use, and re-find information that they have found is not as well understood. Recently, several projects have conducted in-situ studies to explore how people re-find and re-use information. Here, we present results and observations from a controlled, laboratory study of refinding information found on the web.   Our study was conducted as a collaborative exercise with pairs of participants. One participant acted as a retriever, helping the other participant re-find information by telephone. This design allowed us to gain insight into the strategies that users employed to re-find information, and into how domain artifacts and contextual information were used to aid the re-finding process. We also introduced the ability for users to add their own explicitly artifacts in the form of making annotations on the web pages they viewed.   We observe that re-finding often occurs as a two stage, iterative process in which users first attempt to locate an information source (search), and once found, begin a process to find the specific information being sought (browse). Our findings are consistent with research on waypoints; orienteering approaches to re-finding; and navigation of electronic spaces. Furthermore, we observed that annotations were utilized extensively, indicating that explicitly added context by the user can play an important role in re-finding.",
        "published": "2003-10-06T19:36:00Z",
        "link": "http://arxiv.org/abs/cs/0310011v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.1.2; H.3.3; H.5.2"
        ]
    },
    {
        "title": "WebTeach in practice: the entrance test to the Engineering faculty in   Florence",
        "authors": [
            "Franco Bagnoli",
            "Fabio Franci",
            "Francesco Mugelli",
            "Andrea Sterbini"
        ],
        "summary": "We present the WebTeach project, formed by a web interface to database for test management, a wiki site for the diffusion of teaching material and student forums, and a suite for the generation of multiple-choice mathematical quiz with automatic elaboration of forms. This system has been massively tested for the entrance test to the Engineering Faculty of the University of Florence, Italy",
        "published": "2003-10-08T14:07:42Z",
        "link": "http://arxiv.org/abs/cs/0310013v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "K.3.1"
        ]
    },
    {
        "title": "Make search become the internal function of Internet",
        "authors": [
            "Liang Wang",
            "Yiping Guo",
            "Ming Fang"
        ],
        "summary": "Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is a distributed information retrieval system, which will solve problems like poor coverage, long update interval in current web search system. The most distinct character of DRIS is that it's a public opening system, and acts as an internal component of Internet, but not the production of a company. The implementation of DRIS is also represented.",
        "published": "2003-11-14T09:53:51Z",
        "link": "http://arxiv.org/abs/cs/0311015v2",
        "categories": [
            "cs.IR",
            "cs.DL",
            "cs.NI",
            "H.3.3;H.3.5;H.3.7"
        ]
    },
    {
        "title": "Staging Transformations for Multimodal Web Interaction Management",
        "authors": [
            "Michael Narayan",
            "Chris Williams",
            "Saverio Perugini",
            "Naren Ramakrishnan"
        ],
        "summary": "Multimodal interfaces are becoming increasingly ubiquitous with the advent of mobile devices, accessibility considerations, and novel software technologies that combine diverse interaction media. In addition to improving access and delivery capabilities, such interfaces enable flexible and personalized dialogs with websites, much like a conversation between humans. In this paper, we present a software framework for multimodal web interaction management that supports mixed-initiative dialogs between users and websites. A mixed-initiative dialog is one where the user and the website take turns changing the flow of interaction. The framework supports the functional specification and realization of such dialogs using staging transformations -- a theory for representing and reasoning about dialogs based on partial input. It supports multiple interaction interfaces, and offers sessioning, caching, and co-ordination functions through the use of an interaction manager. Two case studies are presented to illustrate the promise of this approach.",
        "published": "2003-11-20T17:27:45Z",
        "link": "http://arxiv.org/abs/cs/0311029v1",
        "categories": [
            "cs.IR",
            "cs.PL",
            "H.5.4; H.5.2; F.3.2"
        ]
    },
    {
        "title": "Data mining and Privacy in Public Sector using Intelligent Agents   (discussion paper)",
        "authors": [
            "Max Voskob",
            "Nuck Punin"
        ],
        "summary": "The public sector comprises government agencies, ministries, education institutions, health providers and other types of government, commercial and not-for-profit organisations. Unlike commercial enterprises, this environment is highly heterogeneous in all aspects. This forms a complex network which is not always optimised. A lack of optimisation and communication hinders information sharing between the network nodes limiting the flow of information. Another limiting aspect is privacy of personal information and security of operations of some nodes or segments of the network. Attempts to reorganise the network or improve communications to make more information available for sharing and analysis may be hindered or completely halted by public concerns over privacy, political agendas, social and technological barriers. This paper discusses a technical solution for information sharing while addressing the privacy concerns with no need for reorganisation of the existing public sector infrastructure . The solution is based on imposing an additional layer of Intelligent Software Agents and Knowledge Bases for data mining and analysis.",
        "published": "2003-11-28T00:06:32Z",
        "link": "http://arxiv.org/abs/cs/0311050v1",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.IR",
            "cs.MA",
            "C.2.4; D.2.11; H.1.1; H.1.2; H.3.5; I.2.11; K.4.1"
        ]
    },
    {
        "title": "Embedding Web-based Statistical Translation Models in Cross-Language   Information Retrieval",
        "authors": [
            "Wessel Kraaij",
            "Jian-Yun Nie",
            "Michel Simard"
        ],
        "summary": "Although more and more language pairs are covered by machine translation services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application which needs translation functionality of a relatively low level of sophistication since current models for information retrieval (IR) are still based on a bag-of-words. The Web provides a vast resource for the automatic construction of parallel corpora which can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this paper, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.",
        "published": "2003-12-03T13:09:28Z",
        "link": "http://arxiv.org/abs/cs/0312008v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.1; H.3.3; I.2.7"
        ]
    },
    {
        "title": "Taking the Initiative with Extempore: Exploring Out-of-Turn Interactions   with Websites",
        "authors": [
            "Saverio Perugini",
            "Mary E. Pinney",
            "Naren Ramakrishnan",
            "Manuel A. Perez-Quinones",
            "Mary Beth Rosson"
        ],
        "summary": "We present the first study to explore the use of out-of-turn interaction in websites. Out-of-turn interaction is a technique which empowers the user to supply unsolicited information while browsing. This approach helps flexibly bridge any mental mismatch between the user and the website, in a manner fundamentally different from faceted browsing and site-specific search tools. We built a user interface (Extempore) which accepts out-of-turn input via voice or text; and employed it in a US congressional website, to determine if users utilize out-of-turn interaction for information-finding tasks, and their rationale for doing so. The results indicate that users are adept at discerning when out-of-turn interaction is necessary in a particular task, and actively interleaved it with browsing. However, users found cascading information across information-finding subtasks challenging. Therefore, this work not only improves our understanding of out-of-turn interaction, but also suggests further opportunities to enrich browsing experiences for users.",
        "published": "2003-12-08T16:25:46Z",
        "link": "http://arxiv.org/abs/cs/0312016v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.5.2; H.5.4"
        ]
    },
    {
        "title": "Mapping Subsets of Scholarly Information",
        "authors": [
            "Paul Ginsparg",
            "Paul Houle",
            "Thorsten Joachims",
            "Jae-Hoon Sul"
        ],
        "summary": "We illustrate the use of machine learning techniques to analyze, structure, maintain, and evolve a large online corpus of academic literature. An emerging field of research can be identified as part of an existing corpus, permitting the implementation of a more coherent community structure for its practitioners.",
        "published": "2003-12-11T20:07:39Z",
        "link": "http://arxiv.org/abs/cs/0312018v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.1; H.3.6; I.2.6"
        ]
    },
    {
        "title": "Evolution: Google vs. DRIS",
        "authors": [
            "Wang Liang",
            "Guo Yiping",
            "Fang Ming"
        ],
        "summary": "This paper gives an absolute new search system that builds the information retrieval infrastructure for Internet. Now most search engine companies are mainly concerned with how to make profit from company users by advertisement and ranking prominence, but never consider what its real customers will feel. Few web search engines can sell billions dollars just at the cost of inconvenience of most Internet users, but not its high quality of search service. When we have to bear the bothersome advertisements in the awful results and have no choices, Internet as the kind of public good will surely be undermined. If current Internet can't fully ensure our right to know, it may need some sound improvements or a revolution.",
        "published": "2003-12-13T03:15:46Z",
        "link": "http://arxiv.org/abs/cs/0312024v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "cs.NI",
            "H.3.3;H.3.5;H.3.7"
        ]
    },
    {
        "title": "Using sensors in the web crawling process",
        "authors": [
            "Ilya Zemskov"
        ],
        "summary": "This paper offers a short description of an Internet information field monitoring system, which places a special module-sensor on the side of the Web-server to detect changes in information resources and subsequently reindexes only the resources signalized by the corresponding sensor. Concise results of simulation research and an implementation attempt of the given \"sensors\" concept are provided.",
        "published": "2003-12-17T11:30:56Z",
        "link": "http://arxiv.org/abs/cs/0312033v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.4; I.6.3"
        ]
    },
    {
        "title": "Acquiring Lexical Paraphrases from a Single Corpus",
        "authors": [
            "Oren Glickman",
            "Ido Dagan"
        ],
        "summary": "This paper studies the potential of identifying lexical paraphrases within a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information. A comparison suggests that an instance-based approach may be combined with a vector based approach in order to assess better the paraphrase likelihood for many verb pairs.",
        "published": "2003-12-25T16:45:20Z",
        "link": "http://arxiv.org/abs/cs/0312058v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG",
            "I.7"
        ]
    },
    {
        "title": "Polyhierarchical Classifications Induced by Criteria Polyhierarchies,   and Taxonomy Algebra",
        "authors": [
            "Pavel Babikov",
            "Oleg Gontcharov",
            "Maria Babikova"
        ],
        "summary": "A new approach to the construction of general persistent polyhierarchical classifications is proposed. It is based on implicit description of category polyhierarchy by a generating polyhierarchy of classification criteria. Similarly to existing approaches, the classification categories are defined by logical functions encoded by attributive expressions. However, the generating hierarchy explicitly predefines domains of criteria applicability, and the semantics of relations between categories is invariant to changes in the universe composition, extending variety of criteria, and increasing their cardinalities. The generating polyhierarchy is an independent, compact, portable, and re-usable information structure serving as a template classification. It can be associated with one or more particular sets of objects, included in more general classifications as a standard component, or used as a prototype for more comprehensive classifications. The approach dramatically simplifies development and unplanned modifications of persistent hierarchical classifications compared with tree, DAG, and faceted schemes. It can be efficiently implemented in common DBMS, while considerably reducing amount of computer resources required for storage, maintenance, and use of complex polyhierarchies.",
        "published": "2003-12-26T05:30:24Z",
        "link": "http://arxiv.org/abs/cs/0312059v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "I.2.4; H.3.3"
        ]
    },
    {
        "title": "Flavor: A Language for Media Representation",
        "authors": [
            "Alexandros Eleftheriadis",
            "Danny Hong"
        ],
        "summary": "Flavor (Formal Language for Audio-Visual Object Representation) has been created as a language for describing coded multimedia bitstreams in a formal way so that the code for reading and writing bitstreams can be automatically generated. It is an extension of C++ and Java, in which the typing system incorporates bitstream representation semantics. This allows describing in a single place both the in-memory representation of data as well as their bitstream-level (compressed) representation. Flavor also comes with a translator that automatically generates standard C++ or Java code from the Flavor source code so that direct access to compressed multimedia information by application developers can be achieved with essentially zero programming. Flavor has gone through many enhancements and this paper fully describes the latest version of the language and the translator. The software has been made into an open source project as of Version 4.1, and the latest downloadable Flavor package is available at http://flavor.sourceforge.net.",
        "published": "2003-01-07T07:53:20Z",
        "link": "http://arxiv.org/abs/cs/0301003v1",
        "categories": [
            "cs.PL",
            "D.3.4; D.3.0"
        ]
    },
    {
        "title": "Subclassing errors, OOP, and practically checkable rules to prevent them",
        "authors": [
            "Oleg Kiselyov"
        ],
        "summary": "This paper considers an example of Object-Oriented Programming (OOP) leading to subtle errors that break separation of interface and implementations. A comprehensive principle that guards against such errors is undecidable. The paper introduces a set of mechanically verifiable rules that prevent these insidious problems. Although the rules seem restrictive, they are powerful and expressive, as we show on several familiar examples. The rules contradict both the spirit and the letter of the OOP. The present examples as well as available theoretical and experimental results pose a question if OOP is conducive to software development at all.",
        "published": "2003-01-28T19:41:41Z",
        "link": "http://arxiv.org/abs/cs/0301032v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "F.3.3; D.1.5; D.1.1; D.2.3; D.2.4; D.2.11"
        ]
    },
    {
        "title": "Unique Pattern Matching in Strings",
        "authors": [
            "Stijn Vansummeren"
        ],
        "summary": "Regular expression patterns are a key feature of document processing languages like Perl and XDuce. It is in this context that the first and longest match policies have been proposed to disambiguate the pattern matching process. We formally define a matching semantics with these policies and show that the generally accepted method of simulating longest match by first match and recursion is incorrect. We continue by solving the associated type inference problem, which consists in calculating for every subexpression the set of words the subexpression can still match when these policies are in effect, and show how this algorithm can be used to efficiently implement the matching process.",
        "published": "2003-02-03T15:52:17Z",
        "link": "http://arxiv.org/abs/cs/0302004v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "D.3.3; F.1.1; H.2.3; H.3.3; H.2.4; I.5.5"
        ]
    },
    {
        "title": "Cg in Two Pages",
        "authors": [
            "Mark J. Kilgard"
        ],
        "summary": "Cg is a language for programming GPUs. This paper describes Cg briefly.",
        "published": "2003-02-12T05:16:12Z",
        "link": "http://arxiv.org/abs/cs/0302013v1",
        "categories": [
            "cs.GR",
            "cs.PL",
            "I.3.6; C.1.3"
        ]
    },
    {
        "title": "Recursive function templates as a solution of linear algebra expressions   in C++",
        "authors": [
            "Volodymyr Myrnyy"
        ],
        "summary": "The article deals with a kind of recursive function templates in C++, where the recursion is realized corresponding template parameters to achieve better computational performance. Some specialization of these template functions ends the recursion and can be implemented using optimized hardware dependent or independent routines. The method is applied in addition to the known expression templates technique to solve linear algebra expressions with the help of the BLAS library. The whole implementation produces a new library, which keeps object-oriented benefits and has a higher computational speed represented in the tests.",
        "published": "2003-02-19T15:59:28Z",
        "link": "http://arxiv.org/abs/cs/0302026v1",
        "categories": [
            "cs.MS",
            "cs.PL",
            "G.4; I.1.2; I.1.3"
        ]
    },
    {
        "title": "A Development Calculus for Specifications",
        "authors": [
            "Wei Li"
        ],
        "summary": "A first order inference system, called R-calculus, is defined to develop the specifications. It is used to eliminate the laws which is not consistent with the user's requirements. The R-calculus consists of the structural rules, an axiom, a cut rule, and the rules for logical connectives. Some examples are given to demonstrate the usage of the R-calculus. The properties about reachability and completeness of the R-calculus are formally defined and are proved.",
        "published": "2003-03-21T08:23:33Z",
        "link": "http://arxiv.org/abs/cs/0303021v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1"
        ]
    },
    {
        "title": "Minimum Model Semantics for Logic Programs with Negation-as-Failure",
        "authors": [
            "Panos Rondogiannis",
            "William W. Wadge"
        ],
        "summary": "We give a purely model-theoretic characterization of the semantics of logic programs with negation-as-failure allowed in clause bodies. In our semantics the meaning of a program is, as in the classical case, the unique minimum model in a program-independent ordering. We use an expanded truth domain that has an uncountable linearly ordered set of truth values between False (the minimum element) and True (the maximum), with a Zero element in the middle. The truth values below Zero are ordered like the countable ordinals. The values above Zero have exactly the reverse order. Negation is interpreted as reflection about Zero followed by a step towards Zero; the only truth value that remains unaffected by negation is Zero. We show that every program has a unique minimum model M_P, and that this model can be constructed with a T_P iteration which proceeds through the countable ordinals. Furthermore, we demonstrate that M_P can also be obtained through a model intersection construction which generalizes the well-known model intersection theorem for classical logic programming. Finally, we show that by collapsing the true and false values of the infinite-valued model M_P to (the classical) True and False, we obtain a three-valued model identical to the well-founded one.",
        "published": "2003-06-03T12:12:40Z",
        "link": "http://arxiv.org/abs/cs/0306017v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL",
            "F.3.2;F.4.1;D.1.6;I.2.3"
        ]
    },
    {
        "title": "Quanta: a Language for Modeling and Manipulating Information Structures",
        "authors": [
            "Bruce Long"
        ],
        "summary": "We present a theory for modeling the structure of information and a language (Quanta) expressing the theory. Unlike Shannon's information theory, which focuses on the amount of information in an information system, we focus on the structure of the information in the system. For example, we can model the information structure corresponding to an algorithm or a physical process such as the structure of a quantum interaction. After a brief discussion of the relation between an evolving state-system and an information structure, we develop an algebra of information pieces (infons) to represent the structure of systems where descriptions of complex systems are constructed from expressions involving descriptions of simpler information systems. We map the theory to the Von Neumann computing model of sequences/conditionals/repetitions, and to the class/object theory of object-oriented programming (OOP).",
        "published": "2003-06-09T20:13:42Z",
        "link": "http://arxiv.org/abs/cs/0306038v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "I.1.3; D.3.2; F.4.1"
        ]
    },
    {
        "title": "Quantum Domain Theory - Definitions and Applications",
        "authors": [
            "Elham Kashefi"
        ],
        "summary": "Classically domain theory is a rigourous mathematical structure to describe denotational semantics for programming languages and to study the computability of partial functions. Recently, the application of domain theory has also been extended to the quantum setting. In this note we review these results and we present some new thoughts in this field.",
        "published": "2003-06-11T10:34:16Z",
        "link": "http://arxiv.org/abs/quant-ph/0306077v1",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "Distributive Computability",
        "authors": [
            "Sebastiano Vigna"
        ],
        "summary": "This thesis presents a series of theoretical results and practical realisations about the theory of computation in distributive categories. Distributive categories have been proposed as a foundational tool for Computer Science in the last years, starting from the papers of R.F.C. Walters. We shall focus on two major topics: distributive computability, i.e., a generalized theory of computability based on distributive categories, and the Imp(G) language, which is a language based on the syntax of distributive categories. The link between the former and the latter is that the functions computed by Imp(G) programs are exactly the distributively computable functions.",
        "published": "2003-06-27T14:06:59Z",
        "link": "http://arxiv.org/abs/cs/0306136v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.1.1"
        ]
    },
    {
        "title": "Transformations of Logic Programs with Goals as Arguments",
        "authors": [
            "Alberto Pettorossi",
            "Maurizio Proietti"
        ],
        "summary": "We consider a simple extension of logic programming where variables may range over goals and goals may be arguments of predicates. In this language we can write logic programs which use goals as data. We give practical evidence that, by exploiting this capability when transforming programs, we can improve program efficiency.   We propose a set of program transformation rules which extend the familiar unfolding and folding rules and allow us to manipulate clauses with goals which occur as arguments of predicates. In order to prove the correctness of these transformation rules, we formally define the operational semantics of our extended logic programming language. This semantics is a simple variant of LD-resolution. When suitable conditions are satisfied this semantics agrees with LD-resolution and, thus, the programs written in our extended language can be run by ordinary Prolog systems.   Our transformation rules are shown to preserve the operational semantics and termination.",
        "published": "2003-07-09T16:54:44Z",
        "link": "http://arxiv.org/abs/cs/0307022v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "On Applying Or-Parallelism and Tabling to Logic Programs",
        "authors": [
            "Ricardo Rocha",
            "Fernando Silva",
            "Vitor Santos Costa"
        ],
        "summary": "The past years have seen widening efforts at increasing Prolog's declarativeness and expressiveness. Tabling has proved to be a viable technique to efficiently overcome SLD's susceptibility to infinite loops and redundant subcomputations. Our research demonstrates that implicit or-parallelism is a natural fit for logic programs with tabling. To substantiate this belief, we have designed and implemented an or-parallel tabling engine -- OPTYap -- and we used a shared-memory parallel machine to evaluate its performance. To the best of our knowledge, OPTYap is the first implementation of a parallel tabling engine for logic programming systems. OPTYap builds on Yap's efficient sequential Prolog engine. Its execution model is based on the SLG-WAM for tabling, and on the environment copying for or-parallelism.   Preliminary results indicate that the mechanisms proposed to parallelize search in the context of SLD resolution can indeed be effectively and naturally generalized to parallelize tabled computations, and that the resulting systems can achieve good performance on shared-memory parallel machines. More importantly, it emphasizes our belief that through applying or-parallelism and tabling to logic programs the range of applications for Logic Programming can be increased.",
        "published": "2003-08-04T18:59:42Z",
        "link": "http://arxiv.org/abs/cs/0308007v1",
        "categories": [
            "cs.PL",
            "D.1.6;D.3.2"
        ]
    },
    {
        "title": "Model Checking Linear Logic Specifications",
        "authors": [
            "M. Bozzano",
            "G. Delzanno",
            "M. Martelli"
        ],
        "summary": "The overall goal of this paper is to investigate the theoretical foundations of algorithmic verification techniques for first order linear logic specifications. The fragment of linear logic we consider in this paper is based on the linear logic programming language called LO enriched with universally quantified goal formulas. Although LO was originally introduced as a theoretical foundation for extensions of logic programming languages, it can also be viewed as a very general language to specify a wide range of infinite-state concurrent systems.   Our approach is based on the relation between backward reachability and provability highlighted in our previous work on propositional LO programs. Following this line of research, we define here a general framework for the bottom-up evaluation of first order linear logic specifications. The evaluation procedure is based on an effective fixpoint operator working on a symbolic representation of infinite collections of first order linear logic formulas. The theory of well quasi-orderings can be used to provide sufficient conditions for the termination of the evaluation of non trivial fragments of first order linear logic.",
        "published": "2003-09-01T09:34:15Z",
        "link": "http://arxiv.org/abs/cs/0309003v1",
        "categories": [
            "cs.PL",
            "cs.SC",
            "D.3.1;F.3.1;F.3.2"
        ]
    },
    {
        "title": "Proceedings of the Fifth International Workshop on Automated Debugging   (AADEBUG 2003)",
        "authors": [
            "Michiel Ronsse",
            "Koen De Bosschere"
        ],
        "summary": "Over the past decades automated debugging has seen major achievements. However, as debugging is by necessity attached to particular programming paradigms, the results are scattered. To alleviate this problem, the Automated and Algorithmic Debugging workshop (AADEBUG for short) was organised in 1993 in Link\"oping (Sweden). As this workshop proved to be successful, subsequent workshops have been organised in 1995 (Saint-Malo, France), 1997 (again in Link\"oping, Sweden) and 2000 (Munich, Germany). In 2003, the workshop is organised in Ghent, Belgium, the proceedings of which you are reading right now.",
        "published": "2003-09-15T14:58:32Z",
        "link": "http://arxiv.org/abs/cs/0309027v4",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "cTI: A constraint-based termination inference tool for ISO-Prolog",
        "authors": [
            "Fred Mesnard",
            "Roberto Bagnara"
        ],
        "summary": "We present cTI, the first system for universal left-termination inference of logic programs. Termination inference generalizes termination analysis and checking. Traditionally, a termination analyzer tries to prove that a given class of queries terminates. This class must be provided to the system, for instance by means of user annotations. Moreover, the analysis must be redone every time the class of queries of interest is updated. Termination inference, in contrast, requires neither user annotations nor recomputation. In this approach, terminating classes for all predicates are inferred at once. We describe the architecture of cTI and report an extensive experimental evaluation of the system covering many classical examples from the logic programming termination literature and several Prolog programs of respectable size and complexity.",
        "published": "2003-09-16T07:43:50Z",
        "link": "http://arxiv.org/abs/cs/0309028v1",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "A uniform approach to constraint-solving for lists, multisets, compact   lists, and sets",
        "authors": [
            "Agostino Dovier",
            "Carla Piazza",
            "Gianfranco Rossi"
        ],
        "summary": "Lists, multisets, and sets are well-known data structures whose usefulness is widely recognized in various areas of Computer Science. These data structures have been analyzed from an axiomatic point of view with a parametric approach in (*) where the relevant unification algorithms have been developed. In this paper we extend these results considering more general constraints including not only equality but also membership constraints as well as their negative counterparts.   (*) A. Dovier, A. Policriti, and G. Rossi. A uniform axiomatic view of lists, multisets, and sets, and the relevant unification algorithms. Fundamenta Informaticae, 36(2/3):201--234, 1998.",
        "published": "2003-09-24T10:08:00Z",
        "link": "http://arxiv.org/abs/cs/0309045v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SC",
            "D.3.3; F.4.1; F.2.2; I.1.2; I.2.3"
        ]
    },
    {
        "title": "Pure Prolog Execution in 21 Rules",
        "authors": [
            "Marija Kulas"
        ],
        "summary": "A simple mathematical definition of the 4-port model for pure Prolog is given. The model combines the intuition of ports with a compact representation of execution state. Forward and backward derivation steps are possible. The model satisfies a modularity claim, making it suitable for formal reasoning.",
        "published": "2003-10-10T18:46:15Z",
        "link": "http://arxiv.org/abs/cs/0310020v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2; F.3.1; D.2.5; I.2.3"
        ]
    },
    {
        "title": "A Monitoring Language for Run Time and Post-Mortem Behavior Analysis and   Visualization",
        "authors": [
            "Mikhail Auguston",
            "Clinton Jeffery",
            "Scott Underwood"
        ],
        "summary": "UFO is a new implementation of FORMAN, a declarative monitoring language, in which rules are compiled into execution monitors that run on a virtual machine supported by the Alamo monitor architecture.",
        "published": "2003-10-14T21:06:50Z",
        "link": "http://arxiv.org/abs/cs/0310025v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Computing Convex Hulls with a Linear Solver",
        "authors": [
            "Florence Benoy",
            "Andy King",
            "Fred Mesnard"
        ],
        "summary": "A programming tactic involving polyhedra is reported that has been widely applied in the polyhedral analysis of (constraint) logic programs. The method enables the computations of convex hulls that are required for polyhedral analysis to be coded with linear constraint solving machinery that is available in many Prolog systems.   To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2003-11-04T12:43:54Z",
        "link": "http://arxiv.org/abs/cs/0311002v1",
        "categories": [
            "cs.PL",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "Generic and Efficient Program Monitoring by trace analysis",
        "authors": [
            "Erwan Jahier",
            "Mireille Ducass'e"
        ],
        "summary": "Program execution monitoring consists of checking whole executions for given properties in order to collect global run-time information.   Monitoring is very useful to maintain programs. However, application developers face the following dilemma: either they use existing tools which never exactly fit their needs, or they invest a lot of effort to implement monitoring code.   In this article we argue that, when an event-oriented tracer exists, the compiler developers can enable the application developers to easily code their own, relevant, monitors which will run efficiently.   We propose a high-level operator, called foldt, which operates on execution traces. One of the key advantages of our approach is that it allows a clean separation of concerns; the definition of monitors is neither intertwined in the user source code nor in the language compiler.   We give a number of applications of the foldt operator to compute monitors for Mercury program executions: execution profiles, graphical abstract views, and two test coverage measurements. Each example is implemented by a few simple lines of Mercury.   Detailed measurements show acceptable performance of the basic mechanism of foldt for executions of several millions of execution events.",
        "published": "2003-11-14T10:23:11Z",
        "link": "http://arxiv.org/abs/cs/0311016v1",
        "categories": [
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "The Chameleon Type Debugger (Tool Demonstration)",
        "authors": [
            "Peter J. Stuckey",
            "Martin Sulzmann",
            "Jeremy Wazny"
        ],
        "summary": "In this tool demonstration, we give an overview of the Chameleon type debugger. The type debugger's primary use is to identify locations within a source program which are involved in a type error. By further examining these (potentially) problematic program locations, users gain a better understanding of their program and are able to work towards the actual mistake which was the cause of the type error. The debugger is interactive, allowing the user to provide additional information to narrow down the search space. One of the novel aspects of the debugger is the ability to explain erroneous-looking types. In the event that an unexpected type is inferred, the debugger can highlight program locations which contributed to that result. Furthermore, due to the flexible constraint-based foundation that the debugger is built upon, it can naturally handle advanced type system features such as Haskell's type classes and functional dependencies.",
        "published": "2003-11-17T23:06:18Z",
        "link": "http://arxiv.org/abs/cs/0311023v1",
        "categories": [
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Staging Transformations for Multimodal Web Interaction Management",
        "authors": [
            "Michael Narayan",
            "Chris Williams",
            "Saverio Perugini",
            "Naren Ramakrishnan"
        ],
        "summary": "Multimodal interfaces are becoming increasingly ubiquitous with the advent of mobile devices, accessibility considerations, and novel software technologies that combine diverse interaction media. In addition to improving access and delivery capabilities, such interfaces enable flexible and personalized dialogs with websites, much like a conversation between humans. In this paper, we present a software framework for multimodal web interaction management that supports mixed-initiative dialogs between users and websites. A mixed-initiative dialog is one where the user and the website take turns changing the flow of interaction. The framework supports the functional specification and realization of such dialogs using staging transformations -- a theory for representing and reasoning about dialogs based on partial input. It supports multiple interaction interfaces, and offers sessioning, caching, and co-ordination functions through the use of an interaction manager. Two case studies are presented to illustrate the promise of this approach.",
        "published": "2003-11-20T17:27:45Z",
        "link": "http://arxiv.org/abs/cs/0311029v1",
        "categories": [
            "cs.IR",
            "cs.PL",
            "H.5.4; H.5.2; F.3.2"
        ]
    },
    {
        "title": "A Very Short Self-Interpreter",
        "authors": [
            "Oleg Mazonka",
            "Daniel B. Cristofani"
        ],
        "summary": "In this paper we would like to present a very short (possibly the shortest) self-interpreter, based on a simplistic Turing-complete imperative language. This interpreter explicitly processes the statements of the language, which means the interpreter constitutes a description of the language inside that same language. The paper does not require any specific knowledge; however, experience in programming and a vivid imagination are beneficial.",
        "published": "2003-11-21T06:18:19Z",
        "link": "http://arxiv.org/abs/cs/0311032v1",
        "categories": [
            "cs.PL",
            "D.3"
        ]
    },
    {
        "title": "Idempotent I/O for safe time travel",
        "authors": [
            "Zoltan Somogyi"
        ],
        "summary": "Debuggers for logic programming languages have traditionally had a capability most other debuggers did not: the ability to jump back to a previous state of the program, effectively travelling back in time in the history of the computation. This ``retry'' capability is very useful, allowing programmers to examine in detail a part of the computation that they previously stepped over. Unfortunately, it also creates a problem: while the debugger may be able to restore the previous values of variables, it cannot restore the part of the program's state that is affected by I/O operations. If the part of the computation being jumped back over performs I/O, then the program will perform these I/O operations twice, which will result in unwanted effects ranging from the benign (e.g. output appearing twice) to the fatal (e.g. trying to close an already closed file). We present a simple mechanism for ensuring that every I/O action called for by the program is executed at most once, even if the programmer asks the debugger to travel back in time from after the action to before the action. The overhead of this mechanism is low enough and can be controlled well enough to make it practical to use it to debug computations that do significant amounts of I/O.",
        "published": "2003-11-26T04:45:22Z",
        "link": "http://arxiv.org/abs/cs/0311040v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Combining Logic Programs and Monadic Second Order Logics by Program   Transformation",
        "authors": [
            "F. Fioravanti",
            "A. Pettorossi",
            "M. Proietti"
        ],
        "summary": "We present a program synthesis method based on unfold/fold transformation rules which can be used for deriving terminating definite logic programs from formulas of the Weak Monadic Second Order theory of one successor (WS1S). This synthesis method can also be used as a proof method which is a decision procedure for closed formulas of WS1S. We apply our synthesis method for translating CLP(WS1S) programs into logic programs and we use it also as a proof method for verifying safety properties of infinite state systems.",
        "published": "2003-11-27T09:47:28Z",
        "link": "http://arxiv.org/abs/cs/0311043v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "Derivation of Efficient Logic Programs by Specialization and Reduction   of Nondeterminism",
        "authors": [
            "Alberto Pettorossi",
            "Maurizio Proietti",
            "Sophie Renault"
        ],
        "summary": "Program specialization is a program transformation methodology which improves program efficiency by exploiting the information about the input data which are available at compile time. We show that current techniques for program specialization based on partial evaluation do not perform well on nondeterministic logic programs. We then consider a set of transformation rules which extend the ones used for partial evaluation, and we propose a strategy for guiding the application of these extended rules so to derive very efficient specialized programs. The efficiency improvements which sometimes are exponential, are due to the reduction of nondeterminism and to the fact that the computations which are performed by the initial programs in different branches of the computation trees, are performed by the specialized programs within single branches. In order to reduce nondeterminism we also make use of mode information for guiding the unfolding process. To exemplify our technique, we show that we can automatically derive very efficient matching programs and parsers for regular languages. The derivations we have performed could not have been done by previously known partial evaluation techniques.",
        "published": "2003-11-27T10:04:53Z",
        "link": "http://arxiv.org/abs/cs/0311044v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.2;D.1.6;I.2.2;F.3.1"
        ]
    },
    {
        "title": "Inferring Termination Conditions for Logic Programs using Backwards   Analysis",
        "authors": [
            "Samir Genaim",
            "Michael Codish"
        ],
        "summary": "This paper focuses on the inference of modes for which a logic program is guaranteed to terminate. This generalises traditional termination analysis where an analyser tries to verify termination for a specified mode. Our contribution is a methodology in which components of traditional termination analysis are combined with backwards analysis to obtain an analyser for termination inference. We identify a condition on the components of the analyser which guarantees that termination inference will infer all modes which can be checked to terminate. The application of this methodology to enhance a traditional termination analyser to perform also termination inference is demonstrated.",
        "published": "2003-12-12T16:14:20Z",
        "link": "http://arxiv.org/abs/cs/0312023v1",
        "categories": [
            "cs.PL",
            "D.1.6,F.3.1"
        ]
    },
    {
        "title": "Speedup of Logic Programs by Binarization and Partial Deduction",
        "authors": [
            "Jan Hruza",
            "Petr Stepanek"
        ],
        "summary": "Binary logic programs can be obtained from ordinary logic programs by a binarizing transformation. In most cases, binary programs obtained this way are less efficient than the original programs. (Demoen, 1992) showed an interesting example of a logic program whose computational behaviour was improved when it was transformed to a binary program and then specialized by partial deduction. The class of B-stratifiable logic programs is defined. It is shown that for every B-stratifiable logic program, binarization and subsequent partial deduction produce a binary program which does not contain variables for continuations introduced by binarization. Such programs usually have a better computational behaviour than the original ones. Both binarization and partial deduction can be easily automated. A comparison with other related approaches to program transformation is given.",
        "published": "2003-12-15T12:23:42Z",
        "link": "http://arxiv.org/abs/cs/0312026v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6;I.2.2;I.2.3;F.4.1"
        ]
    },
    {
        "title": "An Open Ended Tree",
        "authors": [
            "Henk Vandecasteele",
            "Gerda Janssens"
        ],
        "summary": "An open ended list is a well known data structure in Prolog programs. It is frequently used to represent a value changing over time, while this value is referred to from several places in the data structure of the application. A weak point in this technique is that the time complexity is linear in the number of updates to the value represented by the open ended list. In this programming pearl we present a variant of the open ended list, namely an open ended tree, with an update and access time complexity logarithmic in the number of updates to the value.",
        "published": "2003-12-15T13:43:37Z",
        "link": "http://arxiv.org/abs/cs/0312027v1",
        "categories": [
            "cs.PL",
            "D.1.6;D.3.3;.E.2"
        ]
    },
    {
        "title": "Distributed WWW Programming using (Ciao-)Prolog and the PiLLoW library",
        "authors": [
            "Daniel Cabeza",
            "Manuel V. Hermenegildo"
        ],
        "summary": "We discuss from a practical point of view a number of issues involved in writing distributed Internet and WWW applications using LP/CLP systems. We describe PiLLoW, a public-domain Internet and WWW programming library for LP/CLP systems that we have designed in order to simplify the process of writing such applications. PiLLoW provides facilities for accessing documents and code on the WWW; parsing, manipulating and generating HTML and XML structured documents and data; producing HTML forms; writing form handlers and CGI-scripts; and processing HTML/XML templates. An important contribution of PiLLoW is to model HTML/XML code (and, thus, the content of WWW pages) as terms. The PiLLoW library has been developed in the context of the Ciao Prolog system, but it has been adapted to a number of popular LP/CLP systems, supporting most of its functionality. We also describe the use of concurrency and a high-level model of client-server interaction, Ciao Prolog's active modules, in the context of WWW programming. We propose a solution for client-side downloading and execution of Prolog code, using generic browsers. Finally, we also provide an overview of related work on the topic.",
        "published": "2003-12-16T19:09:04Z",
        "link": "http://arxiv.org/abs/cs/0312031v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.1.6"
        ]
    },
    {
        "title": "Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated   Rendering",
        "authors": [
            "Cass Everitt",
            "Mark J. Kilgard"
        ],
        "summary": "Twenty-five years ago, Crow published the shadow volume approach for determining shadowed regions in a scene. A decade ago, Heidmann described a hardware-accelerated stencil buffer-based shadow volume algorithm.   Unfortunately hardware-accelerated stenciled shadow volume techniques have not been widely adopted by 3D games and applications due in large part to the lack of robustness of described techniques. This situation persists despite widely available hardware support. Specifically what has been lacking is a technique that robustly handles various \"hard\" situations created by near or far plane clipping of shadow volumes.   We describe a robust, artifact-free technique for hardware-accelerated rendering of stenciled shadow volumes. Assuming existing hardware, we resolve the issues otherwise caused by shadow volume near and far plane clipping through a combination of (1) placing the conventional far clip plane \"at infinity\", (2) rasterization with infinite shadow volume polygons via homogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth clamping, a new rasterization feature provided by NVIDIA's GeForce3, preserves existing depth precision by not requiring the far plane to be placed at infinity. We also propose two-sided stencil testing to improve the efficiency of rendering stenciled shadow volumes.",
        "published": "2003-01-06T20:57:51Z",
        "link": "http://arxiv.org/abs/cs/0301002v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "I.3.6; I.3.1"
        ]
    },
    {
        "title": "Tiling space and slabs with acute tetrahedra",
        "authors": [
            "David Eppstein",
            "John M. Sullivan",
            "Alper Ungor"
        ],
        "summary": "We show it is possible to tile three-dimensional space using only tetrahedra with acute dihedral angles. We present several constructions to achieve this, including one in which all dihedral angles are less than $77.08^\\circ$, and another which tiles a slab in space.",
        "published": "2003-02-19T20:40:54Z",
        "link": "http://arxiv.org/abs/cs/0302027v1",
        "categories": [
            "cs.CG",
            "math.MG",
            "F.2.2"
        ]
    },
    {
        "title": "Relaxed Scheduling in Dynamic Skin Triangulation",
        "authors": [
            "Herbert Edelsbrunner",
            "Alper Ungor"
        ],
        "summary": "We introduce relaxed scheduling as a paradigm for mesh maintenance and demonstrate its applicability to triangulating a skin surface in $\\Rspace^3$.",
        "published": "2003-02-20T22:20:50Z",
        "link": "http://arxiv.org/abs/cs/0302031v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "When Crossings Count - Approximating the Minimum Spanning Tree",
        "authors": [
            "Sariel Har-Peled",
            "Piotr Indyk"
        ],
        "summary": "In the first part of the paper, we present an (1+\\mu)-approximation algorithm to the minimum-spanning tree of points in a planar arrangement of lines, where the metric is the number of crossings between the spanning tree and the lines. The expected running time is O((n/\\mu^5) alpha^3(n) log^5 n), where \\mu > 0 is a prescribed constant.   In the second part of our paper, we show how to embed such a crossing metric, into high-dimensions, so that the distances are preserved. As a result, we can deploy a large collection of subquadratic approximations algorithms \\cite im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric as a distance function. Applications include matching, clustering, nearest-neighbor, and furthest-neighbor.",
        "published": "2003-03-01T22:09:20Z",
        "link": "http://arxiv.org/abs/cs/0303001v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "Quasiconvex Analysis of Backtracking Algorithms",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We consider a class of multivariate recurrences frequently arising in the worst case analysis of Davis-Putnam-style exponential time backtracking algorithms for NP-hard problems. We describe a technique for proving asymptotic upper bounds on these recurrences, by using a suitable weight function to reduce the problem to that of solving univariate linear recurrences; show how to use quasiconvex programming to determine the weight function yielding the smallest upper bound; and prove that the resulting upper bounds are within a polynomial factor of the true asymptotics of the recurrence. We develop and implement a multiple-gradient descent algorithm for the resulting quasiconvex programs, using a real-number arithmetic package for guaranteed accuracy of the computed worst case time bounds.",
        "published": "2003-04-10T20:25:12Z",
        "link": "http://arxiv.org/abs/cs/0304018v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "math.CO",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Partitioning Regular Polygons into Circular Pieces I: Convex Partitions",
        "authors": [
            "Mirela Damian",
            "Joseph O'Rourke"
        ],
        "summary": "We explore an instance of the question of partitioning a polygon into pieces, each of which is as ``circular'' as possible, in the sense of having an aspect ratio close to 1. The aspect ratio of a polygon is the ratio of the diameters of the smallest circumscribing circle to the largest inscribed disk. The problem is rich even for partitioning regular polygons into convex pieces, the focus of this paper. We show that the optimal (most circular) partition for an equilateral triangle has an infinite number of pieces, with the lower bound approachable to any accuracy desired by a particular finite partition. For pentagons and all regular k-gons, k > 5, the unpartitioned polygon is already optimal. The square presents an interesting intermediate case. Here the one-piece partition is not optimal, but nor is the trivial lower bound approachable. We narrow the optimal ratio to an aspect-ratio gap of 0.01082 with several somewhat intricate partitions.",
        "published": "2003-04-17T12:29:29Z",
        "link": "http://arxiv.org/abs/cs/0304023v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Computational Geometry Column 44",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The open problem of whether or not every pair of equal-area polygons has a hinged dissection is discussed.",
        "published": "2003-04-18T21:12:35Z",
        "link": "http://arxiv.org/abs/cs/0304025v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "The envelope of lines meeting a fixed line that are tangent to two   spheres",
        "authors": [
            "Gábor Megyesi",
            "Frank Sottile"
        ],
        "summary": "We study the set of lines that meet a fixed line and are tangent to two spheres and classify the configurations consisting of a single line and three spheres for which there are infinitely many lines tangent to the three spheres that also meet the given line. All such configurations are degenerate. The path to this result involves the interplay of some beautiful and intricate geometry of real surfaces in 3-space, complex algebraic geometry, explicit computation and graphics.",
        "published": "2003-04-23T01:10:39Z",
        "link": "http://arxiv.org/abs/math/0304346v2",
        "categories": [
            "math.AG",
            "cs.CG",
            "math.MG",
            "68U05, 51N20, 14N10, 14Q15; ACM I.3.5"
        ]
    },
    {
        "title": "The one-round Voronoi game replayed",
        "authors": [
            "Sandor P. Fekete",
            "Henk Meijer"
        ],
        "summary": "We consider the one-round Voronoi game, where player one (``White'', called ``Wilma'') places a set of n points in a rectangular area of aspect ratio r <=1, followed by the second player (``Black'', called ``Barney''), who places the same number of points. Each player wins the fraction of the board closest to one of his points, and the goal is to win more than half of the total area. This problem has been studied by Cheong et al., who showed that for large enough $n$ and r=1, Barney has a strategy that guarantees a fraction of 1/2+a, for some small fixed a.   We resolve a number of open problems raised by that paper. In particular, we give a precise characterization of the outcome of the game for optimal play: We show that Barney has a winning strategy for n>2 and r>sqrt{2}/n, and for n=2 and r>sqrt{3}/2. Wilma wins in all remaining cases, i.e., for n>=3 and r<=sqrt{2}/n, for n=2 and r<=sqrt{3}/2, and for n=1. We also discuss complexity aspects of the game on more general boards, by proving that for a polygon with holes, it is NP-hard to maximize the area Barney can win against a given set of points by Wilma.",
        "published": "2003-05-16T14:43:04Z",
        "link": "http://arxiv.org/abs/cs/0305016v2",
        "categories": [
            "cs.CG",
            "cs.GT",
            "F.2.2"
        ]
    },
    {
        "title": "On multiple connectedness of regions visible due to multiple diffuse   reflections",
        "authors": [
            "Sudebkumar Prasant Pal",
            "Dilip Sarkar"
        ],
        "summary": "It is known that the region $V(s)$ of a simple polygon $P$, directly visible (illuminable) from an internal point $s$, is simply connected. Aronov et al. \\cite{addpp981} established that the region $V_1(s)$ of a simple polygon visible from an internal point $s$ due to at most one diffuse reflection on the boundary of the polygon $P$, is also simply connected. In this paper we establish that the region $V_2(s)$, visible from $s$ due to at most two diffuse reflections may be multiply connected; we demonstrate the construction of an $n$-sided simple polygon with a point $s$ inside it so that and the region of $P$ visible from $s$ after at most two diffuse reflections is multiple connected.",
        "published": "2003-06-02T11:40:37Z",
        "link": "http://arxiv.org/abs/cs/0306010v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.GR",
            "F2.2, G2.1"
        ]
    },
    {
        "title": "The number of transversals to line segments in R^3",
        "authors": [
            "Hervé Brönnimann",
            "Hazel Everett",
            "Sylvain Lazard",
            "Frank Sottile",
            "Sue Whitesides"
        ],
        "summary": "We completely describe the structure of the connected components of transversals to a collection of n line segments in R^3. We show that n>2 arbitrary line segments in R^3 admit 0, 1, ..., n or infinitely many line transversals. In the latter case, the transversals form up to n connected components.",
        "published": "2003-06-27T18:36:03Z",
        "link": "http://arxiv.org/abs/math/0306401v1",
        "categories": [
            "math.MG",
            "cs.CG"
        ]
    },
    {
        "title": "State complexes for metamorphic robots",
        "authors": [
            "A. Abrams",
            "R. Ghrist"
        ],
        "summary": "A metamorphic robotic system is an aggregate of homogeneous robot units which can individually and selectively locomote in such a way as to change the global shape of the system. We introduce a mathematical framework for defining and analyzing general metamorphic robots. This formal structure, combined with ideas from geometric group theory, leads to a natural extension of a configuration space for metamorphic robots -- the state complex -- which is especially adapted to parallelization. We present an algorithm for optimizing reconfiguration sequences with respect to elapsed time. A universal geometric property of state complexes -- non-positive curvature -- is the key to proving convergence to the globally time-optimal solution.",
        "published": "2003-07-02T19:41:07Z",
        "link": "http://arxiv.org/abs/cs/0307004v1",
        "categories": [
            "cs.RO",
            "cs.CG",
            "I.2.9"
        ]
    },
    {
        "title": "Optimal Adaptive Algorithms for Finding the Nearest and Farthest Point   on a Parametric Black-Box Curve",
        "authors": [
            "Ilya Baran",
            "Erik D. Demaine"
        ],
        "summary": "We consider a general model for representing and manipulating parametric curves, in which a curve is specified by a black box mapping a parameter value between 0 and 1 to a point in Euclidean d-space. In this model, we consider the nearest-point-on-curve and farthest-point-on-curve problems: given a curve C and a point p, find a point on C nearest to p or farthest from p. In the general black-box model, no algorithm can solve these problems. Assuming a known bound on the speed of the curve (a Lipschitz condition), the answer can be estimated up to an additive error of epsilon using O(1/epsilon) samples, and this bound is tight in the worst case. However, many instances can be solved with substantially fewer samples, and we give algorithms that adapt to the inherent difficulty of the particular instance, up to a logarithmic factor. More precisely, if OPT(C,p,epsilon) is the minimum number of samples of C that every correct algorithm must perform to achieve tolerance epsilon, then our algorithm performs O(OPT(C,p,epsilon) log (epsilon^(-1)/OPT(C,p,epsilon))) samples. Furthermore, any algorithm requires Omega(k log (epsilon^(-1)/k)) samples for some instance C' with OPT(C',p,epsilon) = k; except that, for the nearest-point-on-curve problem when the distance between C and p is less than epsilon, OPT is 1 but the upper and lower bounds on the number of samples are both Theta(1/epsilon). When bounds on relative error are desired, we give algorithms that perform O(OPT log (2+(1+epsilon^(-1)) m^(-1)/OPT)) samples (where m is the exact minimum or maximum distance from p to C) and prove that Omega(OPT log (1/epsilon)) samples are necessary on some problem instances.",
        "published": "2003-07-03T01:14:57Z",
        "link": "http://arxiv.org/abs/cs/0307005v3",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Testing Bipartiteness of Geometric Intersection Graphs",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show how to test the bipartiteness of an intersection graph of n line segments or simple polygons in the plane, or of balls in R^d, in time O(n log n). More generally we find subquadratic algorithms for connectivity and bipartiteness testing of intersection graphs of a broad class of geometric objects. For unit balls in R^d, connectivity testing has equivalent randomized complexity to construction of Euclidean minimum spanning trees, and hence is unlikely to be solved as efficiently as bipartiteness testing. For line segments or planar disks, testing k-colorability of intersection graphs for k>2 is NP-complete.",
        "published": "2003-07-09T21:29:49Z",
        "link": "http://arxiv.org/abs/cs/0307023v2",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Deterministic Sampling and Range Counting in Geometric Data Streams",
        "authors": [
            "Amitabha Bagchi",
            "Amitabh Chaudhary",
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "We present memory-efficient deterministic algorithms for constructing epsilon-nets and epsilon-approximations of streams of geometric data. Unlike probabilistic approaches, these deterministic samples provide guaranteed bounds on their approximation factors. We show how our deterministic samples can be used to answer approximate online iceberg geometric queries on data streams. We use these techniques to approximate several robust statistics of geometric data streams, including Tukey depth, simplicial depth, regression depth, the Thiel-Sen estimator, and the least median of squares. Our algorithms use only a polylogarithmic amount of memory, provided the desired approximation factors are inverse-polylogarithmic. We also include a lower bound for non-iceberg geometric queries.",
        "published": "2003-07-11T00:43:00Z",
        "link": "http://arxiv.org/abs/cs/0307027v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "A Note on Objects Built From Bricks without Corners",
        "authors": [
            "Mirela Damian",
            "Joseph O'Rourke"
        ],
        "summary": "We report a small advance on a question raised by Robertson, Schweitzer, and Wagon in [RSW02]. They constructed a genus-13 polyhedron built from bricks without corners, and asked whether every genus-0 such polyhedron must have a corner. A brick is a parallelopiped, and a corner is a brick of degree three or less in the brick graph. We describe a genus-3 polyhedron built from bricks with no corner, narrowing the genus gap.",
        "published": "2003-07-17T20:46:25Z",
        "link": "http://arxiv.org/abs/cs/0307042v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2,G.2"
        ]
    },
    {
        "title": "Two- versus three-dimensional connectivity testing of first-order   queries to semi-algebraic sets",
        "authors": [
            "Floris Geerts",
            "Lieven Smits",
            "Jan Van den Bussche"
        ],
        "summary": "This paper addresses the question whether one can determine the connectivity of a semi-algebraic set in three dimensions by testing the connectivity of a finite number of two-dimensional ``samples'' of the set, where these samples are defined by first-order queries. The question is answered negatively for two classes of first-order queries: cartesian-product-free, and positive one-pass.",
        "published": "2003-08-01T00:59:41Z",
        "link": "http://arxiv.org/abs/cs/0308001v2",
        "categories": [
            "cs.LO",
            "cs.CG",
            "cs.DB",
            "F.4.1; F.2.2; H.2.8"
        ]
    },
    {
        "title": "A Bernstein-Bezier Sufficient Condition for Invertibility of Polynomial   Mapping Functions",
        "authors": [
            "Stephen Vavasis"
        ],
        "summary": "We propose a sufficient condition for invertibility of a polynomial mapping function defined on a cube or simplex. This condition is applicable to finite element analysis using curved meshes. The sufficient condition is based on an analysis of the Bernstein-B\\'ezier form of the columns of the derivative.",
        "published": "2003-08-11T18:43:35Z",
        "link": "http://arxiv.org/abs/cs/0308021v1",
        "categories": [
            "cs.NA",
            "cs.CG",
            "G.1.8"
        ]
    },
    {
        "title": "Optimal Covering Tours with Turn Costs",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Erik D. Demaine",
            "Sandor P. Fekete",
            "Joseph S. B. Mitchell",
            "Saurabh Sethia"
        ],
        "summary": "We give the first algorithmic study of a class of ``covering tour'' problems related to the geometric Traveling Salesman Problem: Find a polygonal tour for a cutter so that it sweeps out a specified region (``pocket''), in order to minimize a cost that depends mainly on the number of em turns. These problems arise naturally in manufacturing applications of computational geometry to automatic tool path generation and automatic inspection systems, as well as arc routing (``postman'') problems with turn penalties. We prove the NP-completeness of minimum-turn milling and give efficient approximation algorithms for several natural versions of the problem, including a polynomial-time approximation scheme based on a novel adaptation of the m-guillotine method.",
        "published": "2003-09-09T20:16:30Z",
        "link": "http://arxiv.org/abs/cs/0309014v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Fast Verification of Convexity of Piecewise-linear Surfaces",
        "authors": [
            "Konstantin Rybnikov"
        ],
        "summary": "We show that a realization of a closed connected PL-manifold of dimension n-1 in n-dimensional Euclidean space (n>2) is the boundary of a convex polyhedron (finite or infinite) if and only if the interior of each (n-3)-face has a point, which has a neighborhood lying on the boundary of an n-dimensional convex body. No initial assumptions about the topology or orientability of the input surface are made. The theorem is derived from a refinement and generalization of Van Heijenoort's theorem on locally convex manifolds to spherical spaces. Our convexity criterion for PL-manifolds implies an easy polynomial-time algorithm for checking convexity of a given PL-surface in n-dimensional Euclidean or spherical space, n>2. The algorithm is worst case optimal with respect to both the number of operations and the algebraic degree. The algorithm works under significantly weaker assumptions and is easier to implement than convexity verification algorithms suggested by Mehlhorn et al (1996-1999), and Devillers et al.(1998). A paradigm of approximate convexity is suggested and a simplified algorithm of smaller degree and complexity is suggested for approximate floating point convexity verification.",
        "published": "2003-09-23T06:47:28Z",
        "link": "http://arxiv.org/abs/cs/0309041v2",
        "categories": [
            "cs.CG",
            "cs.CV",
            "Primary: G.4, F.2.2; Secondary: 1.4.7, 1.4.8"
        ]
    },
    {
        "title": "Circle and sphere blending with conformal geometric algebra",
        "authors": [
            "Chris Doran"
        ],
        "summary": "Blending schemes based on circles provide smooth `fair' interpolations between series of points. Here we demonstrate a simple, robust set of algorithms for performing circle blends for a range of cases. An arbitrary level of G-continuity can be achieved by simple alterations to the underlying parameterisation. Our method exploits the computational framework provided by conformal geometric algebra. This employs a five-dimensional representation of points in space, in contrast to the four-dimensional representation typically used in projective geometry. The advantage of the conformal scheme is that straight lines and circles are treated in a single, unified framework. As a further illustration of the power of the conformal framework, the basic idea is extended to the case of sphere blending to interpolate over a surface.",
        "published": "2003-10-09T15:15:41Z",
        "link": "http://arxiv.org/abs/cs/0310017v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "I.3.5"
        ]
    },
    {
        "title": "On the continuous Fermat-Weber problem",
        "authors": [
            "Sandor P. Fekete",
            "Joseph S. B. Mitchell",
            "Karin Beurer"
        ],
        "summary": "We give the first exact algorithmic study of facility location problems that deal with finding a median for a continuum of demand points. In particular, we consider versions of the ``continuous k-median (Fermat-Weber) problem'' where the goal is to select one or more center points that minimize the average distance to a set of points in a demand region. In such problems, the average is computed as an integral over the relevant region, versus the usual discrete sum of distances. The resulting facility location problems are inherently geometric, requiring analysis techniques of computational geometry. We provide polynomial-time algorithms for various versions of the L1 1-median (Fermat-Weber) problem. We also consider the multiple-center version of the L1 k-median problem, which we prove is NP-hard for large k.",
        "published": "2003-10-15T15:59:30Z",
        "link": "http://arxiv.org/abs/cs/0310027v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "A weak definition of Delaunay triangulation",
        "authors": [
            "Vin de Silva"
        ],
        "summary": "We show that the traditional criterion for a simplex to belong to the Delaunay triangulation of a point set is equivalent to a criterion which is a priori weaker. The argument is quite general; as well as the classical Euclidean case, it applies to hyperbolic and hemispherical geometries and to Edelsbrunner's weighted Delaunay triangulation. In spherical geometry, we establish a similar theorem under a genericity condition. The weak definition finds natural application in the problem of approximating a point-cloud data set with a simplical complex.",
        "published": "2003-10-16T00:14:55Z",
        "link": "http://arxiv.org/abs/cs/0310031v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "A combinatorial characterization of higher-dimensional orthogonal   packing",
        "authors": [
            "Sandor P. Fekete",
            "Joerg Schepers"
        ],
        "summary": "Higher-dimensional orthogonal packing problems have a wide range of practical applications, including packing, cutting, and scheduling. Previous efforts for exact algorithms have been unable to avoid structural problems that appear for instances in two- or higher-dimensional space. We present a new approach for modeling packings, using a graph-theoretical characterization of feasible packings. Our characterization allows it to deal with classes of packings that share a certain combinatorial structure, instead of having to consider one packing at a time. In addition, we can make use of elegant algorithmic properties of certain classes of graphs. This allows our characterization to be the basis for a successful branch-and-bound framework.   This is the first in a series of papers describing new approaches to higher-dimensional packing.",
        "published": "2003-10-16T08:27:08Z",
        "link": "http://arxiv.org/abs/cs/0310032v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Minimizing the stabbing number of matchings, trees, and triangulations",
        "authors": [
            "Sandor P. Fekete",
            "Marco Luebbecke",
            "Henk Meijer"
        ],
        "summary": "The (axis-parallel) stabbing number of a given set of line segments is the maximum number of segments that can be intersected by any one (axis-parallel) line. This paper deals with finding perfect matchings, spanning trees, or triangulations of minimum stabbing number for a given set of points. The complexity of these problems has been a long-standing open question; in fact, it is one of the original 30 outstanding open problems in computational geometry on the list by Demaine, Mitchell, and O'Rourke. The answer we provide is negative for a number of minimum stabbing problems by showing them NP-hard by means of a general proof technique. It implies non-trivial lower bounds on the approximability. On the positive side we propose a cut-based integer programming formulation for minimizing the stabbing number of matchings and spanning trees. We obtain lower bounds (in polynomial time) from the corresponding linear programming relaxations, and show that an optimal fractional solution always contains an edge of at least constant weight. This result constitutes a crucial step towards a constant-factor approximation via an iterated rounding scheme. In computational experiments we demonstrate that our approach allows for actually solving problems with up to several hundred points optimally or near-optimally.",
        "published": "2003-10-16T14:01:32Z",
        "link": "http://arxiv.org/abs/cs/0310034v3",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Maximum dispersion and geometric maximum weight cliques",
        "authors": [
            "Sandor P. Fekete",
            "Henk Meijer"
        ],
        "summary": "We consider a facility location problem, where the objective is to ``disperse'' a number of facilities, i.e., select a given number k of locations from a discrete set of n candidates, such that the average distance between selected locations is maximized. In particular, we present algorithmic results for the case where vertices are represented by points in d-dimensional space, and edge weights correspond to rectilinear distances. Problems of this type have been considered before, with the best result being an approximation algorithm with performance ratio 2. For the case where k is fixed, we establish a linear-time algorithm that finds an optimal solution. For the case where k is part of the input, we present a polynomial-time approximation scheme.",
        "published": "2003-10-17T15:48:46Z",
        "link": "http://arxiv.org/abs/cs/0310037v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "A rigorous definition of axial lines: ridges on isovist fields",
        "authors": [
            "Rui Carvalho",
            "Michael Batty"
        ],
        "summary": "We suggest that 'axial lines' defined by (Hillier and Hanson, 1984) as lines of uninterrupted movement within urban streetscapes or buildings, appear as ridges in isovist fields (Benedikt, 1979). These are formed from the maximum diametric lengths of the individual isovists, sometimes called viewsheds, that make up these fields (Batty and Rana, 2004). We present an image processing technique for the identification of lines from ridges, discuss current strengths and weaknesses of the method, and show how it can be implemented easily and effectively.",
        "published": "2003-11-12T19:15:41Z",
        "link": "http://arxiv.org/abs/cs/0311012v1",
        "categories": [
            "cs.CV",
            "cs.CG",
            "I2.10; I.4.10"
        ]
    },
    {
        "title": "Transforming triangulations on non planar-surfaces",
        "authors": [
            "C. Cortes",
            "C. I. Grima",
            "F. Hurtado",
            "A. Marquez",
            "F. Santos",
            "J. Valenzuela"
        ],
        "summary": "We consider whether any two triangulations of a polygon or a point set on a non-planar surface with a given metric can be transformed into each other by a sequence of edge flips. The answer is negative in general with some remarkable exceptions, such as polygons on the cylinder, and on the flat torus, and certain configurations of points on the cylinder.",
        "published": "2003-11-13T20:25:14Z",
        "link": "http://arxiv.org/abs/math/0311228v2",
        "categories": [
            "math.MG",
            "cs.CG",
            "68U05"
        ]
    },
    {
        "title": "The Geometric Thickness of Low Degree Graphs",
        "authors": [
            "Christian A. Duncan",
            "David Eppstein",
            "Stephen G. Kobourov"
        ],
        "summary": "We prove that the geometric thickness of graphs whose maximum degree is no more than four is two. All of our algorithms run in O(n) time, where n is the number of vertices in the graph. In our proofs, we present an embedding algorithm for graphs with maximum degree three that uses an n x n grid and a more complex algorithm for embedding a graph with maximum degree four. We also show a variation using orthogonal edges for maximum degree four graphs that also uses an n x n grid. The results have implications in graph theory, graph drawing, and VLSI design.",
        "published": "2003-12-24T06:59:09Z",
        "link": "http://arxiv.org/abs/cs/0312056v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "I.3.5; F.2.2; G.2.2"
        ]
    },
    {
        "title": "Practical and Robust Stenciled Shadow Volumes for Hardware-Accelerated   Rendering",
        "authors": [
            "Cass Everitt",
            "Mark J. Kilgard"
        ],
        "summary": "Twenty-five years ago, Crow published the shadow volume approach for determining shadowed regions in a scene. A decade ago, Heidmann described a hardware-accelerated stencil buffer-based shadow volume algorithm.   Unfortunately hardware-accelerated stenciled shadow volume techniques have not been widely adopted by 3D games and applications due in large part to the lack of robustness of described techniques. This situation persists despite widely available hardware support. Specifically what has been lacking is a technique that robustly handles various \"hard\" situations created by near or far plane clipping of shadow volumes.   We describe a robust, artifact-free technique for hardware-accelerated rendering of stenciled shadow volumes. Assuming existing hardware, we resolve the issues otherwise caused by shadow volume near and far plane clipping through a combination of (1) placing the conventional far clip plane \"at infinity\", (2) rasterization with infinite shadow volume polygons via homogeneous coordinates, and (3) adopting a zfail stencil-testing scheme. Depth clamping, a new rasterization feature provided by NVIDIA's GeForce3, preserves existing depth precision by not requiring the far plane to be placed at infinity. We also propose two-sided stencil testing to improve the efficiency of rendering stenciled shadow volumes.",
        "published": "2003-01-06T20:57:51Z",
        "link": "http://arxiv.org/abs/cs/0301002v1",
        "categories": [
            "cs.GR",
            "cs.CG",
            "I.3.6; I.3.1"
        ]
    },
    {
        "title": "Cg in Two Pages",
        "authors": [
            "Mark J. Kilgard"
        ],
        "summary": "Cg is a language for programming GPUs. This paper describes Cg briefly.",
        "published": "2003-02-12T05:16:12Z",
        "link": "http://arxiv.org/abs/cs/0302013v1",
        "categories": [
            "cs.GR",
            "cs.PL",
            "I.3.6; C.1.3"
        ]
    },
    {
        "title": "Embedded Reflection Mapping",
        "authors": [
            "Paul Anderson",
            "Goncalo Carvalho"
        ],
        "summary": "Environment maps are used to simulate reflections off curved objects. We present a technique to reflect a user, or a group of users, in a real environment, onto a virtual object, in a virtual reality application, using the live video feeds from a set of cameras, in real-time. Our setup can be used in a variety of environments ranging from outdoor or indoor scenes.",
        "published": "2003-04-08T14:17:53Z",
        "link": "http://arxiv.org/abs/cs/0304011v1",
        "categories": [
            "cs.GR",
            "I.3.7"
        ]
    },
    {
        "title": "The Persint visualization program for the ATLAS experiment",
        "authors": [
            "D. Pomarede",
            "M. Virchaux"
        ],
        "summary": "The Persint program is designed for the three-dimensional representation of objects and for the interfacing and access to a variety of independent applications, in a fully interactive way. Facilities are provided for the spatial navigation and the definition of the visualization properties, in order to interactively set the viewing and viewed points, and to obtain the desired perspective. In parallel, applications may be launched through the use of dedicated interfaces, such as the interactive reconstruction and display of physics events. Recent developments have focalized on the interfacing to the XML ATLAS General Detector Description AGDD, making it a widely used tool for XML developers. The graphics capabilities of this program were exploited in the context of the ATLAS 2002 Muon Testbeam where it was used as an online event display, integrated in the online software framework and participating in the commissioning and debug of the detector system.",
        "published": "2003-05-29T22:06:27Z",
        "link": "http://arxiv.org/abs/cs/0305057v1",
        "categories": [
            "cs.GR",
            "I.3.0"
        ]
    },
    {
        "title": "GraXML - Modular Geometric Modeler",
        "authors": [
            "Julius Hrivnac"
        ],
        "summary": "Many entities managed by HEP Software Frameworks represent spatial (3-dimensional) real objects. Effective definition, manipulation and visualization of such objects is an indispensable functionality.   GraXML is a modular Geometric Modeling toolkit capable of processing geometric data of various kinds (detector geometry, event geometry) from different sources and delivering them in ways suitable for further use. Geometric data are first modeled in one of the Generic Models. Those Models are then used to populate powerful Geometric Model based on the Java3D technology. While Java3D has been originally created just to provide visualization of 3D objects, its light weight and high functionality allow an effective reuse as a general geometric component. This is possible also thanks to a large overlap between graphical and general geometric functionality and modular design of Java3D itself. Its graphical functionalities also allow a natural visualization of all manipulated elements.   All these techniques have been developed primarily (or only) for the Java environment. It is, however, possible to interface them transparently to Frameworks built in other languages, like for example C++.   The GraXML toolkit has been tested with data from several sources, as for example ATLAS and ALICE detector description and ATLAS event data. Prototypes for other sources, like Geometry Description Markup Language (GDML) exist too and interface to any other source is easy to add.",
        "published": "2003-06-02T09:04:18Z",
        "link": "http://arxiv.org/abs/cs/0306012v1",
        "categories": [
            "cs.GR",
            "I.2.10; I.3.7"
        ]
    },
    {
        "title": "On multiple connectedness of regions visible due to multiple diffuse   reflections",
        "authors": [
            "Sudebkumar Prasant Pal",
            "Dilip Sarkar"
        ],
        "summary": "It is known that the region $V(s)$ of a simple polygon $P$, directly visible (illuminable) from an internal point $s$, is simply connected. Aronov et al. \\cite{addpp981} established that the region $V_1(s)$ of a simple polygon visible from an internal point $s$ due to at most one diffuse reflection on the boundary of the polygon $P$, is also simply connected. In this paper we establish that the region $V_2(s)$, visible from $s$ due to at most two diffuse reflections may be multiply connected; we demonstrate the construction of an $n$-sided simple polygon with a point $s$ inside it so that and the region of $P$ visible from $s$ after at most two diffuse reflections is multiple connected.",
        "published": "2003-06-02T11:40:37Z",
        "link": "http://arxiv.org/abs/cs/0306010v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "cs.GR",
            "F2.2, G2.1"
        ]
    },
    {
        "title": "The FRED Event Display: an Extensible HepRep Client for GLAST",
        "authors": [
            "Marco Frailis",
            "Riccardo Giannitrapani"
        ],
        "summary": "A new graphics client prototype for the HepRep protocol is presented. Based on modern toolkits and high level languages (C++ and Ruby), Fred is an experiment to test applicability of scripting facilities to the high energy physics event display domain. Its flexible structure, extensibility and the use of the HepRep protocol are key features for its use in the astroparticle experiment GLAST.",
        "published": "2003-06-06T12:34:53Z",
        "link": "http://arxiv.org/abs/cs/0306031v1",
        "categories": [
            "cs.GR",
            "I.3.2,I.3.3,I.3.7"
        ]
    },
    {
        "title": "IGUANA Architecture, Framework and Toolkit for Interactive Graphics",
        "authors": [
            "George Alverson",
            "Giulio Eulisse",
            "Shahzad Muzaffar",
            "Ianna Osborne",
            "Lassi A. Tuura",
            "Lucas Taylor"
        ],
        "summary": "IGUANA is a generic interactive visualisation framework based on a C++ component model. It provides powerful user interface and visualisation primitives in a way that is not tied to any particular physics experiment or detector design. The article describes interactive visualisation tools built using IGUANA for the CMS and D0 experiments, as well as generic GEANT4 and GEANT3 applications. It covers features of the graphical user interfaces, 3D and 2D graphics, high-quality vector graphics output for print media, various textual, tabular and hierarchical data views, and integration with the application through control panels, a command line and different multi-threading models.",
        "published": "2003-06-10T16:15:41Z",
        "link": "http://arxiv.org/abs/cs/0306042v1",
        "categories": [
            "cs.SE",
            "cs.GR",
            "D.2.11;I.3.8;J.2"
        ]
    },
    {
        "title": "The Use of HepRep in GLAST",
        "authors": [
            "J. Perl",
            "R. Giannitrapani",
            "M. Frailis"
        ],
        "summary": "HepRep is a generic, hierarchical format for description of graphics representables that can be augmented by physics information and relational properties. It was developed for high energy physics event display applications and is especially suited to client/server or component frameworks. The GLAST experiment, an international effort led by NASA for a gamma-ray telescope to launch in 2006, chose HepRep to provide a flexible, extensible and maintainable framework for their event display without tying their users to any one graphics application. To support HepRep in their GUADI infrastructure, GLAST developed a HepRep filler and builder architecture. The architecture hides the details of XML and CORBA in a set of base and helper classes allowing physics experts to focus on what data they want to represent. GLAST has two GAUDI services: HepRepSvc, which registers HepRep fillers in a global registry and allows the HepRep to be exported to XML, and CorbaSvc, which allows the HepRep to be published through a CORBA interface and which allows the client application to feed commands back to GAUDI (such as start next event, or run some GAUDI algorithm). GLAST's HepRep solution gives users a choice of client applications, WIRED (written in Java) or FRED (written in C++ and Ruby), and leaves them free to move to any future HepRep-compliant event display.",
        "published": "2003-06-12T20:37:32Z",
        "link": "http://arxiv.org/abs/cs/0306059v1",
        "categories": [
            "cs.GR",
            "I.3.2; I.3.4; I.3.6"
        ]
    },
    {
        "title": "OO Model of the STAR offline production \"Event Display\" and its   implementation based on Qt-ROOT",
        "authors": [
            "Valeri Fine",
            "Jerome Lauret",
            "Victor Perevoztchikov"
        ],
        "summary": "The paper presents the \"Event Display\" package for the STAR offline production as a special visualization tool to debug the reconstruction code. This can be achieved if an author of the algorithm / code may build his/her own custom Event Display alone from the base software blocks and re-used some well-designed, easy to learn user-friendly patterns. For STAR offline production Event Display ROOT with Qt lower level interface was chosen as the base tools.",
        "published": "2003-06-14T05:42:43Z",
        "link": "http://arxiv.org/abs/cs/0306087v1",
        "categories": [
            "cs.HC",
            "cs.GR",
            "I.3.7; D.1.5"
        ]
    },
    {
        "title": "Application of interactive parallel visualization for commodity-based   clusters using visualization APIs",
        "authors": [
            "Stanimire Tomov",
            "Robert Bennett",
            "Michael McGuigan",
            "Arnold Peskin",
            "Gordon Smith",
            "John Spiletic"
        ],
        "summary": "We present an efficient and inexpensive to develop application for interactive high-performance parallel visualization. We extend popular APIs such as Open Inventor and VTK to support commodity-based cluster visualization. Our implementation follows a standard master/slave concept: the general idea is to have a ``Master'' node, which will intercept a sequential graphical user interface (GUI) and broadcast it to the ``Slave'' nodes. The interactions between the nodes are implemented using MPI. The parallel remote rendering uses Chromium. This paper is mainly the report of our implementation experiences. We present in detail the proposed model and key aspects of its implementation. Also, we present performance measurements, we benchmark and quantitatively demonstrate the dependence of the visualization speed on the data size and the network bandwidth, and we identify the singularities and draw conclusions on Chromium's sort-first rendering architecture. The most original part of this work is the combined use of Open Inventor and Chromium.",
        "published": "2003-07-29T13:40:12Z",
        "link": "http://arxiv.org/abs/cs/0307065v1",
        "categories": [
            "cs.GR",
            "I.3.6; I.3.4"
        ]
    },
    {
        "title": "The Graphics Card as a Streaming Computer",
        "authors": [
            "Suresh Venkatasubramanian"
        ],
        "summary": "Massive data sets have radically changed our understanding of how to design efficient algorithms; the streaming paradigm, whether it in terms of number of passes of an external memory algorithm, or the single pass and limited memory of a stream algorithm, appears to be the dominant method for coping with large data.   A very different kind of massive computation has had the same effect at the level of the CPU. The most prominent example is that of the computations performed by a graphics card. The operations themselves are very simple, and require very little memory, but require the ability to perform many computations extremely fast and in parallel to whatever degree possible. What has resulted is a stream processor that is highly optimized for stream computations. An intriguing side effect of this is the growing use of a graphics card as a general purpose stream processing engine. In an ever-increasing array of applications, researchers are discovering that performing a computation on a graphics card is far faster than performing it on a CPU, and so are using a GPU as a stream co-processor.",
        "published": "2003-10-05T06:30:56Z",
        "link": "http://arxiv.org/abs/cs/0310002v2",
        "categories": [
            "cs.GR",
            "cs.AR",
            "C.1.2;F.1.1;I.3.1"
        ]
    },
    {
        "title": "Poster on MPI application in Computational Fluid Dynamics",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "Poster-presentation of the paper \"Message Passing Fluids: molecules as processes in parallel computational fluids\" held at \"EURO PVMMPI 2003\" Congress; the paper is on the proceedings \"Recent Advances in Parallel Virtual Machine and Message Passing Interface\", 10th European PVM/MPI User's Group Meeting, LNCS 2840, Springer-Verlag, Dongarra-Laforenza-Orlando editors, pp. 550-554.",
        "published": "2003-10-06T14:20:00Z",
        "link": "http://arxiv.org/abs/cs/0310008v1",
        "categories": [
            "cs.DC",
            "cs.GR",
            "D.1.3"
        ]
    },
    {
        "title": "Circle and sphere blending with conformal geometric algebra",
        "authors": [
            "Chris Doran"
        ],
        "summary": "Blending schemes based on circles provide smooth `fair' interpolations between series of points. Here we demonstrate a simple, robust set of algorithms for performing circle blends for a range of cases. An arbitrary level of G-continuity can be achieved by simple alterations to the underlying parameterisation. Our method exploits the computational framework provided by conformal geometric algebra. This employs a five-dimensional representation of points in space, in contrast to the four-dimensional representation typically used in projective geometry. The advantage of the conformal scheme is that straight lines and circles are treated in a single, unified framework. As a further illustration of the power of the conformal framework, the basic idea is extended to the case of sphere blending to interpolate over a surface.",
        "published": "2003-10-09T15:15:41Z",
        "link": "http://arxiv.org/abs/cs/0310017v1",
        "categories": [
            "cs.CG",
            "cs.GR",
            "I.3.5"
        ]
    },
    {
        "title": "Visualization of variations in human brain morphology using   differentiating reflection functions",
        "authors": [
            "Gibby Koldenhof"
        ],
        "summary": "Conventional visualization media such as MRI prints and computer screens are inherently two dimensional, making them incapable of displaying true 3D volume data sets. By applying only transparency or intensity projection, and ignoring light-matter interaction, results will likely fail to give optimal results. Little research has been done on using reflectance functions to visually separate the various segments of a MRI volume. We will explore if applying specific reflectance functions to individual anatomical structures can help in building an intuitive 2D image from a 3D dataset. We will test our hypothesis by visualizing a statistical analysis of the genetic influences on variations in human brain morphology because it inherently contains complex and many different types of data making it a good candidate for our approach",
        "published": "2003-11-22T18:17:26Z",
        "link": "http://arxiv.org/abs/cs/0311034v1",
        "categories": [
            "cs.GR",
            "I.4.7;I.4.8;I.4.10;I.3.7"
        ]
    },
    {
        "title": "Benchmarking and Implementation of Probability-Based Simulations on   Programmable Graphics Cards",
        "authors": [
            "S. Tomov",
            "M. McGuigan",
            "R. Bennett",
            "G. Smith",
            "J. Spiletic"
        ],
        "summary": "The latest Graphics Processing Units (GPUs) are reported to reach up to   200 billion floating point operations per second (200 Gflops) and to have price performance of 0.1 cents per M flop. These facts raise great interest in the plausibility of extending the GPUs' use to non-graphics applications, in particular numerical simulations on structured grids (lattice).   We review previous work on using GPUs for non-graphics applications, implement probability-based simulations on the GPU, namely the   Ising and percolation models, implement vector operation benchmarks for the GPU, and finally compare the CPU's and GPU's performance.   A general conclusion from the results obtained is that moving computations from the CPU to the GPU is feasible, yielding good time and price performance, for certain lattice computations.   Preliminary results also show that it is feasible to use them in parallel",
        "published": "2003-12-02T15:47:19Z",
        "link": "http://arxiv.org/abs/cs/0312006v1",
        "categories": [
            "cs.GR",
            "cs.PF",
            "I.6.3; I.3.1; B.8.2"
        ]
    },
    {
        "title": "Differential Methods in Catadioptric Sensor Design with Applications to   Panoramic Imaging",
        "authors": [
            "R. Andrew Hicks"
        ],
        "summary": "We discuss design techniques for catadioptric sensors that realize given projections. In general, these problems do not have solutions, but approximate solutions may often be found that are visually acceptable. There are several methods to approach this problem, but here we focus on what we call the ``vector field approach''. An application is given where a true panoramic mirror is derived, i.e. a mirror that yields a cylindrical projection to the viewer without any digital unwarping.",
        "published": "2003-03-24T02:36:21Z",
        "link": "http://arxiv.org/abs/cs/0303024v1",
        "categories": [
            "cs.CV",
            "cs.RO",
            "I.2.9"
        ]
    },
    {
        "title": "State complexes for metamorphic robots",
        "authors": [
            "A. Abrams",
            "R. Ghrist"
        ],
        "summary": "A metamorphic robotic system is an aggregate of homogeneous robot units which can individually and selectively locomote in such a way as to change the global shape of the system. We introduce a mathematical framework for defining and analyzing general metamorphic robots. This formal structure, combined with ideas from geometric group theory, leads to a natural extension of a configuration space for metamorphic robots -- the state complex -- which is especially adapted to parallelization. We present an algorithm for optimizing reconfiguration sequences with respect to elapsed time. A universal geometric property of state complexes -- non-positive curvature -- is the key to proving convergence to the globally time-optimal solution.",
        "published": "2003-07-02T19:41:07Z",
        "link": "http://arxiv.org/abs/cs/0307004v1",
        "categories": [
            "cs.RO",
            "cs.CG",
            "I.2.9"
        ]
    },
    {
        "title": "Using Propagation for Solving Complex Arithmetic Constraints",
        "authors": [
            "M. H. van Emden",
            "B. Moa"
        ],
        "summary": "Solving a system of nonlinear inequalities is an important problem for which conventional numerical analysis has no satisfactory method. With a box-consistency algorithm one can compute a cover for the solution set to arbitrarily close approximation. Because of difficulties in the use of propagation for complex arithmetic expressions, box consistency is computed with interval arithmetic. In this paper we present theorems that support a simple modification of propagation that allows complex arithmetic expressions to be handled efficiently. The version of box consistency that is obtained in this way is stronger than when interval arithmetic is used.",
        "published": "2003-09-11T18:37:09Z",
        "link": "http://arxiv.org/abs/cs/0309018v1",
        "categories": [
            "math.NA",
            "cs.AR",
            "cs.CC",
            "cs.NA",
            "cs.PF",
            "cs.RO",
            "B.8; G.1.5;G.1.6;I.2.9;I.3.1;C.1.4;D.2.4;F.2"
        ]
    },
    {
        "title": "Replay Debugging of Complex Real-Time Systems: Experiences from Two   Industrial Case Studies",
        "authors": [
            "Daniel Sundmark",
            "Henrik Thane",
            "Joel Huselius",
            "Anders Pettersson",
            "Roger Mellander",
            "Ingemar Reiyer",
            "Mattias Kallvi"
        ],
        "summary": "Deterministic replay is a method for allowing complex multitasking real-time systems to be debugged using standard interactive debuggers. Even though several replay techniques have been proposed for parallel, multi-tasking and real-time systems, the solutions have so far lingered on a prototype academic level, with very little results to show from actual state-of-the-practice commercial applications. This paper describes a major deterministic replay debugging case study performed on a full-scale industrial robot control system, as well as a minor replay instrumentation case study performed on a military aircraft radar system. In this article, we will show that replay debugging is feasible in complex multi-million lines of code software projects running on top of off-the-shelf real-time operating systems. Furthermore, we will discuss how replay debugging can be introduced in existing systems without impracticable analysis efforts. In addition, we will present benchmarking results from both studies, indicating that the instrumentation overhead is acceptable and affordable.",
        "published": "2003-11-17T19:15:59Z",
        "link": "http://arxiv.org/abs/cs/0311019v1",
        "categories": [
            "cs.RO",
            "D.2.5"
        ]
    },
    {
        "title": "A Script Language for Data Integration in Database",
        "authors": [
            "Qingguo Zheng"
        ],
        "summary": "A Script Language in this paper is designed to transform the original data into the target data by the computing formula. The Script Language can be translated into the corresponding SQL Language, and the computation is finally implemented by the first type of dynamic SQL. The Script Language has the operations of insert, update, delete, union, intersect, and minus for the table in the database.The Script Language is edited by a text file and you can easily modify the computing formula in the text file to deal with the situations when the computing formula have been changed. So you only need modify the text of the script language, but needn't change the programs that have complied.",
        "published": "2003-01-13T05:34:39Z",
        "link": "http://arxiv.org/abs/cs/0301009v1",
        "categories": [
            "cs.DB",
            "H.2.5"
        ]
    },
    {
        "title": "Completeness and Decidability Properties for Functional Dependencies in   XML",
        "authors": [
            "Millist W. Vincent",
            "Jixue Liu"
        ],
        "summary": "XML is of great importance in information storage and retrieval because of its recent emergence as a standard for data representation and interchange on the Internet. However XML provides little semantic content and as a result several papers have addressed the topic of how to improve the semantic expressiveness of XML. Among the most important of these approaches has been that of defining integrity constraints in XML. In a companion paper we defined strong functional dependencies in XML(XFDs). We also presented a set of axioms for reasoning about the implication of XFDs and showed that the axiom system is sound for arbitrary XFDs. In this paper we prove that the axioms are also complete for unary XFDs (XFDs with a single path on the l.h.s.). The second contribution of the paper is to prove that the implication problem for unary XFDs is decidable and to provide a linear time algorithm for it.",
        "published": "2003-01-20T00:13:18Z",
        "link": "http://arxiv.org/abs/cs/0301017v1",
        "categories": [
            "cs.DB",
            "H.2.1"
        ]
    },
    {
        "title": "Unique Pattern Matching in Strings",
        "authors": [
            "Stijn Vansummeren"
        ],
        "summary": "Regular expression patterns are a key feature of document processing languages like Perl and XDuce. It is in this context that the first and longest match policies have been proposed to disambiguate the pattern matching process. We formally define a matching semantics with these policies and show that the generally accepted method of simulating longest match by first match and recursion is incorrect. We continue by solving the associated type inference problem, which consists in calculating for every subexpression the set of words the subexpression can still match when these policies are in effect, and show how this algorithm can be used to efficiently implement the matching process.",
        "published": "2003-02-03T15:52:17Z",
        "link": "http://arxiv.org/abs/cs/0302004v1",
        "categories": [
            "cs.PL",
            "cs.DB",
            "D.3.3; F.1.1; H.2.3; H.3.3; H.2.4; I.5.5"
        ]
    },
    {
        "title": "Probabilistic behavior of hash tables",
        "authors": [
            "Dawei Hong",
            "Jean-Camille Birget",
            "Shushuang Man"
        ],
        "summary": "We extend a result of Goldreich and Ron about estimating the collision probability of a hash function. Their estimate has a polynomial tail. We prove that when the load factor is greater than a certain constant, the estimator has a gaussian tail. As an application we find an estimate of an upper bound for the average search time in hashing with chaining, for a particular user (we allow the overall key distribution to be different from the key distribution of a particular user). The estimator has a gaussian tail.",
        "published": "2003-03-21T17:25:15Z",
        "link": "http://arxiv.org/abs/cs/0303022v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "E.2"
        ]
    },
    {
        "title": "Applying Data Mining and Machine Learning Techniques to Submarine   Intelligence Analysis",
        "authors": [
            "Ulla Bergsten",
            "Johan Schubert",
            "Per Svensson"
        ],
        "summary": "We describe how specialized database technology and data analysis methods were applied by the Swedish defense to help deal with the violation of Swedish marine territory by foreign submarine intruders during the Eighties and early Nineties. Among several approaches tried some yielded interesting information, although most of the key questions remain unanswered. We conclude with a survey of belief-function- and genetic-algorithm-based methods which were proposed to support interpretation of intelligence reports and prediction of future submarine positions, respectively.",
        "published": "2003-05-16T15:41:58Z",
        "link": "http://arxiv.org/abs/cs/0305022v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.NE",
            "H.2.8; H.4.2; I.2.3; I.5.3"
        ]
    },
    {
        "title": "Beslutstödssystemet Dezzy - en översikt",
        "authors": [
            "Ulla Bergsten",
            "Johan Schubert",
            "Per Svensson"
        ],
        "summary": "Within the scope of the three-year ANTI-SUBMARINE WARFARE project of the National Defence Research Establishment, the INFORMATION SYSTEMS subproject has developed the demonstration prototype Dezzy for handling and analysis of intelligence reports concerning foreign underwater activities.   -----   Inom ramen f\\\"or FOA:s tre{\\aa}riga huvudprojekt UB{\\AA}TSSKYDD har delprojekt INFORMATIONSSYSTEM utvecklat demonstrationsprototypen Dezzy till ett beslutsst\\\"odsystem f\\\"or hantering och analys av underr\\\"attelser om fr\\\"ammande undervattensverksamhet.",
        "published": "2003-05-16T18:26:22Z",
        "link": "http://arxiv.org/abs/cs/0305033v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "H.4.2; I.2.3"
        ]
    },
    {
        "title": "The Evolution of the Computerized Database",
        "authors": [
            "Nancy Hartline Bercich"
        ],
        "summary": "Databases, collections of related data, are as old as the written word. A database can be anything from a homemaker's metal recipe file to a sophisticated data warehouse. Yet today, when we think of a database we invariably think of computerized data and their DBMSs (database management systems). How did we go from organizing our data in a simple metal filing box or cabinet to storing our data in a sophisticated computerized database? How did the computerized database evolve?   This paper defines what we mean by a database. It traces the evolution of the database, from its start as a non-computerized set of related data, to the, now standard, computerized RDBMS (relational database management system). Early computerized storage methods are reviewed including both the ISAM (Indexed Sequential Access Method) and VSAM (Virtual Storage Access Method) storage methods. Early database models are explored including the network and hierarchical database models. Eventually, the relational, object-relational and object-oriented databases models are discussed. An appendix of diagrams, including hierarchical occurrence tree, network schema, ER (entity relationship) and UML (unified modeling language) diagrams, is included to support the text.   This paper concludes with an exploration of current and future trends in DBMS development. It discusses the factors affecting these trends. It delves into the relationship between DBMSs and the increasingly popular object-oriented development methodologies. Finally, it speculates on the future of the DBMS.",
        "published": "2003-05-21T15:59:56Z",
        "link": "http://arxiv.org/abs/cs/0305038v1",
        "categories": [
            "cs.DB",
            "H.2.m"
        ]
    },
    {
        "title": "Configuration Database for BaBar On-line",
        "authors": [
            "R. Bartoldus",
            "G. Dubois-Felsmann",
            "Y. Kolomensky",
            "A. Salnikov"
        ],
        "summary": "The configuration database is one of the vital systems in the BaBar on-line system. It provides services for the different parts of the data acquisition system and control system, which require run-time parameters. The original design and implementation of the configuration database played a significant role in the successful BaBar operations since the beginning of experiment. Recent additions to the design of the configuration database provide better means for the management of data and add new tools to simplify main configuration tasks. We describe the design of the configuration database, its implementation with the Objectivity/DB object-oriented database, and our experience collected during the years of operation.",
        "published": "2003-05-29T21:37:47Z",
        "link": "http://arxiv.org/abs/cs/0305056v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.2.4; H.2.8"
        ]
    },
    {
        "title": "Experience with the Open Source based implementation for ATLAS   Conditions Data Management System",
        "authors": [
            "A. Amorim",
            "J. Lima",
            "C. Oliveira",
            "L. Pedro",
            "N. Barros"
        ],
        "summary": "Conditions Data in high energy physics experiments is frequently seen as every data needed for reconstruction besides the event data itself. This includes all sorts of slowly evolving data like detector alignment, calibration and robustness, and data from detector control system. Also, every Conditions Data Object is associated with a time interval of validity and a version. Besides that, quite often is useful to tag collections of Conditions Data Objects altogether. These issues have already been investigated and a data model has been proposed and used for different implementations based in commercial DBMSs, both at CERN and for the BaBar experiment. The special case of the ATLAS complex trigger that requires online access to calibration and alignment data poses new challenges that have to be met using a flexible and customizable solution more in the line of Open Source components. Motivated by the ATLAS challenges we have developed an alternative implementation, based in an Open Source RDBMS. Several issues were investigated land will be described in this paper:   -The best way to map the conditions data model into the relational database concept considering what are foreseen as the most frequent queries.   -The clustering model best suited to address the scalability problem. -Extensive tests were performed and will be described.   The very promising results from these tests are attracting the attention from the HEP community and driving further developments.",
        "published": "2003-05-30T23:08:44Z",
        "link": "http://arxiv.org/abs/cs/0306006v2",
        "categories": [
            "cs.DB",
            "H.2.4; H.2.2; H.3.3"
        ]
    },
    {
        "title": "Transparent Persistence with Java Data Objects",
        "authors": [
            "Julius Hrivnac"
        ],
        "summary": "Flexible and performant Persistency Service is a necessary component of any HEP Software Framework. The building of a modular, non-intrusive and performant persistency component have been shown to be very difficult task. In the past, it was very often necessary to sacrifice modularity to achieve acceptable performance. This resulted in the strong dependency of the overall Frameworks on their Persistency subsystems.   Recent development in software technology has made possible to build a Persistency Service which can be transparently used from other Frameworks. Such Service doesn't force a strong architectural constraints on the overall Framework Architecture, while satisfying high performance requirements. Java Data Object standard (JDO) has been already implemented for almost all major databases. It provides truly transparent persistency for any Java object (both internal and external). Objects in other languages can be handled via transparent proxies. Being only a thin layer on top of a used database, JDO doesn't introduce any significant performance degradation. Also Aspect-Oriented Programming (AOP) makes possible to treat persistency as an orthogonal Aspect of the Application Framework, without polluting it with persistence-specific concepts.   All these techniques have been developed primarily (or only) for the Java environment. It is, however, possible to interface them transparently to Frameworks built in other languages, like for example C++.   Fully functional prototypes of flexible and non-intrusive persistency modules have been build for several other packages, as for example FreeHEP AIDA and LCG Pool AttributeSet (package Indicium).",
        "published": "2003-06-02T09:38:47Z",
        "link": "http://arxiv.org/abs/cs/0306013v1",
        "categories": [
            "cs.DB",
            "H.2"
        ]
    },
    {
        "title": "Relational databases for data management in PHENIX",
        "authors": [
            "I. Sourikova",
            "D. Morrison"
        ],
        "summary": "PHENIX is one of the two large experiments at the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory (BNL) and archives roughly 100TB of experimental data per year. In addition, large volumes of simulated data are produced at multiple off-site computing centers. For any file catalog to play a central role in data management it has to face problems associated with the need for distributed access and updates. To be used effectively by the hundreds of PHENIX collaborators in 12 countries the catalog must satisfy the following requirements: 1) contain up-to-date data, 2) provide fast and reliable access to the data, 3) have write permissions for the sites that store portions of data. We present an analysis of several available Relational Database Management Systems (RDBMS) to support a catalog meeting the above requirements and discuss the PHENIX experience with building and using the distributed file catalog.",
        "published": "2003-06-04T17:38:23Z",
        "link": "http://arxiv.org/abs/cs/0306019v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "On the Verge of One Petabyte - the Story Behind the BaBar Database   System",
        "authors": [
            "Adeyemi Adesanya",
            "Tofigh Azemoon",
            "Jacek Becla",
            "Andrew Hanushevsky",
            "Adil Hasan",
            "Wilko Kroeger",
            "Artem Trunov",
            "Daniel Wang",
            "Igor Gaponenko",
            "Simon Patton",
            "David Quarrie"
        ],
        "summary": "The BaBar database has pioneered the use of a commercial ODBMS within the HEP community. The unique object-oriented architecture of Objectivity/DB has made it possible to manage over 700 terabytes of production data generated since May'99, making the BaBar database the world's largest known database. The ongoing development includes new features, addressing the ever-increasing luminosity of the detector as well as other changing physics requirements. Significant efforts are focused on reducing space requirements and operational costs. The paper discusses our experience with developing a large scale database system, emphasizing universal aspects which may be applied to any large scale system, independently of underlying technology used.",
        "published": "2003-06-04T19:22:52Z",
        "link": "http://arxiv.org/abs/cs/0306020v2",
        "categories": [
            "cs.DB",
            "C.2.4; H.2"
        ]
    },
    {
        "title": "The Redesigned BaBar Event Store: Believe the Hype",
        "authors": [
            "Adeyemi Adesanya",
            "Jacek Becla",
            "Daniel Wang"
        ],
        "summary": "As the BaBar experiment progresses, it produces new and unforeseen requirements and increasing demands on capacity and feature base. The current system is being utilized well beyond its original design specifications, and has scaled appropriately, maintaining data consistency and durability. The persistent event storage system has remained largely unchanged since the initial implementation, and thus includes many design features which have become performance bottlenecks. Programming interfaces were designed before sufficient usage information became available. Performance and efficiency were traded off for added flexibility to cope with future demands. With significant experience in managing actual production data under our belt, we are now in a position to recraft the system to better suit current needs. The Event Store redesign is intended to eliminate redundant features while adding new ones, increase overall performance, and contain the physical storage cost of the world's largest database.",
        "published": "2003-06-04T23:51:52Z",
        "link": "http://arxiv.org/abs/cs/0306023v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "H.2.1; H.2.4; E.2"
        ]
    },
    {
        "title": "A ROOT/IO Based Software Framework for CMS",
        "authors": [
            "William Tanenbaum"
        ],
        "summary": "The implementation of persistency in the Compact Muon Solenoid (CMS) Software Framework uses the core I/O functionality of ROOT. We will discuss the current ROOT/IO implementation, its evolution from the prior Objectivity/DB implementation, and the plans and ongoing work for the conversion to \"POOL\", provided by the LHC Computing Grid (LCG) persistency project.",
        "published": "2003-06-07T15:09:16Z",
        "link": "http://arxiv.org/abs/cs/0306034v1",
        "categories": [
            "cs.DB",
            "E.1"
        ]
    },
    {
        "title": "Hyperdense Coding Modulo 6 with Filter-Machines",
        "authors": [
            "Vince Grolmusz"
        ],
        "summary": "We show how one can encode $n$ bits with $n^{o(1)}$ ``wave-bits'' using still hypothetical filter-machines (here $o(1)$ denotes a positive quantity which goes to 0 as $n$ goes to infity). Our present result - in a completely different computational model - significantly improves on the quantum superdense-coding breakthrough of Bennet and Wiesner (1992) which encoded $n$ bits by $\\lceil{n/2}\\rceil$ quantum-bits. We also show that our earlier algorithm (Tech. Rep. TR03-001, ECCC, See ftp://ftp.eccc.uni-trier.de/pub/eccc/reports/2003/TR03-001/index.html) which used $n^{o(1)}$ muliplication for computing a representation of the dot-product of two $n$-bit sequences modulo 6, and, similarly, an algorithm for computing a representation of the multiplication of two $n\\times n$ matrices with $n^{2+o(1)}$ multiplications can be turned to algorithms computing the exact dot-product or the exact matrix-product with the same number of multiplications with filter-machines. With classical computation, computing the dot-product needs $\\Omega(n)$ multiplications and the best known algorithm for matrix multiplication (D. Coppersmith and S. Winograd, Matrix multiplication via arithmetic progressions, J. Symbolic Comput., 9(3):251--280, 1990) uses $n^{2.376}$ multiplications.",
        "published": "2003-06-11T20:31:46Z",
        "link": "http://arxiv.org/abs/cs/0306049v1",
        "categories": [
            "cs.CC",
            "cs.DB",
            "F.1.1"
        ]
    },
    {
        "title": "Twelve Ways to Build CMS Crossings from ROOT Files",
        "authors": [
            "D. Chamont",
            "C. Charlot"
        ],
        "summary": "The simulation of CMS raw data requires the random selection of one hundred and fifty pileup events from a very large set of files, to be superimposed in memory to the signal event. The use of ROOT I/O for that purpose is quite unusual: the events are not read sequentially but pseudo-randomly, they are not processed one by one in memory but by bunches, and they do not contain orthodox ROOT objects but many foreign objects and templates. In this context, we have compared the performance of ROOT containers versus the STL vectors, and the use of trees versus a direct storage of containers. The strategy with best performances is by far the one using clones within trees, but it stays hard to tune and very dependant on the exact use-case. The use of STL vectors could bring more easily similar performances in a future ROOT release.",
        "published": "2003-06-12T17:24:16Z",
        "link": "http://arxiv.org/abs/cs/0306056v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "Operational Aspects of Dealing with the Large BaBar Data Set",
        "authors": [
            "Tofigh Azemoon",
            "Adil Hasan",
            "Wilko Kroeger",
            "Artem Trunov"
        ],
        "summary": "To date, the BaBar experiment has stored over 0.7PB of data in an Objectivity/DB database. Approximately half this data-set comprises simulated data of which more than 70% has been produced at more than 20 collaborating institutes outside of SLAC. The operational aspects of managing such a large data set and providing access to the physicists in a timely manner is a challenging and complex problem. We describe the operational aspects of managing such a large distributed data-set as well as importing and exporting data from geographically spread BaBar collaborators. We also describe problems common to dealing with such large datasets.",
        "published": "2003-06-13T00:40:18Z",
        "link": "http://arxiv.org/abs/cs/0306061v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "H.2.7; E.5; J.2"
        ]
    },
    {
        "title": "The COMPASS Event Store in 2002",
        "authors": [
            "Venicio Duic",
            "Massimo Lamanna"
        ],
        "summary": "COMPASS, the fixed-target experiment at CERN studying the structure of the nucleon and spectroscopy, collected over 260 TB during summer 2002 run. All these data, together with reconstructed events information, were put from the beginning in a database infrastructure based on Objectivity/DB and on the hierarchical storage manager CASTOR. The experience in the usage of the database is reviewed and the evolution of the system outlined.",
        "published": "2003-06-13T14:33:53Z",
        "link": "http://arxiv.org/abs/cs/0306066v1",
        "categories": [
            "cs.DB",
            "H.2; H.3"
        ]
    },
    {
        "title": "POOL File Catalog, Collection and Metadata Components",
        "authors": [
            "C. Cioffi",
            "S. Eckmann",
            "M. Girone",
            "J. Hrivnac",
            "D. Malon",
            "H. Schmuecker",
            "A. Vaniachine",
            "J. Wojcieszuk",
            "Z. Xie"
        ],
        "summary": "The POOL project is the common persistency framework for the LHC experiments to store petabytes of experiment data and metadata in a distributed and grid enabled way. POOL is a hybrid event store consisting of a data streaming layer and a relational layer. This paper describes the design of file catalog, collection and metadata components which are not part of the data streaming layer of POOL and outlines how POOL aims to provide transparent and efficient data access for a wide range of environments and use cases - ranging from a large production site down to a single disconnected laptops. The file catalog is the central POOL component translating logical data references to physical data files in a grid environment. POOL collections with their associated metadata provide an abstract way of accessing experiment data via their logical grouping into sets of related data objects.",
        "published": "2003-06-13T16:21:53Z",
        "link": "http://arxiv.org/abs/cs/0306065v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "The TESLA Requirements Database",
        "authors": [
            "Lars Hagge",
            "Jens Kreutzkamp",
            "Kathrin Lappe"
        ],
        "summary": "In preparation for the planned linear collider TESLA, DESY is designing the required buildings and facilities. The accelerator and infrastructure components have to be allocated to buildings, and their required areas for installation, operation and maintenance have to be determined. Interdisciplinary working groups specify the project from different viewpoints and need to develop a common vision as a precondition for an optimal solution. A commercial requirements database is used as a collaborative tool, enabling concurrent requirements specification by independent working groups. The requirements database ensures long term storage and availability of the emerging knowledge, and it offers a central platform for communication which is available for all project members. It is successfully operating since summer 2002 and has since then become an important tool for the design team.",
        "published": "2003-06-13T22:10:36Z",
        "link": "http://arxiv.org/abs/cs/0306077v1",
        "categories": [
            "cs.DB",
            "D.2.1;H.4.0;K.6.1"
        ]
    },
    {
        "title": "Integrated Information Management for TESLA",
        "authors": [
            "Jochen Buerger",
            "Lars Hagge",
            "Jens Kreutzkamp",
            "Kathrin Lappe",
            "Andrea Robben"
        ],
        "summary": "Next-generation projects in High Energy Physics will reach again a new dimension of complexity. Information management has to ensure an efficient and economic information flow within the collaborations, offering world-wide up-to-date information access to the collaborators as one condition for successful projects. DESY introduces several information systems in preparation for the planned linear collider TESLA: a Requirements Management System (RMS) is in production for the TESLA planning group, a Product Data Management System (PDMS) is in production since the beginning of 2002 and is supporting the cavity preparation and the general engineering of accelerator components. A pilot Asset Management System (AMS) is in production for supporting the management and maintenance of the technical infrastructure, and a Facility Management System (FMS) with a Geographic Information System (GIS) is currently being introduced to support civil engineering. Efforts have been started to integrate the systems with the goal that users can retrieve information through a single point of access. The paper gives an introduction to information management and the activities at DESY.",
        "published": "2003-06-13T23:04:10Z",
        "link": "http://arxiv.org/abs/cs/0306079v1",
        "categories": [
            "cs.DB",
            "H.4.0;K.6.1"
        ]
    },
    {
        "title": "An on-line Integrated Bookkeeping: electronic run log book and Meta-Data   Repository for ATLAS",
        "authors": [
            "M. Barczyc",
            "D. Burckhart-Chromek",
            "M. Caprini",
            "J. Da Silva Conceicao",
            "M. Dobson",
            "J. Flammer",
            "R. Jones",
            "A. Kazarov",
            "S. Kolos",
            "D. Liko",
            "L. Mapelli",
            "I. Soloviev",
            "R. Hart NIKHEF",
            "A. Amorim",
            "D. Klose",
            "J. Lima",
            "L. Lucio",
            "L. Pedro",
            "H. Wolters",
            "E. Badescu NIPNE",
            "I. Alexandrov",
            "V. Kotov",
            "M. Mineev JINR",
            "Yu. Ryabov PNPI"
        ],
        "summary": "In the context of the ATLAS experiment there is growing evidence of the importance of different kinds of Meta-data including all the important details of the detector and data acquisition that are vital for the analysis of the acquired data. The Online BookKeeper (OBK) is a component of ATLAS online software that stores all information collected while running the experiment, including the Meta-data associated with the event acquisition, triggering and storage. The facilities for acquisition of control data within the on-line software framework, together with a full functional Web interface, make the OBK a powerful tool containing all information needed for event analysis, including an electronic log book.   In this paper we explain how OBK plays a role as one of the main collectors and managers of Meta-data produced on-line, and we'll also focus on the Web facilities already available. The usage of the web interface as an electronic run logbook is also explained, together with the future extensions.   We describe the technology used in OBK development and how we arrived at the present level explaining the previous experience with various DBMS technologies. The extensive performance evaluations that have been performed and the usage in the production environment of the ATLAS test beams are also analysed.",
        "published": "2003-06-14T00:42:30Z",
        "link": "http://arxiv.org/abs/cs/0306081v1",
        "categories": [
            "cs.DB",
            "H.2.4;H.2.8"
        ]
    },
    {
        "title": "The MammoGrid Project Grids Architecture",
        "authors": [
            "Richard McClatchey",
            "Predrag Buncic",
            "David Manset",
            "Tamas Hauer",
            "Florida Estrella",
            "Pablo Saiz",
            "Dmitri Rogulin"
        ],
        "summary": "The aim of the recently EU-funded MammoGrid project is, in the light of emerging Grid technology, to develop a European-wide database of mammograms that will be used to develop a set of important healthcare applications and investigate the potential of this Grid to support effective co-working between healthcare professionals throughout the EU. The MammoGrid consortium intends to use a Grid model to enable distributed computing that spans national borders. This Grid infrastructure will be used for deploying novel algorithms as software directly developed or enhanced within the project. Using the MammoGrid clinicians will be able to harness the use of massive amounts of medical image data to perform epidemiological studies, advanced image processing, radiographic education and ultimately, tele-diagnosis over communities of medical \"virtual organisations\". This is achieved through the use of Grid-compliant services [1] for managing (versions of) massively distributed files of mammograms, for handling the distributed execution of mammograms analysis software, for the development of Grid-aware algorithms and for the sharing of resources between multiple collaborating medical centres. All this is delivered via a novel software and hardware information infrastructure that, in addition guarantees the integrity and security of the medical data. The MammoGrid implementation is based on AliEn, a Grid framework developed by the ALICE Collaboration. AliEn provides a virtual file catalogue that allows transparent access to distributed data-sets and provides top to bottom implementation of a lightweight Grid applicable to cases when handling of a large number of files is required. This paper details the architecture that will be implemented by the MammoGrid project.",
        "published": "2003-06-16T06:53:57Z",
        "link": "http://arxiv.org/abs/cs/0306095v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C2.4,H2.4,J.3"
        ]
    },
    {
        "title": "Prototyping Virtual Data Technologies in ATLAS Data Challenge 1   Production",
        "authors": [
            "A. Vaniachine",
            "D. Malon",
            "P. Nevski",
            "K. De"
        ],
        "summary": "For efficiency of the large production tasks distributed worldwide, it is essential to provide shared production management tools comprised of integratable and interoperable services. To enhance the ATLAS DC1 production toolkit, we introduced and tested a Virtual Data services component. For each major data transformation step identified in the ATLAS data processing pipeline (event generation, detector simulation, background pile-up and digitization, etc) the Virtual Data Cookbook (VDC) catalogue encapsulates the specific data transformation knowledge and the validated parameters settings that must be provided before the data transformation invocation. To provide for local-remote transparency during DC1 production, the VDC database server delivered in a controlled way both the validated production parameters and the templated production recipes for thousands of the event generation and detector simulation jobs around the world, simplifying the production management solutions.",
        "published": "2003-06-16T19:54:33Z",
        "link": "http://arxiv.org/abs/cs/0306102v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4; H.2.4"
        ]
    },
    {
        "title": "Primary Numbers Database for ATLAS Detector Description Parameters",
        "authors": [
            "A. Vaniachine",
            "S. Eckmann",
            "D. Malon",
            "P. Nevski",
            "T. Wenaus"
        ],
        "summary": "We present the design and the status of the database for detector description parameters in ATLAS experiment. The ATLAS Primary Numbers are the parameters defining the detector geometry and digitization in simulations, as well as certain reconstruction parameters. Since the detailed ATLAS detector description needs more than 10,000 such parameters, a preferred solution is to have a single verified source for all these data. The database stores the data dictionary for each parameter collection object, providing schema evolution support for object-based retrieval of parameters. The same Primary Numbers are served to many different clients accessing the database: the ATLAS software framework Athena, the Geant3 heritage framework Atlsim, the Geant4 developers framework FADS/Goofy, the generator of XML output for detector description, and several end-user clients for interactive data navigation, including web-based browsers and ROOT. The choice of the MySQL database product for the implementation provides additional benefits: the Primary Numbers database can be used on the developers laptop when disconnected (using the MySQL embedded server technology), with data being updated when the laptop is connected (using the MySQL database replication).",
        "published": "2003-06-16T19:59:17Z",
        "link": "http://arxiv.org/abs/cs/0306103v1",
        "categories": [
            "cs.DB",
            "cs.HC",
            "H.2.4"
        ]
    },
    {
        "title": "Distributed Heterogeneous Relational Data Warehouse In A Grid   Environment",
        "authors": [
            "Saima Iqbal",
            "Julian J. Bunn",
            "Harvey B. Newman"
        ],
        "summary": "This paper examines how a \"Distributed Heterogeneous Relational Data Warehouse\" can be integrated in a Grid environment that will provide physicists with efficient access to large and small object collections drawn from databases at multiple sites. This paper investigates the requirements of Grid-enabling such a warehouse, and explores how these requirements may be met by extensions to existing Grid middleware. We present initial results obtained with a working prototype warehouse of this kind using both SQLServer and Oracle9i, where a Grid-enabled web-services interface makes it easier for web-applications to access the distributed contents of the databases securely. Based on the success of the prototype, we proposes a framework for using heterogeneous relational data warehouse through the web-service interface and create a single \"Virtual Database System\" for users. The ability to transparently access data in this way, as shown in prototype, is likely to be a very powerful facility for HENP and other grid users wishing to collate and analyze information distributed over Grid.",
        "published": "2003-06-18T09:30:03Z",
        "link": "http://arxiv.org/abs/cs/0306109v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.2.1;H.2.2;H.2.4;H.2.7;H.3.1;H.3.5"
        ]
    },
    {
        "title": "Serving Database Information Using a Flexible Server in a Three Tier   Architecture",
        "authors": [
            "Herbert Greenlee",
            "Robert Illingworth",
            "Jim Kowalkowski",
            "Anil Kumar",
            "Lee Lueking",
            "Taka Yasuda",
            "Margherita Vittone",
            "Stephen White"
        ],
        "summary": "The D0 experiment at Fermilab relies on a central Oracle database for storing all detector calibration information. Access to this data is needed by hundreds of physics applications distributed worldwide. In order to meet the demands of these applications from scarce resources, we have created a distributed system that isolates the user applications from the database facilities. This system, known as the Database Application Network (DAN) operates as the middle tier in a three tier architecture. A DAN server employs a hierarchical caching scheme and database connection management facility that limits access to the database resource. The modular design allows for caching strategies and database access components to be determined by runtime configuration. To solve scalability problems, a proxy database component allows for DAN servers to be arranged in a hierarchy. Also included is an event based monitoring system that is currently being used to collect statistics for performance analysis and problem diagnosis. DAN servers are currently implemented as a Python multithreaded program using CORBA for network communications and interface specification. The requirement details, design, and implementation of DAN are discussed along with operational experience and future plans.",
        "published": "2003-07-01T02:13:32Z",
        "link": "http://arxiv.org/abs/cs/0307001v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "A.0;H.2.8"
        ]
    },
    {
        "title": "Architecture of an Open-Sourced, Extensible Data Warehouse Builder:   InterBase 6 Data Warehouse Builder (IB-DWB)",
        "authors": [
            "Maurice HT Ling",
            "Chi Wai So"
        ],
        "summary": "We report the development of an open-sourced data warehouse builder, InterBase Data Warehouse Builder (IB-DWB), based on Borland InterBase 6 Open Edition Database Server. InterBase 6 is used for its low maintenance and small footprint. IB-DWB is designed modularly and consists of 5 main components, Data Plug Platform, Discoverer Platform, Multi-Dimensional Cube Builder, and Query Supporter, bounded together by a Kernel. It is also an extensible system, made possible by the Data Plug Platform and the Discoverer Platform. Currently, extensions are only possible via dynamic linked-libraries (DLLs). Multi-Dimensional Cube Builder represents a basal mean of data aggregation. The architectural philosophy of IB-DWB centers around providing a base platform that is extensible, which is functionally supported by expansion modules. IB-DWB is currently being hosted by sourceforge.net (Project Unix Name: ib-dwb), licensed under GNU General Public License, Version 2.",
        "published": "2003-07-07T14:07:59Z",
        "link": "http://arxiv.org/abs/cs/0307015v2",
        "categories": [
            "cs.DB",
            "H.2.8; H.4.2"
        ]
    },
    {
        "title": "Data Management and Mining in Astrophysical Databases",
        "authors": [
            "M. Frailis",
            "A. De Angelis",
            "V. Roberto"
        ],
        "summary": "We analyse the issues involved in the management and mining of astrophysical data. The traditional approach to data management in the astrophysical field is not able to keep up with the increasing size of the data gathered by modern detectors. An essential role in the astrophysical research will be assumed by automatic tools for information extraction from large datasets, i.e. data mining techniques, such as clustering and classification algorithms. This asks for an approach to data management based on data warehousing, emphasizing the efficiency and simplicity of data access; efficiency is obtained using multidimensional access methods and simplicity is achieved by properly handling metadata. Clustering and classification techniques, on large datasets, pose additional requirements: computational and memory scalability with respect to the data size, interpretability and objectivity of clustering or classification results. In this study we address some possible solutions.",
        "published": "2003-07-12T12:35:37Z",
        "link": "http://arxiv.org/abs/cs/0307032v2",
        "categories": [
            "cs.DB",
            "astro-ph",
            "physics.data-an",
            "H.2.4; H.2.8"
        ]
    },
    {
        "title": "Search and Navigation in Relational Databases",
        "authors": [
            "Richard Wheeldon",
            "Mark Levene",
            "Kevin Keenoy"
        ],
        "summary": "We present a new application for keyword search within relational databases, which uses a novel algorithm to solve the join discovery problem by finding Memex-like trails through the graph of foreign key dependencies. It differs from previous efforts in the algorithms used, in the presentation mechanism and in the use of primary-key only database queries at query-time to maintain a fast response for users. We present examples using the DBLP data set.",
        "published": "2003-07-31T11:18:51Z",
        "link": "http://arxiv.org/abs/cs/0307073v1",
        "categories": [
            "cs.DB",
            "H.3;H.4;H.5"
        ]
    },
    {
        "title": "Two- versus three-dimensional connectivity testing of first-order   queries to semi-algebraic sets",
        "authors": [
            "Floris Geerts",
            "Lieven Smits",
            "Jan Van den Bussche"
        ],
        "summary": "This paper addresses the question whether one can determine the connectivity of a semi-algebraic set in three dimensions by testing the connectivity of a finite number of two-dimensional ``samples'' of the set, where these samples are defined by first-order queries. The question is answered negatively for two classes of first-order queries: cartesian-product-free, and positive one-pass.",
        "published": "2003-08-01T00:59:41Z",
        "link": "http://arxiv.org/abs/cs/0308001v2",
        "categories": [
            "cs.LO",
            "cs.CG",
            "cs.DB",
            "F.4.1; F.2.2; H.2.8"
        ]
    },
    {
        "title": "DPG: A Cache-Efficient Accelerator for Sorting and for Join Operators",
        "authors": [
            "Gene Cooperman",
            "Xiaoqin Ma",
            "Viet Ha Nguyen"
        ],
        "summary": "We present a new algorithm for fast record retrieval, distribute-probe-gather, or DPG. DPG has important applications both in sorting and in joins. Current main memory sorting algorithms split their work into three phases: extraction of key-pointer pairs; sorting of the key-pointer pairs; and copying of the original records into the destination array according the sorted key-pointer pairs. The copying in the last phase dominates today's sorting time. Hence, the use of DPG in the third phase provides an accelerator for existing sorting algorithms.   DPG also provides two new join methods for foreign key joins: DPG-move join and DPG-sort join. The resulting join methods with DPG are faster because DPG join is cache-efficient and at the same time DPG join avoids the need for sorting or for hashing. The ideas presented for foreign key join can also be extended to faster record pair retrieval for spatial and temporal databases.",
        "published": "2003-08-02T08:13:06Z",
        "link": "http://arxiv.org/abs/cs/0308004v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "E.1; E.2; F.2.2"
        ]
    },
    {
        "title": "A Robust and Computational Characterisation of Peer-to-Peer Database   Systems",
        "authors": [
            "Enrico Franconi",
            "Gabriel Kuper",
            "Andrei Lopatenko",
            "Luciano Serafini"
        ],
        "summary": "In this paper we give a robust logical and computational characterisation of peer-to-peer database systems. We first define a pre- cise model-theoretic semantics of a peer-to-peer system, which allows for local inconsistency handling. We then characterise the general computa- tional properties for the problem of answering queries to such a peer-to- peer system. Finally, we devise tight complexity bounds and distributed procedures for the problem of answering queries in few relevant special cases.",
        "published": "2003-08-06T12:50:22Z",
        "link": "http://arxiv.org/abs/cs/0308013v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.2.4;H.2.5;C.2.4"
        ]
    },
    {
        "title": "On the expressive power of semijoin queries",
        "authors": [
            "Dirk Leinders",
            "Jerzy Tyszkiewicz",
            "Jan Van den Bussche"
        ],
        "summary": "The semijoin algebra is the variant of the relational algebra obtained by replacing the join operator by the semijoin operator. We provide an Ehrenfeucht-Fraiss\\'{e} game, characterizing the discerning power of the semijoin algebra. This game gives a method for showing that queries are not expressible in the semijoin algebra.",
        "published": "2003-08-06T14:12:11Z",
        "link": "http://arxiv.org/abs/cs/0308014v2",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3; F.4.1"
        ]
    },
    {
        "title": "Indexing of Tables Referencing Complex Structures",
        "authors": [
            "Agust S. Egilsson",
            "Hakon Gudbjartsson"
        ],
        "summary": "We introduce indexing of tables referencing complex structures such as digraphs and spatial objects, appearing in genetics and other data intensive analysis. The indexing is achieved by extracting dimension schemas from the referenced structures. The schemas and their dimensionality are determined by proper coloring algorithms and the duality between all such schemas and all such possible proper colorings is established. This duality, in turn, provides us with an extensive library of solutions when addressing indexing questions. It is illustrated how to use the schemas, in connection with additional relational database technologies, to optimize queries conditioned on the structural information being referenced. Comparisons using bitmap indexing in the Oracle 9.2i database, on the one hand, and multidimensional clustering in DB2 8.1.2, on the other hand, are used to illustrate the applicability of the indexing to different technology settings. Finally, we illustrate how the indexing can be used to extract low dimensional schemas from a binary interval tree in order to resolve efficiently interval and stabbing queries.",
        "published": "2003-09-08T19:57:46Z",
        "link": "http://arxiv.org/abs/cs/0309011v1",
        "categories": [
            "cs.DB",
            "H.3.1;H.2.8;J.3"
        ]
    },
    {
        "title": "The Lowell Database Research Self Assessment",
        "authors": [
            "Serge Abiteboul",
            "Rakesh Agrawal",
            "Phil Bernstein",
            "Mike Carey",
            "Stefano Ceri",
            "Bruce Croft",
            "David DeWitt",
            "Mike Franklin",
            "Hector Garcia Molina",
            "Dieter Gawlick",
            "Jim Gray",
            "Laura Haas",
            "Alon Halevy",
            "Joe Hellerstein",
            "Yannis Ioannidis",
            "Martin Kersten",
            "Michael Pazzani",
            "Mike Lesk",
            "David Maier",
            "Jeff Naughton",
            "Hans Schek",
            "Timos Sellis",
            "Avi Silberschatz",
            "Mike Stonebraker",
            "Rick Snodgrass",
            "Jeff Ullman",
            "Gerhard Weikum",
            "Jennifer Widom",
            "Stan Zdonik"
        ],
        "summary": "A group of senior database researchers gathers every few years to assess the state of database research and to point out problem areas that deserve additional focus. This report summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6, 2003 in Lowell, Mass. It observes that information management continues to be a critical component of most complex software systems. It recommends that database researchers increase focus on: integration of text, data, code, and streams; fusion of information from heterogeneous data sources; reasoning about uncertain data; unsupervised data mining for interesting correlations; information privacy; and self-adaptation and repair.",
        "published": "2003-10-06T05:42:49Z",
        "link": "http://arxiv.org/abs/cs/0310006v1",
        "categories": [
            "cs.DB",
            "H;H.2; H.3; H.4; H.5"
        ]
    },
    {
        "title": "A Formal Comparison of Visual Web Wrapper Generators",
        "authors": [
            "Georg Gottlob",
            "Christoph Koch"
        ],
        "summary": "We study the core fragment of the Elog wrapping language used in the Lixto system (a visual wrapper generator) and formally compare Elog to other wrapping languages proposed in the literature.",
        "published": "2003-10-08T00:18:31Z",
        "link": "http://arxiv.org/abs/cs/0310012v1",
        "categories": [
            "cs.DB",
            "F.1.1, F.4.1, F.4.3, H.2.3, I.7.2"
        ]
    },
    {
        "title": "Providing Diversity in K-Nearest Neighbor Query Results",
        "authors": [
            "Anoop Jain",
            "Parag Sarda",
            "Jayant R. Haritsa"
        ],
        "summary": "Given a point query Q in multi-dimensional space, K-Nearest Neighbor (KNN) queries return the K closest answers according to given distance metric in the database with respect to Q. In this scenario, it is possible that a majority of the answers may be very similar to some other, especially when the data has clusters. For a variety of applications, such homogeneous result sets may not add value to the user. In this paper, we consider the problem of providing diversity in the results of KNN queries, that is, to produce the closest result set such that each answer is sufficiently different from the rest. We first propose a user-tunable definition of diversity, and then present an algorithm, called MOTLEY, for producing a diverse result set as per this definition. Through a detailed experimental evaluation on real and synthetic data, we show that MOTLEY can produce diverse result sets by reading only a small fraction of the tuples in the database. Further, it imposes no additional overhead on the evaluation of traditional KNN queries, thereby providing a seamless interface between diversity and distance.",
        "published": "2003-10-15T16:07:56Z",
        "link": "http://arxiv.org/abs/cs/0310028v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "Supporting Exploratory Queries in Database Centric Web Applications",
        "authors": [
            "Abhijit Kadlag",
            "Amol Wanjari",
            "Juliana Freire",
            "Jayant R. Haritsa"
        ],
        "summary": "Users of database-centric Web applications, especially in the e-commerce domain, often resort to exploratory ``trial-and-error'' queries since the underlying data space is huge and unfamiliar, and there are several alternatives for search attributes in this space. For example, scouting for cheap airfares typically involves posing multiple queries, varying flight times, dates, and airport locations. Exploratory queries are problematic from the perspective of both the user and the server. For the database server, it results in a drastic reduction in effective throughput since much of the processing is duplicated in each successive query. For the client, it results in a marked increase in response times, especially when accessing the service through wireless channels.   In this paper, we investigate the design of automated techniques to minimize the need for repetitive exploratory queries. Specifically, we present SAUNA, a server-side query relaxation algorithm that, given the user's initial range query and a desired cardinality for the answer set, produces a relaxed query that is expected to contain the required number of answers. The algorithm incorporates a range-query-specific distance metric that is weighted to produce relaxed queries of a desired shape (e.g. aspect ratio preserving), and utilizes multi-dimensional histograms for query size estimation. A detailed performance evaluation of SAUNA over a variety of multi-dimensional data sets indicates that its relaxed queries can significantly reduce the costs associated with exploratory query processing.",
        "published": "2003-10-17T10:02:11Z",
        "link": "http://arxiv.org/abs/cs/0310035v1",
        "categories": [
            "cs.DB",
            "H.2.4"
        ]
    },
    {
        "title": "On Addressing Efficiency Concerns in Privacy Preserving Data Mining",
        "authors": [
            "Shipra Agrawal",
            "Vijay Krishnan",
            "Jayant Haritsa"
        ],
        "summary": "Data mining services require accurate input data for their results to be meaningful, but privacy concerns may influence users to provide spurious information. To encourage users to provide correct inputs, we recently proposed a data distortion scheme for association rule mining that simultaneously provides both privacy to the user and accuracy in the mining results. However, mining the distorted database can be orders of magnitude more time-consuming as compared to mining the original database. In this paper, we address this issue and demonstrate that by (a) generalizing the distortion process to perform symbol-specific distortion, (b) appropriately choosing the distortion parameters, and (c) applying a variety of optimizations in the reconstruction process, runtime efficiencies that are well within an order of magnitude of undistorted mining can be achieved.",
        "published": "2003-10-17T16:55:08Z",
        "link": "http://arxiv.org/abs/cs/0310038v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "Managing Evolving Business Workflows through the Capture of Descriptive   Information",
        "authors": [
            "Sebastien Gaspard",
            "Florida Estrella",
            "Richard McClatchey",
            "Regis Dindeleux"
        ],
        "summary": "Business systems these days need to be agile to address the needs of a changing world. In particular the discipline of Enterprise Application Integration requires business process management to be highly reconfigurable with the ability to support dynamic workflows, inter-application integration and process reconfiguration. Basing EAI systems on model-resident or on a so-called description-driven approach enables aspects of flexibility, distribution, system evolution and integration to be addressed in a domain-independent manner. Such a system called CRISTAL is described in this paper with particular emphasis on its application to EAI problem domains. A practical example of the CRISTAL technology in the domain of manufacturing systems, called Agilium, is described to demonstrate the principles of model-driven system evolution and integration. The approach is compared to other model-driven development approaches such as the Model-Driven Architecture of the OMG and so-called Adaptive Object Models.",
        "published": "2003-10-24T13:31:34Z",
        "link": "http://arxiv.org/abs/cs/0310048v1",
        "categories": [
            "cs.SE",
            "cs.DB",
            "H3.4;K4.4"
        ]
    },
    {
        "title": "Towards an Intelligent Database System Founded on the SP Theory of   Computing and Cognition",
        "authors": [
            "J. Gerard Wolff"
        ],
        "summary": "The SP theory of computing and cognition, described in previous publications, is an attractive model for intelligent databases because it provides a simple but versatile format for different kinds of knowledge, it has capabilities in artificial intelligence, and it can also function like established database models when that is required.   This paper describes how the SP model can emulate other models used in database applications and compares the SP model with those other models. The artificial intelligence capabilities of the SP model are reviewed and its relationship with other artificial intelligence systems is described. Also considered are ways in which current prototypes may be translated into an 'industrial strength' working system.",
        "published": "2003-11-21T14:44:22Z",
        "link": "http://arxiv.org/abs/cs/0311031v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.1"
        ]
    },
    {
        "title": "XPath-Logic and XPathLog: A Logic-Programming Style XML Data   Manipulation Language",
        "authors": [
            "Wolfgang May"
        ],
        "summary": "We define XPathLog as a Datalog-style extension of XPath. XPathLog provides a clear, declarative language for querying and manipulating XML whose perspectives are especially in XML data integration. In our characterization, the formal semantics is defined wrt. an edge-labeled graph-based model which covers the XML data model. We give a complete, logic-based characterization of XML data and the main language concept for XML, XPath. XPath-Logic extends the XPath language with variable bindings and embeds it into first-order logic. XPathLog is then the Horn fragment of XPath-Logic, providing a Datalog-style, rule-based language for querying and manipulating XML data. The model-theoretic semantics of XPath-Logic serves as the base of XPathLog as a logic-programming language, whereas also an equivalent answer-set semantics for evaluating XPathLog queries is given. In contrast to other approaches, the XPath syntax and semantics is also used for a declarative specification how the database should be updated: when used in rule heads, XPath filters are interpreted as specifications of elements and properties which should be added to the database.",
        "published": "2003-11-25T09:42:59Z",
        "link": "http://arxiv.org/abs/cs/0311038v1",
        "categories": [
            "cs.DB",
            "H.2; D.3"
        ]
    },
    {
        "title": "S-ToPSS: Semantic Toronto Publish/Subscribe System",
        "authors": [
            "Milenko Petrovic",
            "Ioana Burcea",
            "Hans-Arno Jacobsen"
        ],
        "summary": "The increase in the amount of data on the Internet has led to the development of a new generation of applications based on selective information dissemination where, data is distributed only to interested clients. Such applications require a new middleware architecture that can efficiently match user interests with available information. Middleware that can satisfy this requirement include event-based architectures such as publish-subscribe systems. In this demonstration paper we address the problem of semantic matching. We investigate how current publish/subscribe systems can be extended with semantic capabilities. Our main contribution is the development and validation (through demonstration) of a semantic pub/sub system prototype S-ToPSS (Semantic Toronto Publish/Subscribe System).",
        "published": "2003-11-26T21:55:53Z",
        "link": "http://arxiv.org/abs/cs/0311041v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4"
        ]
    },
    {
        "title": "I know what you mean: semantic issues in Internet-scale   publish/subscribe systems",
        "authors": [
            "Ioana Burcea",
            "Milenko Petrovic",
            "Hans-Arno Jacobsen"
        ],
        "summary": "In recent years, the amount of information on the Internet has increased exponentially developing great interest in selective information dissemination systems. The publish/subscribe paradigm is particularly suited for designing systems for routing information and requests according to their content throughout wide-area network of brokers. Current publish/subscribe systems use limited syntax-based content routing but since publishers and subscribers are anonymous and decoupled in time, space and location, often over wide-area network boundary, they do not necessarily speak the same language. Consequently, adding semantics to current publish/subscribe systems is important. In this paper we identify and examine the issues in developing semantic-based content routing for publish/subscribe broker networks.",
        "published": "2003-11-27T16:00:32Z",
        "link": "http://arxiv.org/abs/cs/0311047v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4"
        ]
    },
    {
        "title": "Greedy Algorithms in Datalog",
        "authors": [
            "Sergio Greco",
            "Carlo Zaniolo"
        ],
        "summary": "In the design of algorithms, the greedy paradigm provides a powerful tool for solving efficiently classical computational problems, within the framework of procedural languages. However, expressing these algorithms within the declarative framework of logic-based languages has proven a difficult research challenge. In this paper, we extend the framework of Datalog-like languages to obtain simple declarative formulations for such problems, and propose effective implementation techniques to ensure computational complexities comparable to those of procedural formulations. These advances are achieved through the use of the \"choice\" construct, extended with preference annotations to effect the selection of alternative stable-models and nondeterministic fixpoints. We show that, with suitable storage structures, the differential fixpoint computation of our programs matches the complexity of procedural algorithms in classical search and optimization problems.",
        "published": "2003-12-18T17:29:05Z",
        "link": "http://arxiv.org/abs/cs/0312041v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "D.1.6; F.3.1; F.4.1"
        ]
    },
    {
        "title": "Declarative Semantics for Active Rules",
        "authors": [
            "Sergio Flesca",
            "Sergio Greco"
        ],
        "summary": "In this paper we analyze declarative deterministic and non-deterministic semantics for active rules. In particular we consider several (partial) stable model semantics, previously defined for deductive rules, such as well-founded, max deterministic, unique total stable model, total stable model, and maximal stable model semantics. The semantics of an active program AP is given by first rewriting it into a deductive program P, then computing a model M defining the declarative semantics of P and, finally, applying `consistent' updates contained in M to the source database. The framework we propose permits a natural integration of deductive and active rules and can also be applied to queries with function symbols or to queries over infinite databases.",
        "published": "2003-12-18T17:43:43Z",
        "link": "http://arxiv.org/abs/cs/0312042v1",
        "categories": [
            "cs.DB",
            "D.1.6; F.3.1; F.4.1"
        ]
    },
    {
        "title": "On A Theory of Probabilistic Deductive Databases",
        "authors": [
            "Laks V. S. Lakshmanan",
            "Fereidoon Sadri"
        ],
        "summary": "We propose a framework for modeling uncertainty where both belief and doubt can be given independent, first-class status. We adopt probability theory as the mathematical formalism for manipulating uncertainty. An agent can express the uncertainty in her knowledge about a piece of information in the form of a confidence level, consisting of a pair of intervals of probability, one for each of her belief and doubt. The space of confidence levels naturally leads to the notion of a trilattice, similar in spirit to Fitting's bilattices. Intuitively, thep oints in such a trilattice can be ordered according to truth, information, or precision. We develop a framework for probabilistic deductive databases by associating confidence levels with the facts and rules of a classical deductive database. While the trilattice structure offers a variety of choices for defining the semantics of probabilistic deductive databases, our choice of semantics is based on the truth-ordering, which we find to be closest to the classical framework for deductive databases. In addition to proposing a declarative semantics based on valuations and an equivalent semantics based on fixpoint theory, we also propose a proof procedure and prove it sound and complete. We show that while classical Datalog query programs have a polynomial time data complexity, certain query programs in the probabilistic deductive database framework do not even terminate on some input databases. We identify a large natural class of query programs of practical interest in our framework, and show that programs in this class possess polynomial time data complexity, i.e., not only do they terminate on every input database, they are guaranteed to do so in a number of steps polynomial in the input database size.",
        "published": "2003-12-18T20:08:57Z",
        "link": "http://arxiv.org/abs/cs/0312043v1",
        "categories": [
            "cs.DB",
            "H.2.m"
        ]
    },
    {
        "title": "On the Abductive or Deductive Nature of Database Schema Validation and   Update Processing Problems",
        "authors": [
            "Ernest Teniente",
            "Toni Urpi"
        ],
        "summary": "We show that database schema validation and update processing problems such as view updating, materialized view maintenance, integrity constraint checking, integrity constraint maintenance or condition monitoring can be classified as problems of either abductive or deductive nature, according to the reasoning paradigm that inherently suites them. This is done by performing abductive and deductive reasoning on the event rules [Oli91], a set of rules that define the difference between consecutive database states In this way, we show that it is possible to provide methods able to deal with all these problems as a whole. We also show how some existing general deductive and abductive procedures may be used to reason on the event rules. In this way, we show that these procedures can deal with all database schema validation and update processing problems considered in this paper.",
        "published": "2003-12-19T15:25:53Z",
        "link": "http://arxiv.org/abs/cs/0312046v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.1;H.2.4; H.2.3"
        ]
    },
    {
        "title": "Grassmannian Frames with Applications to Coding and Communication",
        "authors": [
            "Thomas Strohmer",
            "Robert Heath"
        ],
        "summary": "For a given class ${\\cal F}$ of uniform frames of fixed redundancy we define a Grassmannian frame as one that minimizes the maximal correlation $|< f_k,f_l >|$ among all frames $\\{f_k\\}_{k \\in {\\cal I}} \\in {\\cal F}$. We first analyze finite-dimensional Grassmannian frames. Using links to packings in Grassmannian spaces and antipodal spherical codes we derive bounds on the minimal achievable correlation for Grassmannian frames. These bounds yield a simple condition under which Grassmannian frames coincide with uniform tight frames. We exploit connections to graph theory, equiangular line sets, and coding theory in order to derive explicit constructions of Grassmannian frames. Our findings extend recent results on uniform tight frames. We then introduce infinite-dimensional Grassmannian frames and analyze their connection to uniform tight frames for frames which are generated by group-like unitary systems. We derive an example of a Grassmannian Gabor frame by using connections to sphere packing theory. Finally we discuss the application of Grassmannian frames to wireless communication and to multiple description coding.",
        "published": "2003-01-13T19:07:08Z",
        "link": "http://arxiv.org/abs/math/0301135v1",
        "categories": [
            "math.FA",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Extended visual cryptography systems",
        "authors": [
            "Andreas Klein",
            "Markus Wessler"
        ],
        "summary": "Visual cryptography schemes have been introduced in 1994 by Naor and Shamir. Their idea was to encode a secret image into $n$ shadow images and to give exactly one such shadow image to each member of a group $P$ of $n$ persons. Whereas most work in recent years has been done concerning the problem of qualified and forbidden subsets of $P$ or the question of contrast optimizing, in this paper we study extended visual cryptography schemes, i.e. shared secret systems where any subset of $P$ shares its own secret.",
        "published": "2003-02-04T15:19:12Z",
        "link": "http://arxiv.org/abs/math/0302043v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Computing Symmetrized Weight Enumerators for Lifted Quadratic Residue   Codes",
        "authors": [
            "I. M. Duursma",
            "M. Greferath"
        ],
        "summary": "The paper describes a method to determine symmetrized weight enumerators of $p^m$-linear codes based on the notion of a disjoint weight enumerator. Symmetrized weight enumerators are given for the lifted quadratic residue codes of length 24 modulo $2^m$ and modulo $3^m$, for any positive $m$.",
        "published": "2003-02-12T02:59:19Z",
        "link": "http://arxiv.org/abs/math/0302132v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B60"
        ]
    },
    {
        "title": "Twisted Klein curves modulo 2",
        "authors": [
            "I. M. Duursma"
        ],
        "summary": "We give an explicit description of all 168 quartic curves over the field of two elements that are isomorphic to the Klein curve over an algebraic extension. Some of the curves have been known for their small class number, others for attaining the maximal number of rational points.",
        "published": "2003-02-13T02:29:43Z",
        "link": "http://arxiv.org/abs/math/0302154v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "14H45"
        ]
    },
    {
        "title": "Results on zeta functions for codes",
        "authors": [
            "I. M. Duursma"
        ],
        "summary": "We give a new and short proof of the Mallows-Sloane upper bound for self-dual codes. We formulate a version of Greene's theorem for normalized weight enumerators. We relate normalized rank-generating polynomials to two-variable zeta functions. And we show that a self-dual code has the Clifford property, but that the same property does not hold in general for formally self-dual codes.",
        "published": "2003-02-14T17:10:36Z",
        "link": "http://arxiv.org/abs/math/0302172v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "math.NT",
            "05B35; 14G15; 94B65"
        ]
    },
    {
        "title": "Bounding the trellis state complexity of algebraic geometric codes",
        "authors": [
            "Carlos Munuera",
            "Fernando Torres"
        ],
        "summary": "Let C be an algebraic geometric code of dimension k and length n constructed on a curve X over $F_q$. Let s(C) be the state complexity of C and set w(C):=min{k,n-k}, the Wolf upper bound on s(C). We introduce a numerical function R that depends on the gonality sequence of X and show that s(C)\\geq w(C)-R(2g-2), where g is the genus of X. As a matter of fact, R(2g-2)\\leq g-(\\gamma_2-2) with \\gamma_2 being the gonality over F_q of X, and thus in particular we have that s(C)\\geq w(C)-g+\\gamma_2-2.",
        "published": "2003-03-08T19:24:47Z",
        "link": "http://arxiv.org/abs/math/0303104v1",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.IT",
            "94B05, 94B27, 14G50"
        ]
    },
    {
        "title": "Strongly MDS Convolutional Codes",
        "authors": [
            "Heide Gluesing-Luerssen",
            "Joachim Rosenthal",
            "Roxana Smarandache"
        ],
        "summary": "MDS convolutional codes have the property that their free distance is maximal among all codes of the same rate and the same degree. In this paper we introduce a class of MDS convolutional codes whose column distances reach the generalized Singleton bound at the earliest possible instant. We call these codes strongly MDS convolutional codes. It is shown that these codes can decode a maximum number of errors per time interval when compared with other convolutional codes of the same rate and degree. These codes have also a maximum or near maximum distance profile. A code has a maximum distance profile if and only if the dual code has this property.",
        "published": "2003-03-20T16:00:53Z",
        "link": "http://arxiv.org/abs/math/0303254v1",
        "categories": [
            "math.RA",
            "cs.IT",
            "math.IT",
            "math.OC",
            "94B10 (Primary) 94B65, 11T71 (Secondary)"
        ]
    },
    {
        "title": "The Ubiquity of Order Domains for the Construction of Error Control   Codes",
        "authors": [
            "John B. Little"
        ],
        "summary": "The order domains are a class of commutative rings introduced by H{\\o}holdt, van Lint, and Pellikaan to simplify the theory of error control codes using ideas from algebraic geometry. The definition is largely motivated by the structures utilized in the Berlekamp-Massey-Sakata (BMS) decoding algorithm, with Feng-Rao majority voting for unknown syndromes, applied to one-point geometric Goppa codes constructed from curves. However, order domains are much more general, and O'Sullivan has shown that the BMS algorithm can be applied to decode all codes constructed from order domains by a suitable generalization of Goppa's procedure for curves. In this article we will first discuss the connection between order domains and valuations on function fields over a finite field. Under some mild conditions, we will see that a general projective variety over a finite field has projective models which can be used to construct order domains and Goppa-type codes for which the BMS algorithm is applicable. We will then give a slightly different interpretation of Geil and Pellikaan's extrinsic characterization of order domains via the theory of Gr\\\"obner bases, and show that their results are related to the existence of toric deformations of varieties. To illustrate the potential usefulness of these observations, we present a series of new explicit examples of order domains associated to varieties with many rational points over finite fields: Hermitian hypersurfaces, Grassmannians, and flag varieties.",
        "published": "2003-04-21T16:18:51Z",
        "link": "http://arxiv.org/abs/math/0304292v2",
        "categories": [
            "math.AC",
            "cs.IT",
            "math.AG",
            "math.IT",
            "math.RA",
            "94B27; 13P10; 13A18"
        ]
    },
    {
        "title": "Robust Estimators under the Imprecise Dirichlet Model",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Walley's Imprecise Dirichlet Model (IDM) for categorical data overcomes several fundamental problems which other approaches to uncertainty suffer from. Yet, to be useful in practice, one needs efficient ways for computing the imprecise=robust sets or intervals. The main objective of this work is to derive exact, conservative, and approximate, robust and credible interval estimates under the IDM for a large class of statistical estimators, including the entropy and mutual information.",
        "published": "2003-05-08T17:11:45Z",
        "link": "http://arxiv.org/abs/math/0305121v1",
        "categories": [
            "math.PR",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Distance bounds for convolutional codes and some optimal codes",
        "authors": [
            "Heide Gluesing-Luerssen",
            "Wiland Schmale"
        ],
        "summary": "After a discussion of the Griesmer and Heller bound for the distance of a convolutional code we present several codes with various parameters, over various fields, and meeting the given distance bounds. Moreover, the Griesmer bound is used for deriving a lower bound for the field size of an MDS convolutional code and examples are presented showing that, in most cases, the lower bound is tight. Most of the examples in this paper are cyclic convolutional codes in a generalized sense as it has been introduced in the seventies. A brief introduction to this promising type of cyclicity is given at the end of the paper in order to make the examples more transparent.",
        "published": "2003-05-09T08:14:26Z",
        "link": "http://arxiv.org/abs/math/0305135v1",
        "categories": [
            "math.RA",
            "cs.IT",
            "math.IT",
            "math.OC",
            "94B10; 94B15; 16S36"
        ]
    },
    {
        "title": "Numerical Analogues of Aronson's Sequence",
        "authors": [
            "Benoit Cloitre",
            "N. J. A. Sloane",
            "Matthew J. Vandermast"
        ],
        "summary": "Aronson's sequence 1, 4, 11, 16, ... is defined by the English sentence ``t is the first, fourth, eleventh, sixteenth, ... letter of this sentence.'' This paper introduces some numerical analogues, such as: a(n) is taken to be the smallest positive integer greater than a(n-1) which is consistent with the condition ``n is a member of the sequence if and only if a(n) is odd.'' This sequence can also be characterized by its ``square'', the sequence a^(2)(n) = a(a(n)), which equals 2n+3 for n >= 1. There are many generalizations of this sequence, some of which are new, while others throw new light on previously known sequences.",
        "published": "2003-05-21T21:56:26Z",
        "link": "http://arxiv.org/abs/math/0305308v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.IT",
            "11B37"
        ]
    },
    {
        "title": "Sequence Prediction based on Monotone Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "This paper studies sequence prediction based on the monotone Kolmogorov complexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is extremely close to Solomonoff's prior M, the latter being an excellent predictor in deterministic as well as probabilistic environments, where performance is measured in terms of convergence of posteriors or losses. Despite this closeness to M, it is difficult to assess the prediction quality of m, since little is known about the closeness of their posteriors, which are the important quantities for prediction. We show that for deterministic computable environments, the \"posterior\" and losses of m converge, but rapid convergence could only be shown on-sequence; the off-sequence behavior is unclear. In probabilistic environments, neither the posterior nor the losses converge, in general.",
        "published": "2003-06-07T19:21:20Z",
        "link": "http://arxiv.org/abs/cs/0306036v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2"
        ]
    },
    {
        "title": "Coding and tiling of Julia sets for subhyperbolic rational maps",
        "authors": [
            "Atsushi Kameyama"
        ],
        "summary": "Let $f:\\hat{C}\\to\\hat{C}$ be a subhyperbolic rational map of degree $d$. We construct a set of coding maps $Cod(f)=\\{\\pi_r:\\Sigma\\to J\\}_r$ of the Julia set $J$ by geometric coding trees, where the parameter $r$ ranges over mappings from a certain tree to the Riemann sphere. Using the universal covering space $\\phi:\\tilde S\\to S$ for the corresponding orbifold, we lift the inverse of $f$ to an iterated function system $I=(g_i)_{i=1,2,...,d}$. For the purpose of studying the structure of $Cod(f)$, we generalize Kenyon and Lagarias-Wang's results : If the attractor $K$ of $I$ has positive measure, then $K$ tiles $\\phi^{-1}(J)$, and the multiplicity of $\\pi_r$ is well-defined. Moreover, we see that the equivalence relation induced by $\\pi_r$ is described by a finite directed graph, and give a necessary and sufficient condition for two coding maps $\\pi_r$ and $\\pi_{r'}$ to be equal.",
        "published": "2003-06-25T06:53:54Z",
        "link": "http://arxiv.org/abs/math/0306354v1",
        "categories": [
            "math.DS",
            "cs.IT",
            "math.IT",
            "37F20 (Primary); 28A80 (Secondary)"
        ]
    },
    {
        "title": "Sur la non-linearite des fonctions booleennes",
        "authors": [
            "Francois Rodier"
        ],
        "summary": "Boolean functions on the space $F_{2}^m$ are not only important in the theory of error-correcting codes, but also in cryptography, where they occur in private key systems. In these two cases, the nonlinearity of these function is a main concept. In this article, I show that the spectral amplitude of boolean functions, which is linked to their nonlinearity, is of the order of $2^{m/2}\\sqrt{m}$ in mean, whereas its range is bounded by $2^{m/2}$ and $2^m$. Moreover I examine a conjecture of Patterson and Wiedemann saying that the minimum of this spectral amplitude is as close as desired to $2^{m/2}$. I also study a weaker conjecture about the moments of order 4 of their Fourier transform. This article is inspired by works of Salem, Zygmund, Kahane and others about the related problem of real polynomials with random coefficients.",
        "published": "2003-06-27T12:49:14Z",
        "link": "http://arxiv.org/abs/math/0306395v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.IT",
            "Primaire: 11T71, secondaire: 06E30, 42A05, 94B75"
        ]
    },
    {
        "title": "The Number of Hierarchical Orderings",
        "authors": [
            "N. J. A. Sloane",
            "Thomas Wieder"
        ],
        "summary": "An ordered set-partition (or preferential arrangement) of n labeled elements represents a single ``hierarchy''; these are enumerated by the ordered Bell numbers. In this note we determine the number of ``hierarchical orderings'' or ``societies'', where the n elements are first partitioned into m <= n subsets and a hierarchy is specified for each subset. We also consider the unlabeled case, where the ordered Bell numbers are replaced by the composition numbers. If there is only a single hierarchy, we show that the average rank of an element is asymptotic to n/(4 log 2) in the labeled case and to n/4 in the unlabeled case.",
        "published": "2003-07-04T16:02:58Z",
        "link": "http://arxiv.org/abs/math/0307064v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "06A07"
        ]
    },
    {
        "title": "Convolutional Codes with Maximum Distance Profile",
        "authors": [
            "R. Hutchinson",
            "J. Rosenthal",
            "R. Smarandache"
        ],
        "summary": "Maximum distance profile codes are characterized by the property that two trajectories which start at the same state and proceed to a different state will have the maximum possible distance from each other relative to any other convolutional code of the same rate and degree.   In this paper we use methods from systems theory to characterize maximum distance profile codes algebraically. Tha main result shows that maximum distance profile codes form a generic set inside the variety which parametrizes the set of convolutional codes of a fixed rate and a fixed degree.",
        "published": "2003-07-14T16:59:21Z",
        "link": "http://arxiv.org/abs/math/0307196v1",
        "categories": [
            "math.OC",
            "cs.IT",
            "math.IT",
            "math.RA",
            "94B10"
        ]
    },
    {
        "title": "Quantum Stein's lemma revisited, inequalities for quantum entropies, and   a concavity theorem of Lieb",
        "authors": [
            "Igor Bjelakovic",
            "Rainer Siegmund-Schultze"
        ],
        "summary": "We derive the monotonicity of the quantum relative entropy by an elementary operational argument based on Stein's lemma in quantum hypothesis testing. For the latter we present an elementary and short proof that requires the law of large numbers only. Joint convexity of the quantum relative entropy is proven too, resulting in a self-contained elementary version of Tropp's approach to Lieb's concavity theorem, according to which the map tr(exp(h+log a)) is concave in a on positive operators for self-adjoint h.",
        "published": "2003-07-24T15:18:39Z",
        "link": "http://arxiv.org/abs/quant-ph/0307170v2",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math-ph",
            "math.IT",
            "math.MP"
        ]
    },
    {
        "title": "Still better nonlinear codes from modular curves",
        "authors": [
            "Noam D. Elkies"
        ],
        "summary": "We give a new construction of nonlinear error-correcting codes over suitable finite fields k from the geometry of modular curves with many rational points over k, combining two recent improvements on Goppa's construction. The resulting codes are asymptotically the best currently known.",
        "published": "2003-08-05T22:22:41Z",
        "link": "http://arxiv.org/abs/math/0308046v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "94B27; 94B65"
        ]
    },
    {
        "title": "Sphere packing bounds in the Grassmann and Stiefel manifolds",
        "authors": [
            "Oliver Henkel"
        ],
        "summary": "Applying the Riemann geometric machinery of volume estimates in terms of curvature, bounds for the minimal distance of packings/codes in the Grassmann and Stiefel manifolds will be derived and analyzed. In the context of space-time block codes this leads to a monotonically increasing minimal distance lower bound as a function of the block length. This advocates large block lengths for the code design.",
        "published": "2003-08-12T09:43:14Z",
        "link": "http://arxiv.org/abs/math/0308110v4",
        "categories": [
            "math.MG",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Asymmetric binary covering codes",
        "authors": [
            "Joshua N. Cooper",
            "Robert B. Ellis",
            "Andrew B. Kahng"
        ],
        "summary": "An asymmetric binary covering code of length n and radius R is a subset C of the n-cube Q_n such that every vector x in Q_n can be obtained from some vector c in C by changing at most R 1's of c to 0's, where R is as small as possible. K^+(n,R) is defined as the smallest size of such a code. We show K^+(n,R) is of order 2^n/n^R for constant R, using an asymmetric sphere-covering bound and probabilistic methods. We show K^+(n,n-R')=R'+1 for constant coradius R' iff n>=R'(R'+1)/2. These two results are extended to near-constant R and R', respectively. Various bounds on K^+ are given in terms of the total number of 0's or 1's in a minimal code. The dimension of a minimal asymmetric linear binary code ([n,R]^+ code) is determined to be min(0,n-R). We conclude by discussing open problems and techniques to compute explicit values for K^+, giving a table of best known bounds.",
        "published": "2003-09-04T20:56:55Z",
        "link": "http://arxiv.org/abs/math/0309081v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B75"
        ]
    },
    {
        "title": "Error Correcting Codes on Algebraic Surfaces",
        "authors": [
            "Chris Lomont"
        ],
        "summary": "Error correcting codes are defined and important parameters for a code are explained. Parameters of new codes constructed on algebraic surfaces are studied. In particular, codes resulting from blowing up points in $\\proj^2$ are briefly studied, then codes resulting from ruled surfaces are covered. Codes resulting from ruled surfaces over curves of genus 0 are completely analyzed, and some codes are discovered that are better than direct product Reed Solomon codes of similar length. Ruled surfaces over genus 1 curves are also studied, but not all classes are completely analyzed. However, in this case a family of codes are found that are comparable in performance to the direct product code of a Reed Solomon code and a Goppa code. Some further work is done on surfaces from higher genus curves, but there remains much work to be done in this direction to understand fully the resulting codes. Codes resulting from blowing points on surfaces are also studied, obtaining necessary parameters for constructing infinite families of such codes.   Also included is a paper giving explicit formulas for curves with more \\field{q}-rational points than were previously known for certain combinations of field size and genus. Some upper bounds are now known to be optimal from these examples.",
        "published": "2003-09-07T00:07:48Z",
        "link": "http://arxiv.org/abs/math/0309123v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "14J60; 14Q10; 94B27"
        ]
    },
    {
        "title": "An invariant of finitary codes with finite expected square root coding   length",
        "authors": [
            "Nate Harvey",
            "Yuval Peres"
        ],
        "summary": "Let $p$ and $q$ be probability vectors with the same entropy $h$. Denote by $B(p)$ the Bernoulli shift indexed by $\\Z$ with marginal distribution $p$. Suppose that $\\phi$ is a measure preserving homomorphism from $B(p)$ to $B(q)$. We prove that if the coding length of $\\phi$ has a finite 1/2 moment, then $\\sigma_p^2=\\sigma_q^2$, where $\\sigma_p^2=\\sum_i p_i(-\\log p_i-h)^2$ is the {\\dof informational variance} of $p$. In this result, which sharpens a theorem of Parry (1979), the 1/2 moment cannot be replaced by a lower moment. On the other hand, for any $\\theta<1$, we exhibit probability vectors $p$ and $q$ that are not permutations of each other, such that there exists a finitary isomorphism $\\Phi$ from $B(p)$ to $B(q)$ where the coding lengths of $\\Phi$ and of its inverse have a finite $\\theta$ moment. We also present an extension to ergodic Markov chains.",
        "published": "2003-09-08T19:36:11Z",
        "link": "http://arxiv.org/abs/math/0309120v1",
        "categories": [
            "math.PR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An Algorithm for Optimal Partitioning of Data on an Interval",
        "authors": [
            "Brad Jackson",
            "Jeffrey D. Scargle",
            "David Barnes",
            "Sundararajan Arabhi",
            "Alina Alt",
            "Peter Gioumousis",
            "Elyus Gwin",
            "Paungkaew Sangtrakulcharoen",
            "Linda Tan",
            "Tun Tao Tsai"
        ],
        "summary": "Many signal processing problems can be solved by maximizing the fitness of a segmented model over all possible partitions of the data interval. This letter describes a simple but powerful algorithm that searches the exponentially large space of partitions of $N$ data points in time $O(N^2)$. The algorithm is guaranteed to find the exact global optimum, automatically determines the model order (the number of segments), has a convenient real-time mode, can be extended to higher dimensional data spaces, and solves a surprising variety of problems in signal detection and characterization, density estimation, cluster analysis and classification.",
        "published": "2003-09-17T18:27:00Z",
        "link": "http://arxiv.org/abs/math/0309285v2",
        "categories": [
            "math.NA",
            "astro-ph",
            "cs.CE",
            "cs.DS",
            "cs.IT",
            "math.CO",
            "math.IT",
            "65C60"
        ]
    },
    {
        "title": "Approximate Squaring",
        "authors": [
            "J. C. Lagarias",
            "N. J. A. Sloane"
        ],
        "summary": "We study the ``approximate squaring'' map f(x) := x ceiling(x) and its behavior when iterated. We conjecture that if f is repeatedly applied to a rational number r = l/d > 1 then eventually an integer will be reached. We prove this when d=2, and provide evidence that it is true in general by giving an upper bound on the density of the ``exceptional set'' of numbers which fail to reach an integer. We give similar results for a p-adic analogue of f, when the exceptional set is nonempty, and for iterating the ``approximate multiplication'' map f_r(x) := r ceiling(x) where r is a fixed rational number.",
        "published": "2003-09-23T19:49:45Z",
        "link": "http://arxiv.org/abs/math/0309389v2",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.IT",
            "Primary 26A18; Secondary 11B83, 11K31, 11Y99"
        ]
    },
    {
        "title": "Algebraic Aspects of Multiple Zeta Values",
        "authors": [
            "Michael E. Hoffman"
        ],
        "summary": "Multiple zeta values have been studied by a wide variety of methods. In this article we summarize some of the results about them that can be obtained by an algebraic approach. This involves \"coding\" the multiple zeta values by monomials in two noncommuting variables x and y. Multiple zeta values can then be thought of as defining a map \\zeta: H^0 -> R, where H^0 is the graded rational vector space generated by the \"admissible words\" of the noncommutative polynomial algebra Q<x,y>. Now H^0 admits two (commutative) products making \\zeta a homomorphism: the shuffle product and the \"harmonic\" product. The latter makes H^0 a subalgebra of the algebra QSym of quasi-symmetric functions. We also discuss some results about multiple zeta values that can be stated in terms of derivations and cyclic derivations of Q<x,y>, and define an action of the Hopf algebra QSym on Q<x,y> that appears useful. Finally, we apply the algebraic approach to finite partial sums of multiple zeta value series.",
        "published": "2003-09-25T22:08:05Z",
        "link": "http://arxiv.org/abs/math/0309425v2",
        "categories": [
            "math.QA",
            "cs.IT",
            "math.IT",
            "math.NT",
            "11M06; 16W30; 11B50"
        ]
    },
    {
        "title": "Convolutional Codes of Goppa Type",
        "authors": [
            "J. A. Dominguez Perez",
            "J. M. Muñoz Porras",
            "G. Serrano Sotelo"
        ],
        "summary": "A new kind of Convolutional Codes generalizing Goppa Codes is proposed. This provides a systematic method for constructing convolutional codes with prefixed properties. In particular, examples of Maximum-Distance Separable (MDS) convolutional codes are obtained.",
        "published": "2003-10-10T15:25:09Z",
        "link": "http://arxiv.org/abs/math/0310148v1",
        "categories": [
            "math.OC",
            "cs.IT",
            "math.AG",
            "math.IT",
            "94B27; 94B10"
        ]
    },
    {
        "title": "Convolutional Goppa Codes",
        "authors": [
            "J. M. Muñoz Porras",
            "J. A. Dominguez Perez",
            "J. I. Iglesias Curto",
            "G. Serrano Sotelo"
        ],
        "summary": "We define Convolutional Goppa Codes over algebraic curves and construct their corresponding dual codes. Examples over the projective line and over elliptic curves are described, obtaining in particular some Maximum-Distance Separable (MDS) convolutional codes.",
        "published": "2003-10-10T15:31:56Z",
        "link": "http://arxiv.org/abs/math/0310149v1",
        "categories": [
            "math.OC",
            "cs.IT",
            "math.AG",
            "math.IT",
            "94B27; 94B10"
        ]
    },
    {
        "title": "Symmetric Informationally Complete Quantum Measurements",
        "authors": [
            "Joseph M. Renes",
            "Robin Blume-Kohout",
            "A. J. Scott",
            "Carlton M. Caves"
        ],
        "summary": "We consider the existence in arbitrary finite dimensions d of a POVM comprised of d^2 rank-one operators all of whose operator inner products are equal. Such a set is called a ``symmetric, informationally complete'' POVM (SIC-POVM) and is equivalent to a set of d^2 equiangular lines in C^d. SIC-POVMs are relevant for quantum state tomography, quantum cryptography, and foundational issues in quantum mechanics. We construct SIC-POVMs in dimensions two, three, and four. We further conjecture that a particular kind of group-covariant SIC-POVM exists in arbitrary dimensions, providing numerical results up to dimension 45 to bolster this claim.",
        "published": "2003-10-13T19:40:15Z",
        "link": "http://arxiv.org/abs/quant-ph/0310075v1",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.FA",
            "math.IT"
        ]
    },
    {
        "title": "De Bruijn Cycles for Covering Codes",
        "authors": [
            "Fan Chung",
            "Joshua N. Cooper"
        ],
        "summary": "A de Bruijn covering code is a q-ary string S so that every q-ary string is at most R symbol changes from some n-word appearing consecutively in S. We introduce these codes and prove that they can have length close to the smallest possible covering code. The proof employs tools from field theory, probability, and linear algebra. We also prove a number of ``spectral'' results on de Bruijn covering codes. Included is a table of the best known bounds on the lengths of small binary de Bruijn covering codes, up to R=11 and n=13, followed by several open questions in this area.",
        "published": "2003-10-24T00:54:27Z",
        "link": "http://arxiv.org/abs/math/0310385v2",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "05B99"
        ]
    },
    {
        "title": "Codes and Invariant Theory",
        "authors": [
            "Gabriele Nebe",
            "E. M. Rains",
            "N. J. A. Sloane"
        ],
        "summary": "The main theorem in this paper is a far-reaching generalization of Gleason's theorem on the weight enumerators of codes which applies to arbitrary-genus weight enumerators of self-dual codes defined over a large class of finite rings and modules. The proof of the theorem uses a categorical approach, and will be the subject of a forthcoming book. However, the theorem can be stated and applied without using category theory, and we illustrate it here by applying it to generalized doubly-even codes over fields of characteristic 2, doubly-even codes over the integers modulo a power of 2, and self-dual codes over the noncommutative ring $\\F_q + \\F_q u$, where $u^2 = 0$..",
        "published": "2003-11-04T20:20:57Z",
        "link": "http://arxiv.org/abs/math/0311046v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.IT",
            "94B05, 13A50, 94B60"
        ]
    },
    {
        "title": "Cayley-Bacharach and evaluation codes on complete intersections",
        "authors": [
            "Leah Gold",
            "John Little",
            "Hal Schenck"
        ],
        "summary": "In recent work, J. Hansen uses cohomological methods to find a lower bound for the minimum distance of an evaluation code determined by a reduced complete intersection in the projective plane. In this paper, we generalize Hansen's results from P^2 to P^m; we also show that the hypotheses in Hansen's work may be weakened. The proof is succinct and follows by combining the Cayley-Bacharach theorem and bounds on evaluation codes obtained from reduced zero-schemes.",
        "published": "2003-11-09T20:27:56Z",
        "link": "http://arxiv.org/abs/math/0311129v2",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.AC",
            "math.IT",
            "14G50 (Primary) 94B27 (Secondary)"
        ]
    },
    {
        "title": "Complete Weight Enumerators of Generalized Doubly-Even Self-Dual Codes",
        "authors": [
            "Gabriele Nebe",
            "H. -G. Quebbemann",
            "E. M. Rains",
            "N. J. A. Sloane"
        ],
        "summary": "For any q which is a power of 2 we describe a finite subgroup of the group of invertible complex q by q matrices under which the complete weight enumerators of generalized doubly-even self-dual codes over the field with q elements are invariant.   An explicit description of the invariant ring and some applications to extremality of such codes are obtained in the case q=4.",
        "published": "2003-11-17T16:55:33Z",
        "link": "http://arxiv.org/abs/math/0311289v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.IT",
            "94B05, 13A50, 94B60"
        ]
    },
    {
        "title": "Modular and p-adic cyclic codes",
        "authors": [
            "A. R. Calderbank",
            "N. J. A. Sloane"
        ],
        "summary": "This paper presents some basic theorems giving the structure of cyclic codes of length n over the ring of integers modulo p^a and over the p-adic numbers, where p is a prime not dividing n. An especially interesting example is the 2-adic cyclic code of length 7 with generator polynomial X^3 + lambda X^2 + (lambda - 1) X - 1, where lambda satisfies lambda^2 - lambda + 2 =0. This is the 2-adic generalization of both the binary Hamming code and the quaternary octacode (the latter being equivalent to the Nordstrom-Robinson code). Other examples include the 2-adic Golay code of length 24 and the 3-adic Golay code of length 12.",
        "published": "2003-11-18T21:35:21Z",
        "link": "http://arxiv.org/abs/math/0311319v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B15, 94B05"
        ]
    },
    {
        "title": "On the Parameters of Convolutional Codes with Cyclic Structure",
        "authors": [
            "Heide Gluesing-Luerssen",
            "Barbara Langfeld"
        ],
        "summary": "In this paper convolutional codes with cyclic structure will be investigated. These codes can be understood as left principal ideals in a suitable skew-polynomial ring. It has been shown in [3] that only certain combinations of the parameters (field size, length, dimension, and Forney indices) can occur for cyclic codes. We will investigate whether all these combinations can indeed be realized by a suitable cyclic code and, if so, how to construct such a code. A complete characterization and construction will be given for minimal cyclic codes. It is derived from a detailed investigation of the units in the skew-polynomial ring.",
        "published": "2003-12-03T19:34:56Z",
        "link": "http://arxiv.org/abs/math/0312092v1",
        "categories": [
            "math.RA",
            "cs.IT",
            "math.CO",
            "math.IT",
            "94B10; 94B15; 16S36"
        ]
    },
    {
        "title": "Shannon information, LMC complexity and Renyi entropies: a   straightforward approach",
        "authors": [
            "Ricardo Lopez-Ruiz"
        ],
        "summary": "The LMC complexity, an indicator of complexity based on a probabilistic description, is revisited. A straightforward approach allows us to establish the time evolution of this indicator in a near-equilibrium situation and gives us a new insight for interpreting the LMC complexity for a general non equilibrium system. Its relationship with the Renyi entropies is also explained. One of the advantages of this indicator is that its calculation does not require a considerable computational effort in many cases of physical and biological interest.",
        "published": "2003-12-22T17:30:42Z",
        "link": "http://arxiv.org/abs/nlin/0312056v1",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT",
            "nlin.CD",
            "physics.comp-ph",
            "q-bio.QM"
        ]
    },
    {
        "title": "Selective pressures on genomes in molecular evolution",
        "authors": [
            "Charles Ofria",
            "Christoph Adami",
            "Travis C. Collier"
        ],
        "summary": "We describe the evolution of macromolecules as an information transmission process and apply tools from Shannon information theory to it. This allows us to isolate three independent, competing selective pressures that we term compression, transmission, and neutrality selection. The first two affect genome length: the pressure to conserve resources by compressing the code, and the pressure to acquire additional information that improves the channel, increasing the rate of information transmission into each offspring. Noisy transmission channels (replication with mutations) gives rise to a third pressure that acts on the actual encoding of information; it maximizes the fraction of mutations that are neutral with respect to the phenotype. This neutrality selection has important implications for the evolution of evolvability. We demonstrate each selective pressure in experiments with digital organisms.",
        "published": "2003-01-15T20:29:08Z",
        "link": "http://arxiv.org/abs/quant-ph/0301075v1",
        "categories": [
            "quant-ph",
            "cs.NE",
            "nlin.AO",
            "physics.bio-ph",
            "q-bio.PE"
        ]
    },
    {
        "title": "Optimizing GoTools' Search Heuristics using Genetic Algorithms",
        "authors": [
            "Matthew Pratola",
            "Thomas Wolf"
        ],
        "summary": "GoTools is a program which solves life & death problems in the game of Go. This paper describes experiments using a Genetic Algorithm to optimize heuristic weights used by GoTools' tree-search. The complete set of heuristic weights is composed of different subgroups, each of which can be optimized with a suitable fitness function. As a useful side product, an MPI interface for FreePascal was implemented to allow the use of a parallelized fitness function running on a Beowulf cluster. The aim of this exercise is to optimize the current version of GoTools, and to make tools available in preparation of an extension of GoTools for solving open boundary life & death problems, which will introduce more heuristic parameters to be fine tuned.",
        "published": "2003-02-02T03:30:52Z",
        "link": "http://arxiv.org/abs/cs/0302002v1",
        "categories": [
            "cs.NE",
            "I.2.1"
        ]
    },
    {
        "title": "A Neural Network Assembly Memory Model with Maximum-Likelihood Recall   and Recognition Properties",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "It has been shown that a neural network model recently proposed to describe basic memory performance is based on a ternary/binary coding/decoding algorithm which leads to a new neural network assembly memory model (NNAMM) providing maximum-likelihood recall/recognition properties and implying a new memory unit architecture with Hopfield two-layer network, N-channel time gate, auxiliary reference memory, and two nested feedback loops. For the data coding used, conditions are found under which a version of Hopfied network implements maximum-likelihood convolutional decoding algorithm and, simultaneously, linear statistical classifier of arbitrary binary vectors with respect to Hamming distance between vector analyzed and reference vector given. In addition to basic memory performance and etc, the model explicitly describes the dependence on time of memory trace retrieval, gives a possibility of one-trial learning, metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. It has been shown that an assembly memory unit may be viewed as a model of a smallest inseparable part or an 'atom' of consciousness. Some nontraditional neurobiological backgrounds (dynamic spatiotemporal synchrony, properties of time dependent and error detector neurons, early precise spike firing, etc) and the model's application to solve some interdisciplinary problems from different scientific fields are discussed.",
        "published": "2003-03-19T23:17:16Z",
        "link": "http://arxiv.org/abs/cs/0303017v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "Recent Results on No-Free-Lunch Theorems for Optimization",
        "authors": [
            "Christian Igel",
            "Marc Toussaint"
        ],
        "summary": "The sharpened No-Free-Lunch-theorem (NFL-theorem) states that the performance of all optimization algorithms averaged over any finite set F of functions is equal if and only if F is closed under permutation (c.u.p.) and each target function in F is equally likely. In this paper, we first summarize some consequences of this theorem, which have been proven recently: The average number of evaluations needed to find a desirable (e.g., optimal) solution can be calculated; the number of subsets c.u.p. can be neglected compared to the overall number of possible subsets; and problem classes relevant in practice are not likely to be c.u.p. Second, as the main result, the NFL-theorem is extended. Necessary and sufficient conditions for NFL-results to hold are given for arbitrary, non-uniform distributions of target functions. This yields the most general NFL-theorem for optimization presented so far.",
        "published": "2003-03-31T14:46:10Z",
        "link": "http://arxiv.org/abs/cs/0303032v1",
        "categories": [
            "cs.NE",
            "math.OC",
            "nlin.AO",
            "G.1.6"
        ]
    },
    {
        "title": "Self-Replicating Machines in Continuous Space with Virtual Physics",
        "authors": [
            "Arnold Smith",
            "Peter Turney",
            "Robert Ewaschuk"
        ],
        "summary": "JohnnyVon is an implementation of self-replicating machines in continuous two-dimensional space. Two types of particles drift about in a virtual liquid. The particles are automata with discrete internal states but continuous external relationships. Their internal states are governed by finite state machines but their external relationships are governed by a simulated physics that includes Brownian motion, viscosity, and spring-like attractive and repulsive forces. The particles can be assembled into patterns that can encode arbitrary strings of bits. We demonstrate that, if an arbitrary \"seed\" pattern is put in a \"soup\" of separate individual particles, the pattern will replicate by assembling the individual particles into copies of itself. We also show that, given sufficient time, a soup of separate individual particles will eventually spontaneously form self-replicating patterns. We discuss the implications of JohnnyVon for research in nanotechnology, theoretical biology, and artificial life.",
        "published": "2003-04-15T19:33:45Z",
        "link": "http://arxiv.org/abs/cs/0304022v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "q-bio.PE",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "Whitehead method and Genetic Algorithms",
        "authors": [
            "Alexei D. Miasnikov",
            "Alexei G. Myasnikov"
        ],
        "summary": "In this paper we discuss a genetic version (GWA) of the Whitehead's algorithm, which is one of the basic algorithms in combinatorial group theory. It turns out that GWA is surprisingly fast and outperforms the standard Whitehead's algorithm in free groups of rank >= 5. Experimenting with GWA we collected an interesting numerical data that clarifies the time-complexity of the Whitehead's Problem in general. These experiments led us to several mathematical conjectures. If confirmed they will shed light on hidden mechanisms of Whitehead Method and geometry of automorphic orbits in free groups.",
        "published": "2003-04-20T16:24:44Z",
        "link": "http://arxiv.org/abs/math/0304283v1",
        "categories": [
            "math.GR",
            "cs.NE",
            "cs.SC",
            "20F28, 68Q17, 68T05"
        ]
    },
    {
        "title": "Genetic algorithms and the Andrews-Curtis conjecture",
        "authors": [
            "Alexei D. Miasnikov"
        ],
        "summary": "The Andrews-Curtis conjecture claims that every balanced presentation of the trivial group can be transformed into the trivial presentation by a finite sequence of \"elementary transformations\" which are Nielsen transformations together with an arbitrary conjugation of a relator. It is believed that the Andrews-Curtis conjecture is false; however, not so many possible counterexamples are known. It is not a trivial matter to verify whether the conjecture holds for a given balanced presentation or not. The purpose of this paper is to describe some non-deterministic methods, called Genetic Algorithms, designed to test the validity of the Andrews-Curtis conjecture. Using such algorithm we have been able to prove that all known (to us) balanced presentations of the trivial group where the total length of the relators is at most 12 satisfy the conjecture. In particular, the Andrews-Curtis conjecture holds for the presentation <x,y|x y x = y x y, x^2 = y^3> which was one of the well known potential counterexamples.",
        "published": "2003-04-21T21:07:18Z",
        "link": "http://arxiv.org/abs/math/0304306v1",
        "categories": [
            "math.GR",
            "cs.NE",
            "cs.SC",
            "20E05, 20F05, 68T05 (Primary) 57M05,57M20"
        ]
    },
    {
        "title": "On Nonspecific Evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "When simultaneously reasoning with evidences about several different events it is necessary to separate the evidence according to event. These events should then be handled independently. However, when propositions of evidences are weakly specified in the sense that it may not be certain to which event they are referring, this may not be directly possible. In this paper a criterion for partitioning evidences into subsets representing events is established. This criterion, derived from the conflict within each subset, involves minimising a criterion function for the overall conflict of the partition. An algorithm based on characteristics of the criterion function and an iterative optimisation among partitionings of evidences is proposed.",
        "published": "2003-05-16T13:39:19Z",
        "link": "http://arxiv.org/abs/cs/0305013v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Finding a Posterior Domain Probability Distribution by Specifying   Nonspecific Evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "This article is an extension of the results of two earlier articles. In [J. Schubert, On nonspecific evidence, Int. J. Intell. Syst. 8 (1993) 711-725] we established within Dempster-Shafer theory a criterion function called the metaconflict function. With this criterion we can partition into subsets a set of several pieces of evidence with propositions that are weakly specified in the sense that it may be uncertain to which event a proposition is referring. In a second article [J. Schubert, Specifying nonspecific evidence, in Cluster-based specification techniques in Dempster-Shafer theory for an evidential intelligence analysis of multiple target tracks, Ph.D. Thesis, TRITA-NA-9410, Royal Institute of Technology, Stockholm, 1994, ISBN 91-7170-801-4] we not only found the most plausible subset for each piece of evidence, we also found the plausibility for every subset that this piece of evidence belongs to the subset. In this article we aim to find a posterior probability distribution regarding the number of subsets. We use the idea that each piece of evidence in a subset supports the existence of that subset to the degree that this piece of evidence supports anything at all. From this we can derive a bpa that is concerned with the question of how many subsets we have. That bpa can then be combined with a given prior domain probability distribution in order to obtain the sought-after posterior domain distribution.",
        "published": "2003-05-16T14:36:03Z",
        "link": "http://arxiv.org/abs/cs/0305015v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Cluster-based Specification Techniques in Dempster-Shafer Theory",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "When reasoning with uncertainty there are many situations where evidences are not only uncertain but their propositions may also be weakly specified in the sense that it may not be certain to which event a proposition is referring. It is then crucial not to combine such evidences in the mistaken belief that they are referring to the same event. This situation would become manageable if the evidences could be clustered into subsets representing events that should be handled separately. In an earlier article we established within Dempster-Shafer theory a criterion function called the metaconflict function. With this criterion we can partition a set of evidences into subsets. Each subset representing a separate event. In this article we will not only find the most plausible subset for each piece of evidence, we will also find the plausibility for every subset that the evidence belongs to the subset. Also, when the number of subsets are uncertain we aim to find a posterior probability distribution regarding the number of subsets.",
        "published": "2003-05-16T14:46:37Z",
        "link": "http://arxiv.org/abs/cs/0305017v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Cluster-based Specification Techniques in Dempster-Shafer Theory for an   Evidential Intelligence Analysis of MultipleTarget Tracks (Thesis Abstract)",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In Intelligence Analysis it is of vital importance to manage uncertainty. Intelligence data is almost always uncertain and incomplete, making it necessary to reason and taking decisions under uncertainty. One way to manage the uncertainty in Intelligence Analysis is Dempster-Shafer Theory. This thesis contains five results regarding multiple target tracks and intelligence specification.",
        "published": "2003-05-16T14:59:45Z",
        "link": "http://arxiv.org/abs/cs/0305018v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Specifying nonspecific evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In an earlier article [J. Schubert, On nonspecific evidence, Int. J. Intell. Syst. 8(6), 711-725 (1993)] we established within Dempster-Shafer theory a criterion function called the metaconflict function. With this criterion we can partition into subsets a set of several pieces of evidence with propositions that are weakly specified in the sense that it may be uncertain to which event a proposition is referring. Each subset in the partitioning is representing a separate event. The metaconflict function was derived as the plausibility that the partitioning is correct when viewing the conflict in Dempster's rule within each subset as a newly constructed piece of metalevel evidence with a proposition giving support against the entire partitioning. In this article we extend the results of the previous article. We will not only find the most plausible subset for each piece of evidence as was done in the earlier article. In addition we will specify each piece of nonspecific evidence, in the sense that we find to which events the proposition might be referring, by finding the plausibility for every subset that this piece of evidence belong to the subset. In doing this we will automatically receive indication that some evidence might be false. We will then develop a new methodology to exploit these newly specified pieces of evidence in a subsequent reasoning process. This will include methods to discount evidence based on their degree of falsity and on their degree of credibility due to a partial specification of affiliation, as well as a refined method to infer the event of each subset.",
        "published": "2003-05-16T15:13:22Z",
        "link": "http://arxiv.org/abs/cs/0305020v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Creating Prototypes for Fast Classification in Dempster-Shafer   Clustering",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "We develop a classification method for incoming pieces of evidence in Dempster-Shafer theory. This methodology is based on previous work with clustering and specification of originally nonspecific evidence. This methodology is here put in order for fast classification of future incoming pieces of evidence by comparing them with prototypes representing the clusters, instead of making a full clustering of all evidence. This method has a computational complexity of O(M * N) for each new piece of evidence, where M is the maximum number of subsets and N is the number of prototypes chosen for each subset. That is, a computational complexity independent of the total number of previously arrived pieces of evidence. The parameters M and N are typically fixed and domain dependent in any application.",
        "published": "2003-05-16T15:32:14Z",
        "link": "http://arxiv.org/abs/cs/0305021v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Applying Data Mining and Machine Learning Techniques to Submarine   Intelligence Analysis",
        "authors": [
            "Ulla Bergsten",
            "Johan Schubert",
            "Per Svensson"
        ],
        "summary": "We describe how specialized database technology and data analysis methods were applied by the Swedish defense to help deal with the violation of Swedish marine territory by foreign submarine intruders during the Eighties and early Nineties. Among several approaches tried some yielded interesting information, although most of the key questions remain unanswered. We conclude with a survey of belief-function- and genetic-algorithm-based methods which were proposed to support interpretation of intelligence reports and prediction of future submarine positions, respectively.",
        "published": "2003-05-16T15:41:58Z",
        "link": "http://arxiv.org/abs/cs/0305022v1",
        "categories": [
            "cs.AI",
            "cs.DB",
            "cs.NE",
            "H.2.8; H.4.2; I.2.3; I.5.3"
        ]
    },
    {
        "title": "Fast Dempster-Shafer clustering using a neural network structure",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we study a problem within Dempster-Shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. However, for large scale problems we need a method with lower computational complexity. The neural structure was found to be effective and much faster than iterative optimization for larger problems. While the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems, the metaconflict per cluster and evidence was moderate. The neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters.",
        "published": "2003-05-16T15:48:24Z",
        "link": "http://arxiv.org/abs/cs/0305023v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "A neural network and iterative optimization hybrid for Dempster-Shafer   clustering",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we extend an earlier result within Dempster-Shafer theory [\"Fast Dempster-Shafer Clustering Using a Neural Network Structure,\" in Proc. Seventh Int. Conf. Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU 98)] where a large number of pieces of evidence are clustered into subsets by a neural network structure. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. While the neural method had a much lower computation time than iterative optimization its average clustering performance was not as good. Here, we develop a hybrid of the two methods. We let the neural structure do the initial clustering in order to achieve a high computational performance. Its solution is fed as the initial state to the iterative optimization in order to improve the clustering performance.",
        "published": "2003-05-16T15:54:46Z",
        "link": "http://arxiv.org/abs/cs/0305024v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Simultaneous Dempster-Shafer clustering and gradual determination of   number of clusters using a neural network structure",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we extend an earlier result within Dempster-Shafer theory [\"Fast Dempster-Shafer Clustering Using a Neural Network Structure,\" in Proc. Seventh Int. Conf. Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU'98)] where several pieces of evidence were clustered into a fixed number of clusters using a neural structure. This was done by minimizing a metaconflict function. We now develop a method for simultaneous clustering and determination of number of clusters during iteration in the neural structure. We let the output signals of neurons represent the degree to which a pieces of evidence belong to a corresponding cluster. From these we derive a probability distribution regarding the number of clusters, which gradually during the iteration is transformed into a determination of number of clusters. This gradual determination is fed back into the neural structure at each iteration to influence the clustering process.",
        "published": "2003-05-16T16:00:23Z",
        "link": "http://arxiv.org/abs/cs/0305025v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Fast Dempster-Shafer clustering using a neural network structure",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this article we study a problem within Dempster-Shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. However, for large scale problems we need a method with lower computational complexity. The neural structure was found to be effective and much faster than iterative optimization for larger problems. While the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems, the metaconflict per cluster and evidence was moderate. The neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters.",
        "published": "2003-05-16T16:06:22Z",
        "link": "http://arxiv.org/abs/cs/0305026v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Managing Inconsistent Intelligence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we demonstrate that it is possible to manage intelligence in constant time as a pre-process to information fusion through a series of processes dealing with issues such as clustering reports, ranking reports with respect to importance, extraction of prototypes from clusters and immediate classification of newly arriving intelligence reports. These methods are used when intelligence reports arrive which concerns different events which should be handled independently, when it is not known a priori to which event each intelligence report is related. We use clustering that runs as a back-end process to partition the intelligence into subsets representing the events, and in parallel, a fast classification that runs as a front-end process in order to put the newly arriving intelligence into its correct information fusion process.",
        "published": "2003-05-16T16:16:17Z",
        "link": "http://arxiv.org/abs/cs/0305027v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Dempster-Shafer clustering using Potts spin mean field theory",
        "authors": [
            "Mats Bengtsson",
            "Johan Schubert"
        ],
        "summary": "In this article we investigate a problem within Dempster-Shafer theory where 2**q - 1 pieces of evidence are clustered into q clusters by minimizing a metaconflict function, or equivalently, by minimizing the sum of weight of conflict over all clusters. Previously one of us developed a method based on a Hopfield and Tank model. However, for very large problems we need a method with lower computational complexity. We demonstrate that the weight of conflict of evidence can, as an approximation, be linearized and mapped to an antiferromagnetic Potts Spin model. This facilitates efficient numerical solution, even for large problem sizes. Optimal or nearly optimal solutions are found for Dempster-Shafer clustering benchmark tests with a time complexity of approximately O(N**2 log**2 N). Furthermore, an isomorphism between the antiferromagnetic Potts spin model and a graph optimization problem is shown. The graph model has dynamic variables living on the links, which have a priori probabilities that are directly related to the pairwise conflict between pieces of evidence. Hence, the relations between three different models are shown.",
        "published": "2003-05-16T16:28:20Z",
        "link": "http://arxiv.org/abs/cs/0305028v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Conflict-based Force Aggregation",
        "authors": [
            "John Cantwell",
            "Johan Schubert",
            "Johan Walter"
        ],
        "summary": "In this paper we present an application where we put together two methods for clustering and classification into a force aggregation method. Both methods are based on conflicts between elements. These methods work with different type of elements (intelligence reports, vehicles, military units) on different hierarchical levels using specific conflict assessment methods on each level. We use Dempster-Shafer theory for conflict calculation between elements, Dempster-Shafer clustering for clustering these elements, and templates for classification. The result of these processes is a complete force aggregation on all levels handled.",
        "published": "2003-05-16T16:37:01Z",
        "link": "http://arxiv.org/abs/cs/0305029v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "H.4.2; I.2.3; I.2.6; I.5.3; J.7"
        ]
    },
    {
        "title": "Reliable Force Aggregation Using a Refined Evidence Specification from   Dempster-Shafer Clustering",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we develop methods for selection of templates and use these templates to recluster an already performed Dempster-Shafer clustering taking into account intelligence to template fit during the reclustering phase. By this process the risk of erroneous force aggregation based on some misplace pieces of evidence from the first clustering process is greatly reduced. Finally, a more reliable force aggregation is performed using the result of the second clustering. These steps are taken in order to maintain most of the excellent computational performance of Dempster-Shafer clustering, while at the same time improve on the clustering result by including some higher relations among intelligence reports described by the templates. The new improved algorithm has a computational complexity of O(n**3 log**2 n) compared to O(n**2 log**2 n) of standard Dempster-Shafer clustering using Potts spin mean field theory.",
        "published": "2003-05-16T16:45:38Z",
        "link": "http://arxiv.org/abs/cs/0305030v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.5.3"
        ]
    },
    {
        "title": "Clustering belief functions based on attracting and conflicting   metalevel evidence",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we develop a method for clustering belief functions based on attracting and conflicting metalevel evidence. Such clustering is done when the belief functions concern multiple events, and all belief functions are mixed up. The clustering process is used as the means for separating the belief functions into subsets that should be handled independently. While the conflicting metalevel evidence is generated internally from pairwise conflicts of all belief functions, the attracting metalevel evidence is assumed given by some external source.",
        "published": "2003-05-16T16:52:48Z",
        "link": "http://arxiv.org/abs/cs/0305031v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.5.3"
        ]
    },
    {
        "title": "Robust Report Level Cluster-to-Track Fusion",
        "authors": [
            "Johan Schubert"
        ],
        "summary": "In this paper we develop a method for report level tracking based on Dempster-Shafer clustering using Potts spin neural networks where clusters of incoming reports are gradually fused into existing tracks, one cluster for each track. Incoming reports are put into a cluster and continuous reclustering of older reports is made in order to obtain maximum association fit within the cluster and towards the track. Over time, the oldest reports of the cluster leave the cluster for the fixed track at the same rate as new incoming reports are put into it. Fusing reports to existing tracks in this fashion allows us to take account of both existing tracks and the probable future of each track, as represented by younger reports within the corresponding cluster. This gives us a robust report-to-track association. Compared to clustering of all available reports this approach is computationally faster and has a better report-to-track association than simple step-by-step association.",
        "published": "2003-05-16T16:58:38Z",
        "link": "http://arxiv.org/abs/cs/0305032v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.3; I.2.6; I.4.8; I.5.3"
        ]
    },
    {
        "title": "Neural network modeling of data with gaps: method of principal curves,   Carleman's formula, and other",
        "authors": [
            "A. N. Gorban",
            "A. A. Rossiev",
            "D. C. Wunsch II"
        ],
        "summary": "A method of modeling data with gaps by a sequence of curves has been developed. The new method is a generalization of iterative construction of singular expansion of matrices with gaps. Under discussion are three versions of the method featuring clear physical interpretation: linear - modeling the data by a sequence of linear manifolds of small dimension; quasilinear - constructing \"principal curves: (or \"principal surfaces\"), univalently projected on the linear principal components; essentially non-linear - based on constructing \"principal curves\": (principal strings and beams) employing the variation principle; the iteration implementation of this method is close to Kohonen self-organizing maps. The derived dependencies are extrapolated by Carleman's formulas. The method is interpreted as a construction of neural network conveyor designed to solve the following problems: to fill gaps in data; to repair data - to correct initial data values in such a way as to make the constructed models work best; to construct a calculator to fill gaps in the data line fed to the input.",
        "published": "2003-05-21T19:03:10Z",
        "link": "http://arxiv.org/abs/cond-mat/0305508v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.NE",
            "physics.data-an"
        ]
    },
    {
        "title": "Back-propagation of accuracy",
        "authors": [
            "M. Yu. Senashova",
            "A. N. Gorban",
            "D. C. Wunsch II"
        ],
        "summary": "In this paper we solve the problem: how to determine maximal allowable errors, possible for signals and parameters of each element of a network proceeding from the condition that the vector of output signals of the network should be calculated with given accuracy? \"Back-propagation of accuracy\" is developed to solve this problem. The calculation of allowable errors for each element of network by back-propagation of accuracy is surprisingly similar to a back-propagation of error, because it is the backward signals motion, but at the same time it is very different because the new rules of signals transformation in the passing back through the elements are different. The method allows us to formulate the requirements to the accuracy of calculations and to the realization of technical devices, if the requirements to the accuracy of output signals of the network are known.",
        "published": "2003-05-22T11:22:40Z",
        "link": "http://arxiv.org/abs/cond-mat/0305527v2",
        "categories": [
            "cond-mat.dis-nn",
            "cs.NA",
            "cs.NE",
            "math.NA"
        ]
    },
    {
        "title": "Predicting Response-Function Results of Electrical/Mechanical Systems   Through Artificial Neural Network",
        "authors": [
            "R. C. Gupta",
            "Ankur Agarwal",
            "Ruchi Gupta",
            "Sanjay Gupta"
        ],
        "summary": "In the present paper a newer application of Artificial Neural Network (ANN) has been developed i.e., predicting response-function results of electrical-mechanical system through ANN. This method is specially useful to complex systems for which it is not possible to find the response-function because of complexity of the system. The proposed approach suggests that how even without knowing the response-function, the response-function results can be predicted with the use of ANN to the system. The steps used are: (i) Depending on the system, the ANN-architecture and the input & output parameters are decided, (ii) Training & test data are generated from simplified circuits and through tactic-superposition of it for complex circuits, (iii) Training the ANN with training data through many cycles and (iv) Test-data are used for predicting the response-function results. It is found that the proposed novel method for response prediction works satisfactorily. Thus this method could be used specially for complex systems where other methods are unable to tackle it. In this paper the application of ANN is particularly demonstrated to electrical-circuit system but can be applied to other systems too.",
        "published": "2003-06-24T06:28:12Z",
        "link": "http://arxiv.org/abs/cs/0306125v1",
        "categories": [
            "cs.NE",
            "F1.1;I2.6;I5,1"
        ]
    },
    {
        "title": "Generation of Explicit Knowledge from Empirical Data through Pruning of   Trainable Neural Networks",
        "authors": [
            "A. N. Gorban",
            "Eu. M. Mirkes",
            "V. G. Tsaregorodtsev"
        ],
        "summary": "This paper presents a generalized technology of extraction of explicit knowledge from data. The main ideas are 1) maximal reduction of network complexity (not only removal of neurons or synapses, but removal all the unnecessary elements and signals and reduction of the complexity of elements), 2) using of adjustable and flexible pruning process (the pruning sequence shouldn't be predetermined - the user should have a possibility to prune network on his own way in order to achieve a desired network structure for the purpose of extraction of rules of desired type and form), and 3) extraction of rules not in predetermined but any desired form. Some considerations and notes about network architecture and training process and applicability of currently developed pruning techniques and rule extraction algorithms are discussed. This technology, being developed by us for more than 10 years, allowed us to create dozens of knowledge-based expert systems. In this paper we present a generalized three-step technology of extraction of explicit knowledge from empirical data.",
        "published": "2003-07-03T15:48:56Z",
        "link": "http://arxiv.org/abs/cond-mat/0307083v1",
        "categories": [
            "cond-mat",
            "cs.NE",
            "physics.data-an"
        ]
    },
    {
        "title": "Automatic Classification using Self-Organising Neural Networks in   Astrophysical Experiments",
        "authors": [
            "P. Boinee",
            "A. De Angelis",
            "E. Milotti"
        ],
        "summary": "Self-Organising Maps (SOMs) are effective tools in classification problems, and in recent years the even more powerful Dynamic Growing Neural Networks, a variant of SOMs, have been developed. Automatic Classification (also called clustering) is an important and difficult problem in many Astrophysical experiments, for instance, Gamma Ray Burst classification, or gamma-hadron separation. After a brief introduction to classification problem, we discuss Self-Organising Maps in section 2. Section 3 discusses with various models of growing neural networks and finally in section 4 we discuss the research perspectives in growing neural networks for efficient classification in astrophysical problems.",
        "published": "2003-07-12T12:04:33Z",
        "link": "http://arxiv.org/abs/cs/0307031v2",
        "categories": [
            "cs.NE",
            "astro-ph",
            "cs.AI",
            "I.5.1; I.5.3"
        ]
    },
    {
        "title": "Neural realisation of the SP theory: cell assemblies revisited",
        "authors": [
            "J. Gerard Wolff"
        ],
        "summary": "This paper describes how the elements of the SP theory (Wolff, 2003a) may be realised with neural structures and processes. To the extent that this is successful, the insights that have been achieved in the SP theory - the integration and simplification of a range of phenomena in perception and cognition - may be incorporated in a neural view of brain function.   These proposals may be seen as a development of Hebb's (1949) concept of a 'cell assembly'. By contrast with that concept and variants of it, the version described in this paper proposes that any one neuron can belong in one assembly and only one assembly. A distinctive feature of the present proposals is that any neuron or cluster of neurons within a cell assembly may serve as a proxy or reference for another cell assembly or class of cell assemblies. This device provides solutions to many of the problems associated with cell assemblies, it allows information to be stored in a compressed form, and it provides a robust mechanism by which assemblies may be connected to form hierarchies, grammars and other kinds of knowledge structure.   Drawing on insights derived from the SP theory, the paper also describes how unsupervised learning may be achieved with neural structures and processes. This theory of learning overcomes weaknesses in the Hebbian concept of learning and it is, at the same time, compatible with the observations that Hebb's theory was designed to explain.",
        "published": "2003-07-27T22:53:12Z",
        "link": "http://arxiv.org/abs/cs/0307060v2",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.0"
        ]
    },
    {
        "title": "Ensembles of Protein Molecules as Statistical Analog Computers",
        "authors": [
            "Victor Eliashberg"
        ],
        "summary": "A class of analog computers built from large numbers of microscopic probabilistic machines is discussed. It is postulated that such computers are implemented in biological systems as ensembles of protein molecules. The formalism is based on an abstract computational model referred to as Protein Molecule Machine (PMM). A PMM is a continuous-time first-order Markov system with real input and output vectors, a finite set of discrete states, and the input-dependent conditional probability densities of state transitions. The output of a PMM is a function of its input and state. The components of input vector, called generalized potentials, can be interpreted as membrane potential, and concentrations of neurotransmitters. The components of output vector, called generalized currents, can represent ion currents, and the flows of second messengers. An Ensemble of PMMs (EPMM) is a set of independent identical PMMs with the same input vector, and the output vector equal to the sum of output vectors of individual PMMs. The paper suggests that biological neurons have much more sophisticated computational resources than the presently popular models of artificial neurons.",
        "published": "2003-08-11T16:32:47Z",
        "link": "http://arxiv.org/abs/physics/0308041v2",
        "categories": [
            "physics.bio-ph",
            "cs.AI",
            "cs.NE",
            "physics.comp-ph",
            "physics.data-an",
            "q-bio.NC"
        ]
    },
    {
        "title": "Controlled hierarchical filtering: Model of neocortical sensory   processing",
        "authors": [
            "Andras Lorincz"
        ],
        "summary": "A model of sensory information processing is presented. The model assumes that learning of internal (hidden) generative models, which can predict the future and evaluate the precision of that prediction, is of central importance for information extraction. Furthermore, the model makes a bridge to goal-oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippocampal entorhinal loop. This generative control architecture is mapped to the neocortex and to the hippocampal entorhinal loop. Implicit memory phenomena; priming and prototype learning are emerging features of the model. Mathematical theorems ensure stability and attractive learning properties of the architecture. Connections to reinforcement learning are also established: both the control network, and the network with a hidden model converge to (near) optimal policy under suitable conditions. Falsifying predictions, including the role of the feedback connections between neocortical areas are made.",
        "published": "2003-08-16T07:31:57Z",
        "link": "http://arxiv.org/abs/cs/0308025v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "q-bio.NC",
            "C.1.3; F.1.1.; I.2.0; I.2.6; I.2.10; I.4.3.; I.4.10; I.5.1"
        ]
    },
    {
        "title": "Artificial Neural Networks for Beginners",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "The scope of this teaching package is to make a brief induction to Artificial Neural Networks (ANNs) for people who have no previous knowledge of them. We first make a brief introduction to models of networks, for then describing in general terms ANNs. As an application, we explain the backpropagation algorithm, since it is widely used and many other algorithms are derived from it. The user should know algebra and the handling of functions and vectors. Differential calculus is recommendable, but not necessary. The contents of this package should be understood by people with high school education. It would be useful for people who are just curious about what are ANNs, or for people who want to become familiar with them, so when they study them more fully, they will already have clear notions of ANNs. Also, people who only want to apply the backpropagation algorithm without a detailed and formal explanation of it will find this material useful. This work should not be seen as \"Nets for dummies\", but of course it is not a treatise. Much of the formality is skipped for the sake of simplicity. Detailed explanations and demonstrations can be found in the referred readings. The included exercises complement the understanding of the theory. The on-line resources are highly recommended for extending this brief induction.",
        "published": "2003-08-20T09:40:25Z",
        "link": "http://arxiv.org/abs/cs/0308031v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "C.1.3; I.5.1"
        ]
    },
    {
        "title": "What Is Working Memory and Mental Imagery? A Robot that Learns to   Perform Mental Computations",
        "authors": [
            "Victor Eliashberg"
        ],
        "summary": "This paper goes back to Turing (1936) and treats his machine as a cognitive model (W,D,B), where W is an \"external world\" represented by memory device (the tape divided into squares), and (D,B) is a simple robot that consists of the sensory-motor devices, D, and the brain, B. The robot's sensory-motor devices (the \"eye\", the \"hand\", and the \"organ of speech\") allow the robot to simulate the work of any Turing machine. The robot simulates the internal states of a Turing machine by \"talking to itself.\" At the stage of training, the teacher forces the robot (by acting directly on its motor centers) to perform several examples of an algorithm with different input data presented on tape. Two effects are achieved: 1) The robot learns to perform the shown algorithm with any input data using the tape. 2) The robot learns to perform the algorithm \"mentally\" using an \"imaginary tape.\" The model illustrates the simplest concept of a universal learning neurocomputer, demonstrates universality of associative learning as the mechanism of programming, and provides a simplified, but nontrivial neurobiologically plausible explanation of the phenomena of working memory and mental imagery. The model is implemented as a user-friendly program for Windows called EROBOT. The program is available at www.brain0.com/software.html.",
        "published": "2003-09-08T07:31:55Z",
        "link": "http://arxiv.org/abs/cs/0309009v1",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.0"
        ]
    },
    {
        "title": "Exploration of RNA Editing and Design of Robust Genetic Algorithms",
        "authors": [
            "C. Huang",
            "L. M. Rocha"
        ],
        "summary": "This paper presents our computational methodology using Genetic Algorithms (GA) for exploring the nature of RNA editing. These models are constructed using several genetic editing characteristics that are gleaned from the RNA editing system as observed in several organisms. We have expanded the traditional Genetic Algorithm with artificial editing mechanisms as proposed by (Rocha, 1997). The incorporation of editing mechanisms provides a means for artificial agents with genetic descriptions to gain greater phenotypic plasticity, which may be environmentally regulated. Our first implementations of these ideas have shed some light into the evolutionary implications of RNA editing. Based on these understandings, we demonstrate how to select proper RNA editors for designing more robust GAs, and the results will show promising applications to real-world problems. We expect that the framework proposed will both facilitate determining the evolutionary role of RNA editing in biology, and advance the current state of research in Genetic Algorithms.",
        "published": "2003-09-09T04:25:26Z",
        "link": "http://arxiv.org/abs/cs/0309012v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "nlin.AO",
            "q-bio.GN",
            "I.2"
        ]
    },
    {
        "title": "Using Simulated Annealing to Calculate the Trembles of Trembling Hand   Perfection",
        "authors": [
            "Stuart McDonald",
            "Liam Wagner"
        ],
        "summary": "Within the literature on non-cooperative game theory, there have been a number of attempts to propose logorithms which will compute Nash equilibria. Rather than derive a new algorithm, this paper shows that the family of algorithms known as Markov chain Monte Carlo (MCMC) can be used to calculate Nash equilibria. MCMC is a type of Monte Carlo simulation that relies on Markov chains to ensure its regularity conditions. MCMC has been widely used throughout the statistics and optimization literature, where variants of this algorithm are known as simulated annealing. This paper shows that there is interesting connection between the trembles that underlie the functioning of this algorithm and the type of Nash refinement known as trembling hand perfection.",
        "published": "2003-09-10T15:11:44Z",
        "link": "http://arxiv.org/abs/cs/0309016v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.NE",
            "q-bio.PE",
            "F.1.1;F.2.2;G.3;I.2.1;J.4"
        ]
    },
    {
        "title": "A Neural Network Assembly Memory Model Based on an Optimal Binary Signal   Detection Theory",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "A ternary/binary data coding algorithm and conditions under which Hopfield networks implement optimal convolutional or Hamming decoding algorithms has been described. Using the coding/decoding approach (an optimal Binary Signal Detection Theory, BSDT) introduced a Neural Network Assembly Memory Model (NNAMM) is built. The model provides optimal (the best) basic memory performance and demands the use of a new memory unit architecture with two-layer Hopfield network, N-channel time gate, auxiliary reference memory, and two nested feedback loops. NNAMM explicitly describes the dependence on time of a memory trace retrieval, gives a possibility of metamemory simulation, generalized knowledge representation, and distinct description of conscious and unconscious mental processes. A model of smallest inseparable part or an \"atom\" of consciousness is also defined. The NNAMM's neurobiological backgrounds and its applications to solving some interdisciplinary problems are shortly discussed. BSDT could implement the \"best neural code\" used in nervous tissues of animals and humans.",
        "published": "2003-09-21T17:11:11Z",
        "link": "http://arxiv.org/abs/cs/0309036v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "cs.NE",
            "q-bio.NC",
            "q-bio.QM",
            "I.2; E.4; J.3; J.4"
        ]
    },
    {
        "title": "A novel evolutionary formulation of the maximum independent set problem",
        "authors": [
            "V. C. Barbosa",
            "L. C. D. Campos"
        ],
        "summary": "We introduce a novel evolutionary formulation of the problem of finding a maximum independent set of a graph. The new formulation is based on the relationship that exists between a graph's independence number and its acyclic orientations. It views such orientations as individuals and evolves them with the aid of evolutionary operators that are very heavily based on the structure of the graph and its acyclic orientations. The resulting heuristic has been tested on some of the Second DIMACS Implementation Challenge benchmark graphs, and has been found to be competitive when compared to several of the other heuristics that have also been tested on those graphs.",
        "published": "2003-09-22T13:05:51Z",
        "link": "http://arxiv.org/abs/cs/0309038v1",
        "categories": [
            "cs.NE",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Two novel evolutionary formulations of the graph coloring problem",
        "authors": [
            "V. C. Barbosa",
            "C. A. G. Assis",
            "J. O. do Nascimento"
        ],
        "summary": "We introduce two novel evolutionary formulations of the problem of coloring the nodes of a graph. The first formulation is based on the relationship that exists between a graph's chromatic number and its acyclic orientations. It views such orientations as individuals and evolves them with the aid of evolutionary operators that are very heavily based on the structure of the graph and its acyclic orientations. The second formulation, unlike the first one, does not tackle one graph at a time, but rather aims at evolving a `program' to color all graphs belonging to a class whose members all have the same number of nodes and other common attributes. The heuristics that result from these formulations have been tested on some of the Second DIMACS Implementation Challenge benchmark graphs, and have been found to be competitive when compared to the several other heuristics that have also been tested on those graphs.",
        "published": "2003-09-23T00:53:20Z",
        "link": "http://arxiv.org/abs/cs/0309039v1",
        "categories": [
            "cs.NE",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "On Interference of Signals and Generalization in Feedforward Neural   Networks",
        "authors": [
            "Artur Rataj"
        ],
        "summary": "This paper studies how the generalization ability of neurons can be affected by mutual processing of different signals. This study is done on the basis of a feedforward artificial neural network. The mutual processing of signals can possibly be a good model of patterns in a set generalized by a neural network and in effect may improve generalization. In this paper it is discussed that the interference may also cause a highly random generalization. Adaptive activation functions are discussed as a way of reducing that type of generalization. A test of a feedforward neural network is performed that shows the discussed random generalization.",
        "published": "2003-10-06T15:40:44Z",
        "link": "http://arxiv.org/abs/cs/0310009v3",
        "categories": [
            "cs.NE",
            "I.2.6"
        ]
    },
    {
        "title": "Pattern Excitation-Based Processing: The Music of The Brain",
        "authors": [
            "Lev Koyrakh"
        ],
        "summary": "An approach to information processing based on the excitation of patterns of activity by non-linear active resonators in response to their input patterns is proposed. Arguments are presented to show that any computation performed by a conventional Turing machine-based computer, called T-machine in this paper, could also be performed by the pattern excitation-based machine, which will be called P-machine. A realization of this processing scheme by neural networks is discussed. In this realization, the role of the resonators is played by neural pattern excitation networks, which are the neural circuits capable of exciting different spatio-temporal patterns of activity in response to different inputs. Learning in the neural pattern excitation networks is also considered. It is shown that there is a duality between pattern excitation and pattern recognition neural networks, which allows to create new pattern excitation modes corresponding to recognizable input patterns, based on Hebbian learning rules. Hierarchically organized, such networks can produce complex behavior. Animal behavior, human language and thought are treated as examples produced by such networks.",
        "published": "2003-10-20T19:36:04Z",
        "link": "http://arxiv.org/abs/q-bio/0310025v3",
        "categories": [
            "q-bio.NC",
            "cs.NE",
            "physics.bio-ph"
        ]
    },
    {
        "title": "Feedforward Neural Networks with Diffused Nonlinear Weight Functions",
        "authors": [
            "Artur Rataj"
        ],
        "summary": "In this paper, feedforward neural networks are presented that have nonlinear weight functions based on look--up tables, that are specially smoothed in a regularization called the diffusion. The idea of such a type of networks is based on the hypothesis that the greater number of adaptive parameters per a weight function might reduce the total number of the weight functions needed to solve a given problem. Then, if the computational complexity of a propagation through a single such a weight function would be kept low, then the introduced neural networks might possibly be relatively fast.   A number of tests is performed, showing that the presented neural networks may indeed perform better in some cases than the classic neural networks and a number of other learning machines.",
        "published": "2003-10-27T14:27:14Z",
        "link": "http://arxiv.org/abs/cs/0310050v4",
        "categories": [
            "cs.NE",
            "I.2.6"
        ]
    },
    {
        "title": "Hybrid LQG-Neural Controller for Inverted Pendulum System",
        "authors": [
            "E. S. Sazonov",
            "P. Klinkhachorn",
            "R. L. Klein"
        ],
        "summary": "The paper presents a hybrid system controller, incorporating a neural and an LQG controller. The neural controller has been optimized by genetic algorithms directly on the inverted pendulum system. The failure free optimization process stipulated a relatively small region of the asymptotic stability of the neural controller, which is concentrated around the regulation point. The presented hybrid controller combines benefits of a genetically optimized neural controller and an LQG controller in a single system controller. High quality of the regulation process is achieved through utilization of the neural controller, while stability of the system during transient processes and a wide range of operation are assured through application of the LQG controller. The hybrid controller has been validated by applying it to a simulation model of an inherently unstable system of inverted pendulum.",
        "published": "2003-11-30T00:19:19Z",
        "link": "http://arxiv.org/abs/cs/0312003v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6;C.1.3;I.5.1"
        ]
    },
    {
        "title": "Failure-Free Genetic Algorithm Optimization of a System Controller Using   SAFE/LEARNING Controllers in Tandem",
        "authors": [
            "E. S. Sazonov",
            "D. Del Gobbo",
            "P. Klinkhachorn",
            "R. L. Klein"
        ],
        "summary": "The paper presents a method for failure free genetic algorithm optimization of a system controller. Genetic algorithms present a powerful tool that facilitates producing near-optimal system controllers. Applied to such methods of computational intelligence as neural networks or fuzzy logic, these methods are capable of combining the non-linear mapping capabilities of the latter with learning the system behavior directly, that is, without a prior model. At the same time, genetic algorithms routinely produce solutions that lead to the failure of the controlled system. Such solutions are generally unacceptable for applications where safe operation must be guaranteed. We present here a method of design, which allows failure-free application of genetic algorithms through utilization of SAFE and LEARNING controllers in tandem, where the SAFE controller recovers the system from dangerous states while the LEARNING controller learns its behavior. The method has been validated by applying it to an inherently unstable system of inverted pendulum.",
        "published": "2003-12-03T22:29:01Z",
        "link": "http://arxiv.org/abs/cs/0312009v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "I.2.6;C.1.3;I.5.1"
        ]
    },
    {
        "title": "Mapping weblog communities",
        "authors": [
            "Juan-J. Merelo-Guervos",
            "Beatriz Prieto",
            "Fatima Rateb",
            "Fernando Tricas"
        ],
        "summary": "Websites of a particular class form increasingly complex networks, and new tools are needed to map and understand them. A way of visualizing this complex network is by mapping it. A map highlights which members of the community have similar interests, and reveals the underlying social network. In this paper, we will map a network of websites using Kohonen's self-organizing map (SOM), a neural-net like method generally used for clustering and visualization of complex data sets. The set of websites considered has been the Blogalia weblog hosting site (based at http://www.blogalia.com/), a thriving community of around 200 members, created in January 2002. In this paper we show how SOM discovers interesting community features, its relation with other community-discovering algorithms, and the way it highlights the set of communities formed over the network.",
        "published": "2003-12-20T08:55:48Z",
        "link": "http://arxiv.org/abs/cs/0312047v1",
        "categories": [
            "cs.NE",
            "J.4;H.3.5;I.2.m"
        ]
    },
    {
        "title": "Some remarks on the survey decimation algorithm for K-satisfiability",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In this note we study the convergence of the survey decimation algorithm. An analytic formula for the reduction of the complexity during the decimation is derived. The limit of the converge of the algorithm are estimated in the random case: interesting phenomena appear near the boundary of convergence.",
        "published": "2003-01-16T10:38:36Z",
        "link": "http://arxiv.org/abs/cs/0301015v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DS",
            "G.3, G.2.1"
        ]
    },
    {
        "title": "Smoothed Analysis of Interior-Point Algorithms: Termination",
        "authors": [
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We perform a smoothed analysis of the termination phase of an interior-point method. By combining this analysis with the smoothed analysis of Renegar's interior-point algorithm by Dunagan, Spielman and Teng, we show that the smoothed complexity of an interior-point algorithm for linear programming is $O (m^{3} \\log (m/\\sigma))$. In contrast, the best known bound on the worst-case complexity of linear programming is $O (m^{3} L)$, where $L$ could be as large as $m$. We include an introduction to smoothed analysis and a tutorial on proof techniques that have been useful in smoothed analyses.",
        "published": "2003-01-21T17:47:05Z",
        "link": "http://arxiv.org/abs/cs/0301019v1",
        "categories": [
            "cs.DS",
            "F.2.1; G.1.6"
        ]
    },
    {
        "title": "PHORMA: Perfectly Hashable Order Restricted Multidimensional Arrays",
        "authors": [
            "Lauro Lins",
            "Sostenes Lins",
            "Silvio Melo"
        ],
        "summary": "In this paper we propose a simple and efficient data structure yielding a perfect hashing of quite general arrays. The data structure is named phorma, which is an acronym for perfectly hashable order restricted multidimensional array.   Keywords: Perfect hash function, Digraph, Implicit enumeration, Nijenhuis-Wilf combinatorial family.",
        "published": "2003-01-21T23:55:17Z",
        "link": "http://arxiv.org/abs/cs/0301021v2",
        "categories": [
            "cs.DS",
            "E.2;E.1"
        ]
    },
    {
        "title": "Bounds on the Number of Longest Common Subsequences",
        "authors": [
            "Ronald I. Greenberg"
        ],
        "summary": "This paper performs the analysis necessary to bound the running time of known, efficient algorithms for generating all longest common subsequences. That is, we bound the running time as a function of input size for algorithms with time essentially proportional to the output size. This paper considers both the case of computing all distinct LCSs and the case of computing all LCS embeddings. Also included is an analysis of how much better the efficient algorithms are than the standard method of generating LCS embeddings. A full analysis is carried out with running times measured as a function of the total number of input characters, and much of the analysis is also provided for cases in which the two input sequences are of the same specified length or of two independently specified lengths.",
        "published": "2003-01-28T17:53:16Z",
        "link": "http://arxiv.org/abs/cs/0301030v2",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.1"
        ]
    },
    {
        "title": "Computing the Number of Longest Common Subsequences",
        "authors": [
            "Ronald I. Greenberg"
        ],
        "summary": "This note provides very simple, efficient algorithms for computing the number of distinct longest common subsequences of two input strings and for computing the number of LCS embeddings.",
        "published": "2003-01-29T20:02:22Z",
        "link": "http://arxiv.org/abs/cs/0301034v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Algorithms using Java for Spreadsheet Dependent Cell Recomputation",
        "authors": [
            "Joe Francoeur"
        ],
        "summary": "Java implementations of algorithms used by spreadsheets to automatically recompute the set of cells dependent on a changed cell are described using a mathematical model for spreadsheets based on graph theory. These solutions comprise part of a Java API that allows a client application to read, modify, and maintain spreadsheet data without using the spreadsheet application program that produced it. Features of the Java language that successfully improve the running time performance of the algorithms are also described.",
        "published": "2003-01-31T20:19:21Z",
        "link": "http://arxiv.org/abs/cs/0301036v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "E.1;E.2;F.2.2"
        ]
    },
    {
        "title": "Barnacle: An Assembly Algorithm for Clone-based Sequences of Whole   Genomes",
        "authors": [
            "Vicky Choi",
            "Martin Farach-Colton"
        ],
        "summary": "We propose an assembly algorithm {\\sc Barnacle} for sequences generated by the clone-based approach. We illustrate our approach by assembling the human genome. Our novel method abandons the original physical-mapping-first framework. As we show, {\\sc Barnacle} more effectively resolves conflicts due to repeated sequences. The latter is the main difficulty of the sequence assembly problem. Inaddition, we are able to detect inconsistencies in the underlying data. We present and compare our results on the December 2001 freeze of the public working draft of the human genome with NCBI's assembly (Build 28).   The assembly of December 2001 freeze of the public working draft generated by {\\sc Barnacle} and the source code of {\\sc Barnacle} are available at (http://www.cs.rutgers.edu/~vchoi).",
        "published": "2003-02-03T22:42:08Z",
        "link": "http://arxiv.org/abs/cs/0302005v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "q-bio",
            "G.4; G2.3"
        ]
    },
    {
        "title": "Data Structure for a Time-Based Bandwidth Reservations Problem",
        "authors": [
            "Andrej Brodnik",
            "Andreas Nilsson"
        ],
        "summary": "We discuss a problem of handling resource reservations. The resource can be reserved for some time, it can be freed or it can be queried what is the largest amount of reserved resource during a time interval. We show that the problem has a lower bound of $\\Omega(\\log n)$ per operation on average and we give a matching upper bound algorithm. Our solution also solves a dynamic version of the related problems of a prefix sum and a partial sum.",
        "published": "2003-02-06T17:48:37Z",
        "link": "http://arxiv.org/abs/cs/0302009v1",
        "categories": [
            "cs.DS",
            "cs.NI",
            "E.1; C.2.3"
        ]
    },
    {
        "title": "Smoothed Analysis of Interior-Point Algorithms: Condition Number",
        "authors": [
            "John Dunagan",
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We show that the smoothed complexity of the logarithm of Renegar's condition number is O(log (n/sigma)).",
        "published": "2003-02-10T06:14:26Z",
        "link": "http://arxiv.org/abs/cs/0302011v2",
        "categories": [
            "cs.DS",
            "cs.NA",
            "F.2.1; G.1.6"
        ]
    },
    {
        "title": "Power and beauty of interval methods",
        "authors": [
            "Marek W. Gutowski"
        ],
        "summary": "Interval calculus is a relatively new branch of mathematics. Initially understood as a set of tools to assess the quality of numerical calculations (rigorous control of rounding errors), it became a discipline in its own rights today. Interval methods are usefull whenever we have to deal with uncertainties, which can be rigorously bounded. Fuzzy sets, rough sets and probability calculus can perform similar tasks, yet only the interval methods are able to (dis)prove, with mathematical rigor, the (non)existence of desired solution(s). Known are several problems, not presented here, which cannot be effectively solved by any other means.   This paper presents basic notions and main ideas of interval calculus and two examples of useful algorithms.",
        "published": "2003-02-11T16:27:50Z",
        "link": "http://arxiv.org/abs/physics/0302034v2",
        "categories": [
            "physics.data-an",
            "cs.DS",
            "physics.gen-ph"
        ]
    },
    {
        "title": "Fault-tolerant routing in peer-to-peer systems",
        "authors": [
            "James Aspnes",
            "Zoe Diamadi",
            "Gauri Shah"
        ],
        "summary": "We consider the problem of designing an overlay network and routing mechanism that permits finding resources efficiently in a peer-to-peer system. We argue that many existing approaches to this problem can be modeled as the construction of a random graph embedded in a metric space whose points represent resource identifiers, where the probability of a connection between two nodes depends only on the distance between them in the metric space. We study the performance of a peer-to-peer system where nodes are embedded at grid points in a simple metric space: a one-dimensional real line. We prove upper and lower bounds on the message complexity of locating particular resources in such a system, under a variety of assumptions about failures of either nodes or the connections between them. Our lower bounds in particular show that the use of inverse power-law distributions in routing, as suggested by Kleinberg (1999), is close to optimal. We also give efficient heuristics to dynamically maintain such a system as new nodes arrive and old nodes depart. Finally, we give experimental results that suggest promising directions for future work.",
        "published": "2003-02-15T17:15:46Z",
        "link": "http://arxiv.org/abs/cs/0302022v2",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; C.2.4; E.1"
        ]
    },
    {
        "title": "The traveling salesman problem for cubic graphs",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show how to find a Hamiltonian cycle in a graph of degree at most three with n vertices, in time O(2^{n/3}) ~= 1.260^n and linear space. Our algorithm can find the minimum weight Hamiltonian cycle (traveling salesman problem), in the same time bound. We can also count or list all Hamiltonian cycles in a degree three graph in time O(2^{3n/8}) ~= 1.297^n. We also solve the traveling salesman problem in graphs of degree at most four, by randomized and deterministic algorithms with runtime O((27/4)^{n/3}) ~= 1.890^n and O((27/4+epsilon)^{n/3}) respectively. Our algorithms allow the input to specify a set of forced edges which must be part of any generated cycle. Our cycle listing algorithm shows that every degree three graph has O(2^{3n/8}) Hamiltonian cycles; we also exhibit a family of graphs with 2^{n/3} Hamiltonian cycles per graph.",
        "published": "2003-02-20T06:36:35Z",
        "link": "http://arxiv.org/abs/cs/0302030v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Memory Efficient Arithmetic",
        "authors": [
            "Ernie Croot"
        ],
        "summary": "In this paper we give an algorithm for computing the mth base-b digit (m=1 is the least significant digit) of an integer n (actually, it finds sharp approximations to n/b^m mod 1), where n is defined as the last number in a sequence of integers s1,s2,...,sL=n, where s1=0, s2=1, and each successive si is either the sum, product, or difference of two previous sj's in the sequence. In many cases, the algorithm will find this mth digit using far less memory than it takes to write down all the base-b digits of n, while the number of bit operations will grow only slighly worse than linear in the number of digits.   One consequence of this result is that the mth base-10 digit of 2^t can be found using O(t^{2/3} log^C t) bits of storage (for some C>0), and O(t log^C t) bit operations.   The algorithm is also highly parallelizable, and an M-fold reduction in running time can be achieved using M processors, although the memory required will then grow by a factor of M.",
        "published": "2003-02-25T19:42:25Z",
        "link": "http://arxiv.org/abs/math/0302315v3",
        "categories": [
            "math.NT",
            "cs.DS",
            "11Y70"
        ]
    },
    {
        "title": "Quantum random walks - an introductory overview",
        "authors": [
            "Julia Kempe"
        ],
        "summary": "This article aims to provide an introductory survey on quantum random walks. Starting from a physical effect to illustrate the main ideas we will introduce quantum random walks, review some of their properties and outline their striking differences to classical walks. We will touch upon both physical effects and computer science applications, introducing some of the main concepts and language of present day quantum information science in this context. We will mention recent developments in this new area and outline some open questions.",
        "published": "2003-03-13T13:28:03Z",
        "link": "http://arxiv.org/abs/quant-ph/0303081v1",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Lock-free dynamic hash tables with open addressing",
        "authors": [
            "Hui Gao",
            "Jan Friso Groote",
            "Wim H. Hesselink"
        ],
        "summary": "We present an efficient lock-free algorithm for parallel accessible hash tables with open addressing, which promises more robust performance and reliability than conventional lock-based implementations. ``Lock-free'' means that it is guaranteed that always at least one process completes its operation within a bounded number of steps. For a single processor architecture our solution is as efficient as sequential hash tables. On a multiprocessor architecture this is also the case when all processors have comparable speeds. The algorithm allows processors that have widely different speeds or come to a halt. It can easily be implemented using C-like languages and requires on average only constant time for insertion, deletion or accessing of elements. The algorithm allows the hash tables to grow and shrink when needed.   Lock-free algorithms are hard to design correctly, even when apparently straightforward. Ensuring the correctness of the design at the earliest possible stage is a major challenge in any responsible system development. In view of the complexity of the algorithm, we turned to the interactive theorem prover PVS for mechanical support. We employ standard deductive verification techniques to prove around 200 invariance properties of our algorithm, and describe how this is achieved with the theorem prover PVS.",
        "published": "2003-03-18T09:38:34Z",
        "link": "http://arxiv.org/abs/cs/0303011v4",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.1"
        ]
    },
    {
        "title": "Probabilistic behavior of hash tables",
        "authors": [
            "Dawei Hong",
            "Jean-Camille Birget",
            "Shushuang Man"
        ],
        "summary": "We extend a result of Goldreich and Ron about estimating the collision probability of a hash function. Their estimate has a polynomial tail. We prove that when the load factor is greater than a certain constant, the estimator has a gaussian tail. As an application we find an estimate of an upper bound for the average search time in hashing with chaining, for a particular user (we allow the overall key distribution to be different from the key distribution of a particular user). The estimator has a gaussian tail.",
        "published": "2003-03-21T17:25:15Z",
        "link": "http://arxiv.org/abs/cs/0303022v1",
        "categories": [
            "cs.DS",
            "cs.DB",
            "E.2"
        ]
    },
    {
        "title": "Quantum Computation and Lattice Problems",
        "authors": [
            "Oded Regev"
        ],
        "summary": "We present the first explicit connection between quantum computation and lattice problems. Namely, we show a solution to the Unique Shortest Vector Problem (SVP) under the assumption that there exists an algorithm that solves the hidden subgroup problem on the dihedral group by coset sampling. Moreover, we solve the hidden subgroup problem on the dihedral group by using an average case subset sum routine. By combining the two results, we get a quantum reduction from $\\Theta(n^{2.5})$-unique-SVP to the average case subset sum problem.",
        "published": "2003-04-01T23:35:11Z",
        "link": "http://arxiv.org/abs/cs/0304005v1",
        "categories": [
            "cs.DS",
            "F.2.1"
        ]
    },
    {
        "title": "Quasiconvex Analysis of Backtracking Algorithms",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We consider a class of multivariate recurrences frequently arising in the worst case analysis of Davis-Putnam-style exponential time backtracking algorithms for NP-hard problems. We describe a technique for proving asymptotic upper bounds on these recurrences, by using a suitable weight function to reduce the problem to that of solving univariate linear recurrences; show how to use quasiconvex programming to determine the weight function yielding the smallest upper bound; and prove that the resulting upper bounds are within a polynomial factor of the true asymptotics of the recurrence. We develop and implement a multiple-gradient descent algorithm for the resulting quasiconvex programs, using a real-number arithmetic package for guaranteed accuracy of the computed worst case time bounds.",
        "published": "2003-04-10T20:25:12Z",
        "link": "http://arxiv.org/abs/cs/0304018v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "math.CO",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Hybrid Rounding Techniques for Knapsack Problems",
        "authors": [
            "Monaldo Mastrolilli",
            "Marcus Hutter"
        ],
        "summary": "We address the classical knapsack problem and a variant in which an upper bound is imposed on the number of items that can be selected. We show that appropriate combinations of rounding techniques yield novel and powerful ways of rounding. As an application of these techniques, we present a linear-storage Polynomial Time Approximation Scheme (PTAS) and a Fully Polynomial Time Approximation Scheme (FPTAS) that compute an approximate solution, of any fixed accuracy, in linear time. This linear complexity bound gives a substantial improvement of the best previously known polynomial bounds.",
        "published": "2003-05-02T20:40:20Z",
        "link": "http://arxiv.org/abs/cs/0305002v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "Traveling Front Solutions to Directed Diffusion Limited Aggregation,   Digital Search Trees and the Lempel-Ziv Data Compression Algorithm",
        "authors": [
            "Satya N. Majumdar"
        ],
        "summary": "We use the traveling front approach to derive exact asymptotic results for the statistics of the number of particles in a class of directed diffusion limited aggregation models on a Cayley tree. We point out that some aspects of these models are closely connected to two different problems in computer science, namely the digital search tree problem in data structures and the Lempel-Ziv algorithm for data compression. The statistics of the number of particles studied here is related to the statistics of height in digital search trees which, in turn, is related to the statistics of the length of the longest word formed by the Lempel-Ziv algorithm. Implications of our results to these computer science problems are pointed out.",
        "published": "2003-05-06T05:23:39Z",
        "link": "http://arxiv.org/abs/cond-mat/0305097v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS"
        ]
    },
    {
        "title": "An In-Place Sorting with O(n log n) Comparisons and O(n) Moves",
        "authors": [
            "Gianni Franceschini",
            "Viliam Geffert"
        ],
        "summary": "We present the first in-place algorithm for sorting an array of size n that performs, in the worst case, at most O(n log n) element comparisons and O(n) element transports.   This solves a long-standing open problem, stated explicitly, e.g., in [J.I. Munro and V. Raman, Sorting with minimum data movement, J. Algorithms, 13, 374-93, 1992], of whether there exists a sorting algorithm that matches the asymptotic lower bounds on all computational resources simultaneously.",
        "published": "2003-05-09T14:56:07Z",
        "link": "http://arxiv.org/abs/cs/0305005v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Fast n-point correlation functions and three-point lensing application",
        "authors": [
            "Lucy Liuxuan Zhang",
            "Ue-Li Pen"
        ],
        "summary": "We present a new algorithm to rapidly compute the two-point (2PCF), three-point (3PCF) and n-point (n-PCF) correlation functions in roughly O(N log N) time for N particles, instead of O(N^n) as required by brute force approaches. The algorithm enables an estimate of the full 3PCF for as many as 10^6 galaxies. This technique exploits node-to-node correlations of a recursive bisectional binary tree. A balanced tree construction minimizes the depth of the tree and the worst case error at each node. The algorithm presented in this paper can be applied to problems with arbitrary geometry.   We describe the detailed implementation to compute the two point function and all eight components of the 3PCF for a two-component field, with attention to shear fields generated by gravitational lensing. We also generalize the algorithm to compute the n-point correlation function for a scalar field in k dimensions where n and k are arbitrary positive integers.",
        "published": "2003-05-23T05:01:28Z",
        "link": "http://arxiv.org/abs/astro-ph/0305447v5",
        "categories": [
            "astro-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "The Redesigned BaBar Event Store: Believe the Hype",
        "authors": [
            "Adeyemi Adesanya",
            "Jacek Becla",
            "Daniel Wang"
        ],
        "summary": "As the BaBar experiment progresses, it produces new and unforeseen requirements and increasing demands on capacity and feature base. The current system is being utilized well beyond its original design specifications, and has scaled appropriately, maintaining data consistency and durability. The persistent event storage system has remained largely unchanged since the initial implementation, and thus includes many design features which have become performance bottlenecks. Programming interfaces were designed before sufficient usage information became available. Performance and efficiency were traded off for added flexibility to cope with future demands. With significant experience in managing actual production data under our belt, we are now in a position to recraft the system to better suit current needs. The Event Store redesign is intended to eliminate redundant features while adding new ones, increase overall performance, and contain the physical storage cost of the world's largest database.",
        "published": "2003-06-04T23:51:52Z",
        "link": "http://arxiv.org/abs/cs/0306023v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "H.2.1; H.2.4; E.2"
        ]
    },
    {
        "title": "Permutation Generation: Two New Permutation Algorithms",
        "authors": [
            "Jie Gao",
            "Dianjun Wang"
        ],
        "summary": "Two completely new algorithms for generating permutations, shift-cursor algorithm and level algorithm, and their efficient implementations are presented in this paper. One implementation of the shift cursor algorithm gives an optimal solution of the permutation generation problem, and one implementation of the level algorithm can be used to generate random permutations.",
        "published": "2003-06-05T11:49:22Z",
        "link": "http://arxiv.org/abs/cs/0306025v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Skip Graphs",
        "authors": [
            "James Aspnes",
            "Gauri Shah"
        ],
        "summary": "Skip graphs are a novel distributed data structure, based on skip lists, that provide the full functionality of a balanced tree in a distributed system where resources are stored in separate nodes that may fail at any time. They are designed for use in searching peer-to-peer systems, and by providing the ability to perform queries based on key ordering, they improve on existing search tools that provide only hash table functionality. Unlike skip lists or other tree data structures, skip graphs are highly resilient, tolerating a large fraction of failed nodes without losing connectivity. In addition, constructing, inserting new nodes into, searching a skip graph, and detecting and repairing errors in the data structure introduced by node failures can be done using simple and straightforward algorithms.",
        "published": "2003-06-10T23:14:16Z",
        "link": "http://arxiv.org/abs/cs/0306043v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "C.2.4; E.1"
        ]
    },
    {
        "title": "Compositional competitiveness for distributed algorithms",
        "authors": [
            "James Aspnes",
            "Orli Waarts"
        ],
        "summary": "We define a measure of competitive performance for distributed algorithms based on throughput, the number of tasks that an algorithm can carry out in a fixed amount of work. This new measure complements the latency measure of Ajtai et al., which measures how quickly an algorithm can finish tasks that start at specified times. The novel feature of the throughput measure, which distinguishes it from the latency measure, is that it is compositional: it supports a notion of algorithms that are competitive relative to a class of subroutines, with the property that an algorithm that is k-competitive relative to a class of subroutines, combined with an l-competitive member of that class, gives a combined algorithm that is kl-competitive.   In particular, we prove the throughput-competitiveness of a class of algorithms for collect operations, in which each of a group of n processes obtains all values stored in an array of n registers. Collects are a fundamental building block of a wide variety of shared-memory distributed algorithms, and we show that several such algorithms are competitive relative to collects. Inserting a competitive collect in these algorithms gives the first examples of competitive distributed algorithms obtained by composition using a general construction.",
        "published": "2003-06-11T03:13:50Z",
        "link": "http://arxiv.org/abs/cs/0306044v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.1.2; F.2.m"
        ]
    },
    {
        "title": "Compact Approximation of Lattice Functions with Applications to   Large-Alphabet Text Search",
        "authors": [
            "Paolo Boldi",
            "Sebastiano Vigna"
        ],
        "summary": "We propose a very simple randomised data structure that stores an approximation from above of a lattice-valued function. Computing the function value requires a constant number of steps, and the error probability can be balanced with space usage, much like in Bloom filters. The structure is particularly well suited for functions that are bottom on most of their domain. We then show how to use our methods to store in a compact way the bad-character shift function for variants of the Boyer-Moore text search algorithms. As a result, we obtain practical implementations of these algorithms that can be used with large alphabets, such as Unicode collation elements, with a small setup time. The ideas described in this paper have been implemented as free software under the GNU General Public License within the MG4J project (http://mg4j.dsi.unimi.it/).",
        "published": "2003-06-11T09:13:39Z",
        "link": "http://arxiv.org/abs/cs/0306046v1",
        "categories": [
            "cs.DS",
            "E.2"
        ]
    },
    {
        "title": "Efficient pebbling for list traversal synopses",
        "authors": [
            "Yossi Matias",
            "Ely Porat"
        ],
        "summary": "We show how to support efficient back traversal in a unidirectional list, using small memory and with essentially no slowdown in forward steps. Using $O(\\log n)$ memory for a list of size $n$, the $i$'th back-step from the farthest point reached so far takes $O(\\log i)$ time in the worst case, while the overhead per forward step is at most $\\epsilon$ for arbitrary small constant $\\epsilon>0$. An arbitrary sequence of forward and back steps is allowed. A full trade-off between memory usage and time per back-step is presented: $k$ vs. $kn^{1/k}$ and vice versa. Our algorithms are based on a novel pebbling technique which moves pebbles on a virtual binary, or $t$-ary, tree that can only be traversed in a pre-order fashion. The compact data structures used by the pebbling algorithms, called list traversal synopses, extend to general directed graphs, and have other interesting applications, including memory efficient hash-chain implementation. Perhaps the most surprising application is in showing that for any program, arbitrary rollback steps can be efficiently supported with small overhead in memory, and marginal overhead in its ordinary execution. More concretely: Let $P$ be a program that runs for at most $T$ steps, using memory of size $M$. Then, at the cost of recording the input used by the program, and increasing the memory by a factor of $O(\\log T)$ to $O(M \\log T)$, the program $P$ can be extended to support an arbitrary sequence of forward execution and rollback steps: the $i$'th rollback step takes $O(\\log i)$ time in the worst case, while forward steps take O(1) time in the worst case, and $1+\\epsilon$ amortized time per step.",
        "published": "2003-06-16T21:31:36Z",
        "link": "http://arxiv.org/abs/cs/0306104v1",
        "categories": [
            "cs.DS",
            "D.2.5;E.1;E.3;I.6.7;F.2.3"
        ]
    },
    {
        "title": "Symbolic Parametric Analysis of Embedded Systems with BDD-like   Data-Structures",
        "authors": [
            "Farn Wang"
        ],
        "summary": "We use dense variable-ordering to define HRD (Hybrid-Restriction Diagram), a new BDD-like data-structure for the representation and manipulation of state-spaces of linear hybrid automata. We present and discuss various manipulation algorithms for HRD, including the basic set-oriented operations, weakest precondition calculation, and normalization. We implemented the ideas and experimented to see their performance. Finally, we have also developed a pruning technique for state-space exploration based on parameter valuation space characterization. The technique showed good promise in our experiment.",
        "published": "2003-06-19T09:57:09Z",
        "link": "http://arxiv.org/abs/cs/0306113v2",
        "categories": [
            "cs.DS",
            "cs.LO",
            "B.2.2; B.4.4; B.5.2; B.6.3; D.2.4; F.1.1; F.3.1; F.4.1"
        ]
    },
    {
        "title": "The Best Trail Algorithm for Assisted Navigation of Web Sites",
        "authors": [
            "Richard Wheeldon",
            "Mark Levene"
        ],
        "summary": "We present an algorithm called the Best Trail Algorithm, which helps solve the hypertext navigation problem by automating the construction of memex-like trails through the corpus. The algorithm performs a probabilistic best-first expansion of a set of navigation trees to find relevant and compact trails. We describe the implementation of the algorithm, scoring methods for trails, filtering algorithms and a new metric called \\emph{potential gain} which measures the potential of a page for future navigation opportunities.",
        "published": "2003-06-22T17:38:13Z",
        "link": "http://arxiv.org/abs/cs/0306122v1",
        "categories": [
            "cs.DS",
            "cs.IR",
            "H.3.3;H.5.4;G.2.2;F.2.2"
        ]
    },
    {
        "title": "Heuristic to reduce the complexity of complete bipartite graphs to   accelerate the search for maximum weighted matchings with small error",
        "authors": [
            "Daniel Etzold"
        ],
        "summary": "A maximum weighted matching for bipartite graphs $G=(A \\cup B,E)$ can be found by using the algorithm of Edmonds and Karp with a Fibonacci Heap and a modified Dijkstra in $O(nm + n^2 \\log{n})$ time where n is the number of nodes and m the number of edges. For the case that $|A|=|B|$ the number of edges is $n^2$ and therefore the complexity is $O(n^3)$. In this paper we want to present a simple heuristic method to reduce the number of edges of complete bipartite graphs $G=(A \\cup B,E)$ with $|A|=|B|$ such that $m = n\\log{n}$ and therefore the complexity of such that $m = n\\log{n}$ and therefore the complexity of $O(n^2 \\log{n})$. The weights of all edges in G must be uniformly distributed in [0,1].",
        "published": "2003-06-23T19:37:42Z",
        "link": "http://arxiv.org/abs/cs/0306123v1",
        "categories": [
            "cs.DS",
            "G.2.2"
        ]
    },
    {
        "title": "Finite size scaling approach to dynamic storage allocation problem",
        "authors": [
            "Hamed Seyed-allaei"
        ],
        "summary": "It is demonstrated how dynamic storage allocation algorithms can be analyzed in terms of finite size scaling. The method is illustrated in the three simple cases of the it first-fit, next-fit and it best-fit algorithms, and the system works at full capacity. The analysis is done from two different points of view - running speed and employed memory. In both cases, and for all algorithms, it is shown that a simple scaling function exists and the relevant exponents are calculated. The method can be applied on similar problems as well.",
        "published": "2003-07-02T16:37:47Z",
        "link": "http://arxiv.org/abs/cond-mat/0307058v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS"
        ]
    },
    {
        "title": "Optimal Adaptive Algorithms for Finding the Nearest and Farthest Point   on a Parametric Black-Box Curve",
        "authors": [
            "Ilya Baran",
            "Erik D. Demaine"
        ],
        "summary": "We consider a general model for representing and manipulating parametric curves, in which a curve is specified by a black box mapping a parameter value between 0 and 1 to a point in Euclidean d-space. In this model, we consider the nearest-point-on-curve and farthest-point-on-curve problems: given a curve C and a point p, find a point on C nearest to p or farthest from p. In the general black-box model, no algorithm can solve these problems. Assuming a known bound on the speed of the curve (a Lipschitz condition), the answer can be estimated up to an additive error of epsilon using O(1/epsilon) samples, and this bound is tight in the worst case. However, many instances can be solved with substantially fewer samples, and we give algorithms that adapt to the inherent difficulty of the particular instance, up to a logarithmic factor. More precisely, if OPT(C,p,epsilon) is the minimum number of samples of C that every correct algorithm must perform to achieve tolerance epsilon, then our algorithm performs O(OPT(C,p,epsilon) log (epsilon^(-1)/OPT(C,p,epsilon))) samples. Furthermore, any algorithm requires Omega(k log (epsilon^(-1)/k)) samples for some instance C' with OPT(C',p,epsilon) = k; except that, for the nearest-point-on-curve problem when the distance between C and p is less than epsilon, OPT is 1 but the upper and lower bounds on the number of samples are both Theta(1/epsilon). When bounds on relative error are desired, we give algorithms that perform O(OPT log (2+(1+epsilon^(-1)) m^(-1)/OPT)) samples (where m is the exact minimum or maximum distance from p to C) and prove that Omega(OPT log (1/epsilon)) samples are necessary on some problem instances.",
        "published": "2003-07-03T01:14:57Z",
        "link": "http://arxiv.org/abs/cs/0307005v3",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2; I.3.5"
        ]
    },
    {
        "title": "Range Mode and Range Median Queries on Lists and Trees",
        "authors": [
            "Danny Krizanc",
            "Pat Morin",
            "Michiel Smid"
        ],
        "summary": "We consider algorithms for preprocessing labelled lists and trees so that, for any two nodes u and v we can answer queries of the form: What is the mode or median label in the sequence of labels on the path from u to v.",
        "published": "2003-07-12T21:41:56Z",
        "link": "http://arxiv.org/abs/cs/0307034v1",
        "categories": [
            "cs.DS",
            "E.1"
        ]
    },
    {
        "title": "An Extension of the Lovasz Local Lemma, and its Applications to Integer   Programming",
        "authors": [
            "Aravind Srinivasan"
        ],
        "summary": "The Lovasz Local Lemma due to Erdos and Lovasz is a powerful tool in proving the existence of rare events. We present an extension of this lemma, which works well when the event to be shown to exist is a conjunction of individual events, each of which asserts that a random variable does not deviate much from its mean. As applications, we consider two classes of NP-hard integer programs: minimax and covering integer programs. A key technique, randomized rounding of linear relaxations, was developed by Raghavan and Thompson to derive good approximation algorithms for such problems. We use our extension of the Local Lemma to prove that randomized rounding produces, with non-zero probability, much better feasible solutions than known before, if the constraint matrices of these integer programs are column-sparse (e.g., routing using short paths, problems on hypergraphs with small dimension/degree). This complements certain well-known results from discrepancy theory. We also generalize the method of pessimistic estimators due to Raghavan, to obtain constructive (algorithmic) versions of our results for covering integer programs.",
        "published": "2003-07-18T04:02:18Z",
        "link": "http://arxiv.org/abs/cs/0307043v1",
        "categories": [
            "cs.DS",
            "F.1.2; F.2.2; G.2.2; G.3"
        ]
    },
    {
        "title": "Euclidean algorithms are Gaussian",
        "authors": [
            "Viviane Baladi",
            "Brigitte Vallee"
        ],
        "summary": "This study provides new results about the probabilistic behaviour of a class of Euclidean algorithms: the asymptotic distribution of a whole class of cost-parameters associated to these algorithms is normal. For the cost corresponding to the number of steps Hensley already has proved a Local Limit Theorem; we give a new proof, and extend his result to other euclidean algorithms and to a large class of digit costs, obtaining a faster, optimal, rate of convergence. The paper is based on the dynamical systems methodology, and the main tool is the transfer operator. In particular, we use recent results of Dolgopyat.",
        "published": "2003-07-28T10:31:53Z",
        "link": "http://arxiv.org/abs/cs/0307062v4",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.1, I.1.2"
        ]
    },
    {
        "title": "DPG: A Cache-Efficient Accelerator for Sorting and for Join Operators",
        "authors": [
            "Gene Cooperman",
            "Xiaoqin Ma",
            "Viet Ha Nguyen"
        ],
        "summary": "We present a new algorithm for fast record retrieval, distribute-probe-gather, or DPG. DPG has important applications both in sorting and in joins. Current main memory sorting algorithms split their work into three phases: extraction of key-pointer pairs; sorting of the key-pointer pairs; and copying of the original records into the destination array according the sorted key-pointer pairs. The copying in the last phase dominates today's sorting time. Hence, the use of DPG in the third phase provides an accelerator for existing sorting algorithms.   DPG also provides two new join methods for foreign key joins: DPG-move join and DPG-sort join. The resulting join methods with DPG are faster because DPG join is cache-efficient and at the same time DPG join avoids the need for sorting or for hashing. The ideas presented for foreign key join can also be extended to faster record pair retrieval for spatial and temporal databases.",
        "published": "2003-08-02T08:13:06Z",
        "link": "http://arxiv.org/abs/cs/0308004v1",
        "categories": [
            "cs.DB",
            "cs.DS",
            "E.1; E.2; F.2.2"
        ]
    },
    {
        "title": "Higher-Dimensional Packing with Order Constraints",
        "authors": [
            "Sandor P. Fekete",
            "Ekkehard Koehler",
            "Juergen Teich"
        ],
        "summary": "We present a first exact study on higher-dimensional packing problems with order constraints. Problems of this type occur naturally in applications such as logistics or computer architecture and can be interpreted as higher-dimensional generalizations of scheduling problems. Using graph-theoretic structures to describe feasible solutions, we develop a novel exact branch-and-bound algorithm. This extends previous work by Fekete and Schepers; a key tool is a new order-theoretic characterization of feasible extensions of a partial order to a given complementarity graph that is tailor-made for use in a branch-and-bound environment. The usefulness of our approach is validated by computational results.",
        "published": "2003-08-04T14:28:00Z",
        "link": "http://arxiv.org/abs/cs/0308006v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Short Cycles Connectivity",
        "authors": [
            "V. Batagelj",
            "M. Zaversnik"
        ],
        "summary": "Short cycles connectivity is a generalization of ordinary connectivity. Instead by a path (sequence of edges), two vertices have to be connected by a sequence of short cycles, in which two adjacent cycles have at least one common vertex. If all adjacent cycles in the sequence share at least one edge, we talk about edge short cycles connectivity.   It is shown that the short cycles connectivity is an equivalence relation on the set of vertices, while the edge short cycles connectivity components determine an equivalence relation on the set of edges. Efficient algorithms for determining equivalence classes are presented.   Short cycles connectivity can be extended to directed graphs (cyclic and transitive connectivity). For further generalization we can also consider connectivity by small cliques or other families of graphs.",
        "published": "2003-08-05T12:58:10Z",
        "link": "http://arxiv.org/abs/cs/0308011v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "A new approach to relevancy in Internet searching - the \"Vox Populi   Algorithm\"",
        "authors": [
            "Andreas Schaale",
            "Carsten Wulf-Mathies",
            "Soenke Lieberam-Schmidt"
        ],
        "summary": "In this paper we will derive a new algorithm for Internet searching. The main idea of this algorithm is to extend the existing algorithms by a component, which reflects the interests of the users more than existing methods. The \"Vox Populi Algorithm\" (VPA) creates a feedback from the users to the content of the search index. The information derived from the users query analysis is used to modify the existing crawling algorithms. The VPA controls the distribution of the resources of the crawler. Finally, we also discuss methods of suppressing unwanted content (spam).",
        "published": "2003-08-23T13:34:25Z",
        "link": "http://arxiv.org/abs/cs/0308039v1",
        "categories": [
            "cs.DS",
            "cond-mat.dis-nn",
            "cs.IR",
            "H.3.1; H.3.2; H.3.3"
        ]
    },
    {
        "title": "Static Data Structure for Discrete Advance Bandwidth Reservations on the   Internet",
        "authors": [
            "Andrej Brodnik",
            "Andreas Nilsson"
        ],
        "summary": "In this paper we present a discrete data structure for reservations of limited resources. A reservation is defined as a tuple consisting of the time interval of when the resource should be reserved, $I_R$, and the amount of the resource that is reserved, $B_R$, formally $R=\\{I_R,B_R\\}$.   The data structure is similar to a segment tree. The maximum spanning interval of the data structure is fixed and defined in advance. The granularity and thereby the size of the intervals of the leaves is also defined in advance. The data structure is built only once. Neither nodes nor leaves are ever inserted, deleted or moved. Hence, the running time of the operations does not depend on the number of reservations previously made. The running time does not depend on the size of the interval of the reservation either. Let $n$ be the number of leaves in the data structure. In the worst case, the number of touched (i.e. traversed) nodes is in any operation $O(\\log n)$, hence the running time of any operation is also $O(\\log n)$.",
        "published": "2003-08-24T06:30:41Z",
        "link": "http://arxiv.org/abs/cs/0308041v1",
        "categories": [
            "cs.DS",
            "E.1, C.2.3"
        ]
    },
    {
        "title": "EqRank: A Self-Consistent Equivalence Relation on Graph Vertexes",
        "authors": [
            "Grigorii Pivovarov",
            "Sergei Trunov"
        ],
        "summary": "A new method of hierarchical clustering of graph vertexes is suggested. In the method, the graph partition is determined with an equivalence relation satisfying a recursive definition stating that vertexes are equivalent if the vertexes they point to (or vertexes pointing to them) are equivalent. Iterative application of the partitioning yields a hierarchical clustering of graph vertexes. The method is applied to the citation graph of hep-th. The outcome is a two-level classification scheme for the subject field presented in hep-th, and indexing of the papers from hep-th in this scheme. A number of tests show that the classification obtained is adequate.",
        "published": "2003-08-29T13:20:03Z",
        "link": "http://arxiv.org/abs/cs/0308044v1",
        "categories": [
            "cs.DS",
            "cs.DL",
            "H.3.3"
        ]
    },
    {
        "title": "Indexing Schemes for Similarity Search In Datasets of Short Protein   Fragments",
        "authors": [
            "Aleksandar Stojmirovic",
            "Vladimir Pestov"
        ],
        "summary": "We propose a family of very efficient hierarchical indexing schemes for ungapped, score matrix-based similarity search in large datasets of short (4-12 amino acid) protein fragments. This type of similarity search has importance in both providing a building block to more complex algorithms and for possible use in direct biological investigations where datasets are of the order of 60 million objects. Our scheme is based on the internal geometry of the amino acid alphabet and performs exceptionally well, for example outputting 100 nearest neighbours to any possible fragment of length 10 after scanning on average less than one per cent of the entire dataset.",
        "published": "2003-09-05T22:59:40Z",
        "link": "http://arxiv.org/abs/cs/0309005v4",
        "categories": [
            "cs.DS",
            "q-bio.BM",
            "H.3.1; J.3"
        ]
    },
    {
        "title": "Optimal Covering Tours with Turn Costs",
        "authors": [
            "Esther M. Arkin",
            "Michael A. Bender",
            "Erik D. Demaine",
            "Sandor P. Fekete",
            "Joseph S. B. Mitchell",
            "Saurabh Sethia"
        ],
        "summary": "We give the first algorithmic study of a class of ``covering tour'' problems related to the geometric Traveling Salesman Problem: Find a polygonal tour for a cutter so that it sweeps out a specified region (``pocket''), in order to minimize a cost that depends mainly on the number of em turns. These problems arise naturally in manufacturing applications of computational geometry to automatic tool path generation and automatic inspection systems, as well as arc routing (``postman'') problems with turn penalties. We prove the NP-completeness of minimum-turn milling and give efficient approximation algorithms for several natural versions of the problem, including a polynomial-time approximation scheme based on a novel adaptation of the m-guillotine method.",
        "published": "2003-09-09T20:16:30Z",
        "link": "http://arxiv.org/abs/cs/0309014v2",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Using Simulated Annealing to Calculate the Trembles of Trembling Hand   Perfection",
        "authors": [
            "Stuart McDonald",
            "Liam Wagner"
        ],
        "summary": "Within the literature on non-cooperative game theory, there have been a number of attempts to propose logorithms which will compute Nash equilibria. Rather than derive a new algorithm, this paper shows that the family of algorithms known as Markov chain Monte Carlo (MCMC) can be used to calculate Nash equilibria. MCMC is a type of Monte Carlo simulation that relies on Markov chains to ensure its regularity conditions. MCMC has been widely used throughout the statistics and optimization literature, where variants of this algorithm are known as simulated annealing. This paper shows that there is interesting connection between the trembles that underlie the functioning of this algorithm and the type of Nash refinement known as trembling hand perfection.",
        "published": "2003-09-10T15:11:44Z",
        "link": "http://arxiv.org/abs/cs/0309016v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DS",
            "cs.LG",
            "cs.NE",
            "q-bio.PE",
            "F.1.1;F.2.2;G.3;I.2.1;J.4"
        ]
    },
    {
        "title": "Efficient Algorithms for Citation Network Analysis",
        "authors": [
            "Vladimir Batagelj"
        ],
        "summary": "In the paper very efficient, linear in number of arcs, algorithms for determining Hummon and Doreian's arc weights SPLC and SPNP in citation network are proposed, and some theoretical properties of these weights are presented. The nonacyclicity problem in citation networks is discussed. An approach to identify on the basis of arc weights an important small subnetwork is proposed and illustrated on the citation networks of SOM (self organizing maps) literature and US patents.",
        "published": "2003-09-14T03:53:49Z",
        "link": "http://arxiv.org/abs/cs/0309023v1",
        "categories": [
            "cs.DL",
            "cs.DM",
            "cs.DS",
            "E.1; G.2.2; H.3.7"
        ]
    },
    {
        "title": "An Algorithm for Optimal Partitioning of Data on an Interval",
        "authors": [
            "Brad Jackson",
            "Jeffrey D. Scargle",
            "David Barnes",
            "Sundararajan Arabhi",
            "Alina Alt",
            "Peter Gioumousis",
            "Elyus Gwin",
            "Paungkaew Sangtrakulcharoen",
            "Linda Tan",
            "Tun Tao Tsai"
        ],
        "summary": "Many signal processing problems can be solved by maximizing the fitness of a segmented model over all possible partitions of the data interval. This letter describes a simple but powerful algorithm that searches the exponentially large space of partitions of $N$ data points in time $O(N^2)$. The algorithm is guaranteed to find the exact global optimum, automatically determines the model order (the number of segments), has a convenient real-time mode, can be extended to higher dimensional data spaces, and solves a surprising variety of problems in signal detection and characterization, density estimation, cluster analysis and classification.",
        "published": "2003-09-17T18:27:00Z",
        "link": "http://arxiv.org/abs/math/0309285v2",
        "categories": [
            "math.NA",
            "astro-ph",
            "cs.CE",
            "cs.DS",
            "cs.IT",
            "math.CO",
            "math.IT",
            "65C60"
        ]
    },
    {
        "title": "Lower bounds for predecessor searching in the cell probe model",
        "authors": [
            "Pranab Sen",
            "S. Venkatesh"
        ],
        "summary": "We consider a fundamental problem in data structures, static predecessor searching: Given a subset S of size n from the universe [m], store S so that queries of the form \"What is the predecessor of x in S?\" can be answered efficiently. We study this problem in the cell probe model introduced by Yao. Recently, Beame and Fich obtained optimal bounds on the number of probes needed by any deterministic query scheme if the associated storage scheme uses only n^{O(1)} cells of word size (\\log m)^{O(1)} bits. We give a new lower bound proof for this problem that matches the bounds of Beame and Fich. Our lower bound proof has the following advantages: it works for randomised query schemes too, while Beame and Fich's proof works for deterministic query schemes only. It also extends to `quantum address-only' query schemes that we define in this paper, and is simpler than Beame and Fich's proof. We prove our lower bound using the round elimination approach of Miltersen, Nisan, Safra and Wigderson. Using tools from information theory, we prove a strong round elimination lemma for communication complexity that enables us to obtain a tight lower bound for the predecessor problem. Our strong round elimination lemma also extends to quantum communication complexity. We also use our round elimination lemma to obtain a rounds versus communication tradeoff for the `greater-than' problem, improving on the tradeoff in Miltersen et al. We believe that our round elimination lemma is of independent interest and should have other applications.",
        "published": "2003-09-17T19:14:05Z",
        "link": "http://arxiv.org/abs/cs/0309033v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "quant-ph",
            "E.1; E.4"
        ]
    },
    {
        "title": "Finding approximate palindromes in strings",
        "authors": [
            "A. H. L. Porto",
            "V. C. Barbosa"
        ],
        "summary": "We introduce a novel definition of approximate palindromes in strings, and provide an algorithm to find all maximal approximate palindromes in a string with up to $k$ errors. Our definition is based on the usual edit operations of approximate pattern matching, and the algorithm we give, for a string of size $n$ on a fixed alphabet, runs in $O(k^2 n)$ time. We also discuss two implementation-related improvements to the algorithm, and demonstrate their efficacy in practice by means of both experiments and an average-case analysis.",
        "published": "2003-09-23T13:45:48Z",
        "link": "http://arxiv.org/abs/cs/0309043v1",
        "categories": [
            "cs.DS",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "The Wake Up and Report Problem is Time-Equivalent to the Firing Squad   Synchronization Problem",
        "authors": [
            "Darin Goldstein",
            "Nick Meyer"
        ],
        "summary": "We consider several problems relating to strongly-connected directed networks of identical finite-state processors that work synchronously in discrete time steps. The conceptually simplest of these is the Wake Up and Report Problem; this is the problem of having a unique \"root\" processor send a signal to all other processors in the network and then enter a special \"done\" state only when all other processors have received the signal. The most difficult of the problems we consider is the classic Firing Squad Synchronization Problem; this is the much-studied problem of achieving macro-synchronization in a network given micro-synchronization. We show via a complex algorithmic application of the \"snake\" data structure first introduced in Even, Litman, and Winkler [ELW], that these two problems are asymptotically time-equivalent up to a constant factor. This result leads immediately to the inclusion of several other related problems into this new asymptotic time-class.",
        "published": "2003-10-05T18:27:22Z",
        "link": "http://arxiv.org/abs/cs/0310003v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "E.1; C.2.1; C.2.2"
        ]
    },
    {
        "title": "Determination of the Topology of a Directed Network",
        "authors": [
            "Darin Goldstein"
        ],
        "summary": "We consider strongly-connected directed networks of identical synchronous, finite-state processors with in- and out-degree uniformly bounded by a network constant. Via a straightforward extension of Ostrovsky and Wilkerson's Backwards Communication Algorithm in [OW], we exhibit a protocol which solves the Global Topology Determination Problem, the problem of having the root processor map the global topology of a network of unknown size and topology, with running time O(ND) where N represents the number of processors and D represents the diameter of the network. A simple counting argument suffices to show that the Global Topology Determination Problem has time-complexity Omega(N logN) which makes the protocol presented asymptotically time-optimal for many large networks.",
        "published": "2003-10-05T18:41:16Z",
        "link": "http://arxiv.org/abs/cs/0310004v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "C.2.1; C.2.2; E.1"
        ]
    },
    {
        "title": "A hierarchical Algorithm to Solve the Shortest Path Problem in Valued   Graphs",
        "authors": [
            "Michel Koskas"
        ],
        "summary": "This paper details a new algorithm to solve the shortest path problem in valued graphs. Its complexity is $O(D \\log v)$ where $D$ is the graph diameter and $v$ its number of vertices. This complexity has to be compared to the one of the Dijkstra's algorithm, which is $O(e\\log v)$ where $e$ is the number of edges of the graph. This new algorithm lies on a hierarchical representation of the graph, using radix trees. The performances of this algorithm show a major improvement over the ones of the algorithms known up to now.",
        "published": "2003-10-10T18:01:25Z",
        "link": "http://arxiv.org/abs/cs/0310019v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.2.2; G.4"
        ]
    },
    {
        "title": "Smoothed Analysis of the Condition Numbers and Growth Factors of   Matrices",
        "authors": [
            "Arvind Sankar",
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "Let $\\orig{A}$ be any matrix and let $A$ be a slight random perturbation of $\\orig{A}$. We prove that it is unlikely that $A$ has large condition number. Using this result, we prove it is unlikely that $A$ has large growth factor under Gaussian elimination without pivoting. By combining these results, we bound the smoothed precision needed by Gaussian elimination without pivoting. Our results improve the average-case analysis of Gaussian elimination without pivoting performed by Yeung and Chan (SIAM J. Matrix Anal. Appl., 1997).",
        "published": "2003-10-12T04:06:09Z",
        "link": "http://arxiv.org/abs/cs/0310022v4",
        "categories": [
            "cs.NA",
            "cs.DS",
            "G.1.3"
        ]
    },
    {
        "title": "On the continuous Fermat-Weber problem",
        "authors": [
            "Sandor P. Fekete",
            "Joseph S. B. Mitchell",
            "Karin Beurer"
        ],
        "summary": "We give the first exact algorithmic study of facility location problems that deal with finding a median for a continuum of demand points. In particular, we consider versions of the ``continuous k-median (Fermat-Weber) problem'' where the goal is to select one or more center points that minimize the average distance to a set of points in a demand region. In such problems, the average is computed as an integral over the relevant region, versus the usual discrete sum of distances. The resulting facility location problems are inherently geometric, requiring analysis techniques of computational geometry. We provide polynomial-time algorithms for various versions of the L1 1-median (Fermat-Weber) problem. We also consider the multiple-center version of the L1 k-median problem, which we prove is NP-hard for large k.",
        "published": "2003-10-15T15:59:30Z",
        "link": "http://arxiv.org/abs/cs/0310027v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "A combinatorial characterization of higher-dimensional orthogonal   packing",
        "authors": [
            "Sandor P. Fekete",
            "Joerg Schepers"
        ],
        "summary": "Higher-dimensional orthogonal packing problems have a wide range of practical applications, including packing, cutting, and scheduling. Previous efforts for exact algorithms have been unable to avoid structural problems that appear for instances in two- or higher-dimensional space. We present a new approach for modeling packings, using a graph-theoretical characterization of feasible packings. Our characterization allows it to deal with classes of packings that share a certain combinatorial structure, instead of having to consider one packing at a time. In addition, we can make use of elegant algorithmic properties of certain classes of graphs. This allows our characterization to be the basis for a successful branch-and-bound framework.   This is the first in a series of papers describing new approaches to higher-dimensional packing.",
        "published": "2003-10-16T08:27:08Z",
        "link": "http://arxiv.org/abs/cs/0310032v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Minimizing the stabbing number of matchings, trees, and triangulations",
        "authors": [
            "Sandor P. Fekete",
            "Marco Luebbecke",
            "Henk Meijer"
        ],
        "summary": "The (axis-parallel) stabbing number of a given set of line segments is the maximum number of segments that can be intersected by any one (axis-parallel) line. This paper deals with finding perfect matchings, spanning trees, or triangulations of minimum stabbing number for a given set of points. The complexity of these problems has been a long-standing open question; in fact, it is one of the original 30 outstanding open problems in computational geometry on the list by Demaine, Mitchell, and O'Rourke. The answer we provide is negative for a number of minimum stabbing problems by showing them NP-hard by means of a general proof technique. It implies non-trivial lower bounds on the approximability. On the positive side we propose a cut-based integer programming formulation for minimizing the stabbing number of matchings and spanning trees. We obtain lower bounds (in polynomial time) from the corresponding linear programming relaxations, and show that an optimal fractional solution always contains an edge of at least constant weight. This result constitutes a crucial step towards a constant-factor approximation via an iterated rounding scheme. In computational experiments we demonstrate that our approach allows for actually solving problems with up to several hundred points optimally or near-optimally.",
        "published": "2003-10-16T14:01:32Z",
        "link": "http://arxiv.org/abs/cs/0310034v3",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Solving Sparse, Symmetric, Diagonally-Dominant Linear Systems in Time $O   (m^{1.31})$",
        "authors": [
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We present a linear-system solver that, given an $n$-by-$n$ symmetric positive semi-definite, diagonally dominant matrix $A$ with $m$ non-zero entries and an $n$-vector $\\bb $, produces a vector $\\xxt$ within relative distance $\\epsilon$ of the solution to $A \\xx = \\bb$ in time $O (m^{1.31} \\log (n \\kappa_{f} (A)/\\epsilon)^{O (1)})$, where $\\kappa_{f} (A)$ is the log of the ratio of the largest to smallest non-zero eigenvalue of $A$. In particular, $\\log (\\kappa_{f} (A)) = O (b \\log n)$, where $b$ is the logarithm of the ratio of the largest to smallest non-zero entry of $A$. If the graph of $A$ has genus $m^{2\\theta}$ or does not have a $K_{m^{\\theta}} $ minor, then the exponent of $m$ can be improved to the minimum of $1 + 5 \\theta $ and $(9/8) (1+\\theta)$. The key contribution of our work is an extension of Vaidya's techniques for constructing and analyzing combinatorial preconditioners.",
        "published": "2003-10-17T15:43:01Z",
        "link": "http://arxiv.org/abs/cs/0310036v2",
        "categories": [
            "cs.DS",
            "cs.NA",
            "F.2.1; G.1.3"
        ]
    },
    {
        "title": "Maximum dispersion and geometric maximum weight cliques",
        "authors": [
            "Sandor P. Fekete",
            "Henk Meijer"
        ],
        "summary": "We consider a facility location problem, where the objective is to ``disperse'' a number of facilities, i.e., select a given number k of locations from a discrete set of n candidates, such that the average distance between selected locations is maximized. In particular, we present algorithmic results for the case where vertices are represented by points in d-dimensional space, and edge weights correspond to rectilinear distances. Problems of this type have been considered before, with the best result being an approximation algorithm with performance ratio 2. For the case where k is fixed, we establish a linear-time algorithm that finds an optimal solution. For the case where k is part of the input, we present a polynomial-time approximation scheme.",
        "published": "2003-10-17T15:48:46Z",
        "link": "http://arxiv.org/abs/cs/0310037v1",
        "categories": [
            "cs.DS",
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "An O(m) Algorithm for Cores Decomposition of Networks",
        "authors": [
            "V. Batagelj",
            "M. Zaversnik"
        ],
        "summary": "The structure of large networks can be revealed by partitioning them to smaller parts, which are easier to handle. One of such decompositions is based on $k$--cores, proposed in 1983 by Seidman. In the paper an efficient, $O(m)$, $m$ is the number of lines, algorithm for determining the cores decomposition of a given network is presented.",
        "published": "2003-10-25T16:03:58Z",
        "link": "http://arxiv.org/abs/cs/0310049v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Finding Communities in Linear Time: A Physics Approach",
        "authors": [
            "Fang Wu",
            "Bernardo A. Huberman"
        ],
        "summary": "We present a method that allows for the discovery of communities within graphs of arbitrary size in times that scale linearly with their size. This method avoids edge cutting and is based on notions of voltage drops across networks that are both intuitive and easy to solve regardless of the complexity of the graph involved. We additionally show how this algorithm allows for the swift discovery of the community surrounding a given node without having to extract all the communities out of a graph.",
        "published": "2003-10-26T20:37:21Z",
        "link": "http://arxiv.org/abs/cond-mat/0310600v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS"
        ]
    },
    {
        "title": "Nearly-Linear Time Algorithms for Graph Partitioning, Graph   Sparsification, and Solving Linear Systems",
        "authors": [
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "This paper has been divided into three papers. arXiv:0809.3232, arXiv:0808.4134, arXiv:cs/0607105",
        "published": "2003-10-28T09:23:25Z",
        "link": "http://arxiv.org/abs/cs/0310051v10",
        "categories": [
            "cs.DS",
            "cs.NA",
            "F.2.1; G.1.3"
        ]
    },
    {
        "title": "Maintaining Information in Fully-Dynamic Trees with Top Trees",
        "authors": [
            "Stephen Alstrup",
            "Jacob Holm",
            "Kristian de Lichtenberg",
            "Mikkel Thorup"
        ],
        "summary": "We introduce top trees as a design of a new simpler interface for data structures maintaining information in a fully-dynamic forest. We demonstrate how easy and versatile they are to use on a host of different applications. For example, we show how to maintain the diameter, center, and median of each tree in the forest. The forest can be updated by insertion and deletion of edges and by changes to vertex and edge weights. Each update is supported in O(log n) time, where n is the size of the tree(s) involved in the update. Also, we show how to support nearest common ancestor queries and level ancestor queries with respect to arbitrary roots in O(log n) time. Finally, with marked and unmarked vertices, we show how to compute distances to a nearest marked vertex. The later has applications to approximate nearest marked vertex in general graphs, and thereby to static optimization problems over shortest path metrics.   Technically speaking, top trees are easily implemented either with Frederickson's topology trees [Ambivalent Data Structures for Dynamic 2-Edge-Connectivity and k Smallest Spanning Trees, SIAM J. Comput. 26 (2) pp. 484-538, 1997] or with Sleator and Tarjan's dynamic trees [A Data Structure for Dynamic Trees. J. Comput. Syst. Sc. 26 (3) pp. 362-391, 1983]. However, we claim that the interface is simpler for many applications, and indeed our new bounds are quadratic improvements over previous bounds where they exist.",
        "published": "2003-10-31T18:37:47Z",
        "link": "http://arxiv.org/abs/cs/0310065v2",
        "categories": [
            "cs.DS",
            "E.1;F.2.2;G.2.2"
        ]
    },
    {
        "title": "Quantum walk algorithm for element distinctness",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We use quantum walks to construct a new quantum algorithm for element distinctness and its generalization. For element distinctness (the problem of finding two equal items among N given items), we get an O(N^{2/3}) query quantum algorithm. This improves the previous O(N^{3/4}) query quantum algorithm of Buhrman et.al. (quant-ph/0007016) and matches the lower bound by Shi (quant-ph/0112086). The algorithm also solves the generalization of element distinctness in which we have to find k equal items among N items. For this problem, we get an O(N^{k/(k+1)}) query quantum algorithm.",
        "published": "2003-11-01T02:27:48Z",
        "link": "http://arxiv.org/abs/quant-ph/0311001v9",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Ackermann Encoding, Bisimulations, and OBDDs",
        "authors": [
            "Carla Piazza",
            "Alberto Policriti"
        ],
        "summary": "We propose an alternative way to represent graphs via OBDDs based on the observation that a partition of the graph nodes allows sharing among the employed OBDDs. In the second part of the paper we present a method to compute at the same time the quotient w.r.t. the maximum bisimulation and the OBDD representation of a given graph. The proposed computation is based on an OBDD-rewriting of the notion of Ackermann encoding of hereditarily finite sets into natural numbers.",
        "published": "2003-11-16T19:30:28Z",
        "link": "http://arxiv.org/abs/cs/0311018v1",
        "categories": [
            "cs.LO",
            "cs.DS",
            "E.1; F.2.2; D.2.4"
        ]
    },
    {
        "title": "An Optimal Algorithm for the Maximum-Density Segment Problem",
        "authors": [
            "Kai-min Chung",
            "Hsueh-I Lu"
        ],
        "summary": "We address a fundamental problem arising from analysis of biomolecular sequences. The input consists of two numbers $w_{\\min}$ and $w_{\\max}$ and a sequence $S$ of $n$ number pairs $(a_i,w_i)$ with $w_i>0$. Let {\\em segment} $S(i,j)$ of $S$ be the consecutive subsequence of $S$ between indices $i$ and $j$. The {\\em density} of $S(i,j)$ is $d(i,j)=(a_i+a_{i+1}+...+a_j)/(w_i+w_{i+1}+...+w_j)$. The {\\em maximum-density segment problem} is to find a maximum-density segment over all segments $S(i,j)$ with $w_{\\min}\\leq w_i+w_{i+1}+...+w_j \\leq w_{\\max}$. The best previously known algorithm for the problem, due to Goldwasser, Kao, and Lu, runs in $O(n\\log(w_{\\max}-w_{\\min}+1))$ time. In the present paper, we solve the problem in O(n) time. Our approach bypasses the complicated {\\em right-skew decomposition}, introduced by Lin, Jiang, and Chao. As a result, our algorithm has the capability to process the input sequence in an online manner, which is an important feature for dealing with genome-scale sequences. Moreover, for a type of input sequences $S$ representable in $O(m)$ space, we show how to exploit the sparsity of $S$ and solve the maximum-density segment problem for $S$ in $O(m)$ time.",
        "published": "2003-11-17T09:37:57Z",
        "link": "http://arxiv.org/abs/cs/0311020v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "J.3; F.2.2; G.2.1; I.1.2"
        ]
    },
    {
        "title": "Set K-Cover Algorithms for Energy Efficient Monitoring in Wireless   Sensor Networks",
        "authors": [
            "Zoe Abrams",
            "Ashish Goel",
            "Serge Plotkin"
        ],
        "summary": "Wireless sensor networks (WSNs) are emerging as an effective means for environment monitoring. This paper investigates a strategy for energy efficient monitoring in WSNs that partitions the sensors into covers, and then activates the covers iteratively in a round-robin fashion. This approach takes advantage of the overlap created when many sensors monitor a single area. Our work builds upon previous work in \"Power Efficient Organization of Wireless Sensor Networks\" by Slijepcevic and Potkonjak, where the model is first formulated. We have designed three approximation algorithms for a variation of the SET K-COVER problem, where the objective is to partition the sensors into covers such that the number of covers that include an area, summed over all areas, is maximized. The first algorithm is randomized and partitions the sensors, in expectation, within a fraction 1 - 1/e (~.63) of the optimum. We present two other deterministic approximation algorithms. One is a distributed greedy algorithm with a 1/2 approximation ratio and the other is a centralized greedy algorithm with a 1 - 1/e approximation ratio. We show that it is NP-Complete to guarantee better than 15/16 of the optimal coverage, indicating that all three algorithms perform well with respect to the best approximation algorithm possible. Simulations indicate that in practice, the deterministic algorithms perform far above their worst case bounds, consistently covering more than 72% of what is covered by an optimum solution. Simulations also indicate that the increase in longevity is proportional to the amount of overlap amongst the sensors. The algorithms are fast, easy to use, and according to simulations, significantly increase the longevity of sensor networks. The randomized algorithm in particular seems quite practical.",
        "published": "2003-11-20T22:47:11Z",
        "link": "http://arxiv.org/abs/cs/0311030v1",
        "categories": [
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "Extremal Properties of Random Structures",
        "authors": [
            "E. Ben-Naim",
            "P. L. Krapivsky",
            "S. Redner"
        ],
        "summary": "The extremal characteristics of random structures, including trees, graphs, and networks, are discussed. A statistical physics approach is employed in which extremal properties are obtained through suitably defined rate equations. A variety of unusual time dependences and system-size dependences for basic extremal properties are obtained.",
        "published": "2003-11-24T19:01:26Z",
        "link": "http://arxiv.org/abs/cond-mat/0311552v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS",
            "math.PR"
        ]
    },
    {
        "title": "Constraint Optimization and Statistical Mechanics",
        "authors": [
            "Giorgio Parisi"
        ],
        "summary": "In these lectures I will present an introduction to the results that have been recently obtained in constraint optimization of random problems using statistical mechanics techniques. After presenting the general results, in order to simplify the presentation I will describe in details only the problems related to the coloring of a random graph.",
        "published": "2003-12-05T16:46:59Z",
        "link": "http://arxiv.org/abs/cs/0312011v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.DS",
            "G.3; G.2.1 G.3, G.2.1 G.3, G.2.1"
        ]
    },
    {
        "title": "Partitioning schemes for quicksort and quickselect",
        "authors": [
            "Krzysztof C. Kiwiel"
        ],
        "summary": "We introduce several modifications of the partitioning schemes used in Hoare's quicksort and quickselect algorithms, including ternary schemes which identify keys less or greater than the pivot. We give estimates for the numbers of swaps made by each scheme. Our computational experiments indicate that ternary schemes allow quickselect to identify all keys equal to the selected key at little additional cost.",
        "published": "2003-12-23T03:47:55Z",
        "link": "http://arxiv.org/abs/cs/0312054v1",
        "categories": [
            "cs.DS",
            "F.2.2; G3"
        ]
    },
    {
        "title": "Randomized selection with quintary partitions",
        "authors": [
            "Krzysztof C. Kiwiel"
        ],
        "summary": "We show that several versions of Floyd and Rivest's algorithm Select for finding the $k$th smallest of $n$ elements require at most $n+\\min\\{k,n-k\\}+o(n)$ comparisons on average and with high probability. This rectifies the analysis of Floyd and Rivest, and extends it to the case of nondistinct elements. Our computational results confirm that Select may be the best algorithm in practice.",
        "published": "2003-12-23T04:12:30Z",
        "link": "http://arxiv.org/abs/cs/0312055v1",
        "categories": [
            "cs.DS",
            "F.2.2; G3"
        ]
    },
    {
        "title": "Building an Open Language Archives Community on the OAI Foundation",
        "authors": [
            "Gary Simons",
            "Steven Bird"
        ],
        "summary": "The Open Language Archives Community (OLAC) is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources. The Dublin Core (DC) Element Set and the OAI Protocol have provided a solid foundation for the OLAC framework. However, we need more precision in community-specific aspects of resource description than is offered by DC. Furthermore, many of the institutions and individuals who might participate in OLAC do not have the technical resources to support the OAI protocol. This paper presents our solutions to these two problems. To address the first, we have developed an extensible application profile for language resource metadata. To address the second, we have implemented Vida (the virtual data provider) and Viser (the virtual service provider), which permit community members to provide data and services without having to implement the OAI protocol. These solutions are generic and could be adopted by other specialized subcommunities.",
        "published": "2003-02-14T07:11:50Z",
        "link": "http://arxiv.org/abs/cs/0302021v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Preserving Peer Replicas By Rate-Limited Sampled Voting in LOCKSS",
        "authors": [
            "Petros Maniatis",
            "Mema Roussopoulos",
            "TJ Giuli",
            "David S. H. Rosenthal",
            "Mary Baker",
            "Yanto Muliadi"
        ],
        "summary": "The LOCKSS project has developed and deployed in a world-wide test a peer-to-peer system for preserving access to journals and other archival information published on the Web. It consists of a large number of independent, low-cost, persistent web caches that cooperate to detect and repair damage to their content by voting in \"opinion polls.\" Based on this experience, we present a design for and simulations of a novel protocol for voting in systems of this kind. It incorporates rate limitation and intrusion detection to ensure that even some very powerful adversaries attacking over many years have only a small probability of causing irrecoverable damage before being detected.",
        "published": "2003-03-25T20:11:57Z",
        "link": "http://arxiv.org/abs/cs/0303026v3",
        "categories": [
            "cs.DC",
            "cs.DL",
            "C.2.4; H.3.7; D.4.5"
        ]
    },
    {
        "title": "A Digital Preservation Appliance Based on OpenBSD",
        "authors": [
            "David S. H. Rosenthal"
        ],
        "summary": "The LOCKSS program has developed and deployed in a world-wide test a system for preserving access to academic journals published on the Web. The fundamental problem for any digital preservation system is that it must be affordable for the long term. To reduce the cost of ownership, the LOCKSS system uses generic PC hardware, open source software, and peer-to-peer technology. It is packaged as a ``network appliance'', a single-function box that can be connected to the Internet, configured and left alone to do its job with minimal monitoring or administration. The first version of this system was based on a Linux boot floppy. After three years of testing it was replaced by a second version, based on OpenBSD and booting from CD-ROM.   We focus in this paper on the design, implementation and deployment of a network appliance based on an open source operating system. We provide an overview of the LOCKSS application and describe the experience of deploying and supporting its first version. We list the requirements we took from this to drive the design of the second version, describe how we satisfied them in the OpenBSD environment, and report on the initial",
        "published": "2003-03-30T18:46:46Z",
        "link": "http://arxiv.org/abs/cs/0303033v3",
        "categories": [
            "cs.DC",
            "cs.DL",
            "D.4.5"
        ]
    },
    {
        "title": "The NPC Framework for Building Information Dissemination Networks",
        "authors": [
            "Lukas C. Faulstich"
        ],
        "summary": "Numerous systems for dissemination, retrieval, and archiving of documents have been developed in the past. Those systems often focus on one of these aspects and are hard to extend and combine. Typically, the transmission protocols, query and filtering languages are fixed as well as the interfaces to other systems. We rather envisage the seamless establishment of networks among the providers, repositories and consumers of information, supporting information retrieval and dissemination while being highly interoperable and extensible.   We propose a framework with a single event-based mechanism that unifies document storage, retrieval, and dissemination. This framework offers complete openness with respect to document and metadata formats, transmission protocols, and filtering mechanisms. It specifies a high-level building kit, by which arbitrary processors for document streams can be incorporated to support the retrieval, transformation, aggregation and disaggregation of documents. Using the same kit, interfaces for different transmission protocols can be added easily to enable the communication with various information sources and information consumers.",
        "published": "2003-05-14T09:49:05Z",
        "link": "http://arxiv.org/abs/cs/0305010v2",
        "categories": [
            "cs.DL",
            "cs.NI",
            "H.3.4, H.3.7"
        ]
    },
    {
        "title": "Developing Open Data Models for Linguistic Field Data",
        "authors": [
            "Baden Hughes"
        ],
        "summary": "The UQ Flint Archive houses the field notes and elicitation recordings made by Elwyn Flint in the 1950's and 1960's during extensive linguistic survey work across Queensland, Australia.   The process of digitizing the contents of the UQ Flint Archive provides a number of interesting challenges in the context of EMELD. Firstly, all of the linguistic data is for languages which are either endangered or extinct, and as such forms a valuable ethnographic repository. Secondly, the physical format of the data is itself in danger of decline, and as such digitization is an important preservation task in the short to medium term. Thirdly, the adoption of open standards for the encoding and presentation of text and audio data for linguistic field data, whilst enabling preservation, represents a new field of research in itself where best practice has yet to be formalised. Fourthly, the provision of this linguistic data online as a new data source for future research introduces concerns of data portability and longevity.   This paper will outline the origins of the data model, the content creation components, presentation forms based on the data model, data capture tools and media conversion components. It will also address some of the larger questions regarding the digitization and annotation of linguistic field work based on experience gained through work with the Flint Archive contents.",
        "published": "2003-05-29T12:12:19Z",
        "link": "http://arxiv.org/abs/cs/0305053v1",
        "categories": [
            "cs.DL",
            "cs.CL",
            "H.3.7; H.2.4; H.2.1; E.2; D.2.11"
        ]
    },
    {
        "title": "The Open Language Archives Community: An infrastructure for distributed   archiving of language resources",
        "authors": [
            "Gary Simons",
            "Steven Bird"
        ],
        "summary": "New ways of documenting and describing language via electronic media coupled with new ways of distributing the results via the World-Wide Web offer a degree of access to language resources that is unparalleled in history. At the same time, the proliferation of approaches to using these new technologies is causing serious problems relating to resource discovery and resource creation. This article describes the infrastructure that the Open Language Archives Community (OLAC) has built in order to address these problems. Its technical and usage infrastructures address problems of resource discovery by constructing a single virtual library of distributed resources. Its governance infrastructure addresses problems of resource creation by providing a mechanism through which the language-resource community can express its consensus on recommended best practices.",
        "published": "2003-06-10T07:33:32Z",
        "link": "http://arxiv.org/abs/cs/0306040v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Eprints and the Open Archives Initiative",
        "authors": [
            "Simeon Warner"
        ],
        "summary": "The Open Archives Initiative (OAI) was created as a practical way to promote interoperability between eprint repositories. Although the scope of the OAI has been broadened, eprint repositories still represent a significant fraction of OAI data providers. In this article I present a brief survey of OAI eprint repositories, and of services using metadata harvested from eprint repositories using the OAI protocol for metadata harvesting (OAI-PMH). I then discuss several situations where metadata harvesting may be used to further improve the utility of eprint archives as a component of the scholarly communication infrastructure.",
        "published": "2003-07-04T03:33:49Z",
        "link": "http://arxiv.org/abs/cs/0307008v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Limit groups and groups acting freely on $\\bbR^n$-trees",
        "authors": [
            "Vincent Guirardel"
        ],
        "summary": "We give a simple proof of the finite presentation of Sela's limit groups by using free actions on $\\bbR^n$-trees. We first prove that Sela's limit groups do have a free action on an $\\bbR^n$-tree. We then prove that a finitely generated group having a free action on an $\\bbR^n$-tree can be obtained from free abelian groups and surface groups by a finite sequence of free products and amalgamations over cyclic groups. As a corollary, such a group is finitely presented, has a finite classifying space, its abelian subgroups are finitely generated and contains only finitely many conjugacy classes of non-cyclic maximal abelian subgroups.",
        "published": "2003-07-21T14:31:57Z",
        "link": "http://arxiv.org/abs/cs/0307049v1",
        "categories": [
            "cs.DL",
            "F.2.2"
        ]
    },
    {
        "title": "Extending Dublin Core Metadata to Support the Description and Discovery   of Language Resources",
        "authors": [
            "Steven Bird",
            "Gary Simons"
        ],
        "summary": "As language data and associated technologies proliferate and as the language resources community expands, it is becoming increasingly difficult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool works with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to find language resources. This paper reports on a new digital infrastructure for discovering language resources being developed by the Open Language Archives Community (OLAC). At the core of OLAC is its metadata format, which is designed to facilitate description and discovery of all kinds of language resources, including data, tools, or advice. The paper describes OLAC metadata, its relationship to Dublin Core metadata, and its dissemination using the metadata harvesting protocol of the Open Archives Initiative.",
        "published": "2003-08-14T23:28:21Z",
        "link": "http://arxiv.org/abs/cs/0308022v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "EqRank: A Self-Consistent Equivalence Relation on Graph Vertexes",
        "authors": [
            "Grigorii Pivovarov",
            "Sergei Trunov"
        ],
        "summary": "A new method of hierarchical clustering of graph vertexes is suggested. In the method, the graph partition is determined with an equivalence relation satisfying a recursive definition stating that vertexes are equivalent if the vertexes they point to (or vertexes pointing to them) are equivalent. Iterative application of the partitioning yields a hierarchical clustering of graph vertexes. The method is applied to the citation graph of hep-th. The outcome is a two-level classification scheme for the subject field presented in hep-th, and indexing of the papers from hep-th in this scheme. A number of tests show that the classification obtained is adequate.",
        "published": "2003-08-29T13:20:03Z",
        "link": "http://arxiv.org/abs/cs/0308044v1",
        "categories": [
            "cs.DS",
            "cs.DL",
            "H.3.3"
        ]
    },
    {
        "title": "Semi-metric Behavior in Document Networks and its Application to   Recommendation Systems",
        "authors": [
            "L. M. Rocha"
        ],
        "summary": "Recommendation systems for different Document Networks (DN) such as the World Wide Web (WWW) and Digital Libraries, often use distance functions extracted from relationships among documents and keywords. For instance, documents in the WWW are related via a hyperlink network, while documents in bibliographic databases are related by citation and collaboration networks. Furthermore, documents are related to keyterms. The distance functions computed from these relations establish associative networks among items of the DN, referred to as Distance Graphs, which allow recommendation systems to identify relevant associations for individual users. However, modern recommendation systems need to integrate associative data from multiple sources such as different databases, web sites, and even other users. Thus, we are presented with a problem of combining evidence (about associations between items) from different sources characterized by distance functions. In this paper we describe our work on (1) inferring relevant associations from, as well as characterizing, semi-metric distance graphs and (2) combining evidence from different distance graphs in a recommendation system. Regarding (1), we present the idea of semi-metric distance graphs, and introduce ratios to measure semi-metric behavior. We compute these ratios for several DN such as digital libraries and web sites and show that they are useful to identify implicit associations. Regarding (2), we describe an algorithm to combine evidence from distance graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed for a recommendation system named TalkMine.",
        "published": "2003-09-09T05:24:03Z",
        "link": "http://arxiv.org/abs/cs/0309013v1",
        "categories": [
            "cs.IR",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.MA",
            "H.3.0; H.3.3, H.3.4; H.3.6; H.3.7; I.2.11; H.3.5"
        ]
    },
    {
        "title": "Efficient Algorithms for Citation Network Analysis",
        "authors": [
            "Vladimir Batagelj"
        ],
        "summary": "In the paper very efficient, linear in number of arcs, algorithms for determining Hummon and Doreian's arc weights SPLC and SPNP in citation network are proposed, and some theoretical properties of these weights are presented. The nonacyclicity problem in citation networks is discussed. An approach to identify on the basis of arc weights an important small subnetwork is proposed and illustrated on the citation networks of SOM (self organizing maps) literature and US patents.",
        "published": "2003-09-14T03:53:49Z",
        "link": "http://arxiv.org/abs/cs/0309023v1",
        "categories": [
            "cs.DL",
            "cs.DM",
            "cs.DS",
            "E.1; G.2.2; H.3.7"
        ]
    },
    {
        "title": "A Dynamic Programming Algorithm for the Segmentation of Greek Texts",
        "authors": [
            "Pavlina Fragkou"
        ],
        "summary": "In this paper we introduce a dynamic programming algorithm to perform linear text segmentation by global minimization of a segmentation cost function which consists of: (a) within-segment word similarity and (b) prior information about segment length. The evaluation of the segmentation accuracy of the algorithm on a text collection consisting of Greek texts showed that the algorithm achieves high segmentation accuracy and appears to be very innovating and promissing.",
        "published": "2003-10-21T18:19:52Z",
        "link": "http://arxiv.org/abs/cs/0310041v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.3.1; H.3.3; H.3.7"
        ]
    },
    {
        "title": "On The Cost Distribution of a Memory Bound Function",
        "authors": [
            "David S. H. Rosenthal"
        ],
        "summary": "Memory Bound Functions have been proposed for fighting spam, resisting Sybil attacks and other purposes. A particular implementation of such functions has been proposed in which the average effort required to generate a proof of effort is set by parameters E and l to E * l. The distribution of effort required to generate an individual proof about this average is fairly broad. When particular uses of these functions are envisaged, the choice of E and l, and the system design surrounding the generation and verification of proofs of effort, need to take the breadth of the distribution into account.   We show the distribution for this implementation, discuss the system design issues in the context of two proposed applications, and suggest an improved implementation.",
        "published": "2003-11-06T18:54:01Z",
        "link": "http://arxiv.org/abs/cs/0311005v1",
        "categories": [
            "cs.CR",
            "cs.DL",
            "D.4.6"
        ]
    },
    {
        "title": "Make search become the internal function of Internet",
        "authors": [
            "Liang Wang",
            "Yiping Guo",
            "Ming Fang"
        ],
        "summary": "Domain Resource Integrated System (DRIS) is introduced in this paper. DRIS is a distributed information retrieval system, which will solve problems like poor coverage, long update interval in current web search system. The most distinct character of DRIS is that it's a public opening system, and acts as an internal component of Internet, but not the production of a company. The implementation of DRIS is also represented.",
        "published": "2003-11-14T09:53:51Z",
        "link": "http://arxiv.org/abs/cs/0311015v2",
        "categories": [
            "cs.IR",
            "cs.DL",
            "cs.NI",
            "H.3.3;H.3.5;H.3.7"
        ]
    },
    {
        "title": "Copyright and Creativity: Authors and Photographers",
        "authors": [
            "Douglas A. Galbi"
        ],
        "summary": "The history of the occupations \"author\" and \"photographer\" provides an insightful perspective on copyright and creativity. The concept of the romantic author, associated with personal creative genius, gained prominence in the eighteenth century. However, in the U.S. in 1900 only about three thousand persons professed their occupation to be \"author.\" Self-professed \"photographers\" were then about ten times as numerous as authors. Being a photographer was associated with manufacturing and depended only on mastering technical skills and making a living. Being an author, in contrast, was an elite status associated with science and literature. Across the twentieth century, the number of writers and authors grew much more rapidly than the number of photographers. The relative success of writers and authors in creating jobs seems to have depended not on differences in copyright or possibilities for self-production, but on greater occupational innovation. Creativity in organizing daily work is an important form of creativity.",
        "published": "2003-11-28T19:33:55Z",
        "link": "http://arxiv.org/abs/cs/0311054v1",
        "categories": [
            "cs.CY",
            "cs.DL",
            "K.5.1"
        ]
    },
    {
        "title": "Designing of a Community-based Translation Center",
        "authors": [
            "Kathleen McDevitt",
            "Manuel A. Perez-Quinones",
            "Olga I. Padilla-Falto"
        ],
        "summary": "Interfaces that support multi-lingual content can reach a broader community. We wish to extend the reach of CITIDEL, a digital library for computing education materials, to support multiple languages. By doing so, we hope that it will increase the number of users, and in turn the number of resources. This paper discusses three approaches to translation (automated translation, developer-based, and community-based), and a brief evaluation of these approaches. It proposes a design for an online community translation center where volunteers help translate interface components and educational materials available in CITIDEL.",
        "published": "2003-12-04T05:26:51Z",
        "link": "http://arxiv.org/abs/cs/0312010v1",
        "categories": [
            "cs.HC",
            "cs.DL",
            "H.5.2; H.3.7"
        ]
    },
    {
        "title": "Evolution: Google vs. DRIS",
        "authors": [
            "Wang Liang",
            "Guo Yiping",
            "Fang Ming"
        ],
        "summary": "This paper gives an absolute new search system that builds the information retrieval infrastructure for Internet. Now most search engine companies are mainly concerned with how to make profit from company users by advertisement and ranking prominence, but never consider what its real customers will feel. Few web search engines can sell billions dollars just at the cost of inconvenience of most Internet users, but not its high quality of search service. When we have to bear the bothersome advertisements in the awful results and have no choices, Internet as the kind of public good will surely be undermined. If current Internet can't fully ensure our right to know, it may need some sound improvements or a revolution.",
        "published": "2003-12-13T03:15:46Z",
        "link": "http://arxiv.org/abs/cs/0312024v1",
        "categories": [
            "cs.DL",
            "cs.IR",
            "cs.NI",
            "H.3.3;H.3.5;H.3.7"
        ]
    },
    {
        "title": "Using sensors in the web crawling process",
        "authors": [
            "Ilya Zemskov"
        ],
        "summary": "This paper offers a short description of an Internet information field monitoring system, which places a special module-sensor on the side of the Web-server to detect changes in information resources and subsequently reindexes only the resources signalized by the corresponding sensor. Concise results of simulation research and an implementation attempt of the given \"sensors\" concept are provided.",
        "published": "2003-12-17T11:30:56Z",
        "link": "http://arxiv.org/abs/cs/0312033v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.4; I.6.3"
        ]
    },
    {
        "title": "Subclassing errors, OOP, and practically checkable rules to prevent them",
        "authors": [
            "Oleg Kiselyov"
        ],
        "summary": "This paper considers an example of Object-Oriented Programming (OOP) leading to subtle errors that break separation of interface and implementations. A comprehensive principle that guards against such errors is undecidable. The paper introduces a set of mechanically verifiable rules that prevent these insidious problems. Although the rules seem restrictive, they are powerful and expressive, as we show on several familiar examples. The rules contradict both the spirit and the letter of the OOP. The present examples as well as available theoretical and experimental results pose a question if OOP is conducive to software development at all.",
        "published": "2003-01-28T19:41:41Z",
        "link": "http://arxiv.org/abs/cs/0301032v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "F.3.3; D.1.5; D.1.1; D.2.3; D.2.4; D.2.11"
        ]
    },
    {
        "title": "Experimental Software Schedulability Estimation For Varied Processor   Frequencies",
        "authors": [
            "Sampsa Fabritius",
            "Raimondas Lencevicius",
            "Edu Metz",
            "Alexander Ran"
        ],
        "summary": "This paper describes a new approach to experimentally estimate the application schedulability for various processor frequencies. We use additional workload generated by an artificial high priority routine to simulate the frequency decrease of a processor. Then we estimate the schedulability of applications at different frequencies. The results of such estimation can be used to determine the frequencies and control algorithms of dynamic voltage scaling/dynamic frequency scaling (DVS/DFS) implementations. The paper presents a general problem description, the proposed schedulability estimation method, its analysis and evaluation.",
        "published": "2003-02-24T22:03:30Z",
        "link": "http://arxiv.org/abs/cs/0302033v1",
        "categories": [
            "cs.SE",
            "cs.OS",
            "D.2.8;D.4.1"
        ]
    },
    {
        "title": "Techniques and Applications of Computation Slicing",
        "authors": [
            "Neeraj Mittal",
            "Vijay K. Garg"
        ],
        "summary": "Writing correct distributed programs is hard. In spite of extensive testing and debugging, software faults persist even in commercial grade software. Many distributed systems, especially those employed in safety-critical environments, should be able to operate properly even in the presence of software faults. Monitoring the execution of a distributed system, and, on detecting a fault, initiating the appropriate corrective action is an important way to tolerate such faults. This gives rise to the predicate detection problem which requires finding a consistent cut of a given computation that satisfies a given global predicate, if it exists.   Detecting a predicate in a computation is, however, an NP-complete problem. To ameliorate the associated combinatorial explosion problem, we introduce the notion of computation slice. Formally, the slice of a computation with respect to a predicate is a (sub)computation with the least number of consistent cuts that contains all consistent cuts of the computation satisfying the predicate. To detect a predicate, rather than searching the state-space of the computation, it is much more efficient to search the state-space of the slice.   We prove that the slice exists and is uniquely defined for all predicates. We present efficient slicing algorithms for several useful classes of predicates. We develop efficient heuristic algorithms for computing an approximate slice for predicates for which computing the slice is otherwise provably intractable. Our experimental results show that slicing can lead to an exponential improvement over existing techniques for predicate detection in terms of time and space.",
        "published": "2003-03-15T07:41:07Z",
        "link": "http://arxiv.org/abs/cs/0303010v1",
        "categories": [
            "cs.DC",
            "cs.SE",
            "C.2.4; D.4.5; D.2.2"
        ]
    },
    {
        "title": "Extending the code generation capabilities of the Together CASE tool to   support Data Definition languages",
        "authors": [
            "Massimo Marino"
        ],
        "summary": "Together is the recommended software development tool in the Atlas collaboration. The programmatic API, which provides the capability to use and augment Together's internal functionality, is comprised of three major components - IDE, RWI and SCI. IDE is a read-only interface used to generate custom outputs based on the information contained in a Together model. RWI allows to both extract and write information to a Together model. SCI is the Source Code Interface, as the name implies it allows to work at the level of the source code. Together is extended by writing modules (java classes) extensively making use of the relevant API. We exploited Together extensibility to add support for the Atlas Dictionary Language. ADL is an extended subset of OMG IDL. The implemented module (ADLModule) makes Together to support ADL keywords, enables options and generate ADL object descriptions directly from UML Class diagrams. The module thoroughly accesses a Together reverse engineered C++ project - and/or design only class diagrams - and it is general enough to allow for possibly additional HEP-specific Together tool tailoring.",
        "published": "2003-03-18T15:51:06Z",
        "link": "http://arxiv.org/abs/cs/0303013v2",
        "categories": [
            "cs.SE",
            "D.2.2"
        ]
    },
    {
        "title": "Numerical Coverage Estimation for the Symbolic Simulation of Real-Time   Systems",
        "authors": [
            "Farn Wang",
            "Geng-Dian Hwang",
            "Fang Yu"
        ],
        "summary": "Three numerical coverage metrics for the symbolic simulation of dense-time systems and their estimation methods are presented. Special techniques to derive numerical estimations of dense-time state-spaces have also been developed. Properties of the metrics are also discussed with respect to four criteria. Implementation and experiments are then reported.",
        "published": "2003-03-25T21:57:50Z",
        "link": "http://arxiv.org/abs/cs/0303027v1",
        "categories": [
            "cs.SE",
            "cs.SC",
            "B.1.2"
        ]
    },
    {
        "title": "Power Law Distributions in Class Relationships",
        "authors": [
            "Richard Wheeldon",
            "Steve Counsell"
        ],
        "summary": "Power law distributions have been found in many natural and social phenomena, and more recently in the source code and run-time characteristics of Object-Oriented (OO) systems. A power law implies that small values are extremely common, whereas large values are extremely rare. In this paper, we identify twelve new power laws relating to the static graph structures of Java programs. The graph structures analyzed represented different forms of OO coupling, namely, inheritance, aggregation, interface, parameter type and return type. Identification of these new laws provide the basis for predicting likely features of classes in future developments. The research in this paper ties together work in object-based coupling and World Wide Web structures.",
        "published": "2003-05-20T10:21:17Z",
        "link": "http://arxiv.org/abs/cs/0305037v2",
        "categories": [
            "cs.SE",
            "D.1.5;D.2.8;D.3.3;K.6.2"
        ]
    },
    {
        "title": "Software systems as complex networks: structure, function, and   evolvability of software collaboration graphs",
        "authors": [
            "C. R. Myers"
        ],
        "summary": "Software systems emerge from mere keystrokes to form intricate functional networks connecting many collaborating modules, objects, classes, methods, and subroutines. Building on recent advances in the study of complex networks, I have examined software collaboration graphs contained within several open-source software systems, and have found them to reveal scale-free, small-world networks similar to those identified in other technological, sociological, and biological systems. I present several measures of these network topologies, and discuss their relationship to software engineering practices. I also present a simple model of software system evolution based on refactoring processes which captures some of the salient features of the observed systems. Some implications of object-oriented design for questions about network robustness, evolvability, degeneracy, and organization are discussed in the wake of these findings.",
        "published": "2003-05-23T20:29:17Z",
        "link": "http://arxiv.org/abs/cond-mat/0305575v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.SE"
        ]
    },
    {
        "title": "The Athena Data Dictionary and Description Language",
        "authors": [
            "Alain Bazan",
            "Thierry Bouedo",
            "Philippe Ghez",
            "Massimo Marino",
            "Craig Tull"
        ],
        "summary": "Athena is the ATLAS off-line software framework, based upon the GAUDI architecture from LHCb. As part of ATLAS' continuing efforts to enhance and customise the architecture to meet our needs, we have developed a data object description tool suite and service for Athena. The aim is to provide a set of tools to describe, manage, integrate and use the Event Data Model at a design level according to the concepts of the Athena framework (use of patterns, relationships, ...). Moreover, to ensure stability and reusability this must be fully independent from the implementation details. After an extensive investigation into the many options, we have developed a language grammar based upon a description language (IDL, ODL) to provide support for object integration in Athena. We have then developed a compiler front end based upon this language grammar, JavaCC, and a Java Reflection API-like interface. We have then used these tools to develop several compiler back ends which meet specific needs in ATLAS such as automatic generation of object converters, and data object scripting interfaces. We present here details of our work and experience to date on the Athena Definition Language and Athena Data Dictionary.",
        "published": "2003-05-28T15:16:08Z",
        "link": "http://arxiv.org/abs/cs/0305049v1",
        "categories": [
            "cs.SE",
            "D.2.2"
        ]
    },
    {
        "title": "The Virtual Monte Carlo",
        "authors": [
            "I. Hrivnacova",
            "D. Adamova",
            "V. Berejnoi",
            "R. Brun",
            "F. Carminati",
            "A. Fasso",
            "E. Futo",
            "A. Gheata",
            "I. Gonzalez Caballero",
            "A. Morsch"
        ],
        "summary": "The concept of Virtual Monte Carlo (VMC) has been developed by the ALICE Software Project to allow different Monte Carlo simulation programs to run without changing the user code, such as the geometry definition, the detector response simulation or input and output formats. Recently, the VMC classes have been integrated into the ROOT framework, and the other relevant packages have been separated from the AliRoot framework and can be used individually by any other HEP project. The general concept of the VMC and its set of base classes provided in ROOT will be presented. Existing implementations for Geant3, Geant4 and FLUKA and simple examples of usage will be described.",
        "published": "2003-05-30T21:59:14Z",
        "link": "http://arxiv.org/abs/cs/0306005v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "An Abstract Programming System",
        "authors": [
            "David A. Plaisted"
        ],
        "summary": "The system PL permits the translation of abstract proofs of program correctness into programs in a variety of programming languages. A programming language satisfying certain axioms may be the target of such a translation. The system PL also permits the construction and proof of correctness of programs in an abstract programming language, and permits the translation of these programs into correct programs in a variety of languages. The abstract programming language has an imperative style of programming with assignment statements and side-effects, to allow the efficient generation of code. The abstract programs may be written by humans and then translated, avoiding the need to write the same program repeatedly in different languages or even the same language. This system uses classical logic, is conceptually simple, and permits reasoning about nonterminating programs using Scott-Strachey style denotational semantics.",
        "published": "2003-06-05T18:31:14Z",
        "link": "http://arxiv.org/abs/cs/0306028v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.1.2"
        ]
    },
    {
        "title": "IGUANA Architecture, Framework and Toolkit for Interactive Graphics",
        "authors": [
            "George Alverson",
            "Giulio Eulisse",
            "Shahzad Muzaffar",
            "Ianna Osborne",
            "Lassi A. Tuura",
            "Lucas Taylor"
        ],
        "summary": "IGUANA is a generic interactive visualisation framework based on a C++ component model. It provides powerful user interface and visualisation primitives in a way that is not tied to any particular physics experiment or detector design. The article describes interactive visualisation tools built using IGUANA for the CMS and D0 experiments, as well as generic GEANT4 and GEANT3 applications. It covers features of the graphical user interfaces, 3D and 2D graphics, high-quality vector graphics output for print media, various textual, tabular and hierarchical data views, and integration with the application through control panels, a command line and different multi-threading models.",
        "published": "2003-06-10T16:15:41Z",
        "link": "http://arxiv.org/abs/cs/0306042v1",
        "categories": [
            "cs.SE",
            "cs.GR",
            "D.2.11;I.3.8;J.2"
        ]
    },
    {
        "title": "Concrete uses of XML in software development and data analysis",
        "authors": [
            "S. Patton"
        ],
        "summary": "XML is now becoming an industry standard for data description and exchange. Despite this there are still some questions about how or if this technology can be useful in High Energy Physics software development and data analysis. This paper aims to answer these questions by demonstrating how XML is used in the IceCube software development system, data handling and analysis. It does this by first surveying the concepts and tools that make up the XML technology. It then goes on to discuss concrete examples of how these concepts and tools are used to speed up software development in IceCube and what are the benefits of using XML in IceCube's data handling and analysis chain. The overall aim of this paper it to show that XML does have many benefits to bring High Energy Physics software development and data analysis.",
        "published": "2003-06-11T19:59:16Z",
        "link": "http://arxiv.org/abs/cs/0306047v1",
        "categories": [
            "cs.SE",
            "D.2.0"
        ]
    },
    {
        "title": "OVAL: the CMS Testing Robot",
        "authors": [
            "D. Chamont",
            "C. Charlot"
        ],
        "summary": "Oval is a testing tool which help developers to detect unexpected changes in the behavior of their software. It is able to automatically compile some test programs, to prepare on the fly the needed configuration files, to run the tests within a specified Unix environment, and finally to analyze the output and check expectations. Oval does not provide utility code to help writing the tests, therefore it is quite independant of the programming/scripting language of the software to be tested. It can be seen as a kind of robot which apply the tests and warn about any unexpected change in the output. Oval was developed by the LLR laboratory for the needs of the CMS experiment, and it is now recommended by the CERN LCG project.",
        "published": "2003-06-12T17:08:34Z",
        "link": "http://arxiv.org/abs/cs/0306054v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "IceCube's Development Environment",
        "authors": [
            "S. Patton",
            "D. Glowacki"
        ],
        "summary": "When the IceCube experiment started serious software development it needed a development environment in which both its developers and clients could work and that would encourage and support a good software development process. Some of the key features that IceCube wanted in such a environment were: the separation of the configuration and build tools; inclusion of an issue tracking system; support for the Unified Change Model; support for unit testing; and support for continuous building. No single, affordable, off the shelf, environment offered all these features. However there are many open source tools that address subsets of these feature, therefore IceCube set about selecting those tools which it could use in developing its own environment and adding its own tools where no suitable tools were found. This paper outlines the tools that where chosen, what are their responsibilities in the development environment and how they fit together. The complete environment will be demonstrated with a walk through of single cycle of the development process.",
        "published": "2003-06-12T17:31:32Z",
        "link": "http://arxiv.org/abs/cs/0306057v1",
        "categories": [
            "cs.SE",
            "D.2.6"
        ]
    },
    {
        "title": "FAYE: A Java Implement of the Frame/Stream/Stop Analysis Model",
        "authors": [
            "S. Patton"
        ],
        "summary": "FAYE, The Frame AnalYsis Executable, is a Java based implementation of the Frame/Stream/Stop model for analyzing data. Unlike traditional Event based analysis models, the Frame/Stream/Stop model has no preference as to which part of any data is to be analyzed, and an Event get as equal treatment as a change in the high voltage. This model means that FAYE is a suitable analysis framework for many different type of data analysis, such as detector trends or as a visualization core. During the design of FAYE the emphasis has been on clearly delineating each of the executable's responsibilities and on keeping their implementations as completely independent as possible. This leads to the large part of FAYE being a generic core which is experiment independent, and smaller section that customizes this core to an experiments own data structures. This customization can even be done in C++, using JNI, while the executable's control remains in Java. This paper reviews the Frame/Stream/Stop model and then looks at how FAYE has approached its implementation, with an emphasis on which responsibilities are handled by the generic core, and which parts an experiment must provide as part of the customization portion of the executable.",
        "published": "2003-06-13T21:36:06Z",
        "link": "http://arxiv.org/abs/cs/0306076v1",
        "categories": [
            "cs.SE",
            "J.2"
        ]
    },
    {
        "title": "BOA: Framework for Automated Builds",
        "authors": [
            "N. Ratnikova"
        ],
        "summary": "Managing large-scale software products is a complex software engineering task. The automation of the software development, release and distribution process is most beneficial in the large collaborations, where the big number of developers, multiple platforms and distributed environment are typical factors. This paper describes Build and Output Analyzer framework and its components that have been developed in CMS to facilitate software maintenance and improve software quality. The system allows to generate, control and analyze various types of automated software builds and tests, such as regular rebuilds of the development code, software integration for releases and installation of the existing versions.",
        "published": "2003-06-14T00:07:06Z",
        "link": "http://arxiv.org/abs/cs/0306080v1",
        "categories": [
            "cs.SE",
            "D.2.7; D.2.9"
        ]
    },
    {
        "title": "The Community Authorization Service: Status and Future",
        "authors": [
            "L. Pearlman",
            "V. Welch",
            "I. Foster",
            "C. Kesselman",
            "S. Tuecke"
        ],
        "summary": "Virtual organizations (VOs) are communities of resource providers and users distributed over multiple policy domains. These VOs often wish to define and enforce consistent policies in addition to the policies of their underlying domains. This is challenging, not only because of the problems in distributing the policy to the domains, but also because of the fact that those domains may each have different capabilities for enforcing the policy. The Community Authorization Service (CAS) solves this problem by allowing resource providers to delegate some policy authority to the VO while maintaining ultimate control over their resources. In this paper we describe CAS and our past and current implementations of CAS, and we discuss our plans for CAS-related research.",
        "published": "2003-06-14T01:16:51Z",
        "link": "http://arxiv.org/abs/cs/0306082v1",
        "categories": [
            "cs.SE",
            "C.2.4"
        ]
    },
    {
        "title": "GANGA: a user-Grid interface for Atlas and LHCb",
        "authors": [
            "K. Harrison",
            "W. T. L. P. Lavrijsen",
            "P. Mato",
            "A. Soroko",
            "C. L. Tan",
            "C. E. Tull",
            "N. Brook",
            "R. W. L. Jones"
        ],
        "summary": "The Gaudi/Athena and Grid Alliance (GANGA) is a front-end for the configuration, submission, monitoring, bookkeeping, output collection, and reporting of computing jobs run on a local batch system or on the grid. In particular, GANGA handles jobs that use applications written for the Gaudi software framework shared by the Atlas and LHCb experiments. GANGA exploits the commonality of Gaudi-based computing jobs, while insulating against grid-, batch- and framework-specific technicalities, to maximize end-user productivity in defining, configuring, and executing jobs. Designed for a python-based component architecture, GANGA has a modular underpinning and is therefore well placed for contributing to, and benefiting from, work in related projects. Its functionality is accessible both from a scriptable command-line interface, for expert users and automated tasks, and through a graphical interface, which simplifies the interaction with GANGA for beginning and c1asual users.   This paper presents the GANGA design and implementation, the development of the underlying software bus architecture, and the functionality of the first public GANGA release.",
        "published": "2003-06-14T02:57:32Z",
        "link": "http://arxiv.org/abs/cs/0306085v1",
        "categories": [
            "cs.SE",
            "D.2.6"
        ]
    },
    {
        "title": "The Athena Startup Kit",
        "authors": [
            "W. T. L. P. Lavrijsen"
        ],
        "summary": "The Athena Startup Kit (ASK), is an interactive front-end to the Atlas software framework (ATHENA). Written in python, a very effective \"glue\" language, it is build on top of the, in principle unrelated, code repository, build, configuration, debug, binding, and analysis tools. ASK automates many error-prone tasks that are otherwise left to the end-user, thereby pre-empting a whole category of potential problems. Through the existing tools, which ASK will setup for the user if and as needed, it locates available resources, maintains job coherency, manages the run-time environment, allows for interactivity and debugging, and provides standalone execution scripts. An end-user who wants to run her own analysis algorithms within the standard environment can let ASK generate the appropriate skeleton package, the needed dependencies and run-time, as well as a default job options script. For new and casual users, ASK comes with a graphical user interface; for advanced users, ASK has a scriptable command line interface. Both are built on top of the same set of libraries. ASK does not need to be, and isn't, experiment neutral. Thus it has built-in workarounds for known gotcha's, that would otherwise be a major time-sink for each and every new user. ASK minimizes the overhead for those physicists in Atlas who just want to write and run their analysis code.",
        "published": "2003-06-14T03:03:11Z",
        "link": "http://arxiv.org/abs/cs/0306083v1",
        "categories": [
            "cs.SE",
            "D.2.6"
        ]
    },
    {
        "title": "The StoreGate: a Data Model for the Atlas Software Architecture",
        "authors": [
            "P. Calafiura",
            "C. G. Leggett",
            "D. R. Quarrie",
            "H. Ma",
            "S. Rajagopalan"
        ],
        "summary": "The Atlas collaboration at CERN has adopted the Gaudi software architecture which belongs to the blackboard family: data objects produced by knowledge sources (e.g. reconstruction modules) are posted to a common in-memory data base from where other modules can access them and produce new data objects. The StoreGate has been designed, based on the Atlas requirements and the experience of other HENP systems such as Babar, CDF, CLEO, D0 and LHCB, to identify in a simple and efficient fashion (collections of) data objects based on their type and/or the modules which posted them to the Transient Data Store (the blackboard). The developer also has the freedom to use her preferred key class to uniquely identify a data object according to any other criterion. Besides this core functionality, the StoreGate provides the developers with a powerful interface to handle in a coherent fashion persistable references, object lifetimes, memory management and access control policy for the data objects in the Store. It also provides a Handle/Proxy mechanism to define and hide the cache fault mechanism: upon request, a missing Data Object can be transparently created and added to the Transient Store presumably retrieving it from a persistent data-base, or even reconstructing it on demand.",
        "published": "2003-06-14T06:56:24Z",
        "link": "http://arxiv.org/abs/cs/0306089v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "Making refactoring decisions in large-scale Java systems: an empirical   stance",
        "authors": [
            "Richard Wheeldon",
            "Steve Counsell"
        ],
        "summary": "Decisions on which classes to refactor are fraught with difficulty. The problem of identifying candidate classes becomes acute when confronted with large systems comprising hundreds or thousands of classes. In this paper, we describe a metric by which key classes, and hence candidates for refactoring, can be identified. Measures quantifying the usage of two forms of coupling, inheritance and aggregation, together with two other class features (number of methods and attributes) were extracted from the source code of three large Java systems. Our research shows that metrics from other research domains can be adapted to the software engineering process. Substantial differences were found between each of the systems in terms of the key classes identified and hence opportunities for refactoring those classes varied between those systems.",
        "published": "2003-06-16T12:19:18Z",
        "link": "http://arxiv.org/abs/cs/0306098v1",
        "categories": [
            "cs.SE",
            "D.2.8"
        ]
    },
    {
        "title": "ROOT Status and Future Developments",
        "authors": [
            "Fons Rademakers",
            "Masaharu Goto",
            "Philippe Canal",
            "Rene Brun"
        ],
        "summary": "In this talk we will review the major additions and improvements made to the ROOT system in the last 18 months and present our plans for future developments. The additons and improvements range from modifications to the I/O sub-system to allow users to save and restore objects of classes that have not been instrumented by special ROOT macros, to the addition of a geometry package designed for building, browsing, tracking and visualizing detector geometries. Other improvements include enhancements to the quick analysis sub-system (TTree::Draw()), the addition of classes that allow inter-file object references (TRef, TRefArray), better support for templated and STL classes, amelioration of the Automatic Script Compiler and the incorporation of new fitting and mathematical tools. Efforts have also been made to increase the modularity of the ROOT system with the introduction of more abstract interfaces and the development of a plug-in manager. In the near future, we intend to continue the development of PROOF and its interfacing with GRID environments. We plan on providing an interface between Geant3, Geant4 and Fluka and the new geometry package. The ROOT GUI classes will finally be available on Windows and we plan to release a GUI inspector and builder. In the last year, ROOT has drawn the endorsement of additional experiments and institutions. It is now officially supported by CERN and used as key I/O component by the LCG project.",
        "published": "2003-06-16T17:22:59Z",
        "link": "http://arxiv.org/abs/cs/0306078v1",
        "categories": [
            "cs.SE",
            "D.3.3"
        ]
    },
    {
        "title": "The DataFlow System of the ATLAS Trigger and DAQ",
        "authors": [
            "G. Lehmann"
        ],
        "summary": "The baseline design and implementation of the DataFlow system, to be documented in the ATLAS DAQ/HLT Technical Design Report in summer 2003, will be presented. Empahsis will be placed on the system performance and scalability based on the results from prototyping studies which have maximised the use of commercially available hardware.",
        "published": "2003-06-16T17:25:53Z",
        "link": "http://arxiv.org/abs/cs/0306101v1",
        "categories": [
            "cs.SE",
            "C.4"
        ]
    },
    {
        "title": "Web Engineering",
        "authors": [
            "Yogesh Deshpande",
            "San Murugesan",
            "Athula Ginige",
            "Steve Hansen",
            "Daniel Schwabe",
            "Martin Gaedke",
            "Bebo White"
        ],
        "summary": "Web Engineering is the application of systematic, disciplined and quantifiable approaches to development, operation, and maintenance of Web-based applications. It is both a pro-active approach and a growing collection of theoretical and empirical research in Web application development. This paper gives an overview of Web Engineering by addressing the questions: a) why is it needed? b) what is its domain of operation? c) how does it help and what should it do to improve Web application development? and d) how should it be incorporated in education and training? The paper discusses the significant differences that exist between Web applications and conventional software, the taxonomy of Web applications, the progress made so far and the research issues and experience of creating a specialisation at the master's level. The paper reaches a conclusion that Web Engineering at this stage is a moving target since Web technologies are constantly evolving, making new types of applications possible, which in turn may require innovations in how they are built, deployed and maintained.",
        "published": "2003-06-18T03:13:44Z",
        "link": "http://arxiv.org/abs/cs/0306108v1",
        "categories": [
            "cs.SE",
            "K.6"
        ]
    },
    {
        "title": "Bug propagation and debugging in asymmetric software structures",
        "authors": [
            "Damien Challet",
            "Andrea Lombardoni"
        ],
        "summary": "Software dependence networks are shown to be scale-free and asymmetric. We then study how software components are affected by the failure of one of them, and the inverse problem of locating the faulty component. Software at all levels is fragile with respect to the failure of a random single component. Locating a faulty component is easy if the failures only affect their nearest neighbors, while it is hard if the failures propagate further.",
        "published": "2003-06-19T18:05:17Z",
        "link": "http://arxiv.org/abs/cond-mat/0306509v5",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.SE"
        ]
    },
    {
        "title": "Closed source versus open source in a model of software bug dynamics",
        "authors": [
            "Damien Challet",
            "Yann Le Du"
        ],
        "summary": "We introduce a simple microscopic description of software bug dynamics where users, programmers and a maintainer interact through a given program, with a particular emphasis on bug creation, detection and fixing. When the program is written from scratch, the first phase of development is characterized by a fast decline of the number of bugs, followed by a slow phase where most bugs have been fixed, hence, are hard to find. Releasing immediately bug fixes speeds up the debugging process, which substantiates bazaar open-source methodology. We provide a mathematical analysis that supports our numerical simulations. Finally, we apply our model to Linux history and determine the existence of a lower bound to the quality of its programmers.",
        "published": "2003-06-19T18:27:07Z",
        "link": "http://arxiv.org/abs/cond-mat/0306511v5",
        "categories": [
            "cond-mat.stat-mech",
            "cs.SE"
        ]
    },
    {
        "title": "Punctuated Equilibrium in Software Evolution",
        "authors": [
            "A. A. Gorshenev",
            "Yu. M. Pis'mak"
        ],
        "summary": "The approach based on paradigm of self-organized criticality proposed for experimental investigation and theoretical modelling of software evolution. The dynamics of modifications studied for three free, open source programs Mozilla, Free-BSD and Emacs using the data from version control systems. Scaling laws typical for the self-organization criticality found. The model of software evolution presenting the natural selection principle is proposed. The results of numerical and analytical investigation of the model are presented. They are in a good agreement with the data collected for the real-world software.",
        "published": "2003-07-09T13:26:32Z",
        "link": "http://arxiv.org/abs/cond-mat/0307201v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.SE"
        ]
    },
    {
        "title": "Contributions to the Development and Improvement of a Regulatory and   Pre-Regulatory Digitally System for the Tools within Flexible Fabrication   Systems",
        "authors": [
            "Viorel Putz",
            "Mihai V. Putz"
        ],
        "summary": "The paper reports the obtained results for the projection and realization of a digitally system aiming to assist the equipment for a regulatory and pre-regulatory tools and holding tools within the flexible fabrication systems (FFS). Moreover, based on the present results, the same methodology can be applied for assisting tools from the point of view of their integrity and to wear compensation in the FFS framework.",
        "published": "2003-07-24T14:01:16Z",
        "link": "http://arxiv.org/abs/cs/0307054v1",
        "categories": [
            "cs.CE",
            "cs.SE",
            "D.2.2; J.2"
        ]
    },
    {
        "title": "Efficient Instrumentation for Performance Profiling",
        "authors": [
            "Edu Metz",
            "Raimondas Lencevicius"
        ],
        "summary": "Performance profiling consists of tracing a software system during execution and then analyzing the obtained traces. However, traces themselves affect the performance of the system distorting its execution. Therefore, there is a need to minimize the effect of the tracing on the underlying system's performance. To achieve this, the trace set needs to be optimized according to the performance profiling problem being solved. Our position is that such minimization can be achieved only by adding the software trace design and implementation to the overall software development process. In such a process, the performance analyst supplies the knowledge of performance measurement requirements, while the software developer supplies the knowledge of the software. Both of these are needed for an optimal trace placement.",
        "published": "2003-07-25T21:45:15Z",
        "link": "http://arxiv.org/abs/cs/0307058v1",
        "categories": [
            "cs.PF",
            "cs.SE",
            "C.4; D.2.8"
        ]
    },
    {
        "title": "Open source software and peer review",
        "authors": [
            "Sanatan Rai"
        ],
        "summary": "We compare the open source model of software development to peer review in academia.",
        "published": "2003-08-23T21:11:41Z",
        "link": "http://arxiv.org/abs/cs/0308040v2",
        "categories": [
            "cs.SE",
            "cs.CY",
            "K.7.0;D.2.9"
        ]
    },
    {
        "title": "Proceedings of the Fifth International Workshop on Automated Debugging   (AADEBUG 2003)",
        "authors": [
            "Michiel Ronsse",
            "Koen De Bosschere"
        ],
        "summary": "Over the past decades automated debugging has seen major achievements. However, as debugging is by necessity attached to particular programming paradigms, the results are scattered. To alleviate this problem, the Automated and Algorithmic Debugging workshop (AADEBUG for short) was organised in 1993 in Link\"oping (Sweden). As this workshop proved to be successful, subsequent workshops have been organised in 1995 (Saint-Malo, France), 1997 (again in Link\"oping, Sweden) and 2000 (Munich, Germany). In 2003, the workshop is organised in Ghent, Belgium, the proceedings of which you are reading right now.",
        "published": "2003-09-15T14:58:32Z",
        "link": "http://arxiv.org/abs/cs/0309027v4",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Instrumenting self-modifying code",
        "authors": [
            "J. Maebe",
            "K. De Bosschere"
        ],
        "summary": "Adding small code snippets at key points to existing code fragments is called instrumentation. It is an established technique to debug certain otherwise hard to solve faults, such as memory management issues and data races. Dynamic instrumentation can already be used to analyse code which is loaded or even generated at run time.With the advent of environments such as the Java Virtual Machine with optimizing Just-In-Time compilers, a new obstacle arises: self-modifying code. In order to instrument this kind of code correctly, one must be able to detect modifications and adapt the instrumentation code accordingly, preferably without incurring a high penalty speedwise. In this paper we propose an innovative technique that uses the hardware page protection mechanism of modern processors to detect such modifications. We also show how an instrumentor can adapt the instrumented version depending on the kind of modificiations as well as an experimental evaluation of said techniques.",
        "published": "2003-09-16T13:55:32Z",
        "link": "http://arxiv.org/abs/cs/0309029v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Model-Based Debugging using Multiple Abstract Models",
        "authors": [
            "Wolfgang Mayer",
            "Markus Stumptner"
        ],
        "summary": "This paper introduces an automatic debugging framework that relies on model-based reasoning techniques to locate faults in programs. In particular, model-based diagnosis, together with an abstract interpretation based conflict detection mechanism is used to derive diagnoses, which correspond to possible faults in programs. Design information and partial specifications are applied to guide a model revision process, which allows for automatic detection and correction of structural faults.",
        "published": "2003-09-17T03:49:05Z",
        "link": "http://arxiv.org/abs/cs/0309030v1",
        "categories": [
            "cs.SE",
            "cs.AI",
            "D.2.5"
        ]
    },
    {
        "title": "Timestamp Based Execution Control for C and Java Programs",
        "authors": [
            "Kazutaka Maruyama",
            "Minoru Terada"
        ],
        "summary": "Many programmers have had to deal with an overwritten variable resulting for example from an aliasing problem. The culprit is obviously the last write-access to that memory location before the manifestation of the bug. The usual technique for removing such bugs starts with the debugger by (1) finding the last write and (2) moving the control point of execution back to that time by re-executing the program from the beginning. We wish to automate this. Step (2) is easy if we can somehow mark the last write found in step (1) and control the execution-point to move it back to this time.   In this paper we propose a new concept, position, that is, a point in the program execution trace, as needed for step (2) above. The position enables debuggers to automate the control of program execution to support common debugging activities. We have implemented position in C by modifying GCC and in Java with a bytecode transformer. Measurements show that position can be provided with an acceptable amount of overhead.",
        "published": "2003-09-17T06:35:44Z",
        "link": "http://arxiv.org/abs/cs/0309031v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Towards declarative diagnosis of constraint programs over finite domains",
        "authors": [
            "Gerard Ferrand",
            "Willy Lesaint",
            "Alexandre Tessier"
        ],
        "summary": "The paper proposes a theoretical approach of the debugging of constraint programs based on a notion of explanation tree. The proposed approach is an attempt to adapt algorithmic debugging to constraint programming. In this theoretical framework for domain reduction, explanations are proof trees explaining value removals. These proof trees are defined by inductive definitions which express the removals of values as consequences of other value removals. Explanations may be considered as the essence of constraint programming. They are a declarative view of the computation trace. The diagnosis consists in locating an error in an explanation rooted by a symptom.",
        "published": "2003-09-17T12:42:58Z",
        "link": "http://arxiv.org/abs/cs/0309032v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Postmortem Object Type Identification",
        "authors": [
            "Bryan M. Cantrill"
        ],
        "summary": "This paper presents a novel technique for the automatic type identification of arbitrary memory objects from a memory dump. Our motivating application is debugging memory corruption problems in optimized, production systems -- a problem domain largely unserved by extant methodologies. We describe our algorithm as applicable to any typed language, and we discuss it with respect to the formidable obstacles posed by C. We describe the heuristics that we have developed to overcome these difficulties and achieve effective type identification on C-based systems. We further describe the implementation of our heuristics on one C-based system -- the Solaris operating system kernel -- and describe the extensions that we have added to the Solaris postmortem debugger to allow for postmortem type identification. We show that our implementation yields a sufficiently high rate of type identification to be useful for debugging memory corruption problems. Finally, we discuss some of the novel automated debugging mechanisms that can be layered upon postmortem type identification.",
        "published": "2003-09-22T00:52:59Z",
        "link": "http://arxiv.org/abs/cs/0309037v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Causes and Effects in Computer Programs",
        "authors": [
            "Andreas Zeller"
        ],
        "summary": "Debugging is commonly understood as finding and fixing the cause of a problem. But what does ``cause'' mean? How can we find causes? How can we prove that a cause is a cause--or even ``the'' cause? This paper defines common terms in debugging, highlights the principal techniques, their capabilities and limitations.",
        "published": "2003-09-24T12:51:18Z",
        "link": "http://arxiv.org/abs/cs/0309047v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "A mathematical framework for automated bug localization",
        "authors": [
            "Tsuyoshi Ohta",
            "Tadanori Mizuno"
        ],
        "summary": "In this paper, we propose a mathematical framework for automated bug localization. This framework can be briefly summarized as follows. A program execution can be represented as a rooted acyclic directed graph. We define an execution snapshot by a cut-set on the graph. A program state can be regarded as a conjunction of labels on edges in a cut-set. Then we argue that a debugging task is a pruning process of the execution graph by using cut-sets. A pruning algorithm, i.e., a debugging task, is also presented.",
        "published": "2003-09-30T01:19:11Z",
        "link": "http://arxiv.org/abs/cs/0309055v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "A Performance Analysis Tool for Nokia Mobile Phone Software",
        "authors": [
            "Edu Metz",
            "Raimondas Lencevicius"
        ],
        "summary": "Performance problems are often observed in embedded software systems. The reasons for poor performance are frequently not obvious. Bottlenecks can occur in any of the software components along the execution path. Therefore it is important to instrument and monitor the different components contributing to the runtime behavior of an embedded software system. Performance analysis tools can help locate performance bottlenecks in embedded software systems by monitoring the software's execution and producing easily understandable performance data. We maintain and further develop a tool for analyzing the performance of Nokia mobile phone software. The user can select among four performance analysis reports to be generated: average processor load, processor utilization, task execution time statistics, and task execution timeline. Each of these reports provides important information about where execution time is being spent. The demo will show how the tool helps to identify performance bottlenecks in Nokia mobile phone software and better understand areas of poor performance.",
        "published": "2003-10-03T21:01:49Z",
        "link": "http://arxiv.org/abs/cs/0310001v1",
        "categories": [
            "cs.SE",
            "cs.PF",
            "D.2.5"
        ]
    },
    {
        "title": "Event-based Program Analysis with DeWiz",
        "authors": [
            "Ch. Schaubschlaeger",
            "D. Kranzlmueller",
            "J. Volkert"
        ],
        "summary": "Due to the increased complexity of parallel and distributed programs, debugging of them is considered to be the most difficult and time consuming part of the software lifecycle. Tool support is hence a crucial necessity to hide complexity from the user. However, most existing tools seem inadequate as soon as the program under consideration exploits more than a few processors over a long execution time. This problem is addressed by the novel debugging tool DeWiz (Debugging Wizard), whose focus lies on scalability. DeWiz has a modular, scalable architecture, and uses the event graph model as a representation of the investigated program. DeWiz provides a set of modules, which can be combined to generate, analyze, and visualize event graph data. Within this processing pipeline the toolset tries to extract useful information, which is presented to the user at an arbitrary level of abstraction. Additionally, DeWiz is a framework, which can be used to easily implement arbitrary user-defined modules.",
        "published": "2003-10-06T11:51:15Z",
        "link": "http://arxiv.org/abs/cs/0310007v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Debugging Tool for Localizing Faulty Processes in Message Passing   Programs",
        "authors": [
            "Masao Okita",
            "Fumihiko Ino",
            "Kenichi Hagihara"
        ],
        "summary": "In message passing programs, once a process terminates with an unexpected error, the terminated process can propagate the error to the rest of processes through communication dependencies, resulting in a program failure. Therefore, to locate faults, developers must identify the group of processes involved in the original error and faulty processes that activate faults. This paper presents a novel debugging tool, named MPI-PreDebugger (MPI-PD), for localizing faulty processes in message passing programs. MPI-PD automatically distinguishes the original and the propagated errors by checking communication errors during program execution. If MPI-PD observes any communication errors, it backtraces communication dependencies and points out potential faulty processes in a timeline view. We also introduce three case studies, in which MPI-PD has been shown to play the key role in their debugging. From these studies, we believe that MPI-PD helps developers to locate faults and allows them to concentrate in correcting their programs.",
        "published": "2003-10-09T07:00:11Z",
        "link": "http://arxiv.org/abs/cs/0310015v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Debugging Backwards in Time",
        "authors": [
            "Bil Lewis"
        ],
        "summary": "By recording every state change in the run of a program, it is possible to present the programmer every bit of information that might be desired. Essentially, it becomes possible to debug the program by going ``backwards in time,'' vastly simplifying the process of debugging. An implementation of this idea, the ``Omniscient Debugger,'' is used to demonstrate its viability and has been used successfully on a number of large programs. Integration with an event analysis engine for searching and control is presented. Several small-scale user studies provide encouraging results. Finally performance issues and implementation are discussed along with possible optimizations.   This paper makes three contributions of interest: the concept and technique of ``going backwards in time,'' the GUI which presents a global view of the program state and has a formal notion of ``navigation through time,'' and the integration with an event analyzer.",
        "published": "2003-10-09T14:51:57Z",
        "link": "http://arxiv.org/abs/cs/0310016v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Availability Guarantee for Deterministic Replay Starting Points in   Real-Time Systems",
        "authors": [
            "Joel Huselius",
            "Henrik Thane",
            "Daniel Sundmark"
        ],
        "summary": "Cyclic debugging requires repeatable executions. As non-deterministic or real-time systems typically do not have the potential to provide this, special methods are required. One such method is replay, a process that requires monitoring of a running system and logging of the data produced by that monitoring. We shall discuss the process of preparing the replay, a part of the process that has not been very well described before.",
        "published": "2003-10-14T08:42:16Z",
        "link": "http://arxiv.org/abs/cs/0310024v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "A Monitoring Language for Run Time and Post-Mortem Behavior Analysis and   Visualization",
        "authors": [
            "Mikhail Auguston",
            "Clinton Jeffery",
            "Scott Underwood"
        ],
        "summary": "UFO is a new implementation of FORMAN, a declarative monitoring language, in which rules are compiled into execution monitors that run on a virtual machine supported by the Alamo monitor architecture.",
        "published": "2003-10-14T21:06:50Z",
        "link": "http://arxiv.org/abs/cs/0310025v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Generalized Systematic Debugging for Attribute Grammars",
        "authors": [
            "Akira Sasaki",
            "Masataka Sassa"
        ],
        "summary": "Attribute grammars (AGs) are known to be a useful formalism for semantic analysis and translation. However, debugging AGs is complex owing to inherent difficulties of AGs, such as recursive grammar structure and attribute dependency. In this paper, a new systematic method of debugging AGs is proposed. Our approach is, in principle, based on previously proposed algorithmic debugging of AGs, but is more general. This easily enables integration of various query-based systematic debugging methods, including the slice-based method. The proposed method has been implemented in Aki, a debugger for AG description. We evaluated our new approach experimentally using Aki, which demonstrates the usability of our debugging method.",
        "published": "2003-10-15T08:26:22Z",
        "link": "http://arxiv.org/abs/cs/0310026v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Automated Fault Localization Using Potential Invariants",
        "authors": [
            "Brock Pytlik",
            "Manos Renieris",
            "Shriram Krishnamurthi",
            "Steven P. Reiss"
        ],
        "summary": "We present a general method for fault localization based on abstracting over program traces, and a tool that implements the method using Ernst's notion of potential invariants. Our experiments so far have been unsatisfactory, suggesting that further research is needed before invariants can be used to locate faults.",
        "published": "2003-10-18T22:44:47Z",
        "link": "http://arxiv.org/abs/cs/0310040v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Rigorous design of tracers: an experiment for constraint logic   programming",
        "authors": [
            "Mireille Ducasse",
            "Ludovic Langevine",
            "Pierre Deransart"
        ],
        "summary": "In order to design and implement tracers, one must decide what exactly to trace and how to produce this trace. On the one hand, trace designs are too often guided by implementation concerns and are not as useful as they should be. On the other hand, an interesting trace which cannot be produced efficiently, is not very useful either. In this article we propose a methodology which helps to efficiently produce accurate traces. Firstly, design a formal specification of the trace model. Secondly, derive a prototype tracer from this specification. Thirdly, analyze the produced traces. Fourthly, implement an efficient tracer. Lastly, compare the traces of the two tracers. At each step, problems can be found. In that case one has to iterate the process. We have successfully applied the proposed methodology to the design and implementation of a real tracer for constraint logic programming which is able to efficiently generate information required to build interesting graphical views of executions.",
        "published": "2003-10-22T09:31:18Z",
        "link": "http://arxiv.org/abs/cs/0310042v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Managing Evolving Business Workflows through the Capture of Descriptive   Information",
        "authors": [
            "Sebastien Gaspard",
            "Florida Estrella",
            "Richard McClatchey",
            "Regis Dindeleux"
        ],
        "summary": "Business systems these days need to be agile to address the needs of a changing world. In particular the discipline of Enterprise Application Integration requires business process management to be highly reconfigurable with the ability to support dynamic workflows, inter-application integration and process reconfiguration. Basing EAI systems on model-resident or on a so-called description-driven approach enables aspects of flexibility, distribution, system evolution and integration to be addressed in a domain-independent manner. Such a system called CRISTAL is described in this paper with particular emphasis on its application to EAI problem domains. A practical example of the CRISTAL technology in the domain of manufacturing systems, called Agilium, is described to demonstrate the principles of model-driven system evolution and integration. The approach is compared to other model-driven development approaches such as the Model-Driven Architecture of the OMG and so-called Adaptive Object Models.",
        "published": "2003-10-24T13:31:34Z",
        "link": "http://arxiv.org/abs/cs/0310048v1",
        "categories": [
            "cs.SE",
            "cs.DB",
            "H3.4;K4.4"
        ]
    },
    {
        "title": "Modeling State in Software Debugging of VHDL-RTL Designs -- A   Model-Based Diagnosis Approach",
        "authors": [
            "Bernhard Peischl",
            "Franz Wotawa"
        ],
        "summary": "In this paper we outline an approach of applying model-based diagnosis to the field of automatic software debugging of hardware designs. We present our value-level model for debugging VHDL-RTL designs and show how to localize the erroneous component responsible for an observed misbehavior. Furthermore, we discuss an extension of our model that supports the debugging of sequential circuits, not only at a given point in time, but also allows for considering the temporal behavior of VHDL-RTL designs. The introduced model is capable of handling state inherently present in every sequential circuit. The principal applicability of the new model is outlined briefly and we use industrial-sized real world examples from the ISCAS'85 benchmark suite to discuss the scalability of our approach.",
        "published": "2003-11-03T19:29:31Z",
        "link": "http://arxiv.org/abs/cs/0311001v1",
        "categories": [
            "cs.AI",
            "cs.SE",
            "D. 2.5"
        ]
    },
    {
        "title": "DUCT: An Interactive Define-Use Chain Navigation Tool for Relative   Debugging",
        "authors": [
            "Aaron Searle",
            "John Gough",
            "David Abramson"
        ],
        "summary": "This paper describes an interactive tool that facilitates following define-use chains in large codes. The motivation for the work is to support relative debugging, where it is necessary to iteratively refine a set of asser-tions between different versions of a program. DUCT is novel because it exploits the Microsoft Intermediate Language (MSIL) that underpins the .NET Framework. Accordingly, it works on a wide range of programming languages without any modification. The paper describes the design and implementation of DUCT, and then illustrates its use with a small case study.",
        "published": "2003-11-25T01:38:12Z",
        "link": "http://arxiv.org/abs/cs/0311037v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Idempotent I/O for safe time travel",
        "authors": [
            "Zoltan Somogyi"
        ],
        "summary": "Debuggers for logic programming languages have traditionally had a capability most other debuggers did not: the ability to jump back to a previous state of the program, effectively travelling back in time in the history of the computation. This ``retry'' capability is very useful, allowing programmers to examine in detail a part of the computation that they previously stepped over. Unfortunately, it also creates a problem: while the debugger may be able to restore the previous values of variables, it cannot restore the part of the program's state that is affected by I/O operations. If the part of the computation being jumped back over performs I/O, then the program will perform these I/O operations twice, which will result in unwanted effects ranging from the benign (e.g. output appearing twice) to the fatal (e.g. trying to close an already closed file). We present a simple mechanism for ensuring that every I/O action called for by the program is executed at most once, even if the programmer asks the debugger to travel back in time from after the action to before the action. The overhead of this mechanism is low enough and can be controlled well enough to make it practical to use it to debug computations that do significant amounts of I/O.",
        "published": "2003-11-26T04:45:22Z",
        "link": "http://arxiv.org/abs/cs/0311040v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Nonextensive statistical mechanics and economics",
        "authors": [
            "Constantino Tsallis",
            "Celia Anteneodo",
            "Lisa Borland",
            "Roberto Osorio"
        ],
        "summary": "Ergodicity, this is to say, dynamics whose time averages coincide with ensemble averages, naturally leads to Boltzmann-Gibbs (BG) statistical mechanics, hence to standard thermodynamics. This formalism has been at the basis of an enormous success in describing, among others, the particular stationary state corresponding to thermal equilibrium. There are, however, vast classes of complex systems which accomodate quite badly, or even not at all, within the BG formalism. Such dynamical systems exhibit, in one way or another, nonergodic aspects. In order to be able to theoretically study at least some of these systems, a formalism was proposed 14 years ago, which is sometimes referred to as nonextensive statistical mechanics. We briefly introduce this formalism, its foundations and applications. Furthermore, we provide some bridging to important economical phenomena, such as option pricing, return and volume distributions observed in the financial markets, and the fascinating and ubiquitous concept of risk aversion. One may summarize the whole approach by saying that BG statistical mechanics is based on the entropy $S_{BG}=-k \\sum_i p_i \\ln p_i$, and typically provides {\\it exponential laws} for describing stationary states and basic time-dependent phenomena, while nonextensive statistical mechanics is instead based on the entropic form $S_q=k(1-\\sum_ip_i^q)/(q-1)$ (with $S_1=S_{BG}$), and typically provides, for the same type of description, (asymptotic) {\\it power laws}.",
        "published": "2003-01-16T21:45:23Z",
        "link": "http://arxiv.org/abs/cond-mat/0301307v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CE",
            "q-fin.ST"
        ]
    },
    {
        "title": "Novel Runtime Systems Support for Adaptive Compositional Modeling on the   Grid",
        "authors": [
            "Srinidhi Varadarajan",
            "Naren Ramakrishnan"
        ],
        "summary": "Grid infrastructures and computing environments have progressed significantly in the past few years. The vision of truly seamless Grid usage relies on runtime systems support that is cognizant of the operational issues underlying grid computations and, at the same time, is flexible enough to accommodate diverse application scenarios. This paper addresses the twin aspects of Grid infrastructure and application support through a novel combination of two computational technologies: Weaves - a source-language independent parallel runtime compositional framework that operates through reverse-analysis of compiled object files, and runtime recommender systems that aid in dynamic knowledge-based application composition. Domain-specific adaptivity is exploited through a novel compositional system that supports runtime recommendation of code modules and a sophisticated checkpointing and runtime migration solution that can be transparently deployed over Grid infrastructures. A core set of \"adaptivity schemas\" are provided as templates for adaptive composition of large-scale scientific computations. Implementation issues, motivating application contexts, and preliminary results are described.",
        "published": "2003-01-21T02:26:47Z",
        "link": "http://arxiv.org/abs/cs/0301018v1",
        "categories": [
            "cs.CE",
            "cs.DC",
            "D.4.1; I.6"
        ]
    },
    {
        "title": "Interest Rate Model Calibration Using Semidefinite Programming",
        "authors": [
            "Alexandre d'Aspremont"
        ],
        "summary": "We show that, for the purpose of pricing Swaptions, the Swap rate and the corresponding Forward rates can be considered lognormal under a single martingale measure. Swaptions can then be priced as options on a basket of lognormal assets and an approximation formula is derived for such options. This formula is centered around a Black-Scholes price with an appropriate volatility, plus a correction term that can be interpreted as the expected tracking error. The calibration problem can then be solved very efficiently using semidefinite programming.",
        "published": "2003-02-25T02:48:42Z",
        "link": "http://arxiv.org/abs/cs/0302034v2",
        "categories": [
            "cs.CE",
            "J.1"
        ]
    },
    {
        "title": "Risk-Management Methods for the Libor Market Model Using Semidefinite   Programming",
        "authors": [
            "Alexandre d'Aspremont"
        ],
        "summary": "When interest rate dynamics are described by the Libor Market Model as in BGM97, we show how some essential risk-management results can be obtained from the dual of the calibration program. In particular, if the objetive is to maximize another swaption's price, we show that the optimal dual variables describe a hedging portfolio in the sense of \\cite{Avel96}. In the general case, the local sensitivity of the covariance matrix to all market movement scenarios can be directly computed from the optimal dual solution. We also show how semidefinite programming can be used to manage the Gamma exposure of a portfolio.",
        "published": "2003-02-25T03:09:11Z",
        "link": "http://arxiv.org/abs/cs/0302035v2",
        "categories": [
            "cs.CE",
            "J.1"
        ]
    },
    {
        "title": "Multiplicative point process as a model of trading activity",
        "authors": [
            "Vygintas Gontis",
            "Bronislovas Kaulakys"
        ],
        "summary": "Signals consisting of a sequence of pulses show that inherent origin of the 1/f noise is a Brownian fluctuation of the average interevent time between subsequent pulses of the pulse sequence. In this paper we generalize the model of interevent time to reproduce a variety of self-affine time series exhibiting power spectral density S(f) scaling as a power of the frequency f. Furthermore, we analyze the relation between the power-law correlations and the origin of the power-law probability distribution of the signal intensity. We introduce a stochastic multiplicative model for the time intervals between point events and analyze the statistical properties of the signal analytically and numerically. Such model system exhibits power-law spectral density S(f)~1/f**beta for various values of beta, including beta=1/2, 1 and 3/2. Explicit expressions for the power spectra in the low frequency limit and for the distribution density of the interevent time are obtained. The counting statistics of the events is analyzed analytically and numerically, as well. The specific interest of our analysis is related with the financial markets, where long-range correlations of price fluctuations largely depend on the number of transactions. We analyze the spectral density and counting statistics of the number of transactions. The model reproduces spectral properties of the real markets and explains the mechanism of power-law distribution of trading activity. The study provides evidence that the statistical properties of the financial markets are enclosed in the statistics of the time interval between trades. A multiplicative point process serves as a consistent model generating this statistics.",
        "published": "2003-03-05T18:20:26Z",
        "link": "http://arxiv.org/abs/cond-mat/0303089v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CE",
            "math.SP",
            "nlin.AO",
            "nlin.CD",
            "q-fin.TR"
        ]
    },
    {
        "title": "A Bird's eye view of Matrix Distributed Processing",
        "authors": [
            "Massimo Di Pierro"
        ],
        "summary": "We present Matrix Distributed Processing, a C++ library for fast development of efficient parallel algorithms. MDP is based on MPI and consists of a collection of C++ classes and functions such as lattice, site and field. Once an algorithm is written using these components the algorithm is automatically parallel and no explicit call to communication functions is required. MDP is particularly suitable for implementing parallel solvers for multi-dimensional differential equations and mesh-like problems.",
        "published": "2003-03-28T20:47:50Z",
        "link": "http://arxiv.org/abs/cs/0303031v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.DM",
            "cs.MS",
            "hep-lat",
            "physics.comp-ph",
            "D.1.3; D.3.2;G.1;G.4;I.6.8"
        ]
    },
    {
        "title": "Causalities of the Taiwan Stock Market",
        "authors": [
            "Juhi-Lian Julian Ting"
        ],
        "summary": "Volatility, fitting with first order Landau expansion, stationarity, and causality of the Taiwan stock market (TAIEX) are investigated based on daily records. Instead of consensuses that consider stock market index change as a random time series we propose the market change as a dual time series consists of the index and the corresponding volume. Therefore, causalities between these two time series are investigated.",
        "published": "2003-04-06T02:15:44Z",
        "link": "http://arxiv.org/abs/cond-mat/0304132v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CE",
            "q-fin.ST"
        ]
    },
    {
        "title": "Stochastic Volatility in a Quantitative Model of Stock Market Returns",
        "authors": [
            "Gilles Daniel"
        ],
        "summary": "Standard quantitative models of the stock market predict a log-normal distribution for stock returns (Bachelier 1900, Osborne 1959), but it is recognised (Fama 1965) that empirical data, in comparison with a Gaussian, exhibit leptokurtosis (it has more probability mass in its tails and centre) and fat tails (probabilities of extreme events are underestimated). Different attempts to explain this departure from normality have coexisted. In particular, since one of the strong assumptions of the Gaussian model concerns the volatility, considered finite and constant, the new models were built on a non finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985) volatility. We investigate in this thesis a very recent model (Dragulescu et al. 2002) based on a Brownian motion process for the returns, and a stochastic mean-reverting process for the volatility. In this model, the forward Kolmogorov equation that governs the time evolution of returns is solved analytically. We test this new theory against different stock indexes (Dow Jones Industrial Average, Standard and Poor s and Footsie), over different periods (from 20 to 105 years). Our aim is to compare this model with the classical Gaussian and with a simple Neural Network, used as a benchmark. We perform the usual statistical tests on the kurtosis and tails of the expected distributions, paying particular attention to the outliers. As claimed by the authors, the new model outperforms the Gaussian for any time lag, but is artificially too complex for medium and low frequencies, where the Gaussian is preferable. Moreover this model is still rejected for high frequencies, at a 0.05 level of significance, due to the kurtosis, incorrectly handled.",
        "published": "2003-04-07T17:37:55Z",
        "link": "http://arxiv.org/abs/cs/0304009v1",
        "categories": [
            "cs.CE",
            "G3"
        ]
    },
    {
        "title": "Self-Replicating Machines in Continuous Space with Virtual Physics",
        "authors": [
            "Arnold Smith",
            "Peter Turney",
            "Robert Ewaschuk"
        ],
        "summary": "JohnnyVon is an implementation of self-replicating machines in continuous two-dimensional space. Two types of particles drift about in a virtual liquid. The particles are automata with discrete internal states but continuous external relationships. Their internal states are governed by finite state machines but their external relationships are governed by a simulated physics that includes Brownian motion, viscosity, and spring-like attractive and repulsive forces. The particles can be assembled into patterns that can encode arbitrary strings of bits. We demonstrate that, if an arbitrary \"seed\" pattern is put in a \"soup\" of separate individual particles, the pattern will replicate by assembling the individual particles into copies of itself. We also show that, given sufficient time, a soup of separate individual particles will eventually spontaneously form self-replicating patterns. We discuss the implications of JohnnyVon for research in nanotechnology, theoretical biology, and artificial life.",
        "published": "2003-04-15T19:33:45Z",
        "link": "http://arxiv.org/abs/cs/0304022v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "q-bio.PE",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "Using Dynamic Simulation in the Development of Construction Machinery",
        "authors": [
            "Reno Filla",
            "Jan-Ove Palmberg"
        ],
        "summary": "As in the car industry for quite some time, dynamic simulation of complete vehicles is being practiced more and more in the development of off-road machinery. However, specific questions arise due not only to company structure and size, but especially to the type of product. Tightly coupled, non-linear subsystems of different domains make prediction and optimisation of the complete system's dynamic behaviour a challenge. Furthermore, the demand for versatile machines leads to sometimes contradictory target requirements and can turn the design process into a hunt for the least painful compromise. This can be avoided by profound system knowledge, assisted by simulation-driven product development. This paper gives an overview of joint research into this issue by Volvo Wheel Loaders and Linkoping University on that matter, lists the results of a related literature review and introduces the term \"operateability\". Rather than giving detailed answers, the problem space for ongoing and future research is examined and possible solutions are sketched.",
        "published": "2003-05-19T20:51:50Z",
        "link": "http://arxiv.org/abs/cs/0305036v4",
        "categories": [
            "cs.CE",
            "I.6.3; I.6.5; J.6"
        ]
    },
    {
        "title": "Goodness-of-fit of the Heston model",
        "authors": [
            "Gilles Daniel"
        ],
        "summary": "An analytical formula for the probability distribution of stock-market returns, derived from the Heston model assuming a mean-reverting stochastic volatility, was recently proposed by Dragulescu and Yakovenko in Quantitative Finance 2002. While replicating their results, we found two significant weaknesses in their method to pre-process the data, which cast a shadow over the effective goodness-of-fit of the model. We propose a new method, more truly capturing the market, and perform a Kolmogorov-Smirnov test and a Chi Square test on the resulting probability distribution. The results raise some significant questions for large time lags -- 40 to 250 days -- where the smoothness of the data does not require such a complex model; nevertheless, we also provide some statistical evidence in favour of the Heston model for small time lags -- 1 and 5 days -- compared with the traditional Gaussian model assuming constant volatility.",
        "published": "2003-05-29T18:13:08Z",
        "link": "http://arxiv.org/abs/cs/0305055v1",
        "categories": [
            "cs.CE",
            "G3"
        ]
    },
    {
        "title": "Modelling Biochemical Operations on RNA Secondary Structures",
        "authors": [
            "Merce Llabres",
            "Francesc Rossello"
        ],
        "summary": "In this paper we model several simple biochemical operations on RNA molecules that modify their secondary structure by means of a suitable variation of Gro\\ss e-Rhode's Algebra Transformation Systems.",
        "published": "2003-06-03T08:48:54Z",
        "link": "http://arxiv.org/abs/cs/0306016v1",
        "categories": [
            "cs.CE",
            "q-bio",
            "J.3;F.4.2"
        ]
    },
    {
        "title": "A family of metrics on contact structures based on edge ideals",
        "authors": [
            "Mercé Llabrés",
            "Francesc Rosselló"
        ],
        "summary": "The measurement of the similarity of RNA secondary structures, and in general of contact structures, of a fixed length has several specific applications. For instance, it is used in the analysis of the ensemble of suboptimal secondary structures generated by a given algorithm on a given RNA sequence, and in the comparison of the secondary structures predicted by different algorithms on a given RNA molecule. It is also a useful tool in the quantitative study of sequence-structure maps. A way to measure this similarity is by means of metrics. In this paper we introduce a new class of metrics $d_{m}$, $m\\geq 3$, on the set of all contact structures of a fixed length, based on their representation by means of edge ideals in a polynomial ring. These metrics can be expressed in terms of Hilbert functions of monomial ideals, which allows the use of several public domain computer algebra systems to compute them. We study some abstract properties of these metrics, and we obtain explicit descriptions of them for $m=3,4$ on arbitrary contact structures and for $m=5,6$ on RNA secondary structures.",
        "published": "2003-06-16T11:54:05Z",
        "link": "http://arxiv.org/abs/cs/0306097v1",
        "categories": [
            "cs.DM",
            "cs.CE",
            "q-bio",
            "G.2.3; J.3"
        ]
    },
    {
        "title": "Design, implementation and deployment of the Saclay muon reconstruction   algorithms (Muonbox/y) in the Athena software framework of the ATLAS   experiment",
        "authors": [
            "Andrea Formica"
        ],
        "summary": "This paper gives an overview of a reconstruction algorithm for muon events in ATLAS experiment at CERN. After a short introduction on ATLAS Muon Spectrometer, we will describe the procedure performed by the algorithms Muonbox and Muonboy (last version) in order to achieve correctly the reconstruction task. These algorithms have been developed in Fortran language and are working in the official C++ framework Athena, as well as in stand alone mode. A description of the interaction between Muonboy and Athena will be given, together with the reconstruction performances (efficiency and momentum resolution) obtained with MonteCarlo data.",
        "published": "2003-06-17T07:01:43Z",
        "link": "http://arxiv.org/abs/cs/0306105v1",
        "categories": [
            "cs.CE",
            "J.2"
        ]
    },
    {
        "title": "Modeling Business",
        "authors": [
            "Valdis Vitolins",
            "Audris Kalnins"
        ],
        "summary": "Business concepts are studied using a metamodel-based approach, using UML 2.0. The Notation Independent Business concepts metamodel is introduced. The approach offers a mapping between different business modeling notations which could be used for bridging BM tools and boosting the MDA approach.",
        "published": "2003-07-17T15:41:13Z",
        "link": "http://arxiv.org/abs/cs/0307039v1",
        "categories": [
            "cs.CE",
            "I.6.5; J.1"
        ]
    },
    {
        "title": "Hamevol1.0: a C++ code for differential equations based on Runge-Kutta   algorithm. An application to matter enhanced neutrino oscillation",
        "authors": [
            "P. Aliani",
            "V. Antonelli",
            "M. Picariello",
            "Emilio Torrente-Lujan"
        ],
        "summary": "We present a C++ implementation of a fifth order semi-implicit Runge-Kutta algorithm for solving Ordinary Differential Equations. This algorithm can be used for studying many different problems and in particular it can be applied for computing the evolution of any system whose Hamiltonian is known. We consider in particular the problem of calculating the neutrino oscillation probabilities in presence of matter interactions. The time performance and the accuracy of this implementation is competitive with respect to the other analytical and numerical techniques used in literature. The algorithm design and the salient features of the code are presented and discussed and some explicit examples of code application are given.",
        "published": "2003-07-23T19:30:25Z",
        "link": "http://arxiv.org/abs/cs/0307053v1",
        "categories": [
            "cs.CE",
            "J.2"
        ]
    },
    {
        "title": "Contributions to the Development and Improvement of a Regulatory and   Pre-Regulatory Digitally System for the Tools within Flexible Fabrication   Systems",
        "authors": [
            "Viorel Putz",
            "Mihai V. Putz"
        ],
        "summary": "The paper reports the obtained results for the projection and realization of a digitally system aiming to assist the equipment for a regulatory and pre-regulatory tools and holding tools within the flexible fabrication systems (FFS). Moreover, based on the present results, the same methodology can be applied for assisting tools from the point of view of their integrity and to wear compensation in the FFS framework.",
        "published": "2003-07-24T14:01:16Z",
        "link": "http://arxiv.org/abs/cs/0307054v1",
        "categories": [
            "cs.CE",
            "cs.SE",
            "D.2.2; J.2"
        ]
    },
    {
        "title": "Boundary knot method for Laplace and biharmonic problems",
        "authors": [
            "W. Chen"
        ],
        "summary": "The boundary knot method (BKM) [1] is a meshless boundary-type radial basis function (RBF) collocation scheme, where the nonsingular general solution is used instead of fundamental solution to evaluate the homogeneous solution, while the dual reciprocity method (DRM) is employed to approximation of particular solution. Despite the fact that there are not nonsingular RBF general solutions available for Laplace and biharmonic problems, this study shows that the method can be successfully applied to these problems. The high-order general and fundamental solutions of Burger and Winkler equations are also first presented here.",
        "published": "2003-07-28T09:35:52Z",
        "link": "http://arxiv.org/abs/cs/0307061v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "G1.3; G1.8"
        ]
    },
    {
        "title": "Implementing an Agent Trade Server",
        "authors": [
            "Magnus Boman",
            "Anna Sandin"
        ],
        "summary": "An experimental server for stock trading autonomous agents is presented and made available, together with an agent shell for swift development. The server, written in Java, was implemented as proof-of-concept for an agent trade server for a real financial exchange.",
        "published": "2003-07-29T12:58:45Z",
        "link": "http://arxiv.org/abs/cs/0307064v1",
        "categories": [
            "cs.CE",
            "{I.2.11}{Artificial Intelligence}{Distributed Artificial\n  Intelligence}[Intelligent Agents];{K.4.4}{Computers and Society}{Electronic\n  Commerce}[Distributed Commercial Transactions]"
        ]
    },
    {
        "title": "The Generalized Riemann or Henstock Integral Underpinning Multivariate   Data Analysis: Application to Faint Structure Finding in Price Processes",
        "authors": [
            "Pat Muldowney",
            "Fionn Murtagh"
        ],
        "summary": "Practical data analysis involves many implicit or explicit assumptions about the good behavior of the data, and excludes consideration of various potentially pathological or limit cases. In this work, we present a new general theory of data, and of data processing, to bypass some of these assumptions. The new framework presented is focused on integration, and has direct applicability to expectation, distance, correlation, and aggregation. In a case study, we seek to reveal faint structure in financial data. Our new foundation for data encoding and handling offers increased justification for our conclusions.",
        "published": "2003-08-05T09:31:38Z",
        "link": "http://arxiv.org/abs/cs/0308009v3",
        "categories": [
            "cs.CE",
            "cs.CV",
            "G.3; I.5.3"
        ]
    },
    {
        "title": "New Approachs to Quantum Computer Simulaton in a Classical Supercomputer",
        "authors": [
            "John Robert Burger"
        ],
        "summary": "Classical simulation is important because it sets a benchmark for quantum computer performance. Classical simulation is currently the only way to exercise larger numbers of qubits. To achieve larger simulations, sparse matrix processing is emphasized below while trading memory for processing. It performed well within NCSA supercomputers, giving a state vector in convenient continuous portions ready for post processing.",
        "published": "2003-08-28T16:02:25Z",
        "link": "http://arxiv.org/abs/quant-ph/0308158v1",
        "categories": [
            "quant-ph",
            "cs.CE"
        ]
    },
    {
        "title": "An Algorithm for Optimal Partitioning of Data on an Interval",
        "authors": [
            "Brad Jackson",
            "Jeffrey D. Scargle",
            "David Barnes",
            "Sundararajan Arabhi",
            "Alina Alt",
            "Peter Gioumousis",
            "Elyus Gwin",
            "Paungkaew Sangtrakulcharoen",
            "Linda Tan",
            "Tun Tao Tsai"
        ],
        "summary": "Many signal processing problems can be solved by maximizing the fitness of a segmented model over all possible partitions of the data interval. This letter describes a simple but powerful algorithm that searches the exponentially large space of partitions of $N$ data points in time $O(N^2)$. The algorithm is guaranteed to find the exact global optimum, automatically determines the model order (the number of segments), has a convenient real-time mode, can be extended to higher dimensional data spaces, and solves a surprising variety of problems in signal detection and characterization, density estimation, cluster analysis and classification.",
        "published": "2003-09-17T18:27:00Z",
        "link": "http://arxiv.org/abs/math/0309285v2",
        "categories": [
            "math.NA",
            "astro-ph",
            "cs.CE",
            "cs.DS",
            "cs.IT",
            "math.CO",
            "math.IT",
            "65C60"
        ]
    },
    {
        "title": "Complex Independent Component Analysis of Frequency-Domain   Electroencephalographic Data",
        "authors": [
            "Jorn Anemuller",
            "Terrence J. Sejnowski",
            "Scott Makeig"
        ],
        "summary": "Independent component analysis (ICA) has proven useful for modeling brain and electroencephalographic (EEG) data. Here, we present a new, generalized method to better capture the dynamics of brain signals than previous ICA algorithms. We regard EEG sources as eliciting spatio-temporal activity patterns, corresponding to, e.g., trajectories of activation propagating across cortex. This leads to a model of convolutive signal superposition, in contrast with the commonly used instantaneous mixing model. In the frequency-domain, convolutive mixing is equivalent to multiplicative mixing of complex signal sources within distinct spectral bands. We decompose the recorded spectral-domain signals into independent components by a complex infomax ICA algorithm. First results from a visual attention EEG experiment exhibit (1) sources of spatio-temporal dynamics in the data, (2) links to subject behavior, (3) sources with a limited spectral extent, and (4) a higher degree of independence compared to sources derived by standard ICA.",
        "published": "2003-10-10T12:23:47Z",
        "link": "http://arxiv.org/abs/q-bio/0310011v2",
        "categories": [
            "q-bio.QM",
            "cs.CE",
            "physics.data-an",
            "q-bio.NC"
        ]
    },
    {
        "title": "Fuzzy Relational Modeling of Cost and Affordability for Advanced   Technology Manufacturing Environment",
        "authors": [
            "Ladislav J. Kohout",
            "Eunjin Kim",
            "Gary Zenz"
        ],
        "summary": "Relational representation of knowledge makes it possible to perform all the computations and decision making in a uniform relational way by means of special relational compositions called triangle and square products. In this paper some applications in manufacturing related to cost analysis are described. Testing fuzzy relational structures for various relational properties allows us to discover dependencies, hierarchies, similarities, and equivalences of the attributes characterizing technological processes and manufactured artifacts in their relationship to costs and performance.   A brief overview of mathematical aspects of BK-relational products is given in Appendix 1 together with further references in the literature.",
        "published": "2003-10-11T14:39:00Z",
        "link": "http://arxiv.org/abs/cs/0310021v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "math.OC",
            "J2; J1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Value-at-Risk and Expected Shortfall for Quadratic portfolio of   securities with mixture of elliptic Distributed Risk Factors",
        "authors": [
            "Jules Sadefo Kamdem"
        ],
        "summary": "Generally, in the financial literature, the notion of quadratic VaR is implicitly confused with the Delta-Gamma VaR, because more authors dealt with portfolios that contains derivatives instruments.   In this paper, we postpone to estimate the Value-at-Risk of a quadratic portfolio of securities (i.e equities) without the Delta and Gamma greeks, when the joint log-returns changes with multivariate elliptic distribution. We have reduced the estimation of the quadratic VaR of such portfolio to a resolution of one dimensional integral equation. To illustrate our method, we give special attention to the mixture of normal and mixture of t-student distribution. For given VaR, when joint Risk Factors changes with elliptic distribution, we show how to estimate an Expected Shortfall .",
        "published": "2003-10-22T18:04:24Z",
        "link": "http://arxiv.org/abs/cs/0310043v3",
        "categories": [
            "cs.CE",
            "math.CA",
            "G.1.9; G.1.10; G.1.2; G.1.1; J.1; J.2; J.4"
        ]
    },
    {
        "title": "On an explicit finite difference method for fractional diffusion   equations",
        "authors": [
            "S. B. Yuste",
            "L. Acedo"
        ],
        "summary": "A numerical method to solve the fractional diffusion equation, which could also be easily extended to many other fractional dynamics equations, is considered. These fractional equations have been proposed in order to describe anomalous transport characterized by non-Markovian kinetics and the breakdown of Fick's law. In this paper we combine the forward time centered space (FTCS) method, well known for the numerical integration of ordinary diffusion equations, with the Grunwald-Letnikov definition of the fractional derivative operator to obtain an explicit fractional FTCS scheme for solving the fractional diffusion equation. The resulting method is amenable to a stability analysis a la von Neumann. We show that the analytical stability bounds are in excellent agreement with numerical tests. Comparison between exact analytical solutions and numerical predictions are made.",
        "published": "2003-11-10T13:13:32Z",
        "link": "http://arxiv.org/abs/cs/0311011v1",
        "categories": [
            "cs.NA",
            "cond-mat.stat-mech",
            "cs.CE",
            "physics.comp-ph",
            "G.1.9, G.1.4, G.1.0"
        ]
    },
    {
        "title": "Turning CARTwheels: An Alternating Algorithm for Mining Redescriptions",
        "authors": [
            "Deept Kumar",
            "Naren Ramakrishnan",
            "Malcolm Potts",
            "Richard F. Helm"
        ],
        "summary": "We present an unusual algorithm involving classification trees where two trees are grown in opposite directions so that they are matched at their leaves. This approach finds application in a new data mining task we formulate, called \"redescription mining\". A redescription is a shift-of-vocabulary, or a different way of communicating information about a given subset of data; the goal of redescription mining is to find subsets of data that afford multiple descriptions. We highlight the importance of this problem in domains such as bioinformatics, which exhibit an underlying richness and diversity of data descriptors (e.g., genes can be studied in a variety of ways). Our approach helps integrate multiple forms of characterizing datasets, situates the knowledge gained from one dataset in the context of others, and harnesses high-level abstractions for uncovering cryptic and subtle features of data. Algorithm design decisions, implementation details, and experimental results are presented.",
        "published": "2003-11-27T18:13:38Z",
        "link": "http://arxiv.org/abs/cs/0311048v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "A theoretical investigation of ferromagnetic tunnel junctions with   4-valued conductances",
        "authors": [
            "Satoshi Kokado",
            "Kikuo Harigaya"
        ],
        "summary": "In considering a novel function in ferromagnetic tunnel junctions consisting of ferromagnet(FM)/barrier/FM junctions, we theoretically investigate multiple valued (or multi-level) cell property, which is in principle realized by sensing conductances of four states recorded with magnetization configurations of two FMs; that is, (up,up), (up,down), (down,up), (down,down). To obtain such 4-valued conductances, we propose FM1/spin-polarized barrier/FM2 junctions, where the FM1 and FM2 are different ferromagnets, and the barrier has spin dependence. The proposed idea is applied to the case of the barrier having localized spins. Assuming that all the localized spins are pinned parallel to magnetization axes of the FM1 and FM2, 4-valued conductances are explicitly obtained for the case of many localized spins. Furthermore, objectives for an ideal spin-polarized barrier are discussed.",
        "published": "2003-12-01T05:11:28Z",
        "link": "http://arxiv.org/abs/cond-mat/0312019v1",
        "categories": [
            "cond-mat.mes-hall",
            "cond-mat.mtrl-sci",
            "cs.CE",
            "physics.ins-det",
            "quant-ph"
        ]
    },
    {
        "title": "Recursive function templates as a solution of linear algebra expressions   in C++",
        "authors": [
            "Volodymyr Myrnyy"
        ],
        "summary": "The article deals with a kind of recursive function templates in C++, where the recursion is realized corresponding template parameters to achieve better computational performance. Some specialization of these template functions ends the recursion and can be implemented using optimized hardware dependent or independent routines. The method is applied in addition to the known expression templates technique to solve linear algebra expressions with the help of the BLAS library. The whole implementation produces a new library, which keeps object-oriented benefits and has a higher computational speed represented in the tests.",
        "published": "2003-02-19T15:59:28Z",
        "link": "http://arxiv.org/abs/cs/0302026v1",
        "categories": [
            "cs.MS",
            "cs.PL",
            "G.4; I.1.2; I.1.3"
        ]
    },
    {
        "title": "Reliability Conditions in Quadrature Algorithms",
        "authors": [
            "Gh. Adam",
            "S. Adam",
            "N. M. Plakida"
        ],
        "summary": "The detection of insufficiently resolved or ill-conditioned integrand structures is critical for the reliability assessment of the quadrature rule outputs. We discuss a method of analysis of the profile of the integrand at the quadrature knots which allows inferences approaching the theoretical 100% rate of success, under error estimate sharpening. The proposed procedure is of the highest interest for the solution of parametric integrals arising in complex physical models.",
        "published": "2003-03-06T09:19:42Z",
        "link": "http://arxiv.org/abs/cs/0303004v1",
        "categories": [
            "cs.NA",
            "cs.MS",
            "physics.comp-ph",
            "G.4; G.1.4; G.1.0; J.2; D.2.4"
        ]
    },
    {
        "title": "A Bird's eye view of Matrix Distributed Processing",
        "authors": [
            "Massimo Di Pierro"
        ],
        "summary": "We present Matrix Distributed Processing, a C++ library for fast development of efficient parallel algorithms. MDP is based on MPI and consists of a collection of C++ classes and functions such as lattice, site and field. Once an algorithm is written using these components the algorithm is automatically parallel and no explicit call to communication functions is required. MDP is particularly suitable for implementing parallel solvers for multi-dimensional differential equations and mesh-like problems.",
        "published": "2003-03-28T20:47:50Z",
        "link": "http://arxiv.org/abs/cs/0303031v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.DM",
            "cs.MS",
            "hep-lat",
            "physics.comp-ph",
            "D.1.3; D.3.2;G.1;G.4;I.6.8"
        ]
    },
    {
        "title": "A Representation of Changes of Images and its Application for   Developmental Biolology",
        "authors": [
            "Gene Kim",
            "MyungHo Kim"
        ],
        "summary": "In this paper, we consider a series of events observed at spaced time intervals and present a method of representation of the series. To explain an idea, by dealing with a set of gene expression data, which could be obtained from developmental biology, the procedures are sketched with comments in some details. We mean representation by choosing a proper function, which fits well with observed data of a series, and turning its characteristics into numbers, which extract the intrinsic properties of fluctuating data. With help of a machine learning techniques, this method will give a classification of developmental biological data as well as any varying data during a certain period and the classification can be applied for diagnosis of a disease.",
        "published": "2003-05-13T17:48:35Z",
        "link": "http://arxiv.org/abs/cs/0305008v1",
        "categories": [
            "cs.CC",
            "cs.MS",
            "q-bio",
            "I.1.2; H.1.1;I.5.0"
        ]
    },
    {
        "title": "Development of a Java Package for Matrix Programming",
        "authors": [
            "Ngee-Peng Lim",
            "Maurice HT Ling",
            "Shawn YC Lim",
            "Ji-Hee Choi",
            "Henry BK Teo"
        ],
        "summary": "We had assembled a Java package, known as MatrixPak, of four classes for the purpose of numerical matrix computation. The classes are matrix, matrix_operations, StrToMatrix, and MatrixToStr; all of which are inherited from java.lang.Object class. Class matrix defines a matrix as a two-dimensional array of float types, and contains the following mathematical methods: transpose, adjoint, determinant, inverse, minor and cofactor. Class matrix_operations contains the following mathematical methods: matrix addition, matrix subtraction, matrix multiplication, and matrix exponential. Class StrToMatrix contains methods necessary to parse a string representation (for example, [[2 3 4]-[5 6 7]]) of a matrix into a matrix definition, whereas class MatrixToStr does the reverse.",
        "published": "2003-06-24T12:37:26Z",
        "link": "http://arxiv.org/abs/cs/0306127v1",
        "categories": [
            "cs.MS",
            "K.3.0; G.m"
        ]
    },
    {
        "title": "Finding the \"truncated\" polynomial that is closest to a function",
        "authors": [
            "Nicolas Brisebarre",
            "Jean-Michel Muller"
        ],
        "summary": "When implementing regular enough functions (e.g., elementary or special functions) on a computing system, we frequently use polynomial approximations. In most cases, the polynomial that best approximates (for a given distance and in a given interval) a function has coefficients that are not exactly representable with a finite number of bits. And yet, the polynomial approximations that are actually implemented do have coefficients that are represented with a finite - and sometimes small - number of bits: this is due to the finiteness of the floating-point representations (for software implementations), and to the need to have small, hence fast and/or inexpensive, multipliers (for hardware implementations). We then have to consider polynomial approximations for which the degree-$i$ coefficient has at most $m_i$ fractional bits (in other words, it is a rational number with denominator $2^{m_i}$). We provide a general method for finding the best polynomial approximation under this constraint. Then, we suggest refinements than can be used to accelerate our method.",
        "published": "2003-07-04T13:15:09Z",
        "link": "http://arxiv.org/abs/cs/0307009v1",
        "categories": [
            "cs.MS",
            "G.1.0, G.1.2, B.2.4"
        ]
    },
    {
        "title": "Boundary knot method for Laplace and biharmonic problems",
        "authors": [
            "W. Chen"
        ],
        "summary": "The boundary knot method (BKM) [1] is a meshless boundary-type radial basis function (RBF) collocation scheme, where the nonsingular general solution is used instead of fundamental solution to evaluate the homogeneous solution, while the dual reciprocity method (DRM) is employed to approximation of particular solution. Despite the fact that there are not nonsingular RBF general solutions available for Laplace and biharmonic problems, this study shows that the method can be successfully applied to these problems. The high-order general and fundamental solutions of Burger and Winkler equations are also first presented here.",
        "published": "2003-07-28T09:35:52Z",
        "link": "http://arxiv.org/abs/cs/0307061v1",
        "categories": [
            "cs.CE",
            "cs.MS",
            "G1.3; G1.8"
        ]
    },
    {
        "title": "Mace4 Reference Manual and Guide",
        "authors": [
            "William McCune"
        ],
        "summary": "Mace4 is a program that searches for finite models of first-order formulas. For a given domain size, all instances of the formulas over the domain are constructed. The result is a set of ground clauses with equality. Then, a decision procedure based on ground equational rewriting is applied. If satisfiability is detected, one or more models are printed. Mace4 is a useful complement to first-order theorem provers, with the prover searching for proofs and Mace4 looking for countermodels, and it is useful for work on finite algebras. Mace4 performs better on equational problems than did our previous model-searching program Mace2.",
        "published": "2003-10-28T16:56:44Z",
        "link": "http://arxiv.org/abs/cs/0310055v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "F.4.1"
        ]
    },
    {
        "title": "OTTER 3.3 Reference Manual",
        "authors": [
            "William McCune"
        ],
        "summary": "OTTER is a resolution-style theorem-proving program for first-order logic with equality. OTTER includes the inference rules binary resolution, hyperresolution, UR-resolution, and binary paramodulation. Some of its other abilities and features are conversion from first-order formulas to clauses, forward and back subsumption, factoring, weighting, answer literals, term ordering, forward and back demodulation, evaluable functions and predicates, Knuth-Bendix completion, and the hints strategy. OTTER is coded in ANSI C, is free, and is portable to many different kinds of computer.",
        "published": "2003-10-28T19:17:38Z",
        "link": "http://arxiv.org/abs/cs/0310056v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "F.4.1"
        ]
    },
    {
        "title": "An Introduction to Using Software Tools for Automatic Differentiation",
        "authors": [
            "Uwe Naumann",
            "Andrea Walther"
        ],
        "summary": "We give a gentle introduction to using various software tools for automatic differentiation (AD). Ready-to-use examples are discussed, and links to further information are presented. Our target audience includes all those who are looking for a straightforward way to get started using the available AD technology. The document is dynamic in the sense that its content will be updated as the AD software evolves.",
        "published": "2003-10-28T19:41:44Z",
        "link": "http://arxiv.org/abs/cs/0310057v1",
        "categories": [
            "cs.MS",
            "G.4"
        ]
    },
    {
        "title": "On the symmetry classes of the first covariant derivatives of tensor   fields",
        "authors": [
            "B. Fiedler"
        ],
        "summary": "We show that the symmetry classes of torsion-free covariant derivatives $\\nabla T$ of r-times covariant tensor fields T can be characterized by Littlewood-Richardson products $\\sigma [1]$ where $\\sigma$ is a representation of the symmetric group $S_r$ which is connected with the symmetry class of T. If $\\sigma = [\\lambda]$ is irreducible then $\\sigma [1]$ has a multiplicity free reduction $[\\lambda][1] = \\sum [\\mu]$ and all primitive idempotents belonging to that sum can be calculated from a generating idempotent e of the symmetry class of T by means of the irreducible characters or of a discrete Fourier transform of $S_{r+1}$. We apply these facts to derivatives $\\nabla S$, $\\nabla A$ of symmetric or alternating tensor fields. The symmetry classes of the differences $\\nabla S - sym(\\nabla S)$ and $\\nabla A - alt(\\nabla A)$ are characterized by Young frames (r, 1) and (2, 1^{r-1}), respectively. However, while the symmetry class of $\\nabla A - alt(\\nabla A)$ can be generated by Young symmetrizers of (2, 1^{r-1}), no Young symmetrizer of (r, 1) generates the symmetry class of $\\nabla S - sym(\\nabla S)$. Furthermore we show in the case r = 2 that $\\nabla S - sym(\\nabla S)$ and $\\nabla A - alt(\\nabla A)$ can be applied in generator formulas of algebraic covariant derivative curvature tensors. For certain symbolic calculations we used the Mathematica packages Ricci and PERMS.",
        "published": "2003-01-06T16:28:59Z",
        "link": "http://arxiv.org/abs/math/0301042v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.DG",
            "53B20, 15A72, 05E10, 16D60, 05-04"
        ]
    },
    {
        "title": "A comparison of four approaches to the calculation of conservation laws",
        "authors": [
            "Thomas Wolf"
        ],
        "summary": "The paper compares computational aspects of four approaches to compute conservation laws of single differential equations (DEs) or systems of them, ODEs and PDEs. The only restriction, required by two of the four corresponding computer algebra programs, is that each DE has to be solvable for a leading derivative. Extra constraints for the conservation laws can be specified. Examples include new conservation laws that are non-polynomial in the functions, that have an explicit variable dependence and families of conservation laws involving arbitrary functions. The following equations are investigated in examples: Ito, Liouville, Burgers, Kadomtsev-Petviashvili, Karney-Sen-Chu-Verheest, Boussinesq, Tzetzeica, Benney.",
        "published": "2003-01-25T04:42:12Z",
        "link": "http://arxiv.org/abs/cs/0301027v2",
        "categories": [
            "cs.SC",
            "math-ph",
            "math.MP",
            "I.1.2"
        ]
    },
    {
        "title": "The integration of systems of linear PDEs using conservation laws of   syzygies",
        "authors": [
            "Thomas Wolf"
        ],
        "summary": "A new integration technique is presented for systems of linear partial differential equations (PDEs) for which syzygies can be formulated that obey conservation laws. These syzygies come for free as a by-product of the differential Groebner Basis computation. Compared with the more obvious way of integrating a single equation and substituting the result in other equations the new technique integrates more than one equation at once and therefore introduces temporarily fewer new functions of integration that in addition depend on fewer variables. Especially for high order PDE systems in many variables the conventional integration technique may lead to an explosion of the number of functions of integration which is avoided with the new method. A further benefit is that redundant free functions in the solution are either prevented or that their number is at least reduced.",
        "published": "2003-01-28T05:20:44Z",
        "link": "http://arxiv.org/abs/cs/0301028v1",
        "categories": [
            "cs.SC",
            "math.AP",
            "I.1.2"
        ]
    },
    {
        "title": "Size reduction and partial decoupling of systems of equations",
        "authors": [
            "Thomas Wolf"
        ],
        "summary": "A method is presented that reduces the number of terms of systems of linear equations (algebraic, ordinary and partial differential equations). As a byproduct these systems have a tendency to become partially decoupled and are more likely to be factorizable or integrable. A variation of this method is applicable to non-linear systems. Modifications to improve efficiency are given and examples are shown. This procedure can be used in connection with the computation of the radical of a differential ideal (differential Groebner basis).",
        "published": "2003-01-28T06:16:25Z",
        "link": "http://arxiv.org/abs/cs/0301029v1",
        "categories": [
            "cs.SC",
            "I.1.2"
        ]
    },
    {
        "title": "Hidden Polynomial(s) Cryptosystems",
        "authors": [
            "Ilia Toli"
        ],
        "summary": "We propose variations of the class of hidden monomial cryptosystems in order to make it resistant to all known attacks. We use identities built upon a single bivariate polynomial equation with coefficients in a finite field. Indeed, it can be replaced by a ``small'' ideal, as well. Throughout, we set up probabilistic encryption protocols, too. The same ideas extend to digital signature algorithms, as well. Our schemes work as well on differential fields of positive characteristic, and elsewhere.",
        "published": "2003-02-26T18:30:49Z",
        "link": "http://arxiv.org/abs/cs/0302037v4",
        "categories": [
            "cs.CR",
            "cs.SC",
            "E.3"
        ]
    },
    {
        "title": "Numerical Coverage Estimation for the Symbolic Simulation of Real-Time   Systems",
        "authors": [
            "Farn Wang",
            "Geng-Dian Hwang",
            "Fang Yu"
        ],
        "summary": "Three numerical coverage metrics for the symbolic simulation of dense-time systems and their estimation methods are presented. Special techniques to derive numerical estimations of dense-time state-spaces have also been developed. Properties of the metrics are also discussed with respect to four criteria. Implementation and experiments are then reported.",
        "published": "2003-03-25T21:57:50Z",
        "link": "http://arxiv.org/abs/cs/0303027v1",
        "categories": [
            "cs.SE",
            "cs.SC",
            "B.1.2"
        ]
    },
    {
        "title": "TCTL Inevitability Analysis of Dense-time Systems",
        "authors": [
            "Farn Wang",
            "Geng-Dian Hwang",
            "Fang Yu"
        ],
        "summary": "Inevitability properties in branching temporal logics are of the syntax forall eventually \\phi, where \\phi is an arbitrary (timed) CTL formula. In the sense that \"good things will happen\", they are parallel to the \"liveness\" properties in linear temporal logics. Such inevitability properties in dense-time logics can be analyzed with greatest fixpoint calculation. We present algorithms to model-check inevitability properties both with and without requirement of non-Zeno computations. We discuss a technique for early decision on greatest fixpoints in the temporal logics, and experiment with the effect of non-Zeno computations on the evaluation of greatest fixpoints. We also discuss the TCTL subclass with only universal path quantifiers which allows for the safe abstraction analysis of inevitability properties. Finally, we report our implementation and experiments to show the plausibility of our ideas.",
        "published": "2003-04-01T07:28:29Z",
        "link": "http://arxiv.org/abs/cs/0304003v2",
        "categories": [
            "cs.SC",
            "B.1.2"
        ]
    },
    {
        "title": "Quasi-Optimal Arithmetic for Quaternion Polynomials",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "Fast algorithms for arithmetic on real or complex polynomials are well-known and have proven to be not only asymptotically efficient but also very practical. Based on Fast Fourier Transform (FFT), they for instance multiply two polynomials of degree up to N or multi-evaluate one at N points simultaneously within quasi-linear time O(N.polylog N). An extension to (and in fact the mere definition of) polynomials over the skew-field H of quaternions is promising but still missing. The present work proposes three such definitions which in the commutative case coincide but for H turn out to differ, each one satisfying some desirable properties while lacking others. For each notion we devise algorithms for according arithmetic; these are quasi-optimal in that their running times match lower complexity bounds up to polylogarithmic factors.",
        "published": "2003-04-01T19:21:21Z",
        "link": "http://arxiv.org/abs/cs/0304004v2",
        "categories": [
            "cs.SC",
            "I.1;F.2.1"
        ]
    },
    {
        "title": "Hidden Polynomial(s) Cryptosystems",
        "authors": [
            "Ilia Toli"
        ],
        "summary": "We propose public-key cryptosystems with public key a system of polynomial equations, algebraic or differential, and private key a single polynomial or a small-size ideal. We set up probabilistic encryption, signature, and signcryption protocols.",
        "published": "2003-04-09T12:28:35Z",
        "link": "http://arxiv.org/abs/cs/0304013v1",
        "categories": [
            "cs.CR",
            "cs.SC",
            "E.3"
        ]
    },
    {
        "title": "On reconstructing n-point configurations from the distribution of   distances or areas",
        "authors": [
            "Mireille Boutin",
            "Gregor Kemper"
        ],
        "summary": "One way to characterize configurations of points up to congruence is by considering the distribution of all mutual distances between points. This paper deals with the question if point configurations are uniquely determined by this distribution. After giving some counterexamples, we prove that this is the case for the vast majority of configurations. In the second part of the paper, the distribution of areas of sub-triangles is used for characterizing point configurations. Again it turns out that most configurations are reconstructible from the distribution of areas, though there are counterexamples.",
        "published": "2003-04-15T10:01:26Z",
        "link": "http://arxiv.org/abs/math/0304192v1",
        "categories": [
            "math.AC",
            "cs.CV",
            "cs.SC",
            "13A50;13-04;68T45"
        ]
    },
    {
        "title": "Whitehead method and Genetic Algorithms",
        "authors": [
            "Alexei D. Miasnikov",
            "Alexei G. Myasnikov"
        ],
        "summary": "In this paper we discuss a genetic version (GWA) of the Whitehead's algorithm, which is one of the basic algorithms in combinatorial group theory. It turns out that GWA is surprisingly fast and outperforms the standard Whitehead's algorithm in free groups of rank >= 5. Experimenting with GWA we collected an interesting numerical data that clarifies the time-complexity of the Whitehead's Problem in general. These experiments led us to several mathematical conjectures. If confirmed they will shed light on hidden mechanisms of Whitehead Method and geometry of automorphic orbits in free groups.",
        "published": "2003-04-20T16:24:44Z",
        "link": "http://arxiv.org/abs/math/0304283v1",
        "categories": [
            "math.GR",
            "cs.NE",
            "cs.SC",
            "20F28, 68Q17, 68T05"
        ]
    },
    {
        "title": "Balanced presentations of the trivial group on two generators and the   Andrews-Curtis conjecture",
        "authors": [
            "Alexei D. Miasnikov",
            "Alexei G. Myasnikov"
        ],
        "summary": "The Andrews-Curtis conjecture states that every balanced presentation of the trivial group can be reduced to the standard one by a sequence of the elementary Nielsen transformations and conjugations. In this paper we describe all balanced presentations of the trivial group on two generators and with the total length of relators <= 12. We show that all these presentations satisfy the Andrews-Curtis conjecture.",
        "published": "2003-04-21T21:01:49Z",
        "link": "http://arxiv.org/abs/math/0304305v1",
        "categories": [
            "math.GR",
            "cs.SC",
            "20E05, 20F05, 68T05 (Primary), 57M05,57M20. (Secondary)"
        ]
    },
    {
        "title": "Genetic algorithms and the Andrews-Curtis conjecture",
        "authors": [
            "Alexei D. Miasnikov"
        ],
        "summary": "The Andrews-Curtis conjecture claims that every balanced presentation of the trivial group can be transformed into the trivial presentation by a finite sequence of \"elementary transformations\" which are Nielsen transformations together with an arbitrary conjugation of a relator. It is believed that the Andrews-Curtis conjecture is false; however, not so many possible counterexamples are known. It is not a trivial matter to verify whether the conjecture holds for a given balanced presentation or not. The purpose of this paper is to describe some non-deterministic methods, called Genetic Algorithms, designed to test the validity of the Andrews-Curtis conjecture. Using such algorithm we have been able to prove that all known (to us) balanced presentations of the trivial group where the total length of the relators is at most 12 satisfy the conjecture. In particular, the Andrews-Curtis conjecture holds for the presentation <x,y|x y x = y x y, x^2 = y^3> which was one of the well known potential counterexamples.",
        "published": "2003-04-21T21:07:18Z",
        "link": "http://arxiv.org/abs/math/0304306v1",
        "categories": [
            "math.GR",
            "cs.NE",
            "cs.SC",
            "20E05, 20F05, 68T05 (Primary) 57M05,57M20"
        ]
    },
    {
        "title": "gTybalt - a free computer algebra system",
        "authors": [
            "Stefan Weinzierl"
        ],
        "summary": "This article documents the free computer algebra system \"gTybalt\". The program is build on top of other packages, among others GiNaC, TeXmacs and Root. It offers the possibility of interactive symbolic calculations within the C++ programming language. Mathematical formulae are visualized using TeX fonts.",
        "published": "2003-04-29T18:22:47Z",
        "link": "http://arxiv.org/abs/cs/0304043v1",
        "categories": [
            "cs.SC",
            "hep-ph",
            "I.1.3"
        ]
    },
    {
        "title": "Cryptanalysis of HFE",
        "authors": [
            "Ilia Toli"
        ],
        "summary": "I transform the trapdoor problem of HFE into a linear algebra problem.",
        "published": "2003-05-17T19:41:32Z",
        "link": "http://arxiv.org/abs/cs/0305034v3",
        "categories": [
            "cs.CR",
            "cs.SC",
            "E.3"
        ]
    },
    {
        "title": "Mapping the vacuum structure of gauged maximal supergravities: an   application of high-performance symbolic algebra",
        "authors": [
            "Thomas Fischbacher"
        ],
        "summary": "The analysis of the extremal structure of the scalar potentials of gauged maximally extended supergravity models in five, four, and three dimensions, and hence the determination of possible vacuum states of these models is a computationally challenging task due to the occurrence of the exceptional Lie groups $E_6$, $E_7$, $E_8$ in the definition of these potentials. At present, the most promising approach to gain information about nontrivial vacua of these models is to perform a truncation of the potential to submanifolds of the $G/H$ coset manifold of scalars which are invariant under a subgroup of the gauge group and of sufficiently low dimension to make an analytic treatment possible.   New tools are presented which allow a systematic and highly effective study of these potentials up to a previously unreached level of complexity. Explicit forms of new truncations of the potentials of four- and three-dimensional models are given, and for N=16, D=3 supergravities, which are much more rich in structure than their higher-dimensional cousins, a series of new nontrivial vacua is identified and analysed.",
        "published": "2003-05-20T13:42:15Z",
        "link": "http://arxiv.org/abs/hep-th/0305176v1",
        "categories": [
            "hep-th",
            "cs.SC"
        ]
    },
    {
        "title": "Model Checking Linear Logic Specifications",
        "authors": [
            "M. Bozzano",
            "G. Delzanno",
            "M. Martelli"
        ],
        "summary": "The overall goal of this paper is to investigate the theoretical foundations of algorithmic verification techniques for first order linear logic specifications. The fragment of linear logic we consider in this paper is based on the linear logic programming language called LO enriched with universally quantified goal formulas. Although LO was originally introduced as a theoretical foundation for extensions of logic programming languages, it can also be viewed as a very general language to specify a wide range of infinite-state concurrent systems.   Our approach is based on the relation between backward reachability and provability highlighted in our previous work on propositional LO programs. Following this line of research, we define here a general framework for the bottom-up evaluation of first order linear logic specifications. The evaluation procedure is based on an effective fixpoint operator working on a symbolic representation of infinite collections of first order linear logic formulas. The theory of well quasi-orderings can be used to provide sufficient conditions for the termination of the evaluation of non trivial fragments of first order linear logic.",
        "published": "2003-09-01T09:34:15Z",
        "link": "http://arxiv.org/abs/cs/0309003v1",
        "categories": [
            "cs.PL",
            "cs.SC",
            "D.3.1;F.3.1;F.3.2"
        ]
    },
    {
        "title": "Digital Version of Green`s Theorem and its Application to The Coverage   Problem in Formal Verification",
        "authors": [
            "Eli Appleboim",
            "Emil Saucan"
        ],
        "summary": "We present a novel scheme to the coverage problem, introducing a quantitative way to estimate the interaction between a block and its enviroment.This is achieved by setting a discrete version of Green`s theorem, specially adapted for Model Checking based verification of integrated circuits.This method is best suited for the coverage problem since it enables one to quantify the incompleteness or, on the other hand, the redundancy of a set of rules, describing the model under verification.Moreover this can be done continuously throughout the verification process, thus enabling the user to pinpoint the stages at which incompleteness/redundancy occurs. Although the method is presented locally on a small hardware example, we additionally show its possibility to provide precise coverage estimation also for large scale systems. We compare this method to others by checking it on the same test-cases.",
        "published": "2003-09-07T05:13:44Z",
        "link": "http://arxiv.org/abs/cs/0309008v1",
        "categories": [
            "cs.SC",
            "B.6.3"
        ]
    },
    {
        "title": "A uniform approach to constraint-solving for lists, multisets, compact   lists, and sets",
        "authors": [
            "Agostino Dovier",
            "Carla Piazza",
            "Gianfranco Rossi"
        ],
        "summary": "Lists, multisets, and sets are well-known data structures whose usefulness is widely recognized in various areas of Computer Science. These data structures have been analyzed from an axiomatic point of view with a parametric approach in (*) where the relevant unification algorithms have been developed. In this paper we extend these results considering more general constraints including not only equality but also membership constraints as well as their negative counterparts.   (*) A. Dovier, A. Policriti, and G. Rossi. A uniform axiomatic view of lists, multisets, and sets, and the relevant unification algorithms. Fundamenta Informaticae, 36(2/3):201--234, 1998.",
        "published": "2003-09-24T10:08:00Z",
        "link": "http://arxiv.org/abs/cs/0309045v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SC",
            "D.3.3; F.4.1; F.2.2; I.1.2; I.2.3"
        ]
    },
    {
        "title": "Computing Igusa's Local Zeta Functions of Univariate Polynomials, and   Linear Feedback Shift Registers",
        "authors": [
            "W. A. Zuniga-Galindo"
        ],
        "summary": "We give a polynomial time algorithm for computing the Igusa local zeta function $Z(s,f)$ attached to a polynomial $f(x)\\in \\QTR{Bbb}{Z}[x]$, in one variable, with splitting field $\\QTR{Bbb}{Q}$, and a prime number $p$. We also propose a new class of Linear Feedback Shift Registers based on the computation of Igusa's local zeta function.",
        "published": "2003-09-26T22:57:24Z",
        "link": "http://arxiv.org/abs/cs/0309050v1",
        "categories": [
            "cs.SC",
            "cs.CR",
            "I.1.2"
        ]
    },
    {
        "title": "Generators of algebraic covariant derivative curvature tensors and Young   symmetrizers",
        "authors": [
            "B. Fiedler"
        ],
        "summary": "We show that the space of algebraic covariant derivative curvature tensors R' is generated by Young symmetrized tensor products W*U or U*W, where W and U are covariant tensors of order 2 and 3 whose symmetry classes are irreducible and characterized by the following pairs of partitions: {(2),(3)}, {(2),(2 1)} or {(1 1),(2 1)}. Each of the partitions (2), (3) and (1 1) describes exactly one symmetry class, whereas the partition (2 1) characterizes an infinite set S of irreducible symmetry classes. This set S contains exactly one symmetry class S_0 whose elements U can not play the role of generators of tensors R'. The tensors U of all other symmetry classes from S\\{S_0} can be used as generators for tensors R'. Foundation of our investigations is a theorem of S. A. Fulling, R. C. King, B. G. Wybourne and C. J. Cummins about a Young symmetrizer that generates the symmetry class of algebraic covariant derivative curvature tensors. Furthermore we apply ideals and idempotents in group rings C[Sr], the Littlewood-Richardson rule and discrete Fourier transforms for symmetric groups Sr. For certain symbolic calculations we used the Mathematica packages Ricci and PERMS.",
        "published": "2003-10-02T11:37:41Z",
        "link": "http://arxiv.org/abs/math/0310020v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.DG",
            "53B20; 15A72; 05E10; 16D60; 05-04"
        ]
    },
    {
        "title": "Mace4 Reference Manual and Guide",
        "authors": [
            "William McCune"
        ],
        "summary": "Mace4 is a program that searches for finite models of first-order formulas. For a given domain size, all instances of the formulas over the domain are constructed. The result is a set of ground clauses with equality. Then, a decision procedure based on ground equational rewriting is applied. If satisfiability is detected, one or more models are printed. Mace4 is a useful complement to first-order theorem provers, with the prover searching for proofs and Mace4 looking for countermodels, and it is useful for work on finite algebras. Mace4 performs better on equational problems than did our previous model-searching program Mace2.",
        "published": "2003-10-28T16:56:44Z",
        "link": "http://arxiv.org/abs/cs/0310055v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "F.4.1"
        ]
    },
    {
        "title": "OTTER 3.3 Reference Manual",
        "authors": [
            "William McCune"
        ],
        "summary": "OTTER is a resolution-style theorem-proving program for first-order logic with equality. OTTER includes the inference rules binary resolution, hyperresolution, UR-resolution, and binary paramodulation. Some of its other abilities and features are conversion from first-order formulas to clauses, forward and back subsumption, factoring, weighting, answer literals, term ordering, forward and back demodulation, evaluable functions and predicates, Knuth-Bendix completion, and the hints strategy. OTTER is coded in ANSI C, is free, and is portable to many different kinds of computer.",
        "published": "2003-10-28T19:17:38Z",
        "link": "http://arxiv.org/abs/cs/0310056v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "F.4.1"
        ]
    },
    {
        "title": "Weak Bezout inequality for D-modules",
        "authors": [
            "Dima Grigoriev"
        ],
        "summary": "Let $\\{w_{i,j}\\}_{1\\leq i\\leq n, 1\\leq j\\leq s} \\subset L_m=F(X_1,...,X_m)[{\\partial \\over \\partial X_1},..., {\\partial \\over \\partial X_m}]$ be linear partial differential operators of orders with respect to ${\\partial \\over \\partial X_1},..., {\\partial \\over \\partial X_m}$ at most $d$. We prove an upper bound n(4m^2d\\min\\{n,s\\})^{4^{m-t-1}(2(m-t))} on the leading coefficient of the Hilbert-Kolchin polynomial of the left $L_m$-module $<\\{w_{1,j}, ..., w_{n,j}\\}_{1\\leq j \\leq s} > \\subset L_m^n$ having the differential type $t$ (also being equal to the degree of the Hilbert-Kolchin polynomial). The main technical tool is the complexity bound on solving systems of linear equations over {\\it algebras of fractions} of the form $$L_m(F[X_1,..., X_m, {\\partial \\over \\partial X_1},..., {\\partial \\over \\partial X_k}])^{-1}.$$",
        "published": "2003-11-28T15:28:04Z",
        "link": "http://arxiv.org/abs/cs/0311053v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1.2"
        ]
    },
    {
        "title": "Short formulas for algebraic covariant derivative curvature tensors via   Algebraic Combinatorics",
        "authors": [
            "Bernd Fiedler"
        ],
        "summary": "We consider generators of algebraic covariant derivative curvature tensors R' which can be constructed by a Young symmetrization of product tensors W*U or U*W, where W and U are covariant tensors of order 2 and 3. W is a symmetric or alternating tensor whereas U belongs to a class of the infinite set S of irreducible symmetry classes characterized by the partition (2,1). Using Computer Algebra we search for such generators whose coordinate representations are polynomials with a minimal number of summands. For a generic choice of the symmetry class of U we obtain lengths of 16 or 20 summands if W is symmetric or skew-symmetric, respectively. In special cases these numbers can be reduced to the minima 12 or 10. If these minima occur then U admits an index commutation symmetry. Furthermore minimal lengths are possible if U is formed from torsion-free covariant derivatives of symmetric or alternating 2-tensor fields. Foundation of our investigations is a theorem of S. A. Fulling, R. C. King, B. G. Wybourne and C. J. Cummins about a Young symmetrizer that generates the symmetry class of algebraic covariant derivative curvature tensors. Furthermore we apply ideals and idempotents in group rings C[S_r] and discrete Fourier transforms for symmetric groups S_r. For symbolic calculations we used the Mathematica packages Ricci and PERMS.",
        "published": "2003-12-08T19:31:51Z",
        "link": "http://arxiv.org/abs/math/0312171v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.DG",
            "53B20, 15A72, 05E10, 16D60, 05-04"
        ]
    },
    {
        "title": "Novel Runtime Systems Support for Adaptive Compositional Modeling on the   Grid",
        "authors": [
            "Srinidhi Varadarajan",
            "Naren Ramakrishnan"
        ],
        "summary": "Grid infrastructures and computing environments have progressed significantly in the past few years. The vision of truly seamless Grid usage relies on runtime systems support that is cognizant of the operational issues underlying grid computations and, at the same time, is flexible enough to accommodate diverse application scenarios. This paper addresses the twin aspects of Grid infrastructure and application support through a novel combination of two computational technologies: Weaves - a source-language independent parallel runtime compositional framework that operates through reverse-analysis of compiled object files, and runtime recommender systems that aid in dynamic knowledge-based application composition. Domain-specific adaptivity is exploited through a novel compositional system that supports runtime recommendation of code modules and a sophisticated checkpointing and runtime migration solution that can be transparently deployed over Grid infrastructures. A core set of \"adaptivity schemas\" are provided as templates for adaptive composition of large-scale scientific computations. Implementation issues, motivating application contexts, and preliminary results are described.",
        "published": "2003-01-21T02:26:47Z",
        "link": "http://arxiv.org/abs/cs/0301018v1",
        "categories": [
            "cs.CE",
            "cs.DC",
            "D.4.1; I.6"
        ]
    },
    {
        "title": "Fine-Grain Authorization for Resource Management in the Grid Environment",
        "authors": [
            "K. Keahey",
            "V. Welch"
        ],
        "summary": "In this document we describe our work-in-progress for enabling fine-grain authorization of resource management. In particular, we address the needs of Virtual Organizations (VOs) to enforce their own policies in addition to those of the resource owners.",
        "published": "2003-01-28T19:27:29Z",
        "link": "http://arxiv.org/abs/cs/0301031v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "D.2.1; D.2.2"
        ]
    },
    {
        "title": "Computational Grids in Action: The Natinal Fusion Collaboratory",
        "authors": [
            "K. Keahey",
            "T. Fredian",
            "Q. Peng",
            "D. P. Schissel",
            "M. Thompson",
            "I. Foster",
            "M. Greenwald",
            "D. McCune"
        ],
        "summary": "The National Fusion Collaboratory (NFC) project was created to advance scientific understanding and innovation in magnetic fusion research by enabling more efficient use of existing experimental facilities through more effective integration of experiment, theory, and modeling. To achieve this objective, NFC introduced the concept of \"network services\", which build on top of computational Grids, and provide Fusion codes, together with their maintenance and hardware resources as a service to the community. This mode of operation requires the development of new authorization and enforcement capabilities. In addition, the nature of Fusion experiments places strident quality of service requirements on codes run during the experimental cycle. In this paper, we describe Grid computing requirements of the Fusion community, and present our first experiments in meeting those requirements.",
        "published": "2003-01-28T20:11:16Z",
        "link": "http://arxiv.org/abs/cs/0301033v1",
        "categories": [
            "cs.DC",
            "D.2.2; J.2"
        ]
    },
    {
        "title": "On the Complexity of Buffer Allocation in Message Passing Systems",
        "authors": [
            "Alex Brodsky",
            "Jan B. Pedersen",
            "Alan Wagner"
        ],
        "summary": "Message passing programs commonly use buffers to avoid unnecessary synchronizations and to improve performance by overlapping communication with computation. Unfortunately, using buffers makes the program no longer portable, potentially unable to complete on systems without a sufficient number of buffers. Effective buffer use entails that the minimum number needed for a safe execution be allocated.   We explore a variety of problems related to buffer allocation for safe and efficient execution of message passing programs. We show that determining the minimum number of buffers or verifying a buffer assignment are intractable problems. However, we give a polynomial time algorithm to determine the minimum number of buffers needed to allow for asynchronous execution. We extend these results to several different buffering schemes, which in some cases make the problems tractable.",
        "published": "2003-01-31T01:21:21Z",
        "link": "http://arxiv.org/abs/cs/0301035v1",
        "categories": [
            "cs.DC",
            "D.1.3;D.4.2;D.4.4;F.1.2;F.1.3"
        ]
    },
    {
        "title": "Suppressing Roughness of Virtual Times in Parallel Discrete-Event   Simulations",
        "authors": [
            "G. Korniss",
            "M. A. Novotny",
            "H. Guclu",
            "Z. Toroczkai",
            "P. A. Rikvold"
        ],
        "summary": "In a parallel discrete-event simulation (PDES) scheme, tasks are distributed among processing elements (PEs), whose progress is controlled by a synchronization scheme. For lattice systems with short-range interactions, the progress of the conservative PDES scheme is governed by the Kardar-Parisi-Zhang equation from the theory of non-equilibrium surface growth. Although the simulated (virtual) times of the PEs progress at a nonzero rate, their standard deviation (spread) diverges with the number of PEs, hindering efficient data collection. We show that weak random interactions among the PEs can make this spread nondivergent. The PEs then progress at a nonzero, near-uniform rate without requiring global synchronizations.",
        "published": "2003-02-03T17:30:21Z",
        "link": "http://arxiv.org/abs/cond-mat/0302050v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Grid Market Directory: A Web Services based Grid Service Publication   Directory",
        "authors": [
            "Jia Yu",
            "Srikumar Venugopal",
            "Rajkumar Buyya"
        ],
        "summary": "As Grids are emerging as the next generation service-oriented computing platforms, they need to support Grid economy that helps in the management of supply and demand for resources and offers an economic incentive for Grid resource providers. To enable this Grid economy, a market-like Grid environment including an infrastructure that supports the publication of services and their discovery is needed. As part of the Gridbus project, we proposed and have developed a Grid Market Directory (GMD) that serves as a registry for high-level service publication and discovery in Virtual Organisations.",
        "published": "2003-02-06T03:31:02Z",
        "link": "http://arxiv.org/abs/cs/0302006v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "G-Monitor: Gridbus web portal for monitoring and steering application   execution on global grids",
        "authors": [
            "Martin Placek",
            "Rajkumar Buyya"
        ],
        "summary": "Grids are experiencing a rapid growth in their application and along with this there is a requirement for a portal which is easy to use and scalable. We have responded to this requirement by developing an easy to use, scalable, web-based portal called G-Monitor. This paper proposes a generic architecture for a web portal into a grid environment and discusses our implementation and its application.",
        "published": "2003-02-06T03:36:29Z",
        "link": "http://arxiv.org/abs/cs/0302007v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Visual Environment for Rapid Composition of Parameter-Sweep Applications   for Distributed Processing on Global Grids",
        "authors": [
            "Shoib Burq",
            "Steve Melnikoff",
            "Kim Branson",
            "Rajkumar Buyya"
        ],
        "summary": "Computational Grids are emerging as a platform for next-generation parallel and distributed computing. Large-scale parametric studies and parameter sweep applications find a natural place in the Grid?s distribution model. There is little or no communication between jobs. The task of parallelizing and distributing existing applications is conceptually trivial. These properties of parametric studies make it an ideal place to start developing integrated development environments (IDEs) for rapidly Grid-enabling applications. However, the availability of IDEs for scientists to Grid-enable their applications, without the need of developing them as parallel applications explicitly, is still lacking. This paper presents a Java based IDE called Visual Parametric Tool (VPT), developed as part of the Gridbus project, for rapid creation of parameter sweep (data parallel/SPMD) applications. It supports automatic creation of parameter script and parameterisation of the input data files, which is compatible with the Nimrod-G parameter specification language. The usefulness of VPT is demonstrated by a case study on composition of molecular docking application as a parameter sweep application. Such applications can be deployed on clusters using the Nimrod/enFuzion system and on global Grids using the Nimrod-G grid resource broker.",
        "published": "2003-02-06T04:06:02Z",
        "link": "http://arxiv.org/abs/cs/0302008v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Authenticated Append-only Skip Lists",
        "authors": [
            "Petros Maniatis",
            "Mary Baker"
        ],
        "summary": "In this work we describe, design and analyze the security of a tamper-evident, append-only data structure for maintaining secure data sequences in a loosely coupled distributed system where individual system components may be mutually distrustful. The resulting data structure, called an Authenticated Append-Only Skip List (AASL), allows its maintainers to produce one-way digests of the entire data sequence, which they can publish to others as a commitment on the contents and order of the sequence. The maintainer can produce efficiently succinct proofs that authenticate a particular datum in a particular position of the data sequence against a published digest. AASLs are secure against tampering even by malicious data structure maintainers. First, we show that a maintainer cannot ``invent'' and authenticate data elements for the AASL after he has committed to the structure. Second, he cannot equivocate by being able to prove conflicting facts about a particular position of the data sequence. This is the case even when the data sequence grows with time and its maintainer publishes successive commitments at times of his own choosing.   AASLs can be invaluable in reasoning about the integrity of system logs maintained by untrusted components of a loosely-coupled distributed system.",
        "published": "2003-02-07T01:51:54Z",
        "link": "http://arxiv.org/abs/cs/0302010v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "K.6.5; D.4.6; C.2.4"
        ]
    },
    {
        "title": "Guided Google: A Meta Search Engine and its Implementation using the   Google Distributed Web Services",
        "authors": [
            "Ding Choon Hoong",
            "Rajkumar Buyya"
        ],
        "summary": "With the advent of the Internet, search engines have begun sprouting like mushrooms after a rainfall. Only in recent years, have developers become more innovative, and came up with guided searching facilities online. The goals of these applications are to help ease and guide the searching efforts of a novice web user toward their desired objectives. A number of implementations of such services are emerging. This paper proposes a guided meta-search engine, called \"Guided Google\", as it serves as an interface to the actual Google.com search engine, using the Google Web Services.",
        "published": "2003-02-13T04:49:26Z",
        "link": "http://arxiv.org/abs/cs/0302018v1",
        "categories": [
            "cs.DC",
            "C.2.1"
        ]
    },
    {
        "title": "Economic and On Demand Brain Activity Analysis on Global Grids",
        "authors": [
            "R. Buyya",
            "S. Date",
            "Y. Mizuno-Matsumoto",
            "S. Venugopal",
            "D. Abramson"
        ],
        "summary": "The lack of computational power within an organization for analyzing scientific data, and the distribution of knowledge (by scientists) and technologies (advanced scientific devices) are two major problems commonly observed in scientific disciplines. One such scientific discipline is brain science. The analysis of brain activity data gathered from the MEG (Magnetoencephalography) instrument is an important research topic in medical science since it helps doctors in identifying symptoms of diseases. The data needs to be analyzed exhaustively to efficiently diagnose and analyze brain functions and requires access to large-scale computational resources. The potential platform for solving such resource intensive applications is the Grid. This paper describes a MEG data analysis system developed by us, leveraging Grid technologies, primarily Nimrod-G, Gridbus, and Globus. This paper explains the application of economy-based grid scheduling algorithms to the problem domain for on-demand processing of analysis jobs.",
        "published": "2003-02-13T04:53:10Z",
        "link": "http://arxiv.org/abs/cs/0302019v1",
        "categories": [
            "cs.DC",
            "C.2.1, J.3"
        ]
    },
    {
        "title": "Analytical formulations of Peer-to-Peer Connection Efficiency",
        "authors": [
            "Aaron Harwood"
        ],
        "summary": "Use of Peer-to-Peer (P2P) service networks introduces a new communication paradigm because peers are both clients and servers and so each peer may provide/request services to/from other peers. Empirical studies of P2P networks have been undertaken and reveal useful characteristics. However there is to date little analytical work to describe P2P networks with respect to their communication paradigm and their interconnections. This paper provides an analytical formulation and optimisation of peer connection efficiency, in terms of minimising the fraction of wasted connection time. Peer connection efficiency is analysed for both a uni- and multi-connected peer. Given this fundamental optimisation, the paper optimises the number of connections that peers should make use of as a function of network load, in terms of minimising the total queue size that requests in the P2P network experience. The results of this paper provide a basis for engineering high performance P2P interconnection networks. The optimisations are useful for reducing bandwidth and power consumption, e.g. in the case of peers being mobile devices with a limited power supply. Also these results could be used to determine when a (virtual) circuit should be switched to support a connection.",
        "published": "2003-02-13T10:13:01Z",
        "link": "http://arxiv.org/abs/cs/0302020v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.NI",
            "B.4.3; B.4.4; C.2.4"
        ]
    },
    {
        "title": "Fault-tolerant routing in peer-to-peer systems",
        "authors": [
            "James Aspnes",
            "Zoe Diamadi",
            "Gauri Shah"
        ],
        "summary": "We consider the problem of designing an overlay network and routing mechanism that permits finding resources efficiently in a peer-to-peer system. We argue that many existing approaches to this problem can be modeled as the construction of a random graph embedded in a metric space whose points represent resource identifiers, where the probability of a connection between two nodes depends only on the distance between them in the metric space. We study the performance of a peer-to-peer system where nodes are embedded at grid points in a simple metric space: a one-dimensional real line. We prove upper and lower bounds on the message complexity of locating particular resources in such a system, under a variety of assumptions about failures of either nodes or the connections between them. Our lower bounds in particular show that the use of inverse power-law distributions in routing, as suggested by Kleinberg (1999), is close to optimal. We also give efficient heuristics to dynamically maintain such a system as new nodes arrive and old nodes depart. Finally, we give experimental results that suggest promising directions for future work.",
        "published": "2003-02-15T17:15:46Z",
        "link": "http://arxiv.org/abs/cs/0302022v2",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; C.2.4; E.1"
        ]
    },
    {
        "title": "A first approach for a possible cellular automaton model of fluids   dynamics",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper I present a first attempt for a possible description of fluids dynamics by mean of a cellular automata technique. With the use of simple and elementary rules, based on random behaviour either, the model permits to obtain the evolution in time for a two-dimensional grid, where one molecule of the material fluid can ideally place itself on a single geometric square. By mean of computational simulations, some realistic effects, here showed by use of digital pictures, have been obtained. In a subsequent step of this work I think to use a parallel program for a high performances computational simulation, for increasing the degree of realism of the digital rendering by mean of a three-dimensional grid too. For the execution of the simulations, numerical methods of resolution for differential equations have not been used.",
        "published": "2003-03-06T00:02:11Z",
        "link": "http://arxiv.org/abs/cs/0303003v1",
        "categories": [
            "cs.CC",
            "cs.DC",
            "nlin.CG",
            "physics.comp-ph",
            "F.1.1"
        ]
    },
    {
        "title": "Fair Solution to the Reader-Writer-Problem with Semaphores only",
        "authors": [
            "H. Ballhausen"
        ],
        "summary": "The reader-writer-problem is a standard problem in concurrent programming. A resource is shared by several processes which need either inclusive reading or exclusive writing access. The known solutions to this problem typically involve a number of global counters and queues. Here a very simple algorithm is presented which needs only two semaphores for synchronisation and no other global objects. The approach yields a fair solution without starving.",
        "published": "2003-03-08T17:15:00Z",
        "link": "http://arxiv.org/abs/cs/0303005v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "Techniques and Applications of Computation Slicing",
        "authors": [
            "Neeraj Mittal",
            "Vijay K. Garg"
        ],
        "summary": "Writing correct distributed programs is hard. In spite of extensive testing and debugging, software faults persist even in commercial grade software. Many distributed systems, especially those employed in safety-critical environments, should be able to operate properly even in the presence of software faults. Monitoring the execution of a distributed system, and, on detecting a fault, initiating the appropriate corrective action is an important way to tolerate such faults. This gives rise to the predicate detection problem which requires finding a consistent cut of a given computation that satisfies a given global predicate, if it exists.   Detecting a predicate in a computation is, however, an NP-complete problem. To ameliorate the associated combinatorial explosion problem, we introduce the notion of computation slice. Formally, the slice of a computation with respect to a predicate is a (sub)computation with the least number of consistent cuts that contains all consistent cuts of the computation satisfying the predicate. To detect a predicate, rather than searching the state-space of the computation, it is much more efficient to search the state-space of the slice.   We prove that the slice exists and is uniquely defined for all predicates. We present efficient slicing algorithms for several useful classes of predicates. We develop efficient heuristic algorithms for computing an approximate slice for predicates for which computing the slice is otherwise provably intractable. Our experimental results show that slicing can lead to an exponential improvement over existing techniques for predicate detection in terms of time and space.",
        "published": "2003-03-15T07:41:07Z",
        "link": "http://arxiv.org/abs/cs/0303010v1",
        "categories": [
            "cs.DC",
            "cs.SE",
            "C.2.4; D.4.5; D.2.2"
        ]
    },
    {
        "title": "Lock-free dynamic hash tables with open addressing",
        "authors": [
            "Hui Gao",
            "Jan Friso Groote",
            "Wim H. Hesselink"
        ],
        "summary": "We present an efficient lock-free algorithm for parallel accessible hash tables with open addressing, which promises more robust performance and reliability than conventional lock-based implementations. ``Lock-free'' means that it is guaranteed that always at least one process completes its operation within a bounded number of steps. For a single processor architecture our solution is as efficient as sequential hash tables. On a multiprocessor architecture this is also the case when all processors have comparable speeds. The algorithm allows processors that have widely different speeds or come to a halt. It can easily be implemented using C-like languages and requires on average only constant time for insertion, deletion or accessing of elements. The algorithm allows the hash tables to grow and shrink when needed.   Lock-free algorithms are hard to design correctly, even when apparently straightforward. Ensuring the correctness of the design at the earliest possible stage is a major challenge in any responsible system development. In view of the complexity of the algorithm, we turned to the interactive theorem prover PVS for mechanical support. We employ standard deductive verification techniques to prove around 200 invariance properties of our algorithm, and describe how this is achieved with the theorem prover PVS.",
        "published": "2003-03-18T09:38:34Z",
        "link": "http://arxiv.org/abs/cs/0303011v4",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.1"
        ]
    },
    {
        "title": "Fast Parallel I/O on Cluster Computers",
        "authors": [
            "Thomas Duessel",
            "Norbert Eicker",
            "Florin Isaila",
            "Thomas Lippert",
            "Thomas Moschny",
            "Hartmut Neff",
            "Klaus Schilling",
            "Walter Tichy"
        ],
        "summary": "Today's cluster computers suffer from slow I/O, which slows down I/O-intensive applications. We show that fast disk I/O can be achieved by operating a parallel file system over fast networks such as Myrinet or Gigabit Ethernet.   In this paper, we demonstrate how the ParaStation3 communication system helps speed-up the performance of parallel I/O on clusters using the open source parallel virtual file system (PVFS) as testbed and production system. We will describe the set-up of PVFS on the Alpha-Linux-Cluster-Engine (ALiCE) located at Wuppertal University, Germany. Benchmarks on ALiCE achieve write-performances of up to 1 GB/s from a 32-processor compute-partition to a 32-processor PVFS I/O-partition, outperforming known benchmark results for PVFS on the same network by more than a factor of 2. Read-performance from buffer-cache reaches up to 2.2 GB/s. Our benchmarks are giant, I/O-intensive eigenmode problems from lattice quantum chromodynamics, demonstrating stability and performance of PVFS over Parastation in large-scale production runs.",
        "published": "2003-03-19T11:08:44Z",
        "link": "http://arxiv.org/abs/cs/0303016v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "hep-lat",
            "B.4.3; C.1.2; C.2.2; D.4.3"
        ]
    },
    {
        "title": "Preserving Peer Replicas By Rate-Limited Sampled Voting in LOCKSS",
        "authors": [
            "Petros Maniatis",
            "Mema Roussopoulos",
            "TJ Giuli",
            "David S. H. Rosenthal",
            "Mary Baker",
            "Yanto Muliadi"
        ],
        "summary": "The LOCKSS project has developed and deployed in a world-wide test a peer-to-peer system for preserving access to journals and other archival information published on the Web. It consists of a large number of independent, low-cost, persistent web caches that cooperate to detect and repair damage to their content by voting in \"opinion polls.\" Based on this experience, we present a design for and simulations of a novel protocol for voting in systems of this kind. It incorporates rate limitation and intrusion detection to ensure that even some very powerful adversaries attacking over many years have only a small probability of causing irrecoverable damage before being detected.",
        "published": "2003-03-25T20:11:57Z",
        "link": "http://arxiv.org/abs/cs/0303026v3",
        "categories": [
            "cs.DC",
            "cs.DL",
            "C.2.4; H.3.7; D.4.5"
        ]
    },
    {
        "title": "A Bird's eye view of Matrix Distributed Processing",
        "authors": [
            "Massimo Di Pierro"
        ],
        "summary": "We present Matrix Distributed Processing, a C++ library for fast development of efficient parallel algorithms. MDP is based on MPI and consists of a collection of C++ classes and functions such as lattice, site and field. Once an algorithm is written using these components the algorithm is automatically parallel and no explicit call to communication functions is required. MDP is particularly suitable for implementing parallel solvers for multi-dimensional differential equations and mesh-like problems.",
        "published": "2003-03-28T20:47:50Z",
        "link": "http://arxiv.org/abs/cs/0303031v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.DM",
            "cs.MS",
            "hep-lat",
            "physics.comp-ph",
            "D.1.3; D.3.2;G.1;G.4;I.6.8"
        ]
    },
    {
        "title": "A Digital Preservation Appliance Based on OpenBSD",
        "authors": [
            "David S. H. Rosenthal"
        ],
        "summary": "The LOCKSS program has developed and deployed in a world-wide test a system for preserving access to academic journals published on the Web. The fundamental problem for any digital preservation system is that it must be affordable for the long term. To reduce the cost of ownership, the LOCKSS system uses generic PC hardware, open source software, and peer-to-peer technology. It is packaged as a ``network appliance'', a single-function box that can be connected to the Internet, configured and left alone to do its job with minimal monitoring or administration. The first version of this system was based on a Linux boot floppy. After three years of testing it was replaced by a second version, based on OpenBSD and booting from CD-ROM.   We focus in this paper on the design, implementation and deployment of a network appliance based on an open source operating system. We provide an overview of the LOCKSS application and describe the experience of deploying and supporting its first version. We list the requirements we took from this to drive the design of the second version, describe how we satisfied them in the OpenBSD environment, and report on the initial",
        "published": "2003-03-30T18:46:46Z",
        "link": "http://arxiv.org/abs/cs/0303033v3",
        "categories": [
            "cs.DC",
            "cs.DL",
            "D.4.5"
        ]
    },
    {
        "title": "Individual Communication Complexity",
        "authors": [
            "Harry Buhrman",
            "Hartmut Klauck",
            "Nikolai Vereshchagin",
            "Paul Vitanyi"
        ],
        "summary": "We initiate the theory of communication complexity of individual inputs held by the agents, rather than worst-case or average-case. We consider total, partial, and partially correct protocols, one-way versus two-way, with and without help bits. The results are expressed in trems of Kolmogorov complexity.",
        "published": "2003-04-08T15:41:27Z",
        "link": "http://arxiv.org/abs/cs/0304012v1",
        "categories": [
            "cs.CC",
            "cs.DC",
            "F.1; F.2"
        ]
    },
    {
        "title": "Message Passing Fluids: molecules as processes in parallel computational   fluids",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper we present the concept of MPF, Message Passing Fluid, an abstract fluid where the molecules move by mean of the informations that they exchange each other, on the basis of rules and methods of a generalized Cellular Automaton. The model is intended for its simulation by mean of message passing libraries on the field of parallel computing. We present a critical analysis of the necessary computational effort in a possible implementation of such an object.",
        "published": "2003-04-10T17:23:11Z",
        "link": "http://arxiv.org/abs/physics/0304041v1",
        "categories": [
            "physics.flu-dyn",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Multiparty Quantum Coin Flipping",
        "authors": [
            "Andris Ambainis",
            "Harry Buhrman",
            "Yevgeniy Dodis",
            "Hein Roehrig"
        ],
        "summary": "We investigate coin-flipping protocols for multiple parties in a quantum broadcast setting:   (1) We propose and motivate a definition for quantum broadcast. Our model of quantum broadcast channel is new.   (2) We discovered that quantum broadcast is essentially a combination of pairwise quantum channels and a classical broadcast channel. This is a somewhat surprising conclusion, but helps us in both our lower and upper bounds.   (3) We provide tight upper and lower bounds on the optimal bias epsilon of a coin which can be flipped by k parties of which exactly g parties are honest: for any 1 <= g <= k, epsilon = 1/2 - Theta(g/k).   Thus, as long as a constant fraction of the players are honest, they can prevent the coin from being fixed with at least a constant probability. This result stands in sharp contrast with the classical setting, where no non-trivial coin-flipping is possible when g <= k/2.",
        "published": "2003-04-16T11:24:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0304112v2",
        "categories": [
            "quant-ph",
            "cs.CR",
            "cs.DC"
        ]
    },
    {
        "title": "Grid-Enabling Natural Language Engineering By Stealth",
        "authors": [
            "Baden Hughes",
            "Steven Bird"
        ],
        "summary": "We describe a proposal for an extensible, component-based software architecture for natural language engineering applications. Our model leverages existing linguistic resource description and discovery mechanisms based on extended Dublin Core metadata. In addition, the application design is flexible, allowing disparate components to be combined to suit the overall application functionality. An application specification language provides abstraction from the programming environment and allows ease of interface with computational grids via a broker.",
        "published": "2003-04-22T02:51:51Z",
        "link": "http://arxiv.org/abs/cs/0304028v1",
        "categories": [
            "cs.DC",
            "cs.CL",
            "J.5; D.1; C.2"
        ]
    },
    {
        "title": "Using Regression Techniques to Predict Large Data Transfers",
        "authors": [
            "Sudharshan Vazhkudai",
            "Jennifer M. Schopf"
        ],
        "summary": "The recent proliferation of Data Grids and the increasingly common practice of using resources as distributed data stores provide a convenient environment for communities of researchers to share, replicate, and manage access to copies of large datasets. This has led to the question of which replica can be accessed most efficiently. In such environments, fetching data from one of the several replica locations requires accurate predictions of end-to-end transfer times. The answer to this question can depend on many factors, including physical characteristics of the resources and the load behavior on the CPUs, networks, and storage devices that are part of the end-to-end data path linking possible sources and sinks. Our approach combines end-to-end application throughput observations with network and disk load variations and captures whole-system performance and variations in load patterns. Our predictions characterize the effect of load variations of several shared devices (network and disk) on file transfer times. We develop a suite of univariate and multivariate predictors that can use multiple data sources to improve the accuracy of the predictions as well as address Data Grid variations (availability of data and sporadic nature of transfers). We ran a large set of data transfer experiments using GridFTP and observed performance predictions within 15% error for our testbed sites, which is quite promising for a pragmatic system.",
        "published": "2003-04-23T20:36:09Z",
        "link": "http://arxiv.org/abs/cs/0304037v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Towards automation of computing fabrics using tools from the fabric   management workpackage of the EU DataGrid project",
        "authors": [
            "Olof Barring"
        ],
        "summary": "The EU DataGrid project workpackage 4 has as an objective to provide the necessary tools for automating the management of medium size to very large computing fabrics. At the end of the second project year subsystems for centralized configuration management (presented at LISA'02) and performance/exception monitoring have been delivered. This will soon be augmented with a subsystem for node installation and service configuration, which is based on existing widely used standards where available (e.g. rpm, kickstart, init.d scripts) and clean interfaces to OS dependent components (e.g. base installation and service management). The three subsystems together allow for centralized management of very large computer farms. Finally, a fault tolerance system is being developed for tying together the above subsystems to form a complete framework for automated enterprise computing management by 3Q03. All software developed is open source covered by the EU DataGrid project license agreements. This article describes the architecture behind the designed fabric management system and the status of the different developments. It also covers the experience with an existing tool for automated configuration and installation that have been adapted and used from the beginning to manage the EU DataGrid testbed, which is now used for LHC data challenges.",
        "published": "2003-05-28T16:25:45Z",
        "link": "http://arxiv.org/abs/cs/0305050v2",
        "categories": [
            "cs.DC",
            "K.6.0; K.6.2; K.6.4"
        ]
    },
    {
        "title": "ATLAS and CMS applications on the WorldGrid testbed",
        "authors": [
            "V. Ciaschini",
            "F. Donno",
            "A. Fanfani",
            "F. Fanzago",
            "V. Garbellotto",
            "M. Verlato",
            "L. Vaccarossa"
        ],
        "summary": "WorldGrid is an intercontinental testbed spanning Europe and the US integrating architecturally different Grid implementations based on the Globus toolkit. It has been developed in the context of the DataTAG and iVDGL projects, and successfully demonstrated during the WorldGrid demos at IST2002 (Copenhagen) and SC2002 (Baltimore). Two HEP experiments, ATLAS and CMS, successful exploited the WorldGrid testbed for executing jobs simulating the response of their detectors to physics eve nts produced by real collisions expected at the LHC accelerator starting from 2007. This data intensive activity has been run since many years on local dedicated computing farms consisting of hundreds of nodes and Terabytes of disk and tape storage. Within the WorldGrid testbed, for the first time HEP simulation jobs were submitted and run indifferently on US and European resources, despite of their underlying different Grid implementations, and produced data which could be retrieved and further analysed on the submitting machine, or simply stored on the remote resources and registered on a Replica Catalogue which made them available to the Grid for further processing. In this contribution we describe the job submission from Europe for both ATLAS and CMS applications, performed through the GENIUS portal operating on top of an EDG User Interface submitting to an EDG Resource Broker, pointing out the chosen interoperability solutions which made US and European resources equivalent from the applications point of view, the data management in the WorldGrid environment, and the CMS specific production tools which were interfaced to the GENIUS portal.",
        "published": "2003-05-30T09:18:55Z",
        "link": "http://arxiv.org/abs/cs/0305058v2",
        "categories": [
            "cs.DC",
            "A0"
        ]
    },
    {
        "title": "EU DataGRID testbed management and support at CERN",
        "authors": [
            "E. Leonardi",
            "M. W. Schulz"
        ],
        "summary": "In this paper we report on the first two years of running the CERN testbed site for the EU DataGRID project. The site consists of about 120 dual-processor PCs distributed over several testbeds used for different purposes: software development, system integration, and application tests. Activities at the site included test productions of MonteCarlo data for LHC experiments, tutorials and demonstrations of GRID technologies, and support for individual users analysis. This paper focuses on node installation and configuration techniques, service management, user support in a gridified environment, and includes considerations on scalability and security issues and comparisons with \"traditional\" production systems, as seen from the administrator point of view.",
        "published": "2003-05-30T10:07:12Z",
        "link": "http://arxiv.org/abs/cs/0305059v1",
        "categories": [
            "cs.DC",
            "C.1.4; C.2.4; D.4.7"
        ]
    },
    {
        "title": "A Secure Infrastructure For System Console and Reset Access",
        "authors": [
            "Andras Horvath",
            "Emanuele Leonardi",
            "Markus Schulz"
        ],
        "summary": "During the last years large farms have been built using commodity hardware. This hardware lacks components for remote and automated administration. Products that can be retrofitted to these systems are either costly or inherently insecure. We present a system based on serial ports and simple machine controlled relays. We report on experience gained by setting up a 50-machine test environment as well as current work in progress in the area.",
        "published": "2003-05-30T11:42:37Z",
        "link": "http://arxiv.org/abs/cs/0305061v1",
        "categories": [
            "cs.DC",
            "K.6.4; C.2.4"
        ]
    },
    {
        "title": "DIAMOnDS - DIstributed Agents for MObile & Dynamic Services",
        "authors": [
            "Aamir Shafi",
            "Umer Farooq",
            "Saad Kiani",
            "Maria Riaz",
            "Anjum Shehzad",
            "Arshad Ali",
            "Iosif Legrand",
            "Harvey Newman"
        ],
        "summary": "Distributed Services Architecture with support for mobile agents between services, offer significantly improved communication and computational flexibility. The uses of agents allow execution of complex operations that involve large amounts of data to be processed effectively using distributed resources. The prototype system Distributed Agents for Mobile and Dynamic Services (DIAMOnDS), allows a service to send agents on its behalf, to other services, to perform data manipulation and processing. Agents have been implemented as mobile services that are discovered using the Jini Lookup mechanism and used by other services for task management and communication. Agents provide proxies for interaction with other services as well as specific GUI to monitor and control the agent activity. Thus agents acting on behalf of one service cooperate with other services to carry out a job, providing inter-operation of loosely coupled services in a semi-autonomous way. Remote file system access functionality has been incorporated by the agent framework and allows services to dynamically share and browse the file system resources of hosts, running the services. Generic database access functionality has been implemented in the mobile agent framework that allows performing complex data mining and processing operations efficiently in distributed system. A basic data searching agent is also implemented that performs a query based search in a file system. The testing of the framework was carried out on WAN by moving Connectivity Test agents between AgentStations in CERN, Switzerland and NUST, Pakistan.",
        "published": "2003-05-30T11:48:57Z",
        "link": "http://arxiv.org/abs/cs/0305062v2",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "A Generic Multi-node State Monitoring Subsystem",
        "authors": [
            "James A. Hamilton",
            "Gregory P. Dubois-Felsmann",
            "Rainer Bartoldus"
        ],
        "summary": "The BaBar online data acquisition (DAQ) system includes approximately fifty Unix systems that collectively implement the level-three trigger. These systems all run the same code. Each of these systems has its own state, and this state is expected to change in response to changes in the overall DAQ system. A specialized subsystem has been developed to initiate processing on this collection of systems, and to monitor them both for error conditions and to ensure that they all follow the same state trajectory within a specifiable period of time. This subsystem receives start commands from the main DAQ run control system, and reports major coherent state changes, as well as error conditions, back to the run control system. This state monitoring subsystem has the novel feature that it does not know anything about the state machines that it is monitoring, and hence does not introduce any fundamentally new state machine into the overall system. This feature makes it trivially applicable to other multi-node subsystems. Indeed it has already found a second application beyond the level-three trigger, within the BaBar experiment.",
        "published": "2003-05-30T17:43:03Z",
        "link": "http://arxiv.org/abs/cs/0305065v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "The CMS Integration Grid Testbed",
        "authors": [
            "Gregory E. Graham",
            "M. Anzar Afaq",
            "Shafqat Aziz",
            "L. A. T. Bauerdick",
            "Michael Ernst",
            "Joseph Kaiser",
            "Natalia Ratnikova",
            "Hans Wenzel",
            "Yujun Wu",
            "Erik Aslakson",
            "Julian Bunn",
            "Saima Iqbal",
            "Iosif Legrand",
            "Harvey Newman",
            "Suresh Singh",
            "Conrad Steenberg",
            "James Branson",
            "Ian Fisk",
            "James Letts",
            "Adam Arbree",
            "Paul Avery",
            "Dimitri Bourilkov",
            "Richard Cavanaugh",
            "Jorge Rodriguez",
            "Suchindra Kategari",
            "Peter Couvares",
            "Alan DeSmet",
            "Miron Livny",
            "Alain Roy",
            "Todd Tannenbaum"
        ],
        "summary": "The CMS Integration Grid Testbed (IGT) comprises USCMS Tier-1 and Tier-2 hardware at the following sites: the California Institute of Technology, Fermi National Accelerator Laboratory, the University of California at San Diego, and the University of Florida at Gainesville. The IGT runs jobs using the Globus Toolkit with a DAGMan and Condor-G front end. The virtual organization (VO) is managed using VO management scripts from the European Data Grid (EDG). Gridwide monitoring is accomplished using local tools such as Ganglia interfaced into the Globus Metadata Directory Service (MDS) and the agent based Mona Lisa. Domain specific software is packaged and installed using the Distrib ution After Release (DAR) tool of CMS, while middleware under the auspices of the Virtual Data Toolkit (VDT) is distributed using Pacman. During a continuo us two month span in Fall of 2002, over 1 million official CMS GEANT based Monte Carlo events were generated and returned to CERN for analysis while being demonstrated at SC2002. In this paper, we describe the process that led to one of the world's first continuously available, functioning grids.",
        "published": "2003-05-30T19:45:44Z",
        "link": "http://arxiv.org/abs/cs/0305066v2",
        "categories": [
            "cs.DC",
            "A.0; C.2.4"
        ]
    },
    {
        "title": "McRunjob: A High Energy Physics Workflow Planner for Grid Production   Processing",
        "authors": [
            "Gregory E. Graham",
            "Dave Evans",
            "Iain Bertram"
        ],
        "summary": "McRunjob is a powerful grid workflow manager used to manage the generation of large numbers of production processing jobs in High Energy Physics. In use at both the DZero and CMS experiments, McRunjob has been used to manage large Monte Carlo production processing since 1999 and is being extended to uses in regular production processing for analysis and reconstruction. Described at CHEP 2001, McRunjob converts core metadata into jobs submittable in a variety of environments. The powerful core metadata description language includes methods for converting the metadata into persistent forms, job descriptions, multi-step workflows, and data provenance information. The language features allow for structure in the metadata by including full expressions, namespaces, functional dependencies, site specific parameters in a grid environment, and ontological definitions. It also has simple control structures for parallelization of large jobs. McRunjob features a modular design which allows for easy expansion to new job description languages or new application level tasks.",
        "published": "2003-05-30T19:53:14Z",
        "link": "http://arxiv.org/abs/cs/0305063v2",
        "categories": [
            "cs.DC",
            "A.0; C.2.4"
        ]
    },
    {
        "title": "Clarens Client and Server Applications",
        "authors": [
            "Conrad D. Steenberg",
            "Eric Aslakson",
            "Julian J. Bunn",
            "Harvey B. Newman",
            "Michael Thomas",
            "Frank van Lingen"
        ],
        "summary": "Several applications have been implemented with access via the Clarens web service infrastructure, including virtual organization management, JetMET physics data analysis using relational databases, and Storage Resource Broker (SRB) access. This functionality is accessible transparently from Python scripts, the Root analysis framework and from Java applications and browser applets.",
        "published": "2003-05-30T20:25:50Z",
        "link": "http://arxiv.org/abs/cs/0306001v2",
        "categories": [
            "cs.DC",
            "H.3.4"
        ]
    },
    {
        "title": "The Clarens web services architecture",
        "authors": [
            "Conrad D. Steenberg",
            "Eric Aslakson",
            "Julian J. Bunn",
            "Harvey B. Newman",
            "Michael Thomas",
            "Frank van Lingen"
        ],
        "summary": "Clarens is a uniquely flexible web services infrastructure providing a unified access protocol to a diverse set of functions useful to the HEP community. It uses the standard HTTP protocol combined with application layer, certificate based authentication to provide single sign-on to individuals, organizations and hosts, with fine-grained access control to services, files and virtual organization (VO) management. This contribution describes the server functionality, while client applications are described in a subsequent talk.",
        "published": "2003-05-30T20:34:05Z",
        "link": "http://arxiv.org/abs/cs/0306002v2",
        "categories": [
            "cs.DC",
            "H.3.4"
        ]
    },
    {
        "title": "R-GMA: First results after deployment",
        "authors": [
            "Rob Byrom",
            "Brian Coghlan",
            "Andrew W Cooke",
            "Roney Cordenonsi",
            "Linda Cornwall",
            "Ari Datta",
            "Abdeslem Djaoui",
            "Laurence Field",
            "Steve Fisher",
            "Steve Hicks",
            "Stuart Kenny",
            "James Magowan",
            "Werner Nutt",
            "David O'Callaghan",
            "Manfred Oevers",
            "Norbert Podhorszki",
            "John Ryan",
            "Manish Soni",
            "Paul Taylor",
            "Antony J. Wilson",
            "Xiaomei Zhu"
        ],
        "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which is being developed within the European DataGrid Project as an Grid Information and Monitoring System. Is is based on the GMA from GGF, which is a simple Consumer-Producer model. The special strength of this implementation comes from the power of the relational model. We offer a global view of the information as if each VO had one large relational database. We provide a number of different Producer types with different characteristics; for example some support streaming of information. We also provide combined Consumer/Producers, which are able to combine information and republish it. At the heart of the system is the mediator, which for any query is able to find and connect to the best Producers to do the job. We are able to invoke MDS info-provider scripts and publish the resulting information via R-GMA in addition to having some of our own sensors. APIs are available which allow the user to deploy monitoring and information services for any application that may be needed in the future. We have used it both for information about the grid (primarily to find what services are available at any one time) and for application monitoring. R-GMA has been deployed in Grid testbeds, we describe the results and experiences of this deployment.",
        "published": "2003-05-30T20:39:27Z",
        "link": "http://arxiv.org/abs/cs/0306003v2",
        "categories": [
            "cs.DC",
            "H.2.4;H.m"
        ]
    },
    {
        "title": "Managing Dynamic User Communities in a Grid of Autonomous Resources",
        "authors": [
            "R. Alfieri",
            "R. Cecchini",
            "V. Ciaschini",
            "L. dell'Agnello",
            "A. Gianoli",
            "F. Spataro",
            "F. Bonnassieux",
            "P. Broadfoot",
            "G. Lowe",
            "L. Cornwall",
            "J. Jensen",
            "D. Kelsey",
            "A. Frohner",
            "D. L. Groep",
            "W. Som de Cerff",
            "M. Steenbakkers",
            "G. Venekamp",
            "D. Kouril",
            "A. McNab",
            "O. Mulmo",
            "M. Silander",
            "J. Hahkala",
            "K. Lhorentey"
        ],
        "summary": "One of the fundamental concepts in Grid computing is the creation of Virtual Organizations (VO's): a set of resource consumers and providers that join forces to solve a common problem. Typical examples of Virtual Organizations include collaborations formed around the Large Hadron Collider (LHC) experiments. To date, Grid computing has been applied on a relatively small scale, linking dozens of users to a dozen resources, and management of these VO's was a largely manual operation. With the advance of large collaboration, linking more than 10000 users with a 1000 sites in 150 counties, a comprehensive, automated management system is required. It should be simple enough not to deter users, while at the same time ensuring local site autonomy. The VO Management Service (VOMS), developed by the EU DataGrid and DataTAG projects[1, 2], is a secured system for managing authorization for users and resources in virtual organizations. It extends the existing Grid Security Infrastructure[3] architecture with embedded VO affiliation assertions that can be independently verified by all VO members and resource providers. Within the EU DataGrid project, Grid services for job submission, file- and database access are being equipped with fine- grained authorization systems that take VO membership into account. These also give resource owners the ability to ensure site security and enforce local access policies. This paper will describe the EU DataGrid security architecture, the VO membership service and the local site enforcement mechanisms Local Centre Authorization Service (LCAS), Local Credential Mapping Service(LCMAPS) and the Java Trust and Authorization Manager.",
        "published": "2003-05-30T21:04:49Z",
        "link": "http://arxiv.org/abs/cs/0306004v2",
        "categories": [
            "cs.DC",
            "C.2.4 ; D.4.6"
        ]
    },
    {
        "title": "The NorduGrid architecture and tools",
        "authors": [
            "P. Eerola",
            "T. Ekelof",
            "M. Ellert",
            "J. R. Hansen",
            "A. Konstantinov",
            "B. Konya",
            "J. L. Nielsen",
            "F. Ould-Saada",
            "O. Smirnova",
            "A. Waananen"
        ],
        "summary": "The NorduGrid project designed a Grid architecture with the primary goal to meet the requirements of production tasks of the LHC experiments. While it is meant to be a rather generic Grid system, it puts emphasis on batch processing suitable for problems encountered in High Energy Physics. The NorduGrid architecture implementation uses the \\globus{} as the foundation for various components, developed by the project. While introducing new services, the NorduGrid does not modify the Globus tools, such that the two can eventually co-exist. The NorduGrid topology is decentralized, avoiding a single point of failure. The NorduGrid architecture is thus a light-weight, non-invasive and dynamic one, while robust and scalable, capable of meeting most challenging tasks of High Energy Physics.",
        "published": "2003-05-31T04:05:19Z",
        "link": "http://arxiv.org/abs/physics/0306002v1",
        "categories": [
            "physics.comp-ph",
            "cs.DC"
        ]
    },
    {
        "title": "The first deployment of workload management services on the EU DataGrid   Testbed: feedback on design and implementation",
        "authors": [
            "G. Avellino",
            "S. Beco",
            "B. Cantalupo",
            "F. Pacini",
            "A. Terracina",
            "A. Maraschini",
            "D. Colling",
            "S. Monforte",
            "M. Pappalardo",
            "L. Salconi",
            "F. Giacomini",
            "E. Ronchieri",
            "D. Kouril",
            "A. Krenek",
            "L. Matyska",
            "M. Mulac",
            "J. Pospisil",
            "M. Ruda",
            "Z. Salvet",
            "J. Sitera",
            "M. Vocu",
            "M. Mezzadri",
            "F. Prelz",
            "A. Gianelle",
            "R. Peluso",
            "M. Sgaravatto",
            "S. Barale",
            "A. Guarise",
            "A. Werbrouck"
        ],
        "summary": "Application users have now been experiencing for about a year with the standardized resource brokering services provided by the 'workload management' package of the EU DataGrid project (WP1). Understanding, shaping and pushing the limits of the system has provided valuable feedback on both its design and implementation. A digest of the lessons, and \"better practices\", that were learned, and that were applied towards the second major release of the software, is given.",
        "published": "2003-05-31T19:39:54Z",
        "link": "http://arxiv.org/abs/cs/0306007v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.1"
        ]
    },
    {
        "title": "The new BaBar Data Reconstruction Control System",
        "authors": [
            "A. Ceseracciu",
            "M. Piemontese",
            "F. Safai Tehrani",
            "P. Elmer",
            "D. Johnson",
            "T. M. Pulliam"
        ],
        "summary": "The BaBar experiment is characterized by extremely high luminosity, a complex detector, and a huge data volume, with increasing requirements each year. To fulfill these requirements a new control system has been designed and developed for the offline data reconstruction system. The new control system described in this paper provides the performance and flexibility needed to manage a large number of small computing farms, and takes full benefit of OO design. The infrastructure is well isolated from the processing layer, it is generic and flexible, based on a light framework providing message passing and cooperative multitasking. The system is actively distributed, enforces the separation between different processing tiers by using different naming domains, and glues them together by dedicated brokers. It provides a powerful Finite State Machine framework to describe custom processing models in a simple regular language. This paper describes this new control system, currently in use at SLAC and Padova on ~450 CPUs organized in 12 farms.",
        "published": "2003-05-31T20:48:53Z",
        "link": "http://arxiv.org/abs/cs/0306008v3",
        "categories": [
            "cs.DC",
            "H.2.4"
        ]
    },
    {
        "title": "Virtual Data in CMS Production",
        "authors": [
            "A. Arbree",
            "P. Avery",
            "D. Bourilkov",
            "R. Cavanaugh",
            "G. Graham",
            "S. Katageri",
            "J. Rodriguez",
            "J. Voeckler",
            "M. Wilde"
        ],
        "summary": "Initial applications of the GriPhyN Chimera Virtual Data System have been performed within the context of CMS Production of Monte Carlo Simulated Data. The GriPhyN Chimera system consists of four primary components: 1) a Virtual Data Language, which is used to describe virtual data products, 2) a Virtual Data Catalog, which is used to store virtual data entries, 3) an Abstract Planner, which resolves all dependencies of a particular virtual data product and forms a location and existence independent plan, 4) a Concrete Planner, which maps an abstract, logical plan onto concrete, physical grid resources accounting for staging in/out files and publishing results to a replica location service. A CMS Workflow Planner, MCRunJob, is used to generate virtual data products using the Virtual Data Language. Subsequently, a prototype workflow manager, known as WorkRunner, is used to schedule the instantiation of virtual data products across a grid.",
        "published": "2003-05-31T21:07:57Z",
        "link": "http://arxiv.org/abs/cs/0306009v1",
        "categories": [
            "cs.DC",
            "hep-ex",
            "H.2.4"
        ]
    },
    {
        "title": "Grid Data Management in Action: Experience in Running and Supporting   Data Management Services in the EU DataGrid Project",
        "authors": [
            "Heinz Stockinger",
            "Flavia Donno",
            "Erwin Laure",
            "Shahzad Muzaffar",
            "Peter Kunszt",
            "Giuseppe Andronico",
            "Paul Millar"
        ],
        "summary": "In the first phase of the EU DataGrid (EDG) project, a Data Management System has been implemented and provided for deployment. The components of the current EDG Testbed are: a prototype of a Replica Manager Service built around the basic services provided by Globus, a centralised Replica Catalogue to store information about physical locations of files, and the Grid Data Mirroring Package (GDMP) that is widely used in various HEP collaborations in Europe and the US for data mirroring. During this year these services have been refined and made more robust so that they are fit to be used in a pre-production environment. Application users have been using this first release of the Data Management Services for more than a year. In the paper we present the components and their interaction, our implementation and experience as well as the feedback received from our user communities. We have resolved not only issues regarding integration with other EDG service components but also many of the interoperability issues with components of our partner projects in Europe and the U.S. The paper concludes with the basic lessons learned during this operation. These conclusions provide the motivation for the architecture of the next generation of Data Management Services that will be deployed in EDG during 2003.",
        "published": "2003-06-02T08:50:48Z",
        "link": "http://arxiv.org/abs/cs/0306011v1",
        "categories": [
            "cs.DC",
            "E.0"
        ]
    },
    {
        "title": "A monitoring tool for a GRID operation center",
        "authors": [
            "S. Andreozzi",
            "S. Fantinel",
            "D. Rebatto",
            "L. Vaccarossa",
            "G. Tortone"
        ],
        "summary": "WorldGRID is an intercontinental testbed spanning Europe and the US integrating architecturally different Grid implementations based on the Globus toolkit. The WorldGRID testbed has been successfully demonstrated during the WorldGRID demos at SuperComputing 2002 (Baltimore) and IST2002 (Copenhagen) where real HEP application jobs were transparently submitted from US and Europe using \"native\" mechanisms and run where resources were available, independently of their location. To monitor the behavior and performance of such testbed and spot problems as soon as they arise, DataTAG has developed the EDT-Monitor tool based on the Nagios package that allows for Virtual Organization centric views of the Grid through dynamic geographical maps. The tool has been used to spot several problems during the WorldGRID operations, such as malfunctioning Resource Brokers or Information Servers, sites not correctly configured, job dispatching problems, etc. In this paper we give an overview of the package, its features and scalability solutions and we report on the experience acquired and the benefit that a GRID operation center would gain from such a tool.",
        "published": "2003-06-03T17:22:07Z",
        "link": "http://arxiv.org/abs/cs/0306018v1",
        "categories": [
            "cs.DC",
            "C.2.3"
        ]
    },
    {
        "title": "HEP Applications Evaluation of the EDG Testbed and Middleware",
        "authors": [
            "I. Augustin",
            "F. Carminati",
            "J. Closier",
            "E. van Herwijnen",
            "J. J. Blaising",
            "D. Boutigny",
            "C. Charlot",
            "V. Garonne",
            "A. Tsaregorodtsev",
            "K. Bos",
            "J. Templon",
            "P. Capiluppi",
            "A. Fanfani",
            "R. Barbera",
            "G. Negri",
            "L. Perini",
            "S. Resconi",
            "M. Sitta",
            "M. Reale",
            "D. Vicinanza",
            "S. Bagnasco",
            "P. Cerello",
            "A. Sciaba",
            "O. Smirnova",
            "D. Colling",
            "F. Harris",
            "S. Burke"
        ],
        "summary": "Workpackage 8 of the European Datagrid project was formed in January 2001 with representatives from the four LHC experiments, and with experiment independent people from five of the six main EDG partners. In September 2002 WP8 was strengthened by the addition of effort from BaBar and D0. The original mandate of WP8 was, following the definition of short- and long-term requirements, to port experiment software to the EDG middleware and testbed environment. A major additional activity has been testing the basic functionality and performance of this environment. This paper reviews experiences and evaluations in the areas of job submission, data management, mass storage handling, information systems and monitoring. It also comments on the problems of remote debugging, the portability of code, and scaling problems with increasing numbers of jobs, sites and nodes. Reference is made to the pioneeering work of Atlas and CMS in integrating the use of the EDG Testbed into their data challenges. A forward look is made to essential software developments within EDG and to the necessary cooperation between EDG and LCG for the LCG prototype due in mid 2003.",
        "published": "2003-06-05T15:35:20Z",
        "link": "http://arxiv.org/abs/cs/0306027v1",
        "categories": [
            "cs.DC",
            "J.2"
        ]
    },
    {
        "title": "A Software Data Transport Framework for Trigger Applications on Clusters",
        "authors": [
            "Timm M. Steinbeck",
            "Volker Lindenstruth",
            "Heinz Tilsner"
        ],
        "summary": "In the future ALICE heavy ion experiment at CERN's Large Hadron Collider input data rates of up to 25 GB/s have to be handled by the High Level Trigger (HLT) system, which has to scale them down to at most 1.25 GB/s before being written to permanent storage. The HLT system that is being designed to cope with these data rates consists of a large PC cluster, up to the order of a 1000 nodes, connected by a fast network. For the software that will run on these nodes a flexible data transport and distribution software framework has been developed. This framework consists of a set of separate components, that can be connected via a common interface, allowing to construct different configurations for the HLT, that are even changeable at runtime. To ensure a fault-tolerant operation of the HLT, the framework includes a basic fail-over mechanism that will be further expanded in the future, utilizing the runtime reconnection feature of the framework's component interface. First performance tests show very promising results for the software, indicating that it can achieve an event rate for the data transport sufficiently high to satisfy ALICE's requirements.",
        "published": "2003-06-06T07:02:25Z",
        "link": "http://arxiv.org/abs/cs/0306029v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "Grid-based access control for Unix environments, Filesystems and Web   Sites",
        "authors": [
            "A. McNab"
        ],
        "summary": "The EU DataGrid has deployed a grid testbed at approximately 20 sites across Europe, with several hundred registered users. This paper describes authorisation systems produced by GridPP and currently used on the EU DataGrid Testbed, including local Unix pool accounts and fine-grained access control with Access Control Lists and Grid-aware filesystems, fileservers and web developement environments.",
        "published": "2003-06-06T10:45:43Z",
        "link": "http://arxiv.org/abs/cs/0306030v1",
        "categories": [
            "cs.DC",
            "K.6.5"
        ]
    },
    {
        "title": "Update statistics in conservative parallel discrete event simulations of   asynchronous systems",
        "authors": [
            "A. Kolakowska",
            "M. A. Novotny",
            "Per Arne Rikvold"
        ],
        "summary": "We model the performance of an ideal closed chain of L processing elements that work in parallel in an asynchronous manner. Their state updates follow a generic conservative algorithm. The conservative update rule determines the growth of a virtual time surface. The physics of this growth is reflected in the utilization (the fraction of working processors) and in the interface width. We show that it is possible to nake an explicit connection between the utilization and the macroscopic structure of the virtual time interface. We exploit this connection to derive the theoretical probability distribution of updates in the system within an approximate model. It follows that the theoretical lower bound for the computational speed-up is s=(L+1)/4 for L>3. Our approach uses simple statistics to count distinct surface configuration classes consistent with the model growth rule. It enables one to compute analytically microscopic properties of an interface, which are unavailable by continuum methods.",
        "published": "2003-06-09T20:33:06Z",
        "link": "http://arxiv.org/abs/cond-mat/0306222v1",
        "categories": [
            "cond-mat",
            "cs.DC",
            "F.1.1;F.1.2;G.3;I.6;C.4;B.4.4"
        ]
    },
    {
        "title": "Skip Graphs",
        "authors": [
            "James Aspnes",
            "Gauri Shah"
        ],
        "summary": "Skip graphs are a novel distributed data structure, based on skip lists, that provide the full functionality of a balanced tree in a distributed system where resources are stored in separate nodes that may fail at any time. They are designed for use in searching peer-to-peer systems, and by providing the ability to perform queries based on key ordering, they improve on existing search tools that provide only hash table functionality. Unlike skip lists or other tree data structures, skip graphs are highly resilient, tolerating a large fraction of failed nodes without losing connectivity. In addition, constructing, inserting new nodes into, searching a skip graph, and detecting and repairing errors in the data structure introduced by node failures can be done using simple and straightforward algorithms.",
        "published": "2003-06-10T23:14:16Z",
        "link": "http://arxiv.org/abs/cs/0306043v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "C.2.4; E.1"
        ]
    },
    {
        "title": "Compositional competitiveness for distributed algorithms",
        "authors": [
            "James Aspnes",
            "Orli Waarts"
        ],
        "summary": "We define a measure of competitive performance for distributed algorithms based on throughput, the number of tasks that an algorithm can carry out in a fixed amount of work. This new measure complements the latency measure of Ajtai et al., which measures how quickly an algorithm can finish tasks that start at specified times. The novel feature of the throughput measure, which distinguishes it from the latency measure, is that it is compositional: it supports a notion of algorithms that are competitive relative to a class of subroutines, with the property that an algorithm that is k-competitive relative to a class of subroutines, combined with an l-competitive member of that class, gives a combined algorithm that is kl-competitive.   In particular, we prove the throughput-competitiveness of a class of algorithms for collect operations, in which each of a group of n processes obtains all values stored in an array of n registers. Collects are a fundamental building block of a wide variety of shared-memory distributed algorithms, and we show that several such algorithms are competitive relative to collects. Inserting a competitive collect in these algorithms gives the first examples of competitive distributed algorithms obtained by composition using a general construction.",
        "published": "2003-06-11T03:13:50Z",
        "link": "http://arxiv.org/abs/cs/0306044v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.1.2; F.2.m"
        ]
    },
    {
        "title": "The WorldGrid transatlantic testbed: a successful example of Grid   interoperability across EU and U.S. domains",
        "authors": [
            "Flavia Donno",
            "Vincenzo Ciaschini",
            "David Rebatto",
            "Luca Vaccarossa",
            "Marco Verlato"
        ],
        "summary": "The European DataTAG project has taken a major step towards making the concept of a worldwide computing Grid a reality. In collaboration with the companion U.S. project iVDGL, DataTAG has realized an intercontinental testbed spanning Europe and the U.S. integrating architecturally different Grid implementations based on the Globus toolkit. The WorldGrid testbed has been successfully demonstrated at SuperComputing 2002 and IST2002 where real HEP application jobs were transparently submitted from U.S. and Europe using native mechanisms and run where resources were available, independently of their location. In this paper we describe the architecture of the WorldGrid testbed, the problems encountered and the solutions taken in realizing such a testbed. With our work we present an important step towards interoperability of Grid middleware developed and deployed in Europe and the U.S.. Some of the solutions developed in WorldGrid will be adopted by the LHC Computing Grid first service. To the best of our knowledge, this is the first large-scale testbed that combines middleware components and makes them work together.",
        "published": "2003-06-11T16:47:17Z",
        "link": "http://arxiv.org/abs/cs/0306045v1",
        "categories": [
            "cs.DC",
            "D.2.12"
        ]
    },
    {
        "title": "Parallel netCDF: A Scientific High-Performance I/O Interface",
        "authors": [
            "Jianwei Li",
            "Wei-keng Liao",
            "Alok Choudhary",
            "Robert Ross",
            "Rajeev Thakur",
            "William Gropp",
            "Rob Latham"
        ],
        "summary": "Dataset storage, exchange, and access play a critical role in scientific applications. For such purposes netCDF serves as a portable and efficient file format and programming interface, which is popular in numerous scientific application domains. However, the original interface does not provide an efficient mechanism for parallel data storage and access. In this work, we present a new parallel interface for writing and reading netCDF datasets. This interface is derived with minimum changes from the serial netCDF interface but defines semantics for parallel access and is tailored for high performance. The underlying parallel I/O is achieved through MPI-IO, allowing for dramatic performance gains through the use of collective I/O optimizations. We compare the implementation strategies with HDF5 and analyze both. Our tests indicate programming convenience and significant I/O performance improvement with this parallel netCDF interface.",
        "published": "2003-06-11T20:25:52Z",
        "link": "http://arxiv.org/abs/cs/0306048v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "A data Grid testbed environment in Gigabit WAN with HPSS",
        "authors": [
            "Atsushi Manabe",
            "Kohki Ishikawa",
            "Yoshihiko Itoh",
            "Setsuya Kawabata",
            "Tetsuro Mashimo",
            "Youhei Morita",
            "Hiroshi Sakamoto",
            "Takashi Sasaki",
            "Hiroyuki Sato",
            "Junichi Tanaka",
            "Ikuo Ueda",
            "Yoshiyuki Watase",
            "Satomi Yamamoto",
            "Shigeo Yashiro"
        ],
        "summary": "For data analysis of large-scale experiments such as LHC Atlas and other Japanese high energy and nuclear physics projects, we have constructed a Grid test bed at ICEPP and KEK. These institutes are connected to national scientific gigabit network backbone called SuperSINET. In our test bed, we have installed NorduGrid middleware based on Globus, and connected 120TB HPSS at KEK as a large scale data store. Atlas simulation data at ICEPP has been transferred and accessed using SuperSINET. We have tested various performances and characteristics of HPSS through this high speed WAN. The measurement includes comparison between computing and storage resources are tightly coupled with low latency LAN and long distant WAN.",
        "published": "2003-06-12T08:48:16Z",
        "link": "http://arxiv.org/abs/cs/0306051v2",
        "categories": [
            "cs.DC",
            "C.2.4;J.2;H.3.4"
        ]
    },
    {
        "title": "ATLAS Data Challenge 1",
        "authors": [
            "Gilbert Poulard"
        ],
        "summary": "In 2002 the ATLAS experiment started a series of Data Challenges (DC) of which the goals are the validation of the Computing Model, of the complete software suite, of the data model, and to ensure the correctness of the technical choices to be made. A major feature of the first Data Challenge (DC1) was the preparation and the deployment of the software required for the production of large event samples for the High Level Trigger (HLT) and physics communities, and the production of those samples as a world-wide distributed activity. The first phase of DC1 was run during summer 2002, and involved 39 institutes in 18 countries. More than 10 million physics events and 30 million single particle events were fully simulated. Over a period of about 40 calendar days 71000 CPU-days were used producing 30 Tbytes of data in about 35000 partitions. In the second phase the next processing step was performed with the participation of 56 institutes in 21 countries (~ 4000 processors used in parallel). The basic elements of the ATLAS Monte Carlo production system are described. We also present how the software suite was validated and the participating sites were certified. These productions were already partly performed by using different flavours of Grid middleware at ~ 20 sites.",
        "published": "2003-06-12T12:59:39Z",
        "link": "http://arxiv.org/abs/cs/0306052v1",
        "categories": [
            "cs.DC",
            "J.2"
        ]
    },
    {
        "title": "BlueOx: A Java Framework for Distributed Data Analysis",
        "authors": [
            "Jeremiah Mans",
            "David Bengali"
        ],
        "summary": "High energy physics experiments including those at the Tevatron and the upcoming LHC require analysis of large data sets which are best handled by distributed computation. We present the design and development of a distributed data analysis framework based on Java. Analysis jobs run through three phases: discovery of data sets available, brokering/assignment of data sets to analysis servers, and job execution. Each phase is represented by a set of abstract interfaces. These interfaces allow different techniques to be used without modification to the framework. For example, the communications interface has been implemented by both a packet protocol and a SOAP-based scheme. User authentication can be provided either through simple passwords or through a GSI certificates system. Data from CMS HCAL Testbeams, the L3 LEP experiment, and a hypothetical high-energy linear collider experiment have been interfaced with the framework.",
        "published": "2003-06-12T15:53:17Z",
        "link": "http://arxiv.org/abs/cs/0306055v1",
        "categories": [
            "cs.DC",
            "J.2;D.4.7"
        ]
    },
    {
        "title": "A Community Authorization Service for Group Collaboration",
        "authors": [
            "Laura Pearlman",
            "Von Welch",
            "Ian Foster",
            "Carl Kesselman",
            "Steven Tuecke"
        ],
        "summary": "In \"Grids\" and \"collaboratories,\" we find distributed communities of resource providers and resource consumers, within which often complex and dynamic policies govern who can use which resources for which purpose. We propose a new approach to the representation, maintenance, and enforcement of such policies that provides a scalable mechanism for specifying and enforcing these policies. Our approach allows resource providers to delegate some of the authority for maintaining fine-grained access control policies to communities, while still maintaining ultimate control over their resources. We also describe a prototype implementation of this approach and an application in a data management context.",
        "published": "2003-06-12T16:06:28Z",
        "link": "http://arxiv.org/abs/cs/0306053v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "C.2.4"
        ]
    },
    {
        "title": "Installing, Running and Maintaining Large Linux Clusters at CERN",
        "authors": [
            "Vladimir Bahyl",
            "Benjamin Chardi",
            "Jan van Eldik",
            "Ulrich Fuchs",
            "Thorsten Kleinwort",
            "Martin Murth",
            "Tim Smith"
        ],
        "summary": "Having built up Linux clusters to more than 1000 nodes over the past five years, we already have practical experience confronting some of the LHC scale computing challenges: scalability, automation, hardware diversity, security, and rolling OS upgrades. This paper describes the tools and processes we have implemented, working in close collaboration with the EDG project [1], especially with the WP4 subtask, to improve the manageability of our clusters, in particular in the areas of system installation, configuration, and monitoring. In addition to the purely technical issues, providing shared interactive and batch services which can adapt to meet the diverse and changing requirements of our users is a significant challenge. We describe the developments and tuning that we have introduced on our LSF based systems to maximise both responsiveness to users and overall system utilisation. Finally, this paper will describe the problems we are facing in enlarging our heterogeneous Linux clusters, the progress we have made in dealing with the current issues and the steps we are taking to gridify the clusters",
        "published": "2003-06-12T18:45:34Z",
        "link": "http://arxiv.org/abs/cs/0306058v1",
        "categories": [
            "cs.DC",
            "C.5.3; K.6.3"
        ]
    },
    {
        "title": "DIRAC - Distributed Infrastructure with Remote Agent Control",
        "authors": [
            "N. Brook",
            "A. Bogdanchikov",
            "A. Buckley",
            "J. Closier",
            "U. Egede",
            "M. Frank",
            "D. Galli",
            "M. Gandelman",
            "V. Garonne",
            "C. Gaspar",
            "R. Graciani Diaz",
            "K. Harrison",
            "E. van Herwijnen",
            "A. Khan",
            "S. Klous",
            "I. Korolko",
            "G. Kuznetsov",
            "F. Loverre",
            "U. Marconi",
            "J. P. Palacios",
            "G. N. Patrick",
            "A. Pickford",
            "S. Ponce",
            "V. Romanovski",
            "J. J. Saborido",
            "M. Schmelling",
            "A. Soroko",
            "A. Tsaregorodtsev",
            "V. Vagnoni",
            "A. Washbrook"
        ],
        "summary": "This paper describes DIRAC, the LHCb Monte Carlo production system. DIRAC has a client/server architecture based on: Compute elements distributed among the collaborating institutes; Databases for production management, bookkeeping (the metadata catalogue) and software configuration; Monitoring and cataloguing services for updating and accessing the databases. Locally installed software agents implemented in Python monitor the local batch queue, interrogate the production database for any outstanding production requests using the XML-RPC protocol and initiate the job submission. The agent checks and, if necessary, installs any required software automatically. After the job has processed the events, the agent transfers the output data and updates the metadata catalogue. DIRAC has been successfully installed at 18 collaborating institutes, including the DataGRID, and has been used in recent Physics Data Challenges. In the near to medium term future we must use a mixed environment with different types of grid middleware or no middleware. We describe how this flexibility has been achieved and how ubiquitously available grid middleware would improve DIRAC.",
        "published": "2003-06-12T23:54:24Z",
        "link": "http://arxiv.org/abs/cs/0306060v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Operational Aspects of Dealing with the Large BaBar Data Set",
        "authors": [
            "Tofigh Azemoon",
            "Adil Hasan",
            "Wilko Kroeger",
            "Artem Trunov"
        ],
        "summary": "To date, the BaBar experiment has stored over 0.7PB of data in an Objectivity/DB database. Approximately half this data-set comprises simulated data of which more than 70% has been produced at more than 20 collaborating institutes outside of SLAC. The operational aspects of managing such a large data set and providing access to the physicists in a timely manner is a challenging and complex problem. We describe the operational aspects of managing such a large distributed data-set as well as importing and exporting data from geographically spread BaBar collaborators. We also describe problems common to dealing with such large datasets.",
        "published": "2003-06-13T00:40:18Z",
        "link": "http://arxiv.org/abs/cs/0306061v1",
        "categories": [
            "cs.DB",
            "cs.DC",
            "H.2.7; E.5; J.2"
        ]
    },
    {
        "title": "Exploiting peer group concept for adaptive and highly available services",
        "authors": [
            "Muhammad Asif Jan",
            "Fahd Ali Zahid",
            "Mohammad Moazam Fraz",
            "Arshad Ali"
        ],
        "summary": "This paper presents a prototype for redundant, highly available and fault tolerant peer to peer framework for data management. Peer to peer computing is gaining importance due to its flexible organization, lack of central authority, distribution of functionality to participating nodes and ability to utilize unused computational resources. Emergence of GRID computing has provided much needed infrastructure and administrative domain for peer to peer computing. The components of this framework exploit peer group concept to scope service and information search, arrange services and information in a coherent manner, provide selective redundancy and ensure availability in face of failure and high load conditions. A prototype system has been implemented using JXTA peer to peer technology and XML is used for service description and interfaces, allowing peers to communicate with services implemented in various platforms including web services and JINI services. It utilizes code mobility to achieve role interchange among services and ensure dynamic group membership. Security is ensured by using Public Key Infrastructure (PKI) to implement group level security policies for membership and service access.",
        "published": "2003-06-13T13:38:01Z",
        "link": "http://arxiv.org/abs/cs/0306064v2",
        "categories": [
            "cs.DC",
            "H 3.4"
        ]
    },
    {
        "title": "The AliEn system, status and perspectives",
        "authors": [
            "P. Buncic",
            "P. Saiz",
            "A. J. Peters"
        ],
        "summary": "AliEn is a production environment that implements several components of the Grid paradigm needed to simulate, reconstruct and analyse HEP data in a distributed way. The system is built around Open Source components, uses the Web Services model and standard network protocols to implement the computing platform that is currently being used to produce and analyse Monte Carlo data at over 30 sites on four continents. The aim of this paper is to present the current AliEn architecture and outline its future developments in the light of emerging standards.",
        "published": "2003-06-13T15:43:15Z",
        "link": "http://arxiv.org/abs/cs/0306067v1",
        "categories": [
            "cs.DC",
            "C.2.4; H.2.4"
        ]
    },
    {
        "title": "AliEn Resource Brokers",
        "authors": [
            "Pablo Saiz",
            "Predrag Buncic",
            "Andreas J. Peters"
        ],
        "summary": "AliEn (ALICE Environment) is a lightweight GRID framework developed by the Alice Collaboration. When the experiment starts running, it will collect data at a rate of approximately 2 PB per year, producing O(109) files per year. All these files, including all simulated events generated during the preparation phase of the experiment, must be accounted and reliably tracked in the GRID environment. The backbone of AliEn is a distributed file catalogue, which associates universal logical file name to physical file names for each dataset and provides transparent access to datasets independently of physical location. The file replication and transport is carried out under the control of the File Transport Broker. In addition, the file catalogue maintains information about every job running in the system. The jobs are distributed by the Job Resource Broker that is implemented using a simplified pull (as opposed to traditional push) architecture. This paper describes the Job and File Transport Resource Brokers and shows that a similar architecture can be applied to solve both problems.",
        "published": "2003-06-13T16:00:45Z",
        "link": "http://arxiv.org/abs/cs/0306068v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Distributed Offline Data Reconstruction in BaBar",
        "authors": [
            "Teela Pulliam",
            "Peter Elmer",
            "Alvise Dorigo"
        ],
        "summary": "The BaBar experiment at SLAC is in its fourth year of running. The data processing system has been continuously evolving to meet the challenges of higher luminosity running and the increasing bulk of data to re-process each year. To meet these goals a two-pass processing architecture has been adopted, where 'rolling calibrations' are quickly calculated on a small fraction of the events in the first pass and the bulk data reconstruction done in the second. This allows for quick detector feedback in the first pass and allows for the parallelization of the second pass over two or more separate farms. This two-pass system allows also for distribution of processing farms off-site. The first such site has been setup at INFN Padova. The challenges met here were many. The software was ported to a full Linux-based, commodity hardware system. The raw dataset, 90 TB, was imported from SLAC utilizing a 155 Mbps network link. A system for quality control and export of the processed data back to SLAC was developed. Between SLAC and Padova we are currently running three pass-one farms, with 32 CPUs each, and nine pass-two farms with 64 to 80 CPUs each. The pass-two farms can process between 2 and 4 million events per day. Details about the implementation and performance of the system will be presented.",
        "published": "2003-06-13T16:16:44Z",
        "link": "http://arxiv.org/abs/cs/0306069v1",
        "categories": [
            "cs.DC",
            "C.3"
        ]
    },
    {
        "title": "A Model for Grid User Management",
        "authors": [
            "Richard Baker",
            "Dantong Yu",
            "Tomasz Wlodek"
        ],
        "summary": "Registration and management of users in a large scale Grid computing environment presents new challenges that are not well addressed by existing protocols. Within a single Virtual Organization (VO), thousands of users will potentially need access to hundreds of computing sites, and the traditional model where users register for local accounts at each site will present significant scaling problems. However, computing sites must maintain control over access to the site and site policies generally require individual local accounts for every user. We present here a model that allows users to register once with a VO and yet still provides all of the computing sites the information they require with the required level of trust. We have developed tools to allow sites to automate the management of local accounts and the mappings between Grid identities and local accounts.",
        "published": "2003-06-13T17:01:45Z",
        "link": "http://arxiv.org/abs/cs/0306063v1",
        "categories": [
            "cs.DC",
            "D.1.3; K.6.m; D.4.6"
        ]
    },
    {
        "title": "Fine-Grained Authorization for Job and Resource Management Using Akenti   and the Globus Toolkit",
        "authors": [
            "M. Thompson",
            "A. Essiari",
            "K. Keahey",
            "V. Welch",
            "S. Lang",
            "B. Liu"
        ],
        "summary": "As the Grid paradigm is adopted as a standard way of sharing remote resources across organizational domains, the need for fine-grained access control to these resources increases. This paper presents an authorization solution for job submission and control, developed as part of the National Fusion Collaboratory, that uses the Globus Toolkit 2 and the Akenti authorization service in order to perform fine-grained authorization of job and resource management requests in a Grid environment. At job startup, it allows the system to evaluate a user's Resource Specification Language request against authorization policies on resource usage (determining how many CPUs or memory a user can use on a given resource or which executables the user can run). Furthermore, based on authorization policies, it allows other virtual organization members to manage the user's job.",
        "published": "2003-06-13T18:15:03Z",
        "link": "http://arxiv.org/abs/cs/0306070v2",
        "categories": [
            "cs.DC",
            "cs.CR",
            "C.2.4"
        ]
    },
    {
        "title": "AliEnFS - a Linux File System for the AliEn Grid Services",
        "authors": [
            "Andreas J. Peters",
            "P. Saiz",
            "P. Buncic"
        ],
        "summary": "Among the services offered by the AliEn (ALICE Environment http://alien.cern.ch) Grid framework there is a virtual file catalogue to allow transparent access to distributed data-sets using various file transfer protocols. $alienfs$ (AliEn File System) integrates the AliEn file catalogue as a new file system type into the Linux kernel using LUFS, a hybrid user space file system framework (Open Source http://lufs.sourceforge.net). LUFS uses a special kernel interface level called VFS (Virtual File System Switch) to communicate via a generalised file system interface to the AliEn file system daemon. The AliEn framework is used for authentication, catalogue browsing, file registration and read/write transfer operations. A C++ API implements the generic file system operations. The goal of AliEnFS is to allow users easy interactive access to a worldwide distributed virtual file system using familiar shell commands (f.e. cp,ls,rm ...) The paper discusses general aspects of Grid File Systems, the AliEn implementation and present and future developments for the AliEn Grid File System.",
        "published": "2003-06-13T18:18:59Z",
        "link": "http://arxiv.org/abs/cs/0306071v1",
        "categories": [
            "cs.DC",
            "D.4.3"
        ]
    },
    {
        "title": "The EU DataGrid Workload Management System: towards the second major   release",
        "authors": [
            "G. Avellino",
            "S. Barale",
            "S. Beco",
            "B. Cantalupo",
            "D. Colling",
            "F. Giacomini",
            "A. Gianelle",
            "A. Guarise",
            "A. Krenek",
            "D. Kouril",
            "A. Maraschini",
            "L. Matyska",
            "M. Mezzadri",
            "S. Monforte",
            "M. Mulac",
            "F. Pacini",
            "M. Pappalardo",
            "R. Peluso",
            "J. Pospisil",
            "F. Prelz",
            "E. Ronchieri",
            "M. Ruda",
            "L. Salconi",
            "Z. Salvet",
            "M. Sgaravatto",
            "J. Sitera",
            "A. Terracina",
            "M. Vocu",
            "A. Werbrouck"
        ],
        "summary": "In the first phase of the European DataGrid project, the 'workload management' package (WP1) implemented a working prototype, providing users with an environment allowing to define and submit jobs to the Grid, and able to find and use the ``best'' resources for these jobs. Application users have now been experiencing for about a year now with this first release of the workload management system. The experiences acquired, the feedback received by the user and the need to plug new components implementing new functionalities, triggered an update of the existing architecture. A description of this revised and complemented workload management system is given.",
        "published": "2003-06-13T18:57:35Z",
        "link": "http://arxiv.org/abs/cs/0306072v1",
        "categories": [
            "cs.DC",
            "H.3.4"
        ]
    },
    {
        "title": "GridMonitor: Integration of Large Scale Facility Fabric Monitoring with   Meta Data Service in Grid Environment",
        "authors": [
            "Rich Baker",
            "Dantong Yu",
            "Jason Smith",
            "Anthony Chan",
            "Kaushik De",
            "Patrick McGuigan"
        ],
        "summary": "Grid computing consists of the coordinated use of large sets of diverse, geographically distributed resources for high performance computation. Effective monitoring of these computing resources is extremely important to allow efficient use on the Grid. The large number of heterogeneous computing entities available in Grids makes the task challenging. In this work, we describe a Grid monitoring system, called GridMonitor, that captures and makes available the most important information from a large computing facility. The Grid monitoring system consists of four tiers: local monitoring, archiving, publishing and harnessing. This architecture was applied on a large scale linux farm and network infrastructure. It can be used by many higher-level Grid services including scheduling services and resource brokering.",
        "published": "2003-06-13T20:16:30Z",
        "link": "http://arxiv.org/abs/cs/0306073v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "C.4; D.4.8; H.3.4; K.6.2"
        ]
    },
    {
        "title": "Understanding and Coping with Hardware and Software Failures in a Very   Large Trigger Farm",
        "authors": [
            "Jim Kowalkowski"
        ],
        "summary": "When thousands of processors are involved in performing event filtering on a trigger farm, there is likely to be a large number of failures within the software and hardware systems. BTeV, a proton/antiproton collider experiment at Fermi National Accelerator Laboratory, has designed a trigger, which includes several thousand processors. If fault conditions are not given proper treatment, it is conceivable that this trigger system will experience failures at a high enough rate to have a negative impact on its effectiveness. The RTES (Real Time Embedded Systems) collaboration is a group of physicists, engineers, and computer scientists working to address the problem of reliability in large-scale clusters with real-time constraints such as this. Resulting infrastructure must be highly scalable, verifiable, extensible by users, and dynamically changeable.",
        "published": "2003-06-13T21:24:05Z",
        "link": "http://arxiv.org/abs/cs/0306074v1",
        "categories": [
            "cs.DC",
            "B.8.1"
        ]
    },
    {
        "title": "Data Management for Physics Analysis in Phenix (BNL, RHIC)",
        "authors": [
            "Barbara Jacak",
            "Roy Lacey",
            "Dave Morrison",
            "Irina Sourikova",
            "Andrey Shevel",
            "Qiu Zhiping"
        ],
        "summary": "Every year the PHENIX collaboration deals with increasing volume of data (now about 1/4 PB/year). Apparently the more data the more questions how to process all the data in most efficient way. In recent past many developments in HEP computing were dedicated to the production environment. Now we need more tools to help to obtain physics results from the analysis of distributed simulated and experimental data. Developments in Grid architectures gave many examples how distributed computing facilities can be organized to meet physics analysis needs. We feel that our main task in this area is to try to use already developed systems or system components in PHENIX environment.   We are concentrating here on the followed problems: file/replica catalog which keep names of our files, data moving over WAN, job submission in multicluster environment.   PHENIX is a running experiment and this fact narrowed our ability to test new software on the collaboration computer facilities. We are experimenting with system prototypes at State University of New York at Stony Brook (SUNYSB) where we run midrange computing cluster for physics analysis. The talk is dedicated to discuss some experience with Grid software and achieved results.",
        "published": "2003-06-13T21:24:55Z",
        "link": "http://arxiv.org/abs/cs/0306075v1",
        "categories": [
            "cs.DC",
            "H.3.4"
        ]
    },
    {
        "title": "BaBar Web job submission with Globus authentication and AFS access",
        "authors": [
            "R. J. Barlow",
            "A. Forti",
            "A. McNab",
            "S. Salih",
            "D. Smith",
            "T. Adye"
        ],
        "summary": "We present two versions of a grid job submission system produced for the BaBar experiment. Both use globus job submission to process data spread across various sites, producing output which can be combined for analysis. The problems encountered with authorisation and authentication, data location, job submission, and the input and output sandboxes are described, as are the solutions. The total system is still some way short of the aims of enterprises such as the EDG, but represent a significant step along the way.",
        "published": "2003-06-14T02:39:07Z",
        "link": "http://arxiv.org/abs/cs/0306084v1",
        "categories": [
            "cs.DC",
            "A.0; C.2.4"
        ]
    },
    {
        "title": "GMA Instrumentation of the Athena Framework using NetLogger",
        "authors": [
            "Craig E. Tull",
            "Dan Gunter",
            "Wim Lavrijsen",
            "David Quarrie",
            "Brian Tierney"
        ],
        "summary": "Grid applications are, by their nature, wide-area distributed applications. This WAN aspect of Grid applications makes the use of conventional monitoring and instrumentation tools (such as top, gprof, LSF Monitor, etc) impractical for verification that the application is running correctly and efficiently. To be effective, monitoring data must be \"end-to-end\", meaning that all components between the Grid application endpoints must be monitored. Instrumented applications can generate a large amount of monitoring data, so typically the instrumentation is off by default. For jobs running on a Grid, there needs to be a general mechanism to remotely activate the instrumentation in running jobs. The NetLogger Toolkit Activation Service provides this mechanism.   To demonstrate this, we have instrumented the ATLAS Athena Framework with NetLogger to generate monitoring events. We then use a GMA-based activation service to control NetLogger's trigger mechanism. The NetLogger trigger mechanism allows one to easily start, stop, or change the logging level of a running program by modifying a trigger file. We present here details of the design of the NetLogger implementation of the GMA-based activation service and the instrumentation service for Athena. We also describe how this activation service allows us to non-intrusively collect and visualize the ATLAS Athena Framework monitoring data.",
        "published": "2003-06-14T05:40:27Z",
        "link": "http://arxiv.org/abs/cs/0306086v1",
        "categories": [
            "cs.DC",
            "cs.IR",
            "C.2.2"
        ]
    },
    {
        "title": "Building A High Performance Parallel File System Using Grid Datafarm and   ROOT I/O",
        "authors": [
            "Y. Morita",
            "H. Sato",
            "Y. Watase",
            "O. Tatebe",
            "S. Sekiguchi",
            "S. Matsuoka",
            "N. Soda",
            "A. Dell'Acqua"
        ],
        "summary": "Sheer amount of petabyte scale data foreseen in the LHC experiments require a careful consideration of the persistency design and the system design in the world-wide distributed computing. Event parallelism of the HENP data analysis enables us to take maximum advantage of the high performance cluster computing and networking when we keep the parallelism both in the data processing phase, in the data management phase, and in the data transfer phase. A modular architecture of FADS/ Goofy, a versatile detector simulation framework for Geant4, enables an easy choice of plug-in facilities for persistency technologies such as Objectivity/DB and ROOT I/O. The framework is designed to work naturally with the parallel file system of Grid Datafarm (Gfarm). FADS/Goofy is proven to generate 10^6 Geant4-simulated Atlas Mockup events using a 512 CPU PC cluster. The data in ROOT I/O files is replicated using Gfarm file system. The histogram information is collected from the distributed ROOT files. During the data replication it has been demonstrated to achieve more than 2.3 Gbps data transfer rate between the PC clusters over seven participating PC clusters in the United States and in Japan.",
        "published": "2003-06-14T16:29:16Z",
        "link": "http://arxiv.org/abs/cs/0306092v1",
        "categories": [
            "cs.DC",
            "J.2"
        ]
    },
    {
        "title": "Grid-Brick Event Processing Framework in GEPS",
        "authors": [
            "Antonio Amorim",
            "Luis Pedro",
            "Han Fei",
            "Nuno Almeida",
            "Paulo Trezentos",
            "Jaime E. Villate"
        ],
        "summary": "Experiments like ATLAS at LHC involve a scale of computing and data management that greatly exceeds the capability of existing systems, making it necessary to resort to Grid-based Parallel Event Processing Systems (GEPS). Traditional Grid systems concentrate the data in central data servers which have to be accessed by many nodes each time an analysis or processing job starts. These systems require very powerful central data servers and make little use of the distributed disk space that is available in commodity computers. The Grid-Brick system, which is described in this paper, follows a different approach. The data storage is split among all grid nodes having each one a piece of the whole information. Users submit queries and the system will distribute the tasks through all the nodes and retrieve the result, merging them together in the Job Submit Server. The main advantage of using this system is the huge scalability it provides, while its biggest disadvantage appears in the case of failure of one of the nodes. A workaround for this problem involves data replication or backup.",
        "published": "2003-06-14T22:33:35Z",
        "link": "http://arxiv.org/abs/cs/0306093v1",
        "categories": [
            "cs.DC",
            "C.1.4;C.2.1;C.2.4;D.1.3;D.4.3;D.4.7;H.2.4"
        ]
    },
    {
        "title": "The MammoGrid Project Grids Architecture",
        "authors": [
            "Richard McClatchey",
            "Predrag Buncic",
            "David Manset",
            "Tamas Hauer",
            "Florida Estrella",
            "Pablo Saiz",
            "Dmitri Rogulin"
        ],
        "summary": "The aim of the recently EU-funded MammoGrid project is, in the light of emerging Grid technology, to develop a European-wide database of mammograms that will be used to develop a set of important healthcare applications and investigate the potential of this Grid to support effective co-working between healthcare professionals throughout the EU. The MammoGrid consortium intends to use a Grid model to enable distributed computing that spans national borders. This Grid infrastructure will be used for deploying novel algorithms as software directly developed or enhanced within the project. Using the MammoGrid clinicians will be able to harness the use of massive amounts of medical image data to perform epidemiological studies, advanced image processing, radiographic education and ultimately, tele-diagnosis over communities of medical \"virtual organisations\". This is achieved through the use of Grid-compliant services [1] for managing (versions of) massively distributed files of mammograms, for handling the distributed execution of mammograms analysis software, for the development of Grid-aware algorithms and for the sharing of resources between multiple collaborating medical centres. All this is delivered via a novel software and hardware information infrastructure that, in addition guarantees the integrity and security of the medical data. The MammoGrid implementation is based on AliEn, a Grid framework developed by the ALICE Collaboration. AliEn provides a virtual file catalogue that allows transparent access to distributed data-sets and provides top to bottom implementation of a lightweight Grid applicable to cases when handling of a large number of files is required. This paper details the architecture that will be implemented by the MammoGrid project.",
        "published": "2003-06-16T06:53:57Z",
        "link": "http://arxiv.org/abs/cs/0306095v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C2.4,H2.4,J.3"
        ]
    },
    {
        "title": "MonALISA : A Distributed Monitoring Service Architecture",
        "authors": [
            "H. B. Newman",
            "I. C. Legrand",
            "P. Galvez",
            "R. Voicu",
            "C. Cirstoiu"
        ],
        "summary": "The MonALISA (Monitoring Agents in A Large Integrated Services Architecture) system provides a distributed monitoring service. MonALISA is based on a scalable Dynamic Distributed Services Architecture which is designed to meet the needs of physics collaborations for monitoring global Grid systems, and is implemented using JINI/JAVA and WSDL/SOAP technologies. The scalability of the system derives from the use of multithreaded Station Servers to host a variety of loosely coupled self-describing dynamic services, the ability of each service to register itself and then to be discovered and used by any other services, or clients that require such information, and the ability of all services and clients subscribing to a set of events (state changes) in the system to be notified automatically. The framework integrates several existing monitoring tools and procedures to collect parameters describing computational nodes, applications and network performance. It has built-in SNMP support and network-performance monitoring algorithms that enable it to monitor end-to-end network performance as well as the performance and state of site facilities in a Grid. MonALISA is currently running around the clock on the US CMS test Grid as well as an increasing number of other sites. It is also being used to monitor the performance and optimize the interconnections among the reflectors in the VRVS system.",
        "published": "2003-06-16T08:33:44Z",
        "link": "http://arxiv.org/abs/cs/0306096v1",
        "categories": [
            "cs.DC",
            "H4.3;H5.2;J2;D2.8"
        ]
    },
    {
        "title": "Using CAS to Manage Role-Based VO Sub-Groups",
        "authors": [
            "Craig E. Tull",
            "Shane Canon",
            "Steve Chan",
            "Doug Olson",
            "Laura Pearlman",
            "Von Welch"
        ],
        "summary": "LHC-era HENP experiments will generate unprecidented volumes of data and require commensurately large compute resources. These resources are larger than can be marshalled at any one site within the community. Production reconstruction, analysis, and simulation will need to take maximum advantage of these distributed computing and storage resources using the new capabilities offered by the Grid computing paradigm. Since large-scale, coordinated Grid computing involves user access across many Regional Centers and national and funding boundaries, one of the most crucial aspects of Grid computing is that of user authentication and authorization. While projects such as the DOE Grids CA have gone a long way to solving the problem of distributed authentication, the authorization problem is still largely open.   We have developed and tested a prototype VO-Role management system using the Community Authorization Service (CAS) from the Globus project. CAS allows for a flexible definition of resources. In this protoype we define a role as a resource within the CAS database and assign individuals in the VO access to that resource to indicate their ability to assert the role. The access of an individual to this VO-Role resource is then an annotation of the user's CAS proxy certificate. This annotation is then used by the local resource managers to authorize access to local compute and storage resources at a granularity which is base on neither VOs nor individuals. We report here on the configuration details for the CAS database and the Globus Gatekeeper and on how this general approch could be formalized and extended to meet the clear needs of LHC experiments using the Grid.",
        "published": "2003-06-16T15:11:17Z",
        "link": "http://arxiv.org/abs/cs/0306088v2",
        "categories": [
            "cs.CR",
            "cs.DC",
            "C.2.0"
        ]
    },
    {
        "title": "Site Authorization Service (SAZ)",
        "authors": [
            "Vijay Sehkri",
            "Igor Mandrichenko",
            "Dane Skow"
        ],
        "summary": "In this paper we present a methodology to provide an additional level of centralized control for the grid resources. This centralized control is applied to site-wide distribution of various grids and thus providing an upper hand in the maintenance.",
        "published": "2003-06-16T16:07:24Z",
        "link": "http://arxiv.org/abs/cs/0306100v1",
        "categories": [
            "cs.DC",
            "D.4.6; K.6.5"
        ]
    },
    {
        "title": "Prototyping Virtual Data Technologies in ATLAS Data Challenge 1   Production",
        "authors": [
            "A. Vaniachine",
            "D. Malon",
            "P. Nevski",
            "K. De"
        ],
        "summary": "For efficiency of the large production tasks distributed worldwide, it is essential to provide shared production management tools comprised of integratable and interoperable services. To enhance the ATLAS DC1 production toolkit, we introduced and tested a Virtual Data services component. For each major data transformation step identified in the ATLAS data processing pipeline (event generation, detector simulation, background pile-up and digitization, etc) the Virtual Data Cookbook (VDC) catalogue encapsulates the specific data transformation knowledge and the validated parameters settings that must be provided before the data transformation invocation. To provide for local-remote transparency during DC1 production, the VDC database server delivered in a controlled way both the validated production parameters and the templated production recipes for thousands of the event generation and detector simulation jobs around the world, simplifying the production management solutions.",
        "published": "2003-06-16T19:54:33Z",
        "link": "http://arxiv.org/abs/cs/0306102v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4; H.2.4"
        ]
    },
    {
        "title": "Distributed Heterogeneous Relational Data Warehouse In A Grid   Environment",
        "authors": [
            "Saima Iqbal",
            "Julian J. Bunn",
            "Harvey B. Newman"
        ],
        "summary": "This paper examines how a \"Distributed Heterogeneous Relational Data Warehouse\" can be integrated in a Grid environment that will provide physicists with efficient access to large and small object collections drawn from databases at multiple sites. This paper investigates the requirements of Grid-enabling such a warehouse, and explores how these requirements may be met by extensions to existing Grid middleware. We present initial results obtained with a working prototype warehouse of this kind using both SQLServer and Oracle9i, where a Grid-enabled web-services interface makes it easier for web-applications to access the distributed contents of the databases securely. Based on the success of the prototype, we proposes a framework for using heterogeneous relational data warehouse through the web-service interface and create a single \"Virtual Database System\" for users. The ability to transparently access data in this way, as shown in prototype, is likely to be a very powerful facility for HENP and other grid users wishing to collate and analyze information distributed over Grid.",
        "published": "2003-06-18T09:30:03Z",
        "link": "http://arxiv.org/abs/cs/0306109v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.2.1;H.2.2;H.2.4;H.2.7;H.3.1;H.3.5"
        ]
    },
    {
        "title": "Sharing a conceptual model of grid resources and services",
        "authors": [
            "Sergio Andreozzi",
            "Massimo Sgaravatto",
            "Cristina Vistoli"
        ],
        "summary": "Grid technologies aim at enabling a coordinated resource-sharing and problem-solving capabilities over local and wide area networks and span locations, organizations, machine architectures and software boundaries. The heterogeneity of involved resources and the need for interoperability among different grid middlewares require the sharing of a common information model. Abstractions of different flavors of resources and services and conceptual schemas of domain specific entities require a collaboration effort in order to enable a coherent information services cooperation.   With this paper, we present the result of our experience in grid resources and services modelling carried out within the Grid Laboratory Uniform Environment (GLUE) effort, a joint US and EU High Energy Physics projects collaboration towards grid interoperability. The first implementation-neutral agreement on services such as batch computing and storage manager, resources such as the hierarchy cluster, sub-cluster, host and the storage library are presented. Design guidelines and operational results are depicted together with open issues and future evolutions.",
        "published": "2003-06-18T14:55:40Z",
        "link": "http://arxiv.org/abs/cs/0306111v1",
        "categories": [
            "cs.DC",
            "H.3.4"
        ]
    },
    {
        "title": "Adapting SAM for CDF",
        "authors": [
            "D. Bonham",
            "G. Garzoglio",
            "R. Herber",
            "J. Kowalkowski",
            "D. Litvintsev",
            "L. Lueking",
            "M. Paterno",
            "D. Petravick",
            "L. Piccoli",
            "R. Pordes",
            "N. Stanfield",
            "I. Terekhov",
            "J. Trumbo",
            "J. Tseng",
            "S. Veseli",
            "M. Votava",
            "V. White",
            "T. Huffman",
            "S. Stonjek",
            "K. Waltkins",
            "P. Crosby",
            "D. Waters",
            "R. St. Denis"
        ],
        "summary": "The CDF and D0 experiments probe the high-energy frontier and as they do so have accumulated hundreds of Terabytes of data on the way to petabytes of data over the next two years. The experiments have made a commitment to use the developing Grid based on the SAM system to handle these data. The D0 SAM has been extended for use in CDF as common patterns of design emerged to meet the similar requirements of these experiments. The process by which the merger was achieved is explained with particular emphasis on lessons learned concerning the database design patterns plus realization of the use cases.",
        "published": "2003-06-18T16:28:02Z",
        "link": "http://arxiv.org/abs/cs/0306112v1",
        "categories": [
            "cs.DC",
            "C.2.4; H.3.2; H.3.3; H.3.4"
        ]
    },
    {
        "title": "Run Control and Monitor System for the CMS Experiment",
        "authors": [
            "M. Bellato",
            "L. Berti",
            "V. Brigljevic",
            "G. Bruno",
            "E. Cano",
            "S. Cittolin",
            "A. Csilling",
            "S. Erhan",
            "D. Gigi",
            "F. Glege",
            "R. Gomez-Reino",
            "M. Gulmini",
            "J. Gutleber",
            "C. Jacobs",
            "M. Kozlovszky",
            "H. Larsen",
            "I. Magrans",
            "G. Maron",
            "F. Meijers",
            "E. Meschi",
            "S. Murray",
            "A. Oh",
            "L. Orsini",
            "L. Pollet",
            "A. Racz",
            "G. Rorato",
            "D. Samyn",
            "P. Scharff-Hansen",
            "C. Schwick",
            "P. Sphicas",
            "N. Toniolo",
            "S. Ventura",
            "L. Zangrando"
        ],
        "summary": "The Run Control and Monitor System (RCMS) of the CMS experiment is the set of hardware and software components responsible for controlling and monitoring the experiment during data-taking. It provides users with a \"virtual counting room\", enabling them to operate the experiment and to monitor detector status and data quality from any point in the world. This paper describes the architecture of the RCMS with particular emphasis on its scalability through a distributed collection of nodes arranged in a tree-based hierarchy. The current implementation of the architecture in a prototype RCMS used in test beam setups, detector validations and DAQ demonstrators is documented. A discussion of the key technologies used, including Web Services, and the results of tests performed with a 128-node system are presented.",
        "published": "2003-06-18T16:34:11Z",
        "link": "http://arxiv.org/abs/cs/0306110v1",
        "categories": [
            "cs.DC",
            "C.2.4; J.2"
        ]
    },
    {
        "title": "D0 Data Handling Operational Experience",
        "authors": [
            "A. Baranovski",
            "C. Brock",
            "D. Bonham",
            "L. Carpenter",
            "L. Lueking",
            "W. Merritt",
            "C. Moore",
            "I. Terekhov",
            "J. Trumbo",
            "S. Veseli",
            "J. Weigand",
            "S. White",
            "K. Yip"
        ],
        "summary": "We report on the production experience of the D0 experiment at the Fermilab Tevatron, using the SAM data handling system with a variety of computing hardware configurations, batch systems, and mass storage strategies. We have stored more than 300 TB of data in the Fermilab Enstore mass storage system. We deliver data through this system at an average rate of more than 2 TB/day to analysis programs, with a substantial multiplication factor in the consumed data through intelligent cache management. We handle more than 1.7 Million files in this system and provide data delivery to user jobs at Fermilab on four types of systems: a reconstruction farm, a large SMP system, a Linux batch cluster, and a Linux desktop cluster. In addition, we import simulation data generated at 6 sites worldwide, and deliver data to jobs at many more sites. We describe the scope of the data handling deployment worldwide, the operational experience with this system, and the feedback of that experience.",
        "published": "2003-06-19T18:13:23Z",
        "link": "http://arxiv.org/abs/cs/0306114v1",
        "categories": [
            "cs.DC",
            "cs.AI",
            "H.3"
        ]
    },
    {
        "title": "D0 Regional Analysis Center Concepts",
        "authors": [
            "L. Lueking",
            "representing the D0 Remote Analysis Task Force"
        ],
        "summary": "The D0 experiment is facing many exciting challenges providing a computing environment for its worldwide collaboration. Transparent access to data for processing and analysis has been enabled through deployment of its SAM system to collaborating sites and additional functionality will be provided soon with SAMGrid components. In order to maximize access to global storage, computational and intellectual resources, and to enable the system to scale to the large demands soon to be realized, several strategic sites have been identified as Regional Analysis Centers (RAC's). These sites play an expanded role within the system. The philosophy and function of these centers is discussed and details of their composition and operation are outlined. The plan for future additional centers is also addressed.",
        "published": "2003-06-19T19:58:07Z",
        "link": "http://arxiv.org/abs/cs/0306115v1",
        "categories": [
            "cs.DC",
            "A.0;H.3"
        ]
    },
    {
        "title": "Security for Grid Services",
        "authors": [
            "Von Welch",
            "Frank Siebenlist",
            "Ian Foster",
            "John Bresnahan",
            "Karl Czajkowski",
            "Jarek Gawor",
            "Carl Kesselman",
            "Sam Meder",
            "Laura Pearlman",
            "Steven Tuecke"
        ],
        "summary": "Grid computing is concerned with the sharing and coordinated use of diverse resources in distributed \"virtual organizations.\" The dynamic and multi-institutional nature of these environments introduces challenging security issues that demand new technical approaches. In particular, one must deal with diverse local mechanisms, support dynamic creation of services, and enable dynamic creation of trust domains. We describe how these issues are addressed in two generations of the Globus Toolkit. First, we review the Globus Toolkit version 2 (GT2) approach; then, we describe new approaches developed to support the Globus Toolkit version 3 (GT3) implementation of the Open Grid Services Architecture, an initiative that is recasting Grid concepts within a service oriented framework based on Web services. GT3's security implementation uses Web services security mechanisms for credential exchange and other purposes, and introduces a tight least-privilege model that avoids the need for any privileged network service.",
        "published": "2003-06-24T21:00:17Z",
        "link": "http://arxiv.org/abs/cs/0306129v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "GRAPPA: Grid Access Portal for Physics Applications",
        "authors": [
            "D. Engh",
            "S. Smallen",
            "J. Gieraltowski",
            "L. Fang",
            "R. Gardner",
            "D. Gannon",
            "R. Bramley"
        ],
        "summary": "Grappa is a Grid portal effort designed to provide physicists convenient access to Grid tools and services. The ATLAS analysis and control framework, Athena, was used as the target application. Grappa provides basic Grid functionality such as resource configuration, credential testing, job submission, job monitoring, results monitoring, and preliminary integration with the ATLAS replica catalog system, MAGDA. Grappa uses Jython to combine the ease of scripting with the power of java-based toolkits. This provides a powerful framework for accessing diverse Grid resources with uniform interfaces. The initial prototype system was based on the XCAT Science Portal developed at the Indiana University Extreme Computing Lab and was demonstrated by running Monte Carlo production on the U.S. ATLAS test-bed. The portal also communicated with a European resource broker on WorldGrid as part of the joint iVDGL-DataTAG interoperability project for the IST2002 and SC2002 demonstrations. The current prototype replaces the XCAT Science Portal with an xbooks jetspeed portlet for managing user scripts.",
        "published": "2003-06-26T17:09:07Z",
        "link": "http://arxiv.org/abs/cs/0306133v1",
        "categories": [
            "cs.DC",
            "H.5.2; C.2.4"
        ]
    },
    {
        "title": "Serving Database Information Using a Flexible Server in a Three Tier   Architecture",
        "authors": [
            "Herbert Greenlee",
            "Robert Illingworth",
            "Jim Kowalkowski",
            "Anil Kumar",
            "Lee Lueking",
            "Taka Yasuda",
            "Margherita Vittone",
            "Stephen White"
        ],
        "summary": "The D0 experiment at Fermilab relies on a central Oracle database for storing all detector calibration information. Access to this data is needed by hundreds of physics applications distributed worldwide. In order to meet the demands of these applications from scarce resources, we have created a distributed system that isolates the user applications from the database facilities. This system, known as the Database Application Network (DAN) operates as the middle tier in a three tier architecture. A DAN server employs a hierarchical caching scheme and database connection management facility that limits access to the database resource. The modular design allows for caching strategies and database access components to be determined by runtime configuration. To solve scalability problems, a proxy database component allows for DAN servers to be arranged in a hierarchy. Also included is an event based monitoring system that is currently being used to collect statistics for performance analysis and problem diagnosis. DAN servers are currently implemented as a Python multithreaded program using CORBA for network communications and interface specification. The requirement details, design, and implementation of DAN are discussed along with operational experience and future plans.",
        "published": "2003-07-01T02:13:32Z",
        "link": "http://arxiv.org/abs/cs/0307001v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "A.0;H.2.8"
        ]
    },
    {
        "title": "Management of Grid Jobs and Information within SAMGrid",
        "authors": [
            "A. Baranovski",
            "G. Garzoglio",
            "A. Kreymer",
            "L. Lueking",
            "S. Stonjek",
            "I. Terekhov",
            "F. Wuerthwein",
            "A. Roy",
            "P. Mhashikar",
            "V. Murthi",
            "T. Tannenbaum",
            "R. Walker",
            "F. Ratnikov",
            "T. Rockwell"
        ],
        "summary": "We describe some of the key aspects of the SAMGrid system, used by the D0 and CDF experiments at Fermilab. Having sustained success of the data handling part of SAMGrid, we have developed new services for job and information services. Our job management is rooted in \\CondorG and uses enhancements that are general applicability for HEP grids. Our information system is based on a uniform framework for configuration management based on XML data representation and processing.",
        "published": "2003-07-03T20:26:13Z",
        "link": "http://arxiv.org/abs/cs/0307007v2",
        "categories": [
            "cs.DC",
            "c.1.4"
        ]
    },
    {
        "title": "Lattice QCD Production on Commodity Clusters at Fermilab",
        "authors": [
            "D. Holmgren",
            "A. Singh",
            "P. Mackenzie",
            "J. Simone"
        ],
        "summary": "We describe the construction and results to date of Fermilab's three Myrinet-networked lattice QCD production clusters (an 80-node dual Pentium III cluster, a 48-node dual Xeon cluster, and a 128-node dual Xeon cluster). We examine a number of aspects of performance of the MILC lattice QCD code running on these clusters.",
        "published": "2003-07-08T15:36:56Z",
        "link": "http://arxiv.org/abs/cs/0307019v1",
        "categories": [
            "cs.DC",
            "C.4;C.1.4;D.1.3"
        ]
    },
    {
        "title": "Tools and Techniques for Managing Clusters for SciDAC Lattice QCD at   Fermilab",
        "authors": [
            "A. Singh",
            "D. Holmgren",
            "R. Rechenmacher",
            "S. Epsteyn"
        ],
        "summary": "Fermilab operates several clusters for lattice gauge computing. Minimal manpower is available to manage these clusters. We have written a number of tools and developed techniques to cope with this task. We describe our tools which use the IPMI facilities of our systems for hardware management tasks such as remote power control, remote system resets, and health monitoring. We discuss our techniques involving network booting for installation and upgrades of the operating system on these computers, and for reloading BIOS and other firmware. Finally, we discuss our tools for parallel command processing and their use in monitoring and administrating the PBS batch queue system used on our clusters.",
        "published": "2003-07-08T16:58:57Z",
        "link": "http://arxiv.org/abs/cs/0307021v1",
        "categories": [
            "cs.DC",
            "C.4; C.m"
        ]
    },
    {
        "title": "On the scaling of computational particle physics codes on cluster   computers",
        "authors": [
            "Z. Sroczynski",
            "N. Eicker",
            "Th. Lippert",
            "B. Orth",
            "K. Schilling"
        ],
        "summary": "Many appplications in computational science are sufficiently compute-intensive that they depend on the power of parallel computing for viability. For all but the \"embarrassingly parallel\" problems, the performance depends upon the level of granularity that can be achieved on the computer platform.   Our computational particle physics applications require machines that can support a wide range of granularities, but in general, compute-intensive state-of-the-art projects will require finely grained distributions. Of the different types of machines available for the task, we consider cluster computers.   The use of clusters of commodity computers in high performance computing has many advantages including the raw price/performance ratio and the flexibility of machine configuration and upgrade. Here we focus on what is usually considered the weak point of cluster technology; the scaling behaviour when faced with a numerically intensive parallel computation. To this end we examine the scaling of our own applications from numerical quantum field theory on a cluster and infer conclusions about the more general case.",
        "published": "2003-07-09T14:45:39Z",
        "link": "http://arxiv.org/abs/hep-lat/0307015v2",
        "categories": [
            "hep-lat",
            "cs.DC"
        ]
    },
    {
        "title": "Adaptive Domain Model: Dealing With Multiple Attributes of Self-Managing   Distributed Object Systems",
        "authors": [
            "Pavel Motuzenko"
        ],
        "summary": "Self-managing software has emerged as modern systems have become more complex. Some of the distributed object systems may contain thousands of objects deployed on tens or even hundreds hosts. Development and support of such systems often costs a lot. To solve this issue the systems, which are capable supporting multiple self-managing attributes, should be created. In the paper, the Adaptive domain concept is introduced as an extension to the basic domain concept to support a generic adaptation environment for building distributed object systems with multiple self-managing attributes.",
        "published": "2003-07-13T13:01:19Z",
        "link": "http://arxiv.org/abs/cs/0307035v1",
        "categories": [
            "cs.AR",
            "cs.DC",
            "C.1.3"
        ]
    },
    {
        "title": "Small-World File-Sharing Communities",
        "authors": [
            "Adriana Iamnitchi",
            "Matei Ripeanu",
            "Ian Foster"
        ],
        "summary": "Web caches, content distribution networks, peer-to-peer file sharing networks, distributed file systems, and data grids all have in common that they involve a community of users who generate requests for shared data. In each case, overall system performance can be improved significantly if we can first identify and then exploit interesting structure within a community's access patterns. To this end, we propose a novel perspective on file sharing based on the study of the relationships that form among users based on the files in which they are interested.   We propose a new structure that captures common user interests in data--the data-sharing graph-- and justify its utility with studies on three data-distribution systems: a high-energy physics collaboration, the Web, and the Kazaa peer-to-peer network. We find small-world patterns in the data-sharing graphs of all three communities. We analyze these graphs and propose some probable causes for these emergent small-world patterns. The significance of small-world patterns is twofold: it provides a rigorous support to intuition and, perhaps most importantly, it suggests ways to design mechanisms that exploit these naturally emerging patterns.",
        "published": "2003-07-13T18:44:18Z",
        "link": "http://arxiv.org/abs/cs/0307036v1",
        "categories": [
            "cs.DC",
            "cond-mat",
            "cs.NI",
            "C.2.3"
        ]
    },
    {
        "title": "Gridscape: A Tool for the Creation of Interactive and Dynamic Grid   Testbed Web Portals",
        "authors": [
            "Hussein Gibbins",
            "Rajkumar Buyya"
        ],
        "summary": "The notion of grid computing has gained an increasing popularity recently as a realistic solution to many of our large-scale data storage and processing needs. It enables the sharing, selection and aggregation of resources geographically distributed across collaborative organisations. Now more and more people are beginning to embrace grid computing and thus are seeing the need to set up their own grids and grid testbeds. With this comes the need to have some means to enable them to view and monitor the status of the resources in these testbeds (eg. Web based Grid portal). Generally developers invest a substantial amount of time and effort developing custom monitoring software. To overcome this limitation, this paper proposes Gridscape ? a tool that enables the rapid creation of interactive and dynamic testbed portals (without any programming effort). Gridscape primarily aims to provide a solution for those users who need to be able to create a grid testbed portal but don?t necessarily have the time or resources to build a system of their own from scratch.",
        "published": "2003-07-22T12:27:28Z",
        "link": "http://arxiv.org/abs/cs/0307052v1",
        "categories": [
            "cs.DC",
            "D.2..2, D.4.9"
        ]
    },
    {
        "title": "Augernome & XtremWeb: Monte Carlos computation on a global computing   platform",
        "authors": [
            "Oleg Lodygensky",
            "Gilles Fedak",
            "Vincent Neri",
            "Alain Cordier",
            "Franck Cappello"
        ],
        "summary": "In this paper, we present XtremWeb, a Global Computing platform used to generate monte carlos showers in Auger, an HEP experiment to study the highest energy cosmic rays at Mallargue-Mendoza, Argentina.   XtremWeb main goal, as a Global Computing platform, is to compute distributed applications using idle time of widely interconnected machines. It is especially dedicated to -but not limited to- multi-parameters applications such as monte carlos computations; its security mechanisms ensuring not only hosts integrity but also results certification and its fault tolerant features, encouraged us to test it and, finally, to deploy it as to support our CPU needs to simulate showers.   We first introduce Auger computing needs and how Global Computing could help. We then detail XtremWeb architecture and goals. The fourth and last part presents the profits we have gained to choose this platform. We conclude on what could be done next.",
        "published": "2003-07-29T14:12:07Z",
        "link": "http://arxiv.org/abs/cs/0307066v1",
        "categories": [
            "cs.DC",
            "D.0"
        ]
    },
    {
        "title": "A Grid Based Architecture for High-Performance NLP",
        "authors": [
            "Baden Hughes",
            "Steven Bird"
        ],
        "summary": "We describe the design and early implementation of an extensible, component-based software architecture for natural language engineering applications which interfaces with high performance distributed computing services. The architecture leverages existing linguistic resource description and discovery mechanisms based on metadata descriptions, combining these in a compatible fashion with other software definition abstractions. Within this architecture, application design is highly flexible, allowing disparate components to be combined to suit the overall application functionality, and formally described independently of processing concerns. An application specification language provides abstraction from the programming environment and allows ease of interface with high performance computational grids via a broker.",
        "published": "2003-08-05T00:46:08Z",
        "link": "http://arxiv.org/abs/cs/0308008v1",
        "categories": [
            "cs.DC",
            "cs.CL",
            "J.5; D.1; C.2"
        ]
    },
    {
        "title": "A Robust and Computational Characterisation of Peer-to-Peer Database   Systems",
        "authors": [
            "Enrico Franconi",
            "Gabriel Kuper",
            "Andrei Lopatenko",
            "Luciano Serafini"
        ],
        "summary": "In this paper we give a robust logical and computational characterisation of peer-to-peer database systems. We first define a pre- cise model-theoretic semantics of a peer-to-peer system, which allows for local inconsistency handling. We then characterise the general computa- tional properties for the problem of answering queries to such a peer-to- peer system. Finally, we devise tight complexity bounds and distributed procedures for the problem of answering queries in few relevant special cases.",
        "published": "2003-08-06T12:50:22Z",
        "link": "http://arxiv.org/abs/cs/0308013v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.2.4;H.2.5;C.2.4"
        ]
    },
    {
        "title": "Parallel implementation of a lattice-gauge-theory code: studying quark   confinement on PC clusters",
        "authors": [
            "Attilio Cucchieri",
            "Tereza Mendes",
            "Gonzalo Travieso",
            "Andre R. Taurines"
        ],
        "summary": "We consider the implementation of a parallel Monte Carlo code for high-performance simulations on PC clusters with MPI. We carry out tests of speedup and efficiency. The code is used for numerical simulations of pure SU(2) lattice gauge theory at very large lattice volumes, in order to study the infrared behavior of gluon and ghost propagators. This problem is directly related to the confinement of quarks and gluons in the physics of strong interactions.",
        "published": "2003-08-08T19:44:56Z",
        "link": "http://arxiv.org/abs/hep-lat/0308005v1",
        "categories": [
            "hep-lat",
            "cs.DC"
        ]
    },
    {
        "title": "Relational Grid Monitoring Architecture (R-GMA)",
        "authors": [
            "Rob Byrom",
            "Brian Coghlan",
            "Andrew W Cooke",
            "Roney Cordenonsi",
            "Linda Cornwall",
            "Abdeslem Djaoui",
            "Laurence Field",
            "Steve Fisher",
            "Steve Hicks",
            "Stuart Kenny",
            "Jason Leake",
            "James Magowan",
            "Werner Nutt",
            "David O'Callaghan",
            "Norbert Podhorszki",
            "John Ryan",
            "Manish Soni",
            "Paul Taylor",
            "Antony J Wilson"
        ],
        "summary": "We describe R-GMA (Relational Grid Monitoring Architecture) which has been developed within the European DataGrid Project as a Grid Information and Monitoring System. Is is based on the GMA from GGF, which is a simple Consumer-Producer model. The special strength of this implementation comes from the power of the relational model. We offer a global view of the information as if each Virtual Organisation had one large relational database. We provide a number of different Producer types with different characteristics; for example some support streaming of information. We also provide combined Consumer/Producers, which are able to combine information and republish it. At the heart of the system is the mediator, which for any query is able to find and connect to the best Producers for the job. We have developed components to allow a measure of inter-working between MDS and R-GMA. We have used it both for information about the grid (primarily to find out about what services are available at any one time) and for application monitoring. R-GMA has been deployed in various testbeds; we describe some preliminary results and experiences of this deployment.",
        "published": "2003-08-15T23:53:49Z",
        "link": "http://arxiv.org/abs/cs/0308024v1",
        "categories": [
            "cs.DC",
            "H.2.4;H.m"
        ]
    },
    {
        "title": "Finding Traitors in Secure Networks Using Byzantine Agreements",
        "authors": [
            "Liam Wagner",
            "Stuart McDonald"
        ],
        "summary": "Secure networks rely upon players to maintain security and reliability. However not every player can be assumed to have total loyalty and one must use methods to uncover traitors in such networks. We use the original concept of the Byzantine Generals Problem by Lamport, and the more formal Byzantine Agreement describe by Linial, to nd traitors in secure networks. By applying general fault-tolerance methods to develop a more formal design of secure networks we are able to uncover traitors amongst a group of players. We also propose methods to integrate this system with insecure channels. This new resiliency can be applied to broadcast and peer-to-peer secure communication systems where agents may be traitors or become unreliable due to faults.",
        "published": "2003-08-19T08:42:43Z",
        "link": "http://arxiv.org/abs/cs/0308028v5",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.GT",
            "cs.MA",
            "B.1.3; C.2.0; C.4.0; D.4.5; H.2.0; H.2.7"
        ]
    },
    {
        "title": "Distributed and Parallel Net Imaging",
        "authors": [
            "G. Iovane"
        ],
        "summary": "A very complex vision system is developed to detect luminosity variations connected with the discovery of new planets in the Universe. The traditional imaging system can not manage a so large load. A private net is implemented to perform an automatic vision and decision architecture. It lets to carry out an on-line discrimination of interesting events by using two levels of triggers. This system can even manage many Tbytes of data per day. The architecture avails itself of a distributed parallel network system based on a maximum of 256 standard workstations with Microsoft Window as OS.",
        "published": "2003-08-22T10:31:53Z",
        "link": "http://arxiv.org/abs/cs/0308037v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.DC",
            "I.4,I.5"
        ]
    },
    {
        "title": "Image Analysis in Astronomy for very large vision machine",
        "authors": [
            "G. Iovane"
        ],
        "summary": "It is developed a very complex system (hardware/software) to detect luminosity variations connected with the discovery of new planets outside the Solar System. Traditional imaging approaches are very demanding in terms of computing time; then, the implementation of an automatic vision and decision software architecture is presented. It allows to perform an on-line discrimination of interesting events by using two levels of triggers. A fundamental challenge was to work with very large CCD camera (even 16k*16k pixels) in line with very large telescopes. Then, the architecture can use a distributed parallel network system based on a maximum of 256 standard workstations.",
        "published": "2003-08-22T18:47:33Z",
        "link": "http://arxiv.org/abs/cs/0308038v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.DC",
            "I.4,I.5"
        ]
    },
    {
        "title": "Proposed Specification of a Distributed XML-Query Network",
        "authors": [
            "Christian Thiemann",
            "Michael Schlenker",
            "Thomas Severiens"
        ],
        "summary": "W3C's XML-Query language offers a powerful instrument for information retrieval on XML repositories. This article describes an implementation of this retrieval in a real world's scenario. Distributed XML-Query processing reduces load on every single attending node to an acceptable level. The network allows every participant to control their computing load themselves. Furthermore XML-repositories may stay at the rights holder, so every Data-Provider can decide, whether to process critical queries or not. If Data-Providers keep redundant information, this distributed network improves reliability of information with duplicates removed.",
        "published": "2003-09-13T19:24:56Z",
        "link": "http://arxiv.org/abs/cs/0309022v2",
        "categories": [
            "cs.DC",
            "cs.IR",
            "H.3.4"
        ]
    },
    {
        "title": "A thought experiment on Quantum Mechanics and Distributed Failure   Detection",
        "authors": [
            "Mark C. Little"
        ],
        "summary": "One of the biggest problems in current distributed systems is that presented by one machine attempting to determine the liveness of another in a timely manner. Unfortunately, the symptoms exhibited by a failed machine can also be the result of other causes, e.g., an overloaded machine or network which drops messages, making it impossible to detect a machine failure with cetainty until that machine recovers. This is a well understood problem and one which has led to a large body of research into failure suspectors: since it is not possible to detect a failure, the best one can do is suspect a failure and program accordingly. However, one machine's suspicions may not be the same as another's; therefore, these algorithms spend a considerable effort in ensuring a consistent view among all available machines of who is suspects of being failed. This paper describes a thought experiment on how quantum mechanics may be used to provide a failure detector that is guaranteed to give both accurate and instantaneous information about the liveness of machines, no matter the distances involved.",
        "published": "2003-09-15T10:43:47Z",
        "link": "http://arxiv.org/abs/cs/0309026v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.4.5"
        ]
    },
    {
        "title": "A distributed algorithm to find k-dominating sets",
        "authors": [
            "L. D. Penso",
            "V. C. Barbosa"
        ],
        "summary": "We consider a connected undirected graph $G(n,m)$ with $n$ nodes and $m$ edges. A $k$-dominating set $D$ in $G$ is a set of nodes having the property that every node in $G$ is at most $k$ edges away from at least one node in $D$. Finding a $k$-dominating set of minimum size is NP-hard. We give a new synchronous distributed algorithm to find a $k$-dominating set in $G$ of size no greater than $\\lfloor n/(k+1)\\rfloor$. Our algorithm requires $O(k\\log^*n)$ time and $O(m\\log k+n\\log k\\log^*n)$ messages to run. It has the same time complexity as the best currently known algorithm, but improves on that algorithm's message complexity and is, in addition, conceptually simpler.",
        "published": "2003-09-23T01:14:43Z",
        "link": "http://arxiv.org/abs/cs/0309040v1",
        "categories": [
            "cs.DC",
            "F.1.2; F.2.2"
        ]
    },
    {
        "title": "On reducing the complexity of matrix clocks",
        "authors": [
            "L. M. A. Drummond",
            "V. C. Barbosa"
        ],
        "summary": "Matrix clocks are a generalization of the notion of vector clocks that allows the local representation of causal precedence to reach into an asynchronous distributed computation's past with depth $x$, where $x\\ge 1$ is an integer. Maintaining matrix clocks correctly in a system of $n$ nodes requires that everymessage be accompanied by $O(n^x)$ numbers, which reflects an exponential dependency of the complexity of matrix clocks upon the desired depth $x$. We introduce a novel type of matrix clock, one that requires only $nx$ numbers to be attached to each message while maintaining what for many applications may be the most significant portion of the information that the original matrix clock carries. In order to illustrate the new clock's applicability, we demonstrate its use in the monitoring of certain resource-sharing computations.",
        "published": "2003-09-23T12:57:59Z",
        "link": "http://arxiv.org/abs/cs/0309042v1",
        "categories": [
            "cs.DC",
            "C.2.4; D.1.3"
        ]
    },
    {
        "title": "The combinatorics of resource sharing",
        "authors": [
            "V. C. Barbosa"
        ],
        "summary": "We discuss general models of resource-sharing computations, with emphasis on the combinatorial structures and concepts that underlie the various deadlock models that have been proposed, the design of algorithms and deadlock-handling policies, and concurrency issues. These structures are mostly graph-theoretic in nature, or partially ordered sets for the establishment of priorities among processes and acquisition orders on resources. We also discuss graph-coloring concepts as they relate to resource sharing.",
        "published": "2003-09-23T13:57:01Z",
        "link": "http://arxiv.org/abs/cs/0309044v1",
        "categories": [
            "cs.OS",
            "cs.DC",
            "D.1.3; D.4.1"
        ]
    },
    {
        "title": "Control and Debugging of Distributed Programs Using Fiddle",
        "authors": [
            "Joao Lourenco",
            "Jose C. Cunha",
            "Vitor Moreira"
        ],
        "summary": "The main goal of Fiddle, a distributed debugging engine, is to provide a flexible platform for developing debugging tools. Fiddle provides a layered set of interfaces with a minimal set of debugging functionalities, for the inspection and control of distributed and multi-threaded applications.   This paper illustrates how Fiddle is used to support integrated testing and debugging. The approach described is based on a tool, called Deipa, that interprets sequences of commands read from an input file, generated by an independent testing tool. Deipa acts as a Fiddle client, in order to enforce specific execution paths in a distributed PVM program. Other Fiddle clients may be used along with Deipa for the fine debugging at process level. Fiddle and Deipa functionalities and architectures are described, and a working example shows a step-by-step application of these tools.",
        "published": "2003-09-26T11:49:10Z",
        "link": "http://arxiv.org/abs/cs/0309049v1",
        "categories": [
            "cs.DC",
            "D.2.5"
        ]
    },
    {
        "title": "The Wake Up and Report Problem is Time-Equivalent to the Firing Squad   Synchronization Problem",
        "authors": [
            "Darin Goldstein",
            "Nick Meyer"
        ],
        "summary": "We consider several problems relating to strongly-connected directed networks of identical finite-state processors that work synchronously in discrete time steps. The conceptually simplest of these is the Wake Up and Report Problem; this is the problem of having a unique \"root\" processor send a signal to all other processors in the network and then enter a special \"done\" state only when all other processors have received the signal. The most difficult of the problems we consider is the classic Firing Squad Synchronization Problem; this is the much-studied problem of achieving macro-synchronization in a network given micro-synchronization. We show via a complex algorithmic application of the \"snake\" data structure first introduced in Even, Litman, and Winkler [ELW], that these two problems are asymptotically time-equivalent up to a constant factor. This result leads immediately to the inclusion of several other related problems into this new asymptotic time-class.",
        "published": "2003-10-05T18:27:22Z",
        "link": "http://arxiv.org/abs/cs/0310003v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "E.1; C.2.1; C.2.2"
        ]
    },
    {
        "title": "Determination of the Topology of a Directed Network",
        "authors": [
            "Darin Goldstein"
        ],
        "summary": "We consider strongly-connected directed networks of identical synchronous, finite-state processors with in- and out-degree uniformly bounded by a network constant. Via a straightforward extension of Ostrovsky and Wilkerson's Backwards Communication Algorithm in [OW], we exhibit a protocol which solves the Global Topology Determination Problem, the problem of having the root processor map the global topology of a network of unknown size and topology, with running time O(ND) where N represents the number of processors and D represents the diameter of the network. A simple counting argument suffices to show that the Global Topology Determination Problem has time-complexity Omega(N logN) which makes the protocol presented asymptotically time-optimal for many large networks.",
        "published": "2003-10-05T18:41:16Z",
        "link": "http://arxiv.org/abs/cs/0310004v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "C.2.1; C.2.2; E.1"
        ]
    },
    {
        "title": "Poster on MPI application in Computational Fluid Dynamics",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "Poster-presentation of the paper \"Message Passing Fluids: molecules as processes in parallel computational fluids\" held at \"EURO PVMMPI 2003\" Congress; the paper is on the proceedings \"Recent Advances in Parallel Virtual Machine and Message Passing Interface\", 10th European PVM/MPI User's Group Meeting, LNCS 2840, Springer-Verlag, Dongarra-Laforenza-Orlando editors, pp. 550-554.",
        "published": "2003-10-06T14:20:00Z",
        "link": "http://arxiv.org/abs/cs/0310008v1",
        "categories": [
            "cs.DC",
            "cs.GR",
            "D.1.3"
        ]
    },
    {
        "title": "Optimizing Noncontiguous Accesses in MPI-IO",
        "authors": [
            "Rajeev Thakur",
            "William Gropp",
            "Ewing Lusk"
        ],
        "summary": "The I/O access patterns of many parallel applications consist of accesses to a large number of small, noncontiguous pieces of data. If an application's I/O needs are met by making many small, distinct I/O requests, however, the I/O performance degrades drastically. To avoid this problem, MPI-IO allows users to access noncontiguous data with a single I/O function call, unlike in Unix I/O. In this paper, we explain how critical this feature of MPI-IO is for high performance and how it enables implementations to perform optimizations. We first provide a classification of the different ways of expressing an application's I/O needs in MPI-IO--we classify them into four levels, called level~0 through level~3. We demonstrate that, for applications with noncontiguous access patterns, the I/O performance improves dramatically if users write their applications to make level-3 requests (noncontiguous, collective) rather than level-0 requests (Unix style). We then describe how our MPI-IO implementation, ROMIO, delivers high performance for noncontiguous requests. We explain in detail the two key optimizations ROMIO performs: data sieving for noncontiguous requests from one process and collective I/O for noncontiguous requests from multiple processes. We describe how we have implemented these optimizations portably on multiple machines and file systems, controlled their memory requirements, and also achieved high performance. We demonstrate the performance and portability with performance results for three applications--an astrophysics-application template (DIST3D), the NAS BTIO benchmark, and an unstructured code (UNSTRUC)--on five different parallel machines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000.",
        "published": "2003-10-15T19:35:00Z",
        "link": "http://arxiv.org/abs/cs/0310029v1",
        "categories": [
            "cs.DC",
            "B.4.3; D.1.3"
        ]
    },
    {
        "title": "A Particular Bug Trap: Execution Replay Using Virtual Machines",
        "authors": [
            "Oliver Oppitz"
        ],
        "summary": "Execution-replay (ER) is well known in the literature but has been restricted to special system architectures for many years. Improved hardware resources and the maturity of virtual machine technology promise to make ER useful for a broader range of development projects.   This paper describes an approach to create a practical, generic ER infrastructure for desktop PC systems using virtual machine technology. In the created VM environment arbitrary application programs will run and be replayed unmodified, neither instrumentation nor recompilation are required.",
        "published": "2003-10-15T20:54:14Z",
        "link": "http://arxiv.org/abs/cs/0310030v1",
        "categories": [
            "cs.DC",
            "D.2.5"
        ]
    },
    {
        "title": "Design and Implementation of MPICH2 over InfiniBand with RDMA Support",
        "authors": [
            "Jiuxing Liu",
            "Weihang Jiang",
            "Pete Wyckoff",
            "Dhabaleswar K. Panda",
            "David Ashton",
            "Darius Buntinas",
            "William Gropp",
            "Brian Toonen"
        ],
        "summary": "For several years, MPI has been the de facto standard for writing parallel applications. One of the most popular MPI implementations is MPICH. Its successor, MPICH2, features a completely new design that provides more performance and flexibility. To ensure portability, it has a hierarchical structure based on which porting can be done at different levels. In this paper, we present our experiences designing and implementing MPICH2 over InfiniBand. Because of its high performance and open standard, InfiniBand is gaining popularity in the area of high-performance computing. Our study focuses on optimizing the performance of MPI-1 functions in MPICH2. One of our objectives is to exploit Remote Direct Memory Access (RDMA) in Infiniband to achieve high performance. We have based our design on the RDMA Channel interface provided by MPICH2, which encapsulates architecture-dependent communication functionalities into a very small set of functions. Starting with a basic design, we apply different optimizations and also propose a zero-copy-based design. We characterize the impact of our optimizations and designs using microbenchmarks. We have also performed an application-level evaluation using the NAS Parallel Benchmarks. Our optimized MPICH2 implementation achieves 7.6 $\\mu$s latency and 857 MB/s bandwidth, which are close to the raw performance of the underlying InfiniBand layer. Our study shows that the RDMA Channel interface in MPICH2 provides a simple, yet powerful, abstraction that enables implementations with high performance by exploiting RDMA operations in InfiniBand. To the best of our knowledge, this is the first high-performance design and implementation of MPICH2 on InfiniBand using RDMA support.",
        "published": "2003-10-30T18:11:28Z",
        "link": "http://arxiv.org/abs/cs/0310059v1",
        "categories": [
            "cs.AR",
            "cs.DC",
            "C.1.4; C.2.4"
        ]
    },
    {
        "title": "OGSA/Globus Evaluation for Data Intensive Applications",
        "authors": [
            "A. Demichev",
            "D. Foster",
            "V. Kalyaev",
            "A. Kryukov",
            "M. Lamanna",
            "V. Pose",
            "R. B. Da Rocha",
            "C. Wang"
        ],
        "summary": "We present an architecture of Globus Toolkit 3 based testbed intended for evaluation of applicability of the Open Grid Service Architecture (OGSA) for Data Intensive Applications.",
        "published": "2003-11-10T11:12:26Z",
        "link": "http://arxiv.org/abs/cs/0311009v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Problem of Application Job Monitoring in GRID Systems",
        "authors": [
            "V. Kalyaev",
            "A. Kryukov"
        ],
        "summary": "We present a new approach to monitoring of the execution process of an application job in the GRID environment. The main point of the approach is use of GRID ervices to access monitoring information with the security level available in GRID.",
        "published": "2003-11-10T11:39:04Z",
        "link": "http://arxiv.org/abs/cs/0311010v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "LCG-1 Deployment and usage experience",
        "authors": [
            "L. Shamardin"
        ],
        "summary": "LCG-1 is the second release of the software framework for the LHC Computing Grid project. In our work we describe the installation process, arising problems and their solutions, and configuration tuning details of the complete LCG-1 site, including all LCG elements required for the self-sufficient site.",
        "published": "2003-11-17T13:19:31Z",
        "link": "http://arxiv.org/abs/cs/0311021v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Fine-Grained Authorization for Job Execution in the Grid: Design and   Implementation",
        "authors": [
            "K. Keahey",
            "V. Welch",
            "S. Lang",
            "B. Liu",
            "S. Meder"
        ],
        "summary": "In this paper we describe our work on enabling fine-grained authorization for resource usage and management. We address the need of virtual organizations to enforce their own polices in addition to those of the resource owners, in regard to both resource consumption and job management. To implement this design, we propose changes and extensions to the Globus Toolkit's version 2 resource management mechanism. We describe the prototype and the policy language that we designed to express fine-grained policies, and we present an analysis of our solution.",
        "published": "2003-11-20T15:02:13Z",
        "link": "http://arxiv.org/abs/cs/0311025v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "D.4.6; D.4.7"
        ]
    },
    {
        "title": "Using Counterfactuals in Knowledge-Based Programming",
        "authors": [
            "Joseph Y. Halpern",
            "Yoram Moses"
        ],
        "summary": "This paper adds counterfactuals to the framework of knowledge-based programs of Fagin, Halpern, Moses, and Vardi. The use of counterfactuals is illustrated by designing a protocol in which an agent stops sending messages once it knows that it is safe to do so. Such behavior is difficult to capture in the original framework because it involves reasoning about counterfactual executions, including ones that are not consistent with the protocol. Attempts to formalize these notions without counterfactuals are shown to lead to rather counterintuitive behavior.",
        "published": "2003-11-20T17:26:27Z",
        "link": "http://arxiv.org/abs/cs/0311028v1",
        "categories": [
            "cs.DC",
            "cs.AI",
            "F.4.1, F.3.1, I.2.4, C.2.2, C.2.4"
        ]
    },
    {
        "title": "S-ToPSS: Semantic Toronto Publish/Subscribe System",
        "authors": [
            "Milenko Petrovic",
            "Ioana Burcea",
            "Hans-Arno Jacobsen"
        ],
        "summary": "The increase in the amount of data on the Internet has led to the development of a new generation of applications based on selective information dissemination where, data is distributed only to interested clients. Such applications require a new middleware architecture that can efficiently match user interests with available information. Middleware that can satisfy this requirement include event-based architectures such as publish-subscribe systems. In this demonstration paper we address the problem of semantic matching. We investigate how current publish/subscribe systems can be extended with semantic capabilities. Our main contribution is the development and validation (through demonstration) of a semantic pub/sub system prototype S-ToPSS (Semantic Toronto Publish/Subscribe System).",
        "published": "2003-11-26T21:55:53Z",
        "link": "http://arxiv.org/abs/cs/0311041v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4"
        ]
    },
    {
        "title": "I know what you mean: semantic issues in Internet-scale   publish/subscribe systems",
        "authors": [
            "Ioana Burcea",
            "Milenko Petrovic",
            "Hans-Arno Jacobsen"
        ],
        "summary": "In recent years, the amount of information on the Internet has increased exponentially developing great interest in selective information dissemination systems. The publish/subscribe paradigm is particularly suited for designing systems for routing information and requests according to their content throughout wide-area network of brokers. Current publish/subscribe systems use limited syntax-based content routing but since publishers and subscribers are anonymous and decoupled in time, space and location, often over wide-area network boundary, they do not necessarily speak the same language. Consequently, adding semantics to current publish/subscribe systems is important. In this paper we identify and examine the issues in developing semantic-based content routing for publish/subscribe broker networks.",
        "published": "2003-11-27T16:00:32Z",
        "link": "http://arxiv.org/abs/cs/0311047v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.2.4"
        ]
    },
    {
        "title": "Methods to Model-Check Parallel Systems Software",
        "authors": [
            "Olga Shumsky Matlin",
            "William McCune",
            "Ewing Lusk"
        ],
        "summary": "We report on an effort to develop methodologies for formal verification of parts of the Multi-Purpose Daemon (MPD) parallel process management system. MPD is a distributed collection of communicating processes. While the individual components of the collection execute simple algorithms, their interaction leads to unexpected errors that are difficult to uncover by conventional means. Two verification approaches are discussed here: the standard model checking approach using the software model checker SPIN and the nonstandard use of a general-purpose first-order resolution-style theorem prover OTTER to conduct the traditional state space exploration. We compare modeling methodology and analyze performance and scalability of the two methods with respect to verification of MPD.",
        "published": "2003-12-05T16:57:41Z",
        "link": "http://arxiv.org/abs/cs/0312012v1",
        "categories": [
            "cs.LO",
            "cs.DC",
            "D.2.4; D.1.3"
        ]
    },
    {
        "title": "GridEmail: A Case for Economically Regulated Internet-based   Interpersonal Communications",
        "authors": [
            "Manjuka Soysa",
            "Rajkumar Buyya",
            "Baikunth Nath"
        ],
        "summary": "Email has emerged as a dominant form of electronic communication between people. Spam is a major problem for email users, with estimates of up to 56% of email falling into that category. Control of Spam is being attempted with technical and legislative methods. In this paper we look at email and spam from a supply-demand perspective. We propose Gridemail, an email system based on an economy of communicating parties, where participants? motivations are represented as pricing policies and profiles. This system is expected to help people regulate their personal communications to suit their conditions, and help in removing unwanted messages.",
        "published": "2003-12-12T11:42:17Z",
        "link": "http://arxiv.org/abs/cs/0312022v1",
        "categories": [
            "cs.DC",
            "C.2.2, C.2.4"
        ]
    },
    {
        "title": "Distributed WWW Programming using (Ciao-)Prolog and the PiLLoW library",
        "authors": [
            "Daniel Cabeza",
            "Manuel V. Hermenegildo"
        ],
        "summary": "We discuss from a practical point of view a number of issues involved in writing distributed Internet and WWW applications using LP/CLP systems. We describe PiLLoW, a public-domain Internet and WWW programming library for LP/CLP systems that we have designed in order to simplify the process of writing such applications. PiLLoW provides facilities for accessing documents and code on the WWW; parsing, manipulating and generating HTML and XML structured documents and data; producing HTML forms; writing form handlers and CGI-scripts; and processing HTML/XML templates. An important contribution of PiLLoW is to model HTML/XML code (and, thus, the content of WWW pages) as terms. The PiLLoW library has been developed in the context of the Ciao Prolog system, but it has been adapted to a number of popular LP/CLP systems, supporting most of its functionality. We also describe the use of concurrency and a high-level model of client-server interaction, Ciao Prolog's active modules, in the context of WWW programming. We propose a solution for client-side downloading and execution of Prolog code, using generic browsers. Finally, we also provide an overview of related work on the topic.",
        "published": "2003-12-16T19:09:04Z",
        "link": "http://arxiv.org/abs/cs/0312031v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.1.6"
        ]
    },
    {
        "title": "Using virtual processors for SPMD parallel programs",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this paper I describe some results on the use of virtual processors technology for parallelize some SPMD computational programs. The tested technology is the INTEL Hyper Threading on real processors, and the programs are MATLAB scripts for floating points computation. The conclusions of the work concern on the utility and limits of the used approach. The main result is that using virtual processors is a good technique for improving parallel programs not only for memory-based computations, but in the case of massive disk-storage operations too.",
        "published": "2003-12-21T14:37:24Z",
        "link": "http://arxiv.org/abs/cs/0312049v1",
        "categories": [
            "cs.DC",
            "F.1.2"
        ]
    },
    {
        "title": "Complex Grid Computing",
        "authors": [
            "Luciano da Fontoura Costa",
            "Gonzalo Travieso",
            "Carlos Antonio Ruggiero"
        ],
        "summary": "This article investigates the performance of grid computing systems whose interconnections are given by random and scale-free complex network models. Regular networks, which are common in parallel computing architectures, are also used as a standard for comparison. The processing load is assigned to the processing nodes on demand, and the efficiency of the overall computing is quantified in terms of the respective speed-ups. It is found that random networks allow higher computing efficiency than their scale-free counterparts as a consequence of the smaller number of isolated clusters implied by the former model. At the same time, for fixed cluster sizes, the scale free model tend to provide slightly better efficiency. Two modifications of the random and scale free paradigms, where new connections tend to favor more recently added nodes, are proposed and shown to be more effective for grid computing than the standard models. A well-defined correlation is observed between the topological properties of the network and their respective computing efficiency.",
        "published": "2003-12-23T14:36:39Z",
        "link": "http://arxiv.org/abs/cond-mat/0312603v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.DC"
        ]
    },
    {
        "title": "Design Guidelines for Landmarks to Support Navigation in Virtual   Environments",
        "authors": [
            "Norman G. Vinson"
        ],
        "summary": "Unfamiliar, large-scale virtual environments are difficult to navigate. This paper presents design guidelines to ease navigation in such virtual environments. The guidelines presented here focus on the design and placement of landmarks in virtual environments. Moreover, the guidelines are based primarily on the extensive empirical literature on navigation in the real world. A rationale for this approach is provided by the similarities between navigational behavior in real and virtual environments.",
        "published": "2003-03-31T22:49:29Z",
        "link": "http://arxiv.org/abs/cs/0304001v1",
        "categories": [
            "cs.HC",
            "H.5.4; I.6"
        ]
    },
    {
        "title": "The Mad Hatter&acute;s Cocktail Party: A Social Mobile Audio Space   Supporting Multiple Simultaneous Conversations",
        "authors": [
            "Paul M. Aoki",
            "Matthew Romaine",
            "Margaret H. Szymanski",
            "James D. Thornton",
            "Daniel Wilson",
            "Allison Woodruff"
        ],
        "summary": "This paper presents a mobile audio space intended for use by gelled social groups. In face-to-face interactions in such social groups, conversational floors change frequently, e.g., two participants split off to form a new conversational floor, a participant moves from one conversational floor to another, etc. To date, audio spaces have provided little support for such dynamic regroupings of participants, either requiring that the participants explicitly specify with whom they wish to talk or simply presenting all participants as though they are in a single floor. By contrast, the audio space described here monitors participant behavior to identify conversational floors as they emerge. The system dynamically modifies the audio delivered to each participant to enhance the salience of the participants with whom they are currently conversing. We report a user study of the system, focusing on conversation analytic results.",
        "published": "2003-04-01T05:15:05Z",
        "link": "http://arxiv.org/abs/cs/0304002v1",
        "categories": [
            "cs.HC",
            "cs.SD",
            "H.4.3; H.5.3"
        ]
    },
    {
        "title": "The Ubiquitous Interactor - Device Independent Access to Mobile Services",
        "authors": [
            "Stina Nylander",
            "Markus Bylund",
            "Annika Waern"
        ],
        "summary": "The Ubiquitous Interactor (UBI) addresses the problems of design and development that arise around services that need to be accessed from many different devices. In UBI, the same service can present itself with different user interfaces on different devices. This is done by separating interaction between users and services from presentation. The interaction is kept the same for all devices, and different presentation information is provided for different devices. This way, tailored user interfaces for many different devices can be created without multiplying development and maintenance work. In this paper we describe the system design of UBI, the system implementation, and two services implemented for the system: a calendar service and a stockbroker service.",
        "published": "2003-05-05T11:32:59Z",
        "link": "http://arxiv.org/abs/cs/0305003v1",
        "categories": [
            "cs.HC",
            "H.5.2"
        ]
    },
    {
        "title": "OO Model of the STAR offline production \"Event Display\" and its   implementation based on Qt-ROOT",
        "authors": [
            "Valeri Fine",
            "Jerome Lauret",
            "Victor Perevoztchikov"
        ],
        "summary": "The paper presents the \"Event Display\" package for the STAR offline production as a special visualization tool to debug the reconstruction code. This can be achieved if an author of the algorithm / code may build his/her own custom Event Display alone from the base software blocks and re-used some well-designed, easy to learn user-friendly patterns. For STAR offline production Event Display ROOT with Qt lower level interface was chosen as the base tools.",
        "published": "2003-06-14T05:42:43Z",
        "link": "http://arxiv.org/abs/cs/0306087v1",
        "categories": [
            "cs.HC",
            "cs.GR",
            "I.3.7; D.1.5"
        ]
    },
    {
        "title": "Primary Numbers Database for ATLAS Detector Description Parameters",
        "authors": [
            "A. Vaniachine",
            "S. Eckmann",
            "D. Malon",
            "P. Nevski",
            "T. Wenaus"
        ],
        "summary": "We present the design and the status of the database for detector description parameters in ATLAS experiment. The ATLAS Primary Numbers are the parameters defining the detector geometry and digitization in simulations, as well as certain reconstruction parameters. Since the detailed ATLAS detector description needs more than 10,000 such parameters, a preferred solution is to have a single verified source for all these data. The database stores the data dictionary for each parameter collection object, providing schema evolution support for object-based retrieval of parameters. The same Primary Numbers are served to many different clients accessing the database: the ATLAS software framework Athena, the Geant3 heritage framework Atlsim, the Geant4 developers framework FADS/Goofy, the generator of XML output for detector description, and several end-user clients for interactive data navigation, including web-based browsers and ROOT. The choice of the MySQL database product for the implementation provides additional benefits: the Primary Numbers database can be used on the developers laptop when disconnected (using the MySQL embedded server technology), with data being updated when the laptop is connected (using the MySQL database replication).",
        "published": "2003-06-16T19:59:17Z",
        "link": "http://arxiv.org/abs/cs/0306103v1",
        "categories": [
            "cs.DB",
            "cs.HC",
            "H.2.4"
        ]
    },
    {
        "title": "Supporting Out-of-turn Interactions in a Multimodal Web Interface",
        "authors": [
            "Atul Shenoy",
            "Naren Ramakrishnan",
            "Manuel A. Perez-Quinones",
            "Srinidhi Varadarajan"
        ],
        "summary": "Multimodal interfaces are becoming increasingly important with the advent of mobile devices, accessibility considerations, and novel software technologies that combine diverse interaction media. This article investigates systems support for web browsing in a multimodal interface. Specifically, we outline the design and implementation of a software framework that integrates hyperlink and speech modes of interaction. Instead of viewing speech as merely an alternative interaction medium, the framework uses it to support out-of-turn interaction, providing a flexibility of information access not possible with hyperlinks alone. This approach enables the creation of websites that adapt to the needs of users, yet permits the designer fine-grained control over what interactions to support. Design methodology, implementation details, and two case studies are presented.",
        "published": "2003-07-04T13:44:04Z",
        "link": "http://arxiv.org/abs/cs/0307011v1",
        "categories": [
            "cs.IR",
            "cs.HC",
            "H.5"
        ]
    },
    {
        "title": "Web Access to Cultural Heritage for the Disabled",
        "authors": [
            "Jonathan P. Bowen"
        ],
        "summary": "Physical disabled access is something that most cultural institutions such as museums consider very seriously. Indeed, there are normally legal requirements to do so. However, online disabled access is still a relatively novel and developing field. Many cultural organizations have not yet considered the issues in depth and web developers are not necessarily experts either. The interface for websites is normally tested with major browsers, but not with specialist software like text to audio converters for the blind or against the relevant accessibility and validation standards. We consider the current state of the art in this area, especially with respect to aspects of particular importance to the access of cultural heritage.",
        "published": "2003-07-30T18:37:14Z",
        "link": "http://arxiv.org/abs/cs/0307068v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.IR",
            "H.1.2;H.3.5;H.3.7;H.5.2;H.5.3;H.5.4;I.7.2;K.5.0"
        ]
    },
    {
        "title": "Disabled Access for Museum Websites",
        "authors": [
            "Jonathan P. Bowen"
        ],
        "summary": "Physical disabled access is something that most museums consider very seriously. Indeed, there are normally legal requirements to do so. However, online disabled access is still a relatively novel field. Most museums have not yet considered the issues in depth. The Human-Computer Interface for their websites is normally tested with major browsers, but not with specialist browsers or against the relevant accessibility and validation standards. We consider the current state of the art in this area and mention an accessibility survey of some museum websites.",
        "published": "2003-08-03T17:57:24Z",
        "link": "http://arxiv.org/abs/cs/0308005v2",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.IR",
            "H.1.2;H.3.5;H.3.7;H.5.1;H.5.2;H.5.3;H.5.4;I.7.2;K.5.0"
        ]
    },
    {
        "title": "Media Affordances of a Mobile Push-To-Talk Communication Service",
        "authors": [
            "Allison Woodruff",
            "Paul M. Aoki"
        ],
        "summary": "This paper presents an exploratory study of college-age students using two-way, push-to-talk cellular radios. We describe the observed and reported use of cellular radio by the participants, the activities and purposes for which they adopted it, and their responses. We then examine these empirical results using mediated communication theory. Cellular radios have a unique combination of affordances relative to other media used by this age group, including instant messaging (IM) and mobile phones; the results of our analysis do suggest explanations for some observed phenomena but also highlight the counter-intuitive nature of other phenomena. For example, although the radios have many important dissimilarities with IM from the viewpoint of mediated communication theory, the observed use patterns resembled those of IM to a surprising degree.",
        "published": "2003-08-31T20:13:10Z",
        "link": "http://arxiv.org/abs/cs/0309001v2",
        "categories": [
            "cs.HC",
            "H.4.3; H.5.3"
        ]
    },
    {
        "title": "Semi-metric Behavior in Document Networks and its Application to   Recommendation Systems",
        "authors": [
            "L. M. Rocha"
        ],
        "summary": "Recommendation systems for different Document Networks (DN) such as the World Wide Web (WWW) and Digital Libraries, often use distance functions extracted from relationships among documents and keywords. For instance, documents in the WWW are related via a hyperlink network, while documents in bibliographic databases are related by citation and collaboration networks. Furthermore, documents are related to keyterms. The distance functions computed from these relations establish associative networks among items of the DN, referred to as Distance Graphs, which allow recommendation systems to identify relevant associations for individual users. However, modern recommendation systems need to integrate associative data from multiple sources such as different databases, web sites, and even other users. Thus, we are presented with a problem of combining evidence (about associations between items) from different sources characterized by distance functions. In this paper we describe our work on (1) inferring relevant associations from, as well as characterizing, semi-metric distance graphs and (2) combining evidence from different distance graphs in a recommendation system. Regarding (1), we present the idea of semi-metric distance graphs, and introduce ratios to measure semi-metric behavior. We compute these ratios for several DN such as digital libraries and web sites and show that they are useful to identify implicit associations. Regarding (2), we describe an algorithm to combine evidence from distance graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed for a recommendation system named TalkMine.",
        "published": "2003-09-09T05:24:03Z",
        "link": "http://arxiv.org/abs/cs/0309013v1",
        "categories": [
            "cs.IR",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.AI",
            "cs.DL",
            "cs.HC",
            "cs.MA",
            "H.3.0; H.3.3, H.3.4; H.3.6; H.3.7; I.2.11; H.3.5"
        ]
    },
    {
        "title": "The Interactive Minority Game: a Web-based investigation of human market   interactions",
        "authors": [
            "Paolo Laureti",
            "Peter Ruch",
            "Joseph Wakeling",
            "Yi-Cheng Zhang"
        ],
        "summary": "The unprecedented access offered by the World Wide Web brings with it the potential to gather huge amounts of data on human activities. Here we exploit this by using a toy model of financial markets, the Minority Game (MG), to investigate human speculative trading behaviour and information capacity. Hundreds of individuals have played a total of tens of thousands of game turns against computer-controlled agents in the Web-based_Interactive Minority Game_. The analytical understanding of the MG permits fine-tuning of the market situations encountered, allowing for investigation of human behaviour in a variety of controlled environments. In particular, our results indicate a transition in players' decision-making, as the markets become more difficult, between deductive behaviour making use of short-term trends in the market, and highly repetitive behaviour that ignores entirely the market history, yet outperforms random decision-making.   PACS: 02.50.Le; 89.65.Gh; 89.70.+c   Keywords: Decision theory and game theory; Economics and financial markets; Information theory",
        "published": "2003-09-09T17:25:11Z",
        "link": "http://arxiv.org/abs/nlin/0309033v2",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.HC",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Re-Finding Found Things: An Exploratory Study of How Users Re-Find   Information",
        "authors": [
            "Robert G. Capra",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "The problem of how people find information is studied extensively; however, the problem of how people organize, re-use, and re-find information that they have found is not as well understood. Recently, several projects have conducted in-situ studies to explore how people re-find and re-use information. Here, we present results and observations from a controlled, laboratory study of refinding information found on the web.   Our study was conducted as a collaborative exercise with pairs of participants. One participant acted as a retriever, helping the other participant re-find information by telephone. This design allowed us to gain insight into the strategies that users employed to re-find information, and into how domain artifacts and contextual information were used to aid the re-finding process. We also introduced the ability for users to add their own explicitly artifacts in the form of making annotations on the web pages they viewed.   We observe that re-finding often occurs as a two stage, iterative process in which users first attempt to locate an information source (search), and once found, begin a process to find the specific information being sought (browse). Our findings are consistent with research on waypoints; orienteering approaches to re-finding; and navigation of electronic spaces. Furthermore, we observed that annotations were utilized extensively, indicating that explicitly added context by the user can play an important role in re-finding.",
        "published": "2003-10-06T19:36:00Z",
        "link": "http://arxiv.org/abs/cs/0310011v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.1.2; H.3.3; H.5.2"
        ]
    },
    {
        "title": "WebTeach in practice: the entrance test to the Engineering faculty in   Florence",
        "authors": [
            "Franco Bagnoli",
            "Fabio Franci",
            "Francesco Mugelli",
            "Andrea Sterbini"
        ],
        "summary": "We present the WebTeach project, formed by a web interface to database for test management, a wiki site for the diffusion of teaching material and student forums, and a suite for the generation of multiple-choice mathematical quiz with automatic elaboration of forms. This system has been massively tested for the entrance test to the Engineering Faculty of the University of Florence, Italy",
        "published": "2003-10-08T14:07:42Z",
        "link": "http://arxiv.org/abs/cs/0310013v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "K.3.1"
        ]
    },
    {
        "title": "How Push-To-Talk Makes Talk Less Pushy",
        "authors": [
            "Allison Woodruff",
            "Paul M. Aoki"
        ],
        "summary": "This paper presents an exploratory study of college-age students using two-way, push-to-talk cellular radios. We describe the observed and reported use of cellular radio by the participants. We discuss how the half-duplex, lightweight cellular radio communication was associated with reduced interactional commitment, which meant the cellular radios could be used for a wide range of conversation styles. One such style, intermittent conversation, is characterized by response delays. Intermittent conversation is surprising in an audio medium, since it is typically associated with textual media such as instant messaging. We present design implications of our findings.",
        "published": "2003-11-07T02:17:34Z",
        "link": "http://arxiv.org/abs/cs/0311006v1",
        "categories": [
            "cs.HC",
            "H.4.3; H.5.3"
        ]
    },
    {
        "title": "A Situation Calculus-based Approach To Model Ubiquitous Information   Services",
        "authors": [
            "Dong Wen-Yu",
            "Xu Ke",
            "Lin Meng-Xiang"
        ],
        "summary": "This paper presents an augmented situation calculus-based approach to model autonomous computing paradigm in ubiquitous information services. To make it practical for commercial development and easier to support autonomous paradigm imposed by ubiquitous information services, we made improvements based on Reiter's standard situation calculus. First we explore the inherent relationship between fluents and evolution: since not all fluents contribute to systems' evolution and some fluents can be derived from some others, we define those fluents that are sufficient and necessary to determine evolutional potential as decisive fluents, and then we prove that their successor states wrt to deterministic complex actions satisfy Markov property. Then, within the calculus framework we build, we introduce validity theory to model the autonomous services with application-specific validity requirements, including: validity fluents to axiomatize validity requirements, heuristic multiple alternative service choices ranging from complete acceptance, partial acceptance, to complete rejection, and validity-ensured policy to comprise such alternative service choices into organic, autonomously-computable services. Our approach is demonstrated by a ubiquitous calendaring service, ACS, throughout the paper.",
        "published": "2003-11-28T09:29:35Z",
        "link": "http://arxiv.org/abs/cs/0311052v2",
        "categories": [
            "cs.AI",
            "cs.HC",
            "I.2.0;H.1.2"
        ]
    },
    {
        "title": "Designing of a Community-based Translation Center",
        "authors": [
            "Kathleen McDevitt",
            "Manuel A. Perez-Quinones",
            "Olga I. Padilla-Falto"
        ],
        "summary": "Interfaces that support multi-lingual content can reach a broader community. We wish to extend the reach of CITIDEL, a digital library for computing education materials, to support multiple languages. By doing so, we hope that it will increase the number of users, and in turn the number of resources. This paper discusses three approaches to translation (automated translation, developer-based, and community-based), and a brief evaluation of these approaches. It proposes a design for an online community translation center where volunteers help translate interface components and educational materials available in CITIDEL.",
        "published": "2003-12-04T05:26:51Z",
        "link": "http://arxiv.org/abs/cs/0312010v1",
        "categories": [
            "cs.HC",
            "cs.DL",
            "H.5.2; H.3.7"
        ]
    },
    {
        "title": "Taking the Initiative with Extempore: Exploring Out-of-Turn Interactions   with Websites",
        "authors": [
            "Saverio Perugini",
            "Mary E. Pinney",
            "Naren Ramakrishnan",
            "Manuel A. Perez-Quinones",
            "Mary Beth Rosson"
        ],
        "summary": "We present the first study to explore the use of out-of-turn interaction in websites. Out-of-turn interaction is a technique which empowers the user to supply unsolicited information while browsing. This approach helps flexibly bridge any mental mismatch between the user and the website, in a manner fundamentally different from faceted browsing and site-specific search tools. We built a user interface (Extempore) which accepts out-of-turn input via voice or text; and employed it in a US congressional website, to determine if users utilize out-of-turn interaction for information-finding tasks, and their rationale for doing so. The results indicate that users are adept at discerning when out-of-turn interaction is necessary in a particular task, and actively interleaved it with browsing. However, users found cascading information across information-finding subtasks challenging. Therefore, this work not only improves our understanding of out-of-turn interaction, but also suggests further opportunities to enrich browsing experiences for users.",
        "published": "2003-12-08T16:25:46Z",
        "link": "http://arxiv.org/abs/cs/0312016v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.5.2; H.5.4"
        ]
    },
    {
        "title": "An Exploratory Study of Mobile Computing Use by Knowledge Workers",
        "authors": [
            "Paul Prekop"
        ],
        "summary": "This paper describes some preliminary results from a 20-week study on the use of Compaq iPAQ Personal Digital Assistants (PDAs) by 10 senior developers, analysts, technical managers, and senior organisational managers. The goal of the study was to identify what applications were used, how and where they were used, the problems and issues that arose, and how use of the iPAQs changed over the study period. The paper highlights some interesting uses of the iPAQs, and identifies some of the characteristics of successful mobile applications.",
        "published": "2003-12-09T03:44:07Z",
        "link": "http://arxiv.org/abs/cs/0312017v1",
        "categories": [
            "cs.HC",
            "H1.2;H5.2;J1"
        ]
    }
]