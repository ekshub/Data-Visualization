[
    {
        "title": "A Logic for Non-Monotone Inductive Definitions",
        "authors": [
            "Marc Denecker",
            "Eugenia Ternovska"
        ],
        "summary": "Well-known principles of induction include monotone induction and different sorts of non-monotone induction such as inflationary induction, induction over well-founded sets and iterated induction. In this work, we define a logic formalizing induction over well-founded sets and monotone and iterated induction. Just as the principle of positive induction has been formalized in FO(LFP), and the principle of inflationary induction has been formalized in FO(IFP), this paper formalizes the principle of iterated induction in a new logic for Non-Monotone Inductive Definitions (ID-logic). The semantics of the logic is strongly influenced by the well-founded semantics of logic programming. Our main result concerns the modularity properties of inductive definitions in ID-logic. Specifically, we formulate conditions under which a simultaneous definition $\\D$ of several relations is logically equivalent to a conjunction of smaller definitions $\\D_1 \\land ... \\land \\D_n$ with disjoint sets of defined predicates. The difficulty of the result comes from the fact that predicates $P_i$ and $P_j$ defined in $\\D_i$ and $\\D_j$, respectively, may be mutually connected by simultaneous induction. Since logic programming and abductive logic programming under well-founded semantics are proper fragments of our logic, our modularity results are applicable there as well.",
        "published": "2005-01-13T02:53:03Z",
        "link": "http://arxiv.org/abs/cs/0501025v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "From truth to computability II",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic is a formal theory of computational tasks and resources. Formulas in it represent interactive computational problems, and \"truth\" is understood as algorithmic solvability. Interactive computational problems, in turn, are defined as a certain sort games between a machine and its environment, with logical operators standing for operations on such games. Within the ambitious program of finding axiomatizations for incrementally rich fragments of this semantically introduced logic, the earlier article \"From truth to computability I\" proved soundness and completeness for system CL3, whose language has the so called parallel connectives (including negation), choice connectives, choice quantifiers, and blind quantifiers. The present paper extends that result to the significantly more expressive system CL4 with the same collection of logical operators. What makes CL4 expressive is the presence of two sorts of atoms in its language: elementary atoms, representing elementary computational problems (i.e. predicates, i.e. problems of zero degree of interactivity), and general atoms, representing arbitrary computational problems. CL4 conservatively extends CL3, with the latter being nothing but the general-atom-free fragment of the former. Removing the blind (classical) group of quantifiers from the language of CL4 is shown to yield a decidable logic despite the fact that the latter is still first-order. A comprehensive online source on computability logic can be found at http://www.cis.upenn.edu/~giorgi/cl.html",
        "published": "2005-01-16T13:47:47Z",
        "link": "http://arxiv.org/abs/cs/0501031v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Learning to automatically detect features for mobile robots using   second-order Hidden Markov Models",
        "authors": [
            "Olivier Aycard",
            "Jean-Francois Mari",
            "Richard Washington"
        ],
        "summary": "In this paper, we propose a new method based on Hidden Markov Models to interpret temporal sequences of sensor data from mobile robots to automatically detect features. Hidden Markov Models have been used for a long time in pattern recognition, especially in speech recognition. Their main advantages over other methods (such as neural networks) are their ability to model noisy temporal signals of variable length. We show in this paper that this approach is well suited for interpretation of temporal sequences of mobile-robot sensor data. We present two distinct experiments and results: the first one in an indoor environment where a mobile robot learns to detect features like open doors or T-intersections, the second one in an outdoor environment where a different mobile robot has to identify situations like climbing a hill or crossing a rock.",
        "published": "2005-01-24T11:05:36Z",
        "link": "http://arxiv.org/abs/cs/0501068v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Inferring knowledge from a large semantic network",
        "authors": [
            "Dominique Dutoit",
            "Thierry Poibeau"
        ],
        "summary": "In this paper, we present a rich semantic network based on a differential analysis. We then detail implemented measures that take into account common and differential features between words. In a last section, we describe some industrial applications.",
        "published": "2005-01-25T16:09:11Z",
        "link": "http://arxiv.org/abs/cs/0501072v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Data Mining for Actionable Knowledge: A Survey",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The data mining process consists of a series of steps ranging from data cleaning, data selection and transformation, to pattern evaluation and visualization. One of the central problems in data mining is to make the mined patterns or knowledge actionable. Here, the term actionable refers to the mined patterns suggest concrete and profitable actions to the decision-maker. That is, the user can do something to bring direct benefits (increase in profits, reduction in cost, improvement in efficiency, etc.) to the organization's advantage. However, there has been written no comprehensive survey available on this topic. The goal of this paper is to fill the void.   In this paper, we first present two frameworks for mining actionable knowledge that are inexplicitly adopted by existing research methods. Then we try to situate some of the research on this topic from two different viewpoints: 1) data mining tasks and 2) adopted framework. Finally, we specify issues that are either not addressed or insufficiently studied yet and conclude the paper.",
        "published": "2005-01-27T12:13:16Z",
        "link": "http://arxiv.org/abs/cs/0501079v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Towards Automated Integration of Guess and Check Programs in Answer Set   Programming: A Meta-Interpreter and Applications",
        "authors": [
            "Thomas Eiter",
            "Axel Polleres"
        ],
        "summary": "Answer set programming (ASP) with disjunction offers a powerful tool for declaratively representing and solving hard problems. Many NP-complete problems can be encoded in the answer set semantics of logic programs in a very concise and intuitive way, where the encoding reflects the typical \"guess and check\" nature of NP problems: The property is encoded in a way such that polynomial size certificates for it correspond to stable models of a program. However, the problem-solving capacity of full disjunctive logic programs (DLPs) is beyond NP, and captures a class of problems at the second level of the polynomial hierarchy. While these problems also have a clear \"guess and check\" structure, finding an encoding in a DLP reflecting this structure may sometimes be a non-obvious task, in particular if the \"check\" itself is a coNP-complete problem; usually, such problems are solved by interleaving separate guess and check programs, where the check is expressed by inconsistency of the check program. In this paper, we present general transformations of head-cycle free (extended) disjunctive logic programs into stratified and positive (extended) disjunctive logic programs based on meta-interpretation techniques. The answer sets of the original and the transformed program are in simple correspondence, and, moreover, inconsistency of the original program is indicated by a designated answer set of the transformed program. Our transformations facilitate the integration of separate \"guess\" and \"check\" programs, which are often easy to obtain, automatically into a single disjunctive logic program. Our results complement recent results on meta-interpretation in ASP, and extend methods and techniques for a declarative \"guess and check\" problem solving paradigm through ASP.",
        "published": "2005-01-28T20:19:12Z",
        "link": "http://arxiv.org/abs/cs/0501084v1",
        "categories": [
            "cs.AI",
            "I.2.3; F.4.1"
        ]
    },
    {
        "title": "Multi-Vehicle Cooperative Control Using Mixed Integer Linear Programming",
        "authors": [
            "Matthew G. Earl",
            "Raffaello D'Andrea"
        ],
        "summary": "We present methods to synthesize cooperative strategies for multi-vehicle control problems using mixed integer linear programming. Complex multi-vehicle control problems are expressed as mixed logical dynamical systems. Optimal strategies for these systems are then solved for using mixed integer linear programming. We motivate the methods on problems derived from an adversarial game between two teams of robots called RoboFlag. We assume the strategy for one team is fixed and governed by state machines. The strategy for the other team is generated using our methods. Finally, we perform an average case computational complexity study on our approach.",
        "published": "2005-01-31T01:03:54Z",
        "link": "http://arxiv.org/abs/cs/0501092v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.MA",
            "I.2.9; I.2.8; I.2.11"
        ]
    },
    {
        "title": "Transforming Business Rules Into Natural Language Text",
        "authors": [
            "Manuela Kunze",
            "Dietmar Roesner"
        ],
        "summary": "The aim of the project presented in this paper is to design a system for an NLG architecture, which supports the documentation process of eBusiness models. A major task is to enrich the formal description of an eBusiness model with additional information needed in an NLG task.",
        "published": "2005-01-31T07:59:14Z",
        "link": "http://arxiv.org/abs/cs/0501093v1",
        "categories": [
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Corpus based Enrichment of GermaNet Verb Frames",
        "authors": [
            "Manuela Kunze",
            "Dietmar Roesner"
        ],
        "summary": "Lexical semantic resources, like WordNet, are often used in real applications of natural language document processing. For example, we integrated GermaNet in our document suite XDOC of processing of German forensic autopsy protocols. In addition to the hypernymy and synonymy relation, we want to adapt GermaNet's verb frames for our analysis. In this paper we outline an approach for the domain related enrichment of GermaNet verb frames by corpus based syntactic and co-occurred data analyses of real documents.",
        "published": "2005-01-31T08:36:39Z",
        "link": "http://arxiv.org/abs/cs/0501094v2",
        "categories": [
            "cs.AI",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "Context Related Derivation of Word Senses",
        "authors": [
            "Manuela Kunze",
            "Dietmar Roesner"
        ],
        "summary": "Real applications of natural language document processing are very often confronted with domain specific lexical gaps during the analysis of documents of a new domain. This paper describes an approach for the derivation of domain specific concepts for the extension of an existing ontology. As resources we need an initial ontology and a partially processed corpus of a domain. We exploit the specific characteristic of the sublanguage in the corpus. Our approach is based on syntactical structures (noun phrases) and compound analyses to extract information required for the extension of GermaNet's lexical resources.",
        "published": "2005-01-31T09:25:29Z",
        "link": "http://arxiv.org/abs/cs/0501095v1",
        "categories": [
            "cs.AI",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "Transforming and Enriching Documents for the Semantic Web",
        "authors": [
            "Dietmar Roesner",
            "Manuela Kunze",
            "Sylke Kroetzsch"
        ],
        "summary": "We suggest to employ techniques from Natural Language Processing (NLP) and Knowledge Representation (KR) to transform existing documents into documents amenable for the Semantic Web. Semantic Web documents have at least part of their semantics and pragmatics marked up explicitly in both a machine processable as well as human readable manner. XML and its related standards (XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and methodologies developed for different application scenarios.",
        "published": "2005-01-31T09:48:46Z",
        "link": "http://arxiv.org/abs/cs/0501096v1",
        "categories": [
            "cs.AI",
            "H3.1; I.2.7"
        ]
    },
    {
        "title": "Issues in Exploiting GermaNet as a Resource in Real Applications",
        "authors": [
            "Manuela Kunze",
            "Dietmar Roesner"
        ],
        "summary": "This paper reports about experiments with GermaNet as a resource within domain specific document analysis. The main question to be answered is: How is the coverage of GermaNet in a specific domain? We report about results of a field test of GermaNet for analyses of autopsy protocols and present a sketch about the integration of GermaNet inside XDOC. Our remarks will contribute to a GermaNet user's wish list.",
        "published": "2005-01-31T10:07:52Z",
        "link": "http://arxiv.org/abs/cs/0501089v1",
        "categories": [
            "cs.AI",
            "H3.1; I.2.7"
        ]
    },
    {
        "title": "Clever Search: A WordNet Based Wrapper for Internet Search Engines",
        "authors": [
            "Peter M. Kruse",
            "Andre Naujoks",
            "Dietmar Roesner",
            "Manuela Kunze"
        ],
        "summary": "This paper presents an approach to enhance search engines with information about word senses available in WordNet. The approach exploits information about the conceptual relations within the lexical-semantic net. In the wrapper for search engines presented, WordNet information is used to specify user's request or to classify the results of a publicly available web search engine, like google, yahoo, etc.",
        "published": "2005-01-31T16:00:22Z",
        "link": "http://arxiv.org/abs/cs/0501086v1",
        "categories": [
            "cs.AI",
            "H 3.3, H 5.2"
        ]
    },
    {
        "title": "Neural network ensembles: Evaluation of aggregation algorithms",
        "authors": [
            "P. M. Granitto",
            "P. F. Verdes",
            "H. A. Ceccatto"
        ],
        "summary": "Ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks. However, for aggregation to be effective, the individual networks must be as accurate and diverse as possible. An important problem is, then, how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions. We present here an extensive evaluation of several algorithms for ensemble construction, including new proposals and comparing them with standard methods in the literature. We also discuss a potential problem with sequential aggregation algorithms: the non-frequent but damaging selection through their heuristics of particularly bad ensemble members. We introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members. Our algorithms and their weighted modifications are favorably tested against other methods in the literature, producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks.",
        "published": "2005-02-01T21:22:24Z",
        "link": "http://arxiv.org/abs/cs/0502006v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Estimating mutual information and multi--information in large networks",
        "authors": [
            "Noam Slonim",
            "Gurinder S. Atwal",
            "Gasper Tkacik",
            "William Bialek"
        ],
        "summary": "We address the practical problems of estimating the information relations that characterize large networks. Building on methods developed for analysis of the neural code, we show that reliable estimates of mutual information can be obtained with manageable computational effort. The same methods allow estimation of higher order, multi--information terms. These ideas are illustrated by analyses of gene expression, financial markets, and consumer preferences. In each case, information theoretic measures correlate with independent, intuitive measures of the underlying structures in the system.",
        "published": "2005-02-03T21:11:54Z",
        "link": "http://arxiv.org/abs/cs/0502017v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Population Sizing for Genetic Programming Based Upon Decision Making",
        "authors": [
            "K. Sastry",
            "U. -M. O'Reilly",
            "D. E. Goldberg"
        ],
        "summary": "This paper derives a population sizing relationship for genetic programming (GP). Following the population-sizing derivation for genetic algorithms in Goldberg, Deb, and Clark (1992), it considers building block decision making as a key facet. The analysis yields a GP-unique relationship because it has to account for bloat and for the fact that GP solutions often use subsolution multiple times. The population-sizing relationship depends upon tree size, solution complexity, problem difficulty and building block expression probability. The relationship is used to analyze and empirically investigate population sizing for three model GP problems named ORDER, ON-OFF and LOUD. These problems exhibit bloat to differing extents and differ in whether their solutions require the use of a building block multiple times.",
        "published": "2005-02-04T03:58:26Z",
        "link": "http://arxiv.org/abs/cs/0502020v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Oiling the Wheels of Change: The Role of Adaptive Automatic Problem   Decomposition in Non--Stationary Environments",
        "authors": [
            "H. A. Abbass",
            "K. Sastry",
            "D. E. Goldberg"
        ],
        "summary": "Genetic algorithms (GAs) that solve hard problems quickly, reliably and accurately are called competent GAs. When the fitness landscape of a problem changes overtime, the problem is called non--stationary, dynamic or time--variant problem. This paper investigates the use of competent GAs for optimizing non--stationary optimization problems. More specifically, we use an information theoretic approach based on the minimum description length principle to adaptively identify regularities and substructures that can be exploited to respond quickly to changes in the environment. We also develop a special type of problems with bounded difficulties to test non--stationary optimization problems. The results provide new insights into non-stationary optimization problems and show that a search algorithm which automatically identifies and exploits possible decompositions is more robust and responds quickly to changes than a simple genetic algorithm.",
        "published": "2005-02-04T04:15:15Z",
        "link": "http://arxiv.org/abs/cs/0502021v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Sub-Structural Niching in Non-Stationary Environments",
        "authors": [
            "K. Sastry",
            "H. A. Abbass",
            "D. E. Goldberg"
        ],
        "summary": "Niching enables a genetic algorithm (GA) to maintain diversity in a population. It is particularly useful when the problem has multiple optima where the aim is to find all or as many as possible of these optima. When the fitness landscape of a problem changes overtime, the problem is called non--stationary, dynamic or time--variant problem. In these problems, niching can maintain useful solutions to respond quickly, reliably and accurately to a change in the environment. In this paper, we present a niching method that works on the problem substructures rather than the whole solution, therefore it has less space complexity than previously known niching mechanisms. We show that the method is responding accurately when environmental changes occur.",
        "published": "2005-02-04T04:29:15Z",
        "link": "http://arxiv.org/abs/cs/0502022v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Sub-structural Niching in Estimation of Distribution Algorithms",
        "authors": [
            "K. Sastry",
            "H. A. Abbass",
            "D. E. Goldberg",
            "D. D. Johnson"
        ],
        "summary": "We propose a sub-structural niching method that fully exploits the problem decomposition capability of linkage-learning methods such as the estimation of distribution algorithms and concentrate on maintaining diversity at the sub-structural level. The proposed method consists of three key components: (1) Problem decomposition and sub-structure identification, (2) sub-structure fitness estimation, and (3) sub-structural niche preservation. The sub-structural niching method is compared to restricted tournament selection (RTS)--a niching method used in hierarchical Bayesian optimization algorithm--with special emphasis on sustained preservation of multiple global solutions of a class of boundedly-difficult, additively-separable multimodal problems. The results show that sub-structural niching successfully maintains multiple global optima over large number of generations and does so with significantly less population than RTS. Additionally, the market share of each of the niche is much closer to the expected level in sub-structural niching when compared to RTS.",
        "published": "2005-02-04T04:46:04Z",
        "link": "http://arxiv.org/abs/cs/0502023v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Multiobjective hBOA, Clustering, and Scalability",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry",
            "David E. Goldberg"
        ],
        "summary": "This paper describes a scalable algorithm for solving multiobjective decomposable problems by combining the hierarchical Bayesian optimization algorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and clustering in the objective space. It is first argued that for good scalability, clustering or some other form of niching in the objective space is necessary and the size of each niche should be approximately equal. Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and clustering in the objective space. The algorithm mohBOA differs from the multiobjective variants of BOA and hBOA proposed in the past by including clustering in the objective space and allocating an approximately equally sized portion of the population to each cluster. The algorithm mohBOA is shown to scale up well on a number of problems on which standard multiobjective evolutionary algorithms perform poorly.",
        "published": "2005-02-07T05:26:13Z",
        "link": "http://arxiv.org/abs/cs/0502034v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; I.2.6; G.1.6; I.5.3"
        ]
    },
    {
        "title": "Scalability of Genetic Programming and Probabilistic Incremental Program   Evolution",
        "authors": [
            "Radovan Ondas",
            "Martin Pelikan",
            "Kumara Sastry"
        ],
        "summary": "This paper discusses scalability of standard genetic programming (GP) and the probabilistic incremental program evolution (PIPE). To investigate the need for both effective mixing and linkage learning, two test problems are considered: ORDER problem, which is rather easy for any recombination-based GP, and TRAP or the deceptive trap problem, which requires the algorithm to learn interactions among subsets of terminals. The scalability results show that both GP and PIPE scale up polynomially with problem size on the simple ORDER problem, but they both scale up exponentially on the deceptive problem. This indicates that while standard recombination is sufficient when no interactions need to be considered, for some problems linkage learning is necessary. These results are in agreement with the lessons learned in the domain of binary-string genetic algorithms (GAs). Furthermore, the paper investigates the effects of introducing utnnecessary and irrelevant primitives on the performance of GP and PIPE.",
        "published": "2005-02-07T19:40:01Z",
        "link": "http://arxiv.org/abs/cs/0502029v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; I.2.6; G.1.6"
        ]
    },
    {
        "title": "Decomposable Problems, Niching, and Scalability of Multiobjective   Estimation of Distribution Algorithms",
        "authors": [
            "Kumara Sastry",
            "Martin Pelikan",
            "David E. Goldberg"
        ],
        "summary": "The paper analyzes the scalability of multiobjective estimation of distribution algorithms (MOEDAs) on a class of boundedly-difficult additively-separable multiobjective optimization problems. The paper illustrates that even if the linkage is correctly identified, massive multimodality of the search problems can easily overwhelm the nicher and lead to exponential scale-up. Facetwise models are subsequently used to propose a growth rate of the number of differing substructures between the two objectives to avoid the niching method from being overwhelmed and lead to polynomial scalability of MOEDAs.",
        "published": "2005-02-12T22:29:45Z",
        "link": "http://arxiv.org/abs/cs/0502057v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Perspectives for Strong Artificial Life",
        "authors": [
            "J. -Ph Rennard"
        ],
        "summary": "This text introduces the twin deadlocks of strong artificial life. Conceptualization of life is a deadlock both because of the existence of a continuum between the inert and the living, and because we only know one instance of life. Computationalism is a second deadlock since it remains a matter of faith. Nevertheless, artificial life realizations quickly progress and recent constructions embed an always growing set of the intuitive properties of life. This growing gap between theory and realizations should sooner or later crystallize in some kind of paradigm shift and then give clues to break the twin deadlocks.",
        "published": "2005-02-13T18:20:48Z",
        "link": "http://arxiv.org/abs/cs/0502060v1",
        "categories": [
            "cs.AI",
            "I.2.0"
        ]
    },
    {
        "title": "Master Algorithms for Active Experts Problems based on Increasing Loss   Values",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain master algorithms for \"active experts problems\", which means that the master's actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. This results in a (computationally infeasible) universal master algorithm which performs - in a certain sense - almost as well as any computable strategy, for any online problem.",
        "published": "2005-02-15T14:59:49Z",
        "link": "http://arxiv.org/abs/cs/0502067v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Strong Asymptotic Assertions for Discrete MDL in Regression and   Classification",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We study the properties of the MDL (or maximum penalized complexity) estimator for Regression and Classification, where the underlying model class is countable. We show in particular a finite bound on the Hellinger losses under the only assumption that there is a \"true\" model contained in the class. This implies almost sure convergence of the predictive distribution to the true one at a fast rate. It corresponds to Solomonoff's central theorem of universal induction, however with a bound that is exponentially larger.",
        "published": "2005-02-15T16:26:36Z",
        "link": "http://arxiv.org/abs/math/0502315v1",
        "categories": [
            "math.ST",
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.PR",
            "stat.TH"
        ]
    },
    {
        "title": "Semantical Characterizations and Complexity of Equivalences in Answer   Set Programming",
        "authors": [
            "Thomas Eiter",
            "Michael Fink",
            "Stefan Woltran"
        ],
        "summary": "In recent research on non-monotonic logic programming, repeatedly strong equivalence of logic programs P and Q has been considered, which holds if the programs P union R and Q union R have the same answer sets for any other program R. This property strengthens equivalence of P and Q with respect to answer sets (which is the particular case for R is the empty set), and has its applications in program optimization, verification, and modular logic programming. In this paper, we consider more liberal notions of strong equivalence, in which the actual form of R may be syntactically restricted. On the one hand, we consider uniform equivalence, where R is a set of facts rather than a set of rules. This notion, which is well known in the area of deductive databases, is particularly useful for assessing whether programs P and Q are equivalent as components of a logic program which is modularly structured. On the other hand, we consider relativized notions of equivalence, where R ranges over rules over a fixed alphabet, and thus generalize our results to relativized notions of strong and uniform equivalence. For all these notions, we consider disjunctive logic programs in the propositional (ground) case, as well as some restricted classes, provide semantical characterizations and analyze the computational complexity. Our results, which naturally extend to answer set semantics for programs with strong negation, complement the results on strong equivalence of logic programs and pave the way for optimizations in answer set solvers as a tool for input-based problem solving.",
        "published": "2005-02-18T11:01:03Z",
        "link": "http://arxiv.org/abs/cs/0502078v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.3; F.4.1; I.2.4"
        ]
    },
    {
        "title": "Graphs and colorings for answer set programming",
        "authors": [
            "Kathrin Konczak",
            "Thomas Linke",
            "Torsten Schaub"
        ],
        "summary": "We investigate the usage of rule dependency graphs and their colorings for characterizing and computing answer sets of logic programs. This approach provides us with insights into the interplay between rules when inducing answer sets. We start with different characterizations of answer sets in terms of totally colored dependency graphs that differ in graph-theoretical aspects. We then develop a series of operational characterizations of answer sets in terms of operators on partial colorings. In analogy to the notion of a derivation in proof theory, our operational characterizations are expressed as (non-deterministically formed) sequences of colorings, turning an uncolored graph into a totally colored one. In this way, we obtain an operational framework in which different combinations of operators result in different formal properties. Among others, we identify the basic strategy employed by the noMoRe system and justify its algorithmic approach. Furthermore, we distinguish operations corresponding to Fitting's operator as well as to well-founded semantics. (To appear in Theory and Practice of Logic Programming (TPLP))",
        "published": "2005-02-21T14:28:23Z",
        "link": "http://arxiv.org/abs/cs/0502082v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "The Self-Organization of Speech Sounds",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "summary": "The speech code is a vehicle of language: it defines a set of forms used by a community to carry information. Such a code is necessary to support the linguistic interactions that allow humans to communicate. How then may a speech code be formed prior to the existence of linguistic interactions? Moreover, the human speech code is discrete and compositional, shared by all the individuals of a community but different across communities, and phoneme inventories are characterized by statistical regularities. How can a speech code with these properties form? We try to approach these questions in the paper, using the \"methodology of the artificial\". We build a society of artificial agents, and detail a mechanism that shows the formation of a discrete speech code without pre-supposing the existence of linguistic capacities or of coordinated interactions. The mechanism is based on a low-level model of sensory-motor interactions. We show that the integration of certain very simple and non language-specific neural devices leads to the formation of a speech code that has properties similar to the human speech code. This result relies on the self-organizing properties of a generic coupling between perception and production within agents, and on the interactions between agents. The artificial system helps us to develop better intuitions on how speech might have appeared, by showing how self-organization might have helped natural selection to find speech.",
        "published": "2005-02-22T09:51:16Z",
        "link": "http://arxiv.org/abs/cs/0502086v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE",
            "cs.RO",
            "math.DS"
        ]
    },
    {
        "title": "Towards a Systematic Account of Different Semantics for Logic Programs",
        "authors": [
            "Pascal Hitzler"
        ],
        "summary": "In [Hitzler and Wendt 2002, 2005], a new methodology has been proposed which allows to derive uniform characterizations of different declarative semantics for logic programs with negation. One result from this work is that the well-founded semantics can formally be understood as a stratified version of the Fitting (or Kripke-Kleene) semantics. The constructions leading to this result, however, show a certain asymmetry which is not readily understood. We will study this situation here with the result that we will obtain a coherent picture of relations between different semantics for normal logic programs.",
        "published": "2005-02-22T18:53:01Z",
        "link": "http://arxiv.org/abs/cs/0502088v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; D.1.6; F.4.1"
        ]
    },
    {
        "title": "Property analysis of symmetric travelling salesman problem instances   acquired through evolution",
        "authors": [
            "J. I. van Hemert"
        ],
        "summary": "We show how an evolutionary algorithm can successfully be used to evolve a set of difficult to solve symmetric travelling salesman problem instances for two variants of the Lin-Kernighan algorithm. Then we analyse the instances in those sets to guide us towards deferring general knowledge about the efficiency of the two variants in relation to structural properties of the symmetric travelling sale sman problem.",
        "published": "2005-02-28T16:40:34Z",
        "link": "http://arxiv.org/abs/cs/0502096v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G1.6;I.2.8"
        ]
    },
    {
        "title": "Probabilistic Algorithmic Knowledge",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "The framework of algorithmic knowledge assumes that agents use deterministic knowledge algorithms to compute the facts they explicitly know. We extend the framework to allow for randomized knowledge algorithms. We then characterize the information provided by a randomized knowledge algorithm when its answers have some probability of being incorrect. We formalize this information in terms of evidence; a randomized knowledge algorithm returning ``Yes'' to a query about a fact \\phi provides evidence for \\phi being true. Finally, we discuss the extent to which this evidence can be used as a basis for decisions.",
        "published": "2005-03-08T03:39:23Z",
        "link": "http://arxiv.org/abs/cs/0503018v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; G.3"
        ]
    },
    {
        "title": "Fine-Grained Word Sense Disambiguation Based on Parallel Corpora, Word   Alignment, Word Clustering and Aligned Wordnets",
        "authors": [
            "Dan Tufis",
            "Radu Ion",
            "Nancy Ide"
        ],
        "summary": "The paper presents a method for word sense disambiguation based on parallel corpora. The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system, implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet.",
        "published": "2005-03-10T11:49:51Z",
        "link": "http://arxiv.org/abs/cs/0503024v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "A Suffix Tree Approach to Email Filtering",
        "authors": [
            "Rajesh M. Pampapathi",
            "Boris Mirkin",
            "Mark Levene"
        ],
        "summary": "We present an approach to email filtering based on the suffix tree data structure. A method for the scoring of emails using the suffix tree is developed and a number of scoring and score normalisation functions are tested. Our results show that the character level representation of emails and classes facilitated by the suffix tree can significantly improve classification accuracy when compared with the currently popular methods, such as naive Bayes. We believe the method can be extended to the classification of documents in other domains.",
        "published": "2005-03-14T18:12:03Z",
        "link": "http://arxiv.org/abs/cs/0503030v2",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "Mining Top-k Approximate Frequent Patterns",
        "authors": [
            "Zengyou He"
        ],
        "summary": "Frequent pattern (itemset) mining in transactional databases is one of the most well-studied problems in data mining. One obstacle that limits the practical usage of frequent pattern mining is the extremely large number of patterns generated. Such a large size of the output collection makes it difficult for users to understand and use in practice. Even restricting the output to the border of the frequent itemset collection does not help much in alleviating the problem. In this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem: mining top-k approximate frequent patterns. The union of the power sets of these k sets should satisfy the following conditions: (1) including itemsets with larger support as many as possible and (2) including itemsets with smaller support as less as possible. An integrated objective function is designed to combine these two objectives. Consequently, we derive the upper bounds on objective function and present an approximate branch-and-bound method for finding the feasible solution. We give empirical evidence showing that our formulation and approximation methods work well in practice.",
        "published": "2005-03-17T11:01:25Z",
        "link": "http://arxiv.org/abs/cs/0503037v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Complexity Issues in Finding Succinct Solutions of PSPACE-Complete   Problems",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "We study the problem of deciding whether some PSPACE-complete problems have models of bounded size. Contrary to problems in NP, models of PSPACE-complete problems may be exponentially large. However, such models may take polynomial space in a succinct representation. For example, the models of a QBF are explicitely represented by and-or trees (which are always of exponential size) but can be succinctely represented by circuits (which can be polynomial or exponential). We investigate the complexity of deciding the existence of such succinct models when a bound on size is given.",
        "published": "2005-03-18T17:30:39Z",
        "link": "http://arxiv.org/abs/cs/0503043v3",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "I.2.3; I.2.4; I.2.8; F.1.3; F.2.2"
        ]
    },
    {
        "title": "Generating Hard Satisfiable Formulas by Hiding Solutions Deceptively",
        "authors": [
            "Haixia Jia",
            "Cristopher Moore",
            "Doug Strain"
        ],
        "summary": "To test incomplete search algorithms for constraint satisfaction problems such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances. A simple way to do this is to choose a random truth assignment A, and then choose clauses randomly from among those satisfied by A. However, this method tends to produce easy problems, since the majority of literals point toward the ``hidden'' assignment A. Last year, Achlioptas, Jia and Moore proposed a problem generator that cancels this effect by hiding both A and its complement. While the resulting formulas appear to be just as hard for DPLL algorithms as random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT in only polynomial time. Here we propose a new method to cancel the attraction to A, by choosing a clause with t > 0 literals satisfied by A with probability proportional to q^t for some q < 1. By varying q, we can generate formulas whose variables have no bias, i.e., which are equally likely to be true or false; we can even cause the formula to ``deceptively'' point away from A. We present theoretical and experimental results suggesting that these formulas are exponentially hard both for DPLL algorithms and for incomplete algorithms such as WalkSAT.",
        "published": "2005-03-18T20:15:42Z",
        "link": "http://arxiv.org/abs/cs/0503044v1",
        "categories": [
            "cs.AI",
            "cond-mat.other",
            "cond-mat.stat-mech"
        ]
    },
    {
        "title": "Hiding Satisfying Assignments: Two are Better than One",
        "authors": [
            "Dimitris Achlioptas",
            "Haixia Jia",
            "Cristopher Moore"
        ],
        "summary": "The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances consists of random k-SAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, as the formula's density increases, for a number of different algorithms, A acts as a stronger and stronger attractor. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k-SAT and NAE-k-SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and complement of A are satisfying. It appears that under this \"symmetrization'' the effects of the two attractors largely cancel out, making it much harder for algorithms to find any truth assignment. We give theoretical and experimental evidence supporting this assertion.",
        "published": "2005-03-20T02:46:09Z",
        "link": "http://arxiv.org/abs/cs/0503046v1",
        "categories": [
            "cs.AI",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Les représentations génétiques d'objets : simples analogies   ou modèles pertinents ? Le point de vue de l'   \"évolutique\".<br>&ndash;&ndash;&ndash;<br>Genetic representations of   objects : simple analogies or efficient models ? The \"evolutic\" point of view",
        "authors": [
            "Laurent Krähenbühl"
        ],
        "summary": "Depuis une trentaine d'ann\\'{e}es, les ing\\'{e}nieurs utilisent couramment des analogies avec l'\\'{e}volution naturelle pour optimiser des dispositifs techniques. Le plus souvent, ces m\\'{e}thodes \"g\\'{e}n\\'{e}tiques\" ou \"\\'{e}volutionnaires\" sont consid\\'{e}r\\'{e}es uniquement du point de vue pratique, comme des m\\'{e}thodes d'optimisation performantes, qu'on peut utiliser \\`{a} la place d'autres m\\'{e}thodes (gradients, simplexes, ...). Dans cet article, nous essayons de montrer que les sciences et les techniques, mais aussi les organisations humaines, et g\\'{e}n\\'{e}ralement tous les syst\\`{e}mes complexes, ob\\'{e}issent \\`{a} des lois d'\\'{e}volution dont la g\\'{e}n\\'{e}tique est un bon mod\\`{e}le repr\\'{e}sentatif, m\\^{e}me si g\\^{e}nes et chromosomes sont \"virtuels\" : ainsi loin d'\\^{e}tre seulement un outil ponctuel d'aide \\`{a} la synth\\`{e}se de solutions technologiques, la repr\\'{e}sentation g\\'{e}n\\'{e}tique est-elle un mod\\`{e}le dynamique global de l'\\'{e}volution du monde fa\\c{c}onn\\'{e} par l'agitation humaine.&ndash;&ndash;&ndash;&ndash;For thirty years, engineers commonly use analogies with natural evolution to optimize technical devices. More often that not, these \"genetic\" or \"evolutionary\" methods are only view as efficient tools, which could replace other optimization techniques (gradient methods, simplex, ...). In this paper, we try to show that sciences, techniques, human organizations, and more generally all complex systems, obey to evolution rules, whose the genetic is a good representative model, even if genes and chromosomes are \"virtual\". Thus, the genetic representation is not only a specific tool helping for the design of technological solutions, but also a global and dynamic model for the action of the human agitation on our world.",
        "published": "2005-03-23T08:01:10Z",
        "link": "http://arxiv.org/abs/cs/0503059v2",
        "categories": [
            "cs.AI",
            "nlin.AO"
        ]
    },
    {
        "title": "An Optimization Model for Outlier Detection in Categorical Data",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. Detection of such outliers is important for many applications such as fraud detection and customer migration. Most existing methods are designed for numeric data. They will encounter problems with real-life applications that contain categorical data. In this paper, we formally define the problem of outlier detection in categorical data as an optimization problem from a global viewpoint. Moreover, we present a local-search heuristic based algorithm for efficiently finding feasible solutions. Experimental results on real datasets and large synthetic datasets demonstrate the superiority of our model and algorithm.",
        "published": "2005-03-29T13:31:01Z",
        "link": "http://arxiv.org/abs/cs/0503081v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Spines of Random Constraint Satisfaction Problems: Definition and   Connection with Computational Complexity",
        "authors": [
            "Gabriel Istrate",
            "Stefan Boettcher",
            "Allon G. Percus"
        ],
        "summary": "We study the connection between the order of phase transitions in combinatorial problems and the complexity of decision algorithms for such problems. We rigorously show that, for a class of random constraint satisfaction problems, a limited connection between the two phenomena indeed exists. Specifically, we extend the definition of the spine order parameter of Bollobas et al. to random constraint satisfaction problems, rigorously showing that for such problems a discontinuity of the spine is associated with a $2^{\\Omega(n)}$ resolution complexity (and thus a $2^{\\Omega(n)}$ complexity of DPLL algorithms) on random instances. The two phenomena have a common underlying cause: the emergence of ``large'' (linear size) minimally unsatisfiable subformulas of a random formula at the satisfiability phase transition.   We present several further results that add weight to the intuition that random constraint satisfaction problems with a sharp threshold and a continuous spine are ``qualitatively similar to random 2-SAT''. Finally, we argue that it is the spine rather than the backbone parameter whose continuity has implications for the decision complexity of combinatorial problems, and we provide experimental evidence that the two parameters can behave in a different manner.",
        "published": "2005-03-29T21:58:21Z",
        "link": "http://arxiv.org/abs/cs/0503082v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.AI"
        ]
    },
    {
        "title": "Monotonic and Nonmonotonic Preference Revision",
        "authors": [
            "Jan Chomicki",
            "Joyce Song"
        ],
        "summary": "We study here preference revision, considering both the monotonic case where the original preferences are preserved and the nonmonotonic case where the new preferences may override the original ones. We use a relational framework in which preferences are represented using binary relations (not necessarily finite). We identify several classes of revisions that preserve order axioms, for example the axioms of strict partial or weak orders. We consider applications of our results to preference querying in relational databases.",
        "published": "2005-03-31T19:43:31Z",
        "link": "http://arxiv.org/abs/cs/0503092v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Constraint-Based Qualitative Simulation",
        "authors": [
            "Krzysztof R. Apt",
            "Sebastian Brand"
        ],
        "summary": "We consider qualitative simulation involving a finite set of qualitative relations in presence of complete knowledge about their interrelationship. We show how it can be naturally captured by means of constraints expressed in temporal logic and constraint satisfaction problems. The constraints relate at each stage the 'past' of a simulation with its 'future'. The benefit of this approach is that it readily leads to an implementation based on constraint technology that can be used to generate simulations and to answer queries about them.",
        "published": "2005-04-07T12:06:24Z",
        "link": "http://arxiv.org/abs/cs/0504024v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.6.2; I.6.7; F.4.1; D.3.2"
        ]
    },
    {
        "title": "Sufficient conditions for convergence of the Sum-Product Algorithm",
        "authors": [
            "Joris M. Mooij",
            "Hilbert J. Kappen"
        ],
        "summary": "We derive novel conditions that guarantee convergence of the Sum-Product algorithm (also known as Loopy Belief Propagation or simply Belief Propagation) to a unique fixed point, irrespective of the initial messages. The computational complexity of the conditions is polynomial in the number of variables. In contrast with previously existing conditions, our results are directly applicable to arbitrary factor graphs (with discrete variables) and are shown to be valid also in the case of factors containing zeros, under some additional conditions. We compare our bounds with existing ones, numerically and, if possible, analytically. For binary variables with pairwise interactions, we derive sufficient conditions that take into account local evidence (i.e., single variable factors) and the type of pair interactions (attractive or repulsive). It is shown empirically that this bound outperforms existing bounds.",
        "published": "2005-04-08T15:11:04Z",
        "link": "http://arxiv.org/abs/cs/0504030v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "I.2.3; F.2.1"
        ]
    },
    {
        "title": "Fitness Uniform Deletion: A Simple Way to Preserve Diversity",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "summary": "A commonly experienced problem with population based optimisation methods is the gradual decline in population diversity that tends to occur over time. This can slow a system's progress or even halt it completely if the population converges on a local optimum from which it cannot escape. In this paper we present the Fitness Uniform Deletion Scheme (FUDS), a simple but somewhat unconventional approach to this problem. Under FUDS the deletion operation is modified to only delete those individuals which are \"common\" in the sense that there exist many other individuals of similar fitness in the population. This makes it impossible for the population to collapse to a collection of highly related individuals with similar fitness. Our experimental results on a range of optimisation problems confirm this, in particular for deceptive optimisation problems the performance is significantly more robust to variation in the selection intensity.",
        "published": "2005-04-11T10:42:41Z",
        "link": "http://arxiv.org/abs/cs/0504035v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.M"
        ]
    },
    {
        "title": "Learning Polynomial Networks for Classification of Clinical   Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "We describe a polynomial network technique developed for learning to classify clinical electroencephalograms (EEGs) presented by noisy features. Using an evolutionary strategy implemented within Group Method of Data Handling, we learn classification models which are comprehensively described by sets of short-term polynomials. The polynomial models were learnt to classify the EEGs recorded from Alzheimer and healthy patients and recognize the EEG artifacts. Comparing the performances of our technique and some machine learning methods we conclude that our technique can learn well-suited polynomial models which experts can find easy-to-understand.",
        "published": "2005-04-11T17:11:29Z",
        "link": "http://arxiv.org/abs/cs/0504041v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "The Bayesian Decision Tree Technique with a Sweeping Strategy",
        "authors": [
            "V. Schetinin",
            "J. E. Fieldsend",
            "D. Partridge",
            "W. J. Krzanowski",
            "R. M. Everson",
            "T. C. Bailey",
            "A. Hernandez"
        ],
        "summary": "The uncertainty of classification outcomes is of crucial importance for many safety critical applications including, for example, medical diagnostics. In such applications the uncertainty of classification can be reliably estimated within a Bayesian model averaging technique that allows the use of prior information. Decision Tree (DT) classification models used within such a technique gives experts additional information by making this classification scheme observable. The use of the Markov Chain Monte Carlo (MCMC) methodology of stochastic sampling makes the Bayesian DT technique feasible to perform. However, in practice, the MCMC technique may become stuck in a particular DT which is far away from a region with a maximal posterior. Sampling such DTs causes bias in the posterior estimates, and as a result the evaluation of classification uncertainty may be incorrect. In a particular case, the negative effect of such sampling may be reduced by giving additional prior information on the shape of DTs. In this paper we describe a new approach based on sweeping the DTs without additional priors on the favorite shape of DTs. The performances of Bayesian DT techniques with the standard and sweeping strategies are compared on a synthetic data as well as on real datasets. Quantitatively evaluating the uncertainty in terms of entropy of class posterior probabilities, we found that the sweeping strategy is superior to the standard strategy.",
        "published": "2005-04-11T17:45:09Z",
        "link": "http://arxiv.org/abs/cs/0504042v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Experimental Comparison of Classification Uncertainty for Randomised and   Bayesian Decision Tree Ensembles",
        "authors": [
            "V. Schetinin",
            "D. Partridge",
            "W. J. Krzanowski",
            "R. M. Everson",
            "J. E. Fieldsend",
            "T. C. Bailey",
            "A. Hernandez"
        ],
        "summary": "In this paper we experimentally compare the classification uncertainty of the randomised Decision Tree (DT) ensemble technique and the Bayesian DT technique with a restarting strategy on a synthetic dataset as well as on some datasets commonly used in the machine learning community. For quantitative evaluation of classification uncertainty, we use an Uncertainty Envelope dealing with the class posterior distribution and a given confidence probability. Counting the classifier outcomes, this technique produces feasible evaluations of the classification uncertainty. Using this technique in our experiments, we found that the Bayesian DT technique is superior to the randomised DT ensemble technique.",
        "published": "2005-04-11T17:53:35Z",
        "link": "http://arxiv.org/abs/cs/0504043v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "A Learning Algorithm for Evolving Cascade Neural Networks",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is described. An ECNN starts to learn with one input node and then adding new inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly minimal number of input and hidden neurons as well as connections. The algorithm was successfully applied to classify artifacts and normal segments in clinical electroencephalograms (EEGs). The EEG segments were visually labeled by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing segments. It is slightly better than a standard fully connected neural network.",
        "published": "2005-04-13T13:57:56Z",
        "link": "http://arxiv.org/abs/cs/0504055v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity",
        "authors": [
            "V. Schetinin"
        ],
        "summary": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.",
        "published": "2005-04-13T13:59:55Z",
        "link": "http://arxiv.org/abs/cs/0504056v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Diagnostic Rule Extraction Using Neural Networks",
        "authors": [
            "Vitaly Schetinin",
            "Anatoly Brazhnikov"
        ],
        "summary": "The neural networks have trained on incomplete sets that a doctor could collect. Trained neural networks have correctly classified all the presented instances. The number of intervals entered for encoding the quantitative variables is equal two. The number of features as well as the number of neurons and layers in trained neural networks was minimal. Trained neural networks are adequately represented as a set of logical formulas that more comprehensible and easy-to-understand. These formulas are as the syndrome-complexes, which may be easily tabulated and represented as a diagnostic table that the doctors usually use. Decision rules provide the evaluations of their confidence in which interested a doctor. Conducted clinical researches have shown that iagnostic decisions produced by symbolic rules have coincided with the doctor's conclusions.",
        "published": "2005-04-13T14:03:02Z",
        "link": "http://arxiv.org/abs/cs/0504057v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Polynomial Neural Networks Learnt to Classify EEG Signals",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "A neural network based technique is presented, which is able to successfully extract polynomial classification rules from labeled electroencephalogram (EEG) signals. To represent the classification rules in an analytical form, we use the polynomial neural networks trained by a modified Group Method of Data Handling (GMDH). The classification rules were extracted from clinical EEG data that were recorded from an Alzheimer patient and the sudden death risk patients. The third data is EEG recordings that include the normal and artifact segments. These EEG data were visually identified by medical experts. The extracted polynomial rules verified on the testing EEG data allow to correctly classify 72% of the risk group patients and 96.5% of the segments. These rules performs slightly better than standard feedforward neural networks.",
        "published": "2005-04-13T14:06:32Z",
        "link": "http://arxiv.org/abs/cs/0504058v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Neural Network Decision Tree for Learning Concepts from EEG Data",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "To learn the multi-class conceptions from the electroencephalogram (EEG) data we developed a neural network decision tree (DT), that performs the linear tests, and a new training algorithm. We found that the known methods fail inducting the classification models when the data are presented by the features some of them are irrelevant, and the classes are heavily overlapped. To train the DT, our algorithm exploits a bottom up search of the features that provide the best classification accuracy of the linear tests. We applied the developed algorithm to induce the DT from the large EEG dataset consisted of 65 patients belonging to 16 age groups. In these recordings each EEG segment was represented by 72 calculated features. The DT correctly classified 80.8% of the training and 80.1% of the testing examples. Correspondingly it correctly classified 89.2% and 87.7% of the EEG recordings.",
        "published": "2005-04-13T14:28:48Z",
        "link": "http://arxiv.org/abs/cs/0504059v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Neural-Network Techniques for Visual Mining Clinical   Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult",
            "Anatoly Brazhnikov"
        ],
        "summary": "In this chapter we describe new neural-network techniques developed for visual mining clinical electroencephalograms (EEGs), the weak electrical potentials invoked by brain activity. These techniques exploit fruitful ideas of Group Method of Data Handling (GMDH). Section 2 briefly describes the standard neural-network techniques which are able to learn well-suited classification modes from data presented by relevant features. Section 3 introduces an evolving cascade neural network technique which adds new input nodes as well as new neurons to the network while the training error decreases. This algorithm is applied to recognize artifacts in the clinical EEGs. Section 4 presents the GMDH-type polynomial networks learnt from data. We applied this technique to distinguish the EEGs recorded from an Alzheimer and a healthy patient as well as recognize EEG artifacts. Section 5 describes the new neural-network technique developed to induce multi-class concepts from data. We used this technique for inducing a 16-class concept from the large-scale clinical EEG data. Finally we discuss perspectives of applying the neural-network techniques to clinical EEGs.",
        "published": "2005-04-14T10:27:55Z",
        "link": "http://arxiv.org/abs/cs/0504064v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Estimating Classification Uncertainty of Bayesian Decision Tree   Technique on Financial Data",
        "authors": [
            "Vitaly Schetinin",
            "Jonathan E. Fieldsend",
            "Derek Partridge",
            "Wojtek J. Krzanowski",
            "Richard M. Everson",
            "Trevor C. Bailey",
            "Adolfo Hernandez"
        ],
        "summary": "Bayesian averaging over classification models allows the uncertainty of classification outcomes to be evaluated, which is of crucial importance for making reliable decisions in applications such as financial in which risks have to be estimated. The uncertainty of classification is determined by a trade-off between the amount of data available for training, the diversity of a classifier ensemble and the required performance. The interpretability of classification models can also give useful information for experts responsible for making reliable classifications. For this reason Decision Trees (DTs) seem to be attractive classification models. The required diversity of the DT ensemble can be achieved by using the Bayesian model averaging all possible DTs. In practice, the Bayesian approach can be implemented on the base of a Markov Chain Monte Carlo (MCMC) technique of random sampling from the posterior distribution. For sampling large DTs, the MCMC method is extended by Reversible Jump technique which allows inducing DTs under given priors. For the case when the prior information on the DT size is unavailable, the sweeping technique defining the prior implicitly reveals a better performance. Within this Chapter we explore the classification uncertainty of the Bayesian MCMC techniques on some datasets from the StatLog Repository and real financial data. The classification uncertainty is compared within an Uncertainty Envelope technique dealing with the class posterior distribution and a given confidence probability. This technique provides realistic estimates of the classification uncertainty which can be easily interpreted in statistical terms with the aim of risk evaluation.",
        "published": "2005-04-14T10:30:54Z",
        "link": "http://arxiv.org/abs/cs/0504065v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Comparison of the Bayesian and Randomised Decision Tree Ensembles within   an Uncertainty Envelope Technique",
        "authors": [
            "Vitaly Schetinin",
            "Jonathan E. Fieldsend",
            "Derek Partridge",
            "Wojtek J. Krzanowski",
            "Richard M. Everson",
            "Trevor C. Bailey",
            "Adolfo Hernandez"
        ],
        "summary": "Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of classification outcomes that is of crucial importance for safety critical applications. The uncertainty of classification is determined by a trade-off between the amount of data available for training, the classifier diversity and the required performance. The interpretability of MCSs can also give useful information for experts responsible for making reliable classifications. For this reason Decision Trees (DTs) seem to be attractive classification models for experts. The required diversity of MCSs exploiting such classification models can be achieved by using two techniques, the Bayesian model averaging and the randomised DT ensemble. Both techniques have revealed promising results when applied to real-world problems. In this paper we experimentally compare the classification uncertainty of the Bayesian model averaging with a restarting strategy and the randomised DT ensemble on a synthetic dataset and some domain problems commonly used in the machine learning community. To make the Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo technique. The classification uncertainty is evaluated within an Uncertainty Envelope technique dealing with the class posterior distribution and a given confidence probability. Exploring a full posterior distribution, this technique produces realistic estimates which can be easily interpreted in statistical terms. In our experiments we found out that the Bayesian DTs are superior to the randomised DT ensembles within the Uncertainty Envelope technique.",
        "published": "2005-04-14T10:33:33Z",
        "link": "http://arxiv.org/abs/cs/0504066v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "An Evolving Cascade Neural Network Technique for Cleaning Sleep   Electroencephalograms",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable of selecting informative features are described. The ECNN initially learns with one input node and then evolves by adding new inputs as well as new hidden neurons. The resultant ECNN has a near minimal number of hidden neurons and inputs. The algorithm is successfully used for training ECNN to recognise artefacts in sleep electroencephalograms (EEGs) which were visually labelled by EEG-viewers. In our experiments, the ECNN outperforms the standard neural-network as well as evolutionary techniques.",
        "published": "2005-04-14T10:36:54Z",
        "link": "http://arxiv.org/abs/cs/0504067v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Self-Organization of the Neuron Collective of Optimal Complexity",
        "authors": [
            "V. Schetinin",
            "A. Kostunin"
        ],
        "summary": "The optimal complexity of neural networks is achieved when the self-organization principles is used to eliminate the contradictions existing in accordance with the K. Godel theorem about incompleteness of the systems based on axiomatics. The principle of S. Beer exterior addition the Heuristic Group Method of Data Handling by A. Ivakhnenko realized is used.",
        "published": "2005-04-14T10:45:06Z",
        "link": "http://arxiv.org/abs/cs/0504068v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Neural-Network Technique to Learn Concepts from Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "A new technique is presented developed to learn multi-class concepts from clinical electroencephalograms. A desired concept is represented as a neuronal computational model consisting of the input, hidden, and output neurons. In this model the hidden neurons learn independently to classify the electroencephalogram segments presented by spectral and statistical features. This technique has been applied to the electroencephalogram data recorded from 65 sleeping healthy newborns in order to learn a brain maturation concept of newborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these data have been used for learning and testing the concept, respectively. As a result, the concept has correctly classified 80.1% of the testing segments or 87.7% of the 65 records.",
        "published": "2005-04-14T10:47:38Z",
        "link": "http://arxiv.org/abs/cs/0504069v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "The Combined Technique for Detection of Artifacts in Clinical   Electroencephalograms of Sleeping Newborns",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "In this paper we describe a new method combining the polynomial neural network and decision tree techniques in order to derive comprehensible classification rules from clinical electroencephalograms (EEGs) recorded from sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement, muscle and noise artifacts and as a consequence some EEG features are irrelevant to classification problems. Combining the polynomial network and decision tree techniques, we discover comprehensible classification rules whilst also attempting to keep their classification error down. This technique is shown to outperform a number of commonly used machine learning technique applied to automatically recognize artifacts in the sleep EEGs.",
        "published": "2005-04-14T10:49:55Z",
        "link": "http://arxiv.org/abs/cs/0504070v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Proceedings of the Pacific Knowledge Acquisition Workshop 2004",
        "authors": [
            "Byeong Ho Kang",
            "Achim Hoffmann",
            "Takahira Yamaguchi",
            "Wai Kiang Yeap"
        ],
        "summary": "Artificial intelligence (AI) research has evolved over the last few decades and knowledge acquisition research is at the core of AI research. PKAW-04 is one of three international knowledge acquisition workshops held in the Pacific-Rim, Canada and Europe over the last two decades. PKAW-04 has a strong emphasis on incremental knowledge acquisition, machine learning, neural nets and active mining.   The proceedings contain 19 papers that were selected by the program committee among 24 submitted papers. All papers were peer reviewed by at least two reviewers. The papers in these proceedings cover the methods and tools as well as the applications related to develop expert systems or knowledge based systems.",
        "published": "2005-04-14T13:14:53Z",
        "link": "http://arxiv.org/abs/cs/0504071v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Knowledge Representation Issues in Semantic Graphs for Relationship   Detection",
        "authors": [
            "Marc Barthelemy",
            "Edmond Chow",
            "Tina Eliassi-Rad"
        ],
        "summary": "An important task for Homeland Security is the prediction of threat vulnerabilities, such as through the detection of relationships between seemingly disjoint entities. A structure used for this task is a \"semantic graph\", also known as a \"relational data graph\" or an \"attributed relational graph\". These graphs encode relationships as \"typed\" links between a pair of \"typed\" nodes. Indeed, semantic graphs are very similar to semantic networks used in AI. The node and link types are related through an ontology graph (also known as a schema). Furthermore, each node has a set of attributes associated with it (e.g., \"age\" may be an attribute of a node of type \"person\"). Unfortunately, the selection of types and attributes for both nodes and links depends on human expertise and is somewhat subjective and even arbitrary. This subjectiveness introduces biases into any algorithm that operates on semantic graphs. Here, we raise some knowledge representation issues for semantic graphs and provide some possible solutions using recently developed ideas in the field of complex networks. In particular, we use the concept of transitivity to evaluate the relevance of individual links in the semantic graph for detecting relationships. We also propose new statistical measures for semantic graphs and illustrate these semantic measures on graphs constructed from movies and terrorism data.",
        "published": "2005-04-14T20:15:24Z",
        "link": "http://arxiv.org/abs/cs/0504072v1",
        "categories": [
            "cs.AI",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Metalinguistic Information Extraction for Terminology",
        "authors": [
            "Carlos Rodriguez"
        ],
        "summary": "This paper describes and evaluates the Metalinguistic Operation Processor (MOP) system for automatic compilation of metalinguistic information from technical and scientific documents. This system is designed to extract non-standard terminological resources that we have called Metalinguistic Information Databases (or MIDs), in order to help update changing glossaries, knowledge bases and ontologies, as well as to reflect the metastable dynamics of special-domain knowledge.",
        "published": "2005-04-15T20:10:53Z",
        "link": "http://arxiv.org/abs/cs/0504074v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "I.2.7"
        ]
    },
    {
        "title": "Adaptive Online Prediction by Following the Perturbed Leader",
        "authors": [
            "Marcus Hutter",
            "Jan Poland"
        ],
        "summary": "When applying aggregating strategies to Prediction with Expert Advice, the learning rate must be adaptively tuned. The natural choice of sqrt(complexity/current loss) renders the analysis of Weighted Majority derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative \"Follow the Perturbed Leader\" (FPL) algorithm from Kalai & Vempala (2003) (based on Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are new.",
        "published": "2005-04-16T16:48:49Z",
        "link": "http://arxiv.org/abs/cs/0504078v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Componentwise Least Squares Support Vector Machines",
        "authors": [
            "Kristiaan Pelckmans",
            "Ivan Goethals",
            "Jos De Brabanter",
            "Johan A. K. Suykens",
            "Bart De Moor"
        ],
        "summary": "This chapter describes componentwise Least Squares Support Vector Machines (LS-SVMs) for the estimation of additive models consisting of a sum of nonlinear components. The primal-dual derivations characterizing LS-SVMs for the estimation of the additive model result in a single set of linear equations with size growing in the number of data-points. The derivation is elaborated for the classification as well as the regression case. Furthermore, different techniques are proposed to discover structure in the data by looking for sparse components in the model based on dedicated regularization schemes on the one hand and fusion of the componentwise LS-SVMs training with a validation criterion on the other hand. (keywords: LS-SVMs, additive models, regularization, structure detection)",
        "published": "2005-04-19T15:01:25Z",
        "link": "http://arxiv.org/abs/cs/0504086v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "Universal Similarity",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "We survey a new area of parameter-free similarity distance measures useful in data-mining, pattern recognition, learning and automatic semantics extraction. Given a family of distances on a set of objects, a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set, up to the stated precision (we do not require the universal distance to be an element of the family). We consider similarity distances for two types of objects: literal objects that as such contain all of their meaning, like genomes or books, and names for objects. The latter may have literal embodyments like the first type, but may also be abstract like ``red'' or ``christianity.'' For the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular features between pairs of literal objects. For the second type we consider similarity distances generated by web users corresponding to particular semantic relations between the (names for) the designated objects. For both families we give universal similarity distance measures, incorporating all particular distance measures in the family. In the first case the universal distance is based on compression and in the second case it is based on Google page counts related to search terms. In both cases experiments on a massive scale give evidence of the viability of the approaches.",
        "published": "2005-04-20T17:40:55Z",
        "link": "http://arxiv.org/abs/cs/0504089v2",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "physics.data-an",
            "I.2; I.5; J.5; H.2; H.3; H.4; H.2.8; E.2; E.4"
        ]
    },
    {
        "title": "Single-solution Random 3-SAT Instances",
        "authors": [
            "Marko Znidaric"
        ],
        "summary": "We study a class of random 3-SAT instances having exactly one solution. The properties of this ensemble considerably differ from those of a random 3-SAT ensemble. It is numerically shown that the running time of several complete and stochastic local search algorithms monotonically increases as the clause density is decreased. Therefore, there is no easy-hard-easy pattern of hardness as for standard random 3-SAT ensemble. Furthermore, the running time for short single-solution formulas increases with the problem size much faster than for random 3-SAT formulas from the phase transition region.",
        "published": "2005-04-25T10:40:32Z",
        "link": "http://arxiv.org/abs/cs/0504101v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Cooperative Game Theory within Multi-Agent Systems for Systems   Scheduling",
        "authors": [
            "Derek Messie",
            "Jae C. Oh"
        ],
        "summary": "Research concerning organization and coordination within multi-agent systems continues to draw from a variety of architectures and methodologies. The work presented in this paper combines techniques from game theory and multi-agent systems to produce self-organizing, polymorphic, lightweight, embedded agents for systems scheduling within a large-scale real-time systems environment. Results show how this approach is used to experimentally produce optimum real-time scheduling through the emergent behavior of thousands of agents. These results are obtained using a SWARM simulation of systems scheduling within a High Energy Physics experiment consisting of 2500 digital signal processors.",
        "published": "2005-04-29T13:43:30Z",
        "link": "http://arxiv.org/abs/cs/0504108v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Temporal and Spatial Data Mining with Second-Order Hidden Models",
        "authors": [
            "Jean-Francois Mari",
            "Florence Le Ber"
        ],
        "summary": "In the frame of designing a knowledge discovery system, we have developed stochastic models based on high-order hidden Markov models. These models are capable to map sequences of data into a Markov chain in which the transitions between the states depend on the \\texttt{n} previous states according to the order of the model. We study the process of achieving information extraction fromspatial and temporal data by means of an unsupervised classification. We use therefore a French national database related to the land use of a region, named Teruti, which describes the land use both in the spatial and temporal domain. Land-use categories (wheat, corn, forest, ...) are logged every year on each site regularly spaced in the region. They constitute a temporal sequence of images in which we look for spatial and temporal dependencies. The temporal segmentation of the data is done by means of a second-order Hidden Markov Model (\\hmmd) that appears to have very good capabilities to locate stationary segments, as shown in our previous work in speech recognition. Thespatial classification is performed by defining a fractal scanning ofthe images with the help of a Hilbert-Peano curve that introduces atotal order on the sites, preserving the relation ofneighborhood between the sites. We show that the \\hmmd performs aclassification that is meaningful for the agronomists.Spatial and temporal classification may be achieved simultaneously by means of a 2 levels \\hmmd that measures the \\aposteriori probability to map a temporal sequence of images onto a set of hidden classes.",
        "published": "2005-05-09T06:54:57Z",
        "link": "http://arxiv.org/abs/cs/0505018v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Beyond Hypertree Width: Decomposition Methods Without Decompositions",
        "authors": [
            "Hubie Chen",
            "Victor Dalmau"
        ],
        "summary": "The general intractability of the constraint satisfaction problem has motivated the study of restrictions on this problem that permit polynomial-time solvability. One major line of work has focused on structural restrictions, which arise from restricting the interaction among constraint scopes. In this paper, we engage in a mathematical investigation of generalized hypertree width, a structural measure that has up to recently eluded study. We obtain a number of computational results, including a simple proof of the tractability of CSP instances having bounded generalized hypertree width.",
        "published": "2005-05-12T16:53:04Z",
        "link": "http://arxiv.org/abs/cs/0505035v1",
        "categories": [
            "cs.CC",
            "cs.AI"
        ]
    },
    {
        "title": "Relational reasoning in the region connection calculus",
        "authors": [
            "Yongming Li",
            "Sanjiang Li",
            "Mingsheng Ying"
        ],
        "summary": "This paper is mainly concerned with the relation-algebraical aspects of the well-known Region Connection Calculus (RCC). We show that the contact relation algebra (CRA) of certain RCC model is not atomic complete and hence infinite. So in general an extensional composition table for the RCC cannot be obtained by simply refining the RCC8 relations. After having shown that each RCC model is a consistent model of the RCC11 CT, we give an exhaustive investigation about extensional interpretation of the RCC11 CT. More important, we show the complemented closed disk algebra is a representation for the relation algebra determined by the RCC11 table. The domain of this algebra contains two classes of regions, the closed disks and closures of their complements in the real plane.",
        "published": "2005-05-14T21:16:31Z",
        "link": "http://arxiv.org/abs/cs/0505041v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Separating a Real-Life Nonlinear Image Mixture",
        "authors": [
            "Luis B. Almeida"
        ],
        "summary": "When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation.   This paper addresses a difficult version of this problem, corresponding to the use of \"onion skin\" paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement.",
        "published": "2005-05-16T19:31:36Z",
        "link": "http://arxiv.org/abs/cs/0505044v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Scouting Red Beds for Uncommon Features with   Geological Significance",
        "authors": [
            "Patrick C. McGuire",
            "Enrique Diaz-Martinez",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose A. Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "The `Cyborg Astrobiologist' (CA) has undergone a second geological field trial, at a red sandstone site in northern Guadalajara, Spain, near Riba de Santiuste. The Cyborg Astrobiologist is a wearable computer and video camera system that has demonstrated a capability to find uncommon interest points in geological imagery in real-time in the field. The first (of three) geological structures that we studied was an outcrop of nearly homogeneous sandstone, which exhibits oxidized-iron impurities in red and and an absence of these iron impurities in white. The white areas in these ``red beds'' have turned white because the iron has been removed by chemical reduction, perhaps by a biological agent. The computer vision system found in one instance several (iron-free) white spots to be uncommon and therefore interesting, as well as several small and dark nodules. The second geological structure contained white, textured mineral deposits on the surface of the sandstone, which were found by the CA to be interesting. The third geological structure was a 50 cm thick paleosol layer, with fossilized root structures of some plants, which were found by the CA to be interesting. A quasi-blind comparison of the Cyborg Astrobiologist's interest points for these images with the interest points determined afterwards by a human geologist shows that the Cyborg Astrobiologist concurred with the human geologist 68% of the time (true positive rate), with a 32% false positive rate and a 32% false negative rate.   (abstract has been abridged).",
        "published": "2005-05-23T09:55:37Z",
        "link": "http://arxiv.org/abs/cs/0505058v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "physics.ins-det",
            "q-bio.NC",
            "I.2.10; I.4.6; I.4.8; I.4.9; I.2.9; I.5.4; I.5.5; J.2; J.3; D.2;\n  D.1.7; D.4.7"
        ]
    },
    {
        "title": "A Unified Subspace Outlier Ensemble Framework for Outlier Detection in   High Dimensional Spaces",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. Detection of such outliers is important for many applications such as fraud detection and customer migration. Most such applications are high dimensional domains in which the data may contain hundreds of dimensions. However, the outlier detection problem itself is not well defined and none of the existing definitions are widely accepted, especially in high dimensional space. In this paper, our first contribution is to propose a unified framework for outlier detection in high dimensional spaces from an ensemble-learning viewpoint. In our new framework, the outlying-ness of each data object is measured by fusing outlier factors in different subspaces using a combination function. Accordingly, we show that all existing researches on outlier detection can be regarded as special cases in the unified framework with respect to the set of subspaces considered and the type of combination function used. In addition, to demonstrate the usefulness of the ensemble-learning based outlier detection framework, we developed a very simple and fast algorithm, namely SOE1 (Subspace Outlier Ensemble using 1-dimensional Subspaces) in which only subspaces with one dimension is used for mining outliers from large categorical datasets. The SOE1 algorithm needs only two scans over the dataset and hence is very appealing in real data mining applications. Experimental results on real datasets and large synthetic datasets show that: (1) SOE1 has comparable performance with respect to those state-of-art outlier detection algorithms on identifying true outliers and (2) SOE1 can be an order of magnitude faster than one of the fastest outlier detection algorithms known so far.",
        "published": "2005-05-24T02:41:51Z",
        "link": "http://arxiv.org/abs/cs/0505060v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Multi-Modal Human-Machine Communication for Instructing Robot Grasping   Tasks",
        "authors": [
            "P. C. McGuire",
            "J. Fritsch",
            "J. J. Steil",
            "F. Roethling",
            "G. A. Fink",
            "S. Wachsmuth",
            "G. Sagerer",
            "H. Ritter"
        ],
        "summary": "A major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One way of such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable to establish a common focus of attention and be able to use and integrate spoken instructions, visual perceptions, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and an modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects.",
        "published": "2005-05-24T14:53:49Z",
        "link": "http://arxiv.org/abs/cs/0505064v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "H.1.2; I.2.9; I.2.10; I.2.7; H.5.2; H.5.1; I.2.6; I.4.8; I.4.7;\n  I.4.6"
        ]
    },
    {
        "title": "Summarization Techniques for Pattern Collections in Data Mining",
        "authors": [
            "Taneli Mielikäinen"
        ],
        "summary": "Discovering patterns from data is an important task in data mining. There exist techniques to find large collections of many kinds of patterns from data very efficiently. A collection of patterns can be regarded as a summary of the data. A major difficulty with patterns is that pattern collections summarizing the data well are often very large.   In this dissertation we describe methods for summarizing pattern collections in order to make them also more understandable. More specifically, we focus on the following themes: 1) Quality value simplifications. 2) Pattern orderings. 3) Pattern chains and antichains. 4) Change profiles. 5) Inverse pattern discovery.",
        "published": "2005-05-26T04:41:15Z",
        "link": "http://arxiv.org/abs/cs/0505071v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.DS",
            "E.4; F.2; H.2.8; I.2; I.2.4"
        ]
    },
    {
        "title": "Dominance Based Crossover Operator for Evolutionary Multi-objective   Algorithms",
        "authors": [
            "Olga Roudenko",
            "Marc Schoenauer"
        ],
        "summary": "In spite of the recent quick growth of the Evolutionary Multi-objective Optimization (EMO) research field, there has been few trials to adapt the general variation operators to the particular context of the quest for the Pareto-optimal set. The only exceptions are some mating restrictions that take in account the distance between the potential mates - but contradictory conclusions have been reported. This paper introduces a particular mating restriction for Evolutionary Multi-objective Algorithms, based on the Pareto dominance relation: the partner of a non-dominated individual will be preferably chosen among the individuals of the population that it dominates. Coupled with the BLX crossover operator, two different ways of generating offspring are proposed. This recombination scheme is validated within the well-known NSGA-II framework on three bi-objective benchmark problems and one real-world bi-objective constrained optimization problem. An acceleration of the progress of the population toward the Pareto set is observed on all problems.",
        "published": "2005-05-29T18:40:44Z",
        "link": "http://arxiv.org/abs/cs/0505080v1",
        "categories": [
            "cs.AI",
            "cs.NA"
        ]
    },
    {
        "title": "An ontological approach to the construction of problem-solving models",
        "authors": [
            "Sabine Bruaux",
            "Gilles Kassel",
            "Gilles Morel"
        ],
        "summary": "Our ongoing work aims at defining an ontology-centered approach for building expertise models for the CommonKADS methodology. This approach (which we have named \"OntoKADS\") is founded on a core problem-solving ontology which distinguishes between two conceptualization levels: at an object level, a set of concepts enable us to define classes of problem-solving situations, and at a meta level, a set of meta-concepts represent modeling primitives. In this article, our presentation of OntoKADS will focus on the core ontology and, in particular, on roles - the primitive situated at the interface between domain knowledge and reasoning, and whose ontological status is still much debated. We first propose a coherent, global, ontological framework which enables us to account for this primitive. We then show how this novel characterization of the primitive allows definition of new rules for the construction of expertise models.",
        "published": "2005-05-30T13:42:02Z",
        "link": "http://arxiv.org/abs/cs/0505081v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Defensive forecasting",
        "authors": [
            "Vladimir Vovk",
            "Akimichi Takemura",
            "Glenn Shafer"
        ],
        "summary": "We consider how to make probability forecasts of binary labels. Our main mathematical result is that for any continuous gambling strategy used for detecting disagreement between the forecasts and the actual labels, there exists a forecasting strategy whose forecasts are ideal as far as this gambling strategy is concerned. A forecasting strategy obtained in this way from a gambling strategy demonstrating a strong law of large numbers is simplified and studied empirically.",
        "published": "2005-05-30T21:12:00Z",
        "link": "http://arxiv.org/abs/cs/0505083v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Galactic Gradients, Postbiological Evolution and the Apparent Failure of   SETI",
        "authors": [
            "Milan M. Cirkovic",
            "Robert J. Bradbury"
        ],
        "summary": "Motivated by recent developments impacting our view of Fermi's paradox (absence of extraterrestrials and their manifestations from our past light cone), we suggest a reassessment of the problem itself, as well as of strategies employed by SETI projects so far. The need for such reevaluation is fueled not only by the failure of searches thus far, but also by great advances recently made in astrophysics, astrobiology, computer science and future studies, which have remained largely ignored in SETI practice. As an example of the new approach, we consider the effects of the observed metallicity and temperature gradients in the Milky Way on the spatial distribution of hypothetical advanced extraterrestrial intelligent communities. While, obviously, properties of such communities and their sociological and technological preferences are entirely unknown, we assume that (1) they operate in agreement with the known laws of physics, and (2) that at some point they typically become motivated by a meta-principle embodying the central role of information-processing; a prototype of the latter is the recently suggested Intelligence Principle of Steven J. Dick. There are specific conclusions of practical interest to be drawn from coupling of these reasonable assumptions with the astrophysical and astrochemical structure of the Galaxy. In particular, we suggest that the outer regions of the Galactic disk are most likely locations for advanced SETI targets, and that intelligent communities will tend to migrate outward through the Galaxy as their capacities of information-processing increase, for both thermodynamical and astrochemical reasons. This can also be regarded as a possible generalization of the Galactic Habitable Zone, concept currently much investigated in astrobiology.",
        "published": "2005-06-06T14:10:44Z",
        "link": "http://arxiv.org/abs/astro-ph/0506110v1",
        "categories": [
            "astro-ph",
            "cs.AI",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Sparse Covariance Selection via Robust Maximum Likelihood Estimation",
        "authors": [
            "Onureena Banerjee",
            "Alexandre d'Aspremont",
            "Laurent El Ghaoui"
        ],
        "summary": "We address a problem of covariance selection, where we seek a trade-off between a high likelihood against the number of non-zero elements in the inverse covariance matrix. We solve a maximum likelihood problem with a penalty term given by the sum of absolute values of the elements of the inverse covariance matrix, and allow for imposing bounds on the condition number of the solution. The problem is directly amenable to now standard interior-point algorithms for convex optimization, but remains challenging due to its size. We first give some results on the theoretical computational complexity of the problem, by showing that a recent methodology for non-smooth convex optimization due to Nesterov can be applied to this problem, to greatly improve on the complexity estimate given by interior-point algorithms. We then examine two practical algorithms aimed at solving large-scale, noisy (hence dense) instances: one is based on a block-coordinate descent approach, where columns and rows are updated sequentially, another applies a dual version of Nesterov's method.",
        "published": "2005-06-08T21:08:38Z",
        "link": "http://arxiv.org/abs/cs/0506023v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "F.2.1; G.1.3; G.1.6; G.3; J.3"
        ]
    },
    {
        "title": "The Hyper-Cortex of Human Collective-Intelligence Systems",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "Individual-intelligence research, from a neurological perspective, discusses the hierarchical layers of the cortex as a structure that performs conceptual abstraction and specification. This theory has been used to explain how motor-cortex regions responsible for different behavioral modalities such as writing and speaking can be utilized to express the same general concept represented higher in the cortical hierarchy. For example, the concept of a dog, represented across a region of high-level cortical-neurons, can either be written or spoken about depending on the individual's context. The higher-layer cortical areas project down the hierarchy, sending abstract information to specific regions of the motor-cortex for contextual implementation. In this paper, this idea is expanded to incorporate collective-intelligence within a hyper-cortical construct. This hyper-cortex is a multi-layered network used to represent abstract collective concepts. These ideas play an important role in understanding how collective-intelligence systems can be engineered to handle problem abstraction and solution specification. Finally, a collection of common problems in the scientific community are solved using an artificial hyper-cortex generated from digital-library metadata.",
        "published": "2005-06-08T21:33:44Z",
        "link": "http://arxiv.org/abs/cs/0506024v3",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.DL",
            "cs.NE"
        ]
    },
    {
        "title": "Preferential and Preferential-discriminative Consequence relations",
        "authors": [
            "Jonathan Ben-Naim"
        ],
        "summary": "The present paper investigates consequence relations that are both non-monotonic and paraconsistent. More precisely, we put the focus on preferential consequence relations, i.e. those relations that can be defined by a binary preference relation on states labelled by valuations. We worked with a general notion of valuation that covers e.g. the classical valuations as well as certain kinds of many-valued valuations. In the many-valued cases, preferential consequence relations are paraconsistant (in addition to be non-monotonic), i.e. they are capable of drawing reasonable conclusions which contain contradictions. The first purpose of this paper is to provide in our general framework syntactic characterizations of several families of preferential relations. The second and main purpose is to provide, again in our general framework, characterizations of several families of preferential discriminative consequence relations. They are defined exactly as the plain version, but any conclusion such that its negation is also a conclusion is rejected (these relations bring something new essentially in the many-valued cases).",
        "published": "2005-06-09T12:53:47Z",
        "link": "http://arxiv.org/abs/cs/0506030v6",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "A Constrained Object Model for Configuration Based Workflow Composition",
        "authors": [
            "Patrick Albert",
            "Laurent Henocque",
            "Mathias Kleiner"
        ],
        "summary": "Automatic or assisted workflow composition is a field of intense research for applications to the world wide web or to business process modeling. Workflow composition is traditionally addressed in various ways, generally via theorem proving techniques. Recent research observed that building a composite workflow bears strong relationships with finite model search, and that some workflow languages can be defined as constrained object metamodels . This lead to consider the viability of applying configuration techniques to this problem, which was proven feasible. Constrained based configuration expects a constrained object model as input. The purpose of this document is to formally specify the constrained object model involved in ongoing experiments and research using the Z specification language.",
        "published": "2005-06-09T14:57:53Z",
        "link": "http://arxiv.org/abs/cs/0506031v1",
        "categories": [
            "cs.AI",
            "C.0; D.2.1; D.3.1; F.4.1"
        ]
    },
    {
        "title": "Competitive on-line learning with a convex loss function",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "We consider the problem of sequential decision making under uncertainty in which the loss caused by a decision depends on the following binary observation. In competitive on-line learning, the goal is to design decision algorithms that are almost as good as the best decision rules in a wide benchmark class, without making any assumptions about the way the observations are generated. However, standard algorithms in this area can only deal with finite-dimensional (often countable) benchmark classes. In this paper we give similar results for decision rules ranging over an arbitrary reproducing kernel Hilbert space. For example, it is shown that for a wide class of loss functions (including the standard square, absolute, and log loss functions) the average loss of the master algorithm, over the first $N$ observations, does not exceed the average loss of the best decision rule with a bounded norm plus $O(N^{-1/2})$. Our proof technique is very different from the standard ones and is based on recent results about defensive forecasting. Given the probabilities produced by a defensive forecasting algorithm, which are known to be well calibrated and to have good resolution in the long run, we use the expected loss minimization principle to find a suitable decision.",
        "published": "2005-06-11T18:11:22Z",
        "link": "http://arxiv.org/abs/cs/0506041v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Redundancy in Logic II: 2CNF and Horn Propositional Formulae",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "We report complexity results about redundancy of formulae in 2CNF form. We first consider the problem of checking redundancy and show some algorithms that are slightly better than the trivial one. We then analyze problems related to finding irredundant equivalent subsets (I.E.S.) of a given set. The concept of cyclicity proved to be relevant to the complexity of these problems. Some results about Horn formulae are also shown.",
        "published": "2005-06-17T19:28:29Z",
        "link": "http://arxiv.org/abs/cs/0506074v3",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Field geology with a wearable computer: 1st results of the Cyborg   Astrobiologist System",
        "authors": [
            "Patrick C. McGuire",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Markus Oesker",
            "Robert Haschke",
            "Joerg Ontrup",
            "Helge Ritter"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. This work is more of a test of the whole system, rather than of any one part of the system. However, beyond the concept of the system itself, the uncommon map (despite its simplicity) is the main innovative part of the system. The uncommon map helps to determine interest-points in a context-free manner. Overall, the hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner. We use these capabilities for autonomous guidance towards geological points-of-interest.",
        "published": "2005-06-24T10:25:22Z",
        "link": "http://arxiv.org/abs/cs/0506089v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO"
        ]
    },
    {
        "title": "Deriving a Stationary Dynamic Bayesian Network from a Logic Program with   Recursive Loops",
        "authors": [
            "Y. D. Shen",
            "Q. Yang",
            "J. H. You",
            "L. Y. Yuan"
        ],
        "summary": "Recursive loops in a logic program present a challenging problem to the PLP framework. On the one hand, they loop forever so that the PLP backward-chaining inferences would never stop. On the other hand, they generate cyclic influences, which are disallowed in Bayesian networks. Therefore, in existing PLP approaches logic programs with recursive loops are considered to be problematic and thus are excluded. In this paper, we propose an approach that makes use of recursive loops to build a stationary dynamic Bayesian network. Our work stems from an observation that recursive loops in a logic program imply a time sequence and thus can be used to model a stationary dynamic Bayesian network without using explicit time parameters. We introduce a Bayesian knowledge base with logic clauses of the form $A \\leftarrow A_1,...,A_l, true, Context, Types$, which naturally represents the knowledge that the $A_i$s have direct influences on $A$ in the context $Context$ under the type constraints $Types$. We then use the well-founded model of a logic program to define the direct influence relation and apply SLG-resolution to compute the space of random variables together with their parental connections. We introduce a novel notion of influence clauses, based on which a declarative semantics for a Bayesian knowledge base is established and algorithms for building a two-slice dynamic Bayesian network from a logic program are developed.",
        "published": "2005-06-27T04:07:34Z",
        "link": "http://arxiv.org/abs/cs/0506095v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.LO"
        ]
    },
    {
        "title": "A Study for the Feature Core of Dynamic Reduct",
        "authors": [
            "Jiayang Wang"
        ],
        "summary": "To the reduct problems of decision system, the paper proposes the notion of dynamic core according to the dynamic reduct model. It describes various formal definitions of dynamic core, and discusses some properties about dynamic core. All of these show that dynamic core possesses the essential characters of the feature core.",
        "published": "2005-07-05T13:02:02Z",
        "link": "http://arxiv.org/abs/cs/0507010v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "Two-dimensional cellular automata and the analysis of correlated time   series",
        "authors": [
            "Luis O. Rigo Jr.",
            "Valmir C. Barbosa"
        ],
        "summary": "Correlated time series are time series that, by virtue of the underlying process to which they refer, are expected to influence each other strongly. We introduce a novel approach to handle such time series, one that models their interaction as a two-dimensional cellular automaton and therefore allows them to be treated as a single entity. We apply our approach to the problems of filling gaps and predicting values in rainfall time series. Computational results show that the new approach compares favorably to Kalman smoothing and filtering.",
        "published": "2005-07-08T12:47:38Z",
        "link": "http://arxiv.org/abs/cs/0507023v1",
        "categories": [
            "cs.AI",
            "B.6.1; G.3"
        ]
    },
    {
        "title": "ATNoSFERES revisited",
        "authors": [
            "Samuel Landau",
            "Olivier Sigaud",
            "Marc Schoenauer"
        ],
        "summary": "ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network. Genotypes are strings of tokens of a stack-based language, whose execution builds the labeled graph. The original ATNoSFERES, using a bitstring to represent the language tokens, has been favorably compared in previous work to several Michigan style LCSs architectures in the context of Non Markov problems. Several modifications of ATNoSFERES are proposed here: the most important one conceptually being a representational change: each token is now represented by an integer, hence the genotype is a string of integers; several other modifications of the underlying grammar language are also proposed. The resulting ATNoSFERES-II is validated on several standard animat Non Markov problems, on which it outperforms all previously published results in the LCS literature. The reasons for these improvement are carefully analyzed, and some assumptions are proposed on the underlying mechanisms in order to explain these good results.",
        "published": "2005-07-11T13:11:25Z",
        "link": "http://arxiv.org/abs/cs/0507029v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Enhancing Global SLS-Resolution with Loop Cutting and Tabling Mechanisms",
        "authors": [
            "Yi-Dong Shen",
            "Jia-Huai You",
            "Li-Yan Yuan"
        ],
        "summary": "Global SLS-resolution is a well-known procedural semantics for top-down computation of queries under the well-founded model. It inherits from SLDNF-resolution the {\\em linearity} property of derivations, which makes it easy and efficient to implement using a simple stack-based memory structure. However, like SLDNF-resolution it suffers from the problem of infinite loops and redundant computations. To resolve this problem, in this paper we develop a new procedural semantics, called {\\em SLTNF-resolution}, by enhancing Global SLS-resolution with loop cutting and tabling mechanisms. SLTNF-resolution is sound and complete w.r.t. the well-founded semantics for logic programs with the bounded-term-size property, and is superior to existing linear tabling procedural semantics such as SLT-resolution.",
        "published": "2005-07-14T08:24:14Z",
        "link": "http://arxiv.org/abs/cs/0507035v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Distributed Regression in Sensor Networks: Training Distributively with   Alternating Projections",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "Wireless sensor networks (WSNs) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing. The problem of distributed or decentralized estimation has often been considered in the context of parametric models. However, the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models. In this paper, a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of WSN applications including field estimation. Here, starting with the standard regularized kernel least-squares estimator, a message-passing algorithm for distributed estimation in WSNs is derived. The algorithm can be viewed as an instantiation of the successive orthogonal projection (SOP) algorithm. Various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach.",
        "published": "2005-07-18T00:45:12Z",
        "link": "http://arxiv.org/abs/cs/0507039v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Pattern Recognition for Conditionally Independent Data",
        "authors": [
            "Daniil Ryabko"
        ],
        "summary": "In this work we consider the task of relaxing the i.i.d assumption in pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.   We find a broad class of learning algorithms for which estimations of the probability of a classification error achieved under the classical i.i.d. assumption can be generalised to the similar estimates for the case of conditionally i.i.d. examples.",
        "published": "2005-07-18T08:10:10Z",
        "link": "http://arxiv.org/abs/cs/0507040v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Monotone Conditional Complexity Bounds on Future Prediction Errors",
        "authors": [
            "Alexey Chernov",
            "Marcus Hutter"
        ],
        "summary": "We bound the future loss when predicting any (computably) stochastic sequence online. Solomonoff finitely bounded the total deviation of his universal predictor M from the true distribution m by the algorithmic complexity of m. Here we assume we are at a time t>1 and already observed x=x_1...x_t. We bound the future prediction performance on x_{t+1}x_{t+2}... by a new variant of algorithmic complexity of m given x, plus the complexity of the randomness deficiency of x. The new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged. We also briefly discuss potential generalizations to Bayesian model classes and to classification problems.",
        "published": "2005-07-18T12:34:53Z",
        "link": "http://arxiv.org/abs/cs/0507041v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "I.2.6; E.4; G.3; F.1.3"
        ]
    },
    {
        "title": "In the beginning was game semantics",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "This article presents an overview of computability logic -- the game-semantically constructed logic of interactive computational tasks and resources. There is only one non-overview, technical section in it, devoted to a proof of the soundness of affine logic with respect to the semantics of computability logic. A comprehensive online source on the subject can be found at http://www.cis.upenn.edu/~giorgi/cl.html",
        "published": "2005-07-18T18:45:38Z",
        "link": "http://arxiv.org/abs/cs/0507045v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Redundancy in Logic III: Non-Mononotonic Reasoning",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "Results about the redundancy of circumscriptive and default theories are presented. In particular, the complexity of establishing whether a given theory is redundant is establihsed.",
        "published": "2005-07-19T19:25:11Z",
        "link": "http://arxiv.org/abs/cs/0507048v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Nonrepetitive Paths and Cycles in Graphs with Application to Sudoku",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We provide a simple linear time transformation from a directed or undirected graph with labeled edges to an unlabeled digraph, such that paths in the input graph in which no two consecutive edges have the same label correspond to paths in the transformed graph and vice versa. Using this transformation, we provide efficient algorithms for finding paths and cycles with no two consecutive equal labels. We also consider related problems where the paths and cycles are required to be simple; we find efficient algorithms for the undirected case of these problems but show the directed case to be NP-complete. We apply our path and cycle finding algorithms in a program for generating and solving Sudoku puzzles, and show experimentally that they lead to effective puzzle-solving rules that may also be of interest to human Sudoku puzzle solvers.",
        "published": "2005-07-20T15:58:30Z",
        "link": "http://arxiv.org/abs/cs/0507053v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "F.2.2; I.2.1"
        ]
    },
    {
        "title": "Explorations in engagement for humans and robots",
        "authors": [
            "Candace L. Sidner",
            "Christopher Lee",
            "Cory Kidd",
            "Neal Lesh",
            "Charles Rich"
        ],
        "summary": "This paper explores the concept of engagement, the process by which individuals in an interaction start, maintain and end their perceived connection to one another. The paper reports on one aspect of engagement among human interactors--the effect of tracking faces during an interaction. It also describes the architecture of a robot that can participate in conversational, collaborative interactions with engagement gestures. Finally, the paper reports on findings of experiments with human participants who interacted with a robot when it either performed or did not perform engagement gestures. Results of the human-robot studies indicate that people become engaged with robots: they direct their attention to the robot more often in interactions where engagement gestures are present, and they find interactions more appropriate when engagement gestures are present than when they are not.",
        "published": "2005-07-21T21:56:34Z",
        "link": "http://arxiv.org/abs/cs/0507056v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.RO",
            "I.2.7; I.2.9"
        ]
    },
    {
        "title": "Data complexity of answering conjunctive queries over SHIQ knowledge   bases",
        "authors": [
            "M. Magdalena Ortiz de la Fuente",
            "Diego Calvanese",
            "Thomas Eiter",
            "Enrico Franconi"
        ],
        "summary": "An algorithm for answering conjunctive queries over SHIQ knowledge bases that is coNP in data complexity is given. The algorithm is based on the tableau algorithm for reasoning with individuals in SHIQ. The blocking conditions of the tableau are weakened in such a way that the set of models the modified algorithm yields suffices to check query entailment. The modified blocking conditions are based on the ones proposed by Levy and Rousset for reasoning with Horn Rules in the description logic ALCNR.",
        "published": "2005-07-22T15:43:07Z",
        "link": "http://arxiv.org/abs/cs/0507059v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Algebras of Measurements: the logical structure of Quantum Mechanics",
        "authors": [
            "Daniel Lehmann",
            "Kurt Engesser",
            "Dov M. Gabbay"
        ],
        "summary": "In Quantum Physics, a measurement is represented by a projection on some closed subspace of a Hilbert space. We study algebras of operators that abstract from the algebra of projections on closed subspaces of a Hilbert space. The properties of such operators are justified on epistemological grounds. Commutation of measurements is a central topic of interest. Classical logical systems may be viewed as measurement algebras in which all measurements commute. Keywords: Quantum measurements, Measurement algebras, Quantum Logic. PACS: 02.10.-v.",
        "published": "2005-07-24T12:33:58Z",
        "link": "http://arxiv.org/abs/quant-ph/0507231v2",
        "categories": [
            "quant-ph",
            "cs.AI"
        ]
    },
    {
        "title": "A Fast Greedy Algorithm for Outlier Mining",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. In [38], the problem of outlier detection in categorical data is defined as an optimization problem and a local-search heuristic based algorithm (LSA) is presented. However, as is the case with most iterative type algorithms, the LSA algorithm is still very time-consuming on very large datasets. In this paper, we present a very fast greedy algorithm for mining outliers under the same optimization model. Experimental results on real datasets and large synthetic datasets show that: (1) Our algorithm has comparable performance with respect to those state-of-art outlier detection algorithms on identifying true outliers and (2) Our algorithm can be an order of magnitude faster than LSA algorithm.",
        "published": "2005-07-27T02:14:02Z",
        "link": "http://arxiv.org/abs/cs/0507065v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Conjunctive Query Containment and Answering under Description Logics   Constraints",
        "authors": [
            "Diego Calvanese",
            "Giuseppe De Giacomo",
            "Maurizio Lenzerini"
        ],
        "summary": "Query containment and query answering are two important computational tasks in databases. While query answering amounts to compute the result of a query over a database, query containment is the problem of checking whether for every database, the result of one query is a subset of the result of another query.   In this paper, we deal with unions of conjunctive queries, and we address query containment and query answering under Description Logic constraints. Every such constraint is essentially an inclusion dependencies between concepts and relations, and their expressive power is due to the possibility of using complex expressions, e.g., intersection and difference of relations, special forms of quantification, regular expressions over binary relations, in the specification of the dependencies. These types of constraints capture a great variety of data models, including the relational, the entity-relationship, and the object-oriented model, all extended with various forms of constraints, and also the basic features of the ontology languages used in the context of the Semantic Web.   We present the following results on both query containment and query answering. We provide a method for query containment under Description Logic constraints, thus showing that the problem is decidable, and analyze its computational complexity. We prove that query containment is undecidable in the case where we allow inequalities in the right-hand side query, even for very simple constraints and queries. We show that query answering under Description Logic constraints can be reduced to query containment, and illustrate how such a reduction provides upper bound results with respect to both combined and data complexity.",
        "published": "2005-07-28T08:25:43Z",
        "link": "http://arxiv.org/abs/cs/0507067v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Regularity of Position Sequences",
        "authors": [
            "Manfred Harringer"
        ],
        "summary": "A person is given a numbered sequence of positions on a sheet of paper. The person is asked, \"Which will be the next (or the next after that) position?\" Everyone has an opinion as to how he or she would proceed. There are regular sequences for which there is general agreement on how to continue. However, there are less regular sequences for which this assessment is less certain. There are sequences for which every continuation is perceived to be arbitrary. I would like to present a mathematical model that reflects these opinions and perceptions with the aid of a valuation function. It is necessary to apply a rich set of invariant features of position sequences to ensure the quality of this model. All other properties of the model are arbitrary.",
        "published": "2005-08-01T18:55:57Z",
        "link": "http://arxiv.org/abs/cs/0508007v4",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "q-bio.NC",
            "I.5.1; I.6.8; J.4"
        ]
    },
    {
        "title": "Polymorphic Self-* Agents for Stigmergic Fault Mitigation in Large-Scale   Real-Time Embedded Systems",
        "authors": [
            "Derek Messie",
            "Jae C. Oh"
        ],
        "summary": "Organization and coordination of agents within large-scale, complex, distributed environments is one of the primary challenges in the field of multi-agent systems. A lot of interest has surfaced recently around self-* (self-organizing, self-managing, self-optimizing, self-protecting) agents. This paper presents polymorphic self-* agents that evolve a core set of roles and behavior based on environmental cues. The agents adapt these roles based on the changing demands of the environment, and are directly implementable in computer systems applications. The design combines strategies from game theory, stigmergy, and other biologically inspired models to address fault mitigation in large-scale, real-time, distributed systems. The agents are embedded within the individual digital signal processors of BTeV, a High Energy Physics experiment consisting of 2500 such processors. Results obtained using a SWARM simulation of the BTeV environment demonstrate the polymorphic character of the agents, and show how this design exceeds performance and reliability metrics obtained from comparable centralized, and even traditional decentralized approaches.",
        "published": "2005-08-04T01:43:42Z",
        "link": "http://arxiv.org/abs/cs/0508032v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Lossy source encoding via message-passing and decimation over   generalized codewords of LDGM codes",
        "authors": [
            "Martin J. Wainwright",
            "Elitza Maneva"
        ],
        "summary": "We describe message-passing and decimation approaches for lossy source coding using low-density generator matrix (LDGM) codes. In particular, this paper addresses the problem of encoding a Bernoulli(0.5) source: for randomly generated LDGM codes with suitably irregular degree distributions, our methods yield performance very close to the rate distortion limit over a range of rates. Our approach is inspired by the survey propagation (SP) algorithm, originally developed by Mezard et al. for solving random satisfiability problems. Previous work by Maneva et al. shows how SP can be understood as belief propagation (BP) for an alternative representation of satisfiability problems. In analogy to this connection, our approach is to define a family of Markov random fields over generalized codewords, from which local message-passing rules can be derived in the standard way. The overall source encoding method is based on message-passing, setting a subset of bits to their preferred values (decimation), and reducing the code.",
        "published": "2005-08-15T02:00:55Z",
        "link": "http://arxiv.org/abs/cs/0508068v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "MAP estimation via agreement on (hyper)trees: Message-passing and linear   programming",
        "authors": [
            "Martin J. Wainwright",
            "Tommi S. Jaakkola",
            "Alan S. Willsky"
        ],
        "summary": "We develop and analyze methods for computing provably optimal {\\em maximum a posteriori} (MAP) configurations for a subclass of Markov random fields defined on graphs with cycles. By decomposing the original distribution into a convex combination of tree-structured distributions, we obtain an upper bound on the optimal value of the original problem (i.e., the log probability of the MAP assignment) in terms of the combined optimal values of the tree problems. We prove that this upper bound is tight if and only if all the tree distributions share an optimal configuration in common. An important implication is that any such shared configuration must also be a MAP configuration for the original distribution. Next we develop two approaches to attempting to obtain tight upper bounds: (a) a {\\em tree-relaxed linear program} (LP), which is derived from the Lagrangian dual of the upper bounds; and (b) a {\\em tree-reweighted max-product message-passing algorithm} that is related to but distinct from the max-product algorithm. In this way, we establish a connection between a certain LP relaxation of the mode-finding problem, and a reweighted form of the max-product (min-sum) message-passing algorithm.",
        "published": "2005-08-15T14:32:21Z",
        "link": "http://arxiv.org/abs/cs/0508070v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "Universal Learning of Repeated Matrix Games",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We study and compare the learning dynamics of two universal learning algorithms, one based on Bayesian learning and the other on prediction with expert advice. Both approaches have strong asymptotic performance guarantees. When confronted with the task of finding good long-term strategies in repeated 2x2 matrix games, they behave quite differently.",
        "published": "2005-08-16T16:27:25Z",
        "link": "http://arxiv.org/abs/cs/0508073v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "A primer on Answer Set Programming",
        "authors": [
            "Alessandro Provetti"
        ],
        "summary": "A introduction to the syntax and Semantics of Answer Set Programming intended as an handout to [under]graduate students taking Artificial Intlligence or Logic Programming classes.",
        "published": "2005-08-23T15:05:12Z",
        "link": "http://arxiv.org/abs/cs/0508100v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; I.2.3"
        ]
    },
    {
        "title": "Maximum Weight Matching via Max-Product Belief Propagation",
        "authors": [
            "Mohsen Bayati",
            "Devavrat Shah",
            "Mayank Sharma"
        ],
        "summary": "Max-product \"belief propagation\" is an iterative, local, message-passing algorithm for finding the maximum a posteriori (MAP) assignment of a discrete probability distribution specified by a graphical model. Despite the spectacular success of the algorithm in many application areas such as iterative decoding, computer vision and combinatorial optimization which involve graphs with many cycles, theoretical results about both correctness and convergence of the algorithm are known in few cases (Weiss-Freeman Wainwright, Yeddidia-Weiss-Freeman, Richardson-Urbanke}.   In this paper we consider the problem of finding the Maximum Weight Matching (MWM) in a weighted complete bipartite graph. We define a probability distribution on the bipartite graph whose MAP assignment corresponds to the MWM. We use the max-product algorithm for finding the MAP of this distribution or equivalently, the MWM on the bipartite graph. Even though the underlying bipartite graph has many short cycles, we find that surprisingly, the max-product algorithm always converges to the correct MAP assignment as long as the MAP assignment is unique. We provide a bound on the number of iterations required by the algorithm and evaluate the computational cost of the algorithm. We find that for a graph of size $n$, the computational cost of the algorithm scales as $O(n^3)$, which is the same as the computational cost of the best known algorithm. Finally, we establish the precise relation between the max-product algorithm and the celebrated {\\em auction} algorithm proposed by Bertsekas. This suggests possible connections between dual algorithm and max-product algorithm for discrete optimization problems.",
        "published": "2005-08-23T18:37:50Z",
        "link": "http://arxiv.org/abs/cs/0508101v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "G.2.2; G.3; I.2.6"
        ]
    },
    {
        "title": "Temporal Phylogenetic Networks and Logic Programming",
        "authors": [
            "Esra Erdem",
            "Vladimir Lifschitz",
            "Don Ringe"
        ],
        "summary": "The concept of a temporal phylogenetic network is a mathematical model of evolution of a family of natural languages. It takes into account the fact that languages can trade their characteristics with each other when linguistic communities are in contact, and also that a contact is only possible when the languages are spoken at the same time. We show how computational methods of answer set programming and constraint logic programming can be used to generate plausible conjectures about contacts between prehistoric linguistic communities, and illustrate our approach by applying it to the evolutionary history of Indo-European languages.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2005-08-30T13:04:05Z",
        "link": "http://arxiv.org/abs/cs/0508129v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL"
        ]
    },
    {
        "title": "Planning with Preferences using Logic Programming",
        "authors": [
            "Tran Cao Son",
            "Enrico Pontelli"
        ],
        "summary": "We present a declarative language, PP, for the high-level specification of preferences between possible solutions (or trajectories) of a planning problem. This novel language allows users to elegantly express non-trivial, multi-dimensional preferences and priorities over such preferences. The semantics of PP allows the identification of most preferred trajectories for a given goal. We also provide an answer set programming implementation of planning problems with PP preferences.",
        "published": "2005-08-31T14:50:22Z",
        "link": "http://arxiv.org/abs/cs/0508132v1",
        "categories": [
            "cs.AI",
            "F.4.1"
        ]
    },
    {
        "title": "Lattices for Dynamic, Hierarchic & Overlapping Categorization: the Case   of Epistemic Communities",
        "authors": [
            "Camille Roth",
            "Paul Bourgine"
        ],
        "summary": "We present a method for hierarchic categorization and taxonomy evolution description. We focus on the structure of epistemic communities (ECs), or groups of agents sharing common knowledge concerns. Introducing a formal framework based on Galois lattices, we categorize ECs in an automated and hierarchically structured way and propose criteria for selecting the most relevant epistemic communities - for instance, ECs gathering a certain proportion of agents and thus prototypical of major fields. This process produces a manageable, insightful taxonomy of the community. Then, the longitudinal study of these static pictures makes possible an historical description. In particular, we capture stylized facts such as field progress, decline, specialization, interaction (merging or splitting), and paradigm emergence. The detection of such patterns in social networks could fruitfully be applied to other contexts.",
        "published": "2005-09-04T18:15:40Z",
        "link": "http://arxiv.org/abs/nlin/0509007v1",
        "categories": [
            "nlin.AO",
            "cs.AI",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Clustering Mixed Numeric and Categorical Data: A Cluster Ensemble   Approach",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "Clustering is a widely used technique in data mining applications for discovering patterns in underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either numeric or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining applications. In this paper, we propose a novel divide-and-conquer technique to solve this problem. First, the original mixed dataset is divided into two sub-datasets: the pure categorical dataset and the pure numeric dataset. Next, existing well established clustering algorithms designed for different types of datasets are employed to produce corresponding clusters. Last, the clustering results on the categorical and numeric dataset are combined as a categorical dataset, on which the categorical data clustering algorithm is used to get the final clusters. Our contribution in this paper is to provide an algorithm framework for the mixed attributes clustering problem, in which existing clustering algorithms can be easily integrated, the capabilities of different kinds of clustering algorithms and characteristics of different types of datasets could be fully exploited. Comparisons with other clustering algorithms on real life datasets illustrate the superiority of our approach.",
        "published": "2005-09-05T02:47:12Z",
        "link": "http://arxiv.org/abs/cs/0509011v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Transitive Text Mining for Information Extraction and Hypothesis   Generation",
        "authors": [
            "Johannes Stegmann",
            "Guenter Grohmann"
        ],
        "summary": "Transitive text mining - also named Swanson Linking (SL) after its primary and principal researcher - tries to establish meaningful links between literature sets which are virtually disjoint in the sense that each does not mention the main concept of the other. If successful, SL may give rise to the development of new hypotheses. In this communication we describe our approach to transitive text mining which employs co-occurrence analysis of the medical subject headings (MeSH), the descriptors assigned to papers indexed in PubMed. In addition, we will outline the current state of our web-based information system which will enable our users to perform literature-driven hypothesis building on their own.",
        "published": "2005-09-07T12:16:22Z",
        "link": "http://arxiv.org/abs/cs/0509020v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "A formally verified proof of the prime number theorem",
        "authors": [
            "Jeremy Avigad",
            "Kevin Donnelly",
            "David Gray",
            "Paul Raff"
        ],
        "summary": "The prime number theorem, established by Hadamard and de la Vall'ee Poussin independently in 1896, asserts that the density of primes in the positive integers is asymptotic to 1 / ln x. Whereas their proofs made serious use of the methods of complex analysis, elementary proofs were provided by Selberg and Erd\"os in 1948. We describe a formally verified version of Selberg's proof, obtained using the Isabelle proof assistant.",
        "published": "2005-09-09T15:47:35Z",
        "link": "http://arxiv.org/abs/cs/0509025v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "A Simple Model to Generate Hard Satisfiable Instances",
        "authors": [
            "Ke Xu",
            "Frederic Boussemart",
            "Fred Hemery",
            "Christophe Lecoutre"
        ],
        "summary": "In this paper, we try to further demonstrate that the models of random CSP instances proposed by [Xu and Li, 2000; 2003] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisfiable instances whose hardness is similar to unforced satisfiable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods.",
        "published": "2005-09-12T13:30:16Z",
        "link": "http://arxiv.org/abs/cs/0509032v1",
        "categories": [
            "cs.AI",
            "cond-mat.stat-mech",
            "cs.CC",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "K-Histograms: An Efficient Clustering Algorithm for Categorical Dataset",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng",
            "Bin Dong"
        ],
        "summary": "Clustering categorical data is an integral part of data mining and has attracted much attention recently. In this paper, we present k-histogram, a new efficient algorithm for clustering categorical data. The k-histogram algorithm extends the k-means algorithm to categorical domain by replacing the means of clusters with histograms, and dynamically updates histograms in the clustering process. Experimental results on real datasets show that k-histogram algorithm can produce better clustering results than k-modes algorithm, the one related with our work most closely.",
        "published": "2005-09-13T06:33:08Z",
        "link": "http://arxiv.org/abs/cs/0509033v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Authoring case based training by document data extraction",
        "authors": [
            "Christian Betz",
            "Alexander Hoernlein",
            "Frank Puppe"
        ],
        "summary": "In this paper, we propose an scalable approach to modeling based upon word processing documents, and we describe the tool Phoenix providing the technical infrastructure.   For our training environment d3web.Train, we developed a tool to extract case knowledge from existing documents, usually dismissal records, extending Phoenix to d3web.CaseImporter. Independent authors used this tool to develop training systems, observing a significant decrease of time for setteling-in and a decrease of time necessary for developing a case.",
        "published": "2005-09-14T13:20:21Z",
        "link": "http://arxiv.org/abs/cs/0509040v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Interactive Unawareness Revisited",
        "authors": [
            "Joseph Y. Halpern",
            "Leandro C. Rego"
        ],
        "summary": "We analyze a model of interactive unawareness introduced by Heifetz, Meier and Schipper (HMS). We consider two axiomatizations for their model, which capture different notions of validity. These axiomatizations allow us to compare the HMS approach to both the standard (S5) epistemic logic and two other approaches to unawareness: that of Fagin and Halpern and that of Modica and Rustichini. We show that the differences between the HMS approach and the others are mainly due to the notion of validity used and the fact that the HMS is based on a 3-valued propositional logic.",
        "published": "2005-09-19T17:07:37Z",
        "link": "http://arxiv.org/abs/cs/0509058v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "CP-nets and Nash equilibria",
        "authors": [
            "Krzysztof R. Apt",
            "Francesca Rossi",
            "K. Brent Venable"
        ],
        "summary": "We relate here two formalisms that are used for different purposes in reasoning about multi-agent systems. One of them are strategic games that are used to capture the idea that agents interact with each other while pursuing their own interest. The other are CP-nets that were introduced to express qualitative and conditional preferences of the users and which aim at facilitating the process of preference elicitation. To relate these two formalisms we introduce a natural, qualitative, extension of the notion of a strategic game. We show then that the optimal outcomes of a CP-net are exactly the Nash equilibria of an appropriately defined strategic game in the above sense. This allows us to use the techniques of game theory to search for optimal outcomes of CP-nets and vice-versa, to use techniques developed for CP-nets to search for Nash equilibria of the considered games.",
        "published": "2005-09-22T16:07:40Z",
        "link": "http://arxiv.org/abs/cs/0509071v1",
        "categories": [
            "cs.GT",
            "cs.AI",
            "J.4; I.2.11"
        ]
    },
    {
        "title": "Automatic extraction of paraphrastic phrases from medium size corpora",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "This paper presents a versatile system intended to acquire paraphrastic phrases from a representative corpus. In order to decrease the time spent on the elaboration of resources for NLP system (for example Information Extraction, IE hereafter), we suggest to use a machine learning system that helps defining new templates and associated resources. This knowledge is automatically derived from the text collection, in interaction with a large semantic network.",
        "published": "2005-09-28T16:15:27Z",
        "link": "http://arxiv.org/abs/cs/0509092v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "Sur le statut référentiel des entités nommées",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "We show in this paper that, on the one hand, named entities can be designated using different denominations and that, on the second hand, names denoting named entities are polysemous. The analysis cannot be limited to reference resolution but should take into account naming strategies, which are mainly based on two linguistic operations: synecdoche and metonymy. Lastly, we present a model that explicitly represents the different denominations in discourse, unifying the way to represent linguistic knowledge and world knowledge.",
        "published": "2005-10-07T17:39:40Z",
        "link": "http://arxiv.org/abs/cs/0510020v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "An algorithmic and a geometric characterization of Coarsening At Random",
        "authors": [
            "Richard D. Gill",
            "Peter D. Grunwald"
        ],
        "summary": "We show that the class of conditional distributions satisfying the coarsening at Random (CAR) property for discrete data has a simple and robust algorithmic description based on randomized uniform multicovers: combinatorial objects generalizing the notion of partition of a set. However, the complexity of a given CAR mechanism can be large: the maximal \"height\" of the needed multicovers can be exponential in the number of points in the sample space. The results stem from a geometric interpretation of the set of CAR distributions as a convex polytope and a characterization of its extreme points. The hierarchy of CAR models defined in this way could be useful in parsimonious statistical modelling of CAR mechanisms, though the results also raise doubts in applied work as to the meaningfulness of the CAR assumption in its full generality.",
        "published": "2005-10-13T12:03:06Z",
        "link": "http://arxiv.org/abs/math/0510276v3",
        "categories": [
            "math.ST",
            "cs.AI",
            "stat.ME",
            "stat.TH",
            "62A01 (Primary), 62N01, 60A99, 68T37 (Secondary)"
        ]
    },
    {
        "title": "Semantic Optimization Techniques for Preference Queries",
        "authors": [
            "Jan Chomicki"
        ],
        "summary": "Preference queries are relational algebra or SQL queries that contain occurrences of the winnow operator (\"find the most preferred tuples in a given relation\"). Such queries are parameterized by specific preference relations. Semantic optimization techniques make use of integrity constraints holding in the database. In the context of semantic optimization of preference queries, we identify two fundamental properties: containment of preference relations relative to integrity constraints and satisfaction of order axioms relative to integrity constraints. We show numerous applications of those notions to preference query evaluation and optimization. As integrity constraints, we consider constraint-generating dependencies, a class generalizing functional dependencies. We demonstrate that the problems of containment and satisfaction of order axioms can be captured as specific instances of constraint-generating dependency entailment. This makes it possible to formulate necessary and sufficient conditions for the applicability of our techniques as constraint validity problems. We characterize the computational complexity of such problems.",
        "published": "2005-10-14T13:25:34Z",
        "link": "http://arxiv.org/abs/cs/0510036v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Hiérarchisation des règles d'association en fouille de textes",
        "authors": [
            "Rokia Bendaoud",
            "Yannick Toussaint",
            "Amedeo Napoli"
        ],
        "summary": "Extraction of association rules is widely used as a data mining method. However, one of the limit of this approach comes from the large number of extracted rules and the difficulty for a human expert to deal with the totality of these rules. We propose to solve this problem by structuring the set of rules into hierarchy. The expert can then therefore explore the rules, access from one rule to another one more general when we raise up in the hierarchy, and in other hand, or a more specific rules. Rules are structured at two levels. The global level aims at building a hierarchy from the set of rules extracted. Thus we define a first type of rule-subsomption relying on Galois lattices. The second level consists in a local and more detailed analysis of each rule. It generate for a given rule a set of generalization rules structured into a local hierarchy. This leads to the definition of a second type of subsomption. This subsomption comes from inductive logic programming and integrates a terminological model.",
        "published": "2005-10-14T14:24:26Z",
        "link": "http://arxiv.org/abs/cs/0510037v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Integration of the DOLCE top-level ontology into the OntoSpec   methodology",
        "authors": [
            "Gilles Kassel"
        ],
        "summary": "This report describes a new version of the OntoSpec methodology for ontology building. Defined by the LaRIA Knowledge Engineering Team (University of Picardie Jules Verne, Amiens, France), OntoSpec aims at helping builders to model ontological knowledge (upstream of formal representation). The methodology relies on a set of rigorously-defined modelling primitives and principles. Its application leads to the elaboration of a semi-informal ontology, which is independent of knowledge representation languages. We recently enriched the OntoSpec methodology by endowing it with a new resource, the DOLCE top-level ontology defined at the LOA (IST-CNR, Trento, Italy). The goal of this integration is to provide modellers with additional help in structuring application ontologies, while maintaining independence vis-\\`{a}-vis formal representation languages. In this report, we first provide an overview of the OntoSpec methodology's general principles and then describe the DOLCE re-engineering process. A complete version of DOLCE-OS (i.e. a specification of DOLCE in the semi-informal OntoSpec language) is presented in an appendix.",
        "published": "2005-10-18T08:32:38Z",
        "link": "http://arxiv.org/abs/cs/0510050v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "First-Order Modeling and Stability Analysis of Illusory Contours",
        "authors": [
            "Yoon-Mo Jung",
            "Jianhong Shen"
        ],
        "summary": "In visual cognition, illusions help elucidate certain intriguing latent perceptual functions of the human vision system, and their proper mathematical modeling and computational simulation are therefore deeply beneficial to both biological and computer vision. Inspired by existent prior works, the current paper proposes a first-order energy-based model for analyzing and simulating illusory contours. The lower complexity of the proposed model facilitates rigorous mathematical analysis on the detailed geometric structures of illusory contours. After being asymptotically approximated by classical active contours, the proposed model is then robustly computed using the celebrated level-set method of Osher and Sethian (J. Comput. Phys., 79:12-49, 1988) with a natural supervising scheme. Potential cognitive implications of the mathematical results are addressed, and generic computational examples are demonstrated and discussed.",
        "published": "2005-10-19T20:44:04Z",
        "link": "http://arxiv.org/abs/cs/0510056v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "I.4.6; I.4.8"
        ]
    },
    {
        "title": "Using Interval Particle Filtering for Marker less 3D Human Motion   Capture",
        "authors": [
            "Jamal Saboune",
            "François Charpillet"
        ],
        "summary": "In this paper we present a new approach for marker less human motion capture from conventional camera feeds. The aim of our study is to recover 3D positions of key points of the body that can serve for gait analysis. Our approach is based on foreground segmentation, an articulated body model and particle filters. In order to be generic and simple no restrictive dynamic modelling was used. A new modified particle filtering algorithm was introduced. It is used efficiently to search the model configuration space. This new algorithm which we call Interval Particle Filtering reorganizes the configurations search space in an optimal deterministic way and proved to be efficient in tracking natural human movement. Results for human motion capture from a single camera are presented and compared to results obtained from a marker based system. The system proved to be able to track motion successfully even in partial occlusions.",
        "published": "2005-10-21T13:45:15Z",
        "link": "http://arxiv.org/abs/cs/0510062v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Markerless Human Motion Capture for Gait Analysis",
        "authors": [
            "Jamal Saboune",
            "François Charpillet"
        ],
        "summary": "The aim of our study is to detect balance disorders and a tendency towards the falls in the elderly, knowing gait parameters. In this paper we present a new tool for gait analysis based on markerless human motion capture, from camera feeds. The system introduced here, recovers the 3D positions of several key points of the human body while walking. Foreground segmentation, an articulated body model and particle filtering are basic elements of our approach. No dynamic model is used thus this system can be described as generic and simple to implement. A modified particle filtering algorithm, which we call Interval Particle Filtering, is used to reorganise and search through the model's configurations search space in a deterministic optimal way. This algorithm was able to perform human movement tracking with success. Results from the treatment of a single cam feeds are shown and compared to results obtained using a marker based human motion capture system.",
        "published": "2005-10-21T13:45:49Z",
        "link": "http://arxiv.org/abs/cs/0510063v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Applying Evolutionary Optimisation to Robot Obstacle Avoidance",
        "authors": [
            "Olivier Pauplin",
            "Jean Louchet",
            "Evelyne Lutton",
            "Michel Parent"
        ],
        "summary": "This paper presents an artificial evolutionbased method for stereo image analysis and its application to real-time obstacle detection and avoidance for a mobile robot. It uses the Parisian approach, which consists here in splitting the representation of the robot's environment into a large number of simple primitives, the \"flies\", which are evolved following a biologically inspired scheme and give a fast, low-cost solution to the obstacle detection problem in mobile robotics.",
        "published": "2005-10-25T07:07:01Z",
        "link": "http://arxiv.org/abs/cs/0510076v1",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Evidence with Uncertain Likelihoods",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "An agent often has a number of hypotheses, and must choose among them based on observations, or outcomes of experiments. Each of these observations can be viewed as providing evidence for or against various hypotheses. All the attempts to formalize this intuition up to now have assumed that associated with each hypothesis h there is a likelihood function \\mu_h, which is a probability measure that intuitively describes how likely each observation is, conditional on h being the correct hypothesis. We consider an extension of this framework where there is uncertainty as to which of a number of likelihood functions is appropriate, and discuss how one formal approach to defining evidence, which views evidence as a function from priors to posteriors, can be generalized to accommodate this uncertainty.",
        "published": "2005-10-25T21:15:31Z",
        "link": "http://arxiv.org/abs/cs/0510079v2",
        "categories": [
            "cs.AI",
            "I.2.3; G.3"
        ]
    },
    {
        "title": "When Ignorance is Bliss",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "summary": "It is commonly-accepted wisdom that more information is better, and that information should never be ignored. Here we argue, using both a Bayesian and a non-Bayesian analysis, that in some situations you are better off ignoring information if your uncertainty is represented by a set of probability measures. These include situations in which the information is relevant for the prediction task at hand. In the non-Bayesian analysis, we show how ignoring information avoids dilation, the phenomenon that additional pieces of information sometimes lead to an increase in uncertainty. In the Bayesian analysis, we show that for small sample sizes and certain prediction tasks, the Bayesian posterior based on a noninformative prior yields worse predictions than simply ignoring the given information.",
        "published": "2005-10-25T22:14:33Z",
        "link": "http://arxiv.org/abs/cs/0510080v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.4"
        ]
    },
    {
        "title": "Neuronal Spectral Analysis of EEG and Expert Knowledge Integration for   Automatic Classification of Sleep Stages",
        "authors": [
            "Nizar Kerkeni",
            "Frederic Alexandre",
            "Mohamed Hedi Bedoui",
            "Laurent Bougrain",
            "Mohamed Dogui"
        ],
        "summary": "Being able to analyze and interpret signal coming from electroencephalogram (EEG) recording can be of high interest for many applications including medical diagnosis and Brain-Computer Interfaces. Indeed, human experts are today able to extract from this signal many hints related to physiological as well as cognitive states of the recorded subject and it would be very interesting to perform such task automatically but today no completely automatic system exists. In previous studies, we have compared human expertise and automatic processing tools, including artificial neural networks (ANN), to better understand the competences of each and determine which are the difficult aspects to integrate in a fully automatic system. In this paper, we bring more elements to that study in reporting the main results of a practical experiment which was carried out in an hospital for sleep pathology study. An EEG recording was studied and labeled by a human expert and an ANN. We describe here the characteristics of the experiment, both human and neuronal procedure of analysis, compare their performances and point out the main limitations which arise from this study.",
        "published": "2005-10-26T14:47:07Z",
        "link": "http://arxiv.org/abs/cs/0510083v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "An efficient memetic, permutation-based evolutionary algorithm for   real-world train timetabling",
        "authors": [
            "Marc Schoenauer",
            "Yann Semet"
        ],
        "summary": "Train timetabling is a difficult and very tightly constrained combinatorial problem that deals with the construction of train schedules. We focus on the particular problem of local reconstruction of the schedule following a small perturbation, seeking minimisation of the total accumulated delay by adapting times of departure and arrival for each train and allocation of resources (tracks, routing nodes, etc.). We describe a permutation-based evolutionary algorithm that relies on a semi-greedy heuristic to gradually reconstruct the schedule by inserting trains one after the other following the permutation. This algorithm can be hybridised with ILOG commercial MIP programming tool CPLEX in a coarse-grained manner: the evolutionary part is used to quickly obtain a good but suboptimal solution and this intermediate solution is refined using CPLEX. Experimental results are presented on a large real-world case involving more than one million variables and 2 million constraints. Results are surprisingly good as the evolutionary algorithm, alone or hybridised, produces excellent solutions much faster than CPLEX alone.",
        "published": "2005-10-31T06:06:57Z",
        "link": "http://arxiv.org/abs/cs/0510091v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Evolutionary Computing",
        "authors": [
            "Aguston E. Eiben",
            "Marc Schoenauer"
        ],
        "summary": "Evolutionary computing (EC) is an exciting development in Computer Science. It amounts to building, applying and studying algorithms based on the Darwinian principles of natural selection. In this paper we briefly introduce the main concepts behind evolutionary computing. We present the main components all evolutionary algorithms (EA), sketch the differences between different types of EAs and survey application areas ranging from optimization, modeling and simulation to entertainment.",
        "published": "2005-11-01T19:46:18Z",
        "link": "http://arxiv.org/abs/cs/0511004v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "K-ANMI: A Mutual Information Based Clustering Algorithm for Categorical   Data",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "Clustering categorical data is an integral part of data mining and has attracted much attention recently. In this paper, we present k-ANMI, a new efficient algorithm for clustering categorical data. The k-ANMI algorithm works in a way that is similar to the popular k-means algorithm, and the goodness of clustering in each step is evaluated using a mutual information based criterion (namely, Average Normalized Mutual Information-ANMI) borrowed from cluster ensemble. Experimental results on real datasets show that k-ANMI algorithm is competitive with those state-of-art categorical data clustering algorithms with respect to clustering accuracy.",
        "published": "2005-11-03T01:18:47Z",
        "link": "http://arxiv.org/abs/cs/0511013v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Towards a Hierarchical Model of Consciousness, Intelligence, Mind and   Body",
        "authors": [
            "Prashant"
        ],
        "summary": "This article is taken out.",
        "published": "2005-11-03T16:28:05Z",
        "link": "http://arxiv.org/abs/cs/0511015v2",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Towards a unified theory of logic programming semantics: Level mapping   characterizations of selector generated models",
        "authors": [
            "Pascal Hitzler",
            "Sibylle Schwarz"
        ],
        "summary": "Currently, the variety of expressive extensions and different semantics created for logic programs with negation is diverse and heterogeneous, and there is a lack of comprehensive comparative studies which map out the multitude of perspectives in a uniform way. Most recently, however, new methodologies have been proposed which allow one to derive uniform characterizations of different declarative semantics for logic programs with negation. In this paper, we study the relationship between two of these approaches, namely the level mapping characterizations due to [Hitzler and Wendt 2005], and the selector generated models due to [Schwarz 2004]. We will show that the latter can be captured by means of the former, thereby supporting the claim that level mappings provide a very flexible framework which is applicable to very diversely defined semantics.",
        "published": "2005-11-09T14:22:55Z",
        "link": "http://arxiv.org/abs/cs/0511038v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Dimensions of Neural-symbolic Integration - A Structured Survey",
        "authors": [
            "Sebastian Bader",
            "Pascal Hitzler"
        ],
        "summary": "Research on integrated neural-symbolic systems has made significant progress in the recent past. In particular the understanding of ways to deal with symbolic knowledge within connectionist systems (also called artificial neural networks) has reached a critical mass which enables the community to strive for applicable implementations and use cases. Recent work has covered a great variety of logics used in artificial intelligence and provides a multitude of techniques for dealing with them within the context of artificial neural networks. We present a comprehensive survey of the field of neural-symbolic integration, including a new classification of system according to their architectures and abilities.",
        "published": "2005-11-10T10:00:46Z",
        "link": "http://arxiv.org/abs/cs/0511042v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.NE"
        ]
    },
    {
        "title": "Stochastic Process Semantics for Dynamical Grammar Syntax: An Overview",
        "authors": [
            "Eric Mjolsness"
        ],
        "summary": "We define a class of probabilistic models in terms of an operator algebra of stochastic processes, and a representation for this class in terms of stochastic parameterized grammars. A syntactic specification of a grammar is mapped to semantics given in terms of a ring of operators, so that grammatical composition corresponds to operator addition or multiplication. The operators are generators for the time-evolution of stochastic processes. Within this modeling framework one can express data clustering models, logic programs, ordinary and stochastic differential equations, graph grammars, and stochastic chemical reaction kinetics. This mathematical formulation connects these apparently distant fields to one another and to mathematical methods from quantum field theory and operator algebra.",
        "published": "2005-11-20T00:42:55Z",
        "link": "http://arxiv.org/abs/cs/0511073v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "nlin.AO",
            "D.3.1"
        ]
    },
    {
        "title": "Identifying Interaction Sites in \"Recalcitrant\" Proteins: Predicted   Protein and Rna Binding Sites in Rev Proteins of Hiv-1 and Eiav Agree with   Experimental Data",
        "authors": [
            "Michael Terribilini",
            "Jae-Hyung Lee",
            "Changhui Yan",
            "Robert L. Jernigan",
            "Susan Carpenter",
            "Vasant Honavar",
            "Drena Dobbs"
        ],
        "summary": "Protein-protein and protein nucleic acid interactions are vitally important for a wide range of biological processes, including regulation of gene expression, protein synthesis, and replication and assembly of many viruses. We have developed machine learning approaches for predicting which amino acids of a protein participate in its interactions with other proteins and/or nucleic acids, using only the protein sequence as input. In this paper, we describe an application of classifiers trained on datasets of well-characterized protein-protein and protein-RNA complexes for which experimental structures are available. We apply these classifiers to the problem of predicting protein and RNA binding sites in the sequence of a clinically important protein for which the structure is not known: the regulatory protein Rev, essential for the replication of HIV-1 and other lentiviruses. We compare our predictions with published biochemical, genetic and partial structural information for HIV-1 and EIAV Rev and with our own published experimental mapping of RNA binding sites in EIAV Rev. The predicted and experimentally determined binding sites are in very good agreement. The ability to predict reliably the residues of a protein that directly contribute to specific binding events - without the requirement for structural information regarding either the protein or complexes in which it participates - can potentially generate new disease intervention strategies.",
        "published": "2005-11-21T01:47:53Z",
        "link": "http://arxiv.org/abs/cs/0511075v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "J.3"
        ]
    },
    {
        "title": "Robust Inference of Trees",
        "authors": [
            "Marco Zaffalon",
            "Marcus Hutter"
        ],
        "summary": "This paper is concerned with the reliable inference of optimal tree-approximations to the dependency structure of an unknown distribution generating data. The traditional approach to the problem measures the dependency strength between random variables by the index called mutual information. In this paper reliability is achieved by Walley's imprecise Dirichlet model, which generalizes Bayesian learning with Dirichlet priors. Adopting the imprecise Dirichlet model results in posterior interval expectation for mutual information, and in a set of plausible trees consistent with the data. Reliable inference about the actual tree is achieved by focusing on the substructure common to all the plausible trees. We develop an exact algorithm that infers the substructure in time O(m^4), m being the number of random variables. The new algorithm is applied to a set of data sampled from a known distribution. The method is shown to reliably infer edges of the actual tree even when the data are very scarce, unlike the traditional approach. Finally, we provide lower and upper credibility limits for mutual information under the imprecise Dirichlet model. These enable the previous developments to be extended to a full inferential method for trees.",
        "published": "2005-11-25T10:59:35Z",
        "link": "http://arxiv.org/abs/cs/0511087v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Integration of Declarative and Constraint Programming",
        "authors": [
            "Petra Hofstedt",
            "Peter Pepper"
        ],
        "summary": "Combining a set of existing constraint solvers into an integrated system of cooperating solvers is a useful and economic principle to solve hybrid constraint problems. In this paper we show that this approach can also be used to integrate different language paradigms into a unified framework. Furthermore, we study the syntactic, semantic and operational impacts of this idea for the amalgamation of declarative and constraint programming.",
        "published": "2005-11-27T14:58:22Z",
        "link": "http://arxiv.org/abs/cs/0511090v2",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2"
        ]
    },
    {
        "title": "Evolution of Voronoi based Fuzzy Recurrent Controllers",
        "authors": [
            "Carlos Kavka",
            "Patricia Roggero",
            "Marc Schoenauer"
        ],
        "summary": "A fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules. Among the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi (RFV) model, a representation for recurrent fuzzy systems. It is an extension of the FV model proposed by Kavka and Schoenauer that extends the application domain to include temporal problems. The FV model is a representation for fuzzy controllers based on Voronoi diagrams that can represent fuzzy systems with synergistic rules, fulfilling the $\\epsilon$-completeness property and providing a simple way to introduce a priory knowledge. In the proposed representation, the temporal relations are embedded by including internal units that provide feedback by connecting outputs to inputs. These internal units act as memory elements. In the RFV model, the semantic of the internal units can be specified together with the a priori rules. The geometric interpretation of the rules allows the use of geometric variational operators during the evolution. The representation and the algorithms are validated in two problems in the area of system identification and evolutionary robotics.",
        "published": "2005-11-28T07:14:18Z",
        "link": "http://arxiv.org/abs/cs/0511091v1",
        "categories": [
            "cs.AI"
        ]
    },
    {
        "title": "Artificial Agents and Speculative Bubbles",
        "authors": [
            "Yann Semet",
            "Sylvain Gelly",
            "Marc Schoenauer",
            "Michèle Sebag"
        ],
        "summary": "Pertaining to Agent-based Computational Economics (ACE), this work presents two models for the rise and downfall of speculative bubbles through an exchange price fixing based on double auction mechanisms. The first model is based on a finite time horizon context, where the expected dividends decrease along time. The second model follows the {\\em greater fool} hypothesis; the agent behaviour depends on the comparison of the estimated risk with the greater fool's. Simulations shed some light on the influent parameters and the necessary conditions for the apparition of speculative bubbles in an asset market within the considered framework.",
        "published": "2005-11-28T13:29:38Z",
        "link": "http://arxiv.org/abs/cs/0511093v1",
        "categories": [
            "cs.GT",
            "cs.AI"
        ]
    },
    {
        "title": "Improving ecological niche models by data mining large environmental   datasets for surrogate models",
        "authors": [
            "David R. B. Stockwell"
        ],
        "summary": "WhyWhere is a new ecological niche modeling (ENM) algorithm for mapping and explaining the distribution of species. The algorithm uses image processing methods to efficiently sift through large amounts of data to find the few variables that best predict species occurrence. The purpose of this paper is to describe and justify the main parameterizations and to show preliminary success at rapidly providing accurate, scalable, and simple ENMs. Preliminary results for 6 species of plants and animals in different regions indicate a significant (p<0.01) 14% increase in accuracy over the GARP algorithm using models with few, typically two, variables. The increase is attributed to access to additional data, particularly monthly vs. annual climate averages. WhyWhere is also 6 times faster than GARP on large data sets. A data mining based approach with transparent access to remote data archives is a new paradigm for ENM, particularly suited to finding correlates in large databases of fine resolution surfaces. Software for WhyWhere is freely available, both as a service and in a desktop downloadable form from the web site http://biodi.sdsc.edu/ww_home.html.",
        "published": "2005-11-28T19:47:28Z",
        "link": "http://arxiv.org/abs/q-bio/0511046v1",
        "categories": [
            "q-bio.QM",
            "cs.AI"
        ]
    },
    {
        "title": "On Self-Regulated Swarms, Societal Memory, Speed and Dynamics",
        "authors": [
            "Vitorino Ramos",
            "Carlos Fernandes",
            "Agostinho C. Rosa"
        ],
        "summary": "We propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of our dynamic search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control problems, measuring reaction speeds and performance. Final comparisons were made with standard Genetic Algorithms (GAs), Bacterial Foraging strategies (BFOA), as well as with recent Co-Evolutionary approaches. SRS's were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found. One of the most interesting illustrate that the present SRS collective swarm of bio-inspired ant-like agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system.",
        "published": "2005-12-01T03:10:09Z",
        "link": "http://arxiv.org/abs/cs/0512002v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.11; G.1.6; I.2.9"
        ]
    },
    {
        "title": "Societal Implicit Memory and his Speed on Tracking Extrema over Dynamic   Environments using Self-Regulatory Swarms",
        "authors": [
            "Vitorino Ramos",
            "Carlos Fernandes",
            "Agostinho C. Rosa"
        ],
        "summary": "In order to overcome difficult dynamic optimization and environment extrema tracking problems, We propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of our dynamic search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control problems, measuring reaction speeds and performance. Final comparisons were made with standard Genetic Algorithms (GAs), Bacterial Foraging strategies (BFOA), as well as with recent Co-Evolutionary approaches. SRS's were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found. One of the most interesting illustrate that the present SRS collective swarm of bio-inspired ant-like agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system.",
        "published": "2005-12-01T04:09:22Z",
        "link": "http://arxiv.org/abs/cs/0512003v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11; G.1.6; I.2.9"
        ]
    },
    {
        "title": "Self-Regulated Artificial Ant Colonies on Digital Image Habitats",
        "authors": [
            "Carlos Fernandes",
            "Vitorino Ramos",
            "Agostinho C. Rosa"
        ],
        "summary": "Artificial life models, swarm intelligent and evolutionary computation algorithms are usually built on fixed size populations. Some studies indicate however that varying the population size can increase the adaptability of these systems and their capability to react to changing environments. In this paper we present an extended model of an artificial ant colony system designed to evolve on digital image habitats. We will show that the present swarm can adapt the size of the population according to the type of image on which it is evolving and reacting faster to changing images, thus converging more rapidly to the new desired regions, regulating the number of his image foraging agents. Finally, we will show evidences that the model can be associated with the Mathematical Morphology Watershed algorithm to improve the segmentation of digital grey-scale images. KEYWORDS: Swarm Intelligence, Perception and Image Processing, Pattern Recognition, Mathematical Morphology, Social Cognitive Maps, Social Foraging, Self-Organization, Distributed Search.",
        "published": "2005-12-01T04:39:30Z",
        "link": "http://arxiv.org/abs/cs/0512004v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11; I.2.9; I.3.3; I.4; I.4.10; I.4.6; I.4.9; I.5.4"
        ]
    },
    {
        "title": "A geometry of information, I: Nerves, posets and differential forms",
        "authors": [
            "Jonathan Gratus",
            "Timothy Porter"
        ],
        "summary": "The main theme of this workshop (Dagstuhl seminar 04351) is `Spatial Representation: Continuous vs. Discrete'. Spatial representation has two contrasting but interacting aspects (i) representation of spaces' and (ii) representation by spaces. In this paper, we will examine two aspects that are common to both interpretations of the theme, namely nerve constructions and refinement. Representations change, data changes, spaces change. We will examine the possibility of a `differential geometry' of spatial representations of both types, and in the sequel give an algebra of differential forms that has the potential to handle the dynamical aspect of such a geometry. We will discuss briefly a conjectured class of spaces, generalising the Cantor set which would seem ideal as a test-bed for the set of tools we are developing.",
        "published": "2005-12-02T16:18:11Z",
        "link": "http://arxiv.org/abs/cs/0512010v1",
        "categories": [
            "cs.AI",
            "cs.GR"
        ]
    },
    {
        "title": "Evolving Stochastic Learning Algorithm Based on Tsallis Entropic Index",
        "authors": [
            "Aristoklis D. Anastasiadis",
            "George D. Magoulas"
        ],
        "summary": "In this paper, inspired from our previous algorithm, which was based on the theory of Tsallis statistical mechanics, we develop a new evolving stochastic learning algorithm for neural networks. The new algorithm combines deterministic and stochastic search steps by employing a different adaptive stepsize for each network weight, and applies a form of noise that is characterized by the nonextensive entropic index q, regulated by a weight decay term. The behavior of the learning algorithm can be made more stochastic or deterministic depending on the trade off between the temperature T and the q values. This is achieved by introducing a formula that defines a time--dependent relationship between these two important learning parameters. Our experimental study verifies that there are indeed improvements in the convergence speed of this new evolving stochastic learning algorithm, which makes learning faster than using the original Hybrid Learning Scheme (HLS). In addition, experiments are conducted to explore the influence of the entropic index q and temperature T on the convergence speed and stability of the proposed method.",
        "published": "2005-12-09T20:30:02Z",
        "link": "http://arxiv.org/abs/cs/0512037v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Branch-and-Prune Search Strategies for Numerical Constraint Solving",
        "authors": [
            "Xuan-Ha Vu",
            "Marius-Calin Silaghi",
            "Djamila Sam-Haroud",
            "Boi Faltings"
        ],
        "summary": "When solving numerical constraints such as nonlinear equations and inequalities, solvers often exploit pruning techniques, which remove redundant value combinations from the domains of variables, at pruning steps. To find the complete solution set, most of these solvers alternate the pruning steps with branching steps, which split each problem into subproblems. This forms the so-called branch-and-prune framework, well known among the approaches for solving numerical constraints. The basic branch-and-prune search strategy that uses domain bisections in place of the branching steps is called the bisection search. In general, the bisection search works well in case (i) the solutions are isolated, but it can be improved further in case (ii) there are continuums of solutions (this often occurs when inequalities are involved). In this paper, we propose a new branch-and-prune search strategy along with several variants, which not only allow yielding better branching decisions in the latter case, but also work as well as the bisection search does in the former case. These new search algorithms enable us to employ various pruning techniques in the construction of inner and outer approximations of the solution set. Our experiments show that these algorithms speed up the solving process often by one order of magnitude or more when solving problems with continuums of solutions, while keeping the same performance as the bisection search when the solutions are isolated.",
        "published": "2005-12-11T19:47:42Z",
        "link": "http://arxiv.org/abs/cs/0512045v2",
        "categories": [
            "cs.AI",
            "F.4.1; I.2.8"
        ]
    },
    {
        "title": "Processing Uncertainty and Indeterminacy in Information Systems success   mapping",
        "authors": [
            "Jose L. Salmeron",
            "Florentin Smarandache"
        ],
        "summary": "IS success is a complex concept, and its evaluation is complicated, unstructured and not readily quantifiable. Numerous scientific publications address the issue of success in the IS field as well as in other fields. But, little efforts have been done for processing indeterminacy and uncertainty in success research. This paper shows a formal method for mapping success using Neutrosophic Success Map. This is an emerging tool for processing indeterminacy and uncertainty in success research. EIS success have been analyzed using this tool.",
        "published": "2005-12-13T01:21:58Z",
        "link": "http://arxiv.org/abs/cs/0512047v2",
        "categories": [
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "\"Going back to our roots\": second generation biocomputing",
        "authors": [
            "Jon Timmis",
            "Martyn Amos",
            "Wolfgang Banzhaf",
            "Andy Tyrrell"
        ],
        "summary": "Researchers in the field of biocomputing have, for many years, successfully \"harvested and exploited\" the natural world for inspiration in developing systems that are robust, adaptable and capable of generating novel and even \"creative\" solutions to human-defined problems. However, in this position paper we argue that the time has now come for a reassessment of how we exploit biology to generate new computational systems. Previous solutions (the \"first generation\" of biocomputing techniques), whilst reasonably effective, are crude analogues of actual biological systems. We believe that a new, inherently inter-disciplinary approach is needed for the development of the emerging \"second generation\" of bio-inspired methods. This new modus operandi will require much closer interaction between the engineering and life sciences communities, as well as a bidirectional flow of concepts, applications and expertise. We support our argument by examining, in this new light, three existing areas of biocomputing (genetic programming, artificial immune systems and evolvable hardware), as well as an emerging area (natural genetic engineering) which may provide useful pointers as to the way forward.",
        "published": "2005-12-16T16:42:25Z",
        "link": "http://arxiv.org/abs/cs/0512071v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Mathematical Models in Schema Theory",
        "authors": [
            "Mark Burgin"
        ],
        "summary": "In this paper, a mathematical schema theory is developed. This theory has three roots: brain theory schemas, grid automata, and block-shemas. In Section 2 of this paper, elements of the theory of grid automata necessary for the mathematical schema theory are presented. In Section 3, elements of brain theory necessary for the mathematical schema theory are presented. In Section 4, other types of schemas are considered. In Section 5, the mathematical schema theory is developed. The achieved level of schema representation allows one to model by mathematical tools virtually any type of schemas considered before, including schemas in neurophisiology, psychology, computer science, Internet technology, databases, logic, and mathematics.",
        "published": "2005-12-27T21:29:16Z",
        "link": "http://arxiv.org/abs/cs/0512099v1",
        "categories": [
            "cs.AI",
            "I.2.4"
        ]
    },
    {
        "title": "The logic of interactive Turing reduction",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "The paper gives a soundness and completeness proof for the implicative fragment of intuitionistic calculus with respect to the semantics of computability logic, which understands intuitionistic implication as interactive algorithmic reduction. This concept -- more precisely, the associated concept of reducibility -- is a generalization of Turing reducibility from the traditional, input/output sorts of problems to computational tasks of arbitrary degrees of interactivity. See http://www.cis.upenn.edu/~giorgi/cl.html for a comprehensive online source on computability logic.",
        "published": "2005-12-28T07:25:57Z",
        "link": "http://arxiv.org/abs/cs/0512100v4",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Combining Independent Modules in Lexical Multiple-Choice Problems",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman",
            "Jeffrey Bigham",
            "Victor Shnayder"
        ],
        "summary": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of multiple modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the familiar mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",
        "published": "2005-01-10T21:03:14Z",
        "link": "http://arxiv.org/abs/cs/0501018v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; J.5"
        ]
    },
    {
        "title": "An Empirical Study of MDL Model Selection with Infinite Parametric   Complexity",
        "authors": [
            "Steven de Rooij",
            "Peter Grunwald"
        ],
        "summary": "Parametric complexity is a central concept in MDL model selection. In practice it often turns out to be infinite, even for quite simple models such as the Poisson and Geometric families. In such cases, MDL model selection as based on NML and Bayesian inference based on Jeffreys' prior can not be used. Several ways to resolve this problem have been proposed. We conduct experiments to compare and evaluate their behaviour on small sample sizes.   We find interestingly poor behaviour for the plug-in predictive code; a restricted NML model performs quite well but it is questionable if the results validate its theoretical motivation. The Bayesian model with the improper Jeffreys' prior is the most dependable.",
        "published": "2005-01-14T15:50:28Z",
        "link": "http://arxiv.org/abs/cs/0501028v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "E.3; G.4"
        ]
    },
    {
        "title": "Bandit Problems with Side Observations",
        "authors": [
            "Chih-Chun Wang",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "An extension of the traditional two-armed bandit problem is considered, in which the decision maker has access to some side information before deciding which arm to pull. At each time t, before making a selection, the decision maker is able to observe a random variable X_t that provides some information on the rewards to be obtained. The focus is on finding uniformly good rules (that minimize the growth rate of the inferior sampling time) and on quantifying how much the additional information helps. Various settings are considered and for each setting, lower bounds on the achievable inferior sampling time are developed and asymptotically optimal adaptive schemes achieving these lower bounds are constructed.",
        "published": "2005-01-22T22:07:18Z",
        "link": "http://arxiv.org/abs/cs/0501063v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Asymptotic Log-loss of Prequential Maximum Likelihood Codes",
        "authors": [
            "Peter Grunwald",
            "Steven de Rooij"
        ],
        "summary": "We analyze the Dawid-Rissanen prequential maximum likelihood codes relative to one-parameter exponential family models M. If data are i.i.d. according to an (essentially) arbitrary P, then the redundancy grows at rate c/2 ln n. We show that c=v1/v2, where v1 is the variance of P, and v2 is the variance of the distribution m* in M that is closest to P in KL divergence. This shows that prequential codes behave quite differently from other important universal codes such as the 2-part MDL, Shtarkov and Bayes codes, for which c=1. This behavior is undesirable in an MDL model selection setting.",
        "published": "2005-02-01T13:42:49Z",
        "link": "http://arxiv.org/abs/cs/0502004v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Stability Analysis for Regularized Least Squares Regression",
        "authors": [
            "Cynthia Rudin"
        ],
        "summary": "We discuss stability for a class of learning algorithms with respect to noisy labels. The algorithms we consider are for regression, and they involve the minimization of regularized risk functionals, such as L(f) := 1/N sum_i (f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when y_i is a noisy version of f*(x_i) for some function f* in H, the output of the algorithm converges to f* as the regularization term and noise simultaneously vanish. We consider two flavors of this problem, one where a data set of N points remains fixed, and the other where N -> infinity. For the case where N -> infinity, we give conditions for convergence to f_E (the function which is the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we describe the limiting 'non-noisy', 'non-regularized' function f*, and give conditions for convergence. In the process, we develop a set of tools for dealing with functionals such as L(f), which are applicable to many other problems in learning theory.",
        "published": "2005-02-03T19:54:02Z",
        "link": "http://arxiv.org/abs/cs/0502016v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Estimating mutual information and multi--information in large networks",
        "authors": [
            "Noam Slonim",
            "Gurinder S. Atwal",
            "Gasper Tkacik",
            "William Bialek"
        ],
        "summary": "We address the practical problems of estimating the information relations that characterize large networks. Building on methods developed for analysis of the neural code, we show that reliable estimates of mutual information can be obtained with manageable computational effort. The same methods allow estimation of higher order, multi--information terms. These ideas are illustrated by analyses of gene expression, financial markets, and consumer preferences. In each case, information theoretic measures correlate with independent, intuitive measures of the underlying structures in the system.",
        "published": "2005-02-03T21:11:54Z",
        "link": "http://arxiv.org/abs/cs/0502017v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Master Algorithms for Active Experts Problems based on Increasing Loss   Values",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain master algorithms for \"active experts problems\", which means that the master's actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. This results in a (computationally infeasible) universal master algorithm which performs - in a certain sense - almost as well as any computable strategy, for any online problem.",
        "published": "2005-02-15T14:59:49Z",
        "link": "http://arxiv.org/abs/cs/0502067v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Strong Asymptotic Assertions for Discrete MDL in Regression and   Classification",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We study the properties of the MDL (or maximum penalized complexity) estimator for Regression and Classification, where the underlying model class is countable. We show in particular a finite bound on the Hellinger losses under the only assumption that there is a \"true\" model contained in the class. This implies almost sure convergence of the predictive distribution to the true one at a fast rate. It corresponds to Solomonoff's central theorem of universal induction, however with a bound that is exponentially larger.",
        "published": "2005-02-15T16:26:36Z",
        "link": "http://arxiv.org/abs/math/0502315v1",
        "categories": [
            "math.ST",
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.PR",
            "stat.TH"
        ]
    },
    {
        "title": "On sample complexity for computational pattern recognition",
        "authors": [
            "Daniil Ryabko"
        ],
        "summary": "In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if we restrict our attention to computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples.   The task of pattern recognition is considered in conjunction with another learning problem -- data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.",
        "published": "2005-02-17T14:58:28Z",
        "link": "http://arxiv.org/abs/cs/0502074v2",
        "categories": [
            "cs.LG",
            "cs.CC"
        ]
    },
    {
        "title": "Learning nonsingular phylogenies and hidden Markov models",
        "authors": [
            "Elchanan Mossel",
            "Sébastien Roch"
        ],
        "summary": "In this paper we study the problem of learning phylogenies and hidden Markov models. We call a Markov model nonsingular if all transition matrices have determinants bounded away from 0 (and 1). We highlight the role of the nonsingularity condition for the learning problem. Learning hidden Markov models without the nonsingularity condition is at least as hard as learning parity with noise, a well-known learning problem conjectured to be computationally hard. On the other hand, we give a polynomial-time algorithm for learning nonsingular phylogenies and hidden Markov models.",
        "published": "2005-02-18T01:31:53Z",
        "link": "http://arxiv.org/abs/cs/0502076v2",
        "categories": [
            "cs.LG",
            "cs.CE",
            "math.PR",
            "math.ST",
            "q-bio.PE",
            "stat.TH",
            "60J10, 60J20, 68T05, 92B10 (Primary)"
        ]
    },
    {
        "title": "The Self-Organization of Speech Sounds",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "summary": "The speech code is a vehicle of language: it defines a set of forms used by a community to carry information. Such a code is necessary to support the linguistic interactions that allow humans to communicate. How then may a speech code be formed prior to the existence of linguistic interactions? Moreover, the human speech code is discrete and compositional, shared by all the individuals of a community but different across communities, and phoneme inventories are characterized by statistical regularities. How can a speech code with these properties form? We try to approach these questions in the paper, using the \"methodology of the artificial\". We build a society of artificial agents, and detail a mechanism that shows the formation of a discrete speech code without pre-supposing the existence of linguistic capacities or of coordinated interactions. The mechanism is based on a low-level model of sensory-motor interactions. We show that the integration of certain very simple and non language-specific neural devices leads to the formation of a speech code that has properties similar to the human speech code. This result relies on the self-organizing properties of a generic coupling between perception and production within agents, and on the interactions between agents. The artificial system helps us to develop better intuitions on how speech might have appeared, by showing how self-organization might have helped natural selection to find speech.",
        "published": "2005-02-22T09:51:16Z",
        "link": "http://arxiv.org/abs/cs/0502086v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE",
            "cs.RO",
            "math.DS"
        ]
    },
    {
        "title": "On Generalized Computable Universal Priors and their Convergence",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff unified Occam's razor and Epicurus' principle of multiple explanations to one elegant, formal, universal theory of inductive inference, which initiated the field of algorithmic information theory. His central result is that the posterior of the universal semimeasure M converges rapidly to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal predictor in case of unknown mu. The first part of the paper investigates the existence and convergence of computable universal (semi)measures for a hierarchy of computability classes: recursive, estimable, enumerable, and approximable. For instance, M is known to be enumerable, but not estimable, and to dominate all enumerable semimeasures. We present proofs for discrete and continuous semimeasures. The second part investigates more closely the types of convergence, possibly implied by universality: in difference and in ratio, with probability 1, in mean sum, and for Martin-Loef random sequences. We introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues. In particular, we show that convergence fails (holds) on generalized-random sequences in gappy (dense) Bernoulli classes.",
        "published": "2005-03-11T12:38:30Z",
        "link": "http://arxiv.org/abs/cs/0503026v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "math.PR",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Consistency in Models for Distributed Learning under Communication   Constraints",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "Motivated by sensor networks and other distributed settings, several models for distributed learning are presented. The models differ from classical works in statistical pattern recognition by allocating observations of an independent and identically distributed (i.i.d.) sampling process amongst members of a network of simple learning agents. The agents are limited in their ability to communicate to a central fusion center and thus, the amount of information available for use in classification or regression is constrained. For several basic communication models in both the binary classification and regression frameworks, we question the existence of agent decision rules and fusion rules that result in a universally consistent ensemble. The answers to this question present new issues to consider with regard to universal consistency. Insofar as these models present a useful picture of distributed scenarios, this paper addresses the issue of whether or not the guarantees provided by Stone's Theorem in centralized environments hold in distributed settings.",
        "published": "2005-03-26T05:13:51Z",
        "link": "http://arxiv.org/abs/cs/0503071v2",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "G.3; I.2.6; I.2.11; I.5.1"
        ]
    },
    {
        "title": "Distributed Learning in Wireless Sensor Networks",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "The problem of distributed or decentralized detection and estimation in applications such as wireless sensor networks has often been considered in the framework of parametric models, in which strong assumptions are made about a statistical description of nature. In certain applications, such assumptions are warranted and systems designed from these models show promise. However, in other scenarios, prior knowledge is at best vague and translating such knowledge into a statistical model is undesirable. Applications such as these pave the way for a nonparametric study of distributed detection and estimation. In this paper, we review recent work of the authors in which some elementary models for distributed learning are considered. These models are in the spirit of classical work in nonparametric statistics and are applicable to wireless sensor networks.",
        "published": "2005-03-26T05:42:06Z",
        "link": "http://arxiv.org/abs/cs/0503072v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "C.2.1; I.2.6; G3"
        ]
    },
    {
        "title": "Probabilistic and Team PFIN-type Learning: General Properties",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We consider the probability hierarchy for Popperian FINite learning and study the general properties of this hierarchy. We prove that the probability hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and p_2 and answers whether PFIN-type learning with the probability of success p_1 is equivalent to PFIN-type learning with the probability of success p_2.   To prove our result, we analyze the topological structure of the probability hierarchy. We prove that it is well-ordered in descending ordering and order-equivalent to ordinal epsilon_0. This shows that the structure of the hierarchy is very complicated.   Using similar methods, we also prove that, for PFIN-type learning, team learning and probabilistic learning are of the same power.",
        "published": "2005-03-31T23:04:28Z",
        "link": "http://arxiv.org/abs/cs/0504001v1",
        "categories": [
            "cs.LG",
            "F.1.1, I.2.6"
        ]
    },
    {
        "title": "The Bayesian Decision Tree Technique with a Sweeping Strategy",
        "authors": [
            "V. Schetinin",
            "J. E. Fieldsend",
            "D. Partridge",
            "W. J. Krzanowski",
            "R. M. Everson",
            "T. C. Bailey",
            "A. Hernandez"
        ],
        "summary": "The uncertainty of classification outcomes is of crucial importance for many safety critical applications including, for example, medical diagnostics. In such applications the uncertainty of classification can be reliably estimated within a Bayesian model averaging technique that allows the use of prior information. Decision Tree (DT) classification models used within such a technique gives experts additional information by making this classification scheme observable. The use of the Markov Chain Monte Carlo (MCMC) methodology of stochastic sampling makes the Bayesian DT technique feasible to perform. However, in practice, the MCMC technique may become stuck in a particular DT which is far away from a region with a maximal posterior. Sampling such DTs causes bias in the posterior estimates, and as a result the evaluation of classification uncertainty may be incorrect. In a particular case, the negative effect of such sampling may be reduced by giving additional prior information on the shape of DTs. In this paper we describe a new approach based on sweeping the DTs without additional priors on the favorite shape of DTs. The performances of Bayesian DT techniques with the standard and sweeping strategies are compared on a synthetic data as well as on real datasets. Quantitatively evaluating the uncertainty in terms of entropy of class posterior probabilities, we found that the sweeping strategy is superior to the standard strategy.",
        "published": "2005-04-11T17:45:09Z",
        "link": "http://arxiv.org/abs/cs/0504042v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Experimental Comparison of Classification Uncertainty for Randomised and   Bayesian Decision Tree Ensembles",
        "authors": [
            "V. Schetinin",
            "D. Partridge",
            "W. J. Krzanowski",
            "R. M. Everson",
            "J. E. Fieldsend",
            "T. C. Bailey",
            "A. Hernandez"
        ],
        "summary": "In this paper we experimentally compare the classification uncertainty of the randomised Decision Tree (DT) ensemble technique and the Bayesian DT technique with a restarting strategy on a synthetic dataset as well as on some datasets commonly used in the machine learning community. For quantitative evaluation of classification uncertainty, we use an Uncertainty Envelope dealing with the class posterior distribution and a given confidence probability. Counting the classifier outcomes, this technique produces feasible evaluations of the classification uncertainty. Using this technique in our experiments, we found that the Bayesian DT technique is superior to the randomised DT ensemble technique.",
        "published": "2005-04-11T17:53:35Z",
        "link": "http://arxiv.org/abs/cs/0504043v1",
        "categories": [
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Learning Multi-Class Neural-Network Models from Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult",
            "Burkhart Scheidt",
            "Valery Kuriakin"
        ],
        "summary": "We describe a new algorithm for learning multi-class neural-network models from large-scale clinical electroencephalograms (EEGs). This algorithm trains hidden neurons separately to classify all the pairs of classes. To find best pairwise classifiers, our algorithm searches for input variables which are relevant to the classification problem. Despite patient variability and heavily overlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns correctly classified 80.8% of the training and 80.1% of the testing examples. Additionally, the neural-network model provides a probabilistic interpretation of decisions.",
        "published": "2005-04-13T13:22:49Z",
        "link": "http://arxiv.org/abs/cs/0504052v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Learning from Web: Review of Approaches",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "Knowledge discovery is defined as non-trivial extraction of implicit, previously unknown and potentially useful information from given data. Knowledge extraction from web documents deals with unstructured, free-format documents whose number is enormous and rapidly growing. The artificial neural networks are well suitable to solve a problem of knowledge discovery from web documents because trained networks are able more accurately and easily to classify the learning and testing examples those represent the text mining domain. However, the neural networks that consist of large number of weighted connections and activation units often generate the incomprehensible and hard-to-understand models of text classification. This problem may be also addressed to most powerful recurrent neural networks that employ the feedback links from hidden or output units to their input units. Due to feedback links, recurrent neural networks are able take into account of a context in document. To be useful for data mining, self-organizing neural network techniques of knowledge extraction have been explored and developed. Self-organization principles were used to create an adequate neural-network structure and reduce a dimensionality of features used to describe text documents. The use of these principles seems interesting because ones are able to reduce a neural-network redundancy and considerably facilitate the knowledge representation.",
        "published": "2005-04-13T13:40:38Z",
        "link": "http://arxiv.org/abs/cs/0504054v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Selection in Scale-Free Small World",
        "authors": [
            "Zs. Palotai",
            "Cs. Farkas",
            "A. Lorincz"
        ],
        "summary": "In this paper we compare the performance characteristics of our selection based learning algorithm for Web crawlers with the characteristics of the reinforcement learning algorithm. The task of the crawlers is to find new information on the Web. The selection algorithm, called weblog update, modifies the starting URL lists of our crawlers based on the found URLs containing new information. The reinforcement learning algorithm modifies the URL orderings of the crawlers based on the received reinforcements for submitted documents. We performed simulations based on data collected from the Web. The collected portion of the Web is typical and exhibits scale-free small world (SFSW) structure. We have found that on this SFSW, the weblog update algorithm performs better than the reinforcement learning algorithm. It finds the new information faster than the reinforcement learning algorithm and has better new information/all submitted documents ratio. We believe that the advantages of the selection algorithm over reinforcement learning algorithm is due to the small world property of the Web.",
        "published": "2005-04-14T07:57:01Z",
        "link": "http://arxiv.org/abs/cs/0504063v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "A Neural-Network Technique to Learn Concepts from Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "A new technique is presented developed to learn multi-class concepts from clinical electroencephalograms. A desired concept is represented as a neuronal computational model consisting of the input, hidden, and output neurons. In this model the hidden neurons learn independently to classify the electroencephalogram segments presented by spectral and statistical features. This technique has been applied to the electroencephalogram data recorded from 65 sleeping healthy newborns in order to learn a brain maturation concept of newborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these data have been used for learning and testing the concept, respectively. As a result, the concept has correctly classified 80.1% of the testing segments or 87.7% of the 65 records.",
        "published": "2005-04-14T10:47:38Z",
        "link": "http://arxiv.org/abs/cs/0504069v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "The Combined Technique for Detection of Artifacts in Clinical   Electroencephalograms of Sleeping Newborns",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "In this paper we describe a new method combining the polynomial neural network and decision tree techniques in order to derive comprehensible classification rules from clinical electroencephalograms (EEGs) recorded from sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement, muscle and noise artifacts and as a consequence some EEG features are irrelevant to classification problems. Combining the polynomial network and decision tree techniques, we discover comprehensible classification rules whilst also attempting to keep their classification error down. This technique is shown to outperform a number of commonly used machine learning technique applied to automatically recognize artifacts in the sleep EEGs.",
        "published": "2005-04-14T10:49:55Z",
        "link": "http://arxiv.org/abs/cs/0504070v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "Adaptive Online Prediction by Following the Perturbed Leader",
        "authors": [
            "Marcus Hutter",
            "Jan Poland"
        ],
        "summary": "When applying aggregating strategies to Prediction with Expert Advice, the learning rate must be adaptively tuned. The natural choice of sqrt(complexity/current loss) renders the analysis of Weighted Majority derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative \"Follow the Perturbed Leader\" (FPL) algorithm from Kalai & Vempala (2003) (based on Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are new.",
        "published": "2005-04-16T16:48:49Z",
        "link": "http://arxiv.org/abs/cs/0504078v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Componentwise Least Squares Support Vector Machines",
        "authors": [
            "Kristiaan Pelckmans",
            "Ivan Goethals",
            "Jos De Brabanter",
            "Johan A. K. Suykens",
            "Bart De Moor"
        ],
        "summary": "This chapter describes componentwise Least Squares Support Vector Machines (LS-SVMs) for the estimation of additive models consisting of a sum of nonlinear components. The primal-dual derivations characterizing LS-SVMs for the estimation of the additive model result in a single set of linear equations with size growing in the number of data-points. The derivation is elaborated for the classification as well as the regression case. Furthermore, different techniques are proposed to discover structure in the data by looking for sparse components in the model based on dedicated regularization schemes on the one hand and fusion of the componentwise LS-SVMs training with a validation criterion on the other hand. (keywords: LS-SVMs, additive models, regularization, structure detection)",
        "published": "2005-04-19T15:01:25Z",
        "link": "http://arxiv.org/abs/cs/0504086v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6"
        ]
    },
    {
        "title": "A linear memory algorithm for Baum-Welch training",
        "authors": [
            "Istvan Miklos",
            "Irmtraud M. Meyer"
        ],
        "summary": "Background: Baum-Welch training is an expectation-maximisation algorithm for training the emission and transition probabilities of hidden Markov models in a fully automated way.   Methods and results: We introduce a linear space algorithm for Baum-Welch training. For a hidden Markov model with M states, T free transition and E free emission parameters, and an input sequence of length L, our new algorithm requires O(M) memory and O(L M T_max (T + E)) time for one Baum-Welch iteration, where T_max is the maximum number of states that any state is connected to. The most memory efficient algorithm until now was the checkpointing algorithm with O(log(L) M) memory and O(log(L) L M T_max) time requirement. Our novel algorithm thus renders the memory requirement completely independent of the length of the training sequences. More generally, for an n-hidden Markov model and n input sequences of length L, the memory requirement of O(log(L) L^(n-1) M) is reduced to O(L^(n-1) M) memory while the running time is changed from O(log(L) L^n M T_max + L^n (T + E)) to O(L^n M T_max (T + E)).   Conclusions: For the large class of hidden Markov models used for example in gene prediction, whose number of states does not scale with the length of the input sequence, our novel algorithm can thus be both faster and more memory-efficient than any of the existing algorithms.",
        "published": "2005-05-11T16:45:58Z",
        "link": "http://arxiv.org/abs/cs/0505028v3",
        "categories": [
            "cs.LG",
            "cs.DS",
            "q-bio.QM",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Multi-Modal Human-Machine Communication for Instructing Robot Grasping   Tasks",
        "authors": [
            "P. C. McGuire",
            "J. Fritsch",
            "J. J. Steil",
            "F. Roethling",
            "G. A. Fink",
            "S. Wachsmuth",
            "G. Sagerer",
            "H. Ritter"
        ],
        "summary": "A major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One way of such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable to establish a common focus of attention and be able to use and integrate spoken instructions, visual perceptions, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and an modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects.",
        "published": "2005-05-24T14:53:49Z",
        "link": "http://arxiv.org/abs/cs/0505064v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "H.1.2; I.2.9; I.2.10; I.2.7; H.5.2; H.5.1; I.2.6; I.4.8; I.4.7;\n  I.4.6"
        ]
    },
    {
        "title": "Defensive forecasting",
        "authors": [
            "Vladimir Vovk",
            "Akimichi Takemura",
            "Glenn Shafer"
        ],
        "summary": "We consider how to make probability forecasts of binary labels. Our main mathematical result is that for any continuous gambling strategy used for detecting disagreement between the forecasts and the actual labels, there exists a forecasting strategy whose forecasts are ideal as far as this gambling strategy is concerned. A forecasting strategy obtained in this way from a gambling strategy demonstrating a strong law of large numbers is simplified and studied empirically.",
        "published": "2005-05-30T21:12:00Z",
        "link": "http://arxiv.org/abs/cs/0505083v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Non-asymptotic calibration and resolution",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "We analyze a new algorithm for probability forecasting of binary observations on the basis of the available data, without making any assumptions about the way the observations are generated. The algorithm is shown to be well calibrated and to have good resolution for long enough sequences of observations and for a suitable choice of its parameter, a kernel on the Cartesian product of the forecast space $[0,1]$ and the data space. Our main results are non-asymptotic: we establish explicit inequalities, shown to be tight, for the performance of the algorithm.",
        "published": "2005-06-01T14:03:20Z",
        "link": "http://arxiv.org/abs/cs/0506004v4",
        "categories": [
            "cs.LG",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Defensive forecasting for linear protocols",
        "authors": [
            "Vladimir Vovk",
            "Ilia Nouretdinov",
            "Akimichi Takemura",
            "Glenn Shafer"
        ],
        "summary": "We consider a general class of forecasting protocols, called \"linear protocols\", and discuss several important special cases, including multi-class forecasting. Forecasting is formalized as a game between three players: Reality, whose role is to generate observations; Forecaster, whose goal is to predict the observations; and Skeptic, who tries to make money on any lack of agreement between Forecaster's predictions and the actual observations. Our main mathematical result is that for any continuous strategy for Skeptic in a linear protocol there exists a strategy for Forecaster that does not allow Skeptic's capital to grow. This result is a meta-theorem that allows one to transform any continuous law of probability in a linear protocol into a forecasting strategy whose predictions are guaranteed to satisfy this law. We apply this meta-theorem to a weak law of large numbers in Hilbert spaces to obtain a version of the K29 prediction algorithm for linear protocols and show that this version also satisfies the attractive properties of proper calibration and resolution under a suitable choice of its kernel parameter, with no assumptions about the way the data is generated.",
        "published": "2005-06-02T13:26:43Z",
        "link": "http://arxiv.org/abs/cs/0506007v2",
        "categories": [
            "cs.LG",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "Asymptotics of Discrete MDL for Online Prediction",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "Minimum Description Length (MDL) is an important principle for induction and prediction, with strong relations to optimal Bayesian learning. This paper deals with learning non-i.i.d. processes by means of two-part MDL, where the underlying model class is countable. We consider the online learning framework, i.e. observations come in one by one, and the predictor is allowed to update his state of mind after each time step. We identify two ways of predicting by MDL for this setup, namely a static} and a dynamic one. (A third variant, hybrid MDL, will turn out inferior.) We will prove that under the only assumption that the data is generated by a distribution contained in the model class, the MDL predictions converge to the true values almost surely. This is accomplished by proving finite bounds on the quadratic, the Hellinger, and the Kullback-Leibler loss of the MDL learner, which are however exponentially worse than for Bayesian prediction. We demonstrate that these bounds are sharp, even for model classes containing only Bernoulli distributions. We show how these bounds imply regret bounds for arbitrary loss functions. Our results apply to a wide range of setups, namely sequence prediction, pattern classification, regression, and universal induction in the sense of Algorithmic Information Theory among others.",
        "published": "2005-06-08T09:07:23Z",
        "link": "http://arxiv.org/abs/cs/0506022v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Competitive on-line learning with a convex loss function",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "We consider the problem of sequential decision making under uncertainty in which the loss caused by a decision depends on the following binary observation. In competitive on-line learning, the goal is to design decision algorithms that are almost as good as the best decision rules in a wide benchmark class, without making any assumptions about the way the observations are generated. However, standard algorithms in this area can only deal with finite-dimensional (often countable) benchmark classes. In this paper we give similar results for decision rules ranging over an arbitrary reproducing kernel Hilbert space. For example, it is shown that for a wide class of loss functions (including the standard square, absolute, and log loss functions) the average loss of the master algorithm, over the first $N$ observations, does not exceed the average loss of the best decision rule with a bounded norm plus $O(N^{-1/2})$. Our proof technique is very different from the standard ones and is based on recent results about defensive forecasting. Given the probabilities produced by a defensive forecasting algorithm, which are known to be well calibrated and to have good resolution in the long run, we use the expected loss minimization principle to find a suitable decision.",
        "published": "2005-06-11T18:11:22Z",
        "link": "http://arxiv.org/abs/cs/0506041v3",
        "categories": [
            "cs.LG",
            "cs.AI",
            "I.2.6; I.5.1"
        ]
    },
    {
        "title": "About one 3-parameter Model of Testing",
        "authors": [
            "Kromer Victor"
        ],
        "summary": "This article offers a 3-parameter model of testing, with 1) the difference between the ability level of the examinee and item difficulty; 2) the examinee discrimination and 3) the item discrimination as model parameters.",
        "published": "2005-06-14T04:00:38Z",
        "link": "http://arxiv.org/abs/cs/0506057v2",
        "categories": [
            "cs.LG",
            "I.2.6; K.3.2"
        ]
    },
    {
        "title": "Seeing stars: Exploiting class relationships for sentiment   categorization with respect to rating scales",
        "authors": [
            "Bo Pang",
            "Lillian Lee"
        ],
        "summary": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\". We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.",
        "published": "2005-06-17T20:10:43Z",
        "link": "http://arxiv.org/abs/cs/0506075v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "On the Job Training",
        "authors": [
            "Jason E. Holt"
        ],
        "summary": "We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle changes in system construction can significantly affect agent performance.",
        "published": "2005-06-22T21:21:13Z",
        "link": "http://arxiv.org/abs/cs/0506085v1",
        "categories": [
            "cs.LG",
            "K.3.2"
        ]
    },
    {
        "title": "Deriving a Stationary Dynamic Bayesian Network from a Logic Program with   Recursive Loops",
        "authors": [
            "Y. D. Shen",
            "Q. Yang",
            "J. H. You",
            "L. Y. Yuan"
        ],
        "summary": "Recursive loops in a logic program present a challenging problem to the PLP framework. On the one hand, they loop forever so that the PLP backward-chaining inferences would never stop. On the other hand, they generate cyclic influences, which are disallowed in Bayesian networks. Therefore, in existing PLP approaches logic programs with recursive loops are considered to be problematic and thus are excluded. In this paper, we propose an approach that makes use of recursive loops to build a stationary dynamic Bayesian network. Our work stems from an observation that recursive loops in a logic program imply a time sequence and thus can be used to model a stationary dynamic Bayesian network without using explicit time parameters. We introduce a Bayesian knowledge base with logic clauses of the form $A \\leftarrow A_1,...,A_l, true, Context, Types$, which naturally represents the knowledge that the $A_i$s have direct influences on $A$ in the context $Context$ under the type constraints $Types$. We then use the well-founded model of a logic program to define the direct influence relation and apply SLG-resolution to compute the space of random variables together with their parental connections. We introduce a novel notion of influence clauses, based on which a declarative semantics for a Bayesian knowledge base is established and algorithms for building a two-slice dynamic Bayesian network from a logic program are developed.",
        "published": "2005-06-27T04:07:34Z",
        "link": "http://arxiv.org/abs/cs/0506095v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.LO"
        ]
    },
    {
        "title": "Efficient Multiclass Implementations of L1-Regularized Maximum Entropy",
        "authors": [
            "Patrick Haffner",
            "Steven Phillips",
            "Rob Schapire"
        ],
        "summary": "This paper discusses the application of L1-regularized maximum entropy modeling or SL1-Max [9] to multiclass categorization problems. A new modification to the SL1-Max fast sequential learning algorithm is proposed to handle conditional distributions. Furthermore, unlike most previous studies, the present research goes beyond a single type of conditional distribution. It describes and compares a variety of modeling assumptions about the class distribution (independent or exclusive) and various types of joint or conditional distributions. It results in a new methodology for combining binary regularized classifiers to achieve multiclass categorization. In this context, Maximum Entropy can be considered as a generic and efficient regularized classification tool that matches or outperforms the state-of-the art represented by AdaBoost and SVMs.",
        "published": "2005-06-29T20:26:33Z",
        "link": "http://arxiv.org/abs/cs/0506101v1",
        "categories": [
            "cs.LG",
            "cs.CL"
        ]
    },
    {
        "title": "Multiresolution Kernels",
        "authors": [
            "Marco Cuturi",
            "Kenji Fukumizu"
        ],
        "summary": "We present in this work a new methodology to design kernels on data which is structured with smaller components, such as text, images or sequences. This methodology is a template procedure which can be applied on most kernels on measures and takes advantage of a more detailed \"bag of components\" representation of the objects. To obtain such a detailed description, we consider possible decompositions of the original bag into a collection of nested bags, following a prior knowledge on the objects' structure. We then consider these smaller bags to compare two objects both in a detailed perspective, stressing local matches between the smaller bags, and in a global or coarse perspective, by considering the entire bag. This multiresolution approach is likely to be best suited for tasks where the coarse approach is not precise enough, and where a more subtle mixture of both local and global similarities is necessary to compare objects. The approach presented here would not be computationally tractable without a factorization trick that we introduce before presenting promising results on an image retrieval task.",
        "published": "2005-07-13T05:45:28Z",
        "link": "http://arxiv.org/abs/cs/0507033v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Distributed Regression in Sensor Networks: Training Distributively with   Alternating Projections",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "Wireless sensor networks (WSNs) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing. The problem of distributed or decentralized estimation has often been considered in the context of parametric models. However, the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models. In this paper, a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of WSN applications including field estimation. Here, starting with the standard regularized kernel least-squares estimator, a message-passing algorithm for distributed estimation in WSNs is derived. The algorithm can be viewed as an instantiation of the successive orthogonal projection (SOP) algorithm. Various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach.",
        "published": "2005-07-18T00:45:12Z",
        "link": "http://arxiv.org/abs/cs/0507039v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Pattern Recognition for Conditionally Independent Data",
        "authors": [
            "Daniil Ryabko"
        ],
        "summary": "In this work we consider the task of relaxing the i.i.d assumption in pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.   We find a broad class of learning algorithms for which estimations of the probability of a classification error achieved under the classical i.i.d. assumption can be generalised to the similar estimates for the case of conditionally i.i.d. examples.",
        "published": "2005-07-18T08:10:10Z",
        "link": "http://arxiv.org/abs/cs/0507040v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ]
    },
    {
        "title": "Monotone Conditional Complexity Bounds on Future Prediction Errors",
        "authors": [
            "Alexey Chernov",
            "Marcus Hutter"
        ],
        "summary": "We bound the future loss when predicting any (computably) stochastic sequence online. Solomonoff finitely bounded the total deviation of his universal predictor M from the true distribution m by the algorithmic complexity of m. Here we assume we are at a time t>1 and already observed x=x_1...x_t. We bound the future prediction performance on x_{t+1}x_{t+2}... by a new variant of algorithmic complexity of m given x, plus the complexity of the randomness deficiency of x. The new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged. We also briefly discuss potential generalizations to Bayesian model classes and to classification problems.",
        "published": "2005-07-18T12:34:53Z",
        "link": "http://arxiv.org/abs/cs/0507041v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "I.2.6; E.4; G.3; F.1.3"
        ]
    },
    {
        "title": "Defensive Universal Learning with Experts",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "This paper shows how universal learning can be achieved with expert advice. To this aim, we specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain a master algorithm for \"reactive\" experts problems, which means that the master's actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. The resulting universal learner performs -- in a certain sense -- almost as well as any computable strategy, for any online decision problem. We also specify the (worst-case) convergence speed, which is very slow.",
        "published": "2005-07-18T14:33:56Z",
        "link": "http://arxiv.org/abs/cs/0507044v1",
        "categories": [
            "cs.LG",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "FPL Analysis for Adaptive Bandits",
        "authors": [
            "Jan Poland"
        ],
        "summary": "A main problem of \"Follow the Perturbed Leader\" strategies for online decision problems is that regret bounds are typically proven against oblivious adversary. In partial observation cases, it was not clear how to obtain performance guarantees against adaptive adversary, without worsening the bounds. We propose a conceptually simple argument to resolve this problem. Using this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed bandit problem is shown. This bound holds for the common FPL variant using only the observations from designated exploration rounds. Using all observations allows for the stronger bound of O(t^(1/2)), matching the best bound known so far (and essentially the known lower bound) for adversarial bandits. Surprisingly, this variant does not even need explicit exploration, it is self-stabilizing. However the sampling probabilities have to be either externally provided or approximated to sufficient accuracy, using O(t^2 log t) samples in each step.",
        "published": "2005-07-26T05:00:27Z",
        "link": "http://arxiv.org/abs/cs/0507062v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Regularity of Position Sequences",
        "authors": [
            "Manfred Harringer"
        ],
        "summary": "A person is given a numbered sequence of positions on a sheet of paper. The person is asked, \"Which will be the next (or the next after that) position?\" Everyone has an opinion as to how he or she would proceed. There are regular sequences for which there is general agreement on how to continue. However, there are less regular sequences for which this assessment is less certain. There are sequences for which every continuation is perceived to be arbitrary. I would like to present a mathematical model that reflects these opinions and perceptions with the aid of a valuation function. It is necessary to apply a rich set of invariant features of position sequences to ensure the quality of this model. All other properties of the model are arbitrary.",
        "published": "2005-08-01T18:55:57Z",
        "link": "http://arxiv.org/abs/cs/0508007v4",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "q-bio.NC",
            "I.5.1; I.6.8; J.4"
        ]
    },
    {
        "title": "Expectation maximization as message passing",
        "authors": [
            "J. Dauwels",
            "S. Korl",
            "H. -A. Loeliger"
        ],
        "summary": "Based on prior work by Eckford, it is shown how expectation maximization (EM) may be viewed, and used, as a message passing algorithm in factor graphs.",
        "published": "2005-08-03T16:09:00Z",
        "link": "http://arxiv.org/abs/cs/0508027v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Sequential Predictions based on Algorithmic Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "This paper studies sequence prediction based on the monotone Kolmogorov complexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is extremely close to Solomonoff's universal prior M, the latter being an excellent predictor in deterministic as well as probabilistic environments, where performance is measured in terms of convergence of posteriors or losses. Despite this closeness to M, it is difficult to assess the prediction quality of m, since little is known about the closeness of their posteriors, which are the important quantities for prediction. We show that for deterministic computable environments, the \"posterior\" and losses of m converge, but rapid convergence could only be shown on-sequence; the off-sequence convergence can be slow. In probabilistic environments, neither the posterior nor the losses converge, in general.",
        "published": "2005-08-05T10:16:16Z",
        "link": "http://arxiv.org/abs/cs/0508043v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "G.3; G.1.2; I.2.6; E.4"
        ]
    },
    {
        "title": "Measuring Semantic Similarity by Latent Relational Analysis",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similarity. LRA measures similarity in the semantic relations between two pairs of words. When two pairs have a high degree of relational similarity, they are analogous. For example, the pair cat:meow is analogous to the pair dog:bark. There is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning). In the Vector Space Model (VSM) approach to measuring relational similarity, the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs. The elements in the vectors are based on the frequencies of manually constructed patterns in a large corpus. LRA extends the VSM approach in three ways: (1) patterns are derived automatically from the corpus, (2) Singular Value Decomposition is used to smooth the frequency data, and (3) synonyms are used to reformulate word pairs. This paper describes the LRA algorithm and experimentally compares LRA to VSM on two tasks, answering college-level multiple-choice word analogy questions and classifying semantic relations in noun-modifier expressions. LRA achieves state-of-the-art results, reaching human-level performance on the analogy questions and significantly exceeding VSM performance on both tasks.",
        "published": "2005-08-10T19:35:57Z",
        "link": "http://arxiv.org/abs/cs/0508053v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Universal Learning of Repeated Matrix Games",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We study and compare the learning dynamics of two universal learning algorithms, one based on Bayesian learning and the other on prediction with expert advice. Both approaches have strong asymptotic performance guarantees. When confronted with the task of finding good long-term strategies in repeated 2x2 matrix games, they behave quite differently.",
        "published": "2005-08-16T16:27:25Z",
        "link": "http://arxiv.org/abs/cs/0508073v1",
        "categories": [
            "cs.LG",
            "cs.AI"
        ]
    },
    {
        "title": "Combinations and Mixtures of Optimal Policies in Unichain Markov   Decision Processes are Optimal",
        "authors": [
            "Ronald Ortner"
        ],
        "summary": "We show that combinations of optimal (stationary) policies in unichain Markov decision processes are optimal. That is, let M be a unichain Markov decision process with state space S, action space A and policies \\pi_j^*: S -> A (1\\leq j\\leq n) with optimal average infinite horizon reward. Then any combination \\pi of these policies, where for each state i in S there is a j such that \\pi(i)=\\pi_j^*(i), is optimal as well. Furthermore, we prove that any mixture of optimal policies, where at each visit in a state i an arbitrary action \\pi_j^*(i) of an optimal policy is chosen, yields optimal average reward, too.",
        "published": "2005-08-17T10:13:04Z",
        "link": "http://arxiv.org/abs/math/0508319v1",
        "categories": [
            "math.CO",
            "cs.DM",
            "cs.LG",
            "math.OC",
            "math.PR",
            "90C40"
        ]
    },
    {
        "title": "Corpus-based Learning of Analogies and Semantic Relations",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning \"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \"laser printer\", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations.",
        "published": "2005-08-23T20:21:56Z",
        "link": "http://arxiv.org/abs/cs/0508103v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Learning Optimal Augmented Bayes Networks",
        "authors": [
            "Vikas Hamine",
            "Paul Helman"
        ],
        "summary": "Naive Bayes is a simple Bayesian classifier with strong independence assumptions among the attributes. This classifier, desipte its strong independence assumptions, often performs well in practice. It is believed that relaxing the independence assumptions of a naive Bayes classifier may improve the classification accuracy of the resulting structure. While finding an optimal unconstrained Bayesian Network (for most any reasonable scoring measure) is an NP-hard problem, it is possible to learn in polynomial time optimal networks obeying various structural restrictions. Several authors have examined the possibilities of adding augmenting arcs between attributes of a Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN structure in which the augmenting arcs form a tree on the attributes, and present a polynomial time algorithm that learns an optimal TAN with respect to MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the augmenting arcs form a forest on the attributes (a collection of trees, hence a relaxation of the stuctural restriction of TAN), and present heuristic search methods for learning good, though not optimal, augmenting arc sets. The authors, however, evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric, such as MDL. In this paper, we present a simple, polynomial time greedy algorithm for learning an optimal Augmented Bayes Network with respect to MDL score.",
        "published": "2005-09-19T04:57:26Z",
        "link": "http://arxiv.org/abs/cs/0509055v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Learning Unions of $ω(1)$-Dimensional Rectangles",
        "authors": [
            "Alp Atici",
            "Rocco A. Servedio"
        ],
        "summary": "We consider the problem of learning unions of rectangles over the domain $[b]^n$, in the uniform distribution membership query learning setting, where both b and n are \"large\". We obtain poly$(n, \\log b)$-time algorithms for the following classes:   - poly$(n \\log b)$-way Majority of $O(\\frac{\\log(n \\log b)} {\\log \\log(n \\log b)})$-dimensional rectangles.   - Union of poly$(\\log(n \\log b))$ many $O(\\frac{\\log^2 (n \\log b)} {(\\log \\log(n \\log b) \\log \\log \\log (n \\log b))^2})$-dimensional rectangles.   - poly$(n \\log b)$-way Majority of poly$(n \\log b)$-Or of disjoint $O(\\frac{\\log(n \\log b)} {\\log \\log(n \\log b)})$-dimensional rectangles.   Our main algorithmic tool is an extension of Jackson's boosting- and Fourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$, building on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to obtain the results stated above are techniques from exact learning [Beimel, Kushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$ circuits [Jackson, Klivans, Servedio 2002] and on representing Boolean functions as thresholds of parities [Klivans, Servedio 2001].",
        "published": "2005-10-14T19:26:34Z",
        "link": "http://arxiv.org/abs/cs/0510038v4",
        "categories": [
            "cs.LG",
            "F.2.2; I.2.6"
        ]
    },
    {
        "title": "When Ignorance is Bliss",
        "authors": [
            "Peter D. Grunwald",
            "Joseph Y. Halpern"
        ],
        "summary": "It is commonly-accepted wisdom that more information is better, and that information should never be ignored. Here we argue, using both a Bayesian and a non-Bayesian analysis, that in some situations you are better off ignoring information if your uncertainty is represented by a set of probability measures. These include situations in which the information is relevant for the prediction task at hand. In the non-Bayesian analysis, we show how ignoring information avoids dilation, the phenomenon that additional pieces of information sometimes lead to an increase in uncertainty. In the Bayesian analysis, we show that for small sample sizes and certain prediction tasks, the Bayesian posterior based on a noninformative prior yields worse predictions than simply ignoring the given information.",
        "published": "2005-10-25T22:14:33Z",
        "link": "http://arxiv.org/abs/cs/0510080v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "I.2.4"
        ]
    },
    {
        "title": "The Impact of Social Networks on Multi-Agent Recommender Systems",
        "authors": [
            "Hamilton Link",
            "Jared Saia",
            "Terran Lane",
            "Randall A. LaViolette"
        ],
        "summary": "Awerbuch et al.'s approach to distributed recommender systems (DRSs) is to have agents sample products at random while randomly querying one another for the best item they have found; we improve upon this by adding a communication network. Agents can only communicate with their immediate neighbors in the network, but neighboring agents may or may not represent users with common interests. We define two network structures: in the ``mailing-list model,'' agents representing similar users form cliques, while in the ``word-of-mouth model'' the agents are distributed randomly in a scale-free network (SFN). In both models, agents tell their neighbors about satisfactory products as they are found. In the word-of-mouth model, knowledge of items propagates only through interested agents, and the SFN parameters affect the system's performance. We include a summary of our new results on the character and parameters of random subgraphs of SFNs, in particular SFNs with power-law degree distributions down to minimum degree 1. These networks are not as resilient as Cohen et al. originally suggested. In the case of the widely-cited ``Internet resilience'' result, high failure rates actually lead to the orphaning of half of the surviving nodes after 60% of the network has failed and the complete disintegration of the network at 90%. We show that given an appropriate network, the communication network reduces the number of sampled items, the number of messages sent, and the amount of ``spam.'' We conclude that in many cases DRSs will be useful for sharing information in a multi-agent learning system.",
        "published": "2005-11-02T23:44:34Z",
        "link": "http://arxiv.org/abs/cs/0511011v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.MA",
            "I.2.6; I.2.11"
        ]
    },
    {
        "title": "Learning by message-passing in networks of discrete synapses",
        "authors": [
            "Alfredo Braunstein",
            "Riccardo Zecchina"
        ],
        "summary": "We show that a message-passing process allows to store in binary \"material\" synapses a number of random patterns which almost saturates the information theoretic bounds. We apply the learning algorithm to networks characterized by a wide range of different connection topologies and of size comparable with that of biological systems (e.g. $n\\simeq10^{5}-10^{6}$). The algorithm can be turned into an on-line --fault tolerant-- learning protocol of potential interest in modeling aspects of synaptic plasticity and in building neuromorphic devices.",
        "published": "2005-11-07T13:48:01Z",
        "link": "http://arxiv.org/abs/cond-mat/0511159v2",
        "categories": [
            "cond-mat.dis-nn",
            "cs.LG",
            "q-bio.NC"
        ]
    },
    {
        "title": "Combinatorial Approach to Object Analysis",
        "authors": [
            "Rami Kanhouche"
        ],
        "summary": "We present a perceptional mathematical model for image and signal analysis. A resemblance measure is defined, and submitted to an innovating combinatorial optimization algorithm. Numerical Simulations are also presented",
        "published": "2005-11-09T14:41:00Z",
        "link": "http://arxiv.org/abs/nlin/0511015v1",
        "categories": [
            "nlin.AO",
            "cs.LG"
        ]
    },
    {
        "title": "On-line regression competitive with reproducing kernel Hilbert spaces",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "We consider the problem of on-line prediction of real-valued labels, assumed bounded in absolute value by a known constant, of new objects from known labeled objects. The prediction algorithm's performance is measured by the squared deviation of the predictions from the actual labels. No stochastic assumptions are made about the way the labels and objects are generated. Instead, we are given a benchmark class of prediction rules some of which are hoped to produce good predictions. We show that for a wide range of infinite-dimensional benchmark classes one can construct a prediction algorithm whose cumulative loss over the first N examples does not exceed the cumulative loss of any prediction rule in the class plus O(sqrt(N)); the main differences from the known results are that we do not impose any upper bound on the norm of the considered prediction rules and that we achieve an optimal leading term in the excess loss of our algorithm. If the benchmark class is \"universal\" (dense in the class of continuous functions on each compact set), this provides an on-line non-stochastic analogue of universally consistent prediction in non-parametric statistics. We use two proof techniques: one is based on the Aggregating Algorithm and the other on the recently developed method of defensive forecasting.",
        "published": "2005-11-15T17:13:50Z",
        "link": "http://arxiv.org/abs/cs/0511058v2",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Identifying Interaction Sites in \"Recalcitrant\" Proteins: Predicted   Protein and Rna Binding Sites in Rev Proteins of Hiv-1 and Eiav Agree with   Experimental Data",
        "authors": [
            "Michael Terribilini",
            "Jae-Hyung Lee",
            "Changhui Yan",
            "Robert L. Jernigan",
            "Susan Carpenter",
            "Vasant Honavar",
            "Drena Dobbs"
        ],
        "summary": "Protein-protein and protein nucleic acid interactions are vitally important for a wide range of biological processes, including regulation of gene expression, protein synthesis, and replication and assembly of many viruses. We have developed machine learning approaches for predicting which amino acids of a protein participate in its interactions with other proteins and/or nucleic acids, using only the protein sequence as input. In this paper, we describe an application of classifiers trained on datasets of well-characterized protein-protein and protein-RNA complexes for which experimental structures are available. We apply these classifiers to the problem of predicting protein and RNA binding sites in the sequence of a clinically important protein for which the structure is not known: the regulatory protein Rev, essential for the replication of HIV-1 and other lentiviruses. We compare our predictions with published biochemical, genetic and partial structural information for HIV-1 and EIAV Rev and with our own published experimental mapping of RNA binding sites in EIAV Rev. The predicted and experimentally determined binding sites are in very good agreement. The ability to predict reliably the residues of a protein that directly contribute to specific binding events - without the requirement for structural information regarding either the protein or complexes in which it participates - can potentially generate new disease intervention strategies.",
        "published": "2005-11-21T01:47:53Z",
        "link": "http://arxiv.org/abs/cs/0511075v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "J.3"
        ]
    },
    {
        "title": "Robust Inference of Trees",
        "authors": [
            "Marco Zaffalon",
            "Marcus Hutter"
        ],
        "summary": "This paper is concerned with the reliable inference of optimal tree-approximations to the dependency structure of an unknown distribution generating data. The traditional approach to the problem measures the dependency strength between random variables by the index called mutual information. In this paper reliability is achieved by Walley's imprecise Dirichlet model, which generalizes Bayesian learning with Dirichlet priors. Adopting the imprecise Dirichlet model results in posterior interval expectation for mutual information, and in a set of plausible trees consistent with the data. Reliable inference about the actual tree is achieved by focusing on the substructure common to all the plausible trees. We develop an exact algorithm that infers the substructure in time O(m^4), m being the number of random variables. The new algorithm is applied to a set of data sampled from a known distribution. The method is shown to reliably infer edges of the actual tree even when the data are very scarce, unlike the traditional approach. Finally, we provide lower and upper credibility limits for mutual information under the imprecise Dirichlet model. These enable the previous developments to be extended to a full inferential method for trees.",
        "published": "2005-11-25T10:59:35Z",
        "link": "http://arxiv.org/abs/cs/0511087v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Bounds on Query Convergence",
        "authors": [
            "Barak A. Pearlmutter"
        ],
        "summary": "The problem of finding an optimum using noisy evaluations of a smooth cost function arises in many contexts, including economics, business, medicine, experiment design, and foraging theory. We derive an asymptotic bound E[ (x_t - x*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1, >...) generated by an unbiased feedback process observing noisy evaluations of an unknown quadratic function maximised at x*. The bound is tight, as the proof leads to a simple algorithm which meets it. We further establish a bound on the total regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may impose practical limitations on an agent's performance, as O(eps^-4) queries are made before the queries converge to x* with eps accuracy.",
        "published": "2005-11-25T15:57:56Z",
        "link": "http://arxiv.org/abs/cs/0511088v1",
        "categories": [
            "cs.LG",
            "G.1.6"
        ]
    },
    {
        "title": "The Signed Distance Function: A New Tool for Binary Classification",
        "authors": [
            "Erik M. Boczko",
            "Todd R. Young"
        ],
        "summary": "From a geometric perspective most nonlinear binary classification algorithms, including state of the art versions of Support Vector Machine (SVM) and Radial Basis Function Network (RBFN) classifiers, and are based on the idea of reconstructing indicator functions. We propose instead to use reconstruction of the signed distance function (SDF) as a basis for binary classification. We discuss properties of the signed distance function that can be exploited in classification algorithms. We develop simple versions of such classifiers and test them on several linear and nonlinear problems. On linear tests accuracy of the new algorithm exceeds that of standard SVM methods, with an average of 50% fewer misclassifications. Performance of the new methods also matches or exceeds that of standard methods on several nonlinear problems including classification of benchmark diagnostic micro-array data sets.",
        "published": "2005-11-30T14:15:17Z",
        "link": "http://arxiv.org/abs/cs/0511105v1",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "Parameter Estimation of Hidden Diffusion Processes: Particle Filter vs.   Modified Baum-Welch Algorithm",
        "authors": [
            "A. Benabdallah",
            "G. Radons"
        ],
        "summary": "We propose a new method for the estimation of parameters of hidden diffusion processes. Based on parametrization of the transition matrix, the Baum-Welch algorithm is improved. The algorithm is compared to the particle filter in application to the noisy periodic systems. It is shown that the modified Baum-Welch algorithm is capable of estimating the system parameters with better accuracy than particle filters.",
        "published": "2005-11-30T20:23:19Z",
        "link": "http://arxiv.org/abs/cs/0511108v1",
        "categories": [
            "cs.DS",
            "cs.LG",
            "F.2.1; J.2"
        ]
    },
    {
        "title": "Joint fixed-rate universal lossy coding and identification of   continuous-alphabet memoryless sources",
        "authors": [
            "Maxim Raginsky"
        ],
        "summary": "The problem of joint universal source coding and identification is considered in the setting of fixed-rate lossy coding of continuous-alphabet memoryless sources. For a wide class of bounded distortion measures, it is shown that any compactly parametrized family of $\\R^d$-valued i.i.d. sources with absolutely continuous distributions satisfying appropriate smoothness and Vapnik--Chervonenkis learnability conditions, admits a joint scheme for universal lossy block coding and parameter estimation, such that when the block length $n$ tends to infinity, the overhead per-letter rate and the distortion redundancies converge to zero as $O(n^{-1}\\log n)$ and $O(\\sqrt{n^{-1}\\log n})$, respectively. Moreover, the active source can be determined at the decoder up to a ball of radius $O(\\sqrt{n^{-1} \\log n})$ in variational distance, asymptotically almost surely. The system has finite memory length equal to the block length, and can be thought of as blockwise application of a time-invariant nonlinear filter with initial conditions determined from the previous block. Comparisons are presented with several existing schemes for universal vector quantization, which do not include parameter estimation explicitly, and an extension to unbounded distortion measures is outlined. Finally, finite mixture classes and exponential families are given as explicit examples of parametric sources admitting joint universal compression and modeling schemes of the kind studied here.",
        "published": "2005-12-03T19:21:33Z",
        "link": "http://arxiv.org/abs/cs/0512015v3",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation   framework",
        "authors": [
            "Anthony Mouraud",
            "Didier Puzenat",
            "Hélène Paugam-Moisy"
        ],
        "summary": "In a Spiking Neural Networks (SNN), spike emissions are sparsely and irregularly distributed both in time and in the network architecture. Since a current feature of SNNs is a low average activity, efficient implementations of SNNs are usually based on an Event-Driven Simulation (EDS). On the other hand, simulations of large scale neural networks can take advantage of distributing the neurons on a set of processors (either workstation cluster or parallel computer). This article presents DAMNED, a large scale SNN simulation framework able to gather the benefits of EDS and parallel computing. Two levels of parallelism are combined: Distributed mapping of the neural topology, at the network level, and local multithreaded allocation of resources for simultaneous processing of events, at the neuron level. Based on the causality of events, a distributed solution is proposed for solving the complex problem of scheduling without synchronization barrier.",
        "published": "2005-12-05T06:57:39Z",
        "link": "http://arxiv.org/abs/cs/0512018v2",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Preference Learning in Terminology Extraction: A ROC-based approach",
        "authors": [
            "Jérôme Azé",
            "Mathieu Roche",
            "Yves Kodratoff",
            "Michèle Sebag"
        ],
        "summary": "A key data preparation step in Text Mining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevant/irrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).",
        "published": "2005-12-13T13:25:57Z",
        "link": "http://arxiv.org/abs/cs/0512050v1",
        "categories": [
            "cs.LG"
        ]
    },
    {
        "title": "Online Learning and Resource-Bounded Dimension: Winnow Yields New Lower   Bounds for Hard Sets",
        "authors": [
            "John M. Hitchcock"
        ],
        "summary": "We establish a relationship between the online mistake-bound model of learning and resource-bounded dimension. This connection is combined with the Winnow algorithm to obtain new results about the density of hard sets under adaptive reductions. This improves previous work of Fu (1995) and Lutz and Zhao (2000), and solves one of Lutz and Mayordomo's \"Twelve Problems in Resource-Bounded Measure\" (1999).",
        "published": "2005-12-13T22:01:09Z",
        "link": "http://arxiv.org/abs/cs/0512053v1",
        "categories": [
            "cs.CC",
            "cs.LG"
        ]
    },
    {
        "title": "Competing with wild prediction rules",
        "authors": [
            "Vladimir Vovk"
        ],
        "summary": "We consider the problem of on-line prediction competitive with a benchmark class of continuous but highly irregular prediction rules. It is known that if the benchmark class is a reproducing kernel Hilbert space, there exists a prediction algorithm whose average loss over the first N examples does not exceed the average loss of any prediction rule in the class plus a \"regret term\" of O(N^(-1/2)). The elements of some natural benchmark classes, however, are so irregular that these classes are not Hilbert spaces. In this paper we develop Banach-space methods to construct a prediction algorithm with a regret term of O(N^(-1/p)), where p is in [2,infty) and p-2 reflects the degree to which the benchmark class fails to be a Hilbert space.",
        "published": "2005-12-14T20:03:30Z",
        "link": "http://arxiv.org/abs/cs/0512059v2",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Complex Random Vectors and ICA Models: Identifiability, Uniqueness and   Separability",
        "authors": [
            "Jan Eriksson",
            "Visa Koivunen"
        ],
        "summary": "In this paper the conditions for identifiability, separability and uniqueness of linear complex valued independent component analysis (ICA) models are established. These results extend the well-known conditions for solving real-valued ICA problems to complex-valued models. Relevant properties of complex random vectors are described in order to extend the Darmois-Skitovich theorem for complex-valued models. This theorem is used to construct a proof of a theorem for each of the above ICA model concepts. Both circular and noncircular complex random vectors are covered. Examples clarifying the above concepts are presented.",
        "published": "2005-12-15T14:51:36Z",
        "link": "http://arxiv.org/abs/cs/0512063v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "cs.IR",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Implementation of Motzkin-Burger algorithm in Maple",
        "authors": [
            "P. A. Burovsky"
        ],
        "summary": "Subject of this paper is an implementation of a well-known Motzkin-Burger algorithm, which solves the problem of finding the full set of solutions of a system of linear homogeneous inequalities. There exist a number of implementations of this algorithm, but there was no one in Maple, to the best of the author's knowledge.",
        "published": "2005-01-01T04:55:40Z",
        "link": "http://arxiv.org/abs/cs/0501003v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "cs.SC",
            "F.2.2; I.3.5; G.1.6"
        ]
    },
    {
        "title": "On The Liniar Time Complexity of Finite Languages",
        "authors": [
            "Mircea Alexandru Popescu Moscu"
        ],
        "summary": "The present paper presents and proves a proposition concerning the time complexity of finite languages. It is shown herein, that for any finite language (a language for which the set of words composing it is finite) there is a Turing machine that computes the language in such a way that for any input of length k the machine stops in, at most, k + 1 steps.",
        "published": "2005-01-05T19:13:05Z",
        "link": "http://arxiv.org/abs/cs/0501009v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Algebraic Properties for Selector Functions",
        "authors": [
            "Lane A. Hemaspaandra",
            "Harald Hempel",
            "Arfst Nickelsen"
        ],
        "summary": "The nondeterministic advice complexity of the P-selective sets is known to be exactly linear. Regarding the deterministic advice complexity of the P-selective sets--i.e., the amount of Karp--Lipton advice needed for polynomial-time machines to recognize them in general--the best current upper bound is quadratic [Ko, 1983] and the best current lower bound is linear [Hemaspaandra and Torenvliet, 1996].   We prove that every associatively P-selective set is commutatively, associatively P-selective. Using this, we establish an algebraic sufficient condition for the P-selective sets to have a linear upper bound (which thus would match the existing lower bound) on their deterministic advice complexity: If all P-selective sets are associatively P-selective then the deterministic advice complexity of the P-selective sets is linear. The weakest previously known sufficient condition was P=NP.   We also establish related results for algebraic properties of, and advice complexity of, the nondeterministically selective sets.",
        "published": "2005-01-11T23:55:17Z",
        "link": "http://arxiv.org/abs/cs/0501022v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2; F.1.1"
        ]
    },
    {
        "title": "The Symmetric Group Defies Strong Fourier Sampling: Part I",
        "authors": [
            "Cristopher Moore",
            "Alexander Russell",
            "Leonard J. Schulman"
        ],
        "summary": "We resolve the question of whether Fourier sampling can efficiently solve the hidden subgroup problem. Specifically, we show that the hidden subgroup problem over the symmetric group cannot be efficiently solved by strong Fourier sampling, even if one may perform an arbitrary POVM on the coset state. Our results apply to the special case relevant to the Graph Isomorphism problem.",
        "published": "2005-01-12T14:50:59Z",
        "link": "http://arxiv.org/abs/quant-ph/0501056v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A quantum lower bound for the query complexity of Simon's problem",
        "authors": [
            "Pascal Koiran",
            "Vincent Nesme",
            "Natacha Portier"
        ],
        "summary": "Simon in his FOCS'94 paper was the first to show an exponential gap between classical and quantum computation. The problem he dealt with is now part of a well-studied class of problems, the hidden subgroup problems. We study Simon's problem from the point of view of quantum query complexity and give here a first nontrivial lower bound on the query complexity of a hidden subgroup problem, namely Simon's problem. Our bound is optimal up to a constant factor.",
        "published": "2005-01-12T16:49:08Z",
        "link": "http://arxiv.org/abs/quant-ph/0501060v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The Symmetric Group Defies Strong Fourier Sampling: Part II",
        "authors": [
            "Cristopher Moore",
            "Alexander Russell"
        ],
        "summary": "Part I of this paper showed that the hidden subgroup problem over the symmetric group--including the special case relevant to Graph Isomorphism--cannot be efficiently solved by strong Fourier sampling, even if one may perform an arbitrary POVM on the coset state. In this paper, we extend these results to entangled measurements. Specifically, we show that the hidden subgroup problem on the symmetric group cannot be solved by any POVM applied to pairs of coset states. In particular, these hidden subgroups cannot be determined by any polynomial number of one- or two-register experiments on coset states.",
        "published": "2005-01-13T04:21:15Z",
        "link": "http://arxiv.org/abs/quant-ph/0501066v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "On the Sensitivity of Cyclically-Invariant Boolean Functions",
        "authors": [
            "Sourav Chakraborty"
        ],
        "summary": "In this paper we construct a cyclically invariant Boolean function whose sensitivity is $\\Theta(n^{1/3})$. This result answers two previously published questions. Tur\\'an (1984) asked if any Boolean function, invariant under some transitive group of permutations, has sensitivity $\\Omega(\\sqrt{n})$. Kenyon and Kutin (2004) asked whether for a ``nice'' function the product of 0-sensitivity and 1-sensitivity is $\\Omega(n)$. Our function answers both questions in the negative.   We also prove that for minterm-transitive functions (a natural class of Boolean functions including our example) the sensitivity is $\\Omega(n^{1/3})$. Hence for this class of functions sensitivity and block sensitivity are polynomially related.",
        "published": "2005-01-13T22:06:25Z",
        "link": "http://arxiv.org/abs/cs/0501026v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Efficiently Detecting Torsion Points and Subtori",
        "authors": [
            "J. Maurice Rojas"
        ],
        "summary": "Suppose X is the complex zero set of a finite collection of polynomials in Z[x_1,...,x_n]. We show that deciding whether X contains a point all of whose coordinates are d_th roots of unity can be done within NP^NP (relative to the sparse encoding), under a plausible assumption on primes in arithmetic progression. In particular, our hypothesis can still hold even under certain failures of the Generalized Riemann Hypothesis, such as the presence of Siegel-Landau zeroes. Furthermore, we give a similar (but UNconditional) complexity upper bound for n=1. Finally, letting T be any algebraic subgroup of (C^*)^n we show that deciding whether X contains T is coNP-complete (relative to an even more efficient encoding),unconditionally. We thus obtain new non-trivial families of multivariate polynomial systems where deciding the existence of complex roots can be done unconditionally in the polynomial hierarchy -- a family of complexity classes lying between PSPACE and P, intimately connected with the P=?NP Problem. We also discuss a connection to Laurent's solution of Chabauty's Conjecture from arithmetic geometry.",
        "published": "2005-01-23T07:43:28Z",
        "link": "http://arxiv.org/abs/math/0501388v4",
        "categories": [
            "math.AG",
            "cs.CC",
            "math.NT"
        ]
    },
    {
        "title": "Optimal Union-Find in Constraint Handling Rules",
        "authors": [
            "Tom Schrijvers",
            "Thom Fruehwirth"
        ],
        "summary": "Constraint Handling Rules (CHR) is a committed-choice rule-based language that was originally intended for writing constraint solvers. In this paper we show that it is also possible to write the classic union-find algorithm and variants in CHR. The programs neither compromise in declarativeness nor efficiency. We study the time complexity of our programs: they match the almost-linear complexity of the best known imperative implementations. This fact is illustrated with experimental results.",
        "published": "2005-01-25T13:28:38Z",
        "link": "http://arxiv.org/abs/cs/0501073v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "cs.DS",
            "cs.PF"
        ]
    },
    {
        "title": "Simple extractors via constructions of cryptographic pseudo-random   generators",
        "authors": [
            "Marius Zimand"
        ],
        "summary": "Trevisan has shown that constructions of pseudo-random generators from hard functions (the Nisan-Wigderson approach) also produce extractors. We show that constructions of pseudo-random generators from one-way permutations (the Blum-Micali-Yao approach) can be used for building extractors as well. Using this new technique we build extractors that do not use designs and polynomial-based error-correcting codes and that are very simple and efficient. For example, one extractor produces each output bit separately in $O(\\log^2 n)$ time. These extractors work for weak sources with min entropy $\\lambda n$, for arbitrary constant $\\lambda > 0$, have seed length $O(\\log^2 n)$, and their output length is $\\approx n^{\\lambda/3}$.",
        "published": "2005-01-25T15:43:11Z",
        "link": "http://arxiv.org/abs/cs/0501075v3",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "Geometric Complexity III: on deciding positivity of   Littlewood-Richardson coefficients",
        "authors": [
            "Ketan D. Mulmuley",
            "Milind Sohoni"
        ],
        "summary": "We point out that the remarkable Knutson and Tao Saturation Theorem and polynomial time algorithms for LP have together an important and immediate consequence in Geometric Complexity Theory. The problem of deciding positivity of Littlewood-Richardson coefficients for GLn(C) belongs to P. Furthermore, the algorithm is strongly polynomial.   The main goal of this article is to explain the significance of this result in the context of Geometric Complexity Theory. Furthermore, it is also conjectured that an analogous result holds for arbitrary symmetrizable Kac-Moody algebras.",
        "published": "2005-01-26T11:58:51Z",
        "link": "http://arxiv.org/abs/cs/0501076v1",
        "categories": [
            "cs.CC",
            "math.RT",
            "F1.3"
        ]
    },
    {
        "title": "Focused Local Search for Random 3-Satisfiability",
        "authors": [
            "Sakari Seitz",
            "Mikko Alava",
            "Pekka Orponen"
        ],
        "summary": "A local search algorithm solving an NP-complete optimisation problem can be viewed as a stochastic process moving in an 'energy landscape' towards eventually finding an optimal solution. For the random 3-satisfiability problem, the heuristic of focusing the local moves on the presently unsatisfiedclauses is known to be very effective: the time to solution has been observed to grow only linearly in the number of variables, for a given clauses-to-variables ratio $\\alpha$ sufficiently far below the critical satisfiability threshold $\\alpha_c \\approx 4.27$. We present numerical results on the behaviour of three focused local search algorithms for this problem, considering in particular the characteristics of a focused variant of the simple Metropolis dynamics. We estimate the optimal value for the ``temperature'' parameter $\\eta$ for this algorithm, such that its linear-time regime extends as close to $\\alpha_c$ as possible. Similar parameter optimisation is performed also for the well-known WalkSAT algorithm and for the less studied, but very well performing Focused Record-to-Record Travel method. We observe that with an appropriate choice of parameters, the linear time regime for each of these algorithms seems to extend well into ratios $\\alpha > 4.2$ -- much further than has so far been generally assumed. We discuss the statistics of solution times for the algorithms, relate their performance to the process of ``whitening'', and present some conjectures on the shape of their computational phase diagrams.",
        "published": "2005-01-28T14:13:24Z",
        "link": "http://arxiv.org/abs/cond-mat/0501707v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Model-Checking Problems as a Basis for Parameterized Intractability",
        "authors": [
            "Joerg Flum",
            "Martin Grohe"
        ],
        "summary": "Most parameterized complexity classes are defined in terms of a parameterized version of the Boolean satisfiability problem (the so-called weighted satisfiability problem). For example, Downey and Fellow's W-hierarchy is of this form. But there are also classes, for example, the A-hierarchy, that are more naturally characterised in terms of model-checking problems for certain fragments of first-order logic.   Downey, Fellows, and Regan were the first to establish a connection between the two formalisms by giving a characterisation of the W-hierarchy in terms of first-order model-checking problems. We improve their result and then prove a similar correspondence between weighted satisfiability and model-checking problems for the A-hierarchy and the W^*-hierarchy. Thus we obtain very uniform characterisations of many of the most important parameterized complexity classes in both formalisms.   Our results can be used to give new, simple proofs of some of the core results of structural parameterized complexity theory.",
        "published": "2005-02-01T18:03:51Z",
        "link": "http://arxiv.org/abs/cs/0502005v5",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "Amazons is PSPACE-complete",
        "authors": [
            "Robert A. Hearn"
        ],
        "summary": "Amazons is a board game which combines elements of Chess and Go. It has become popular in recent years, and has served as a useful platform for both game-theoretic study and AI games research. Buro showed that simple Amazons endgames are NP-equivalent, leaving the complexity of the general case as an open problem.   We settle this problem, by showing that deciding the outcome of an n x n Amazons position is PSPACE-hard. We give a reduction from one of the PSPACE-complete two-player formula games described by Schaefer. Since the number of moves in an Amazons game is polynomially bounded (unlike Chess and Go), Amazons is in PSPACE. It is thus on a par with other two-player, bounded-move, perfect-information games such as Hex, Othello, and Kayles. Our construction also provides an alternate proof that simple Amazons endgames are NP-equivalent.   Our reduction uses a number of amazons polynomial in the input formula length; a remaining open problem is the complexity of Amazons when only a constant number of amazons is used.",
        "published": "2005-02-02T18:55:24Z",
        "link": "http://arxiv.org/abs/cs/0502013v1",
        "categories": [
            "cs.CC",
            "cs.GT",
            "F.2.2"
        ]
    },
    {
        "title": "Fixed Type Theorems",
        "authors": [
            "Raju Renjit G"
        ],
        "summary": "This submission has been withdrawn at the request of the author.",
        "published": "2005-02-05T17:13:40Z",
        "link": "http://arxiv.org/abs/cs/0502030v36",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Logarithmic Lower Bounds in the Cell-Probe Model",
        "authors": [
            "Mihai Patrascu",
            "Erik D. Demaine"
        ],
        "summary": "We develop a new technique for proving cell-probe lower bounds on dynamic data structures. This technique enables us to prove an amortized randomized Omega(lg n) lower bound per operation for several data structural problems on n elements, including partial sums, dynamic connectivity among disjoint paths (or a forest or a graph), and several other dynamic graph problems (by simple reductions). Such a lower bound breaks a long-standing barrier of Omega(lg n / lglg n) for any dynamic language membership problem. It also establishes the optimality of several existing data structures, such as Sleator and Tarjan's dynamic trees. We also prove the first Omega(log_B n) lower bound in the external-memory model without assumptions on the data structure (such as the comparison model). Our lower bounds also give a query-update trade-off curve matched, e.g., by several data structures for dynamic connectivity in graphs. We also prove matching upper and lower bounds for partial sums when parameterized by the word size and the maximum additive change in an update.",
        "published": "2005-02-08T03:03:55Z",
        "link": "http://arxiv.org/abs/cs/0502041v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "The complexity of computing the Hilbert polynomial of smooth   equidimensional complex projective varieties",
        "authors": [
            "Peter Buergisser",
            "Martin Lotz"
        ],
        "summary": "We continue the study of counting complexity begun in [Buergisser, Cucker 04] and [Buergisser, Cucker, Lotz 05] by proving upper and lower bounds on the complexity of computing the Hilbert polynomial of a homogeneous ideal. We show that the problem of computing the Hilbert polynomial of a smooth equidimensional complex projective variety can be reduced in polynomial time to the problem of counting the number of complex common zeros of a finite set of multivariate polynomials. Moreover, we prove that the more general problem of computing the Hilbert polynomial of a homogeneous ideal is polynomial space hard. This implies polynomial space lower bounds for both the problems of computing the rank and the Euler characteristic of cohomology groups of coherent sheaves on projective space, improving the #P-lower bound of Bach (JSC 1999).",
        "published": "2005-02-08T17:10:00Z",
        "link": "http://arxiv.org/abs/cs/0502044v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1; F.1"
        ]
    },
    {
        "title": "NP-complete Problems and Physical Reality",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and \"anthropic computing.\" The section on soap bubbles even includes some \"experimental\" results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics.",
        "published": "2005-02-12T15:14:11Z",
        "link": "http://arxiv.org/abs/quant-ph/0502072v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "gr-qc"
        ]
    },
    {
        "title": "The Complexity of Computing the Size of an Interval",
        "authors": [
            "Lane A. Hemaspaandra",
            "Christopher M. Homan",
            "Sven Kosub",
            "Klaus W. Wagner"
        ],
        "summary": "Given a p-order A over a universe of strings (i.e., a transitive, reflexive, antisymmetric relation such that if (x, y) is an element of A then |x| is polynomially bounded by |y|), an interval size function of A returns, for each string x in the universe, the number of strings in the interval between strings b(x) and t(x) (with respect to A), where b(x) and t(x) are functions that are polynomial-time computable in the length of x.   By choosing sets of interval size functions based on feasibility requirements for their underlying p-orders, we obtain new characterizations of complexity classes. We prove that the set of all interval size functions whose underlying p-orders are polynomial-time decidable is exactly #P. We show that the interval size functions for orders with polynomial-time adjacency checks are closely related to the class FPSPACE(poly). Indeed, FPSPACE(poly) is exactly the class of all nonnegative functions that are an interval size function minus a polynomial-time computable function.   We study two important functions in relation to interval size functions. The function #DIV maps each natural number n to the number of nontrivial divisors of n. We show that #DIV is an interval size function of a polynomial-time decidable partial p-order with polynomial-time adjacency checks. The function #MONSAT maps each monotone boolean formula F to the number of satisfying assignments of F. We show that #MONSAT is an interval size function of a polynomial-time decidable total p-order with polynomial-time adjacency checks.   Finally, we explore the related notion of cluster computation.",
        "published": "2005-02-13T15:24:30Z",
        "link": "http://arxiv.org/abs/cs/0502058v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "The Lattice of Machine Invariant Sets and Subword Complexity",
        "authors": [
            "Janis Buls"
        ],
        "summary": "We investigate the lattice of machine invariant classes. This is an infinite completely distributive lattice but it is not a Boolean lattice. We show the subword complexity and the growth function create machine invariant classes. So the lattice would serve as a measure of words cryptographic quality if we like to identify new stream ciphers suitable for widespread adoption.",
        "published": "2005-02-14T15:28:45Z",
        "link": "http://arxiv.org/abs/cs/0502064v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "cs.DM",
            "D.2.8;D.4.6;E.3;F.1.1;F.1.3;F.4.3"
        ]
    },
    {
        "title": "On the Complexity of Real Functions",
        "authors": [
            "Mark Braverman"
        ],
        "summary": "We develop a notion of computability and complexity of functions over the reals, which seems to be very natural when one tries to determine just how \"difficult\" a certain function is. This notion can be viewed as an extension of both BSS computability [Blum, Cucker, Shub, Smale 1998], and bit computability in the tradition of computable analysis [Weihrauch 2000] as it relies on the latter but allows some discontinuities and multiple values.",
        "published": "2005-02-15T05:24:30Z",
        "link": "http://arxiv.org/abs/cs/0502066v1",
        "categories": [
            "cs.CC",
            "cs.NA",
            "math.NA",
            "F. 1.1; F. 4. 1"
        ]
    },
    {
        "title": "Limits of Rush Hour Logic Complexity",
        "authors": [
            "John Tromp",
            "Rudi Cilibrasi"
        ],
        "summary": "Rush Hour Logic was introduced in [Flake&Baum99] as a model of computation inspired by the ``Rush Hour'' toy puzzle, in which cars can move horizontally or vertically within a parking lot. The authors show how the model supports polynomial space computation, using certain car configurations as building blocks to construct boolean circuits for a cpu and memory. They consider the use of cars of length 3 crucial to their construction, and conjecture that cars of size 2 only, which we'll call `Size 2 Rush Hour', do not support polynomial space computation. We settle this conjecture by showing that the required building blocks are constructible in Size 2 Rush Hour. Furthermore, we consider Unit Rush Hour, which was hitherto believed to be trivial, show its relation to maze puzzles, and provide empirical support for its hardness.",
        "published": "2005-02-15T14:22:35Z",
        "link": "http://arxiv.org/abs/cs/0502068v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.2"
        ]
    },
    {
        "title": "On computational complexity of Siegel Julia sets",
        "authors": [
            "I. Binder",
            "M. Braverman",
            "M. Yampolsky"
        ],
        "summary": "It has been previously shown by two of the authors that some polynomial Julia sets are algorithmically impossible to draw with arbitrary magnification. On the other hand, for a large class of examples the problem of drawing a picture has polynomial complexity. In this paper we demonstrate the existence of computable quadratic Julia sets whose computational complexity is arbitrarily high.",
        "published": "2005-02-16T15:53:36Z",
        "link": "http://arxiv.org/abs/math/0502354v2",
        "categories": [
            "math.DS",
            "cs.CC",
            "37F10"
        ]
    },
    {
        "title": "On sample complexity for computational pattern recognition",
        "authors": [
            "Daniil Ryabko"
        ],
        "summary": "In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if we restrict our attention to computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples.   The task of pattern recognition is considered in conjunction with another learning problem -- data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.",
        "published": "2005-02-17T14:58:28Z",
        "link": "http://arxiv.org/abs/cs/0502074v2",
        "categories": [
            "cs.LG",
            "cs.CC"
        ]
    },
    {
        "title": "Semantical Characterizations and Complexity of Equivalences in Answer   Set Programming",
        "authors": [
            "Thomas Eiter",
            "Michael Fink",
            "Stefan Woltran"
        ],
        "summary": "In recent research on non-monotonic logic programming, repeatedly strong equivalence of logic programs P and Q has been considered, which holds if the programs P union R and Q union R have the same answer sets for any other program R. This property strengthens equivalence of P and Q with respect to answer sets (which is the particular case for R is the empty set), and has its applications in program optimization, verification, and modular logic programming. In this paper, we consider more liberal notions of strong equivalence, in which the actual form of R may be syntactically restricted. On the one hand, we consider uniform equivalence, where R is a set of facts rather than a set of rules. This notion, which is well known in the area of deductive databases, is particularly useful for assessing whether programs P and Q are equivalent as components of a logic program which is modularly structured. On the other hand, we consider relativized notions of equivalence, where R ranges over rules over a fixed alphabet, and thus generalize our results to relativized notions of strong and uniform equivalence. For all these notions, we consider disjunctive logic programs in the propositional (ground) case, as well as some restricted classes, provide semantical characterizations and analyze the computational complexity. Our results, which naturally extend to answer set semantics for programs with strong negation, complement the results on strong equivalence of logic programs and pave the way for optimizations in answer set solvers as a tool for input-based problem solving.",
        "published": "2005-02-18T11:01:03Z",
        "link": "http://arxiv.org/abs/cs/0502078v1",
        "categories": [
            "cs.AI",
            "cs.CC",
            "I.2.3; F.4.1; I.2.4"
        ]
    },
    {
        "title": "On Generalized Computable Universal Priors and their Convergence",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff unified Occam's razor and Epicurus' principle of multiple explanations to one elegant, formal, universal theory of inductive inference, which initiated the field of algorithmic information theory. His central result is that the posterior of the universal semimeasure M converges rapidly to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal predictor in case of unknown mu. The first part of the paper investigates the existence and convergence of computable universal (semi)measures for a hierarchy of computability classes: recursive, estimable, enumerable, and approximable. For instance, M is known to be enumerable, but not estimable, and to dominate all enumerable semimeasures. We present proofs for discrete and continuous semimeasures. The second part investigates more closely the types of convergence, possibly implied by universality: in difference and in ratio, with probability 1, in mean sum, and for Martin-Loef random sequences. We introduce a generalized concept of randomness for individual sequences and use it to exhibit difficulties regarding these issues. In particular, we show that convergence fails (holds) on generalized-random sequences in gappy (dense) Bernoulli classes.",
        "published": "2005-03-11T12:38:30Z",
        "link": "http://arxiv.org/abs/cs/0503026v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "math.PR",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Complexity and Approximation of Fixing Numerical Attributes in Databases   Under Integrity Constraints",
        "authors": [
            "L. Bertossi",
            "L. Bravo",
            "E. Franconi",
            "A. Lopatenko"
        ],
        "summary": "Consistent query answering is the problem of computing the answers from a database that are consistent with respect to certain integrity constraints that the database as a whole may fail to satisfy. Those answers are characterized as those that are invariant under minimal forms of restoring the consistency of the database. In this context, we study the problem of repairing databases by fixing integer numerical values at the attribute level with respect to denial and aggregation constraints. We introduce a quantitative definition of database fix, and investigate the complexity of several decision and optimization problems, including DFP, i.e. the existence of fixes within a given distance from the original instance, and CQA, i.e. deciding consistency of answers to aggregate conjunctive queries under different semantics. We provide sharp complexity bounds, identify relevant tractable cases; and introduce approximation algorithms for some of those that are intractable. More specifically, we obtain results like undecidability of existence of fixes for aggregation constraints; MAXSNP-hardness of DFP, but a good approximation algorithm for a relevant special case; and intractability but good approximation for CQA for aggregate queries for one database atom denials (plus built-ins).",
        "published": "2005-03-15T00:03:59Z",
        "link": "http://arxiv.org/abs/cs/0503032v3",
        "categories": [
            "cs.DB",
            "cs.CC"
        ]
    },
    {
        "title": "Comment on \"Some non-conventional ideas about algorithmic complexity\"",
        "authors": [
            "David Poulin",
            "Hugo Touchette"
        ],
        "summary": "We comment on a recent paper by D'Abramo [Chaos, Solitons & Fractals, 25 (2005) 29], focusing on the author's statement that an algorithm can produce a list of strings containing at least one string whose algorithmic complexity is greater than that of the entire list. We show that this statement, although perplexing, is not as paradoxical as it seems when the definition of algorithmic complexity is applied correctly.",
        "published": "2005-03-16T11:09:06Z",
        "link": "http://arxiv.org/abs/cs/0503034v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Complexity Issues in Finding Succinct Solutions of PSPACE-Complete   Problems",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "We study the problem of deciding whether some PSPACE-complete problems have models of bounded size. Contrary to problems in NP, models of PSPACE-complete problems may be exponentially large. However, such models may take polynomial space in a succinct representation. For example, the models of a QBF are explicitely represented by and-or trees (which are always of exponential size) but can be succinctely represented by circuits (which can be polynomial or exponential). We investigate the complexity of deciding the existence of such succinct models when a bound on size is given.",
        "published": "2005-03-18T17:30:39Z",
        "link": "http://arxiv.org/abs/cs/0503043v3",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "I.2.3; I.2.4; I.2.8; F.1.3; F.2.2"
        ]
    },
    {
        "title": "Hiding Satisfying Assignments: Two are Better than One",
        "authors": [
            "Dimitris Achlioptas",
            "Haixia Jia",
            "Cristopher Moore"
        ],
        "summary": "The evaluation of incomplete satisfiability solvers depends critically on the availability of hard satisfiable instances. A plausible source of such instances consists of random k-SAT formulas whose clauses are chosen uniformly from among all clauses satisfying some randomly chosen truth assignment A. Unfortunately, instances generated in this manner tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly speaking, as the formula's density increases, for a number of different algorithms, A acts as a stronger and stronger attractor. Motivated by recent results on the geometry of the space of satisfying truth assignments of random k-SAT and NAE-k-SAT formulas, we introduce a simple twist on this basic model, which appears to dramatically increase its hardness. Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also forbid the clauses violated by its complement, so that both A and complement of A are satisfying. It appears that under this \"symmetrization'' the effects of the two attractors largely cancel out, making it much harder for algorithms to find any truth assignment. We give theoretical and experimental evidence supporting this assertion.",
        "published": "2005-03-20T02:46:09Z",
        "link": "http://arxiv.org/abs/cs/0503046v1",
        "categories": [
            "cs.AI",
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Enforcing and Defying Associativity, Commutativity, Totality, and Strong   Noninvertibility for One-Way Functions in Complexity Theory",
        "authors": [
            "Lane A. Hemaspaandra",
            "Joerg Rothe",
            "Amitabh Saxena"
        ],
        "summary": "Rabi and Sherman [RS97,RS93] proved that the hardness of factoring is a sufficient condition for there to exist one-way functions (i.e., p-time computable, honest, p-time noninvertible functions; this paper is in the worst-case model, not the average-case model) that are total, commutative, and associative but not strongly noninvertible. In this paper we improve the sufficient condition to ``P does not equal NP.''   More generally, in this paper we completely characterize which types of one-way functions stand or fall together with (plain) one-way functions--equivalently, stand or fall together with P not equaling NP. We look at the four attributes used in Rabi and Sherman's seminal work on algebraic properties of one-way functions (see [RS97,RS93]) and subsequent papers--strongness (of noninvertibility), totality, commutativity, and associativity--and for each attribute, we allow it to be required to hold, required to fail, or ``don't care.'' In this categorization there are 3^4 = 81 potential types of one-way functions. We prove that each of these 81 feature-laden types stand or fall together with the existence of (plain) one-way functions.",
        "published": "2005-03-21T19:19:54Z",
        "link": "http://arxiv.org/abs/cs/0503049v3",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Zeta-Dimension",
        "authors": [
            "David Doty",
            "Xiaoyang Gu",
            "Jack H. Lutz",
            "Elvira Mayordomo",
            "Philippe Moser"
        ],
        "summary": "The zeta-dimension of a set A of positive integers is the infimum s such that the sum of the reciprocals of the s-th powers of the elements of A is finite.   Zeta-dimension serves as a fractal dimension on the positive integers that extends naturally usefully to discrete lattices such as the set of all integer lattice points in d-dimensional space.   This paper reviews the origins of zeta-dimension (which date to the eighteenth and nineteenth centuries) and develops its basic theory, with particular attention to its relationship with algorithmic information theory. New results presented include extended connections between zeta-dimension and classical fractal dimensions, a gale characterization of zeta-dimension, and a theorem on the zeta-dimensions of pointwise sums and products of sets of positive integers.",
        "published": "2005-03-22T05:58:43Z",
        "link": "http://arxiv.org/abs/cs/0503052v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Complexity of Nonrecursive XQuery and Functional Query Languages   on Complex Values",
        "authors": [
            "Christoph Koch"
        ],
        "summary": "This paper studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery.   We show that monad algebra with equality restricted to atomic values is complete for the class TA[2^{O(n)}, O(n)] of problems solvable in linear exponential time with a linear number of alternations. The monotone fragment of monad algebra with atomic value equality but without negation is complete for nondeterministic exponential time. For monad algebra with deep equality, we establish TA[2^{O(n)}, O(n)] lower and exponential-space upper bounds.   Then we study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with ``child'' as the only axis) is exhibited, and it is shown that these languages are expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra w.r.t. combined complexity, and that it is in TC0 if the query is assumed fixed.",
        "published": "2005-03-23T15:36:55Z",
        "link": "http://arxiv.org/abs/cs/0503062v2",
        "categories": [
            "cs.DB",
            "cs.CC",
            "F.4.1, H.2.3, I.7.2"
        ]
    },
    {
        "title": "Noise stability of functions with low influences: invariance and   optimality",
        "authors": [
            "Elchanan Mossel",
            "Ryan O'Donnell",
            "Krzysztof Oleszkiewicz"
        ],
        "summary": "In this paper we study functions with low influences on product probability spaces. The analysis of boolean functions with low influences has become a central problem in discrete Fourier analysis. It is motivated by fundamental questions arising from the construction of probabilistically checkable proofs in theoretical computer science and from problems in the theory of social choice in economics.   We prove an invariance principle for multilinear polynomials with low influences and bounded degree; it shows that under mild conditions the distribution of such polynomials is essentially invariant for all product spaces. Ours is one of the very few known non-linear invariance principles. It has the advantage that its proof is simple and that the error bounds are explicit. We also show that the assumption of bounded degree can be eliminated if the polynomials are slightly ``smoothed''; this extension is essential for our applications to ``noise stability''-type problems.   In particular, as applications of the invariance principle we prove two conjectures: the ``Majority Is Stablest'' conjecture from theoretical computer science, which was the original motivation for this work, and the ``It Ain't Over Till It's Over'' conjecture from social choice theory.",
        "published": "2005-03-23T18:54:38Z",
        "link": "http://arxiv.org/abs/math/0503503v2",
        "categories": [
            "math.PR",
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "Spines of Random Constraint Satisfaction Problems: Definition and   Connection with Computational Complexity",
        "authors": [
            "Gabriel Istrate",
            "Stefan Boettcher",
            "Allon G. Percus"
        ],
        "summary": "We study the connection between the order of phase transitions in combinatorial problems and the complexity of decision algorithms for such problems. We rigorously show that, for a class of random constraint satisfaction problems, a limited connection between the two phenomena indeed exists. Specifically, we extend the definition of the spine order parameter of Bollobas et al. to random constraint satisfaction problems, rigorously showing that for such problems a discontinuity of the spine is associated with a $2^{\\Omega(n)}$ resolution complexity (and thus a $2^{\\Omega(n)}$ complexity of DPLL algorithms) on random instances. The two phenomena have a common underlying cause: the emergence of ``large'' (linear size) minimally unsatisfiable subformulas of a random formula at the satisfiability phase transition.   We present several further results that add weight to the intuition that random constraint satisfaction problems with a sharp threshold and a continuous spine are ``qualitatively similar to random 2-SAT''. Finally, we argue that it is the spine rather than the backbone parameter whose continuity has implications for the decision complexity of combinatorial problems, and we provide experimental evidence that the two parameters can behave in a different manner.",
        "published": "2005-03-29T21:58:21Z",
        "link": "http://arxiv.org/abs/cs/0503082v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "cs.AI"
        ]
    },
    {
        "title": "Coarse and Sharp Thresholds of Boolean Constraint Satisfaction Problems",
        "authors": [
            "Gabriel Istrate"
        ],
        "summary": "We study threshold properties of random constraint satisfaction problems under a probabilistic model due to Molloy. We give a sufficient condition for the existence of a sharp threshold that leads (for boolean constraints) to a necessary and sufficient for the existence of a sharp threshold in the case where constraint templates are applied with equal probability, solving thus an open problem of Creignou and Daude.",
        "published": "2005-03-29T22:25:44Z",
        "link": "http://arxiv.org/abs/cs/0503083v1",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "Resource Bounded Unprovability of Computational Lower Bounds",
        "authors": [
            "Tatsuaki Okamoto",
            "Ryo Kashima"
        ],
        "summary": "This paper introduces new notions of asymptotic proofs, PT(polynomial-time)-extensions, PTM(polynomial-time Turing machine)-omega-consistency, etc. on formal theories of arithmetic including PA (Peano Arithmetic). This paper shows that P not= NP (more generally, any super-polynomial-time lower bound in PSPACE) is unprovable in a PTM-omega-consistent theory T, where T is a consistent PT-extension of PA. This result gives a unified view to the existing two major negative results on proving P not= NP, Natural Proofs and relativizable proofs, through the two manners of characterization of PTM-omega-consistency. We also show that the PTM-omega-consistency of T cannot be proven in any PTM-omega-consistent theory S, where S is a consistent PT-extension of T.",
        "published": "2005-03-31T10:40:13Z",
        "link": "http://arxiv.org/abs/cs/0503091v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1; F.2.2"
        ]
    },
    {
        "title": "Quantum search algorithms",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We review some of quantum algorithms for search problems: Grover's search algorithm, its generalization to amplitude amplification, the applications of amplitude amplification to various problems and the recent quantum algorithms based on quantum walks.",
        "published": "2005-04-03T19:03:49Z",
        "link": "http://arxiv.org/abs/quant-ph/0504012v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Clustering of solutions in the random satisfiability problem",
        "authors": [
            "M. Mezard",
            "T. Mora",
            "R. Zecchina"
        ],
        "summary": "Using elementary rigorous methods we prove the existence of a clustered phase in the random $K$-SAT problem, for $K\\geq 8$. In this phase the solutions are grouped into clusters which are far away from each other. The results are in agreement with previous predictions of the cavity method and give a rigorous confirmation to one of its main building blocks. It can be generalized to other systems of both physical and computational interest.",
        "published": "2005-04-04T09:42:16Z",
        "link": "http://arxiv.org/abs/cond-mat/0504070v1",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "Linear Datalog and Bounded Path Duality of Relational Structures",
        "authors": [
            "Victor Dalmau"
        ],
        "summary": "In this paper we systematically investigate the connections between logics with a finite number of variables, structures of bounded pathwidth, and linear Datalog Programs. We prove that, in the context of Constraint Satisfaction Problems, all these concepts correspond to different mathematical embodiments of a unique robust notion that we call bounded path duality. We also study the computational complexity implications of the notion of bounded path duality. We show that every constraint satisfaction problem $\\csp(\\best)$ with bounded path duality is solvable in NL and that this notion explains in a uniform way all families of CSPs known to be in NL. Finally, we use the results developed in the paper to identify new problems in NL.",
        "published": "2005-04-07T16:07:03Z",
        "link": "http://arxiv.org/abs/cs/0504027v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "On Approximating Restricted Cycle Covers",
        "authors": [
            "Bodo Manthey"
        ],
        "summary": "A cycle cover of a graph is a set of cycles such that every vertex is part of exactly one cycle. An L-cycle cover is a cycle cover in which the length of every cycle is in the set L. The weight of a cycle cover of an edge-weighted graph is the sum of the weights of its edges.   We come close to settling the complexity and approximability of computing L-cycle covers. On the one hand, we show that for almost all L, computing L-cycle covers of maximum weight in directed and undirected graphs is APX-hard and NP-hard. Most of our hardness results hold even if the edge weights are restricted to zero and one.   On the other hand, we show that the problem of computing L-cycle covers of maximum weight can be approximated within a factor of 2 for undirected graphs and within a factor of 8/3 in the case of directed graphs. This holds for arbitrary sets L.",
        "published": "2005-04-11T12:47:02Z",
        "link": "http://arxiv.org/abs/cs/0504038v5",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Pushdown dimension",
        "authors": [
            "David Doty",
            "Jared Nichols"
        ],
        "summary": "This paper develops the theory of pushdown dimension and explores its relationship with finite-state dimension. Pushdown dimension is trivially bounded above by finite-state dimension for all sequences, since a pushdown gambler can simulate any finite-state gambler. We show that for every rational 0 < d < 1, there exists a sequence with finite-state dimension d whose pushdown dimension is at most d/2. This establishes a quantitative analogue of the well-known fact that pushdown automata decide strictly more languages than finite automata.",
        "published": "2005-04-12T22:13:12Z",
        "link": "http://arxiv.org/abs/cs/0504047v4",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Oracles Are Subtle But Not Malicious",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "Theoretical computer scientists have been debating the role of oracles since the 1970's. This paper illustrates both that oracles can give us nontrivial insights about the barrier problems in circuit complexity, and that they need not prevent us from trying to solve those problems.   First, we give an oracle relative to which PP has linear-sized circuits, by proving a new lower bound for perceptrons and low- degree threshold polynomials. This oracle settles a longstanding open question, and generalizes earlier results due to Beigel and to Buhrman, Fortnow, and Thierauf. More importantly, it implies the first nonrelativizing separation of \"traditional\" complexity classes, as opposed to interactive proof classes such as MIP and MA-EXP. For Vinodchandran showed, by a nonrelativizing argument, that PP does not have circuits of size n^k for any fixed k. We present an alternative proof of this fact, which shows that PP does not even have quantum circuits of size n^k with quantum advice. To our knowledge, this is the first nontrivial lower bound on quantum circuit size.   Second, we study a beautiful algorithm of Bshouty et al. for learning Boolean circuits in ZPP^NP. We show that the NP queries in this algorithm cannot be parallelized by any relativizing technique, by giving an oracle relative to which ZPP^||NP and even BPP^||NP have linear-size circuits. On the other hand, we also show that the NP queries could be parallelized if P=NP. Thus, classes such as ZPP^||NP inhabit a \"twilight zone,\" where we need to distinguish between relativizing and black-box techniques. Our results on this subject have implications for computational learning theory as well as for the circuit minimization problem.",
        "published": "2005-04-12T22:57:09Z",
        "link": "http://arxiv.org/abs/cs/0504048v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.2; F.1.3"
        ]
    },
    {
        "title": "Conditional Hardness for Approximate Coloring",
        "authors": [
            "Irit Dinur",
            "Elchanan Mossel",
            "Oded Regev"
        ],
        "summary": "We study the coloring problem: Given a graph G, decide whether $c(G) \\leq q$ or $c(G) \\ge Q$, where c(G) is the chromatic number of G. We derive conditional hardness for this problem for any constant $3 \\le q < Q$. For $q\\ge 4$, our result is based on Khot's 2-to-1 conjecture [Khot'02]. For $q=3$, we base our hardness result on a certain `fish shaped' variant of his conjecture.   We also prove that the problem almost coloring is hard for any constant $\\eps>0$, assuming Khot's Unique Games conjecture. This is the problem of deciding for a given graph, between the case where one can 3-color all but a $\\eps$ fraction of the vertices without monochromatic edges, and the case where the graph contains no independent set of relative size at least $\\eps$.   Our result is based on bounding various generalized noise-stability quantities using the invariance principle of Mossel et al [MOO'05].",
        "published": "2005-04-14T00:43:34Z",
        "link": "http://arxiv.org/abs/cs/0504062v1",
        "categories": [
            "cs.CC",
            "math.PR"
        ]
    },
    {
        "title": "Dichotomy for Voting Systems",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra"
        ],
        "summary": "Scoring protocols are a broad class of voting systems. Each is defined by a vector $(\\alpha_1,\\alpha_2,...,\\alpha_m)$, $\\alpha_1 \\geq \\alpha_2 \\geq >... \\geq \\alpha_m$, of integers such that each voter contributes $\\alpha_1$ points to his/her first choice, $\\alpha_2$ points to his/her second choice, and so on, and any candidate receiving the most points is a winner.   What is it about scoring-protocol election systems that makes some have the desirable property of being NP-complete to manipulate, while others can be manipulated in polynomial time? We find the complete, dichotomizing answer: Diversity of dislike. Every scoring-protocol election system having two or more point values assigned to candidates other than the favorite--i.e., having $||\\{\\alpha_i \\condition 2 \\leq i \\leq m\\}||\\geq 2$--is NP-complete to manipulate. Every other scoring-protocol election system can be manipulated in polynomial time. In effect, we show that--other than trivial systems (where all candidates alway tie), plurality voting, and plurality voting's transparently disguised translations--\\emph{every} scoring-protocol election system is NP-complete to manipulate.",
        "published": "2005-04-15T22:11:55Z",
        "link": "http://arxiv.org/abs/cs/0504075v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.1.3; F.2.2"
        ]
    },
    {
        "title": "A Short Proof that Phylogenetic Tree Reconstruction by Maximum   Likelihood is Hard",
        "authors": [
            "S. Roch"
        ],
        "summary": "Maximum likelihood is one of the most widely used techniques to infer evolutionary histories. Although it is thought to be intractable, a proof of its hardness has been lacking. Here, we give a short proof that computing the maximum likelihood tree is NP-hard by exploiting a connection between likelihood and parsimony observed by Tuffley and Steel.",
        "published": "2005-04-19T08:36:17Z",
        "link": "http://arxiv.org/abs/math/0504378v2",
        "categories": [
            "math.PR",
            "cs.CC",
            "cs.CE",
            "math.ST",
            "q-bio.PE",
            "stat.TH"
        ]
    },
    {
        "title": "Time, Space, and Energy in Reversible Computing",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "We survey results of a quarter century of work on computation by reversible general-purpose computers (in this setting Turing machines), and general reversible simulation of irreversible computations, with respect to energy-, time- and space requirements.",
        "published": "2005-04-20T17:13:33Z",
        "link": "http://arxiv.org/abs/cs/0504088v1",
        "categories": [
            "cs.CC",
            "F.1; F.2"
        ]
    },
    {
        "title": "On Optimality Condition of Complex Systems: Computational Evidence",
        "authors": [
            "Victor Korotkikh",
            "Galina Korotkikh",
            "Darryl Bond"
        ],
        "summary": "A general condition determining the optimal performance of a complex system has not yet been found and the possibility of its existence is unknown. To contribute in this direction, an optimization algorithm as a complex system is presented. The performance of the algorithm for any problem is controlled as a convex function with a single optimum. To characterize the performance optimums, certain quantities of the algorithm and the problem are suggested and interpreted as their complexities. An optimality condition of the algorithm is computationally found: if the algorithm shows its best performance for a problem, then the complexity of the algorithm is in a linear relationship with the complexity of the problem. The optimality condition provides a new perspective to the subject by recognizing that the relationship between certain quantities of the complex system and the problem may determine the optimal performance.",
        "published": "2005-04-23T06:52:50Z",
        "link": "http://arxiv.org/abs/cs/0504092v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn",
            "nlin.AO"
        ]
    },
    {
        "title": "Single-solution Random 3-SAT Instances",
        "authors": [
            "Marko Znidaric"
        ],
        "summary": "We study a class of random 3-SAT instances having exactly one solution. The properties of this ensemble considerably differ from those of a random 3-SAT ensemble. It is numerically shown that the running time of several complete and stochastic local search algorithms monotonically increases as the clause density is decreased. Therefore, there is no easy-hard-easy pattern of hardness as for standard random 3-SAT ensemble. Furthermore, the running time for short single-solution formulas increases with the problem size much faster than for random 3-SAT formulas from the phase transition region.",
        "published": "2005-04-25T10:40:32Z",
        "link": "http://arxiv.org/abs/cs/0504101v2",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "P-Selectivity, Immunity, and the Power of One Bit",
        "authors": [
            "Lane A. Hemaspaandra",
            "Leen Torenvliet"
        ],
        "summary": "We prove that P-sel, the class of all P-selective sets, is EXP-immune, but is not EXP/1-immune. That is, we prove that some infinite P-selective set has no infinite EXP-time subset, but we also prove that every infinite P-selective set has some infinite subset in EXP/1. Informally put, the immunity of P-sel is so fragile that it is pierced by a single bit of information.   The above claims follow from broader results that we obtain about the immunity of the P-selective sets. In particular, we prove that for every recursive function f, P-sel is DTIME(f)-immune. Yet we also prove that P-sel is not \\Pi_2^p/1-immune.",
        "published": "2005-04-25T15:18:17Z",
        "link": "http://arxiv.org/abs/cs/0504096v2",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Tight Lower Bounds for Query Processing on Streaming and External Memory   Data",
        "authors": [
            "Martin Grohe",
            "Christoph Koch",
            "Nicole Schweikardt"
        ],
        "summary": "We study a clean machine model for external memory and stream processing. We show that the number of scans of the external data induces a strict hierarchy (as long as work space is sufficiently small, e.g., polylogarithmic in the size of the input). We also show that neither joins nor sorting are feasible if the product of the number $r(n)$ of scans of the external memory and the size $s(n)$ of the internal memory buffers is sufficiently small, e.g., of size $o(\\sqrt[5]{n})$. We also establish tight bounds for the complexity of XPath evaluation and filtering.",
        "published": "2005-04-29T22:28:31Z",
        "link": "http://arxiv.org/abs/cs/0505002v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "H.2.3; I.7.2"
        ]
    },
    {
        "title": "Theories for TC0 and Other Small Complexity Classes",
        "authors": [
            "Phuong Nguyen",
            "Stephen Cook"
        ],
        "summary": "We present a general method for introducing finitely axiomatizable \"minimal\" two-sorted theories for various subclasses of P (problems solvable in polynomial time). The two sorts are natural numbers and finite sets of natural numbers. The latter are essentially the finite binary strings, which provide a natural domain for defining the functions and sets in small complexity classes. We concentrate on the complexity class TC^0, whose problems are defined by uniform polynomial-size families of bounded-depth Boolean circuits with majority gates. We present an elegant theory VTC^0 in which the provably-total functions are those associated with TC^0, and then prove that VTC^0 is \"isomorphic\" to a different-looking single-sorted theory introduced by Johannsen and Pollet. The most technical part of the isomorphism proof is defining binary number multiplication in terms a bit-counting function, and showing how to formalize the proofs of its algebraic properties.",
        "published": "2005-05-05T22:17:01Z",
        "link": "http://arxiv.org/abs/cs/0505013v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Computing the Rank and a Small Nullspace Basis of a Polynomial Matrix",
        "authors": [
            "Arne Storjohann",
            "Gilles Villard"
        ],
        "summary": "We reduce the problem of computing the rank and a nullspace basis of a univariate polynomial matrix to polynomial matrix multiplication. For an input n x n matrix of degree d over a field K we give a rank and nullspace algorithm using about the same number of operations as for multiplying two matrices of dimension n and degree d. If the latter multiplication is done in MM(n,d)=softO(n^omega d) operations, with omega the exponent of matrix multiplication over K, then the algorithm uses softO(MM(n,d)) operations in K. The softO notation indicates some missing logarithmic factors. The method is randomized with Las Vegas certification. We achieve our results in part through a combination of matrix Hensel high-order lifting and matrix minimal fraction reconstruction, and through the computation of minimal or small degree vectors in the nullspace seen as a K[x]-module",
        "published": "2005-05-11T18:29:00Z",
        "link": "http://arxiv.org/abs/cs/0505030v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1; F.2.1"
        ]
    },
    {
        "title": "A Note on Shared Randomness and Shared Entanglement in Communication",
        "authors": [
            "Dmytro Gavinsky"
        ],
        "summary": "We consider several models of 1-round classical and quantum communication, some of these models have not been defined before. We \"almost separate\" the models of simultaneous quantum message passing with shared entanglement and the model of simultaneous quantum message passing with shared randomness. We define a relation which can be efficiently exactly solved in the first model but cannot be solved efficiently, either exactly or in 0-error setup in the second model. In fact, our relation is exactly solvable even in a more restricted model of simultaneous classical message passing with shared entanglement.   As our second contribution we strengthen a result by Yao that a \"very short\" protocol from the model of simultaneous classical message passing with shared randomness can be simulated in the model of simultaneous quantum message passing: for a boolean function f, QII(f) \\in exp(O(RIIp(f))) log n.   We show a similar result for protocols from a (stronger) model of 1-way classical message passing with shared randomness: QII(f) \\in exp(O(RIp(f))) log n.   We demonstrate a problem whose efficient solution in the model of simultaneous quantum message passing follows from our result but not from Yao's.",
        "published": "2005-05-12T02:12:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0505088v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Beyond Hypertree Width: Decomposition Methods Without Decompositions",
        "authors": [
            "Hubie Chen",
            "Victor Dalmau"
        ],
        "summary": "The general intractability of the constraint satisfaction problem has motivated the study of restrictions on this problem that permit polynomial-time solvability. One major line of work has focused on structural restrictions, which arise from restricting the interaction among constraint scopes. In this paper, we engage in a mathematical investigation of generalized hypertree width, a structural measure that has up to recently eluded study. We obtain a number of computational results, including a simple proof of the tractability of CSP instances having bounded generalized hypertree width.",
        "published": "2005-05-12T16:53:04Z",
        "link": "http://arxiv.org/abs/cs/0505035v1",
        "categories": [
            "cs.CC",
            "cs.AI"
        ]
    },
    {
        "title": "A Verifiable Partial Key Escrow, Based on McCurley Encryption Scheme",
        "authors": [
            "Kooshiar Azimian",
            "Javad Mohajeri",
            "Mahmoud Salmasizadeh",
            "Siamak Fayyaz"
        ],
        "summary": "In this paper, firstly we propose two new concepts concerning the notion of key escrow encryption schemes: provable partiality and independency. Roughly speaking we say that a scheme has provable partiality if existing polynomial time algorithm for recovering the secret knowing escrowed information implies a polynomial time algorithm that can solve a well-known intractable problem. In addition, we say that a scheme is independent if the secret key and the escrowed information are independent. Finally, we propose a new verifiable partial key escrow, which has both of above criteria. The new scheme use McCurley encryption scheme as underlying scheme.",
        "published": "2005-05-22T12:40:51Z",
        "link": "http://arxiv.org/abs/cs/0505055v1",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "Lower Bounds on Matrix Rigidity via a Quantum Argument",
        "authors": [
            "Ronald de Wolf"
        ],
        "summary": "The rigidity of a matrix measures how many of its entries need to be changed in order to reduce its rank to some value. Good lower bounds on the rigidity of an explicit matrix would imply good lower bounds for arithmetic circuits as well as for communication complexity. Here we reprove the best known bounds on the rigidity of Hadamard matrices, due to Kashin and Razborov, using tools from quantum computing. Our proofs are somewhat simpler than earlier ones (at least for those familiar with quantum) and give slightly better constants. More importantly, they give a new approach to attack this longstanding open problem.",
        "published": "2005-05-25T09:35:38Z",
        "link": "http://arxiv.org/abs/quant-ph/0505188v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Reasoning about transfinite sequences",
        "authors": [
            "Stéphane Demri",
            "David Nowak"
        ],
        "summary": "We introduce a family of temporal logics to specify the behavior of systems with Zeno behaviors. We extend linear-time temporal logic LTL to authorize models admitting Zeno sequences of actions and quantitative temporal operators indexed by ordinals replace the standard next-time and until future-time operators. Our aim is to control such systems by designing controllers that safely work on $\\omega$-sequences but interact synchronously with the system in order to restrict their behaviors. We show that the satisfiability problem for the logics working on $\\omega^k$-sequences is EXPSPACE-complete when the integers are represented in binary, and PSPACE-complete with a unary representation. To do so, we substantially extend standard results about LTL by introducing a new class of succinct ordinal automata that can encode the interaction between the different quantitative temporal operators.",
        "published": "2005-05-26T09:33:24Z",
        "link": "http://arxiv.org/abs/cs/0505073v4",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "On the Solution of Graph Isomorphism by Dynamical Algorithms",
        "authors": [
            "Marats Golovkins"
        ],
        "summary": "In the recent years, several polynomial algorithms of a dynamical nature have been proposed to address the graph isomorphism problem. In this paper we propose a generalization of an approach exposed in cond-mat/0209112 and find that this dynamical algorithm is covered by a combinatorial approach. It is possible to infer that polynomial dynamical algorithms addressing graph isomorphism are covered by suitable polynomial combinatorial approaches and thus are tackled by the same weaknesses as the last ones.",
        "published": "2005-05-27T00:09:12Z",
        "link": "http://arxiv.org/abs/cs/0505076v1",
        "categories": [
            "cs.CC",
            "G.2.2"
        ]
    },
    {
        "title": "On computational complexity of Riemann mapping",
        "authors": [
            "Ilia Binder",
            "Mark Braverman",
            "Michael Yampolsky"
        ],
        "summary": "In this paper we consider the computational complexity of uniformizing a domain with a given computable boundary. We give nontrivial upper and lower bounds in two settings: when the approximation of boundary is given either as a list of pixels, or by a Turing Machine.",
        "published": "2005-05-27T20:17:03Z",
        "link": "http://arxiv.org/abs/math/0505617v2",
        "categories": [
            "math.CV",
            "cs.CC",
            "30C35"
        ]
    },
    {
        "title": "Application of Kolmogorov complexity and universal codes to identity   testing and nonparametric testing of serial independence for time series",
        "authors": [
            "Boris Ryabko",
            "Jaakko Astola",
            "Alex Gammerman"
        ],
        "summary": "We show that Kolmogorov complexity and such its estimators as universal codes (or data compression methods) can be applied for hypotheses testing in a framework of classical mathematical statistics. The methods for identity testing and nonparametric testing of serial independence for time series are suggested.",
        "published": "2005-05-29T17:12:47Z",
        "link": "http://arxiv.org/abs/cs/0505079v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Fast generators for the Diffie-Hellman key agreement protocol and   malicious standards",
        "authors": [
            "Boaz Tsaban"
        ],
        "summary": "The Diffie-Hellman key agreement protocol is based on taking large powers of a generator of a prime-order cyclic group. Some generators allow faster exponentiation. We show that to a large extent, using the fast generators is as secure as using a randomly chosen generator. On the other hand, we show that if there is some case in which fast generators are less secure, then this could be used by a malicious authority to generate a standard for the Diffie-Hellman key agreement protocol which has a hidden trapdoor.",
        "published": "2005-05-30T19:33:03Z",
        "link": "http://arxiv.org/abs/cs/0505082v4",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "Pairs of SAT Assignment in Random Boolean Formulae",
        "authors": [
            "Hervé Daudé",
            "Marc Mezard",
            "Thierry Mora",
            "Riccardo Zecchina"
        ],
        "summary": "We investigate geometrical properties of the random K-satisfiability problem using the notion of x-satisfiability: a formula is x-satisfiable if there exist two SAT assignments differing in Nx variables. We show the existence of a sharp threshold for this property as a function of the clause density. For large enough K, we prove that there exists a region of clause density, below the satisfiability threshold, where the landscape of Hamming distances between SAT assignments experiences a gap: pairs of SAT-assignments exist at small x, and around x=1/2, but they donot exist at intermediate values of x. This result is consistent with the clustering scenario which is at the heart of the recent heuristic analysis of satisfiability using statistical physics analysis (the cavity method), and its algorithmic counterpart (the survey propagation algorithm). The method uses elementary probabilistic arguments (first and second moment methods), and might be useful in other problems of computational and physical interest where similar phenomena appear.",
        "published": "2005-06-02T15:35:36Z",
        "link": "http://arxiv.org/abs/cond-mat/0506053v3",
        "categories": [
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "The Complexity of Kings",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Osamu Watanabe"
        ],
        "summary": "A king in a directed graph is a node from which each node in the graph can be reached via paths of length at most two. There is a broad literature on tournaments (completely oriented digraphs), and it has been known for more than half a century that all tournaments have at least one king [Lan53]. Recently, kings have proven useful in theoretical computer science, in particular in the study of the complexity of the semifeasible sets [HNP98,HT05] and in the study of the complexity of reachability problems [Tan01,NT02].   In this paper, we study the complexity of recognizing kings. For each succinctly specified family of tournaments, the king problem is known to belong to $\\Pi_2^p$ [HOZZ]. We prove that this bound is optimal: We construct a succinctly specified tournament family whose king problem is $\\Pi_2^p$-complete. It follows easily from our proof approach that the problem of testing kingship in succinctly specified graphs (which need not be tournaments) is $\\Pi_2^p$-complete. We also obtain $\\Pi_2^p$-completeness results for k-kings in succinctly specified j-partite tournaments, $k,j \\geq 2$, and we generalize our main construction to show that $\\Pi_2^p$-completeness holds for testing k-kingship in succinctly specified families of tournaments for all $k \\geq 2$.",
        "published": "2005-06-14T02:31:35Z",
        "link": "http://arxiv.org/abs/cs/0506055v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "Existentially Restricted Quantified Constraint Satisfaction",
        "authors": [
            "Hubie Chen"
        ],
        "summary": "The quantified constraint satisfaction problem (QCSP) is a powerful framework for modelling computational problems. The general intractability of the QCSP has motivated the pursuit of restricted cases that avoid its maximal complexity. In this paper, we introduce and study a new model for investigating QCSP complexity in which the types of constraints given by the existentially quantified variables, is restricted. Our primary technical contribution is the development and application of a general technology for proving positive results on parameterizations of the model, of inclusion in the complexity class coNP.",
        "published": "2005-06-14T11:57:00Z",
        "link": "http://arxiv.org/abs/cs/0506059v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Quantum Arthur-Merlin Games",
        "authors": [
            "Chris Marriott",
            "John Watrous"
        ],
        "summary": "This paper studies quantum Arthur-Merlin games, which are Arthur-Merlin games in which Arthur and Merlin can perform quantum computations and Merlin can send Arthur quantum information. As in the classical case, messages from Arthur to Merlin are restricted to be strings of uniformly generated random bits. It is proved that for one-message quantum Arthur-Merlin games, which correspond to the complexity class QMA, completeness and soundness errors can be reduced exponentially without increasing the length of Merlin's message. Previous constructions for reducing error required a polynomial increase in the length of Merlin's message. Applications of this fact include a proof that logarithmic length quantum certificates yield no increase in power over BQP and a simple proof that QMA is contained in PP. Other facts that are proved include the equivalence of three (or more) message quantum Arthur-Merlin games with ordinary quantum interactive proof systems and some basic properties concerning two-message quantum Arthur-Merlin games.",
        "published": "2005-06-15T22:13:52Z",
        "link": "http://arxiv.org/abs/cs/0506068v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.2; F.1.3"
        ]
    },
    {
        "title": "A generating function method for the average-case analysis of DPLL",
        "authors": [
            "Remi Monasson"
        ],
        "summary": "A method to calculate the average size of Davis-Putnam-Loveland-Logemann (DPLL) search trees for random computational problems is introduced, and applied to the satisfiability of random CNF formulas (SAT) and the coloring of random graph (COL) problems. We establish recursion relations for the generating functions of the average numbers of (variable or color) assignments at a given height in the search tree, which allow us to derive the asymptotics of the expected DPLL tree size, 2^{N w + o(N)}, where N is the instance size. w is calculated as a function of the input distribution parameters (ratio of clauses per variable for SAT, average vertex degree for COL), and the branching heuristics.",
        "published": "2005-06-16T09:17:21Z",
        "link": "http://arxiv.org/abs/cs/0506069v1",
        "categories": [
            "cs.CC",
            "cond-mat.dis-nn"
        ]
    },
    {
        "title": "Quantitative Models and Implicit Complexity",
        "authors": [
            "U. Dal Lago",
            "M. Hofmann"
        ],
        "summary": "We give new proofs of soundness (all representable functions on base types lies in certain complexity classes) for Elementary Affine Logic, LFPL (a language for polytime computation close to realistic functional programming introduced by one of us), Light Affine Logic and Soft Affine Logic. The proofs are based on a common semantical framework which is merely instantiated in four different ways. The framework consists of an innovative modification of realizability which allows us to use resource-bounded computations as realisers as opposed to including all Turing computable functions as is usually the case in realizability constructions. For example, all realisers in the model for LFPL are polynomially bounded computations whence soundness holds by construction of the model. The work then lies in being able to interpret all the required constructs in the model. While being the first entirely semantical proof of polytime soundness for light logi cs, our proof also provides a notable simplification of the original already semantical proof of polytime soundness for LFPL. A new result made possible by the semantic framework is the addition of polymorphism and a modality to LFPL thus allowing for an internal definition of inductive datatypes.",
        "published": "2005-06-20T16:00:10Z",
        "link": "http://arxiv.org/abs/cs/0506079v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "The Geometry of Linear Higher-Order Recursion",
        "authors": [
            "U. Dal Lago"
        ],
        "summary": "Linearity and ramification constraints have been widely used to weaken higher-order (primitive) recursion in such a way that the class of representable functions equals the class of polytime functions. We show that fine-tuning these two constraints leads to different expressive strengths, some of them lying well beyond polynomial time. This is done by introducing a new semantics, called algebraic context semantics. The framework stems from Gonthier's original work and turns out to be a versatile and powerful tool for the quantitative analysis of normalization in presence of constants and higher-order recursion.",
        "published": "2005-06-20T17:52:02Z",
        "link": "http://arxiv.org/abs/cs/0506080v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Three lines proof of the lower bound for the matrix rigidity",
        "authors": [
            "Gatis Midrijanis"
        ],
        "summary": "The rigidity of a matrix describes the minimal number of entries one has to change to reduce matrix's rank to r. We give very simple combinatorial proof of the lower bound for the rigidity of Sylvester (special case of Hadamard) matrix that matches the best known result by de Wolf(2005) for Hadamard matrices proved by quantum information theoretical arguments.",
        "published": "2005-06-20T19:54:48Z",
        "link": "http://arxiv.org/abs/cs/0506081v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Open Questions in the Theory of Semifeasible Computation",
        "authors": [
            "Piotr Faliszewski",
            "Lane A. Hemaspaandra"
        ],
        "summary": "The study of semifeasible algorithms was initiated by Selman's work a quarter of century ago [Sel79,Sel81,Sel82]. Informally put, this research stream studies the power of those sets L for which there is a deterministic (or in some cases, the function may belong to one of various nondeterministic function classes) polynomial-time function f such that when at least one of x and y belongs to L, then f(x,y) \\in L \\cap \\{x,y\\}. The intuition here is that it is saying: ``Regarding membership in L, if you put a gun to my head and forced me to bet on one of x or y as belonging to L, my money would be on f(x,y).''   In this article, we present a number of open problems from the theory of semifeasible algorithms. For each we present its background and review what partial results, if any, are known.",
        "published": "2005-06-20T22:33:51Z",
        "link": "http://arxiv.org/abs/cs/0506082v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Solving Satisfiability Problems by the Ground-State Quantum Computer",
        "authors": [
            "Wenjin Mao"
        ],
        "summary": "A quantum algorithm is proposed to solve the Satisfiability problems by the ground-state quantum computer. The scale of the energy gap of the ground-state quantum computer is analyzed for the 3-bit Exact Cover problem. The time cost of this algorithm on the general SAT problems is discussed.",
        "published": "2005-06-23T22:35:59Z",
        "link": "http://arxiv.org/abs/quant-ph/0506200v1",
        "categories": [
            "quant-ph",
            "cond-mat.mes-hall",
            "cs.CC"
        ]
    },
    {
        "title": "An Exact 2.9416^n Algorithm for the Three Domatic Number Problem",
        "authors": [
            "Tobias Riege",
            "Jörg Rothe"
        ],
        "summary": "The three domatic number problem asks whether a given undirected graph can be partitioned into at least three dominating sets, i.e., sets whose closed neighborhood equals the vertex set of the graph. Since this problem is NP-complete, no polynomial-time algorithm is known for it. The naive deterministic algorithm for this problem runs in time 3^n, up to polynomial factors. In this paper, we design an exact deterministic algorithm for this problem running in time 2.9416^n. Thus, our algorithm can handle problem instances of larger size than the naive algorithm in the same amount of time. We also present another deterministic and a randomized algorithm for this problem that both have an even better performance for graphs with small maximum degree.",
        "published": "2005-06-24T12:58:15Z",
        "link": "http://arxiv.org/abs/cs/0506090v1",
        "categories": [
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Polynomial Synthesis of Asynchronous Automata",
        "authors": [
            "Nicolas Baudru",
            "Rémi Morin"
        ],
        "summary": "Zielonka's theorem shows that each regular set of Mazurkiewicz traces can be implemented as a system of synchronized processes with a distributed control structure called asynchronous automaton. This paper gives a polynomial algorithm for the synthesis of a non-deterministic asynchronous automaton from a regular Mazurkiewicz trace language. This new construction is based on an unfolding approach that improves the complexity of Zielonka's and Pighizzini's techniques in terms of the number of states.",
        "published": "2005-06-27T06:09:37Z",
        "link": "http://arxiv.org/abs/cs/0506096v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "On the NP-Completeness of Some Graph Cluster Measures",
        "authors": [
            "Jiri Sima",
            "Satu Elisa Schaeffer"
        ],
        "summary": "Graph clustering is the problem of identifying sparsely connected dense subgraphs (clusters) in a given graph. Proposed clustering algorithms usually optimize various fitness functions that measure the quality of a cluster within the graph. Examples of such cluster measures include the conductance, the local and relative densities, and single cluster editing. We prove that the decision problems associated with the optimization tasks of finding the clusters that are optimal with respect to these fitness measures are NP-complete.",
        "published": "2005-06-29T18:12:28Z",
        "link": "http://arxiv.org/abs/cs/0506100v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "The role of Quantum Interference in Quantum Computing",
        "authors": [
            "A. Y. Shiekh"
        ],
        "summary": "Quantum interference is proposed as a tool to augment Quantum Computation.",
        "published": "2005-07-01T16:08:23Z",
        "link": "http://arxiv.org/abs/cs/0507003v3",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "The conjugacy problem and related problems in lattice-ordered groups",
        "authors": [
            "W. Charles Holland",
            "Boaz Tsaban"
        ],
        "summary": "We study, from a constructive computational point of view, the techniques used to solve the conjugacy problem in the \"generic\" lattice-ordered group Aut(R) of order automorphisms of the real line. We use these techniques in order to show that for each choice of parameters f,g in Aut(R), the equation xfx=g is effectively solvable in Aut(R).",
        "published": "2005-07-04T14:28:39Z",
        "link": "http://arxiv.org/abs/math/0507041v2",
        "categories": [
            "math.GR",
            "cs.CC",
            "math.GN"
        ]
    },
    {
        "title": "Complexity Science for Simpletons",
        "authors": [
            "Craig Alan Feinstein"
        ],
        "summary": "In this article, we shall describe some of the most interesting topics in the subject of Complexity Science for a general audience. Anyone with a solid foundation in high school mathematics (with some calculus) and an elementary understanding of computer programming will be able to follow this article. First, we shall explain the significance of the P versus NP problem and solve it. Next, we shall describe two other famous mathematics problems, the Collatz 3n+1 Conjecture and the Riemann Hypothesis, and show how both Chaitin's incompleteness theorem and Wolfram's notion of \"computational irreducibility\" are important for understanding why no one has, as of yet, solved these two problems.",
        "published": "2005-07-05T17:03:37Z",
        "link": "http://arxiv.org/abs/cs/0507008v7",
        "categories": [
            "cs.CC",
            "cs.GL",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "First-order queries on structures of bounded degree are computable with   constant delay",
        "authors": [
            "Arnaud Durand",
            "Etienne Grandjean"
        ],
        "summary": "A bounded degree structure is either a relational structure all of whose relations are of bounded degree or a functional structure involving bijective functions only. In this paper, we revisit the complexity of the evaluation problem of not necessarily Boolean first-order queries over structures of bounded degree. Query evaluation is considered here as a dynamical process. We prove that any query on bounded degree structures is $\\constantdelaylin$, i.e., can be computed by an algorithm that has two separate parts: it has a precomputation step of linear time in the size of the structure and then, it outputs all tuples one by one with a constant (i.e. depending on the size of the formula only) delay between each. Seen as a global process, this implies that queries on bounded structures can be evaluated in total time $O(f(|\\phi|).(|\\calS|+|\\phi(\\calS)|))$ and space $O(f(|\\phi|).|\\calS|)$ where $\\calS$ is the structure, $\\phi$ is the formula, $\\phi(\\calS)$ is the result of the query and $f$ is some function.   Among other things, our results generalize a result of \\cite{Seese-96} on the data complexity of the model-checking problem for bounded degree structures. Besides, the originality of our approach compared to that \\cite{Seese-96} and comparable results is that it does not rely on the Hanf's model-theoretic technic (see \\cite{Hanf-65}) and is completely effective.",
        "published": "2005-07-07T09:40:28Z",
        "link": "http://arxiv.org/abs/cs/0507020v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Anyone but Him: The Complexity of Precluding an Alternative",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "Preference aggregation in a multiagent setting is a central issue in both human and computer contexts. In this paper, we study in terms of complexity the vulnerability of preference aggregation to destructive control. That is, we study the ability of an election's chair to, through such mechanisms as voter/candidate addition/suppression/partition, ensure that a particular candidate (equivalently, alternative) does not win. And we study the extent to which election systems can make it impossible, or computationally costly (NP-complete), for the chair to execute such control. Among the systems we study--plurality, Condorcet, and approval voting--we find cases where systems immune or computationally resistant to a chair choosing the winner nonetheless are vulnerable to the chair blocking a victory. Beyond that, we see that among our studied systems no one system offers the best protection against destructive control. Rather, the choice of a preference aggregation system will depend closely on which types of control one wishes to be protected against. We also find concrete cases where the complexity of or susceptibility to control varies dramatically based on the choice among natural tie-handling rules.",
        "published": "2005-07-09T18:20:00Z",
        "link": "http://arxiv.org/abs/cs/0507027v4",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Landscape of solutions in constraint satisfaction problems",
        "authors": [
            "Marc Mezard",
            "Matteo Palassini",
            "Olivier Rivoire"
        ],
        "summary": "We present a theoretical framework for characterizing the geometrical properties of the space of solutions in constraint satisfaction problems, together with practical algorithms for studying this structure on particular instances. We apply our method to the coloring problem, for which we obtain the total number of solutions and analyze in detail the distribution of distances between solutions.",
        "published": "2005-07-19T15:52:08Z",
        "link": "http://arxiv.org/abs/cond-mat/0507451v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Redundancy in Logic III: Non-Mononotonic Reasoning",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "Results about the redundancy of circumscriptive and default theories are presented. In particular, the complexity of establishing whether a given theory is redundant is establihsed.",
        "published": "2005-07-19T19:25:11Z",
        "link": "http://arxiv.org/abs/cs/0507048v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Finite automata for testing uniqueness of Eulerian trails",
        "authors": [
            "Qiang Li",
            "Hui-Min Xie"
        ],
        "summary": "We investigate the condition under which the Eulerian trail of a digraph is unique, and design a finite automaton to examine it. The algorithm is effective, for if the condition is violated, it will be noticed immediately without the need to trace through the whole trail.",
        "published": "2005-07-20T18:55:45Z",
        "link": "http://arxiv.org/abs/cs/0507052v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.3"
        ]
    },
    {
        "title": "A new sibling of BQP",
        "authors": [
            "Tereza Tusarova"
        ],
        "summary": "We present a new quantum complexity class, called MQ^2, which is contained in AWPP. This class has a compact and simple mathematical definition, involving only polynomial-time computable functions and a unitarity condition. It contains both Deutsch-Jozsa's and Shor's algorithm, while its relation to BQP is unknown. This shows that in the complexity class hierarchy, BQP is not an extraordinary isolated island, but has ''siblings'' which as well can solve prime-factorization.",
        "published": "2005-07-21T22:36:41Z",
        "link": "http://arxiv.org/abs/cs/0507057v1",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Data complexity of answering conjunctive queries over SHIQ knowledge   bases",
        "authors": [
            "M. Magdalena Ortiz de la Fuente",
            "Diego Calvanese",
            "Thomas Eiter",
            "Enrico Franconi"
        ],
        "summary": "An algorithm for answering conjunctive queries over SHIQ knowledge bases that is coNP in data complexity is given. The algorithm is based on the tableau algorithm for reasoning with individuals in SHIQ. The blocking conditions of the tableau are weakened in such a way that the set of models the modified algorithm yields suffices to check query entailment. The modified blocking conditions are based on the ones proposed by Levy and Rousset for reasoning with Horn Rules in the description logic ALCNR.",
        "published": "2005-07-22T15:43:07Z",
        "link": "http://arxiv.org/abs/cs/0507059v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Grover's Quantum Search Algorithm and Diophantine Approximation",
        "authors": [
            "Shahar Dolev",
            "Itamar Pitowsky",
            "Boaz Tamir"
        ],
        "summary": "In a fundamental paper [Phys. Rev. Lett. 78, 325 (1997)] Grover showed how a quantum computer can find a single marked object in a database of size N by using only O(N^{1/2}) queries of the oracle that identifies the object. His result was generalized to the case of finding one object in a subset of marked elements. We consider the following computational problem: A subset of marked elements is given whose number of elements is either M or K, M<K, our task is to determine which is the case. We show how to solve this problem with a high probability of success using only iterations of Grover's basic step (and no other algorithm). Let m be the required number of iterations; we prove that under certain restrictions on the sizes of M and K the estimation m < (2N^{1/2})/(K^{1/2}-M^{1/2}) obtains. This bound sharpens previous results and is known to be optimal up to a constant factor. Our method involves simultaneous Diophantine approximations, so that Grover's algorithm is conceptualized as an orbit of an ergodic automorphism of the torus. We comment on situations where the algorithm may be slow, and note the similarity between these cases and the problem of small divisors in classical mechanics.",
        "published": "2005-07-25T08:51:06Z",
        "link": "http://arxiv.org/abs/quant-ph/0507234v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Theoretical cryptanalysis of the Klimov-Shamir number generator TF-1",
        "authors": [
            "Boaz Tsaban"
        ],
        "summary": "The internal state of the Klimov-Shamir number generator TF-1 consists of four words of size w bits each, whereas its intended strength is 2^{2w}. We exploit an asymmetry in its output function to show that the internal state can be recovered after having 2^w outputs, using 2^{1.5w} operations. For w=32 the attack is practical, but for their recommended w=64 it is only of theoretical interest.",
        "published": "2005-07-26T15:30:17Z",
        "link": "http://arxiv.org/abs/cs/0507063v2",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum Minimal One Way Information: Relative Hardness and Quantum   Advantage of Combinatorial Tasks",
        "authors": [
            "Harumichi Nishimura",
            "Tomoyuki Yamakami"
        ],
        "summary": "Two-party one-way quantum communication has been extensively studied in the recent literature. We target the size of minimal information that is necessary for a feasible party to finish a given combinatorial task, such as distinction of instances, using one-way communication from another party. This type of complexity measure has been studied under various names: advice complexity, Kolmogorov complexity, distinguishing complexity, and instance complexity. We present a general framework focusing on underlying combinatorial takes to study these complexity measures using quantum information processing. We introduce the key notions of relative hardness and quantum advantage, which provide the foundations for task-based quantum minimal one-way information complexity theory.",
        "published": "2005-07-28T08:57:34Z",
        "link": "http://arxiv.org/abs/quant-ph/0507270v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Dimensions of Copeland-Erdos Sequences",
        "authors": [
            "Xiaoyang Gu",
            "Jack H. Lutz",
            "Philippe Moser"
        ],
        "summary": "The base-$k$ {\\em Copeland-Erd\\\"os sequence} given by an infinite set $A$ of positive integers is the infinite sequence $\\CE_k(A)$ formed by concatenating the base-$k$ representations of the elements of $A$ in numerical order. This paper concerns the following four quantities.   The {\\em finite-state dimension} $\\dimfs (\\CE_k(A))$, a finite-state version of classical Hausdorff dimension introduced in 2001.   The {\\em finite-state strong dimension} $\\Dimfs(\\CE_k(A))$, a finite-state version of classical packing dimension introduced in 2004. This is a dual of $\\dimfs(\\CE_k(A))$ satisfying $\\Dimfs(\\CE_k(A))$ $\\geq \\dimfs(\\CE_k(A))$.   The {\\em zeta-dimension} $\\Dimzeta(A)$, a kind of discrete fractal dimension discovered many times over the past few decades.   The {\\em lower zeta-dimension} $\\dimzeta(A)$, a dual of $\\Dimzeta(A)$ satisfying $\\dimzeta(A)\\leq \\Dimzeta(A)$.   We prove the following.   $\\dimfs(\\CE_k(A))\\geq \\dimzeta(A)$. This extends the 1946 proof by Copeland and Erd\\\"os that the sequence $\\CE_k(\\mathrm{PRIMES})$ is Borel normal.   $\\Dimfs(\\CE_k(A))\\geq \\Dimzeta(A)$.   These bounds are tight in the strong sense that these four quantities can have (simultaneously) any four values in $[0,1]$ satisfying the four above-mentioned inequalities.",
        "published": "2005-07-30T04:30:05Z",
        "link": "http://arxiv.org/abs/cs/0508001v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Criticality and Universality in the Unit-Propagation Search Rule",
        "authors": [
            "Christophe Deroulers",
            "Rémi Monasson"
        ],
        "summary": "The probability Psuccess(alpha, N) that stochastic greedy algorithms successfully solve the random SATisfiability problem is studied as a function of the ratio alpha of constraints per variable and the number N of variables. These algorithms assign variables according to the unit-propagation (UP) rule in presence of constraints involving a unique variable (1-clauses), to some heuristic (H) prescription otherwise. In the infinite N limit, Psuccess vanishes at some critical ratio alpha\\_H which depends on the heuristic H. We show that the critical behaviour is determined by the UP rule only. In the case where only constraints with 2 and 3 variables are present, we give the phase diagram and identify two universality classes: the power law class, where Psuccess[alpha\\_H (1+epsilon N^{-1/3}), N] ~ A(epsilon)/N^gamma; the stretched exponential class, where Psuccess[alpha\\_H (1+epsilon N^{-1/3}), N] ~ exp[-N^{1/6} Phi(epsilon)]. Which class is selected depends on the characteristic parameters of input data. The critical exponent gamma is universal and calculated; the scaling functions A and Phi weakly depend on the heuristic H and are obtained from the solutions of reaction-diffusion equations for 1-clauses. Computation of some non-universal corrections allows us to match numerical results with good precision. The critical behaviour for constraints with >3 variables is given. Our results are interpreted in terms of dynamical graph percolation and we argue that they should apply to more general situations where UP is used.",
        "published": "2005-08-04T10:50:09Z",
        "link": "http://arxiv.org/abs/cond-mat/0508125v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "The Phase Transition in Exact Cover",
        "authors": [
            "Vamsi Kalapala",
            "Cris Moore"
        ],
        "summary": "We study EC3, a variant of Exact Cover which is equivalent to Positive 1-in-3 SAT. Random instances of EC3 were recently used as benchmarks for simulations of an adiabatic quantum algorithm. Empirical results suggest that EC3 has a phase transition from satisfiability to unsatisfiability when the number of clauses per variable r exceeds some threshold r* ~= 0.62 +- 0.01. Using the method of differential equations, we show that if r <= 0.546 w.h.p. a random instance of EC3 is satisfiable. Combined with previous results this limits the location of the threshold, if it exists, to the range 0.546 < r* < 0.644.",
        "published": "2005-08-04T15:01:48Z",
        "link": "http://arxiv.org/abs/cs/0508037v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "A Universal Scaling Theory for Complexity of Analog Computation",
        "authors": [
            "Yaniv S. Avizrats",
            "Joshua Feinberg",
            "Shmuel Fishman"
        ],
        "summary": "We discuss the computational complexity of solving linear programming problems by means of an analog computer. The latter is modeled by a dynamical system which converges to the optimal vertex solution. We analyze various probability ensembles of linear programming problems. For each one of these we obtain numerically the probability distribution functions of certain quantities which measure the complexity. Remarkably, in the asymptotic limit of very large problems, each of these probability distribution functions reduces to a universal scaling function, depending on a single scaling variable and independent of the details of its parent probability ensemble. These functions are reminiscent of the scaling functions familiar in the theory of phase transitions. The results reported here extend analytical and numerical results obtained recently for the Gaussian ensemble.",
        "published": "2005-08-05T15:53:05Z",
        "link": "http://arxiv.org/abs/cond-mat/0508152v1",
        "categories": [
            "cond-mat.other",
            "cs.CC"
        ]
    },
    {
        "title": "Real Hypercomputation and Continuity",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "By the sometimes so-called 'Main Theorem' of Recursive Analysis, every computable real function is necessarily continuous. We wonder whether and which kinds of HYPERcomputation allow for the effective evaluation of also discontinuous f:R->R. More precisely the present work considers the following three super-Turing notions of real function computability:   * relativized computation; specifically given oracle access to the Halting Problem 0' or its jump 0'';   * encoding real input x and/or output y=f(x) in weaker ways also related to the Arithmetic Hierarchy;   * non-deterministic computation.   It turns out that any f:R->R computable in the first or second sense is still necessarily continuous whereas the third type of hypercomputation does provide the required power to evaluate for instance the discontinuous sign function.",
        "published": "2005-08-15T14:18:36Z",
        "link": "http://arxiv.org/abs/cs/0508069v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.1.1; F.1.2; F.4.1"
        ]
    },
    {
        "title": "Every decision tree has an influential variable",
        "authors": [
            "Ryan O'Donnell",
            "Michael Saks",
            "Oded Schramm",
            "Rocco A. Servedio"
        ],
        "summary": "We prove that for any decision tree calculating a boolean function $f:\\{-1,1\\}^n\\to\\{-1,1\\}$, \\[ \\Var[f] \\le \\sum_{i=1}^n \\delta_i \\Inf_i(f), \\] where $\\delta_i$ is the probability that the $i$th input variable is read and $\\Inf_i(f)$ is the influence of the $i$th variable on $f$. The variance, influence and probability are taken with respect to an arbitrary product measure on $\\{-1,1\\}^n$. It follows that the minimum depth of a decision tree calculating a given balanced function is at least the reciprocal of the largest influence of any input variable. Likewise, any balanced boolean function with a decision tree of depth $d$ has a variable with influence at least $\\frac{1}{d}$. The only previous nontrivial lower bound known was $\\Omega(d 2^{-d})$. Our inequality has many generalizations, allowing us to prove influence lower bounds for randomized decision trees, decision trees on arbitrary product probability spaces, and decision trees with non-boolean outputs. As an application of our results we give a very easy proof that the randomized query complexity of nontrivial monotone graph properties is at least $\\Omega(v^{4/3}/p^{1/3})$, where $v$ is the number of vertices and $p \\leq \\half$ is the critical threshold probability. This supersedes the milestone $\\Omega(v^{4/3})$ bound of Hajnal and is sometimes superior to the best known lower bounds of Chakrabarti-Khot and Friedgut-Kahn-Wigderson.",
        "published": "2005-08-16T01:00:05Z",
        "link": "http://arxiv.org/abs/cs/0508071v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "math.PR"
        ]
    },
    {
        "title": "Toward accurate polynomial evaluation in rounded arithmetic",
        "authors": [
            "James Demmel",
            "Ioana Dumitriu",
            "Olga Holtz"
        ],
        "summary": "Given a multivariate real (or complex) polynomial $p$ and a domain $\\cal D$, we would like to decide whether an algorithm exists to evaluate $p(x)$ accurately for all $x \\in {\\cal D}$ using rounded real (or complex) arithmetic. Here ``accurately'' means with relative error less than 1, i.e., with some correct leading digits. The answer depends on the model of rounded arithmetic: We assume that for any arithmetic operator $op(a,b)$, for example $a+b$ or $a \\cdot b$, its computed value is $op(a,b) \\cdot (1 + \\delta)$, where $| \\delta |$ is bounded by some constant $\\epsilon$ where $0 < \\epsilon \\ll 1$, but $\\delta$ is otherwise arbitrary. This model is the traditional one used to analyze the accuracy of floating point algorithms.Our ultimate goal is to establish a decision procedure that, for any $p$ and $\\cal D$, either exhibits an accurate algorithm or proves that none exists. In contrast to the case where numbers are stored and manipulated as finite bit strings (e.g., as floating point numbers or rational numbers) we show that some polynomials $p$ are impossible to evaluate accurately. The existence of an accurate algorithm will depend not just on $p$ and $\\cal D$, but on which arithmetic operators and which constants are are available and whether branching is permitted. Toward this goal, we present necessary conditions on $p$ for it to be accurately evaluable on open real or complex domains ${\\cal D}$. We also give sufficient conditions, and describe progress toward a complete decision procedure. We do present a complete decision procedure for homogeneous polynomials $p$ with integer coefficients, ${\\cal D} = \\C^n$, and using only the arithmetic operations $+$, $-$ and $\\cdot$.",
        "published": "2005-08-18T17:17:20Z",
        "link": "http://arxiv.org/abs/math/0508350v2",
        "categories": [
            "math.NA",
            "cs.CC",
            "65Y20, 68Q05, 68Q25, 65F30, 68W40, 68W25"
        ]
    },
    {
        "title": "Asymptotically fast polynomial matrix algorithms for multivariable   systems",
        "authors": [
            "Claude-Pierre Jeannerod",
            "Gilles Villard"
        ],
        "summary": "We present the asymptotically fastest known algorithms for some basic problems on univariate polynomial matrices: rank, nullspace, determinant, generic inverse, reduced form. We show that they essentially can be reduced to two computer algebra techniques, minimal basis computations and matrix fraction expansion/reconstruction, and to polynomial matrix multiplication. Such reductions eventually imply that all these problems can be solved in about the same amount of time as polynomial matrix multiplication.",
        "published": "2005-08-25T13:52:56Z",
        "link": "http://arxiv.org/abs/cs/0508113v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1; F.2.1"
        ]
    },
    {
        "title": "A new quantum lower bound method, with an application to strong direct   product theorem for quantum search",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We present a new method for proving lower bounds on quantum query algorithms. The new method is an extension of adversary method, by analyzing the eigenspace structure of the problem.   Using the new method, we prove a strong direct product theorem for quantum search. This result was previously proven by Klauck, Spalek and de Wolf (quant-ph/0402123) using polynomials method. No proof using adversary method was known before.",
        "published": "2005-08-26T16:28:57Z",
        "link": "http://arxiv.org/abs/quant-ph/0508200v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Entanglement in Interactive Proof Systems with Binary Answers",
        "authors": [
            "Stephanie Wehner"
        ],
        "summary": "If two classical provers share an entangled state, the resulting interactive proof system is significantly weakened [quant-ph/0404076]. We show that for the case where the verifier computes the XOR of two binary answers, the resulting proof system is in fact no more powerful than a system based on a single quantum prover: +MIP*[2] is contained in QIP(2). This also implies that +MIP*[2] is contained in EXP which was previously shown using a different method [Presentation of Cleve et al. at CCC'04]. This contrasts with an interactive proof system where the two provers do not share entanglement. In that case, +MIP[2] = NEXP for certain soundness and completeness parameters [quant-ph/0404076].",
        "published": "2005-08-26T18:42:25Z",
        "link": "http://arxiv.org/abs/quant-ph/0508201v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Decompositions of graphs of functions and efficient iterations of lookup   tables",
        "authors": [
            "Boaz Tsaban"
        ],
        "summary": "We show that every function f implemented as a lookup table can be implemented such that the computational complexity of evaluating f^m(x) is small, independently of m and x. The implementation only increases the storage space by a small_constant_ factor.",
        "published": "2005-08-31T17:56:40Z",
        "link": "http://arxiv.org/abs/cs/0508133v4",
        "categories": [
            "cs.CC",
            "cs.CR"
        ]
    },
    {
        "title": "NP-hardness of the cluster minimization problem revisited",
        "authors": [
            "A. B. Adib"
        ],
        "summary": "The computational complexity of the \"cluster minimization problem\" is revisited [L. T. Wille and J. Vennik, J. Phys. A 18, L419 (1985)]. It is argued that the original NP-hardness proof does not apply to pairwise potentials of physical interest, such as those that depend on the geometric distance between the particles. A geometric analog of the original problem is formulated, and a new proof for such potentials is provided by polynomial time transformation from the independent set problem for unit disk graphs. Limitations of this formulation are pointed out, and new subproblems that bear more direct consequences to the numerical study of clusters are suggested.",
        "published": "2005-09-06T14:09:15Z",
        "link": "http://arxiv.org/abs/cs/0509016v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "physics.chem-ph"
        ]
    },
    {
        "title": "A Simple Model to Generate Hard Satisfiable Instances",
        "authors": [
            "Ke Xu",
            "Frederic Boussemart",
            "Fred Hemery",
            "Christophe Lecoutre"
        ],
        "summary": "In this paper, we try to further demonstrate that the models of random CSP instances proposed by [Xu and Li, 2000; 2003] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisfiable instances whose hardness is similar to unforced satisfiable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods.",
        "published": "2005-09-12T13:30:16Z",
        "link": "http://arxiv.org/abs/cs/0509032v1",
        "categories": [
            "cs.AI",
            "cond-mat.stat-mech",
            "cs.CC",
            "F.2.2; I.2.8"
        ]
    },
    {
        "title": "Computing over the Reals: Foundations for Scientific Computing",
        "authors": [
            "Mark Braverman",
            "Stephen Cook"
        ],
        "summary": "We give a detailed treatment of the ``bit-model'' of computability and complexity of real functions and subsets of R^n, and argue that this is a good way to formalize many problems of scientific computation. In the introduction we also discuss the alternative Blum-Shub-Smale model. In the final section we discuss the issue of whether physical systems could defeat the Church-Turing Thesis.",
        "published": "2005-09-14T17:58:09Z",
        "link": "http://arxiv.org/abs/cs/0509042v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.1; G.0"
        ]
    },
    {
        "title": "Cluster Computing and the Power of Edge Recognition",
        "authors": [
            "Lane A. Hemaspaandra",
            "Christopher M. Homan",
            "Sven Kosub"
        ],
        "summary": "We study the robustness--the invariance under definition changes--of the cluster class CL#P [HHKW05]. This class contains each #P function that is computed by a balanced Turing machine whose accepting paths always form a cluster with respect to some length-respecting total order with efficient adjacency checks. The definition of CL#P is heavily influenced by the defining paper's focus on (global) orders. In contrast, we define a cluster class, CLU#P, to capture what seems to us a more natural model of cluster computing. We prove that the naturalness is costless: CL#P = CLU#P. Then we exploit the more natural, flexible features of CLU#P to prove new robustness results for CL#P and to expand what is known about the closure properties of CL#P.   The complexity of recognizing edges--of an ordered collection of computation paths or of a cluster of accepting computation paths--is central to this study. Most particularly, our proofs exploit the power of unique discovery of edges--the ability of nondeterministic functions to, in certain settings, discover on exactly one (in some cases, on at most one) computation path a critical piece of information regarding edges of orderings or clusters.",
        "published": "2005-09-19T21:25:47Z",
        "link": "http://arxiv.org/abs/cs/0509060v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.3; F.1.1; F.1.2; G.2.1"
        ]
    },
    {
        "title": "Distance-2 Edge Coloring is NP-Complete",
        "authors": [
            "Jeff Erickson",
            "Shripad Thite",
            "David P. Bunde"
        ],
        "summary": "We prove that it is NP-complete to determine whether there exists a distance-2 edge coloring (strong edge coloring) with 5 colors of a bipartite 2-inductive graph with girth 6 and maximum degree 3.",
        "published": "2005-09-30T13:15:58Z",
        "link": "http://arxiv.org/abs/cs/0509100v1",
        "categories": [
            "cs.DM",
            "cs.CC"
        ]
    },
    {
        "title": "Candidate One-Way Functions and One-Way Permutations Based on Quasigroup   String Transformations",
        "authors": [
            "Danilo Gligoroski"
        ],
        "summary": "In this paper we propose a definition and construction of a new family of one-way candidate functions ${\\cal R}_N:Q^N \\to Q^N$, where $Q=\\{0,1,...,s-1\\}$ is an alphabet with $s$ elements. Special instances of these functions can have the additional property to be permutations (i.e. one-way permutations). These one-way functions have the property that for achieving the security level of $2^n$ computations in order to invert them, only $n$ bits of input are needed. The construction is based on quasigroup string transformations. Since quasigroups in general do not have algebraic properties such as associativity, commutativity, neutral elements, inverting these functions seems to require exponentially many readings from the lookup table that defines them (a Latin Square) in order to check the satisfiability for the initial conditions, thus making them natural candidates for one-way functions.",
        "published": "2005-10-07T00:29:00Z",
        "link": "http://arxiv.org/abs/cs/0510018v1",
        "categories": [
            "cs.CR",
            "cs.CC"
        ]
    },
    {
        "title": "Gowers Uniformity, Influence of Variables, and PCPs",
        "authors": [
            "Alex Samorodnitsky",
            "Luca Trevisan"
        ],
        "summary": "Gowers introduced, for d\\geq 1, the notion of dimension-d uniformity U^d(f) of a function f: G -> \\C, where G is a finite abelian group and \\C are the complex numbers. Roughly speaking, if U^d(f) is small, then f has certain \"pseudorandomness\" properties.   We prove the following property of functions with large U^d(f). Write G=G_1 x >... x G_n as a product of groups. If a bounded balanced function f:G_1 x ... x G_n -> \\C is such that U^{d} (f) > epsilon, then one of the coordinates of f has influence at least epsilon/2^{O(d)}.   The Gowers inner product of a collection of functions is a related notion of pseudorandomness. We prove that if a collection of bounded functions has large Gowers inner product, and at least one function in the collection is balanced, then there is a variable that has high influence for at least four of the functions in the collection.   Finally, we relate the acceptance probability of the \"hypergraph long-code test\" proposed by Samorodnitsky and Trevisan to the Gowers inner product of the functions being tested and we deduce applications to the construction of Probabilistically Checkable Proofs and to hardness of approximation.",
        "published": "2005-10-12T21:53:10Z",
        "link": "http://arxiv.org/abs/math/0510264v1",
        "categories": [
            "math.CO",
            "cs.CC"
        ]
    },
    {
        "title": "Counting Solutions to Binomial Complete Intersections",
        "authors": [
            "Eduardo Cattani",
            "Alicia Dickenstein"
        ],
        "summary": "We study the problem of counting the total number of affine solutions of a system of n binomials in n variables over an algebraically closed field of characteristic zero. We show that we may decide in polynomial time if that number is finite. We give a combinatorial formula for computing the total number of affine solutions (with or without multiplicity) from which we deduce that this counting problem is #P-complete. We discuss special cases in which this formula may be computed in polynomial time; in particular, this is true for generic exponent vectors.",
        "published": "2005-10-25T02:23:18Z",
        "link": "http://arxiv.org/abs/math/0510520v2",
        "categories": [
            "math.AC",
            "cs.CC",
            "math.CO"
        ]
    },
    {
        "title": "QMA/qpoly Is Contained In PSPACE/poly: De-Merlinizing Quantum Protocols",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "This paper introduces a new technique for removing existential quantifiers over quantum states. Using this technique, we show that there is no way to pack an exponential number of bits into a polynomial-size quantum state, in such a way that the value of any one of those bits can later be proven with the help of a polynomial-size quantum witness. We also show that any problem in QMA with polynomial-size quantum advice, is also in PSPACE with polynomial-size classical advice. This builds on our earlier result that BQP/qpoly is contained in PP/poly, and offers an intriguing counterpoint to the recent discovery of Raz that QIP/qpoly = ALL. Finally, we show that QCMA/qpoly is contained in PP/poly and that QMA/rpoly = QMA/poly.",
        "published": "2005-10-31T00:55:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0510230v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Context Semantics, Linear Logic and Computational Complexity",
        "authors": [
            "Ugo Dal Lago"
        ],
        "summary": "We show that context semantics can be fruitfully applied to the quantitative analysis of proof normalization in linear logic. In particular, context semantics lets us define the weight of a proof-net as a measure of its inherent complexity: it is both an upper bound to normalization time (modulo a polynomial overhead, independently on the reduction strategy) and a lower bound to the number of steps to normal form (for certain reduction strategies). Weights are then exploited in proving strong soundness theorems for various subsystems of linear logic, namely elementary linear logic, soft linear logic and light linear logic.",
        "published": "2005-10-31T11:05:41Z",
        "link": "http://arxiv.org/abs/cs/0510092v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Bounded-Error Quantum State Identification and Exponential Separations   in Communication Complexity",
        "authors": [
            "Dmytro Gavinsky",
            "Julia Kempe",
            "Oded Regev",
            "Ronald de Wolf"
        ],
        "summary": "We consider the problem of bounded-error quantum state identification: given either state \\alpha_0 or state \\alpha_1, we are required to output `0', `1' or `?' (\"don't know\"), such that conditioned on outputting `0' or `1', our guess is correct with high probability. The goal is to maximize the probability of not outputting `?'. We prove a direct product theorem: if we're given two such problems, with optimal probabilities a and b, respectively, and the states in the first problem are pure, then the optimal probability for the joint bounded-error state identification problem is O(ab). Our proof is based on semidefinite programming duality and may be of wider interest.   Using this result, we present two exponential separations in the simultaneous message passing model of communication complexity. Both are shown in the strongest possible sense. First, we describe a relation that can be computed with O(log n) classical bits of communication in the presence of shared randomness, but needs Omega(n^{1/3}) communication if the parties don't share randomness, even if communication is quantum. This shows the optimality of Yao's recent exponential simulation of shared-randomness protocols by quantum protocols without shared randomness. Second, we describe a relation that can be computed with O(log n) classical bits of communication in the presence of shared entanglement, but needs Omega((n/log n)^{1/3}) communication if the parties share randomness but no entanglement, even if communication is quantum. This is the first example in communication complexity of a situation where entanglement buys you much more than quantum communication does.",
        "published": "2005-11-02T16:01:30Z",
        "link": "http://arxiv.org/abs/quant-ph/0511013v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "The Impact of Social Networks on Multi-Agent Recommender Systems",
        "authors": [
            "Hamilton Link",
            "Jared Saia",
            "Terran Lane",
            "Randall A. LaViolette"
        ],
        "summary": "Awerbuch et al.'s approach to distributed recommender systems (DRSs) is to have agents sample products at random while randomly querying one another for the best item they have found; we improve upon this by adding a communication network. Agents can only communicate with their immediate neighbors in the network, but neighboring agents may or may not represent users with common interests. We define two network structures: in the ``mailing-list model,'' agents representing similar users form cliques, while in the ``word-of-mouth model'' the agents are distributed randomly in a scale-free network (SFN). In both models, agents tell their neighbors about satisfactory products as they are found. In the word-of-mouth model, knowledge of items propagates only through interested agents, and the SFN parameters affect the system's performance. We include a summary of our new results on the character and parameters of random subgraphs of SFNs, in particular SFNs with power-law degree distributions down to minimum degree 1. These networks are not as resilient as Cohen et al. originally suggested. In the case of the widely-cited ``Internet resilience'' result, high failure rates actually lead to the orphaning of half of the surviving nodes after 60% of the network has failed and the complete disintegration of the network at 90%. We show that given an appropriate network, the communication network reduces the number of sampled items, the number of messages sent, and the amount of ``spam.'' We conclude that in many cases DRSs will be useful for sharing information in a multi-agent learning system.",
        "published": "2005-11-02T23:44:34Z",
        "link": "http://arxiv.org/abs/cs/0511011v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.MA",
            "I.2.6; I.2.11"
        ]
    },
    {
        "title": "Quantum Weakly Nondeterministic Communication Complexity",
        "authors": [
            "Francois Le Gall"
        ],
        "summary": "We study the weakest model of quantum nondeterminism in which a classical proof has to be checked with probability one by a quantum protocol. We show the first separation between classical nondeterministic communication complexity and this model of quantum nondeterministic communication complexity for a total function. This separation is quadratic.",
        "published": "2005-11-03T13:44:27Z",
        "link": "http://arxiv.org/abs/quant-ph/0511025v3",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Short Quantum Games",
        "authors": [
            "Gus Gutoski"
        ],
        "summary": "In this thesis we introduce quantum refereed games, which are quantum interactive proof systems with two competing provers. We focus on a restriction of this model that we call \"short quantum games\" and we prove an upper bound and a lower bound on the expressive power of these games.   For the lower bound, we prove that every language having an ordinary quantum interactive proof system also has a short quantum game. An important part of this proof is the establishment of a quantum measurement that reliably distinguishes between quantum states chosen from disjoint convex sets.   For the upper bound, we show that certain types of quantum refereed games, including short quantum games, are decidable in deterministic exponential time by supplying a separation oracle for use with the ellipsoid method for convex feasibility.",
        "published": "2005-11-03T23:00:18Z",
        "link": "http://arxiv.org/abs/cs/0511017v1",
        "categories": [
            "cs.CC",
            "quant-ph"
        ]
    },
    {
        "title": "The Linear Arrangement Problem Parameterized Above Guaranteed Value",
        "authors": [
            "G. Gutin",
            "A. Rafiey",
            "S. Szeider",
            "A. Yeo"
        ],
        "summary": "A linear arrangement (LA) is an assignment of distinct integers to the vertices of a graph. The cost of an LA is the sum of lengths of the edges of the graph, where the length of an edge is defined as the absolute value of the difference of the integers assigned to its ends. For many application one hopes to find an LA with small cost. However, it is a classical NP-complete problem to decide whether a given graph $G$ admits an LA of cost bounded by a given integer. Since every edge of $G$ contributes at least one to the cost of any LA, the problem becomes trivially fixed-parameter tractable (FPT) if parameterized by the upper bound of the cost. Fernau asked whether the problem remains FPT if parameterized by the upper bound of the cost minus the number of edges of the given graph; thus whether the problem is FPT ``parameterized above guaranteed value.'' We answer this question positively by deriving an algorithm which decides in time $O(m+n+5.88^k)$ whether a given graph with $m$ edges and $n$ vertices admits an LA of cost at most $m+k$ (the algorithm computes such an LA if it exists). Our algorithm is based on a procedure which generates a problem kernel of linear size in linear time for a connected graph $G$. We also prove that more general parameterized LA problems stated by Serna and Thilikos are not FPT, unless P=NP.",
        "published": "2005-11-07T17:47:55Z",
        "link": "http://arxiv.org/abs/cs/0511030v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "The Role of Redundancy in the Robustness of Random Boolean Networks",
        "authors": [
            "Carlos Gershenson",
            "Stuart A. Kauffman",
            "Ilya Shmulevich"
        ],
        "summary": "Evolution depends on the possibility of successfully exploring fitness landscapes via mutation and recombination. With these search procedures, exploration is difficult in \"rugged\" fitness landscapes, where small mutations can drastically change functionalities in an organism. Random Boolean networks (RBNs), being general models, can be used to explore theories of how evolution can take place in rugged landscapes; or even change the landscapes.   In this paper, we study the effect that redundant nodes have on the robustness of RBNs. Using computer simulations, we have found that the addition of redundant nodes to RBNs increases their robustness. We conjecture that redundancy is a way of \"smoothening\" fitness landscapes. Therefore, redundancy can facilitate evolutionary searches. However, too much redundancy could reduce the rate of adaptation of an evolutionary process. Our results also provide supporting evidence in favour of Kauffman's conjecture (Kauffman, 2000, p.195).",
        "published": "2005-11-09T19:48:17Z",
        "link": "http://arxiv.org/abs/nlin/0511018v2",
        "categories": [
            "nlin.AO",
            "cond-mat.stat-mech",
            "cs.CC",
            "nlin.CG",
            "physics.bio-ph",
            "q-bio.MN",
            "q-bio.QM"
        ]
    },
    {
        "title": "Various Solutions to the Firing Squad Synchronization Problems",
        "authors": [
            "J. Gruska",
            "S. La Torre",
            "M. Napoli",
            "M. Parente"
        ],
        "summary": "We present different classes of solutions to the Firing Squad Synchronization Problem on networks of different shapes. The nodes are finite state processors that work at unison discrete steps. The networks considered are the line, the ring and the square. For all of these models we have considered one and two-way communication modes and also constrained the quantity of information that adjacent processors can exchange each step. We are given a particular time expressed as a function of the number of nodes of the network, $f(n)$ and present synchronization algorithms in time $n^2$, $n \\log n$, $n\\sqrt n$, $2^n$. The solutions are presented as {\\em signals} that are used as building blocks to compose new solutions for all times expressed by polynomials with nonnegative coefficients.",
        "published": "2005-11-12T06:44:20Z",
        "link": "http://arxiv.org/abs/cs/0511044v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "An Invariant Cost Model for the Lambda Calculus",
        "authors": [
            "Ugo Dal Lago",
            "Simone Martini"
        ],
        "summary": "We define a new cost model for the call-by-value lambda-calculus satisfying the invariance thesis. That is, under the proposed cost model, Turing machines and the call-by-value lambda-calculus can simulate each other within a polynomial time overhead. The model only relies on combinatorial properties of usual beta-reduction, without any reference to a specific machine or evaluator. In particular, the cost of a single beta reduction is proportional to the difference between the size of the redex and the size of the reduct. In this way, the total cost of normalizing a lambda term will take into account the size of all intermediate results (as well as the number of steps to normal form).",
        "published": "2005-11-12T11:57:59Z",
        "link": "http://arxiv.org/abs/cs/0511045v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Entropy, Convex Optimization, and Competitive Quantum Interactions",
        "authors": [
            "Gus Gutoski"
        ],
        "summary": "This paper has been withdrawn by the author due to errors.",
        "published": "2005-11-12T21:15:40Z",
        "link": "http://arxiv.org/abs/cs/0511049v3",
        "categories": [
            "cs.CC",
            "cs.GT",
            "quant-ph"
        ]
    },
    {
        "title": "A polynomial-time heuristic for Circuit-SAT",
        "authors": [
            "Francesco Capasso"
        ],
        "summary": "In this paper is presented an heuristic that, in polynomial time and space in the input dimension, determines if a circuit describes a tautology or a contradiction. If the circuit is neither a tautology nor a contradiction, then the heuristic finds an assignment to the circuit inputs such that the circuit is satisfied.",
        "published": "2005-11-18T20:23:46Z",
        "link": "http://arxiv.org/abs/cs/0511071v4",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "A New Quantum Lower Bound Method, with Applications to Direct Product   Theorems and Time-Space Tradeoffs",
        "authors": [
            "Andris Ambainis",
            "Robert Spalek",
            "Ronald de Wolf"
        ],
        "summary": "We give a new version of the adversary method for proving lower bounds on quantum query algorithms. The new method is based on analyzing the eigenspace structure of the problem at hand. We use it to prove a new and optimal strong direct product theorem for 2-sided error quantum algorithms computing k independent instances of a symmetric Boolean function: if the algorithm uses significantly less than k times the number of queries needed for one instance of the function, then its success probability is exponentially small in k. We also use the polynomial method to prove a direct product theorem for 1-sided error algorithms for k threshold functions with a stronger bound on the success probability. Finally, we present a quantum algorithm for evaluating solutions to systems of linear inequalities, and use our direct product theorems to show that the time-space tradeoff of this algorithm is close to optimal.",
        "published": "2005-11-21T10:56:29Z",
        "link": "http://arxiv.org/abs/quant-ph/0511200v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Every Sequence is Decompressible from a Random One",
        "authors": [
            "David Doty"
        ],
        "summary": "Kucera and Gacs independently showed that every infinite sequence is Turing reducible to a Martin-Lof random sequence. This result is extended by showing that every infinite sequence S is Turing reducible to a Martin-Lof random sequence R such that the asymptotic number of bits of R needed to compute n bits of S, divided by n, is precisely the constructive dimension of S. It is shown that this is the optimal ratio of query bits to computed bits achievable with Turing reductions. As an application of this result, a new characterization of constructive dimension is given in terms of Turing reduction compression ratios.",
        "published": "2005-11-21T19:52:39Z",
        "link": "http://arxiv.org/abs/cs/0511074v5",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT",
            "F.1.1; H.1.1"
        ]
    },
    {
        "title": "Proving that P is not equal to NP and that P is not equal to the   intersection of NP and co-NP",
        "authors": [
            "R. A. Cohen"
        ],
        "summary": "The open question, P=NP?, was presented by Cook (1971). In this paper, a proof that P is not equal to NP is presented. In addition, it is shown that P is not equal to the intersection of NP and co-NP. Finally, the exact inclusion relationships between the classes P, NP and co-NP are presented.",
        "published": "2005-11-25T15:23:18Z",
        "link": "http://arxiv.org/abs/cs/0511085v2",
        "categories": [
            "cs.CC"
        ]
    },
    {
        "title": "Quantum Advantage without Entanglement",
        "authors": [
            "Dan Kenigsberg",
            "Tal Mor",
            "Gil Ratsaby"
        ],
        "summary": "We study the advantage of pure-state quantum computation without entanglement over classical computation. For the Deutsch-Jozsa algorithm we present the maximal subproblem that can be solved without entanglement, and show that the algorithm still has an advantage over the classical ones. We further show that this subproblem is of greater significance, by proving that it contains all the Boolean functions whose quantum phase-oracle is non-entangling. For Simon's and Grover's algorithms we provide simple proofs that no non-trivial subproblems can be solved by these algorithms without entanglement.",
        "published": "2005-11-30T16:12:47Z",
        "link": "http://arxiv.org/abs/quant-ph/0511272v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Phase transition in the assignment problem for random matrices",
        "authors": [
            "J. G. Esteve",
            "F. Falceto"
        ],
        "summary": "We report an analytic and numerical study of a phase transition in a P problem (the assignment problem) that separates two phases whose representatives are the simple matching problem (an easy P problem) and the traveling salesman problem (a NP-complete problem). Like other phase transitions found in combinatoric problems (K-satisfiability, number partitioning) this can help to understand the nature of the difficulties in solving NP problems an to find more accurate algorithms for them.",
        "published": "2005-11-30T18:03:43Z",
        "link": "http://arxiv.org/abs/cs/0511107v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech"
        ]
    },
    {
        "title": "Cohomology in Grothendieck Topologies and Lower Bounds in Boolean   Complexity",
        "authors": [
            "Joel Friedman"
        ],
        "summary": "This paper is motivated by questions such as P vs. NP and other questions in Boolean complexity theory. We describe an approach to attacking such questions with cohomology, and we show that using Grothendieck topologies and other ideas from the Grothendieck school gives new hope for such an attack.   We focus on circuit depth complexity, and consider only finite topological spaces or Grothendieck topologies based on finite categories; as such, we do not use algebraic geometry or manifolds.   Given two sheaves on a Grothendieck topology, their \"cohomological complexity\" is the sum of the dimensions of their Ext groups. We seek to model the depth complexity of Boolean functions by the cohomological complexity of sheaves on a Grothendieck topology. We propose that the logical AND of two Boolean functions will have its corresponding cohomological complexity bounded in terms of those of the two functions using ``virtual zero extensions.'' We propose that the logical negation of a function will have its corresponding cohomological complexity equal to that of the original function using duality theory. We explain these approaches and show that they are stable under pullbacks and base change. It is the subject of ongoing work to achieve AND and negation bounds simultaneously in a way that yields an interesting depth lower bound.",
        "published": "2005-12-01T22:40:27Z",
        "link": "http://arxiv.org/abs/cs/0512008v2",
        "categories": [
            "cs.CC",
            "math.AG"
        ]
    },
    {
        "title": "Semidefinite programming and arithmetic circuit evaluation",
        "authors": [
            "Sergey P. Tarasov",
            "Mikhail N. Vyalyi"
        ],
        "summary": "A rational number can be naturally presented by an arithmetic computation (AC): a sequence of elementary arithmetic operations starting from a fixed constant, say 1. The asymptotic complexity issues of such a representation are studied e.g. in the framework of the algebraic complexity theory over arbitrary field.   Here we study a related problem of the complexity of performing arithmetic operations and computing elementary predicates, e.g. ``='' or ``>'', on rational numbers given by AC.   In the first place, we prove that AC can be efficiently simulated by the exact semidefinite programming (SDP).   Secondly, we give a BPP-algorithm for the equality predicate.   Thirdly, we put ``>''-predicate into the complexity class PSPACE.   We conjecture that ``>''-predicate is hard to compute. This conjecture, if true, would clarify the complexity status of the exact SDP - a well known open problem in the field of mathematical programming.",
        "published": "2005-12-09T16:01:16Z",
        "link": "http://arxiv.org/abs/cs/0512035v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "Points on Computable Curves",
        "authors": [
            "Xiaoyang Gu",
            "Jack H. Lutz",
            "Elvira Mayordomo"
        ],
        "summary": "The ``analyst's traveling salesman theorem'' of geometric measure theory characterizes those subsets of Euclidean space that are contained in curves of finite length. This result, proven for the plane by Jones (1990) and extended to higher-dimensional Euclidean spaces by Okikiolu (1991), says that a bounded set $K$ is contained in some curve of finite length if and only if a certain ``square beta sum'', involving the ``width of $K$'' in each element of an infinite system of overlapping ``tiles'' of descending size, is finite.   In this paper we characterize those {\\it points} of Euclidean space that lie on {\\it computable} curves of finite length by formulating and proving a computable extension of the analyst's traveling salesman theorem. Our extension says that a point in Euclidean space lies on some computable curve of finite length if and only if it is ``permitted'' by some computable ``Jones constriction''. A Jones constriction here is an explicit assignment of a rational cylinder to each of the above-mentioned tiles in such a way that, when the radius of the cylinder corresponding to a tile is used in place of the ``width of $K$'' in each tile, the square beta sum is finite. A point is permitted by a Jones constriction if it is contained in the cylinder assigned to each tile containing the point. The main part of our proof is the construction of a computable curve of finite length traversing all the points permitted by a given Jones constriction. Our construction uses the main ideas of Jones's ``farthest insertion'' construction, but our algorithm for computing the curve must work exclusively with the Jones constriction itself, because it has no direct access to the (typically uncomputable) points permitted by the Jones constriction.",
        "published": "2005-12-10T03:08:39Z",
        "link": "http://arxiv.org/abs/cs/0512042v1",
        "categories": [
            "cs.CC",
            "cs.CG"
        ]
    },
    {
        "title": "Mastermind is NP-Complete",
        "authors": [
            "Jeff Stuckman",
            "Guo-Qiang Zhang"
        ],
        "summary": "In this paper we show that the Mastermind Satisfiability Problem (MSP) is NP-complete. The Mastermind is a popular game which can be turned into a logical puzzle called Mastermind Satisfiability Problem in a similar spirit to the Minesweeper puzzle. By proving that MSP is NP-complete, we reveal its intrinsic computational property that makes it challenging and interesting. This serves as an addition to our knowledge about a host of other puzzles, such as Minesweeper, Mah-Jongg, and the 15-puzzle.",
        "published": "2005-12-13T04:19:07Z",
        "link": "http://arxiv.org/abs/cs/0512049v1",
        "categories": [
            "cs.CC",
            "cs.DM"
        ]
    },
    {
        "title": "Online Learning and Resource-Bounded Dimension: Winnow Yields New Lower   Bounds for Hard Sets",
        "authors": [
            "John M. Hitchcock"
        ],
        "summary": "We establish a relationship between the online mistake-bound model of learning and resource-bounded dimension. This connection is combined with the Winnow algorithm to obtain new results about the density of hard sets under adaptive reductions. This improves previous work of Fu (1995) and Lutz and Zhao (2000), and solves one of Lutz and Mayordomo's \"Twelve Problems in Resource-Bounded Measure\" (1999).",
        "published": "2005-12-13T22:01:09Z",
        "link": "http://arxiv.org/abs/cs/0512053v1",
        "categories": [
            "cs.CC",
            "cs.LG"
        ]
    },
    {
        "title": "PURRS: Towards Computer Algebra Support for Fully Automatic Worst-Case   Complexity Analysis",
        "authors": [
            "Roberto Bagnara",
            "Andrea Pescetti",
            "Alessandro Zaccagnini",
            "Enea Zaffanella"
        ],
        "summary": "Fully automatic worst-case complexity analysis has a number of applications in computer-assisted program manipulation. A classical and powerful approach to complexity analysis consists in formally deriving, from the program syntax, a set of constraints expressing bounds on the resources required by the program, which are then solved, possibly applying safe approximations. In several interesting cases, these constraints take the form of recurrence relations. While techniques for solving recurrences are known and implemented in several computer algebra systems, these do not completely fulfill the needs of fully automatic complexity analysis: they only deal with a somewhat restricted class of recurrence relations, or sometimes require user intervention, or they are restricted to the computation of exact solutions that are often so complex to be unmanageable, and thus useless in practice. In this paper we briefly describe PURRS, a system and software library aimed at providing all the computer algebra services needed by applications performing or exploiting the results of worst-case complexity analyses. The capabilities of the system are illustrated by means of examples derived from the analysis of programs written in a domain-specific functional programming language for real-time embedded systems.",
        "published": "2005-12-14T09:54:01Z",
        "link": "http://arxiv.org/abs/cs/0512056v1",
        "categories": [
            "cs.MS",
            "cs.CC"
        ]
    },
    {
        "title": "On the Complexity of finding Stopping Distance in Tanner Graphs",
        "authors": [
            "K. Murali Krishnan",
            "Priti Shankar"
        ],
        "summary": "Two decision problems related to the computation of stopping sets in Tanner graphs are shown to be NP-complete. NP-hardness of the problem of computing the stopping distance of a Tanner graph follows as a consequence",
        "published": "2005-12-28T09:42:36Z",
        "link": "http://arxiv.org/abs/cs/0512101v2",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Formal Languages and Algorithms for Similarity based Retrieval from   Sequence Databases",
        "authors": [
            "A. Prasad Sistla"
        ],
        "summary": "The paper considers various formalisms based on Automata, Temporal Logic and Regular Expressions for specifying queries over sequences. Unlike traditional binary semantics, the paper presents a similarity based semantics for thse formalisms. More specifically, a distance measure in the range [0,1] is associated with a sequence, query pair denoting how closely the sequence satisfies the query. These measures are defined using a spectrum of normed vector distance measures. Various distance measures based on the syntax and the traditional semantics of the query are presented. Efficient algorithms for computing these distance measure are presented. These algorithms can be employed for retrieval of sequence from a database that closely satisfy a given.",
        "published": "2005-01-04T01:05:28Z",
        "link": "http://arxiv.org/abs/cs/0501006v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "I.2.4; I.2.3; D.2.4"
        ]
    },
    {
        "title": "Effectively Open Real Functions",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "A function f is continuous iff the PRE-image f^{-1}[V] of any open set V is open again. Dual to this topological property, f is called OPEN iff the IMAGE f[U] of any open set U is open again. Several classical Open Mapping Theorems in Analysis provide a variety of sufficient conditions for openness.   By the Main Theorem of Recursive Analysis, computable real functions are necessarily continuous. In fact they admit a well-known characterization in terms of the mapping V+->f^{-1}[V] being EFFECTIVE: Given a list of open rational balls exhausting V, a Turing Machine can generate a corresponding list for f^{-1}[V]. Analogously, EFFECTIVE OPENNESS requires the mapping U+->f[U] on open real subsets to be effective.   By effectivizing classical Open Mapping Theorems as well as from application of Tarski's Quantifier Elimination, the present work reveals several rich classes of functions to be effectively open.",
        "published": "2005-01-12T12:16:48Z",
        "link": "http://arxiv.org/abs/cs/0501024v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "A Logic for Non-Monotone Inductive Definitions",
        "authors": [
            "Marc Denecker",
            "Eugenia Ternovska"
        ],
        "summary": "Well-known principles of induction include monotone induction and different sorts of non-monotone induction such as inflationary induction, induction over well-founded sets and iterated induction. In this work, we define a logic formalizing induction over well-founded sets and monotone and iterated induction. Just as the principle of positive induction has been formalized in FO(LFP), and the principle of inflationary induction has been formalized in FO(IFP), this paper formalizes the principle of iterated induction in a new logic for Non-Monotone Inductive Definitions (ID-logic). The semantics of the logic is strongly influenced by the well-founded semantics of logic programming. Our main result concerns the modularity properties of inductive definitions in ID-logic. Specifically, we formulate conditions under which a simultaneous definition $\\D$ of several relations is logically equivalent to a conjunction of smaller definitions $\\D_1 \\land ... \\land \\D_n$ with disjoint sets of defined predicates. The difficulty of the result comes from the fact that predicates $P_i$ and $P_j$ defined in $\\D_i$ and $\\D_j$, respectively, may be mutually connected by simultaneous induction. Since logic programming and abductive logic programming under well-founded semantics are proper fragments of our logic, our modularity results are applicable there as well.",
        "published": "2005-01-13T02:53:03Z",
        "link": "http://arxiv.org/abs/cs/0501025v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "From truth to computability II",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "Computability logic is a formal theory of computational tasks and resources. Formulas in it represent interactive computational problems, and \"truth\" is understood as algorithmic solvability. Interactive computational problems, in turn, are defined as a certain sort games between a machine and its environment, with logical operators standing for operations on such games. Within the ambitious program of finding axiomatizations for incrementally rich fragments of this semantically introduced logic, the earlier article \"From truth to computability I\" proved soundness and completeness for system CL3, whose language has the so called parallel connectives (including negation), choice connectives, choice quantifiers, and blind quantifiers. The present paper extends that result to the significantly more expressive system CL4 with the same collection of logical operators. What makes CL4 expressive is the presence of two sorts of atoms in its language: elementary atoms, representing elementary computational problems (i.e. predicates, i.e. problems of zero degree of interactivity), and general atoms, representing arbitrary computational problems. CL4 conservatively extends CL3, with the latter being nothing but the general-atom-free fragment of the former. Removing the blind (classical) group of quantifiers from the language of CL4 is shown to yield a decidable logic despite the fact that the latter is still first-order. A comprehensive online source on computability logic can be found at http://www.cis.upenn.edu/~giorgi/cl.html",
        "published": "2005-01-16T13:47:47Z",
        "link": "http://arxiv.org/abs/cs/0501031v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "On Partially Additive Kleene Algebras",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "We define the notion of a partially additive Kleene algebra, which is a Kleene algebra where the + operation need only be partially defined. These structures formalize a number of examples that cannot be handled directly by Kleene algebras. We relate partially additive Kleene algebras to existing algebraic structures, by exhibiting categorical connections with Kleene algebras, partially additive categories, and closed semirings.",
        "published": "2005-01-16T20:39:20Z",
        "link": "http://arxiv.org/abs/cs/0501032v1",
        "categories": [
            "cs.LO",
            "F.3.1;I.1.3"
        ]
    },
    {
        "title": "Playful, streamlike computation",
        "authors": [
            "Pierre-Louis Curien"
        ],
        "summary": "We offer a short tour into the interactive interpretation of sequential programs. We emphasize streamlike computation -- that is, computation of successive bits of information upon request. The core of the approach surveyed here dates back to the work of Berry and the author on sequential algorithms on concrete data structures in the late seventies, culminating in the design of the programming language CDS, in which the semantics of programs of any type can be explored interactively. Around one decade later, two major insights of Cartwright and Felleisen on one hand, and of Lamarche on the other hand gave new, decisive impulses to the study of sequentiality. Cartwright and Felleisen observed that sequential algorithms give a direct semantics to control operators like \\\"call-cc\\\" and proposed to include explicit errors both in the syntax and in the semantics of the language PCF. Lamarche (unpublished) connected sequential algorithms to linear logic and games. The successful program of games semantics has spanned over the nineties until now, starting with syntax-independent characterizations of the term model of PCF by Abramsky, Jagadeesan, and Malacaria on one hand, and by Hyland and Ong on the other hand.",
        "published": "2005-01-18T07:39:09Z",
        "link": "http://arxiv.org/abs/cs/0501033v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Symmetry and interactivity in Programming",
        "authors": [
            "Pierre-Louis Curien"
        ],
        "summary": "We recall some of the early occurrences of the notions of interactivity and symmetry in the operational and denotational semantics of programming languages. We suggest some connections with ludics.",
        "published": "2005-01-18T07:41:48Z",
        "link": "http://arxiv.org/abs/cs/0501034v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Introduction to linear logic and ludics, part I",
        "authors": [
            "Pierre-Louis Curien"
        ],
        "summary": "This two-parts paper offers a survey of linear logic and ludics, which were introduced by Girard in 1986 and 2001, respectively. Both theories revisit mathematical logic from first principles, with inspiration from and applications to computer science. The present part I covers an introduction to the connectives and proof rules of linear logic, to its decidability properties, and to its models. Part II will deal with proof nets, a graph-like representation of proofs which is one of the major innovations of linear logic, and will present an introduction to ludics.",
        "published": "2005-01-18T07:42:33Z",
        "link": "http://arxiv.org/abs/cs/0501035v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Introduction to linear logic and ludics, part II",
        "authors": [
            "Pierre-Louis Curien"
        ],
        "summary": "This paper is the second part of an introduction to linear logic and ludics, both due to Girard. It is devoted to proof nets, in the limited, yet central, framework of multiplicative linear logic and to ludics, which has been recently developped in an aim of further unveiling the fundamental interactive nature of computation and logic. We hope to offer a few computer science insights into this new theory.",
        "published": "2005-01-19T15:00:46Z",
        "link": "http://arxiv.org/abs/cs/0501039v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Split-2 Bisimilarity has a Finite Axiomatization over CCS with<br>   Hennessy&#39;s Merge",
        "authors": [
            "Luca Aceto",
            "Wan Fokkink",
            "Anna Ingolfsdottir",
            "Bas Luttik"
        ],
        "summary": "This note shows that split-2 bisimulation equivalence (also known as timed equivalence) affords a finite equational axiomatization over the process algebra obtained by adding an auxiliary operation proposed by Hennessy in 1981 to the recursion, relabelling and restriction free fragment of Milner's Calculus of Communicating Systems. Thus the addition of a single binary operation, viz. Hennessy's merge, is sufficient for the finite equational axiomatization of parallel composition modulo this non-interleaving equivalence. This result is in sharp contrast to a theorem previously obtained by the same authors to the effect that the same language is not finitely based modulo bisimulation equivalence.",
        "published": "2005-01-19T17:55:45Z",
        "link": "http://arxiv.org/abs/cs/0501040v4",
        "categories": [
            "cs.LO",
            "D.3.1; F.1.1; F.1.2; F.3.2; F.3.4; F.4.1"
        ]
    },
    {
        "title": "Under-approximation of the Greatest Fixpoint in Real-Time System   Verification",
        "authors": [
            "Farn Wang"
        ],
        "summary": "Techniques for the efficient successive under-approximation of the greatest fixpoint in TCTL formulas can be useful in fast refutation of inevitability properties and vacuity checking. We first give an integrated algorithmic framework for both under and over-approximate model-checking. We design the {\\em NZF (Non-Zeno Fairness) predicate}, with a greatest fixpoint formulation, as a unified framework for the evaluation of formulas like $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then prove the correctness of a new formulation for the characterization of the NZF predicate based on zone search and the least fixpoint evaluation. The new formulation then leads to the design of an evaluation algorithm, with the capability of successive under-approximation, for $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then present techniques to efficiently search for the zones and to speed up the under-approximate evaluation of those three formulas. Our experiments show that the techniques have significantly enhanced the verification performance against several benchmarks over exact model-checking.",
        "published": "2005-01-22T11:02:01Z",
        "link": "http://arxiv.org/abs/cs/0501059v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "Under-approximation of the Greatest Fixpoints in Real-Time System   Verification",
        "authors": [
            "Farn Wang"
        ],
        "summary": "Techniques for the efficient successive under-approximation of the greatest fixpoint in TCTL formulas can be useful in fast refutation of inevitability properties and vacuity checking. We first give an integrated algorithmic framework for both under and over-approximate model-checking. We design the {\\em NZF (Non-Zeno Fairness) predicate}, with a greatest fixpoint formulation, as a unified framework for the evaluation of formulas like $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then prove the correctness of a new formulation for the characterization of the NZF predicate based on zone search and the least fixpoint evaluation. The new formulation then leads to the design of an evaluation algorithm, with the capability of successive under-approximation, for $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then present techniques to efficiently search for the zones and to speed up the under-approximate evaluation of those three formulas. Our experiments show that the techniques have significantly enhanced the verification performance against several benchmarks over exact model-checking.",
        "published": "2005-01-22T11:03:49Z",
        "link": "http://arxiv.org/abs/cs/0501060v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "Proving Correctness and Completeness of Normal Programs - a Declarative   Approach",
        "authors": [
            "W. Drabent",
            "M. Milkowska"
        ],
        "summary": "We advocate a declarative approach to proving properties of logic programs. Total correctness can be separated into correctness, completeness and clean termination; the latter includes non-floundering. Only clean termination depends on the operational semantics, in particular on the selection rule. We show how to deal with correctness and completeness in a declarative way, treating programs only from the logical point of view. Specifications used in this approach are interpretations (or theories). We point out that specifications for correctness may differ from those for completeness, as usually there are answers which are neither considered erroneous nor required to be computed.   We present proof methods for correctness and completeness for definite programs and generalize them to normal programs. For normal programs we use the 3-valued completion semantics; this is a standard semantics corresponding to negation as finite failure. The proof methods employ solely the classical 2-valued logic. We use a 2-valued characterization of the 3-valued completion semantics which may be of separate interest. The presented methods are compared with an approach based on operational semantics. We also employ the ideas of this work to generalize a known method of proving termination of normal programs.",
        "published": "2005-01-25T16:53:59Z",
        "link": "http://arxiv.org/abs/cs/0501043v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1; D.1.6; D.2.4"
        ]
    },
    {
        "title": "The maximum entropy state",
        "authors": [
            "Keye Martin"
        ],
        "summary": "We give an algorithm for calculating the maximum entropy state as the least fixed point of a Scott continuous mapping on the domain of classical states in their Bayesian order.",
        "published": "2005-02-01T17:46:15Z",
        "link": "http://arxiv.org/abs/math/0502024v1",
        "categories": [
            "math.PR",
            "cs.LO",
            "math-ph",
            "math.MP",
            "quant-ph",
            "94A17; 34A45"
        ]
    },
    {
        "title": "Model-Checking Problems as a Basis for Parameterized Intractability",
        "authors": [
            "Joerg Flum",
            "Martin Grohe"
        ],
        "summary": "Most parameterized complexity classes are defined in terms of a parameterized version of the Boolean satisfiability problem (the so-called weighted satisfiability problem). For example, Downey and Fellow's W-hierarchy is of this form. But there are also classes, for example, the A-hierarchy, that are more naturally characterised in terms of model-checking problems for certain fragments of first-order logic.   Downey, Fellows, and Regan were the first to establish a connection between the two formalisms by giving a characterisation of the W-hierarchy in terms of first-order model-checking problems. We improve their result and then prove a similar correspondence between weighted satisfiability and model-checking problems for the A-hierarchy and the W^*-hierarchy. Thus we obtain very uniform characterisations of many of the most important parameterized complexity classes in both formalisms.   Our results can be used to give new, simple proofs of some of the core results of structural parameterized complexity theory.",
        "published": "2005-02-01T18:03:51Z",
        "link": "http://arxiv.org/abs/cs/0502005v5",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "EPspectra: A Formal Toolkit for Developing DSP Software Applications",
        "authors": [
            "Hahnsang Kim",
            "Theirry Turletti",
            "Amar Bouali"
        ],
        "summary": "The software approach to developing Digital Signal Processing (DSP) applications brings some great features such as flexibility, re-usability of resources and easy upgrading of applications. However, it requires long and tedious tests and verification phases because of the increasing complexity of the software. This implies the need of a software programming environment capable of putting together DSP modules and providing facilities to debug, verify and validate the code. The objective of the work is to provide such facilities as simulation and verification for developing DSP software applications. This led us to develop an extension toolkit, Epspectra, built upon Pspectra, one of the first toolkits available to design basic software radio applications on standard PC workstations. In this paper, we first present Epspectra, an Esterel-based extension of Pspectra that makes the design and implementation of portable DSP applications easier. It allows drastic reduction of testing and verification time while requiring relatively little expertise in formal verification methods. Second, we demonstrate the use of Epspectra, taking as an example the radio interface part of a GSM base station. We also present the verification procedures for the three safety properties of the implementation programs which have complex control-paths. These have to obey strict scheduling rules. In addition, Epspectra achieves the verification of the targeted application since the same model is used for the executable code generation and for the formal verification.",
        "published": "2005-02-04T15:11:26Z",
        "link": "http://arxiv.org/abs/cs/0502025v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "D.2.4, F.3.1, F.4.0, F.4.1, F.4.2"
        ]
    },
    {
        "title": "Logic Column 11: The Finite and the Infinite in Temporal Logic",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "This article examines the interpretation of the LTL temporal operators over finite and infinite sequences. This is used as the basis for deriving a sound and complete axiomatization for Caret, a recent temporal logic for reasoning about programs with nested procedure calls and returns.",
        "published": "2005-02-05T21:00:13Z",
        "link": "http://arxiv.org/abs/cs/0502031v2",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Proof obligations for specification and refinement of liveness   properties under weak fairness",
        "authors": [
            "Hector Ruiz Barradas",
            "Didier Bert"
        ],
        "summary": "In this report, we present a formal model of fair iteration of events for B event systems. The model is used to justify proof obligations for basic liveness properties and preservation under refinement of general liveness properties. The model of fair iteration of events uses the dovetail operator, an operator proposed by Broy and Nelson to model fair choice. The proofs are mainly founded in fixpoint calculations of fair iteration of events and weakest precondition calculus.",
        "published": "2005-02-09T09:23:30Z",
        "link": "http://arxiv.org/abs/cs/0502046v1",
        "categories": [
            "cs.LO",
            "ACM: F3.1"
        ]
    },
    {
        "title": "The succinctness of first-order logic on linear orders",
        "authors": [
            "Martin Grohe",
            "Nicole Schweikardt"
        ],
        "summary": "Succinctness is a natural measure for comparing the strength of different logics. Intuitively, a logic L_1 is more succinct than another logic L_2 if all properties that can be expressed in L_2 can be expressed in L_1 by formulas of (approximately) the same size, but some properties can be expressed in L_1 by (significantly) smaller formulas.   We study the succinctness of logics on linear orders. Our first theorem is concerned with the finite variable fragments of first-order logic. We prove that:   (i) Up to a polynomial factor, the 2- and the 3-variable fragments of first-order logic on linear orders have the same succinctness. (ii) The 4-variable fragment is exponentially more succinct than the 3-variable fragment. Our second main result compares the succinctness of first-order logic on linear orders with that of monadic second-order logic. We prove that the fragment of monadic second-order logic that has the same expressiveness as first-order logic on linear orders is non-elementarily more succinct than first-order logic.",
        "published": "2005-02-09T11:00:12Z",
        "link": "http://arxiv.org/abs/cs/0502047v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Graphs and colorings for answer set programming",
        "authors": [
            "Kathrin Konczak",
            "Thomas Linke",
            "Torsten Schaub"
        ],
        "summary": "We investigate the usage of rule dependency graphs and their colorings for characterizing and computing answer sets of logic programs. This approach provides us with insights into the interplay between rules when inducing answer sets. We start with different characterizations of answer sets in terms of totally colored dependency graphs that differ in graph-theoretical aspects. We then develop a series of operational characterizations of answer sets in terms of operators on partial colorings. In analogy to the notion of a derivation in proof theory, our operational characterizations are expressed as (non-deterministically formed) sequences of colorings, turning an uncolored graph into a totally colored one. In this way, we obtain an operational framework in which different combinations of operators result in different formal properties. Among others, we identify the basic strategy employed by the noMoRe system and justify its algorithmic approach. Furthermore, we distinguish operations corresponding to Fitting's operator as well as to well-founded semantics. (To appear in Theory and Practice of Logic Programming (TPLP))",
        "published": "2005-02-21T14:28:23Z",
        "link": "http://arxiv.org/abs/cs/0502082v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Towards a Systematic Account of Different Semantics for Logic Programs",
        "authors": [
            "Pascal Hitzler"
        ],
        "summary": "In [Hitzler and Wendt 2002, 2005], a new methodology has been proposed which allows to derive uniform characterizations of different declarative semantics for logic programs with negation. One result from this work is that the well-founded semantics can formally be understood as a stratified version of the Fitting (or Kripke-Kleene) semantics. The constructions leading to this result, however, show a certain asymmetry which is not readily understood. We will study this situation here with the result that we will obtain a coherent picture of relations between different semantics for normal logic programs.",
        "published": "2005-02-22T18:53:01Z",
        "link": "http://arxiv.org/abs/cs/0502088v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3; D.1.6; F.4.1"
        ]
    },
    {
        "title": "An Audit Logic for Accountability",
        "authors": [
            "J. G. Cederquist",
            "R. Corin",
            "M. A. C. Dekker",
            "S. Etalle",
            "J. I. den Hartog"
        ],
        "summary": "We describe and implement a policy language. In our system, agents can distribute data along with usage policies in a decentralized architecture. Our language supports the specification of conditions and obligations, and also the possibility to refine policies. In our framework, the compliance with usage policies is not actively enforced. However, agents are accountable for their actions, and may be audited by an authority requiring justifications.",
        "published": "2005-02-24T15:16:03Z",
        "link": "http://arxiv.org/abs/cs/0502091v3",
        "categories": [
            "cs.CR",
            "cs.LO"
        ]
    },
    {
        "title": "Probabilistic Algorithmic Knowledge",
        "authors": [
            "Joseph Y. Halpern",
            "Riccardo Pucella"
        ],
        "summary": "The framework of algorithmic knowledge assumes that agents use deterministic knowledge algorithms to compute the facts they explicitly know. We extend the framework to allow for randomized knowledge algorithms. We then characterize the information provided by a randomized knowledge algorithm when its answers have some probability of being incorrect. We formalize this information in terms of evidence; a randomized knowledge algorithm returning ``Yes'' to a query about a fact \\phi provides evidence for \\phi being true. Finally, we discuss the extent to which this evidence can be used as a basis for decisions.",
        "published": "2005-03-08T03:39:23Z",
        "link": "http://arxiv.org/abs/cs/0503018v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; G.3"
        ]
    },
    {
        "title": "Stabilization of Cooperative Information Agents in Unpredictable   Environment: A Logic Programming Approach",
        "authors": [
            "Phan Minh Dung",
            "Do Duc Hanh",
            "Phan Minh Thang"
        ],
        "summary": "An information agent is viewed as a deductive database consisting of 3 parts: an observation database containing the facts the agent has observed or sensed from its surrounding environment, an input database containing the information the agent has obtained from other agents, and an intensional database which is a set of rules for computing derived information from the information stored in the observation and input databases. Stabilization of a system of information agents represents a capability of the agents to eventually get correct information about their surrounding despite unpredictable environment changes and the incapability of many agents to sense such changes causing them to have temporary incorrect information. We argue that the stabilization of a system of cooperative information agents could be understood as the convergence of the behavior of the whole system toward the behavior of a \"superagent\", who has the sensing and computing capabilities of all agents combined. We show that unfortunately, stabilization is not guaranteed in general, even if the agents are fully cooperative and do not hide any information from each other. We give sufficient conditions for stabilization and discuss the consequences of our results.",
        "published": "2005-03-14T09:07:24Z",
        "link": "http://arxiv.org/abs/cs/0503028v2",
        "categories": [
            "cs.LO",
            "cs.MA",
            "cs.PL",
            "F.4.1; I.2.3; I.2.11"
        ]
    },
    {
        "title": "Complexity Issues in Finding Succinct Solutions of PSPACE-Complete   Problems",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "We study the problem of deciding whether some PSPACE-complete problems have models of bounded size. Contrary to problems in NP, models of PSPACE-complete problems may be exponentially large. However, such models may take polynomial space in a succinct representation. For example, the models of a QBF are explicitely represented by and-or trees (which are always of exponential size) but can be succinctely represented by circuits (which can be polynomial or exponential). We investigate the complexity of deciding the existence of such succinct models when a bound on size is given.",
        "published": "2005-03-18T17:30:39Z",
        "link": "http://arxiv.org/abs/cs/0503043v3",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.LO",
            "I.2.3; I.2.4; I.2.8; F.1.3; F.2.2"
        ]
    },
    {
        "title": "Weakly complete axiomatization of exogenous quantum propositional logic",
        "authors": [
            "P. Mateus",
            "A. Sernadas"
        ],
        "summary": "A weakly complete finitary axiomatization for EQPL (exogenous quantum propositional logic) is presented. The proof is carried out using a non trivial extension of the Fagin-Halpern-Megiddo technique together with three Henkin style completions.",
        "published": "2005-03-22T14:36:13Z",
        "link": "http://arxiv.org/abs/math/0503453v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "quant-ph",
            "03G12"
        ]
    },
    {
        "title": "Optimality in Goal-Dependent Analysis of Sharing",
        "authors": [
            "Gianluca Amato",
            "Francesca Scozzari"
        ],
        "summary": "We face the problems of correctness, optimality and precision for the static analysis of logic programs, using the theory of abstract interpretation. We propose a framework with a denotational, goal-dependent semantics equipped with two unification operators for forward unification (calling a procedure) and backward unification (returning from a procedure). The latter is implemented through a matching operation. Our proposal clarifies and unifies many different frameworks and ideas on static analysis of logic programming in a single, formal setting. On the abstract side, we focus on the domain Sharing by Jacobs and Langen and provide the best correct approximation of all the primitive semantic operators, namely, projection, renaming, forward and backward unification. We show that the abstract unification operators are strictly more precise than those in the literature defined over the same abstract domain. In some cases, our operators are more precise than those developed for more complex domains involving linearity and freeness.   To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2005-03-22T20:56:51Z",
        "link": "http://arxiv.org/abs/cs/0503055v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2; D.1.6"
        ]
    },
    {
        "title": "Resource Bounded Unprovability of Computational Lower Bounds",
        "authors": [
            "Tatsuaki Okamoto",
            "Ryo Kashima"
        ],
        "summary": "This paper introduces new notions of asymptotic proofs, PT(polynomial-time)-extensions, PTM(polynomial-time Turing machine)-omega-consistency, etc. on formal theories of arithmetic including PA (Peano Arithmetic). This paper shows that P not= NP (more generally, any super-polynomial-time lower bound in PSPACE) is unprovable in a PTM-omega-consistent theory T, where T is a consistent PT-extension of PA. This result gives a unified view to the existing two major negative results on proving P not= NP, Natural Proofs and relativizable proofs, through the two manners of characterization of PTM-omega-consistency. We also show that the PTM-omega-consistency of T cannot be proven in any PTM-omega-consistent theory S, where S is a consistent PT-extension of T.",
        "published": "2005-03-31T10:40:13Z",
        "link": "http://arxiv.org/abs/cs/0503091v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.3; F.4.1; F.2.2"
        ]
    },
    {
        "title": "Probabilistic Model--Checking of Quantum Protocols",
        "authors": [
            "Simon Gay",
            "Rajagopal Nagarajan",
            "Nikolaos Papanikolaou"
        ],
        "summary": "We establish fundamental and general techniques for formal verification of quantum protocols. Quantum protocols are novel communication schemes involving the use of quantum-mechanical phenomena for representation, storage and transmission of data. As opposed to quantum computers, quantum communication systems can and have been implemented using present-day technology; therefore, the ability to model and analyse such systems rigorously is of primary importance.   While current analyses of quantum protocols use a traditional mathematical approach and require considerable understanding of the underlying physics, we argue that automated verification techniques provide an elegant alternative. We demonstrate these techniques through the use of PRISM, a probabilistic model-checking tool. Our approach is conceptually simpler than existing proofs, and allows us to disambiguate protocol definitions and assess their properties. It also facilitates detailed analyses of actual implemented systems. We illustrate our techniques by modelling a selection of quantum protocols (namely superdense coding, quantum teleportation, and quantum error correction) and verifying their basic correctness properties. Our results provide a foundation for further work on modelling and analysing larger systems such as those used for quantum cryptography, in which basic protocols are used as components.",
        "published": "2005-04-01T23:09:29Z",
        "link": "http://arxiv.org/abs/quant-ph/0504007v2",
        "categories": [
            "quant-ph",
            "cs.LO"
        ]
    },
    {
        "title": "Modelling Linear Logic Without Units (Preliminary Results)",
        "authors": [
            "Robin Houston",
            "Dominic Hughes",
            "Andrea Schalk"
        ],
        "summary": "We describe a notion of categorical model for unitless fragments of (multiplicative) linear logic. The basic definition uses promonoidal categories, and we also give an equivalent elementary axiomatisation.",
        "published": "2005-04-03T09:55:06Z",
        "link": "http://arxiv.org/abs/math/0504037v1",
        "categories": [
            "math.CT",
            "cs.LO",
            "math.LO"
        ]
    },
    {
        "title": "Numerical Simulations of a Possible Hypercomputational Quantum Algorithm",
        "authors": [
            "Andrés Sicard",
            "Juan Ospina",
            "Mario Vélez"
        ],
        "summary": "The hypercomputers compute functions or numbers, or more generally solve problems or carry out tasks, that cannot be computed or solved by a Turing machine. Several numerical simulations of a possible hypercomputational algorithm based on quantum computations previously constructed by the authors are presented. The hypercomputability of our algorithm is based on the fact that this algorithm could solve a classically non-computable decision problem, Hilbert's tenth problem. The numerical simulations were realized for three types of Diophantine equations: with and without solutions in non-negative integers, and without solutions by way of various traditional mathematical packages.",
        "published": "2005-04-05T00:41:07Z",
        "link": "http://arxiv.org/abs/quant-ph/0504021v1",
        "categories": [
            "quant-ph",
            "cs.LO"
        ]
    },
    {
        "title": "A Rule-Based Logic for Quantum Information",
        "authors": [
            "Olivier Brunet"
        ],
        "summary": "In the present article, we explore a new approach for the study of orthomodular lattices, where we replace the problematic conjunction by a binary operator, called the Sasaki projection. We present a characterization of orthomodular lattices based on the use of an algebraic version of the Sasaki projection operator (together with orthocomplementation) rather than on the conjunction. We then define of a new logic, which we call Sasaki Orthologic, which is closely related to quantum logic, and provide a rule-based definition of this logic.",
        "published": "2005-04-06T08:11:00Z",
        "link": "http://arxiv.org/abs/cs/0504018v1",
        "categories": [
            "cs.LO",
            "quant-ph"
        ]
    },
    {
        "title": "Constraint-Based Qualitative Simulation",
        "authors": [
            "Krzysztof R. Apt",
            "Sebastian Brand"
        ],
        "summary": "We consider qualitative simulation involving a finite set of qualitative relations in presence of complete knowledge about their interrelationship. We show how it can be naturally captured by means of constraints expressed in temporal logic and constraint satisfaction problems. The constraints relate at each stage the 'past' of a simulation with its 'future'. The benefit of this approach is that it readily leads to an implementation based on constraint technology that can be used to generate simulations and to answer queries about them.",
        "published": "2005-04-07T12:06:24Z",
        "link": "http://arxiv.org/abs/cs/0504024v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.6.2; I.6.7; F.4.1; D.3.2"
        ]
    },
    {
        "title": "Linear Datalog and Bounded Path Duality of Relational Structures",
        "authors": [
            "Victor Dalmau"
        ],
        "summary": "In this paper we systematically investigate the connections between logics with a finite number of variables, structures of bounded pathwidth, and linear Datalog Programs. We prove that, in the context of Constraint Satisfaction Problems, all these concepts correspond to different mathematical embodiments of a unique robust notion that we call bounded path duality. We also study the computational complexity implications of the notion of bounded path duality. We show that every constraint satisfaction problem $\\csp(\\best)$ with bounded path duality is solvable in NL and that this notion explains in a uniform way all families of CSPs known to be in NL. Finally, we use the results developed in the paper to identify new problems in NL.",
        "published": "2005-04-07T16:07:03Z",
        "link": "http://arxiv.org/abs/cs/0504027v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.1.3; F.4.1"
        ]
    },
    {
        "title": "Mapping Fusion and Synchronized Hyperedge Replacement into Logic   Programming",
        "authors": [
            "Ivan Lanese",
            "Ugo Montanari"
        ],
        "summary": "In this paper we compare three different formalisms that can be used in the area of models for distributed, concurrent and mobile systems. In particular we analyze the relationships between a process calculus, the Fusion Calculus, graph transformations in the Synchronized Hyperedge Replacement with Hoare synchronization (HSHR) approach and logic programming. We present a translation from Fusion Calculus into HSHR (whereas Fusion Calculus uses Milner synchronization) and prove a correspondence between the reduction semantics of Fusion Calculus and HSHR transitions. We also present a mapping from HSHR into a transactional version of logic programming and prove that there is a full correspondence between the two formalisms. The resulting mapping from Fusion Calculus to logic programming is interesting since it shows the tight analogies between the two formalisms, in particular for handling name generation and mobility. The intermediate step in terms of HSHR is convenient since graph transformations allow for multiple, remote synchronizations, as required by Fusion Calculus semantics.",
        "published": "2005-04-13T14:40:28Z",
        "link": "http://arxiv.org/abs/cs/0504050v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "Theories for TC0 and Other Small Complexity Classes",
        "authors": [
            "Phuong Nguyen",
            "Stephen Cook"
        ],
        "summary": "We present a general method for introducing finitely axiomatizable \"minimal\" two-sorted theories for various subclasses of P (problems solvable in polynomial time). The two sorts are natural numbers and finite sets of natural numbers. The latter are essentially the finite binary strings, which provide a natural domain for defining the functions and sets in small complexity classes. We concentrate on the complexity class TC^0, whose problems are defined by uniform polynomial-size families of bounded-depth Boolean circuits with majority gates. We present an elegant theory VTC^0 in which the provably-total functions are those associated with TC^0, and then prove that VTC^0 is \"isomorphic\" to a different-looking single-sorted theory introduced by Johannsen and Pollet. The most technical part of the isomorphism proof is defining binary number multiplication in terms a bit-counting function, and showing how to formalize the proofs of its algebraic properties.",
        "published": "2005-05-05T22:17:01Z",
        "link": "http://arxiv.org/abs/cs/0505013v4",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Interval Neutrosophic Sets and Logic: Theory and Applications in   Computing",
        "authors": [
            "Haibin Wang",
            "Florentin Smarandache",
            "Yan-Qing Zhang",
            "Rajshekhar Sunderraman"
        ],
        "summary": "This book presents the advancements and applications of neutrosophics. Chapter 1 first introduces the interval neutrosophic sets which is an instance of neutrosophic sets. In this chapter, the definition of interval neutrosophic sets and set-theoretic operators are given and various properties of interval neutrosophic set are proved. Chapter 2 defines the interval neutrosophic logic based on interval neutrosophic sets including the syntax and semantics of first order interval neutrosophic propositional logic and first order interval neutrosophic predicate logic. The interval neutrosophic logic can reason and model fuzzy, incomplete and inconsistent information. In this chapter, we also design an interval neutrosophic inference system based on first order interval neutrosophic predicate logic. The interval neutrosophic inference system can be applied to decision making. Chapter 3 gives one application of interval neutrosophic sets and logic in the field of relational databases. Neutrosophic data model is the generalization of fuzzy data model and paraconsistent data model. Here, we generalize various set-theoretic and relation-theoretic operations of fuzzy data model to neutrosophic data model. Chapter 4 gives another application of interval neutrosophic logic. A soft semantic Web Services agent framework is proposed to faciliate the registration and discovery of high quality semantic Web Services agent. The intelligent inference engine module of soft Semantic Web Services agent is implemented using interval neutrosophic logic.",
        "published": "2005-05-06T13:57:13Z",
        "link": "http://arxiv.org/abs/cs/0505014v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "State Space Computation and Analysis of Time Petri Nets",
        "authors": [
            "Guillaume Gardey",
            "Olivier H. Roux",
            "Olivier F. Roux"
        ],
        "summary": "The theory of Petri Nets provides a general framework to specify the behaviors of real-time reactive systems and Time Petri Nets were introduced to take also temporal specifications into account. We present in this paper a forward zone-based algorithm to compute the state space of a bounded Time Petri Net: the method is different and more efficient than the classical State Class Graph. We prove the algorithm to be exact with respect to the reachability problem. Furthermore, we propose a translation of the computed state space into a Timed Automaton, proved to be timed bisimilar to the original Time Petri Net. As the method produce a single Timed Automaton, syntactical clocks reduction methods (Daws and Yovine for instance) may be applied to produce an automaton with fewer clocks. Then, our method allows to model-check TTPN by the use of efficient Timed Automata tools.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2005-05-10T13:30:16Z",
        "link": "http://arxiv.org/abs/cs/0505023v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Logic Column 12: Logical Verification and Equational Verification",
        "authors": [
            "Riccardo Pucella"
        ],
        "summary": "This article examines two approaches to verification, one based on using a logic for expressing properties of a system, and one based on showing the system equivalent to a simpler system that obviously has whatever property is of interest. Using examples such as process calculi and regular programs, the relationship between these two approaches is explored.",
        "published": "2005-05-10T14:49:13Z",
        "link": "http://arxiv.org/abs/cs/0505024v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Equivalence-Checking on Infinite-State Systems: Techniques and Results",
        "authors": [
            "Antonin Kucera",
            "Petr Jancar"
        ],
        "summary": "The paper presents a selection of recently developed and/or used techniques for equivalence-checking on infinite-state systems, and an up-to-date overview of existing results (as of September 2004).",
        "published": "2005-05-10T16:47:11Z",
        "link": "http://arxiv.org/abs/cs/0505025v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Automatic Verification of Timed Concurrent Constraint Programs",
        "authors": [
            "Moreno Falaschi",
            "Alicia Villanueva"
        ],
        "summary": "The language Timed Concurrent Constraint (tccp) is the extension over time of the Concurrent Constraint Programming (cc) paradigm that allows us to specify concurrent systems where timing is critical, for example reactive systems. Systems which may have an infinite number of states can be specified in tccp. Model checking is a technique which is able to verify finite-state systems with a huge number of states in an automatic way. In the last years several studies have investigated how to extend model checking techniques to systems with an infinite number of states. In this paper we propose an approach which exploits the computation model of tccp. Constraint based computations allow us to define a methodology for applying a model checking algorithm to (a class of) infinite-state systems. We extend the classical algorithm of model checking for LTL to a specific logic defined for the verification of tccp and to the tccp Structure which we define in this work for modeling the program behavior. We define a restriction on the time in order to get a finite model and then we develop some illustrative examples. To the best of our knowledge this is the first approach that defines a model checking methodology for tccp.",
        "published": "2005-05-11T11:33:52Z",
        "link": "http://arxiv.org/abs/cs/0505026v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Parametric Verification of a Group Membership Algorithm",
        "authors": [
            "Ahmed Bouajjani",
            "Agathe Merceron"
        ],
        "summary": "We address the problem of verifying clique avoidance in the TTP protocol. TTP allows several stations embedded in a car to communicate. It has many mechanisms to ensure robustness to faults. In particular, it has an algorithm that allows a station to recognize itself as faulty and leave the communication. This algorithm must satisfy the crucial 'non-clique' property: it is impossible to have two or more disjoint groups of stations communicating exclusively with stations in their own group.   In this paper, we propose an automatic verification method for an arbitrary number of stations $N$ and a given number of faults $k$. We give an abstraction that allows to model the algorithm by means of unbounded (parametric) counter automata. We have checked the non-clique property on this model in the case of one fault, using the ALV tool as well as the LASH tool.",
        "published": "2005-05-12T07:52:36Z",
        "link": "http://arxiv.org/abs/cs/0505033v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Essential Incompleteness of Arithmetic Verified by Coq",
        "authors": [
            "Russell O'Connor"
        ],
        "summary": "A constructive proof of the Goedel-Rosser incompleteness theorem has been completed using the Coq proof assistant. Some theory of classical first-order logic over an arbitrary language is formalized. A development of primitive recursive functions is given, and all primitive recursive functions are proved to be representable in a weak axiom system. Formulas and proofs are encoded as natural numbers, and functions operating on these codes are proved to be primitive recursive. The weak axiom system is proved to be essentially incomplete. In particular, Peano arithmetic is proved to be consistent in Coq's type theory and therefore is incomplete.",
        "published": "2005-05-12T17:31:25Z",
        "link": "http://arxiv.org/abs/cs/0505034v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "General Recursion via Coinductive Types",
        "authors": [
            "Venanzio Capretta"
        ],
        "summary": "A fertile field of research in theoretical computer science investigates the representation of general recursive functions in intensional type theories. Among the most successful approaches are: the use of wellfounded relations, implementation of operational semantics, formalization of domain theory, and inductive definition of domain predicates. Here, a different solution is proposed: exploiting coinductive types to model infinite computations. To every type A we associate a type of partial elements Partial(A), coinductively generated by two constructors: the first, return(a) just returns an element a:A; the second, step(x), adds a computation step to a recursive element x:Partial(A). We show how this simple device is sufficient to formalize all recursive functions between two given types. It allows the definition of fixed points of finitary, that is, continuous, operators. We will compare this approach to different ones from the literature. Finally, we mention that the formalization, with appropriate structural maps, defines a strong monad.",
        "published": "2005-05-13T15:22:57Z",
        "link": "http://arxiv.org/abs/cs/0505037v6",
        "categories": [
            "cs.LO",
            "F.3.1"
        ]
    },
    {
        "title": "Relational reasoning in the region connection calculus",
        "authors": [
            "Yongming Li",
            "Sanjiang Li",
            "Mingsheng Ying"
        ],
        "summary": "This paper is mainly concerned with the relation-algebraical aspects of the well-known Region Connection Calculus (RCC). We show that the contact relation algebra (CRA) of certain RCC model is not atomic complete and hence infinite. So in general an extensional composition table for the RCC cannot be obtained by simply refining the RCC8 relations. After having shown that each RCC model is a consistent model of the RCC11 CT, we give an exhaustive investigation about extensional interpretation of the RCC11 CT. More important, we show the complemented closed disk algebra is a representation for the relation algebra determined by the RCC11 table. The domain of this algebra contains two classes of regions, the closed disks and closures of their complements in the real plane.",
        "published": "2005-05-14T21:16:31Z",
        "link": "http://arxiv.org/abs/cs/0505041v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Internalising modified realisability in constructive type theory",
        "authors": [
            "Erik Palmgren"
        ],
        "summary": "A modified realisability interpretation of infinitary logic is formalised and proved sound in constructive type theory (CTT). The logic considered subsumes first order logic. The interpretation makes it possible to extract programs with simplified types and to incorporate and reason about them in CTT.",
        "published": "2005-05-19T14:43:41Z",
        "link": "http://arxiv.org/abs/math/0505418v3",
        "categories": [
            "math.LO",
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Approximate reasoning for real-time probabilistic processes",
        "authors": [
            "Vineet Gupta",
            "Radha Jagadeesan",
            "Prakash Panangaden"
        ],
        "summary": "We develop a pseudo-metric analogue of bisimulation for generalized semi-Markov processes. The kernel of this pseudo-metric corresponds to bisimulation; thus we have extended bisimulation for continuous-time probabilistic processes to a much broader class of distributions than exponential distributions. This pseudo-metric gives a useful handle on approximate reasoning in the presence of numerical information -- such as probabilities and time -- in the model. We give a fixed point characterization of the pseudo-metric. This makes available coinductive reasoning principles for reasoning about distances. We demonstrate that our approach is insensitive to potentially ad hoc articulations of distance by showing that it is intrinsic to an underlying uniformity. We provide a logical characterization of this uniformity using a real-valued modal logic. We show that several quantitative properties of interest are continuous with respect to the pseudo-metric. Thus, if two processes are metrically close, then observable quantitative properties of interest are indeed close.",
        "published": "2005-05-24T14:05:01Z",
        "link": "http://arxiv.org/abs/cs/0505063v4",
        "categories": [
            "cs.LO",
            "D.2.4; D.2.8; D.4.8; G.3"
        ]
    },
    {
        "title": "Reasoning about transfinite sequences",
        "authors": [
            "Stéphane Demri",
            "David Nowak"
        ],
        "summary": "We introduce a family of temporal logics to specify the behavior of systems with Zeno behaviors. We extend linear-time temporal logic LTL to authorize models admitting Zeno sequences of actions and quantitative temporal operators indexed by ordinals replace the standard next-time and until future-time operators. Our aim is to control such systems by designing controllers that safely work on $\\omega$-sequences but interact synchronously with the system in order to restrict their behaviors. We show that the satisfiability problem for the logics working on $\\omega^k$-sequences is EXPSPACE-complete when the integers are represented in binary, and PSPACE-complete with a unary representation. To do so, we substantially extend standard results about LTL by introducing a new class of succinct ordinal automata that can encode the interaction between the different quantitative temporal operators.",
        "published": "2005-05-26T09:33:24Z",
        "link": "http://arxiv.org/abs/cs/0505073v4",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Feasible Proofs of Matrix Properties with Csanky's Algorithm",
        "authors": [
            "Michael Soltys"
        ],
        "summary": "We show that Csanky's fast parallel algorithm for computing the characteristic polynomial of a matrix can be formalized in the logical theory LAP, and can be proved correct in LAP from the principle of linear independence. LAP is a natural theory for reasoning about linear algebra introduced by Cook and Soltys. Further, we show that several principles of matrix algebra, such as linear independence or the Cayley-Hamilton Theorem, can be shown equivalent in the logical theory QLA. Applying the separation between complexity classes AC^0[2] contained in DET(GF(2)), we show that these principles are in fact not provable in QLA. In a nutshell, we show that linear independence is ``all there is'' to elementary linear algebra (from a proof complexity point of view), and furthermore, linear independence cannot be proved trivially (again, from a proof complexity point of view).",
        "published": "2005-05-31T11:13:59Z",
        "link": "http://arxiv.org/abs/cs/0505087v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Bounds on the Automata Size for Presburger Arithmetic",
        "authors": [
            "Felix Klaedtke"
        ],
        "summary": "Automata provide a decision procedure for Presburger arithmetic. However, until now only crude lower and upper bounds were known on the sizes of the automata produced by this approach. In this paper, we prove an upper bound on the the number of states of the minimal deterministic automaton for a Presburger arithmetic formula. This bound depends on the length of the formula and the quantifiers occurring in the formula. The upper bound is established by comparing the automata for Presburger arithmetic formulas with the formulas produced by a quantifier elimination method. We also show that our bound is tight, even for nondeterministic automata. Moreover, we provide optimal automata constructions for linear equations and inequations.",
        "published": "2005-06-02T15:11:58Z",
        "link": "http://arxiv.org/abs/cs/0506008v1",
        "categories": [
            "cs.LO",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "The Equivalence Problem for Deterministic MSO Tree Transducers is   Decidable",
        "authors": [
            "Joost Engelfriet",
            "Sebastian Maneth"
        ],
        "summary": "It is decidable for deterministic MSO definable graph-to-string or graph-to-tree transducers whether they are equivalent on a context-free set of graphs.",
        "published": "2005-06-06T07:05:11Z",
        "link": "http://arxiv.org/abs/cs/0506014v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Preferential and Preferential-discriminative Consequence relations",
        "authors": [
            "Jonathan Ben-Naim"
        ],
        "summary": "The present paper investigates consequence relations that are both non-monotonic and paraconsistent. More precisely, we put the focus on preferential consequence relations, i.e. those relations that can be defined by a binary preference relation on states labelled by valuations. We worked with a general notion of valuation that covers e.g. the classical valuations as well as certain kinds of many-valued valuations. In the many-valued cases, preferential consequence relations are paraconsistant (in addition to be non-monotonic), i.e. they are capable of drawing reasonable conclusions which contain contradictions. The first purpose of this paper is to provide in our general framework syntactic characterizations of several families of preferential relations. The second and main purpose is to provide, again in our general framework, characterizations of several families of preferential discriminative consequence relations. They are defined exactly as the plain version, but any conclusion such that its negation is also a conclusion is rejected (these relations bring something new essentially in the many-valued cases).",
        "published": "2005-06-09T12:53:47Z",
        "link": "http://arxiv.org/abs/cs/0506030v6",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Existentially Restricted Quantified Constraint Satisfaction",
        "authors": [
            "Hubie Chen"
        ],
        "summary": "The quantified constraint satisfaction problem (QCSP) is a powerful framework for modelling computational problems. The general intractability of the QCSP has motivated the pursuit of restricted cases that avoid its maximal complexity. In this paper, we introduce and study a new model for investigating QCSP complexity in which the types of constraints given by the existentially quantified variables, is restricted. Our primary technical contribution is the development and application of a general technology for proving positive results on parameterizations of the model, of inclusion in the complexity class coNP.",
        "published": "2005-06-14T11:57:00Z",
        "link": "http://arxiv.org/abs/cs/0506059v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Yet another normalisation proof for Martin-Lof's logical   framework--Terms with correct arities are strongly normalising",
        "authors": [
            "Yong Luo"
        ],
        "summary": "In this paper, we prove the strong normalisation for Martin-L\\\"{o}f's Logical Framework, and suggest that {}``correct arity'', a condition weaker than well-typedness, will also guarantee the strong normalisation.",
        "published": "2005-06-14T12:13:22Z",
        "link": "http://arxiv.org/abs/cs/0506060v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Security Policies as Membranes in Systems for Global Computing",
        "authors": [
            "Daniele Gorla",
            "Matthew Hennessy",
            "Vladimiro Sassone"
        ],
        "summary": "We propose a simple global computing framework, whose main concern is code migration. Systems are structured in sites, and each site is divided into two parts: a computing body, and a membrane, which regulates the interactions between the computing body and the external environment. More precisely, membranes are filters which control access to the associated site, and they also rely on the well-established notion of trust between sites. We develop a basic theory to express and enforce security policies via membranes. Initially, these only control the actions incoming agents intend to perform locally. We then adapt the basic theory to encompass more sophisticated policies, where the number of actions an agent wants to perform, and also their order, are considered.",
        "published": "2005-06-14T18:08:10Z",
        "link": "http://arxiv.org/abs/cs/0506061v5",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; F.3.2; F.3.3; F.4.3"
        ]
    },
    {
        "title": "Redundancy in Logic II: 2CNF and Horn Propositional Formulae",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "We report complexity results about redundancy of formulae in 2CNF form. We first consider the problem of checking redundancy and show some algorithms that are slightly better than the trivial one. We then analyze problems related to finding irredundant equivalent subsets (I.E.S.) of a given set. The concept of cyclicity proved to be relevant to the complexity of these problems. Some results about Horn formulae are also shown.",
        "published": "2005-06-17T19:28:29Z",
        "link": "http://arxiv.org/abs/cs/0506074v3",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Quantitative Models and Implicit Complexity",
        "authors": [
            "U. Dal Lago",
            "M. Hofmann"
        ],
        "summary": "We give new proofs of soundness (all representable functions on base types lies in certain complexity classes) for Elementary Affine Logic, LFPL (a language for polytime computation close to realistic functional programming introduced by one of us), Light Affine Logic and Soft Affine Logic. The proofs are based on a common semantical framework which is merely instantiated in four different ways. The framework consists of an innovative modification of realizability which allows us to use resource-bounded computations as realisers as opposed to including all Turing computable functions as is usually the case in realizability constructions. For example, all realisers in the model for LFPL are polynomially bounded computations whence soundness holds by construction of the model. The work then lies in being able to interpret all the required constructs in the model. While being the first entirely semantical proof of polytime soundness for light logi cs, our proof also provides a notable simplification of the original already semantical proof of polytime soundness for LFPL. A new result made possible by the semantic framework is the addition of polymorphism and a modality to LFPL thus allowing for an internal definition of inductive datatypes.",
        "published": "2005-06-20T16:00:10Z",
        "link": "http://arxiv.org/abs/cs/0506079v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "The Geometry of Linear Higher-Order Recursion",
        "authors": [
            "U. Dal Lago"
        ],
        "summary": "Linearity and ramification constraints have been widely used to weaken higher-order (primitive) recursion in such a way that the class of representable functions equals the class of polytime functions. We show that fine-tuning these two constraints leads to different expressive strengths, some of them lying well beyond polynomial time. This is done by introducing a new semantics, called algebraic context semantics. The framework stems from Gonthier's original work and turns out to be a versatile and powerful tool for the quantitative analysis of normalization in presence of constants and higher-order recursion.",
        "published": "2005-06-20T17:52:02Z",
        "link": "http://arxiv.org/abs/cs/0506080v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "The One Page Model Checker",
        "authors": [
            "Jason E. Holt"
        ],
        "summary": "We show how standard IPC mechanisms can be used with the fork() system call to perform explicit state model checking on all interleavings of a multithreaded application. We specifically show how to check for deadlock and race conditions in programs with two threads. Our techniques are easy to apply to other languages, and require only the most rudimentary parsing of the target language. Our fundamental system fits in one page of C code.",
        "published": "2005-06-22T20:54:08Z",
        "link": "http://arxiv.org/abs/cs/0506084v1",
        "categories": [
            "cs.LO",
            "F.3.1"
        ]
    },
    {
        "title": "Foundations of real analysis and computability theory in   non-Aristotelian finitary logic",
        "authors": [
            "Radhakrishnan Srinivasan",
            "H. P. Raghunandan"
        ],
        "summary": "This paper outlines new paradigms for real analysis and computability theory in the recently proposed non-Aristotelian finitary logic (NAFL). Constructive real analysis in NAFL (NRA) is accomplished by a translation of diagrammatic concepts from Euclidean geometry into an extension (NPAR) of the NAFL version of Peano Arithmetic (NPA). Such a translation is possible because NPA proves the existence of every infinite proper class of natural numbers that is definable in the language of NPA. Infinite sets are not permitted in NPAR and quantification over proper classes is banned; hence Cantor's diagonal argument cannot be legally formulated in NRA, and there is no `cardinality' for any collection (`super-class') of real numbers. Many of the useful aspects of classical real analysis, such as, the calculus of Newton and Leibniz, are justifiable in NRA. But the paradoxes, such as, Zeno's paradoxes of motion and the Banach-Tarski paradox, are resolved because NRA admits only closed super-classes of real numbers; in particular, open/semi-open intervals of real numbers are not permitted. The NAFL version of computability theory (NCT) rejects Turing's argument for the undecidability of the halting problem and permits hypercomputation. Important potential applications of NCT are in the areas of quantum and autonomic computing.",
        "published": "2005-06-23T13:05:27Z",
        "link": "http://arxiv.org/abs/math/0506475v3",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.GM"
        ]
    },
    {
        "title": "Deriving a Stationary Dynamic Bayesian Network from a Logic Program with   Recursive Loops",
        "authors": [
            "Y. D. Shen",
            "Q. Yang",
            "J. H. You",
            "L. Y. Yuan"
        ],
        "summary": "Recursive loops in a logic program present a challenging problem to the PLP framework. On the one hand, they loop forever so that the PLP backward-chaining inferences would never stop. On the other hand, they generate cyclic influences, which are disallowed in Bayesian networks. Therefore, in existing PLP approaches logic programs with recursive loops are considered to be problematic and thus are excluded. In this paper, we propose an approach that makes use of recursive loops to build a stationary dynamic Bayesian network. Our work stems from an observation that recursive loops in a logic program imply a time sequence and thus can be used to model a stationary dynamic Bayesian network without using explicit time parameters. We introduce a Bayesian knowledge base with logic clauses of the form $A \\leftarrow A_1,...,A_l, true, Context, Types$, which naturally represents the knowledge that the $A_i$s have direct influences on $A$ in the context $Context$ under the type constraints $Types$. We then use the well-founded model of a logic program to define the direct influence relation and apply SLG-resolution to compute the space of random variables together with their parental connections. We introduce a novel notion of influence clauses, based on which a declarative semantics for a Bayesian knowledge base is established and algorithms for building a two-slice dynamic Bayesian network from a logic program are developed.",
        "published": "2005-06-27T04:07:34Z",
        "link": "http://arxiv.org/abs/cs/0506095v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.LO"
        ]
    },
    {
        "title": "Polynomial Synthesis of Asynchronous Automata",
        "authors": [
            "Nicolas Baudru",
            "Rémi Morin"
        ],
        "summary": "Zielonka's theorem shows that each regular set of Mazurkiewicz traces can be implemented as a system of synchronized processes with a distributed control structure called asynchronous automaton. This paper gives a polynomial algorithm for the synthesis of a non-deterministic asynchronous automaton from a regular Mazurkiewicz trace language. This new construction is based on an unfolding approach that improves the complexity of Zielonka's and Pighizzini's techniques in terms of the number of states.",
        "published": "2005-06-27T06:09:37Z",
        "link": "http://arxiv.org/abs/cs/0506096v1",
        "categories": [
            "cs.CC",
            "cs.LO"
        ]
    },
    {
        "title": "Introduction to Cirquent Calculus and Abstract Resource Semantics",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "This paper introduces a refinement of the sequent calculus approach called cirquent calculus. While in Gentzen-style proof trees sibling (or cousin, etc.) sequents are disjoint sequences of formulas, in cirquent calculus they are permitted to share elements. Explicitly allowing or disallowing shared resources and thus taking to a more subtle level the resource-awareness intuitions underlying substructural logics, cirquent calculus offers much greater flexibility and power than sequent calculus does. A need for substantially new deductive tools came with the birth of computability logic (see http://www.cis.upenn.edu/~giorgi/cl.html) - the semantically constructed formal theory of computational resources, which has stubbornly resisted any axiomatization attempts within the framework of traditional syntactic approaches. Cirquent calculus breaks the ice. Removing contraction from the full collection of its rules yields a sound and complete system for the basic fragment CL5 of computability logic. Doing the same in sequent calculus, on the other hand, throws out the baby with the bath water, resulting in the strictly weaker affine logic. An implied claim of computability logic is that it is CL5 rather than affine logic that adequately materializes the resource philosophy traditionally associated with the latter. To strengthen this claim, the paper further introduces an abstract resource semantics and shows the soundness and completeness of CL5 with respect to it.",
        "published": "2005-06-27T19:45:06Z",
        "link": "http://arxiv.org/abs/math/0506553v3",
        "categories": [
            "math.LO",
            "cs.LO",
            "03B47 (primary) 03B70, 68Q10, 68T27, 68T15 (secondary)"
        ]
    },
    {
        "title": "Computing minimal models, stable models and answer sets",
        "authors": [
            "Z. Lonc",
            "M. Truszczynski"
        ],
        "summary": "We propose and study algorithms to compute minimal models, stable models and answer sets of t-CNF theories, and normal and disjunctive t-programs. We are especially interested in algorithms with non-trivial worst-case performance bounds. The bulk of the paper is concerned with the classes of 2- and 3-CNF theories, and normal and disjunctive 2- and 3-programs, for which we obtain significantly stronger results than those implied by our general considerations. We show that one can find all minimal models of 2-CNF theories and all answer sets of disjunctive 2-programs in time O(m 1.4422..^n). Our main results concern computing stable models of normal 3-programs, minimal models of 3-CNF theories and answer sets of disjunctive 3-programs. We design algorithms that run in time O(m 1.6701..^n), in the case of the first problem, and in time O(mn^2 2.2782..^n), in the case of the latter two. All these bounds improve by exponential factors the best algorithms known previously. We also obtain closely related upper bounds on the number of minimal models, stable models and answer sets a t-CNF theory, a normal t-program or a disjunctive t-program may have.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2005-06-30T01:51:41Z",
        "link": "http://arxiv.org/abs/cs/0506104v1",
        "categories": [
            "cs.LO",
            "cs.DS",
            "D.1.6"
        ]
    },
    {
        "title": "First-order queries on structures of bounded degree are computable with   constant delay",
        "authors": [
            "Arnaud Durand",
            "Etienne Grandjean"
        ],
        "summary": "A bounded degree structure is either a relational structure all of whose relations are of bounded degree or a functional structure involving bijective functions only. In this paper, we revisit the complexity of the evaluation problem of not necessarily Boolean first-order queries over structures of bounded degree. Query evaluation is considered here as a dynamical process. We prove that any query on bounded degree structures is $\\constantdelaylin$, i.e., can be computed by an algorithm that has two separate parts: it has a precomputation step of linear time in the size of the structure and then, it outputs all tuples one by one with a constant (i.e. depending on the size of the formula only) delay between each. Seen as a global process, this implies that queries on bounded structures can be evaluated in total time $O(f(|\\phi|).(|\\calS|+|\\phi(\\calS)|))$ and space $O(f(|\\phi|).|\\calS|)$ where $\\calS$ is the structure, $\\phi$ is the formula, $\\phi(\\calS)$ is the result of the query and $f$ is some function.   Among other things, our results generalize a result of \\cite{Seese-96} on the data complexity of the model-checking problem for bounded degree structures. Besides, the originality of our approach compared to that \\cite{Seese-96} and comparable results is that it does not rely on the Hanf's model-theoretic technic (see \\cite{Hanf-65}) and is completely effective.",
        "published": "2005-07-07T09:40:28Z",
        "link": "http://arxiv.org/abs/cs/0507020v1",
        "categories": [
            "cs.LO",
            "cs.CC"
        ]
    },
    {
        "title": "Enhancing Global SLS-Resolution with Loop Cutting and Tabling Mechanisms",
        "authors": [
            "Yi-Dong Shen",
            "Jia-Huai You",
            "Li-Yan Yuan"
        ],
        "summary": "Global SLS-resolution is a well-known procedural semantics for top-down computation of queries under the well-founded model. It inherits from SLDNF-resolution the {\\em linearity} property of derivations, which makes it easy and efficient to implement using a simple stack-based memory structure. However, like SLDNF-resolution it suffers from the problem of infinite loops and redundant computations. To resolve this problem, in this paper we develop a new procedural semantics, called {\\em SLTNF-resolution}, by enhancing Global SLS-resolution with loop cutting and tabling mechanisms. SLTNF-resolution is sound and complete w.r.t. the well-founded semantics for logic programs with the bounded-term-size property, and is superior to existing linear tabling procedural semantics such as SLT-resolution.",
        "published": "2005-07-14T08:24:14Z",
        "link": "http://arxiv.org/abs/cs/0507035v1",
        "categories": [
            "cs.LO",
            "cs.AI"
        ]
    },
    {
        "title": "Improved Inference for Checking Annotations",
        "authors": [
            "Peter J Stuckey",
            "Martin Sulzmann",
            "Jeremy Wazny"
        ],
        "summary": "We consider type inference in the Hindley/Milner system extended with type annotations and constraints with a particular focus on Haskell-style type classes. We observe that standard inference algorithms are incomplete in the presence of nested type annotations. To improve the situation we introduce a novel inference scheme for checking type annotations. Our inference scheme is also incomplete in general but improves over existing implementations as found e.g. in the Glasgow Haskell Compiler (GHC). For certain cases (e.g. Haskell 98) our inference scheme is complete. Our approach has been fully implemented as part of the Chameleon system (experimental version of Haskell).",
        "published": "2005-07-14T08:47:45Z",
        "link": "http://arxiv.org/abs/cs/0507036v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Type Inference for Guarded Recursive Data Types",
        "authors": [
            "Peter J. Stuckey",
            "Martin Sulzmann"
        ],
        "summary": "We consider type inference for guarded recursive data types (GRDTs) -- a recent generalization of algebraic data types. We reduce type inference for GRDTs to unification under a mixed prefix. Thus, we obtain efficient type inference. Inference is incomplete because the set of type constraints allowed to appear in the type system is only a subset of those type constraints generated by type inference. Hence, inference only succeeds if the program is sufficiently type annotated. We present refined procedures to infer types incrementally and to assist the user in identifying which pieces of type information are missing. Additionally, we introduce procedures to test if a type is not principal and to find a principal type if one exists.",
        "published": "2005-07-14T08:58:31Z",
        "link": "http://arxiv.org/abs/cs/0507037v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Theoretical Setting of Inner Reversible Quantum Measurements",
        "authors": [
            "Paola Zizzi"
        ],
        "summary": "We show that any unitary transformation performed on the quantum state of a closed quantum system, describes an inner, reversible, generalized quantum measurement. We also show that under some specific conditions it is possible to perform a unitary transformation on the state of the closed quantum system by means of a collection of generalized measurement operators. In particular, given a complete set of orthogonal projectors, it is possible to implement a reversible quantum measurement that preserves the probabilities. In this context, we introduce the concept of \"Truth-Observable\", which is the physical counterpart of an inner logical truth.",
        "published": "2005-07-16T15:58:41Z",
        "link": "http://arxiv.org/abs/quant-ph/0507155v2",
        "categories": [
            "quant-ph",
            "cs.LO",
            "math-ph",
            "math.LO",
            "math.MP"
        ]
    },
    {
        "title": "In the beginning was game semantics",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "This article presents an overview of computability logic -- the game-semantically constructed logic of interactive computational tasks and resources. There is only one non-overview, technical section in it, devoted to a proof of the soundness of affine logic with respect to the semantics of computability logic. A comprehensive online source on the subject can be found at http://www.cis.upenn.edu/~giorgi/cl.html",
        "published": "2005-07-18T18:45:38Z",
        "link": "http://arxiv.org/abs/cs/0507045v3",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Redundancy in Logic III: Non-Mononotonic Reasoning",
        "authors": [
            "Paolo Liberatore"
        ],
        "summary": "Results about the redundancy of circumscriptive and default theories are presented. In particular, the complexity of establishing whether a given theory is redundant is establihsed.",
        "published": "2005-07-19T19:25:11Z",
        "link": "http://arxiv.org/abs/cs/0507048v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Finite automata for testing uniqueness of Eulerian trails",
        "authors": [
            "Qiang Li",
            "Hui-Min Xie"
        ],
        "summary": "We investigate the condition under which the Eulerian trail of a digraph is unique, and design a finite automaton to examine it. The algorithm is effective, for if the condition is violated, it will be noticed immediately without the need to trace through the whole trail.",
        "published": "2005-07-20T18:55:45Z",
        "link": "http://arxiv.org/abs/cs/0507052v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.4.3"
        ]
    },
    {
        "title": "Data complexity of answering conjunctive queries over SHIQ knowledge   bases",
        "authors": [
            "M. Magdalena Ortiz de la Fuente",
            "Diego Calvanese",
            "Thomas Eiter",
            "Enrico Franconi"
        ],
        "summary": "An algorithm for answering conjunctive queries over SHIQ knowledge bases that is coNP in data complexity is given. The algorithm is based on the tableau algorithm for reasoning with individuals in SHIQ. The blocking conditions of the tableau are weakened in such a way that the set of models the modified algorithm yields suffices to check query entailment. The modified blocking conditions are based on the ones proposed by Levy and Rousset for reasoning with Horn Rules in the description logic ALCNR.",
        "published": "2005-07-22T15:43:07Z",
        "link": "http://arxiv.org/abs/cs/0507059v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CC"
        ]
    },
    {
        "title": "Termination of rewriting strategies: a generic approach",
        "authors": [
            "Isabelle Gnaedig",
            "Helene Kirchner"
        ],
        "summary": "We propose a generic termination proof method for rewriting under strategies, based on an explicit induction on the termination property. Rewriting trees on ground terms are modeled by proof trees, generated by alternatively applying narrowing and abstracting steps. The induction principle is applied through the abstraction mechanism, where terms are replaced by variables representing any of their normal forms. The induction ordering is not given a priori, but defined with ordering constraints, incrementally set during the proof. Abstraction constraints can be used to control the narrowing mechanism, well known to easily diverge. The generic method is then instantiated for the innermost, outermost and local strategies.",
        "published": "2005-07-26T20:18:14Z",
        "link": "http://arxiv.org/abs/cs/0507064v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.4.2; F.4.3; I.1.3; I.2.2; I.2.3; D.3.1; D.2.4"
        ]
    },
    {
        "title": "Model Checking Probabilistic Pushdown Automata",
        "authors": [
            "Javier Esparza",
            "Antonin Kucera",
            "Richard Mayr"
        ],
        "summary": "We consider the model checking problem for probabilistic pushdown automata (pPDA) and properties expressible in various probabilistic logics. We start with properties that can be formulated as instances of a generalized random walk problem. We prove that both qualitative and quantitative model checking for this class of properties and pPDA is decidable. Then we show that model checking for the qualitative fragment of the logic PCTL and pPDA is also decidable. Moreover, we develop an error-tolerant model checking algorithm for PCTL and the subclass of stateless pPDA. Finally, we consider the class of omega-regular properties and show that both qualitative and quantitative model checking for pPDA is decidable.",
        "published": "2005-07-31T18:33:43Z",
        "link": "http://arxiv.org/abs/cs/0508003v5",
        "categories": [
            "cs.LO",
            "D.2.4; F.1.1; G.3"
        ]
    },
    {
        "title": "A three-valued semantics for logic programmers",
        "authors": [
            "Lee Naish"
        ],
        "summary": "This paper describes a simpler way for programmers to reason about the correctness of their code. The study of semantics of logic programs has shown strong links between the model theoretic semantics (truth and falsity of atoms in the programmer's interpretation of a program), procedural semantics (for example, SLD resolution) and fixpoint semantics (which is useful for program analysis and alternative execution mechanisms). Most of this work assumes that intended interpretations are two-valued: a ground atom is true (and should succeed according to the procedural semantics) or false (and should not succeed). In reality, intended interpretations are less precise. Programmers consider that some atoms \"should not occur\" or are \"ill-typed\" or \"inadmissible\". Programmers don't know and don't care whether such atoms succeed. In this paper we propose a three-valued semantics for (essentially) pure Prolog programs with (ground) negation as failure which reflects this. The semantics of Fitting is similar but only associates the third truth value with non-termination. We provide tools to reason about correctness of programs without the need for unnatural precision or undue restrictions on programming style. As well as theoretical results, we provide a programmer-oriented synopsis. This work has come out of work on declarative debugging, where it has been recognised that inadmissible calls are important. This paper has been accepted to appear in Theory and Practice of Logic Programming.",
        "published": "2005-08-01T05:23:44Z",
        "link": "http://arxiv.org/abs/cs/0508004v1",
        "categories": [
            "cs.LO",
            "F.3.1; F.3.2"
        ]
    },
    {
        "title": "Logic Column 13: Reasoning Formally about Quantum Systems: An Overview",
        "authors": [
            "Nick Papanikolaou"
        ],
        "summary": "This article is intended as an introduction to the subject of quantum logic, and as a brief survey of the relevant literature. Also discussed here are logics for specification and analysis of quantum information systems, in particular, recent work by P. Mateus and A. Sernadas, and also by R. van der Meyden and M. Patra. Overall, our objective is to provide a high-level presentation of the logical aspects of quantum theory. Mateus' and Sernadas' EQPL logic is illustrated with a small example, namely the state of an entangled pair of qubits. The \"KT\" logic of van der Meyden and Patra is demonstrated briefly in the context of the B92 protocol for quantum key distribution.",
        "published": "2005-08-01T14:34:27Z",
        "link": "http://arxiv.org/abs/cs/0508005v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Deciding Quantifier-Free Presburger Formulas Using Parameterized   Solution Bounds",
        "authors": [
            "Sanjit A. Seshia",
            "Randal E. Bryant"
        ],
        "summary": "Given a formula in quantifier-free Presburger arithmetic, if it has a satisfying solution, there is one whose size, measured in bits, is polynomially bounded in the size of the formula. In this paper, we consider a special class of quantifier-free Presburger formulas in which most linear constraints are difference (separation) constraints, and the non-difference constraints are sparse. This class has been observed to commonly occur in software verification. We derive a new solution bound in terms of parameters characterizing the sparseness of linear constraints and the number of non-difference constraints, in addition to traditional measures of formula size. In particular, we show that the number of bits needed per integer variable is linear in the number of non-difference constraints and logarithmic in the number and size of non-zero coefficients in them, but is otherwise independent of the total number of linear constraints in the formula. The derived bound can be used in a decision procedure based on instantiating integer variables over a finite domain and translating the input quantifier-free Presburger formula to an equi-satisfiable Boolean formula, which is then checked using a Boolean satisfiability solver. In addition to our main theoretical result, we discuss several optimizations for deriving tighter bounds in practice. Empirical evidence indicates that our decision procedure can greatly outperform other decision procedures.",
        "published": "2005-08-05T11:09:32Z",
        "link": "http://arxiv.org/abs/cs/0508044v5",
        "categories": [
            "cs.LO",
            "I.2.3; F.4.1; F.3.1"
        ]
    },
    {
        "title": "An Operational Foundation for Delimited Continuations in the CPS   Hierarchy",
        "authors": [
            "Malgorzata Biernacka",
            "Dariusz Biernacki",
            "Olivier Danvy"
        ],
        "summary": "We present an abstract machine and a reduction semantics for the lambda-calculus extended with control operators that give access to delimited continuations in the CPS hierarchy. The abstract machine is derived from an evaluator in continuation-passing style (CPS); the reduction semantics (i.e., a small-step operational semantics with an explicit representation of evaluation contexts) is constructed from the abstract machine; and the control operators are the shift and reset family. We also present new applications of delimited continuations in the CPS hierarchy: finding list prefixes and normalization by evaluation for a hierarchical language of units and products.",
        "published": "2005-08-08T13:57:34Z",
        "link": "http://arxiv.org/abs/cs/0508048v4",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.1; F.3.2"
        ]
    },
    {
        "title": "Real Hypercomputation and Continuity",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "By the sometimes so-called 'Main Theorem' of Recursive Analysis, every computable real function is necessarily continuous. We wonder whether and which kinds of HYPERcomputation allow for the effective evaluation of also discontinuous f:R->R. More precisely the present work considers the following three super-Turing notions of real function computability:   * relativized computation; specifically given oracle access to the Halting Problem 0' or its jump 0'';   * encoding real input x and/or output y=f(x) in weaker ways also related to the Arithmetic Hierarchy;   * non-deterministic computation.   It turns out that any f:R->R computable in the first or second sense is still necessarily continuous whereas the third type of hypercomputation does provide the required power to evaluate for instance the discontinuous sign function.",
        "published": "2005-08-15T14:18:36Z",
        "link": "http://arxiv.org/abs/cs/0508069v2",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.1.1; F.1.2; F.4.1"
        ]
    },
    {
        "title": "Proceedings of the 15th Workshop on Logic-based methods in Programming   Environments WLPE'05 -- October 5, 2005 -- Sitges (Barcelona), Spain",
        "authors": [
            "Alexander Serebrenik",
            "Susana Munoz-Hernandez"
        ],
        "summary": "This volume contains papers presented at WLPE 2005, 15th International Workshop on Logic-based methods in Programming Environments.   The aim of the workshop is to provide an informal meeting for the researchers working on logic-based tools for development and analysis of programs. This year we emphasized two aspects: on one hand the presentation, pragmatics and experiences of tools for logic programming environments; on the other one, logic-based environmental tools for programming in general.   The workshop took place in Sitges (Barcelona), Spain as a satellite workshop of the 21th International Conference on Logic Programming (ICLP 2005). This workshop continues the series of successful international workshops on logic programming environments held in Ohio, USA (1989), Eilat, Israel (1990), Paris, France (1991), Washington, USA (1992), Vancouver, Canada (1993), Santa Margherita Ligure, Italy (1994), Portland, USA (1995), Leuven, Belgium and Port Jefferson, USA (1997), Las Cruces, USA (1999), Paphos, Cyprus (2001), Copenhagen, Denmark (2002), Mumbai, India (2003) and Saint Malo, France (2004).   We have received eight submissions (2 from France, 2 Spain-US cooperations, one Spain-Argentina cooperation, one from Japan, one from the United Kingdom and one Sweden-France cooperation). Program committee has decided to accept seven papers. This volume contains revised versions of the accepted papers.   We are grateful to the authors of the papers, the reviewers and the members of the Program Committee for the help and fruitful discussions.",
        "published": "2005-08-17T13:35:20Z",
        "link": "http://arxiv.org/abs/cs/0508078v4",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "A primer on Answer Set Programming",
        "authors": [
            "Alessandro Provetti"
        ],
        "summary": "A introduction to the syntax and Semantics of Answer Set Programming intended as an handout to [under]graduate students taking Artificial Intlligence or Logic Programming classes.",
        "published": "2005-08-23T15:05:12Z",
        "link": "http://arxiv.org/abs/cs/0508100v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "D.1.6; I.2.3"
        ]
    },
    {
        "title": "Enhancing the Alloy Analyzer with Patterns of Analysis",
        "authors": [
            "William Heaven",
            "Alessandra Russo"
        ],
        "summary": "Formal techniques have been shown to be useful in the development of correct software. But the level of expertise required of practitioners of these techniques prohibits their widespread adoption. Formal techniques need to be tailored to the commercial software developer. Alloy is a lightweight specification language supported by the Alloy Analyzer (AA), a tool based on off-the-shelf SAT technology. The tool allows a user to check interactively whether given properties are consistent or valid with respect to a high-level specification, providing an environment in which the correctness of such a specification may be established. However, Alloy is not particularly suited to expressing program specifications and the feedback provided by AA can be misleading where the specification under analysis or the property being checked contains inconsistencies. In this paper, we address these two shortcomings. Firstly, we present a lightweight language called \"Loy\", tailored to the specification of object-oriented programs. An encoding of Loy into Alloy is provided so that AA can be used for automated analysis of Loy program specifications. Secondly, we present some \"patterns of analysis\" that guide a developer through the analysis of a Loy specification in order to establish its correctness before implementation.",
        "published": "2005-08-24T15:35:07Z",
        "link": "http://arxiv.org/abs/cs/0508109v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "A study of set-sharing analysis via cliques",
        "authors": [
            "Jorge Navas",
            "Francisco Bueno",
            "Manuel Hermenegildo"
        ],
        "summary": "We study the problem of efficient, scalable set-sharing analysis of logic programs. We use the idea of representing sharing information as a pair of abstract substitutions, one of which is a worst-case sharing representation called a clique set, which was previously proposed for the case of inferring pair-sharing. We use the clique-set representation for (1) inferring actual set-sharing information, and (2) analysis within a top-down framework. In particular, we define the abstract functions required by standard top-down analyses, both for sharing alone and also for the case of including freeness in addition to sharing. Our experimental evaluation supports the conclusion that, for inferring set-sharing, as it was the case for inferring pair-sharing, precision losses are limited, while useful efficiency gains are obtained. At the limit, the clique-set representation allowed analyzing some programs that exceeded memory capacity using classical sharing representations.",
        "published": "2005-08-25T04:10:56Z",
        "link": "http://arxiv.org/abs/cs/0508112v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On Algorithms and Complexity for Sets with Cardinality Constraints",
        "authors": [
            "Bruno Marnette",
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "Typestate systems ensure many desirable properties of imperative programs, including initialization of object fields and correct use of stateful library interfaces. Abstract sets with cardinality constraints naturally generalize typestate properties: relationships between the typestates of objects can be expressed as subset and disjointness relations on sets, and elements of sets can be represented as sets of cardinality one. Motivated by these applications, this paper presents new algorithms and new complexity results for constraints on sets and their cardinalities. We study several classes of constraints and demonstrate a trade-off between their expressive power and their complexity.   Our first result concerns a quantifier-free fragment of Boolean Algebra with Presburger Arithmetic. We give a nondeterministic polynomial-time algorithm for reducing the satisfiability of sets with symbolic cardinalities to constraints on constant cardinalities, and give a polynomial-space algorithm for the resulting problem.   In a quest for more efficient fragments, we identify several subclasses of sets with cardinality constraints whose satisfiability is NP-hard. Finally, we identify a class of constraints that has polynomial-time satisfiability and entailment problems and can serve as a foundation for efficient program analysis.",
        "published": "2005-08-28T22:25:22Z",
        "link": "http://arxiv.org/abs/cs/0508123v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "Temporal Phylogenetic Networks and Logic Programming",
        "authors": [
            "Esra Erdem",
            "Vladimir Lifschitz",
            "Don Ringe"
        ],
        "summary": "The concept of a temporal phylogenetic network is a mathematical model of evolution of a family of natural languages. It takes into account the fact that languages can trade their characteristics with each other when linguistic communities are in contact, and also that a contact is only possible when the languages are spoken at the same time. We show how computational methods of answer set programming and constraint logic programming can be used to generate plausible conjectures about contacts between prehistoric linguistic communities, and illustrate our approach by applying it to the evolutionary history of Indo-European languages.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2005-08-30T13:04:05Z",
        "link": "http://arxiv.org/abs/cs/0508129v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL"
        ]
    },
    {
        "title": "Comparing hierarchies of total functionals",
        "authors": [
            "Dag Normann"
        ],
        "summary": "In this paper we consider two hierarchies of hereditarily total and continuous functionals over the reals based on one extensional and one intensional representation of real numbers, and we discuss under which asumptions these hierarchies coincide. This coincidense problem is equivalent to a statement about the topology of the Kleene-Kreisel continuous functionals. As a tool of independent interest, we show that the Kleene-Kreisel functionals may be embedded into both these hierarchies.",
        "published": "2005-09-07T08:32:32Z",
        "link": "http://arxiv.org/abs/cs/0509019v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Well-founded and Stable Semantics of Logic Programs with Aggregates",
        "authors": [
            "Nikolay Pelov",
            "Marc Denecker",
            "Maurice Bruynooghe"
        ],
        "summary": "In this paper, we present a framework for the semantics and the computation of aggregates in the context of logic programming. In our study, an aggregate can be an arbitrary interpreted second order predicate or function. We define extensions of the Kripke-Kleene, the well-founded and the stable semantics for aggregate programs. The semantics is based on the concept of a three-valued immediate consequence operator of an aggregate program. Such an operator approximates the standard two-valued immediate consequence operator of the program, and induces a unique Kripke-Kleene model, a unique well-founded model and a collection of stable models. We study different ways of defining such operators and thus obtain a framework of semantics, offering different trade-offs between precision and tractability. In particular, we investigate conditions on the operator that guarantee that the computation of the three types of semantics remains on the same level as for logic programs without aggregates. Other results show that, in practice, even efficient three-valued immediate consequence operators which are very low in the precision hierarchy, still provide optimal precision.",
        "published": "2005-09-08T19:54:53Z",
        "link": "http://arxiv.org/abs/cs/0509024v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A formally verified proof of the prime number theorem",
        "authors": [
            "Jeremy Avigad",
            "Kevin Donnelly",
            "David Gray",
            "Paul Raff"
        ],
        "summary": "The prime number theorem, established by Hadamard and de la Vall'ee Poussin independently in 1896, asserts that the density of primes in the positive integers is asymptotic to 1 / ln x. Whereas their proofs made serious use of the methods of complex analysis, elementary proofs were provided by Selberg and Erd\"os in 1948. We describe a formally verified version of Selberg's proof, obtained using the Isabelle proof assistant.",
        "published": "2005-09-09T15:47:35Z",
        "link": "http://arxiv.org/abs/cs/0509025v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Deterministic modal Bayesian Logic: derive the Bayesian within the modal   logic T",
        "authors": [
            "Frederic Dambreville"
        ],
        "summary": "In this paper a conditional logic is defined and studied. This conditional logic, DmBL, is constructed as close as possible to the Bayesian and is unrestricted, that is one is able to use any operator without restriction. A notion of logical independence is also defined within the logic itself. This logic is shown to be non trivial and is not reduced to classical propositions. A model is constructed for the logic. Completeness results are proved. It is shown that any unconditioned probability can be extended to the whole logic DmBL. The Bayesian is then recovered from the probabilistic DmBL. At last, it is shown why DmBL is compliant with Lewis triviality.",
        "published": "2005-09-12T06:11:18Z",
        "link": "http://arxiv.org/abs/math/0509248v1",
        "categories": [
            "math.LO",
            "cs.LO",
            "math.PR"
        ]
    },
    {
        "title": "Computing over the Reals: Foundations for Scientific Computing",
        "authors": [
            "Mark Braverman",
            "Stephen Cook"
        ],
        "summary": "We give a detailed treatment of the ``bit-model'' of computability and complexity of real functions and subsets of R^n, and argue that this is a good way to formalize many problems of scientific computation. In the introduction we also discuss the alternative Blum-Shub-Smale model. In the final section we discuss the issue of whether physical systems could defeat the Church-Turing Thesis.",
        "published": "2005-09-14T17:58:09Z",
        "link": "http://arxiv.org/abs/cs/0509042v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.1; G.0"
        ]
    },
    {
        "title": "Interactive Unawareness Revisited",
        "authors": [
            "Joseph Y. Halpern",
            "Leandro C. Rego"
        ],
        "summary": "We analyze a model of interactive unawareness introduced by Heifetz, Meier and Schipper (HMS). We consider two axiomatizations for their model, which capture different notions of validity. These axiomatizations allow us to compare the HMS approach to both the standard (S5) epistemic logic and two other approaches to unawareness: that of Fagin and Halpern and that of Modica and Rustichini. We show that the differences between the HMS approach and the others are mainly due to the notion of validity used and the fact that the HMS is based on a 3-valued propositional logic.",
        "published": "2005-09-19T17:07:37Z",
        "link": "http://arxiv.org/abs/cs/0509058v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "On the Expressiveness of the Ambient Logic",
        "authors": [
            "Daniel Hirschkoff",
            "Etienne Lozes",
            "Davide Sangiorgi"
        ],
        "summary": "The Ambient Logic (AL) has been proposed for expressing properties of process mobility in the calculus of Mobile Ambients (MA), and as a basis for query languages on semistructured data. In this paper, we study the expressiveness of AL. We define formulas for capabilities and for communication in MA. We also derive some formulas that capture finitess of a term, name occurrences and persistence. We study extensions of the calculus involving more complex forms of communications, and we define characteristic formulas for the equivalence induced by the logic on a subcalculus of MA. This subcalculus is defined by imposing an image-finiteness condition on the reducts of a MA process.",
        "published": "2005-10-04T08:47:55Z",
        "link": "http://arxiv.org/abs/cs/0510010v2",
        "categories": [
            "cs.LO",
            "F.4.3"
        ]
    },
    {
        "title": "Diophantus' 20th Problem and Fermat's Last Theorem for n=4:   Formalization of Fermat's Proofs in the Coq Proof Assistant",
        "authors": [
            "David Delahaye",
            "Micaela Mayero"
        ],
        "summary": "We present the proof of Diophantus' 20th problem (book VI of Diophantus' Arithmetica), which consists in wondering if there exist right triangles whose sides may be measured as integers and whose surface may be a square. This problem was negatively solved by Fermat in the 17th century, who used the \"wonderful\" method (ipse dixit Fermat) of infinite descent. This method, which is, historically, the first use of induction, consists in producing smaller and smaller non-negative integer solutions assuming that one exists; this naturally leads to a reductio ad absurdum reasoning because we are bounded by zero. We describe the formalization of this proof which has been carried out in the Coq proof assistant. Moreover, as a direct and no less historical application, we also provide the proof (by Fermat) of Fermat's last theorem for n=4, as well as the corresponding formalization made in Coq.",
        "published": "2005-10-04T08:53:10Z",
        "link": "http://arxiv.org/abs/cs/0510011v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "math.NT"
        ]
    },
    {
        "title": "On relating CTL to Datalog",
        "authors": [
            "Foto Afrati",
            "Theodore Andronikos",
            "Vassia Pavlaki",
            "Eugenie Foustoucos",
            "Irene Guessarian"
        ],
        "summary": "CTL is the dominant temporal specification language in practice mainly due to the fact that it admits model checking in linear time. Logic programming and the database query language Datalog are often used as an implementation platform for logic languages. In this paper we present the exact relation between CTL and Datalog and moreover we build on this relation and known efficient algorithms for CTL to obtain efficient algorithms for fragments of stratified Datalog. The contributions of this paper are: a) We embed CTL into STD which is a proper fragment of stratified Datalog. Moreover we show that STD expresses exactly CTL -- we prove that by embedding STD into CTL. Both embeddings are linear. b) CTL can also be embedded to fragments of Datalog without negation. We define a fragment of Datalog with the successor build-in predicate that we call TDS and we embed CTL into TDS in linear time. We build on the above relations to answer open problems of stratified Datalog. We prove that query evaluation is linear and that containment and satisfiability problems are both decidable. The results presented in this paper are the first for fragments of stratified Datalog that are more general than those containing only unary EDBs.",
        "published": "2005-10-04T17:29:48Z",
        "link": "http://arxiv.org/abs/cs/0510012v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Semantic Optimization Techniques for Preference Queries",
        "authors": [
            "Jan Chomicki"
        ],
        "summary": "Preference queries are relational algebra or SQL queries that contain occurrences of the winnow operator (\"find the most preferred tuples in a given relation\"). Such queries are parameterized by specific preference relations. Semantic optimization techniques make use of integrity constraints holding in the database. In the context of semantic optimization of preference queries, we identify two fundamental properties: containment of preference relations relative to integrity constraints and satisfaction of order axioms relative to integrity constraints. We show numerous applications of those notions to preference query evaluation and optimization. As integrity constraints, we consider constraint-generating dependencies, a class generalizing functional dependencies. We demonstrate that the problems of containment and satisfaction of order axioms can be captured as specific instances of constraint-generating dependency entailment. This makes it possible to formulate necessary and sufficient conditions for the applicability of our techniques as constraint validity problems. We characterize the computational complexity of such problems.",
        "published": "2005-10-14T13:25:34Z",
        "link": "http://arxiv.org/abs/cs/0510036v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Nonmonotonic Trust Management for P2P Applications",
        "authors": [
            "M. Czenko",
            "H. Tran",
            "J. Doumen",
            "S. Etalle",
            "P. Hartel",
            "J. den Hartog"
        ],
        "summary": "Community decisions about access control in virtual communities are non-monotonic in nature. This means that they cannot be expressed in current, monotonic trust management languages such as the family of Role Based Trust Management languages (RT). To solve this problem we propose RT-, which adds a restricted form of negation to the standard RT language, thus admitting a controlled form of non-monotonicity. The semantics of RT- is discussed and presented in terms of the well-founded semantics for Logic Programs. Finally we discuss how chain discovery can be accomplished for RT-.",
        "published": "2005-10-21T11:46:56Z",
        "link": "http://arxiv.org/abs/cs/0510061v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "The monadic second-order logic of graphs XVI : Canonical graph<br>   decompositions",
        "authors": [
            "Bruno Courcelle"
        ],
        "summary": "This article establishes that the split decomposition of graphs introduced by Cunnigham, is definable in Monadic Second-Order Logic.This result is actually an instance of a more general result covering canonical graph decompositions like the modular decomposition and the Tutte decomposition of 2-connected graphs into 3-connected components. As an application, we prove that the set of graphs having the same cycle matroid as a given 2-connected graph can be defined from this graph by Monadic Second-Order formulas.",
        "published": "2005-10-22T13:09:38Z",
        "link": "http://arxiv.org/abs/cs/0510066v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Comparing Computational Power",
        "authors": [
            "Udi Boker",
            "Nachum Dershowitz"
        ],
        "summary": "It is common practice to compare the computational power of different models of computation. For example, the recursive functions are strictly more powerful than the primitive recursive functions, because the latter are a proper subset of the former (which includes Ackermann's function). Side-by-side with this \"containment\" method of measuring power, it is standard to use an approach based on \"simulation\". For example, one says that the (untyped) lambda calculus is as powerful--computationally speaking--as the partial recursive functions, because the lambda calculus can simulate all partial recursive functions by encoding the natural numbers as Church numerals.   The problem is that unbridled use of these two ways of comparing power allows one to show that some computational models are strictly stronger than themselves! We argue that a better definition is that model A is strictly stronger than B if A can simulate B via some encoding, whereas B cannot simulate A under any encoding. We then show that the recursive functions are strictly stronger in this sense than the primitive recursive. We also prove that the recursive functions, partial recursive functions, and Turing machines are \"complete\", in the sense that no injective encoding can make them equivalent to any \"hypercomputational\" model.",
        "published": "2005-10-23T16:51:02Z",
        "link": "http://arxiv.org/abs/cs/0510069v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Semantic Embedding of Petri Nets into Event-B",
        "authors": [
            "Christian Attiogbe"
        ],
        "summary": "We present an embedding of Petri nets into B abstract systems. The embedding is achieved by translating both the static structure (modelling aspect) and the evolution semantics of Petri nets. The static structure of a Petri-net is captured within a B abstract system through a graph structure. This abstract system is then included in another abstract system which captures the evolution semantics of Petri-nets. The evolution semantics results in some B events depending on the chosen policies: basic nets or high level Petri nets. The current embedding enables one to use conjointly Petri nets and Event-B in the same system development, but at different steps and for various analysis.",
        "published": "2005-10-24T09:17:30Z",
        "link": "http://arxiv.org/abs/cs/0510073v1",
        "categories": [
            "cs.LO",
            "D.2.2; D.2.4; F.3.1"
        ]
    },
    {
        "title": "Context Semantics, Linear Logic and Computational Complexity",
        "authors": [
            "Ugo Dal Lago"
        ],
        "summary": "We show that context semantics can be fruitfully applied to the quantitative analysis of proof normalization in linear logic. In particular, context semantics lets us define the weight of a proof-net as a measure of its inherent complexity: it is both an upper bound to normalization time (modulo a polynomial overhead, independently on the reduction strategy) and a lower bound to the number of steps to normal form (for certain reduction strategies). Weights are then exploited in proving strong soundness theorems for various subsystems of linear logic, namely elementary linear logic, soft linear logic and light linear logic.",
        "published": "2005-10-31T11:05:41Z",
        "link": "http://arxiv.org/abs/cs/0510092v3",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Logical Relations for Monadic Types",
        "authors": [
            "Jean Goubault-Larrecq",
            "Slawomir Lasota",
            "David Nowak"
        ],
        "summary": "Logical relations and their generalizations are a fundamental tool in proving properties of lambda-calculi, e.g., yielding sound principles for observational equivalence. We propose a natural notion of logical relations able to deal with the monadic types of Moggi's computational lambda-calculus. The treatment is categorical, and is based on notions of subsconing, mono factorization systems, and monad morphisms. Our approach has a number of interesting applications, including cases for lambda-calculi with non-determinism (where being in logical relation means being bisimilar), dynamic name creation, and probabilistic systems.",
        "published": "2005-11-02T01:45:56Z",
        "link": "http://arxiv.org/abs/cs/0511006v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Flat and One-Variable Clauses: Complexity of Verifying Cryptographic   Protocols with Single Blind Copying",
        "authors": [
            "Helmut Seidl",
            "Kumar Neeraj Verma"
        ],
        "summary": "Cryptographic protocols with single blind copying were defined and modeled by Comon and Cortier using the new class $\\mathcal C$ of first order clauses. They showed its satisfiability problem to be in 3-DEXPTIME. We improve this result by showing that satisfiability for this class is NEXPTIME-complete, using new resolution techniques. We show satisfiability to be DEXPTIME-complete if clauses are Horn, which is what is required for modeling cryptographic protocols. While translation to Horn clauses only gives a DEXPTIME upper bound for the secrecy problem for these protocols, we further show that this secrecy problem is actually DEXPTIME-complete.",
        "published": "2005-11-03T13:40:22Z",
        "link": "http://arxiv.org/abs/cs/0511014v1",
        "categories": [
            "cs.LO",
            "cs.CR"
        ]
    },
    {
        "title": "Verifying nondeterministic probabilistic channel systems against   $ω$-regular linear-time properties",
        "authors": [
            "Christel Baier",
            "Nathalie Bertrand",
            "Philippe Schnoebelen"
        ],
        "summary": "Lossy channel systems (LCSs) are systems of finite state automata that communicate via unreliable unbounded fifo channels. In order to circumvent the undecidability of model checking for nondeterministic   LCSs, probabilistic models have been introduced, where it can be decided whether a linear-time property holds almost surely. However, such fully probabilistic systems are not a faithful model of nondeterministic protocols.   We study a hybrid model for LCSs where losses of messages are seen as faults occurring with some given probability, and where the internal behavior of the system remains nondeterministic. Thus the semantics is in terms of infinite-state Markov decision processes. The purpose of this article is to discuss the decidability of linear-time properties formalized by formulas of linear temporal logic (LTL). Our focus is on the qualitative setting where one asks, e.g., whether a LTL-formula holds almost surely or with zero probability (in case the formula describes the bad behaviors). Surprisingly, it turns out that -- in contrast to finite-state Markov decision processes -- the satisfaction relation for LTL formulas depends on the chosen type of schedulers that resolve the nondeterminism. While all variants of the qualitative LTL model checking problem for the full class of history-dependent schedulers are undecidable, the same questions for finite-memory scheduler can be solved algorithmically. However, the restriction to reachability properties and special kinds of recurrent reachability properties yields decidable verification problems for the full class of schedulers, which -- for this restricted class of properties -- are as powerful as finite-memory schedulers, or even a subclass of them.",
        "published": "2005-11-04T17:03:45Z",
        "link": "http://arxiv.org/abs/cs/0511023v3",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Logic Column 14: Nominal Logic and Abstract Syntax",
        "authors": [
            "James Cheney"
        ],
        "summary": "Formalizing syntactic proofs of properties of logics, programming languages, security protocols, and other formal systems is a significant challenge, in large part because of the obligation to handle name-binding correctly. We present an approach called nominal abstract syntax that has attracted considerable interest since its introduction approximately six years ago. After an overview of other approaches, we describe nominal abstract syntax and nominal logic, a logic for reasoning about nominal abstract syntax. We also discuss applications of nominal techniques to programming, automated reasoning, and identify some future directions.",
        "published": "2005-11-05T02:47:50Z",
        "link": "http://arxiv.org/abs/cs/0511025v1",
        "categories": [
            "cs.LO",
            "F.4.1; F.3.1"
        ]
    },
    {
        "title": "Towards a unified theory of logic programming semantics: Level mapping   characterizations of selector generated models",
        "authors": [
            "Pascal Hitzler",
            "Sibylle Schwarz"
        ],
        "summary": "Currently, the variety of expressive extensions and different semantics created for logic programs with negation is diverse and heterogeneous, and there is a lack of comprehensive comparative studies which map out the multitude of perspectives in a uniform way. Most recently, however, new methodologies have been proposed which allow one to derive uniform characterizations of different declarative semantics for logic programs with negation. In this paper, we study the relationship between two of these approaches, namely the level mapping characterizations due to [Hitzler and Wendt 2005], and the selector generated models due to [Schwarz 2004]. We will show that the latter can be captured by means of the former, thereby supporting the claim that level mappings provide a very flexible framework which is applicable to very diversely defined semantics.",
        "published": "2005-11-09T14:22:55Z",
        "link": "http://arxiv.org/abs/cs/0511038v1",
        "categories": [
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "Logic Programming with Default, Weak and Strict Negations",
        "authors": [
            "Susumu Yamasaki"
        ],
        "summary": "This paper treats logic programming with three kinds of negation: default, weak and strict negations. A 3-valued logic model theory is discussed for logic programs with three kinds of negation. The procedure is constructed for negations so that a soundness of the procedure is guaranteed in terms of 3-valued logic model theory.",
        "published": "2005-11-10T06:29:46Z",
        "link": "http://arxiv.org/abs/cs/0511041v1",
        "categories": [
            "cs.LO",
            "F.3.2; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Dimensions of Neural-symbolic Integration - A Structured Survey",
        "authors": [
            "Sebastian Bader",
            "Pascal Hitzler"
        ],
        "summary": "Research on integrated neural-symbolic systems has made significant progress in the recent past. In particular the understanding of ways to deal with symbolic knowledge within connectionist systems (also called artificial neural networks) has reached a critical mass which enables the community to strive for applicable implementations and use cases. Recent work has covered a great variety of logics used in artificial intelligence and provides a multitude of techniques for dealing with them within the context of artificial neural networks. We present a comprehensive survey of the field of neural-symbolic integration, including a new classification of system according to their architectures and abilities.",
        "published": "2005-11-10T10:00:46Z",
        "link": "http://arxiv.org/abs/cs/0511042v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.NE"
        ]
    },
    {
        "title": "An Invariant Cost Model for the Lambda Calculus",
        "authors": [
            "Ugo Dal Lago",
            "Simone Martini"
        ],
        "summary": "We define a new cost model for the call-by-value lambda-calculus satisfying the invariance thesis. That is, under the proposed cost model, Turing machines and the call-by-value lambda-calculus can simulate each other within a polynomial time overhead. The model only relies on combinatorial properties of usual beta-reduction, without any reference to a specific machine or evaluator. In particular, the cost of a single beta reduction is proportional to the difference between the size of the redex and the size of the reduct. In this way, the total cost of normalizing a lambda term will take into account the size of all intermediate results (as well as the number of steps to normal form).",
        "published": "2005-11-12T11:57:59Z",
        "link": "http://arxiv.org/abs/cs/0511045v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1"
        ]
    },
    {
        "title": "Embedding Defeasible Logic into Logic Programming",
        "authors": [
            "Grigoris Antoniou",
            "David Billington",
            "Guido Governatori",
            "Michael J. Maher"
        ],
        "summary": "Defeasible reasoning is a simple but efficient approach to nonmonotonic reasoning that has recently attracted considerable interest and that has found various applications. Defeasible logic and its variants are an important family of defeasible reasoning methods. So far no relationship has been established between defeasible logic and mainstream nonmonotonic reasoning approaches.   In this paper we establish close links to known semantics of logic programs. In particular, we give a translation of a defeasible theory D into a meta-program P(D). We show that under a condition of decisiveness, the defeasible consequences of D correspond exactly to the sceptical conclusions of P(D) under the stable model semantics. Without decisiveness, the result holds only in one direction (all defeasible consequences of D are included in all stable models of P(D)). If we wish a complete embedding for the general case, we need to use the Kunen semantics of P(D), instead.",
        "published": "2005-11-15T10:39:38Z",
        "link": "http://arxiv.org/abs/cs/0511055v1",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Truly On-The-Fly LTL Model Checking",
        "authors": [
            "Moritz Hammer",
            "Alexander Knapp",
            "Stephan Merz"
        ],
        "summary": "We propose a novel algorithm for automata-based LTL model checking that interleaves the construction of the generalized B\\\"{u}chi automaton for the negation of the formula and the emptiness check. Our algorithm first converts the LTL formula into a linear weak alternating automaton; configurations of the alternating automaton correspond to the locations of a generalized B\\\"{u}chi automaton, and a variant of Tarjan's algorithm is used to decide the existence of an accepting run of the product of the transition system and the automaton. Because we avoid an explicit construction of the B\\\"{u}chi automaton, our approach can yield significant improvements in runtime and memory, for large LTL formulas. The algorithm has been implemented within the SPIN model checker, and we present experimental results for some benchmark examples.",
        "published": "2005-11-16T12:56:17Z",
        "link": "http://arxiv.org/abs/cs/0511061v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Stochastic Process Semantics for Dynamical Grammar Syntax: An Overview",
        "authors": [
            "Eric Mjolsness"
        ],
        "summary": "We define a class of probabilistic models in terms of an operator algebra of stochastic processes, and a representation for this class in terms of stochastic parameterized grammars. A syntactic specification of a grammar is mapped to semantics given in terms of a ring of operators, so that grammatical composition corresponds to operator addition or multiplication. The operators are generators for the time-evolution of stochastic processes. Within this modeling framework one can express data clustering models, logic programs, ordinary and stochastic differential equations, graph grammars, and stochastic chemical reaction kinetics. This mathematical formulation connects these apparently distant fields to one another and to mathematical methods from quantum field theory and operator algebra.",
        "published": "2005-11-20T00:42:55Z",
        "link": "http://arxiv.org/abs/cs/0511073v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "nlin.AO",
            "D.3.1"
        ]
    },
    {
        "title": "Modularizing the Elimination of r=0 in Kleene Algebra",
        "authors": [
            "Christopher Hardin"
        ],
        "summary": "Given a universal Horn formula of Kleene algebra with hypotheses of the form r = 0, it is already known that we can efficiently construct an equation which is valid if and only if the Horn formula is valid. This is an example of <i>elimination of hypotheses</i>, which is useful because the equational theory of Kleene algebra is decidable while the universal Horn theory is not. We show that hypotheses of the form r = 0 can still be eliminated in the presence of other hypotheses. This lets us extend any technique for eliminating hypotheses to include hypotheses of the form r = 0.",
        "published": "2005-11-28T19:15:39Z",
        "link": "http://arxiv.org/abs/cs/0511097v3",
        "categories": [
            "cs.LO",
            "F.3.1"
        ]
    },
    {
        "title": "Almost periodic functions, constructively",
        "authors": [
            "Bas Spitters"
        ],
        "summary": "The almost periodic functions form a natural example of a non-separable normed space. As such, it has been a challenge for constructive mathematicians to find a natural treatment of them. Here we present a simple proof of Bohr's fundamental theorem for almost periodic functions which we then generalize to almost periodic functions on general topological groups.",
        "published": "2005-12-02T11:30:12Z",
        "link": "http://arxiv.org/abs/cs/0512009v3",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Extending the theory of Owicki and Gries with a logic of progress",
        "authors": [
            "Brijesh Dongol",
            "Doug Goldson"
        ],
        "summary": "This paper describes a logic of progress for concurrent programs. The logic is based on that of UNITY, molded to fit a sequential programming model. Integration of the two is achieved by using auxiliary variables in a systematic way that incorporates program counters into the program text. The rules for progress in UNITY are then modified to suit this new system. This modification is however subtle enough to allow the theory of Owicki and Gries to be used without change.",
        "published": "2005-12-03T01:04:16Z",
        "link": "http://arxiv.org/abs/cs/0512012v3",
        "categories": [
            "cs.LO",
            "D.2.4; D.3.1; F.3.1"
        ]
    },
    {
        "title": "Computably Based Locally Compact Spaces",
        "authors": [
            "Paul Taylor"
        ],
        "summary": "ASD (Abstract Stone Duality) is a re-axiomatisation of general topology in which the topology on a space is treated, not as an infinitary lattice, but as an exponential object of the same category as the original space, with an associated lambda-calculus. In this paper, this is shown to be equivalent to a notion of computable basis for locally compact sober spaces or locales, involving a family of open subspaces and accompanying family of compact ones. This generalises Smyth's effectively given domains and Jung's strong proximity lattices. Part of the data for a basis is the inclusion relation of compact subspaces within open ones, which is formulated in locale theory as the way-below relation on a continuous lattice. The finitary properties of this relation are characterised here, including the Wilker condition for the cover of a compact space by two open ones. The real line is used as a running example, being closely related to Scott's domain of intervals. ASD does not use the category of sets, but the full subcategory of overt discrete objects plays this role; it is an arithmetic universe (pretopos with lists). In particular, we use this subcategory to translate computable bases for classical spaces into objects in the ASD calculus.",
        "published": "2005-12-05T22:31:32Z",
        "link": "http://arxiv.org/abs/math/0512110v4",
        "categories": [
            "math.GN",
            "cs.LO",
            "math.CT",
            "54D45, 03D45 (Primary), 06B35, 54D30, 68N18 (Secondary)"
        ]
    },
    {
        "title": "Alternating Timed Automata",
        "authors": [
            "Slawomir Lasota",
            "Igor Walukiewicz"
        ],
        "summary": "A notion of alternating timed automata is proposed. It is shown that such automata with only one clock have decidable emptiness problem over finite words. This gives a new class of timed languages which is closed under boolean operations and which has an effective presentation. We prove that the complexity of the emptiness problem for alternating timed automata with one clock is non-primitive recursive. The proof gives also the same lower bound for the universality problem for nondeterministic timed automata with one clock. We investigate extension of the model with epsilon-transitions and prove that emptiness is undecidable. Over infinite words, we show undecidability of the universality problem.",
        "published": "2005-12-08T08:50:35Z",
        "link": "http://arxiv.org/abs/cs/0512031v2",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "A System of Interaction and Structure II: The Need for Deep Inference",
        "authors": [
            "Alwen Tiu"
        ],
        "summary": "This paper studies properties of the logic BV, which is an extension of multiplicative linear logic (MLL) with a self-dual non-commutative operator. BV is presented in the calculus of structures, a proof theoretic formalism that supports deep inference, in which inference rules can be applied anywhere inside logical expressions. The use of deep inference results in a simple logical system for MLL extended with the self-dual non-commutative operator, which has been to date not known to be expressible in sequent calculus. In this paper, deep inference is shown to be crucial for the logic BV, that is, any restriction on the ``depth'' of the inference rules of BV would result in a strictly less expressive logical system.",
        "published": "2005-12-09T16:36:54Z",
        "link": "http://arxiv.org/abs/cs/0512036v2",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Termination Analysis of General Logic Programs for Moded Queries: A   Dynamic Approach",
        "authors": [
            "Yi-Dong Shen",
            "Danny De Schreye"
        ],
        "summary": "The termination problem of a logic program can be addressed in either a static or a dynamic way. A static approach performs termination analysis at compile time, while a dynamic approach characterizes and tests termination of a logic program by applying a loop checking technique. In this paper, we present a novel dynamic approach to termination analysis for general logic programs with moded queries. We address several interesting questions, including how to formulate an SLDNF-derivation for a moded query, how to characterize an infinite SLDNF-derivation with a moded query, and how to apply a loop checking mechanism to cut infinite SLDNF-derivations for the purpose of termination analysis. The proposed approach is very powerful and useful. It can be used (1) to test if a logic program terminates for a given concrete or moded query, (2) to test if a logic program terminates for all concrete or moded queries, and (3) to find all (most general) concrete/moded queries that are most likely terminating (or non-terminating).",
        "published": "2005-12-14T07:09:50Z",
        "link": "http://arxiv.org/abs/cs/0512055v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6"
        ]
    },
    {
        "title": "A Categorical Quantum Logic",
        "authors": [
            "Samson Abramsky",
            "Ross Duncan"
        ],
        "summary": "We define a strongly normalising proof-net calculus corresponding to the logic of strongly compact closed categories with biproducts. The calculus is a full and faithful representation of the free strongly compact closed category with biproducts on a given category with an involution. This syntax can be used to represent and reason about quantum processes.",
        "published": "2005-12-15T03:37:38Z",
        "link": "http://arxiv.org/abs/quant-ph/0512114v1",
        "categories": [
            "quant-ph",
            "cs.LO"
        ]
    },
    {
        "title": "Solving Partial Order Constraints for LPO Termination",
        "authors": [
            "Michael Codish",
            "Vitaly Lagoon",
            "Peter J. Stuckey"
        ],
        "summary": "This paper introduces a new kind of propositional encoding for reasoning about partial orders. The symbols in an unspecified partial order are viewed as variables which take integer values and are interpreted as indices in the order. For a partial order statement on n symbols each index is represented in log2 n propositional variables and partial order constraints between symbols are modeled on the bit representations. We illustrate the application of our approach to determine LPO termination for term rewrite systems. Experimental results are unequivocal, indicating orders of magnitude speedups in comparison with current implementations for LPO termination. The proposed encoding is general and relevant to other applications which involve propositional reasoning about partial orders.",
        "published": "2005-12-16T01:28:25Z",
        "link": "http://arxiv.org/abs/cs/0512067v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SC",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "A Fixpoint Semantics of Event Systems with and without Fairness   Assumptions",
        "authors": [
            "Hector Ruiz Barradas",
            "Didier Bert"
        ],
        "summary": "We present a fixpoint semantics of event systems. The semantics is presented in a general framework without concerns of fairness. Soundness and completeness of rules for deriving \"leads-to\" properties are proved in this general framework. The general framework is instantiated to minimal progress and weak fairness assumptions and similar results are obtained. We show the power of these results by deriving sufficient conditions for \"leads-to\" under minimal progress proving soundness of proof obligations without reasoning over state-traces.",
        "published": "2005-12-21T08:23:05Z",
        "link": "http://arxiv.org/abs/cs/0512082v1",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "On the Axiomatisation of Boolean Categories with and without Medial",
        "authors": [
            "Lutz Strassburger"
        ],
        "summary": "The term ``Boolean category'' should be used for describing an object that is to categories what a Boolean algebra is to posets. More specifically, a Boolean category should provide the abstract algebraic structure underlying the proofs in Boolean Logic, in the same sense as a Cartesian closed category captures the proofs in intuitionistic logic and a *-autonomous category captures the proofs in linear logic. However, recent work has shown that there is no canonical axiomatisation of a Boolean category. In this work, we will see a series (with increasing strength) of possible such axiomatisations, all based on the notion of *-autonomous category. We will particularly focus on the medial map, which has its origin in an inference rule in KS, a cut-free deductive system for Boolean logic in the calculus of structures. Finally, we will present a category of proof nets as a particularly well-behaved example of a Boolean category.",
        "published": "2005-12-22T17:43:53Z",
        "link": "http://arxiv.org/abs/cs/0512086v3",
        "categories": [
            "cs.LO"
        ]
    },
    {
        "title": "Book review \"The Haskell Road to Logic, Maths and Programming\"",
        "authors": [
            "Ralf Laemmel"
        ],
        "summary": "The textbook by Doets and van Eijck puts the Haskell programming language systematically to work for presenting a major piece of logic and mathematics. The reader is taken through chapters on basic logic, proof recipes, sets and lists, relations and functions, recursion and co-recursion, the number systems, polynomials and power series, ending with Cantor's infinities. The book uses Haskell for the executable and strongly typed manifestation of various mathematical notions at the level of declarative programming. The book adopts a systematic but relaxed mathematical style (definition, example, exercise, ...); the text is very pleasant to read due to a small amount of anecdotal information, and due to the fact that definitions are fluently integrated in the running text. An important goal of the book is to get the reader acquainted with reasoning about programs.",
        "published": "2005-12-24T23:08:25Z",
        "link": "http://arxiv.org/abs/cs/0512096v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1; F.3.1; G.0"
        ]
    },
    {
        "title": "The logic of interactive Turing reduction",
        "authors": [
            "Giorgi Japaridze"
        ],
        "summary": "The paper gives a soundness and completeness proof for the implicative fragment of intuitionistic calculus with respect to the semantics of computability logic, which understands intuitionistic implication as interactive algorithmic reduction. This concept -- more precisely, the associated concept of reducibility -- is a generalization of Turing reducibility from the traditional, input/output sorts of problems to computational tasks of arbitrary degrees of interactivity. See http://www.cis.upenn.edu/~giorgi/cl.html for a comprehensive online source on computability logic.",
        "published": "2005-12-28T07:25:57Z",
        "link": "http://arxiv.org/abs/cs/0512100v4",
        "categories": [
            "cs.LO",
            "cs.AI",
            "math.LO",
            "F.1.1; F.1.2"
        ]
    },
    {
        "title": "Enabling Agents to Dynamically Select Protocols for Interactions",
        "authors": [
            "Jose Ghislain Quenum Samir Aknine"
        ],
        "summary": "in this paper we describe a method which allows agents to dynamically select protocols and roles when they need to execute collaborative tasks",
        "published": "2005-01-18T17:08:05Z",
        "link": "http://arxiv.org/abs/cs/0501036v2",
        "categories": [
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "Multi-Vehicle Cooperative Control Using Mixed Integer Linear Programming",
        "authors": [
            "Matthew G. Earl",
            "Raffaello D'Andrea"
        ],
        "summary": "We present methods to synthesize cooperative strategies for multi-vehicle control problems using mixed integer linear programming. Complex multi-vehicle control problems are expressed as mixed logical dynamical systems. Optimal strategies for these systems are then solved for using mixed integer linear programming. We motivate the methods on problems derived from an adversarial game between two teams of robots called RoboFlag. We assume the strategy for one team is fixed and governed by state machines. The strategy for the other team is generated using our methods. Finally, we perform an average case computational complexity study on our approach.",
        "published": "2005-01-31T01:03:54Z",
        "link": "http://arxiv.org/abs/cs/0501092v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.MA",
            "I.2.9; I.2.8; I.2.11"
        ]
    },
    {
        "title": "Tables, Memorized Semirings and Applications",
        "authors": [
            "Cyrille Bertelle",
            "Gérard Henry Edmond Duchamp",
            "Khalaf Khatatneh"
        ],
        "summary": "We define and construct a new data structure, the tables, this structure generalizes the (finite) $k$-sets sets of Eilenberg \\cite{Ei}, it is versatile (one can vary the letters, the words and the coefficients). We derive from this structure a new semiring (with several semiring structures) which can be applied to the needs of automatic processing multi-agents behaviour problems. The purpose of this account/paper is to present also the basic elements of this new structures from a combinatorial point of view. These structures present a bunch of properties. They will be endowed with several laws namely : Sum, Hadamard product, Cauchy product, Fuzzy operations (min, max, complemented product) Two groups of applications are presented. The first group is linked to the process of \"forgetting\" information in the tables. The second, linked to multi-agent systems, is announced by showing a methodology to manage emergent organization from individual behaviour models.",
        "published": "2005-02-20T09:08:03Z",
        "link": "http://arxiv.org/abs/cs/0502081v1",
        "categories": [
            "cs.MA",
            "cs.DM",
            "G2"
        ]
    },
    {
        "title": "Self-Replicating Strands that Self-Assemble into User-Specified Meshes",
        "authors": [
            "Robert Ewaschuk",
            "Peter Turney"
        ],
        "summary": "It has been argued that a central objective of nanotechnology is to make products inexpensively, and that self-replication is an effective approach to very low-cost manufacturing. The research presented here is intended to be a step towards this vision. In previous work (JohnnyVon 1.0), we simulated machines that bonded together to form self-replicating strands. There were two types of machines (called types 0 and 1), which enabled strands to encode arbitrary bit strings. However, the information encoded in the strands had no functional role in the simulation. The information was replicated without being interpreted, which was a significant limitation for potential manufacturing applications. In the current work (JohnnyVon 2.0), the information in a strand is interpreted as instructions for assembling a polygonal mesh. There are now four types of machines and the information encoded in a strand determines how it folds. A strand may be in an unfolded state, in which the bonds are straight (although they flex slightly due to virtual forces acting on the machines), or in a folded state, in which the bond angles depend on the types of machines. By choosing the sequence of machine types in a strand, the user can specify a variety of polygonal shapes. A simulation typically begins with an initial unfolded seed strand in a soup of unbonded machines. The seed strand replicates by bonding with free machines in the soup. The child strands fold into the encoded polygonal shape, and then the polygons drift together and bond to form a mesh. We demonstrate that a variety of polygonal meshes can be manufactured in the simulation, by simply changing the sequence of machine types in the seed.",
        "published": "2005-02-22T16:53:15Z",
        "link": "http://arxiv.org/abs/cs/0502087v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "Coalition Formation: Concessions, Task Relationships and Complexity   Reduction",
        "authors": [
            "Samir Aknine",
            "Onn Shehory"
        ],
        "summary": "Solutions to the coalition formation problem commonly assume agent rationality and, correspondingly, utility maximization. This in turn may prevent agents from making compromises. As shown in recent studies, compromise may facilitate coalition formation and increase agent utilities. In this study we leverage on those new results. We devise a novel coalition formation mechanism that enhances compromise. Our mechanism can utilize information on task dependencies to reduce formation complexity. Further, it works well with both cardinal and ordinal task values. Via experiments we show that the use of the suggested compromise-based coalition formation mechanism provides significant savings in the computation and communication complexity of coalition formation. Our results also show that when information on task dependencies is used, the complexity of coalition formation is further reduced. We demonstrate successful use of the mechanism for collaborative information filtering, where agents combine linguistic rules to analyze documents' contents.",
        "published": "2005-02-27T18:16:30Z",
        "link": "http://arxiv.org/abs/cs/0502094v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Stabilization of Cooperative Information Agents in Unpredictable   Environment: A Logic Programming Approach",
        "authors": [
            "Phan Minh Dung",
            "Do Duc Hanh",
            "Phan Minh Thang"
        ],
        "summary": "An information agent is viewed as a deductive database consisting of 3 parts: an observation database containing the facts the agent has observed or sensed from its surrounding environment, an input database containing the information the agent has obtained from other agents, and an intensional database which is a set of rules for computing derived information from the information stored in the observation and input databases. Stabilization of a system of information agents represents a capability of the agents to eventually get correct information about their surrounding despite unpredictable environment changes and the incapability of many agents to sense such changes causing them to have temporary incorrect information. We argue that the stabilization of a system of cooperative information agents could be understood as the convergence of the behavior of the whole system toward the behavior of a \"superagent\", who has the sensing and computing capabilities of all agents combined. We show that unfortunately, stabilization is not guaranteed in general, even if the agents are fully cooperative and do not hide any information from each other. We give sufficient conditions for stabilization and discuss the consequences of our results.",
        "published": "2005-03-14T09:07:24Z",
        "link": "http://arxiv.org/abs/cs/0503028v2",
        "categories": [
            "cs.LO",
            "cs.MA",
            "cs.PL",
            "F.4.1; I.2.3; I.2.11"
        ]
    },
    {
        "title": "Dichotomy for Voting Systems",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra"
        ],
        "summary": "Scoring protocols are a broad class of voting systems. Each is defined by a vector $(\\alpha_1,\\alpha_2,...,\\alpha_m)$, $\\alpha_1 \\geq \\alpha_2 \\geq >... \\geq \\alpha_m$, of integers such that each voter contributes $\\alpha_1$ points to his/her first choice, $\\alpha_2$ points to his/her second choice, and so on, and any candidate receiving the most points is a winner.   What is it about scoring-protocol election systems that makes some have the desirable property of being NP-complete to manipulate, while others can be manipulated in polynomial time? We find the complete, dichotomizing answer: Diversity of dislike. Every scoring-protocol election system having two or more point values assigned to candidates other than the favorite--i.e., having $||\\{\\alpha_i \\condition 2 \\leq i \\leq m\\}||\\geq 2$--is NP-complete to manipulate. Every other scoring-protocol election system can be manipulated in polynomial time. In effect, we show that--other than trivial systems (where all candidates alway tie), plurality voting, and plurality voting's transparently disguised translations--\\emph{every} scoring-protocol election system is NP-complete to manipulate.",
        "published": "2005-04-15T22:11:55Z",
        "link": "http://arxiv.org/abs/cs/0504075v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.1.3; F.2.2"
        ]
    },
    {
        "title": "Cooperative Game Theory within Multi-Agent Systems for Systems   Scheduling",
        "authors": [
            "Derek Messie",
            "Jae C. Oh"
        ],
        "summary": "Research concerning organization and coordination within multi-agent systems continues to draw from a variety of architectures and methodologies. The work presented in this paper combines techniques from game theory and multi-agent systems to produce self-organizing, polymorphic, lightweight, embedded agents for systems scheduling within a large-scale real-time systems environment. Results show how this approach is used to experimentally produce optimum real-time scheduling through the emergent behavior of thousands of agents. These results are obtained using a SWARM simulation of systems scheduling within a High Energy Physics experiment consisting of 2500 digital signal processors.",
        "published": "2005-04-29T13:43:30Z",
        "link": "http://arxiv.org/abs/cs/0504108v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "A T Step Ahead Optimal Target Detection Algorithm for a Multi Sensor   Surveillance System",
        "authors": [
            "K Madhava Krishna",
            "Henry Hexmoor",
            "Shravan Sogani"
        ],
        "summary": "This paper presents a methodology for optimal target detection in a multi sensor surveillance system. The system consists of mobile sensors that guard a rectangular surveillance zone crisscrossed by moving targets. Targets percolate the surveillance zone in a poisson fashion with uniform velocities. Under these statistics this paper computes a motion strategy for a sensor that maximizes target detections for the next T time steps. A coordination mechanism between sensors ensures that overlapping areas between sensors is reduced. This coordination mechanism is interleaved with the motion strategy computation to reduce detections of the same target by more than one sensor. To avoid an exhaustive search in the joint space of all sensors the coordination mechanism constraints the search by assigning priorities to the sensors. A comparison of this methodology with other multi target tracking schemes verifies its efficacy in maximizing detections. A tabulation of these comparisons is reported in results section of the paper",
        "published": "2005-05-17T13:06:41Z",
        "link": "http://arxiv.org/abs/cs/0505045v1",
        "categories": [
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "A network analysis of committees in the United States House of   Representatives",
        "authors": [
            "Mason A. Porter",
            "Peter J. Mucha",
            "M. E. J. Newman",
            "Casey M. Warmbrand"
        ],
        "summary": "Network theory provides a powerful tool for the representation and analysis of complex systems of interacting agents. Here we investigate the United States House of Representatives network of committees and subcommittees, with committees connected according to ``interlocks'' or common membership. Analysis of this network reveals clearly the strong links between different committees, as well as the intrinsic hierarchical structure within the House as a whole. We show that network theory, combined with the analysis of roll call votes using singular value decomposition, successfully uncovers political and organizational correlations between committees in the House without the need to incorporate other political information.",
        "published": "2005-05-17T15:30:38Z",
        "link": "http://arxiv.org/abs/nlin/0505043v1",
        "categories": [
            "nlin.AO",
            "cs.MA",
            "math.ST",
            "physics.data-an",
            "physics.soc-ph",
            "stat.TH"
        ]
    },
    {
        "title": "Emergent Statistical Wealth Distributions in Simple Monetary Exchange   Models: A Critical Review",
        "authors": [
            "Thomas Lux"
        ],
        "summary": "This paper reviews recent attempts at modelling inequality of wealth as an emergent phenomenon of interacting-agent processes. We point out that recent models of wealth condensation which draw their inspiration from molecular dynamics have, in fact, reinvented a process introduced quite some time ago by Angle (1986) in the sociological literature. We emphasize some problematic aspects of simple wealth exchange models and contrast them with a monetary model based on economic principles of market mediated exchange. The paper also reports new results on the influence of market power on the wealth distribution in statistical equilibrium. As it turns out, inequality increases but market power alone is not sufficient for changing the exponential tails of simple exchange models into Pareto tails.",
        "published": "2005-06-24T16:46:47Z",
        "link": "http://arxiv.org/abs/cs/0506092v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Anyone but Him: The Complexity of Precluding an Alternative",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Joerg Rothe"
        ],
        "summary": "Preference aggregation in a multiagent setting is a central issue in both human and computer contexts. In this paper, we study in terms of complexity the vulnerability of preference aggregation to destructive control. That is, we study the ability of an election's chair to, through such mechanisms as voter/candidate addition/suppression/partition, ensure that a particular candidate (equivalently, alternative) does not win. And we study the extent to which election systems can make it impossible, or computationally costly (NP-complete), for the chair to execute such control. Among the systems we study--plurality, Condorcet, and approval voting--we find cases where systems immune or computationally resistant to a chair choosing the winner nonetheless are vulnerable to the chair blocking a victory. Beyond that, we see that among our studied systems no one system offers the best protection against destructive control. Rather, the choice of a preference aggregation system will depend closely on which types of control one wishes to be protected against. We also find concrete cases where the complexity of or susceptibility to control varies dramatically based on the choice among natural tie-handling rules.",
        "published": "2005-07-09T18:20:00Z",
        "link": "http://arxiv.org/abs/cs/0507027v4",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.MA",
            "I.2.11; F.2.2; F.1.3"
        ]
    },
    {
        "title": "Metamimetic Games : Modeling Metadynamics in Social Cognition",
        "authors": [
            "David Chavalarias"
        ],
        "summary": "Imitation is fundamental in the understanding of social system dynamics. But the diversity of imitation rules employed by modelers proves that the modeling of mimetic processes cannot avoid the traditional problem of endogenization of all the choices, including the one of the mimetic rules. Starting from the remark that human reflexive capacities are the ground for a new class of mimetic rules, I propose a formal framework, metamimetic games, that enable to endogenize the distribution of imitation rules while being human specific. The corresponding concepts of equilibrium - counterfactually stable state - and attractor are introduced. Finally, I give an interpretation of social differentiation in terms of cultural co-evolution among a set of possible motivations, which departs from the traditional view of optimization indexed to criteria that exist prior to the activity of agents.",
        "published": "2005-08-01T14:46:25Z",
        "link": "http://arxiv.org/abs/nlin/0508006v3",
        "categories": [
            "nlin.AO",
            "cs.MA",
            "nlin.CG"
        ]
    },
    {
        "title": "Truth-telling Reservations",
        "authors": [
            "Fang Wu",
            "Zi Zhang",
            "Bernardo A. Huberman"
        ],
        "summary": "We present a mechanism for reservations of bursty resources that is both truthful and robust. It consists of option contracts whose pricing structure induces users to reveal the true likelihoods that they will purchase a given resource. Users are also allowed to adjust their options as their likelihood changes. This scheme helps users save cost and the providers to plan ahead so as to reduce the risk of under-utilization and overbooking. The mechanism extracts revenue similar to that of a monopoly provider practicing temporal pricing discrimination with a user population whose preference distribution is known in advance.",
        "published": "2005-08-03T17:11:12Z",
        "link": "http://arxiv.org/abs/cs/0508028v1",
        "categories": [
            "cs.GT",
            "cond-mat.stat-mech",
            "cs.MA"
        ]
    },
    {
        "title": "Selfish vs. Unselfish Optimization of Network Creation",
        "authors": [
            "Johannes J. Schneider",
            "Scott Kirkpatrick"
        ],
        "summary": "We investigate several variants of a network creation model: a group of agents builds up a network between them while trying to keep the costs of this network small. The cost function consists of two addends, namely (i) a constant amount for each edge an agent buys and (ii) the minimum number of hops it takes sending messages to other agents. Despite the simplicity of this model, various complex network structures emerge depending on the weight between the two addends of the cost function and on the selfish or unselfish behaviour of the agents.",
        "published": "2005-08-03T18:16:13Z",
        "link": "http://arxiv.org/abs/cs/0508029v1",
        "categories": [
            "cs.NI",
            "cs.AR",
            "cs.MA"
        ]
    },
    {
        "title": "Polymorphic Self-* Agents for Stigmergic Fault Mitigation in Large-Scale   Real-Time Embedded Systems",
        "authors": [
            "Derek Messie",
            "Jae C. Oh"
        ],
        "summary": "Organization and coordination of agents within large-scale, complex, distributed environments is one of the primary challenges in the field of multi-agent systems. A lot of interest has surfaced recently around self-* (self-organizing, self-managing, self-optimizing, self-protecting) agents. This paper presents polymorphic self-* agents that evolve a core set of roles and behavior based on environmental cues. The agents adapt these roles based on the changing demands of the environment, and are directly implementable in computer systems applications. The design combines strategies from game theory, stigmergy, and other biologically inspired models to address fault mitigation in large-scale, real-time, distributed systems. The agents are embedded within the individual digital signal processors of BTeV, a High Energy Physics experiment consisting of 2500 such processors. Results obtained using a SWARM simulation of the BTeV environment demonstrate the polymorphic character of the agents, and show how this design exceeds performance and reliability metrics obtained from comparable centralized, and even traditional decentralized approaches.",
        "published": "2005-08-04T01:43:42Z",
        "link": "http://arxiv.org/abs/cs/0508032v1",
        "categories": [
            "cs.AI",
            "cs.MA"
        ]
    },
    {
        "title": "Traders imprint themselves by adaptively updating their own avatar",
        "authors": [
            "Gilles Daniel",
            "Lev Muchnik",
            "Sorin Solomon"
        ],
        "summary": "Simulations of artificial stock markets were considered as early as 1964 and multi-agent ones were introduced as early as 1989. Starting the early 90's, collaborations of economists and physicists produced increasingly realistic simulation platforms. Currently, the market stylized facts are easily reproduced and one has now to address the realistic details of the Market Microstructure and of the Traders Behaviour. This calls for new methods and tools capable of bridging smoothly between simulations and experiments in economics.   We propose here the following Avatar-Based Method (ABM). The subjects implement and maintain their Avatars (programs encoding their personal decision making procedures) on NatLab, a market simulation platform. Once these procedures are fed in a computer edible format, they can be operationally used as such without the need for belabouring, interpreting or conceptualising them. Thus ABM short-circuits the usual behavioural economics experiments that search for the psychological mechanisms underlying the subjects behaviour. Finally, ABM maintains a level of objectivity close to the classical behaviourism while extending its scope to subjects' decision making mechanisms.   We report on experiments where Avatars designed and maintained by humans from different backgrounds (including real traders) compete in a continuous double-auction market. We hope this unbiased way of capturing the adaptive evolution of real subjects behaviour may lead to a new kind of behavioural economics experiments with a high degree of reliability, analysability and reproducibility.",
        "published": "2005-09-06T17:02:52Z",
        "link": "http://arxiv.org/abs/cs/0509017v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "Sharp transition towards shared vocabularies in multi-agent systems",
        "authors": [
            "A. Baronchelli",
            "M. Felici",
            "E. Caglioti",
            "V. Loreto",
            "L. Steels"
        ],
        "summary": "What processes can explain how very large populations are able to converge on the use of a particular word or grammatical construction without global coordination? Answering this question helps to understand why new language constructs usually propagate along an S-shaped curve with a rather sudden transition towards global agreement. It also helps to analyze and design new technologies that support or orchestrate self-organizing communication systems, such as recent social tagging systems for the web. The article introduces and studies a microscopic model of communicating autonomous agents performing language games without any central control. We show that the system undergoes a disorder/order transition, going trough a sharp symmetry breaking process to reach a shared set of conventions. Before the transition, the system builds up non-trivial scale-invariant correlations, for instance in the distribution of competing synonyms, which display a Zipf-like law. These correlations make the system ready for the transition towards shared conventions, which, observed on the time-scale of collective behaviors, becomes sharper and sharper with system size. This surprising result not only explains why human language can scale up to very large populations but also suggests ways to optimize artificial semiotic dynamics.",
        "published": "2005-09-09T14:03:06Z",
        "link": "http://arxiv.org/abs/physics/0509075v2",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "Friends for Free: Self-Organizing Artificial Social Networks for Trust   and Cooperation",
        "authors": [
            "David Hales",
            "Stefano Arteconi"
        ],
        "summary": "By harvesting friendship networks from e-mail contacts or instant message \"buddy lists\" Peer-to-Peer (P2P) applications can improve performance in low trust environments such as the Internet. However, natural social networks are not always suitable, reliable or available. We propose an algorithm (SLACER) that allows peer nodes to create and manage their own friendship networks.   We evaluate performance using a canonical test application, requiring cooperation between peers for socially optimal outcomes. The Artificial Social Networks (ASN) produced are connected, cooperative and robust - possessing many of the disable properties of human friendship networks such as trust between friends (directly linked peers) and short paths linking everyone via a chain of friends.   In addition to new application possibilities, SLACER could supply ASN to P2P applications that currently depend on human social networks thus transforming them into fully autonomous, self-managing systems.",
        "published": "2005-09-14T07:30:42Z",
        "link": "http://arxiv.org/abs/cs/0509037v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Effect of door delay on aircraft evacuation time",
        "authors": [
            "Martyn Amos",
            "Andrew Wood"
        ],
        "summary": "The recent commercial launch of twin-deck Very Large Transport Aircraft (VLTA) such as the Airbus A380 has raised questions concerning the speed at which they may be evacuated. The abnormal height of emergency exits on the upper deck has led to speculation that emotional factors such as fear may lead to door delay, and thus play a significant role in increasing overall evacuation time. Full-scale evacuation tests are financially expensive and potentially hazardous, and systematic studies of the evacuation of VLTA are rare. Here we present a computationally cheap agent-based framework for the general simulation of aircraft evacuation, and apply it to the particular case of the Airbus A380. In particular, we investigate the effect of door delay, and conclude that even a moderate average delay can lead to evacuation times that exceed the maximum for safety certification. The model suggests practical ways to minimise evacuation time, as well as providing a general framework for the simulation of evacuation.",
        "published": "2005-09-16T17:15:46Z",
        "link": "http://arxiv.org/abs/cs/0509050v1",
        "categories": [
            "cs.MA"
        ]
    },
    {
        "title": "Guarantees for the Success Frequency of an Algorithm for Finding   Dodgson-Election Winners",
        "authors": [
            "Christopher M. Homan",
            "Lane A. Hemaspaandra"
        ],
        "summary": "In the year 1876 the mathematician Charles Dodgson, who wrote fiction under the now more famous name of Lewis Carroll, devised a beautiful voting system that has long fascinated political scientists. However, determining the winner of a Dodgson election is known to be complete for the \\Theta_2^p level of the polynomial hierarchy. This implies that unless P=NP no polynomial-time solution to this problem exists, and unless the polynomial hierarchy collapses to NP the problem is not even in NP. Nonetheless, we prove that when the number of voters is much greater than the number of candidates--although the number of voters may still be polynomial in the number of candidates--a simple greedy algorithm very frequently finds the Dodgson winners in such a way that it ``knows'' that it has found them, and furthermore the algorithm never incorrectly declares a nonwinner to be a winner.",
        "published": "2005-09-19T21:59:24Z",
        "link": "http://arxiv.org/abs/cs/0509061v3",
        "categories": [
            "cs.DS",
            "cs.MA",
            "F.2.2; I.2.8; J.4"
        ]
    },
    {
        "title": "Modeling bursts and heavy tails in human dynamics",
        "authors": [
            "A. Vazquez",
            "J. Gama Oliveira",
            "Z. Dezso",
            "K. -I. Goh",
            "I. Kondor",
            "A. -L. Barabasi"
        ],
        "summary": "Current models of human dynamics, used from risk assessment to communications, assume that human actions are randomly distributed in time and thus well approximated by Poisson processes. We provide direct evidence that for five human activity patterns the timing of individual human actions follow non-Poisson statistics, characterized by bursts of rapidly occurring events separated by long periods of inactivity. We show that the bursty nature of human behavior is a consequence of a decision based queuing process: when individuals execute tasks based on some perceived priority, the timing of the tasks will be heavy tailed, most tasks being rapidly executed, while a few experiencing very long waiting times. We discuss two queueing models that capture human activity. The first model assumes that there are no limitations on the number of tasks an individual can hadle at any time, predicting that the waiting time of the individual tasks follow a heavy tailed distribution with exponent alpha=3/2. The second model imposes limitations on the queue length, resulting in alpha=1. We provide empirical evidence supporting the relevance of these two models to human activity patterns. Finally, we discuss possible extension of the proposed queueing models and outline some future challenges in exploring the statistical mechanisms of human dynamics.",
        "published": "2005-10-12T21:39:51Z",
        "link": "http://arxiv.org/abs/physics/0510117v1",
        "categories": [
            "physics.soc-ph",
            "cs.MA"
        ]
    },
    {
        "title": "Automata-based adaptive behavior for economic modeling using game theory",
        "authors": [
            "Rawan Ghnemat",
            "Khalaf Khatatneh",
            "Saleh Oqeili",
            "Cyrille Bertelle",
            "Gérard Henry Edmond Duchamp"
        ],
        "summary": "In this paper, we deal with some specific domains of applications to game theory. This is one of the major class of models in the new approaches of modelling in the economic domain. For that, we use genetic automata which allow to buid adaptive strategies for the players. We explain how the automata-based formalism proposed - matrix representation of automata with multiplicities - allows to define a semi-distance between the strategy behaviors. With that tools, we are able to generate an automatic processus to compute emergent systems of entities whose behaviors are represented by these genetic automata.",
        "published": "2005-10-30T16:06:56Z",
        "link": "http://arxiv.org/abs/cs/0510089v1",
        "categories": [
            "cs.MA",
            "cs.DM"
        ]
    },
    {
        "title": "Parameters Affecting the Resilience of Scale-Free Networks to Random   Failures",
        "authors": [
            "Hamilton Link",
            "Randall A. LaViolette",
            "Jared Saia",
            "Terran Lane"
        ],
        "summary": "It is commonly believed that scale-free networks are robust to massive numbers of random node deletions. For example, Cohen et al. study scale-free networks including some which approximate the measured degree distribution of the Internet. Their results suggest that if each node in this network failed independently with probability 0.99, the remaining network would continue to have a giant component. In this paper, we show that a large and important subclass of scale-free networks are not robust to massive numbers of random node deletions for practical purposes. In particular, we study finite scale-free networks which have minimum node degree of 1 and a power-law degree distribution beginning with nodes of degree 1 (power-law networks). We show that, in a power-law network approximating the Internet's reported distribution, when the probability of deletion of each node is 0.5 only about 25% of the surviving nodes in the network remain connected in a giant component, and the giant component does not persist beyond a critical failure rate of 0.9. The new result is partially due to improved analytical accommodation of the large number of degree-0 nodes that result after node deletions. Our results apply to finite power-law networks with a wide range of power-law exponents, including Internet-like networks. We give both analytical and empirical evidence that such networks are not generally robust to massive random node deletions.",
        "published": "2005-11-02T22:22:14Z",
        "link": "http://arxiv.org/abs/cs/0511012v1",
        "categories": [
            "cs.NI",
            "cs.AR",
            "cs.MA",
            "C.2.1; C.4"
        ]
    },
    {
        "title": "The Impact of Social Networks on Multi-Agent Recommender Systems",
        "authors": [
            "Hamilton Link",
            "Jared Saia",
            "Terran Lane",
            "Randall A. LaViolette"
        ],
        "summary": "Awerbuch et al.'s approach to distributed recommender systems (DRSs) is to have agents sample products at random while randomly querying one another for the best item they have found; we improve upon this by adding a communication network. Agents can only communicate with their immediate neighbors in the network, but neighboring agents may or may not represent users with common interests. We define two network structures: in the ``mailing-list model,'' agents representing similar users form cliques, while in the ``word-of-mouth model'' the agents are distributed randomly in a scale-free network (SFN). In both models, agents tell their neighbors about satisfactory products as they are found. In the word-of-mouth model, knowledge of items propagates only through interested agents, and the SFN parameters affect the system's performance. We include a summary of our new results on the character and parameters of random subgraphs of SFNs, in particular SFNs with power-law degree distributions down to minimum degree 1. These networks are not as resilient as Cohen et al. originally suggested. In the case of the widely-cited ``Internet resilience'' result, high failure rates actually lead to the orphaning of half of the surviving nodes after 60% of the network has failed and the complete disintegration of the network at 90%. We show that given an appropriate network, the communication network reduces the number of sampled items, the number of messages sent, and the amount of ``spam.'' We conclude that in many cases DRSs will be useful for sharing information in a multi-agent learning system.",
        "published": "2005-11-02T23:44:34Z",
        "link": "http://arxiv.org/abs/cs/0511011v1",
        "categories": [
            "cs.LG",
            "cs.CC",
            "cs.MA",
            "I.2.6; I.2.11"
        ]
    },
    {
        "title": "Strategies for fast convergence in semiotic dynamics",
        "authors": [
            "Andrea Baronchelli",
            "Luca Dall'Asta",
            "Alain Barrat",
            "Vittorio Loreto"
        ],
        "summary": "Semiotic dynamics is a novel field that studies how semiotic conventions spread and stabilize in a population of agents. This is a central issue both for theoretical and technological reasons since large system made up of communicating agents, like web communities or artificial embodied agents teams, are getting widespread. In this paper we discuss a recently introduced simple multi-agent model which is able to account for the emergence of a shared vocabulary in a population of agents. In particular we introduce a new deterministic agents' playing strategy that strongly improves the performance of the game in terms of faster convergence and reduced cognitive effort for the agents.",
        "published": "2005-11-23T10:00:42Z",
        "link": "http://arxiv.org/abs/physics/0511201v1",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "Societal Implicit Memory and his Speed on Tracking Extrema over Dynamic   Environments using Self-Regulatory Swarms",
        "authors": [
            "Vitorino Ramos",
            "Carlos Fernandes",
            "Agostinho C. Rosa"
        ],
        "summary": "In order to overcome difficult dynamic optimization and environment extrema tracking problems, We propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of our dynamic search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control problems, measuring reaction speeds and performance. Final comparisons were made with standard Genetic Algorithms (GAs), Bacterial Foraging strategies (BFOA), as well as with recent Co-Evolutionary approaches. SRS's were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found. One of the most interesting illustrate that the present SRS collective swarm of bio-inspired ant-like agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system.",
        "published": "2005-12-01T04:09:22Z",
        "link": "http://arxiv.org/abs/cs/0512003v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11; G.1.6; I.2.9"
        ]
    },
    {
        "title": "Self-Regulated Artificial Ant Colonies on Digital Image Habitats",
        "authors": [
            "Carlos Fernandes",
            "Vitorino Ramos",
            "Agostinho C. Rosa"
        ],
        "summary": "Artificial life models, swarm intelligent and evolutionary computation algorithms are usually built on fixed size populations. Some studies indicate however that varying the population size can increase the adaptability of these systems and their capability to react to changing environments. In this paper we present an extended model of an artificial ant colony system designed to evolve on digital image habitats. We will show that the present swarm can adapt the size of the population according to the type of image on which it is evolving and reacting faster to changing images, thus converging more rapidly to the new desired regions, regulating the number of his image foraging agents. Finally, we will show evidences that the model can be associated with the Mathematical Morphology Watershed algorithm to improve the segmentation of digital grey-scale images. KEYWORDS: Swarm Intelligence, Perception and Image Processing, Pattern Recognition, Mathematical Morphology, Social Cognitive Maps, Social Foraging, Self-Organization, Distributed Search.",
        "published": "2005-12-01T04:39:30Z",
        "link": "http://arxiv.org/abs/cs/0512004v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2.11; I.2.9; I.3.3; I.4; I.4.10; I.4.6; I.4.9; I.5.4"
        ]
    },
    {
        "title": "Topology Induced Coarsening in Language Games",
        "authors": [
            "A. Baronchelli",
            "L. Dall'Asta",
            "A. Barrat",
            "V. Loreto"
        ],
        "summary": "We investigate how very large populations are able to reach a global consensus, out of local \"microscopic\" interaction rules, in the framework of a recently introduced class of models of semiotic dynamics, the so-called Naming Game. We compare in particular the convergence mechanism for interacting agents embedded in a low-dimensional lattice with respect to the mean-field case. We highlight that in low-dimensions consensus is reached through a coarsening process which requires less cognitive effort of the agents, with respect to the mean-field case, but takes longer to complete. In 1-d the dynamics of the boundaries is mapped onto a truncated Markov process from which we analytically computed the diffusion coefficient. More generally we show that the convergence process requires a memory per agent scaling as N and lasts a time N^{1+2/d} in dimension d<5 (d=4 being the upper critical dimension), while in mean-field both memory and time scale as N^{3/2}, for a population of N agents. We present analytical and numerical evidences supporting this picture.",
        "published": "2005-12-06T17:05:50Z",
        "link": "http://arxiv.org/abs/physics/0512045v1",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.GT",
            "cs.MA"
        ]
    },
    {
        "title": "Modeling Endogenous Social Networks: the Example of Emergence and   Stability of Cooperation without Refusal",
        "authors": [
            "David Chavalarias"
        ],
        "summary": "Aggregated phenomena in social sciences and economics are highly dependent on the way individuals interact. To help understanding the interplay between socio-economic activities and underlying social networks, this paper studies a sequential prisoner's dilemma with binary choice. It proposes an analytical and computational insight about the role of endogenous networks in emergence and sustainability of cooperation and exhibits an alternative to the choice and refusal mechanism that is often proposed to explain cooperation. The study focuses on heterogeneous equilibriums and emergence of cooperation from an all-defector state that are the two stylized facts that this model successfully reconstructs.",
        "published": "2005-12-19T07:59:32Z",
        "link": "http://arxiv.org/abs/nlin/0512048v1",
        "categories": [
            "nlin.AO",
            "cond-mat.other",
            "cs.GT",
            "cs.MA",
            "cs.OH",
            "q-bio.OT",
            "q-bio.PE"
        ]
    },
    {
        "title": "Combining Independent Modules in Lexical Multiple-Choice Problems",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman",
            "Jeffrey Bigham",
            "Victor Shnayder"
        ],
        "summary": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of multiple modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the familiar mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",
        "published": "2005-01-10T21:03:14Z",
        "link": "http://arxiv.org/abs/cs/0501018v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; J.5"
        ]
    },
    {
        "title": "Ontology-Based Users & Requests Clustering in Customer Service   Management System",
        "authors": [
            "Alexander Smirnov",
            "Mikhail Pashkin",
            "Nikolai Chilov",
            "Tatiana Levashova",
            "Andrew Krizhanovsky",
            "Alexey Kashevnik"
        ],
        "summary": "Customer Service Management is one of major business activities to better serve company customers through the introduction of reliable processes and procedures. Today this kind of activities is implemented through e-services to directly involve customers into business processes. Traditionally Customer Service Management involves application of data mining techniques to discover usage patterns from the company knowledge memory. Hence grouping of customers/requests to clusters is one of major technique to improve the level of company customization. The goal of this paper is to present an efficient for implementation approach for clustering users and their requests. The approach uses ontology as knowledge representation model to improve the semantic interoperability between units of the company and customers. Some fragments of the approach tested in an industrial company are also presented in the paper.",
        "published": "2005-01-26T14:11:38Z",
        "link": "http://arxiv.org/abs/cs/0501077v2",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3"
        ]
    },
    {
        "title": "Multi-document Biography Summarization",
        "authors": [
            "Liang Zhou",
            "Miruna Ticrea",
            "Eduard Hovy"
        ],
        "summary": "In this paper we describe a biography summarization system using sentence classification and ideas from information retrieval. Although the individual techniques are not new, assembling and applying them to generate multi-document biographies is new. Our system was evaluated in DUC2004. It is among the top performers in task 5-short summaries focused by person questions.",
        "published": "2005-01-26T22:43:17Z",
        "link": "http://arxiv.org/abs/cs/0501078v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "The Self-Organization of Speech Sounds",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "summary": "The speech code is a vehicle of language: it defines a set of forms used by a community to carry information. Such a code is necessary to support the linguistic interactions that allow humans to communicate. How then may a speech code be formed prior to the existence of linguistic interactions? Moreover, the human speech code is discrete and compositional, shared by all the individuals of a community but different across communities, and phoneme inventories are characterized by statistical regularities. How can a speech code with these properties form? We try to approach these questions in the paper, using the \"methodology of the artificial\". We build a society of artificial agents, and detail a mechanism that shows the formation of a discrete speech code without pre-supposing the existence of linguistic capacities or of coordinated interactions. The mechanism is based on a low-level model of sensory-motor interactions. We show that the integration of certain very simple and non language-specific neural devices leads to the formation of a speech code that has properties similar to the human speech code. This result relies on the self-organizing properties of a generic coupling between perception and production within agents, and on the interactions between agents. The artificial system helps us to develop better intuitions on how speech might have appeared, by showing how self-organization might have helped natural selection to find speech.",
        "published": "2005-02-22T09:51:16Z",
        "link": "http://arxiv.org/abs/cs/0502086v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE",
            "cs.RO",
            "math.DS"
        ]
    },
    {
        "title": "Fine-Grained Word Sense Disambiguation Based on Parallel Corpora, Word   Alignment, Word Clustering and Aligned Wordnets",
        "authors": [
            "Dan Tufis",
            "Radu Ion",
            "Nancy Ide"
        ],
        "summary": "The paper presents a method for word sense disambiguation based on parallel corpora. The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system, implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet.",
        "published": "2005-03-10T11:49:51Z",
        "link": "http://arxiv.org/abs/cs/0503024v1",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "A Suffix Tree Approach to Email Filtering",
        "authors": [
            "Rajesh M. Pampapathi",
            "Boris Mirkin",
            "Mark Levene"
        ],
        "summary": "We present an approach to email filtering based on the suffix tree data structure. A method for the scoring of emails using the suffix tree is developed and a number of scoring and score normalisation functions are tested. Our results show that the character level representation of emails and classes facilitated by the suffix tree can significantly improve classification accuracy when compared with the currently popular methods, such as naive Bayes. We believe the method can be extended to the classification of documents in other domains.",
        "published": "2005-03-14T18:12:03Z",
        "link": "http://arxiv.org/abs/cs/0503030v2",
        "categories": [
            "cs.AI",
            "cs.CL"
        ]
    },
    {
        "title": "An Introduction to the Summarization of Evolving Events: Linear and   Non-linear Evolution",
        "authors": [
            "Stergos D. Afantenos",
            "Konstantina Liontou",
            "Maria Salapata",
            "Vangelis Karkaletsis"
        ],
        "summary": "This paper examines the summarization of events that evolve through time. It discusses different types of evolution taking into account the time in which the incidents of an event are happening and the different sources reporting on the specific event. It proposes an approach for multi-document summarization which employs ``messages'' for representing the incidents of an event and cross-document relations that hold between messages according to certain conditions. The paper also outlines the current version of the summarization system we are implementing to realize this approach.",
        "published": "2005-03-15T15:31:14Z",
        "link": "http://arxiv.org/abs/cs/0503033v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Weighted Automata in Text and Speech Processing",
        "authors": [
            "Mehryar Mohri",
            "Fernando Pereira",
            "Michael Riley"
        ],
        "summary": "Finite-state automata are a very effective tool in natural language processing. However, in a variety of applications and especially in speech precessing, it is necessary to consider more general machines in which arcs are assigned weights or costs. We briefly describe some of the main theoretical and algorithmic aspects of these machines. In particular, we describe an efficient composition algorithm for weighted transducers, and give examples illustrating the value of determinization and minimization algorithms for weighted automata.",
        "published": "2005-03-29T04:59:50Z",
        "link": "http://arxiv.org/abs/cs/0503077v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "F.1.1; F.4.3; H.5.2"
        ]
    },
    {
        "title": "A Matter of Opinion: Sentiment Analysis and Business Intelligence   (position paper)",
        "authors": [
            "Lillian Lee"
        ],
        "summary": "A general-audience introduction to the area of \"sentiment analysis\", the computational treatment of subjective, opinion-oriented language (an example application is determining whether a review is \"thumbs up\" or \"thumbs down\"). Some challenges, applications to business-intelligence tasks, and potential future directions are described.",
        "published": "2005-04-06T20:04:55Z",
        "link": "http://arxiv.org/abs/cs/0504022v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Summarization from Medical Documents: A Survey",
        "authors": [
            "Stergos D. Afantenos",
            "Vangelis Karkaletsis",
            "Panagiotis Stamatopoulos"
        ],
        "summary": "Objective:   The aim of this paper is to survey the recent work in medical documents summarization.   Background:   During the last decade, documents summarization got increasing attention by the AI research community. More recently it also attracted the interest of the medical research community as well, due to the enormous growth of information that is available to the physicians and researchers in medicine, through the large and growing number of published journals, conference proceedings, medical sites and portals on the World Wide Web, electronic medical records, etc.   Methodology:   This survey gives first a general background on documents summarization, presenting the factors that summarization depends upon, discussing evaluation issues and describing briefly the various types of summarization techniques. It then examines the characteristics of the medical domain through the different types of medical documents. Finally, it presents and discusses the summarization techniques used so far in the medical domain, referring to the corresponding systems and their characteristics.   Discussion and conclusions:   The paper discusses thoroughly the promising paths for future research in medical documents summarization. It mainly focuses on the issue of scaling to large collections of documents in various languages and from different media, on personalization issues, on portability to new sub-domains, and on the integration of summarization technology in practical applications",
        "published": "2005-04-13T20:02:25Z",
        "link": "http://arxiv.org/abs/cs/0504061v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Metalinguistic Information Extraction for Terminology",
        "authors": [
            "Carlos Rodriguez"
        ],
        "summary": "This paper describes and evaluates the Metalinguistic Operation Processor (MOP) system for automatic compilation of metalinguistic information from technical and scientific documents. This system is designed to extract non-standard terminological resources that we have called Metalinguistic Information Databases (or MIDs), in order to help update changing glossaries, knowledge bases and ontologies, as well as to reflect the metastable dynamics of special-domain knowledge.",
        "published": "2005-04-15T20:10:53Z",
        "link": "http://arxiv.org/abs/cs/0504074v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "I.2.7"
        ]
    },
    {
        "title": "Universal Similarity",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "We survey a new area of parameter-free similarity distance measures useful in data-mining, pattern recognition, learning and automatic semantics extraction. Given a family of distances on a set of objects, a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set, up to the stated precision (we do not require the universal distance to be an element of the family). We consider similarity distances for two types of objects: literal objects that as such contain all of their meaning, like genomes or books, and names for objects. The latter may have literal embodyments like the first type, but may also be abstract like ``red'' or ``christianity.'' For the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular features between pairs of literal objects. For the second type we consider similarity distances generated by web users corresponding to particular semantic relations between the (names for) the designated objects. For both families we give universal similarity distance measures, incorporating all particular distance measures in the family. In the first case the universal distance is based on compression and in the second case it is based on Google page counts related to search terms. In both cases experiments on a massive scale give evidence of the viability of the approaches.",
        "published": "2005-04-20T17:40:55Z",
        "link": "http://arxiv.org/abs/cs/0504089v2",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "physics.data-an",
            "I.2; I.5; J.5; H.2; H.3; H.4; H.2.8; E.2; E.4"
        ]
    },
    {
        "title": "Seeing stars: Exploiting class relationships for sentiment   categorization with respect to rating scales",
        "authors": [
            "Bo Pang",
            "Lillian Lee"
        ],
        "summary": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\". We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.",
        "published": "2005-06-17T20:10:43Z",
        "link": "http://arxiv.org/abs/cs/0506075v1",
        "categories": [
            "cs.CL",
            "cs.LG",
            "I.2.7; I.2.6"
        ]
    },
    {
        "title": "Efficient Multiclass Implementations of L1-Regularized Maximum Entropy",
        "authors": [
            "Patrick Haffner",
            "Steven Phillips",
            "Rob Schapire"
        ],
        "summary": "This paper discusses the application of L1-regularized maximum entropy modeling or SL1-Max [9] to multiclass categorization problems. A new modification to the SL1-Max fast sequential learning algorithm is proposed to handle conditional distributions. Furthermore, unlike most previous studies, the present research goes beyond a single type of conditional distribution. It describes and compares a variety of modeling assumptions about the class distribution (independent or exclusive) and various types of joint or conditional distributions. It results in a new methodology for combining binary regularized classifiers to achieve multiclass categorization. In this context, Maximum Entropy can be considered as a generic and efficient regularized classification tool that matches or outperforms the state-of-the art represented by AdaBoost and SVMs.",
        "published": "2005-06-29T20:26:33Z",
        "link": "http://arxiv.org/abs/cs/0506101v1",
        "categories": [
            "cs.LG",
            "cs.CL"
        ]
    },
    {
        "title": "On Hilberg's Law and Its Links with Guiraud's Law",
        "authors": [
            "Łukasz Dȩbowski"
        ],
        "summary": "Hilberg (1990) supposed that finite-order excess entropy of a random human text is proportional to the square root of the text length. Assuming that Hilberg's hypothesis is true, we derive Guiraud's law, which states that the number of word types in a text is greater than proportional to the square root of the text length. Our derivation is based on some mathematical conjecture in coding theory and on several experiments suggesting that words can be defined approximately as the nonterminals of the shortest context-free grammar for the text. Such operational definition of words can be applied even to texts deprived of spaces, which do not allow for Mandelbrot's ``intermittent silence'' explanation of Zipf's and Guiraud's laws. In contrast to Mandelbrot's, our model assumes some probabilistic long-memory effects in human narration and might be capable of explaining Menzerath's law.",
        "published": "2005-07-07T18:53:14Z",
        "link": "http://arxiv.org/abs/cs/0507022v1",
        "categories": [
            "cs.CL",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Explorations in engagement for humans and robots",
        "authors": [
            "Candace L. Sidner",
            "Christopher Lee",
            "Cory Kidd",
            "Neal Lesh",
            "Charles Rich"
        ],
        "summary": "This paper explores the concept of engagement, the process by which individuals in an interaction start, maintain and end their perceived connection to one another. The paper reports on one aspect of engagement among human interactors--the effect of tracking faces during an interaction. It also describes the architecture of a robot that can participate in conversational, collaborative interactions with engagement gestures. Finally, the paper reports on findings of experiments with human participants who interacted with a robot when it either performed or did not perform engagement gestures. Results of the human-robot studies indicate that people become engaged with robots: they direct their attention to the robot more often in interactions where engagement gestures are present, and they find interactions more appropriate when engagement gestures are present than when they are not.",
        "published": "2005-07-21T21:56:34Z",
        "link": "http://arxiv.org/abs/cs/0507056v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.RO",
            "I.2.7; I.2.9"
        ]
    },
    {
        "title": "Measuring Semantic Similarity by Latent Relational Analysis",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similarity. LRA measures similarity in the semantic relations between two pairs of words. When two pairs have a high degree of relational similarity, they are analogous. For example, the pair cat:meow is analogous to the pair dog:bark. There is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning). In the Vector Space Model (VSM) approach to measuring relational similarity, the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs. The elements in the vectors are based on the frequencies of manually constructed patterns in a large corpus. LRA extends the VSM approach in three ways: (1) patterns are derived automatically from the corpus, (2) Singular Value Decomposition is used to smooth the frequency data, and (3) synonyms are used to reformulate word pairs. This paper describes the LRA algorithm and experimentally compares LRA to VSM on two tasks, answering college-level multiple-choice word analogy questions and classifying semantic relations in noun-modifier expressions. LRA achieves state-of-the-art results, reaching human-level performance on the analogy questions and significantly exceeding VSM performance on both tasks.",
        "published": "2005-08-10T19:35:57Z",
        "link": "http://arxiv.org/abs/cs/0508053v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Summarizing Reports on Evolving Events; Part I: Linear Evolution",
        "authors": [
            "Stergos D. Afantenos",
            "Vangelis Karkaletsis",
            "Panagiotis Stamatopoulos"
        ],
        "summary": "We present an approach for summarization from multiple documents which report on events that evolve through time, taking into account the different document sources. We distinguish the evolution of an event into linear and non-linear. According to our approach, each document is represented by a collection of messages which are then used in order to instantiate the cross-document relations that determine the summary content. The paper presents the summarization system that implements this approach through a case study on linear evolution.",
        "published": "2005-08-22T12:56:32Z",
        "link": "http://arxiv.org/abs/cs/0508092v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Corpus-based Learning of Analogies and Semantic Relations",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning \"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \"laser printer\", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations.",
        "published": "2005-08-23T20:21:56Z",
        "link": "http://arxiv.org/abs/cs/0508103v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Automatic extraction of paraphrastic phrases from medium size corpora",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "This paper presents a versatile system intended to acquire paraphrastic phrases from a representative corpus. In order to decrease the time spent on the elaboration of resources for NLP system (for example Information Extraction, IE hereafter), we suggest to use a machine learning system that helps defining new templates and associated resources. This knowledge is automatically derived from the text collection, in interaction with a large semantic network.",
        "published": "2005-09-28T16:15:27Z",
        "link": "http://arxiv.org/abs/cs/0509092v1",
        "categories": [
            "cs.CL",
            "cs.AI"
        ]
    },
    {
        "title": "Word sense disambiguation criteria: a systematic study",
        "authors": [
            "Laurent Audibert"
        ],
        "summary": "This article describes the results of a systematic in-depth study of the criteria used for word sense disambiguation. Our study is based on 60 target words: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line with some practices in the field. For example, we show that omitting non-content words decreases performance and that bigrams yield better results than unigrams.",
        "published": "2005-10-05T14:23:19Z",
        "link": "http://arxiv.org/abs/cs/0510015v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "The Nature of Novelty Detection",
        "authors": [
            "Le Zhao",
            "Min Zhang",
            "Shaoping Ma"
        ],
        "summary": "Sentence level novelty detection aims at reducing redundant sentences from a sentence list. In the task, sentences appearing later in the list with no new meanings are eliminated. Aiming at a better accuracy for detecting redundancy, this paper reveals the nature of the novelty detection task currently overlooked by the Novelty community $-$ Novelty as a combination of the partial overlap (PO, two sentences sharing common facts) and complete overlap (CO, the first sentence covers all the facts of the second sentence) relations. By formalizing novelty detection as a combination of the two relations between sentences, new viewpoints toward techniques dealing with Novelty are proposed. Among the methods discussed, the similarity, overlap, pool and language modeling approaches are commonly used. Furthermore, a novel approach, selected pool method is provided, which is immediate following the nature of the task. Experimental results obtained on all the three currently available novelty datasets showed that selected pool is significantly better or no worse than the current methods. Knowledge about the nature of the task also affects the evaluation methodologies. We propose new evaluation measures for Novelty according to the nature of the task, as well as possible directions for future study.",
        "published": "2005-10-19T14:56:48Z",
        "link": "http://arxiv.org/abs/cs/0510054v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3"
        ]
    },
    {
        "title": "Using phonetic constraints in acoustic-to-articulatory inversion",
        "authors": [
            "Blaise Potard",
            "Yves Laprie"
        ],
        "summary": "The goal of this work is to recover articulatory information from the speech signal by acoustic-to-articulatory inversion. One of the main difficulties with inversion is that the problem is underdetermined and inversion methods generally offer no guarantee on the phonetical realism of the inverse solutions. A way to adress this issue is to use additional phonetic constraints. Knowledge of the phonetic caracteristics of French vowels enable the derivation of reasonable articulatory domains in the space of Maeda parameters: given the formants frequencies (F1,F2,F3) of a speech sample, and thus the vowel identity, an \"ideal\" articulatory domain can be derived. The space of formants frequencies is partitioned into vowels, using either speaker-specific data or generic information on formants. Then, to each articulatory vector can be associated a phonetic score varying with the distance to the \"ideal domain\" associated with the corresponding vowel. Inversion experiments were conducted on isolated vowels and vowel-to-vowel transitions. Articulatory parameters were compared with those obtained without using these constraints and those measured from X-ray data.",
        "published": "2005-11-21T14:50:52Z",
        "link": "http://arxiv.org/abs/cs/0511076v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "An elitist approach for extracting automatically well-realized speech   sounds with high confidence",
        "authors": [
            "Jean-Baptiste Maj",
            "Anne Bonneau",
            "Dominique Fohr",
            "Yves Laprie"
        ],
        "summary": "This paper presents an \"elitist approach\" for extracting automatically well-realized speech sounds with high confidence. The elitist approach uses a speech recognition system based on Hidden Markov Models (HMM). The HMM are trained on speech sounds which are systematically well-detected in an iterative procedure. The results show that, by using the HMM models defined in the training phase, the speech recognizer detects reliably specific speech sounds with a small rate of errors.",
        "published": "2005-11-22T07:06:43Z",
        "link": "http://arxiv.org/abs/cs/0511079v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Statistical Parameters of the Novel \"Perekhresni stezhky\" (\"The   Cross-Paths\") by Ivan Franko",
        "authors": [
            "Solomija Buk",
            "Andrij Rovenchak"
        ],
        "summary": "In the paper, a complex statistical characteristics of a Ukrainian novel is given for the first time. The distribution of word-forms with respect to their size is studied. The linguistic laws by Zipf-Mandelbrot and Altmann-Menzerath are analyzed.",
        "published": "2005-12-28T13:45:54Z",
        "link": "http://arxiv.org/abs/cs/0512102v1",
        "categories": [
            "cs.CL"
        ]
    },
    {
        "title": "Combining Independent Modules in Lexical Multiple-Choice Problems",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman",
            "Jeffrey Bigham",
            "Victor Shnayder"
        ],
        "summary": "Existing statistical approaches to natural language problems are very coarse approximations to the true complexity of language processing. As such, no single technique will be best for all problem instances. Many researchers are examining ensemble methods that combine the output of multiple modules to create more accurate solutions. This paper examines three merging rules for combining probability distributions: the familiar mixture rule, the logarithmic rule, and a novel product rule. These rules were applied with state-of-the-art results to two problems used to assess human mastery of lexical semantics -- synonym questions and analogy questions. All three merging rules result in ensembles that are more accurate than any of their component modules. The differences among the three rules are not statistically significant, but it is suggestive that the popular mixture rule is not the best rule for either of the two problems.",
        "published": "2005-01-10T21:03:14Z",
        "link": "http://arxiv.org/abs/cs/0501018v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "I.2.6; I.2.7; H.3.1; J.5"
        ]
    },
    {
        "title": "Clustering SPIRES with EqRank",
        "authors": [
            "G. B. Pivovarov",
            "S. E. Trunov"
        ],
        "summary": "SPIRES is the largest database of scientific papers in the subject field of high energy and nuclear physics. It contains information on the citation graph of more than half a million of papers (vertexes of the citation graph). We outline the EqRank algorithm designed to cluster vertexes of directed graphs, and present the results of EqRank application to the SPIRES citation graph. The hierarchical clustering of SPIRES yielded by EqRank is used to set up a web service, which is also outlined.",
        "published": "2005-01-11T05:00:03Z",
        "link": "http://arxiv.org/abs/cs/0501019v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Augmented Segmentation and Visualization for Presentation Videos",
        "authors": [
            "Alexander Haubold",
            "John R. Kender"
        ],
        "summary": "We investigate methods of segmenting, visualizing, and indexing presentation videos by separately considering audio and visual data. The audio track is segmented by speaker, and augmented with key phrases which are extracted using an Automatic Speech Recognizer (ASR). The video track is segmented by visual dissimilarities and augmented by representative key frames. An interactive user interface combines a visual representation of audio, video, text, and key frames, and allows the user to navigate a presentation video. We also explore clustering and labeling of speaker data and present preliminary results.",
        "published": "2005-01-20T17:12:05Z",
        "link": "http://arxiv.org/abs/cs/0501044v1",
        "categories": [
            "cs.MM",
            "cs.IR",
            "H.2.4;H.3.1"
        ]
    },
    {
        "title": "Ontology-Based Users & Requests Clustering in Customer Service   Management System",
        "authors": [
            "Alexander Smirnov",
            "Mikhail Pashkin",
            "Nikolai Chilov",
            "Tatiana Levashova",
            "Andrew Krizhanovsky",
            "Alexey Kashevnik"
        ],
        "summary": "Customer Service Management is one of major business activities to better serve company customers through the introduction of reliable processes and procedures. Today this kind of activities is implemented through e-services to directly involve customers into business processes. Traditionally Customer Service Management involves application of data mining techniques to discover usage patterns from the company knowledge memory. Hence grouping of customers/requests to clusters is one of major technique to improve the level of company customization. The goal of this paper is to present an efficient for implementation approach for clustering users and their requests. The approach uses ontology as knowledge representation model to improve the semantic interoperability between units of the company and customers. Some fragments of the approach tested in an industrial company are also presented in the paper.",
        "published": "2005-01-26T14:11:38Z",
        "link": "http://arxiv.org/abs/cs/0501077v2",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3"
        ]
    },
    {
        "title": "Log Analysis Case Study Using LoGS",
        "authors": [
            "Dmitry Mogilevsky"
        ],
        "summary": "A very useful technique a network administrator can use to identify problematic network behavior is careful analysis of logs of incoming and outgoing network flows. The challenge one faces when attempting to undertake this course of action, though, is that large networks tend to generate an extremely large quantity of network traffic in a very short period of time, resulting in very large traffic logs which must be analyzed post-generation with an eye for contextual information which may reveal symptoms of problematic traffic. A better technique is to perform real-time log analysis using a real-time context-generating tool such as LoGS.",
        "published": "2005-02-09T19:49:10Z",
        "link": "http://arxiv.org/abs/cs/0502052v1",
        "categories": [
            "cs.CR",
            "cs.IR"
        ]
    },
    {
        "title": "Top-Down Unsupervised Image Segmentation (it sounds like oxymoron, but   actually it is not)",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "Pattern recognition is generally assumed as an interaction of two inversely directed image-processing streams: the bottom-up information details gathering and localization (segmentation) stream, and the top-down information features aggregation, association and interpretation (recognition) stream. Inspired by recent evidence from biological vision research and by the insights of Kolmogorov Complexity theory, we propose a new, just top-down evolving, procedure of initial image segmentation. We claim that traditional top-down cognitive reasoning, which is supposed to guide the segmentation process to its final result, is not at all a part of the image information content evaluation. And that initial image segmentation is certainly an unsupervised process. We present some illustrative examples, which support our claims.",
        "published": "2005-03-01T05:17:33Z",
        "link": "http://arxiv.org/abs/cs/0503001v1",
        "categories": [
            "cs.CV",
            "cs.IR"
        ]
    },
    {
        "title": "Shuffling a Stacked Deck: The Case for Partially Randomized Ranking of   Search Engine Results",
        "authors": [
            "Sandeep Pandey",
            "Sourashis Roy",
            "Christopher Olston",
            "Junghoo Cho",
            "Soumen Chakrabarti"
        ],
        "summary": "In-degree, PageRank, number of visits and other measures of Web page popularity significantly influence the ranking of search results by modern search engines. The assumption is that popularity is closely correlated with quality, a more elusive concept that is difficult to measure directly. Unfortunately, the correlation between popularity and quality is very weak for newly-created pages that have yet to receive many visits and/or in-links. Worse, since discovery of new content is largely done by querying search engines, and because users usually focus their attention on the top few results, newly-created but high-quality pages are effectively ``shut out,'' and it can take a very long time before they become popular.   We propose a simple and elegant solution to this problem: the introduction of a controlled amount of randomness into search result ranking methods. Doing so offers new pages a chance to prove their worth, although clearly using too much randomness will degrade result quality and annul any benefits achieved. Hence there is a tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality. We study this tradeoff both analytically and via simulation, in the context of an economic objective function based on aggregate result quality amortized over time. We show that a modest amount of randomness leads to improved search results.",
        "published": "2005-03-04T10:29:34Z",
        "link": "http://arxiv.org/abs/cs/0503011v1",
        "categories": [
            "cs.IR",
            "H.3.3; G.3"
        ]
    },
    {
        "title": "Earlier Web Usage Statistics as Predictors of Later Citation Impact",
        "authors": [
            "Tim Brody",
            "Stevan Harnad"
        ],
        "summary": "The use of citation counts to assess the impact of research articles is well established. However, the citation impact of an article can only be measured several years after it has been published. As research articles are increasingly accessed through the Web, the number of times an article is downloaded can be instantly recorded and counted. One would expect the number of times an article is read to be related both to the number of times it is cited and to how old the article is. This paper analyses how short-term Web usage impact predicts medium-term citation impact. The physics e-print archive (arXiv.org) is used to test this.",
        "published": "2005-03-08T22:26:07Z",
        "link": "http://arxiv.org/abs/cs/0503020v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Fast-Forward on the Green Road to Open Access: The Case Against Mixing   Up Green and Gold",
        "authors": [
            "Stevan Harnad"
        ],
        "summary": "This article is a critique of: \"The 'Green' and 'Gold' Roads to Open Access: The Case for Mixing and Matching\" by Jean-Claude Guedon (in Serials Review 30(4) 2004).   Open Access (OA) means: free online access to all peer-reviewed journal articles. Jean-Claude Guedon argues against the efficacy of author self-archiving of peer-reviewed journal articles (the \"Green\" road to OA). He suggests instead that we should convert to Open Access Publishing (the \"Golden\" road to OA) by \"mixing and matching\" Green and Gold as follows: o First, self-archive dissertations (not published, peer-reviewed journal articles). o Second, identify and tag how those dissertations have been evaluated and reviewed. o Third, self-archive unrefereed preprints (not published, peer-reviewed journal articles). o Fourth, develop new mechanisms for evaluating and reviewing those unrefereed preprints, at multiple levels. The result will be OA Publishing (Gold). I argue that rather than yet another 10 years of speculation like this, what is actually needed (and imminent) is for OA self-archiving to be mandated by research funders and institutions so that the self-archiving of published, peer-reviewed journal articles (Green) can be fast-forwarded to 100% OA.",
        "published": "2005-03-08T22:44:37Z",
        "link": "http://arxiv.org/abs/cs/0503021v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "An Introduction to the Summarization of Evolving Events: Linear and   Non-linear Evolution",
        "authors": [
            "Stergos D. Afantenos",
            "Konstantina Liontou",
            "Maria Salapata",
            "Vangelis Karkaletsis"
        ],
        "summary": "This paper examines the summarization of events that evolve through time. It discusses different types of evolution taking into account the time in which the incidents of an event are happening and the different sources reporting on the specific event. It proposes an approach for multi-document summarization which employs ``messages'' for representing the incidents of an event and cross-document relations that hold between messages according to certain conditions. The paper also outlines the current version of the summarization system we are implementing to realize this approach.",
        "published": "2005-03-15T15:31:14Z",
        "link": "http://arxiv.org/abs/cs/0503033v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Space-time databases modeling global semantic networks",
        "authors": [
            "A. A. Prikhod'ko",
            "N. A. Prikhod'ko"
        ],
        "summary": "This paper represents an approach to creating global knowledge systems, using new philosophy and infrastructure of global distributed semantic network (frame knowledge representation system) based on the space-time database construction. The main idea of the space-time database environment introduced in the paper is to bind a document (an information frame, a knowledge) to a special kind of entity, that we call permanent entity, -- an object without history and evolution, described by a \"point\" in the generalized, informational space-time (not an evolving object in the real space having history). For documents (information) it means that document content is unchangeable, and documents are absolutely persistent. This approach leads to new knowledge representation and retreival techniques. We discuss the way of applying the concept to a global distributed scientific library and scientific workspace. Some practical aspects of the work are elaborated by the open IT project at http://sourceforge.net/projects/gil/.",
        "published": "2005-03-29T20:18:42Z",
        "link": "http://arxiv.org/abs/cs/0503079v1",
        "categories": [
            "cs.IT",
            "cs.IR",
            "math.IT",
            "E.1; H.1.1; H.3.4; H.3.7"
        ]
    },
    {
        "title": "Scientific impact quantity and quality: Analysis of two sources of   bibliographic data",
        "authors": [
            "Richard K. Belew"
        ],
        "summary": "Attempts to understand the consequence of any individual scientist's activity within the long-term trajectory of science is one of the most difficult questions within the philosophy of science. Because scientific publications play such as central role in the modern enterprise of science, bibliometric techniques which measure the ``impact'' of an individual publication as a function of the number of citations it receives from subsequent authors have provided some of the most useful empirical data on this question. Until recently, Thompson/ISI has provided the only source of large-scale ``inverted'' bibliographic data of the sort required for impact analysis. In the end of 2004, Google introduced a new service, GoogleScholar, making much of this same data available. Here we analyze 203 publications, collectively cited by more than 4000 other publications. We show surprisingly good agreement between data citation counts provided by the two services. Data quality across the systems is analyzed, and potentially useful complementarities between are considered. The additional robustness offered by multiple sources of such data promises to increase the utility of these measurements as open citation protocols and open access increase their impact on electronic scientific publication practices.",
        "published": "2005-04-11T13:52:55Z",
        "link": "http://arxiv.org/abs/cs/0504036v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.3; H.3.7; H.5.4"
        ]
    },
    {
        "title": "Summarization from Medical Documents: A Survey",
        "authors": [
            "Stergos D. Afantenos",
            "Vangelis Karkaletsis",
            "Panagiotis Stamatopoulos"
        ],
        "summary": "Objective:   The aim of this paper is to survey the recent work in medical documents summarization.   Background:   During the last decade, documents summarization got increasing attention by the AI research community. More recently it also attracted the interest of the medical research community as well, due to the enormous growth of information that is available to the physicians and researchers in medicine, through the large and growing number of published journals, conference proceedings, medical sites and portals on the World Wide Web, electronic medical records, etc.   Methodology:   This survey gives first a general background on documents summarization, presenting the factors that summarization depends upon, discussing evaluation issues and describing briefly the various types of summarization techniques. It then examines the characteristics of the medical domain through the different types of medical documents. Finally, it presents and discusses the summarization techniques used so far in the medical domain, referring to the corresponding systems and their characteristics.   Discussion and conclusions:   The paper discusses thoroughly the promising paths for future research in medical documents summarization. It mainly focuses on the issue of scaling to large collections of documents in various languages and from different media, on personalization issues, on portability to new sub-domains, and on the integration of summarization technology in practical applications",
        "published": "2005-04-13T20:02:25Z",
        "link": "http://arxiv.org/abs/cs/0504061v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Selection in Scale-Free Small World",
        "authors": [
            "Zs. Palotai",
            "Cs. Farkas",
            "A. Lorincz"
        ],
        "summary": "In this paper we compare the performance characteristics of our selection based learning algorithm for Web crawlers with the characteristics of the reinforcement learning algorithm. The task of the crawlers is to find new information on the Web. The selection algorithm, called weblog update, modifies the starting URL lists of our crawlers based on the found URLs containing new information. The reinforcement learning algorithm modifies the URL orderings of the crawlers based on the received reinforcements for submitted documents. We performed simulations based on data collected from the Web. The collected portion of the Web is typical and exhibits scale-free small world (SFSW) structure. We have found that on this SFSW, the weblog update algorithm performs better than the reinforcement learning algorithm. It finds the new information faster than the reinforcement learning algorithm and has better new information/all submitted documents ratio. We believe that the advantages of the selection algorithm over reinforcement learning algorithm is due to the small world property of the Web.",
        "published": "2005-04-14T07:57:01Z",
        "link": "http://arxiv.org/abs/cs/0504063v1",
        "categories": [
            "cs.LG",
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "Metalinguistic Information Extraction for Terminology",
        "authors": [
            "Carlos Rodriguez"
        ],
        "summary": "This paper describes and evaluates the Metalinguistic Operation Processor (MOP) system for automatic compilation of metalinguistic information from technical and scientific documents. This system is designed to extract non-standard terminological resources that we have called Metalinguistic Information Databases (or MIDs), in order to help update changing glossaries, knowledge bases and ontologies, as well as to reflect the metastable dynamics of special-domain knowledge.",
        "published": "2005-04-15T20:10:53Z",
        "link": "http://arxiv.org/abs/cs/0504074v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "I.2.7"
        ]
    },
    {
        "title": "Universal Similarity",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "We survey a new area of parameter-free similarity distance measures useful in data-mining, pattern recognition, learning and automatic semantics extraction. Given a family of distances on a set of objects, a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set, up to the stated precision (we do not require the universal distance to be an element of the family). We consider similarity distances for two types of objects: literal objects that as such contain all of their meaning, like genomes or books, and names for objects. The latter may have literal embodyments like the first type, but may also be abstract like ``red'' or ``christianity.'' For the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular features between pairs of literal objects. For the second type we consider similarity distances generated by web users corresponding to particular semantic relations between the (names for) the designated objects. For both families we give universal similarity distance measures, incorporating all particular distance measures in the family. In the first case the universal distance is based on compression and in the second case it is based on Google page counts related to search terms. In both cases experiments on a massive scale give evidence of the viability of the approaches.",
        "published": "2005-04-20T17:40:55Z",
        "link": "http://arxiv.org/abs/cs/0504089v2",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "physics.data-an",
            "I.2; I.5; J.5; H.2; H.3; H.4; H.2.8; E.2; E.4"
        ]
    },
    {
        "title": "Data Mining on Crash Simulation Data",
        "authors": [
            "A. Kuhlmann",
            "R. -M. Vetter",
            "Ch. Luebbing",
            "C. -A. Thole"
        ],
        "summary": "The work presented in this paper is part of the cooperative research project AUTO-OPT carried out by twelve partners from the automotive industries. One major work package concerns the application of data mining methods in the area of automotive design. Suitable methods for data preparation and data analysis are developed. The objective of the work is the re-use of data stored in the crash-simulation department at BMW in order to gain deeper insight into the interrelations between the geometric variations of the car during its design and its performance in crash testing. In this paper a method for data analysis of finite element models and results from crash simulation is proposed and application to recent data from the industrial partner BMW is demonstrated. All necessary steps from data pre-processing to re-integration into the working environment of the engineer are covered.",
        "published": "2005-05-02T15:27:45Z",
        "link": "http://arxiv.org/abs/cs/0505008v1",
        "categories": [
            "cs.IR",
            "cs.CE",
            "H.2.8; D.2.2"
        ]
    },
    {
        "title": "Methods for comparing rankings of search engine results",
        "authors": [
            "Judit Bar-Ilan",
            "Mazlita Mat-Hassan",
            "Mark Levene"
        ],
        "summary": "In this paper we present a number of measures that compare rankings of search engine results. We apply these measures to five queries that were monitored daily for two periods of about 21 days each. Rankings of the different search engines (Google, Yahoo and Teoma for text searches and Google, Yahoo and Picsearch for image searches) are compared on a daily basis, in addition to longitudinal comparisons of the same engine for the same query over time. The results and rankings of the two periods are compared as well.",
        "published": "2005-05-14T17:48:07Z",
        "link": "http://arxiv.org/abs/cs/0505039v1",
        "categories": [
            "cs.IR",
            "H.3.3"
        ]
    },
    {
        "title": "Optimum Signal Linear Detector in the Discrete Wavelet Transform-Domain",
        "authors": [
            "Ignacio Melgar",
            "Jaime Gomez",
            "Juan Seijas"
        ],
        "summary": "The problem of known signal detection in Additive White Gaussian Noise is considered. In this paper a new detection algorithm based on Discrete Wavelet Transform pre-processing and threshold comparison is introduced. Current approaches described in [7] use the maximum value obtained in the wavelet domain for decision. Here, we use all available information in the wavelet domain with excellent results. Detector performance is presented in Probability of detection curves for a fixed probability of false alarm.",
        "published": "2005-05-18T11:03:18Z",
        "link": "http://arxiv.org/abs/cs/0505046v1",
        "categories": [
            "cs.IT",
            "cs.IR",
            "math.IT"
        ]
    },
    {
        "title": "Sub-Optimum Signal Linear Detector Using Wavelets and Support Vector   Machines",
        "authors": [
            "Jaime Gomez",
            "Ignacio Melgar",
            "Juan Seijas",
            "Diego Andina"
        ],
        "summary": "The problem of known signal detection in Additive White Gaussian Noise is considered. In previous work, a new detection scheme was introduced by the authors, and it was demonstrated that optimum performance cannot be reached in a real implementation. In this paper we analyse Support Vector Machines (SVM) as an alternative, evaluating the results in terms of Probability of detection curves for a fixed Probability of false alarm.",
        "published": "2005-05-20T14:54:40Z",
        "link": "http://arxiv.org/abs/cs/0505051v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Upgrading Pulse Detection with Time Shift Properties Using Wavelets and   Support Vector Machines",
        "authors": [
            "Jaime Gomez",
            "Ignacio Melgar",
            "Juan Seijas"
        ],
        "summary": "Current approaches in pulse detection use domain transformations so as to concentrate frequency related information that can be distinguishable from noise. In real cases we do not know when the pulse will begin, so we need a time search process in which time windows are scheduled and analysed. Each window can contain the pulsed signal (either complete or incomplete) and / or noise. In this paper a simple search process will be introduced, allowing the algorithm to process more information, upgrading the capabilities in terms of probability of detection (Pd) and probability of false alarm (Pfa).",
        "published": "2005-05-20T15:01:20Z",
        "link": "http://arxiv.org/abs/cs/0505052v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Wavelet Time Shift Properties Integration with Support Vector Machines",
        "authors": [
            "Jaime Gomez",
            "Ignacio Melgar",
            "Juan Seijas"
        ],
        "summary": "This paper presents a short evaluation about the integration of information derived from wavelet non-linear-time-invariant (non-LTI) projection properties using Support Vector Machines (SVM). These properties may give additional information for a classifier trying to detect known patterns hidden by noise. In the experiments we present a simple electromagnetic pulsed signal recognition scheme, where some improvement is achieved with respect to previous work. SVMs are used as a tool for information integration, exploiting some unique properties not easily found in neural networks.",
        "published": "2005-05-20T15:06:40Z",
        "link": "http://arxiv.org/abs/cs/0505053v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Text Compression and Superfast Searching",
        "authors": [
            "Udayan Khurana",
            "Anirudh Koul"
        ],
        "summary": "In this paper, a new compression scheme for text is presented. The same is efficient in giving high compression ratios and enables super fast searching within the compressed text. Typical compression ratios of 70-80% and reducing the search time by 80-85% are the features of this paper. Till now, a trade-off between high ratios and searchability within compressed text has been seen. In this paper, we show that greater the compression, faster the search. This finds applicability in so many places where data as natural language text is present.",
        "published": "2005-05-23T07:04:49Z",
        "link": "http://arxiv.org/abs/cs/0505056v1",
        "categories": [
            "cs.IR",
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Treillis de concepts et ontologies pour l'interrogation d'un annuaire de   sources de données biologiques (BioRegistry)",
        "authors": [
            "Nizar Messai",
            "Marie-Dominique Devignes",
            "Malika Smaïl-Tabbone",
            "Amedeo Napoli"
        ],
        "summary": "Bioinformatic data sources available on the web are multiple and heterogenous. The lack of documentation and the difficulty of interaction with these data sources require users competence in both informatics and biological fields for an optimal use of sources contents that remain rather under exploited. In this paper we present an approach based on formal concept analysis to classify and search relevant bioinformatic data sources for a given query. It consists in building the concept lattice from the binary relation between bioinformatic data sources and their associated metadata. The concept built from a given query is then merged into the concept lattice. The result is given by the extraction of the set of sources belonging to the extents of the query concept subsumers in the resulting concept lattice. The sources ranking is given by the concept specificity order in the concept lattice. An improvement of the approach consists in automatic query refinement thanks to domain ontologies. Two forms of refinement are possible by generalisation and by specialisation.   -----   Les sources de donn\\'{e}es biologiques disponibles sur le web sont multiples et h\\'{e}t\\'{e}rog\\`{e}nes. L'utilisation optimale de ces ressources n\\'{e}cessite aujourd'hui de la part des utilisateurs des comp\\'{e}tences \\`{a} la fois en informatique et en biologie, du fait du manque de documentation et des difficult\\'{e}s d'interaction avec les sources de donn\\'{e}es. De fait, les contenus de ces ressources restent souvent sous-exploit\\'{e}s. Nous pr\\'{e}sentons ici une approche bas\\'{e}e sur l'analyse de concepts formels, pour organiser et rechercher des sources de donn\\'{e}es biologiques pertinentes pour une requ\\^{e}te donn\\'{e}e. Le travail consiste \\`{a} construire un treillis de concepts \\`{a} partir des m\\'{e}ta-donn\\'{e}es associ\\'{e}es aux sources. Le concept construit \\`{a} partir d'une requ\\^{e}te donn\\'{e}e est alors int\\'{e}gr\\'{e} au treillis. La r\\'{e}ponse \\`{a} la requ\\^{e}te est ensuite fournie par l'extraction des sources de donn\\'{e}es appartenant aux extensions des concepts subsumant le concept requ\\^{e}te dans le treillis. Les sources ainsi retourn\\'{e}es peuvent \\^{e}tre tri\\'{e}es selon l'ordre de sp\\'{e}cificit\\'{e} des concepts dans le treillis. Une proc\\'{e}dure de raffinement de requ\\^{e}te, bas\\'{e}e sur des ontologies du domaine, permet d'am\\'{e}liorer le rappel par g\\'{e}n\\'{e}ralisation ou par sp\\'{e}cialisation",
        "published": "2005-06-06T12:49:53Z",
        "link": "http://arxiv.org/abs/cs/0506017v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Analyse et expansion des textes en question-réponse",
        "authors": [
            "Bernard Jacquemin"
        ],
        "summary": "This paper presents an original methodology to consider question answering. We noticed that query expansion is often incorrect because of a bad understanding of the question. But the automatic good understanding of an utterance is linked to the context length, and the question are often short. This methodology proposes to analyse the documents and to construct an informative structure from the results of the analysis and from a semantic text expansion. The linguistic analysis identifies words (tokenization and morphological analysis), links between words (syntactic analysis) and word sense (semantic disambiguation). The text expansion adds to each word the synonyms matching its sense and replaces the words in the utterances by derivatives, modifying the syntactic schema if necessary. In this way, whatever enrichment may be, the text keeps the same meaning, but each piece of information matches many realisations. The questioning method consists in constructing a local informative structure without enrichment, and matches it with the documentary structure. If a sentence in the informative structure matches the question structure, this sentence is the answer to the question.",
        "published": "2005-06-12T16:39:01Z",
        "link": "http://arxiv.org/abs/cs/0506047v1",
        "categories": [
            "cs.IR",
            "H.3; H.4; H.5"
        ]
    },
    {
        "title": "Enriching a Text by Semantic Disambiguation for Information Extraction",
        "authors": [
            "Bernard Jacquemin",
            "Caroline Brun",
            "Claude Roux"
        ],
        "summary": "External linguistic resources have been used for a very long time in information extraction. These methods enrich a document with data that are semantically equivalent, in order to improve recall. For instance, some of these methods use synonym dictionaries. These dictionaries enrich a sentence with words that have a similar meaning. However, these methods present some serious drawbacks, since words are usually synonyms only in restricted contexts. The method we propose here consists of using word sense disambiguation rules (WSD) to restrict the selection of synonyms to only these that match a specific syntactico-semantic context. We show how WSD rules are built and how information extraction techniques can benefit from the application of these rules.",
        "published": "2005-06-12T16:44:05Z",
        "link": "http://arxiv.org/abs/cs/0506048v1",
        "categories": [
            "cs.IR",
            "H.3; H.4; H.5"
        ]
    },
    {
        "title": "Dynamical Neural Network: Information and Topology",
        "authors": [
            "David Dominguez",
            "Kostadin Koroutchev",
            "Eduardo Serrano",
            "Francisco B. Rodriguez"
        ],
        "summary": "A neural network works as an associative memory device if it has large storage capacity and the quality of the retrieval is good enough. The learning and attractor abilities of the network both can be measured by the mutual information (MI), between patterns and retrieval states. This paper deals with a search for an optimal topology, of a Hebb network, in the sense of the maximal MI. We use small-world topology. The connectivity $\\gamma$ ranges from an extremely diluted to the fully connected network; the randomness $\\omega$ ranges from purely local to completely random neighbors. It is found that, while stability implies an optimal $MI(\\gamma,\\omega)$ at $\\gamma_{opt}(\\omega)\\to 0$, for the dynamics, the optimal topology holds at certain $\\gamma_{opt}>0$ whenever $0\\leq\\omega<0.3$.",
        "published": "2005-06-20T14:54:41Z",
        "link": "http://arxiv.org/abs/cs/0506078v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Experiments in Clustering Homogeneous XML Documents to Validate an   Existing Typology",
        "authors": [
            "Thierry Despeyroux",
            "Yves Lechevallier",
            "Brigitte Trousse",
            "Anne-Marie Vercoustre"
        ],
        "summary": "This paper presents some experiments in clustering homogeneous XMLdocuments to validate an existing classification or more generally anorganisational structure. Our approach integrates techniques for extracting knowledge from documents with unsupervised classification (clustering) of documents. We focus on the feature selection used for representing documents and its impact on the emerging classification. We mix the selection of structured features with fine textual selection based on syntactic characteristics.We illustrate and evaluate this approach with a collection of Inria activity reports for the year 2003. The objective is to cluster projects into larger groups (Themes), based on the keywords or different chapters of these activity reports. We then compare the results of clustering using different feature selections, with the official theme structure used by Inria.",
        "published": "2005-07-08T13:42:42Z",
        "link": "http://arxiv.org/abs/cs/0507024v2",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Users and Assessors in the Context of INEX: Are Relevance Dimensions   Relevant?",
        "authors": [
            "Jovan Pehcevski",
            "James A. Thom",
            "Anne-Marie Vercoustre"
        ],
        "summary": "The main aspects of XML retrieval are identified by analysing and comparing the following two behaviours: the behaviour of the assessor when judging the relevance of returned document components; and the behaviour of users when interacting with components of XML documents. We argue that the two INEX relevance dimensions, Exhaustivity and Specificity, are not orthogonal dimensions; indeed, an empirical analysis of each dimension reveals that the grades of the two dimensions are correlated to each other. By analysing the level of agreement between the assessor and the users, we aim at identifying the best units of retrieval. The results of our analysis show that the highest level of agreement is on highly relevant and on non-relevant document components, suggesting that only the end points of the INEX 10-point relevance scale are perceived in the same way by both the assessor and the users. We propose a new definition of relevance for XML retrieval and argue that its corresponding relevance scale would be a better choice for INEX.",
        "published": "2005-07-28T15:02:04Z",
        "link": "http://arxiv.org/abs/cs/0507069v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Hybrid XML Retrieval: Combining Information Retrieval and a Native XML   Database",
        "authors": [
            "Jovan Pehcevski",
            "James A. Thom",
            "Anne-Marie Vercoustre"
        ],
        "summary": "This paper investigates the impact of three approaches to XML retrieval: using Zettair, a full-text information retrieval system; using eXist, a native XML database; and using a hybrid system that takes full article answers from Zettair and uses eXist to extract elements from those articles. For the content-only topics, we undertake a preliminary analysis of the INEX 2003 relevance assessments in order to identify the types of highly relevant document components. Further analysis identifies two complementary sub-cases of relevance assessments (\"General\" and \"Specific\") and two categories of topics (\"Broad\" and \"Narrow\"). We develop a novel retrieval module that for a content-only topic utilises the information from the resulting answer list of a native XML database and dynamically determines the preferable units of retrieval, which we call \"Coherent Retrieval Elements\". The results of our experiments show that -- when each of the three systems is evaluated against different retrieval scenarios (such as different cases of relevance assessments, different topic categories and different choices of evaluation metrics) -- the XML retrieval systems exhibit varying behaviour and the best performance can be reached for different values of the retrieval parameters. In the case of INEX 2003 relevance assessments for the content-only topics, our newly developed hybrid XML retrieval system is substantially more effective than either Zettair or eXist, and yields a robust and a very effective XML retrieval.",
        "published": "2005-07-28T19:19:12Z",
        "link": "http://arxiv.org/abs/cs/0507070v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Enhancing Content-And-Structure Information Retrieval using a Native XML   Database",
        "authors": [
            "Jovan Pehcevski",
            "James A. Thom",
            "Anne-Marie Vercoustre"
        ],
        "summary": "Three approaches to content-and-structure XML retrieval are analysed in this paper: first by using Zettair, a full-text information retrieval system; second by using eXist, a native XML database, and third by using a hybrid XML retrieval system that uses eXist to produce the final answers from likely relevant articles retrieved by Zettair. INEX 2003 content-and-structure topics can be classified in two categories: the first retrieving full articles as final answers, and the second retrieving more specific elements within articles as final answers. We show that for both topic categories our initial hybrid system improves the retrieval effectiveness of a native XML database. For ranking the final answer elements, we propose and evaluate a novel retrieval model that utilises the structural relationships between the answer elements of a native XML database and retrieves Coherent Retrieval Elements. The final results of our experiments show that when the XML retrieval task focusses on highly relevant elements our hybrid XML retrieval system with the Coherent Retrieval Elements module is 1.8 times more effective than Zettair and 3 times more effective than eXist, and yields an effective content-and-structure XML retrieval.",
        "published": "2005-08-02T15:05:18Z",
        "link": "http://arxiv.org/abs/cs/0508017v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Expériences de classification d'une collection de documents XML de   structure homogène",
        "authors": [
            "Thierry Despeyroux",
            "Yves Lechevallier",
            "Brigitte Trousse",
            "Anne-Marie Vercoustre"
        ],
        "summary": "This paper presents some experiments in clustering homogeneous XMLdocuments to validate an existing classification or more generally anorganisational structure. Our approach integrates techniques for extracting knowledge from documents with unsupervised classification (clustering) of documents. We focus on the feature selection used for representing documents and its impact on the emerging classification. We mix the selection of structured features with fine textual selection based on syntactic characteristics.We illustrate and evaluate this approach with a collection of Inria activity reports for the year 2003. The objective is to cluster projects into larger groups (Themes), based on the keywords or different chapters of these activity reports. We then compare the results of clustering using different feature selections, with the official theme structure used by Inria.",
        "published": "2005-08-04T14:14:59Z",
        "link": "http://arxiv.org/abs/cs/0508036v2",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Measuring Semantic Similarity by Latent Relational Analysis",
        "authors": [
            "Peter D. Turney"
        ],
        "summary": "This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similarity. LRA measures similarity in the semantic relations between two pairs of words. When two pairs have a high degree of relational similarity, they are analogous. For example, the pair cat:meow is analogous to the pair dog:bark. There is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning). In the Vector Space Model (VSM) approach to measuring relational similarity, the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs. The elements in the vectors are based on the frequencies of manually constructed patterns in a large corpus. LRA extends the VSM approach in three ways: (1) patterns are derived automatically from the corpus, (2) Singular Value Decomposition is used to smooth the frequency data, and (3) synonyms are used to reformulate word pairs. This paper describes the LRA algorithm and experimentally compares LRA to VSM on two tasks, answering college-level multiple-choice word analogy questions and classifying semantic relations in noun-modifier expressions. LRA achieves state-of-the-art results, reaching human-level performance on the analogy questions and significantly exceeding VSM performance on both tasks.",
        "published": "2005-08-10T19:35:57Z",
        "link": "http://arxiv.org/abs/cs/0508053v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Can Small Museums Develop Compelling, Educational and Accessible Web   Resources? The Case of Accademia Carrara",
        "authors": [
            "Silvia Filippini-Fantoni",
            "Jonathan P. Bowen"
        ],
        "summary": "Due to the lack of budget, competence, personnel and time, small museums are often unable to develop compelling, educational and accessible web resources for their permanent collections or temporary exhibitions. In an attempt to prove that investing in these types of resources can be very fruitful even for small institutions, we will illustrate the case of Accademia Carrara, a museum in Bergamo, northern Italy, which, for a current temporary exhibition on Cezanne and Renoir's masterpieces from the Paul Guillaume collection, developed a series of multimedia applications, including an accessible website, rich in content and educational material [www.cezannerenoir.it].",
        "published": "2005-08-13T14:46:16Z",
        "link": "http://arxiv.org/abs/cs/0508066v1",
        "categories": [
            "cs.MM",
            "cs.CY",
            "cs.DL",
            "cs.IR",
            "H.3.5; H.3.7; H.4.3; H.5.1; H.5.2; H.5.3; H.5.4; K.3.1; K.4.0"
        ]
    },
    {
        "title": "Summarizing Reports on Evolving Events; Part I: Linear Evolution",
        "authors": [
            "Stergos D. Afantenos",
            "Vangelis Karkaletsis",
            "Panagiotis Stamatopoulos"
        ],
        "summary": "We present an approach for summarization from multiple documents which report on events that evolve through time, taking into account the different document sources. We distinguish the evolution of an event into linear and non-linear. According to our approach, each document is represented by a collection of messages which are then used in order to instantiate the cross-document relations that determine the summary content. The paper presents the summarization system that implements this approach through a case study on linear evolution.",
        "published": "2005-08-22T12:56:32Z",
        "link": "http://arxiv.org/abs/cs/0508092v1",
        "categories": [
            "cs.CL",
            "cs.IR"
        ]
    },
    {
        "title": "Corpus-based Learning of Analogies and Semantic Relations",
        "authors": [
            "Peter D. Turney",
            "Michael L. Littman"
        ],
        "summary": "We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning \"A is to B as C is to D\"; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47% of a collection of 374 college-level analogy questions (random guessing would yield 20% correct; the average college-bound senior high school student answers about 57% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as \"laser printer\", according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5% (random guessing: 3.3%). With 5 classes of semantic relations, the F value is 43.2% (random: 20%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations.",
        "published": "2005-08-23T20:21:56Z",
        "link": "http://arxiv.org/abs/cs/0508103v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.1; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Combining Structured Corporate Data and Document Content to Improve   Expertise Finding",
        "authors": [
            "Alistair McLean",
            "Mingfang Wu",
            "Anne-Marie Vercoustre"
        ],
        "summary": "In this paper, we present an algorithm for automatically building expertise evidence for finding experts within an organization by combining structured corporate information with different content. We also describe our test data collection and our evaluation method. Evaluation of the algorithm shows that using organizational structure leads to a significant improvement in the precision of finding an expert. Furthermore we evaluate the impact of using different data sources on the quality of the results and conclude that Expert Finding is not a \"one engine fits all\" solution. It requires an analysis of the information space into which a solution will be placed and the appropriate selection and weighting scheme of the data sources.",
        "published": "2005-09-02T08:24:07Z",
        "link": "http://arxiv.org/abs/cs/0509005v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Lattices for Dynamic, Hierarchic & Overlapping Categorization: the Case   of Epistemic Communities",
        "authors": [
            "Camille Roth",
            "Paul Bourgine"
        ],
        "summary": "We present a method for hierarchic categorization and taxonomy evolution description. We focus on the structure of epistemic communities (ECs), or groups of agents sharing common knowledge concerns. Introducing a formal framework based on Galois lattices, we categorize ECs in an automated and hierarchically structured way and propose criteria for selecting the most relevant epistemic communities - for instance, ECs gathering a certain proportion of agents and thus prototypical of major fields. This process produces a manageable, insightful taxonomy of the community. Then, the longitudinal study of these static pictures makes possible an historical description. In particular, we capture stylized facts such as field progress, decline, specialization, interaction (merging or splitting), and paradigm emergence. The detection of such patterns in social networks could fruitfully be applied to other contexts.",
        "published": "2005-09-04T18:15:40Z",
        "link": "http://arxiv.org/abs/nlin/0509007v1",
        "categories": [
            "nlin.AO",
            "cs.AI",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Transitive Text Mining for Information Extraction and Hypothesis   Generation",
        "authors": [
            "Johannes Stegmann",
            "Guenter Grohmann"
        ],
        "summary": "Transitive text mining - also named Swanson Linking (SL) after its primary and principal researcher - tries to establish meaningful links between literature sets which are virtually disjoint in the sense that each does not mention the main concept of the other. If successful, SL may give rise to the development of new hypotheses. In this communication we describe our approach to transitive text mining which employs co-occurrence analysis of the medical subject headings (MeSH), the descriptors assigned to papers indexed in PubMed. In addition, we will outline the current state of our web-based information system which will enable our users to perform literature-driven hypothesis building on their own.",
        "published": "2005-09-07T12:16:22Z",
        "link": "http://arxiv.org/abs/cs/0509020v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "Authoring case based training by document data extraction",
        "authors": [
            "Christian Betz",
            "Alexander Hoernlein",
            "Frank Puppe"
        ],
        "summary": "In this paper, we propose an scalable approach to modeling based upon word processing documents, and we describe the tool Phoenix providing the technical infrastructure.   For our training environment d3web.Train, we developed a tool to extract case knowledge from existing documents, usually dismissal records, extending Phoenix to d3web.CaseImporter. Independent authors used this tool to develop training systems, observing a significant decrease of time for setteling-in and a decrease of time necessary for developing a case.",
        "published": "2005-09-14T13:20:21Z",
        "link": "http://arxiv.org/abs/cs/0509040v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Folksonomy as a Complex Network",
        "authors": [
            "Kaikai Shen",
            "Lide Wu"
        ],
        "summary": "Folksonomy is an emerging technology that works to classify the information over WWW through tagging the bookmarks, photos or other web-based contents. It is understood to be organized by every user while not limited to the authors of the contents and the professional editors. This study surveyed the folksonomy as a complex network. The result indicates that the network, which is composed of the tags from the folksonomy, displays both properties of small world and scale-free. However, the statistics only shows a local and static slice of the vast body of folksonomy which is still evolving.",
        "published": "2005-09-23T13:27:18Z",
        "link": "http://arxiv.org/abs/cs/0509072v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Sur le statut référentiel des entités nommées",
        "authors": [
            "Thierry Poibeau"
        ],
        "summary": "We show in this paper that, on the one hand, named entities can be designated using different denominations and that, on the second hand, names denoting named entities are polysemous. The analysis cannot be limited to reference resolution but should take into account naming strategies, which are mainly based on two linguistic operations: synecdoche and metonymy. Lastly, we present a model that explicitly represents the different denominations in discourse, unifying the way to represent linguistic knowledge and world knowledge.",
        "published": "2005-10-07T17:39:40Z",
        "link": "http://arxiv.org/abs/cs/0510020v1",
        "categories": [
            "cs.AI",
            "cs.IR"
        ]
    },
    {
        "title": "Practical Semantic Analysis of Web Sites and Documents",
        "authors": [
            "Thierry Despeyroux"
        ],
        "summary": "As Web sites are now ordinary products, it is necessary to explicit the notion of quality of a Web site. The quality of a site may be linked to the easiness of accessibility and also to other criteria such as the fact that the site is up to date and coherent. This last quality is difficult to insure because sites may be updated very frequently, may have many authors, may be partially generated and in this context proof-reading is very difficult. The same piece of information may be found in different occurrences, but also in data or meta-data, leading to the need for consistency checking. In this paper we make a parallel between programs and Web sites. We present some examples of semantic constraints that one would like to specify (constraints between the meaning of categories and sub-categories in a thematic directory, consistency between the organization chart and the rest of the site in an academic site). We present quickly the Natural Semantics, a way to specify the semantics of programming languages that inspires our works. Then we propose a specification language for semantic constraints in Web sites that, in conjunction with the well known ``make'' program, permits to generate some site verification tools by compiling the specification into Prolog code. We apply our method to a large XML document which is the scientific part of our institute activity report, tracking errors or inconsistencies and also constructing some indicators that can be used by the management of the institute.",
        "published": "2005-10-11T08:40:05Z",
        "link": "http://arxiv.org/abs/cs/0510025v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Hiérarchisation des règles d'association en fouille de textes",
        "authors": [
            "Rokia Bendaoud",
            "Yannick Toussaint",
            "Amedeo Napoli"
        ],
        "summary": "Extraction of association rules is widely used as a data mining method. However, one of the limit of this approach comes from the large number of extracted rules and the difficulty for a human expert to deal with the totality of these rules. We propose to solve this problem by structuring the set of rules into hierarchy. The expert can then therefore explore the rules, access from one rule to another one more general when we raise up in the hierarchy, and in other hand, or a more specific rules. Rules are structured at two levels. The global level aims at building a hierarchy from the set of rules extracted. Thus we define a first type of rule-subsomption relying on Galois lattices. The second level consists in a local and more detailed analysis of each rule. It generate for a given rule a set of generalization rules structured into a local hierarchy. This leads to the definition of a second type of subsomption. This subsomption comes from inductive logic programming and integrates a terminological model.",
        "published": "2005-10-14T14:24:26Z",
        "link": "http://arxiv.org/abs/cs/0510037v1",
        "categories": [
            "cs.IR",
            "cs.AI"
        ]
    },
    {
        "title": "The Nature of Novelty Detection",
        "authors": [
            "Le Zhao",
            "Min Zhang",
            "Shaoping Ma"
        ],
        "summary": "Sentence level novelty detection aims at reducing redundant sentences from a sentence list. In the task, sentences appearing later in the list with no new meanings are eliminated. Aiming at a better accuracy for detecting redundancy, this paper reveals the nature of the novelty detection task currently overlooked by the Novelty community $-$ Novelty as a combination of the partial overlap (PO, two sentences sharing common facts) and complete overlap (CO, the first sentence covers all the facts of the second sentence) relations. By formalizing novelty detection as a combination of the two relations between sentences, new viewpoints toward techniques dealing with Novelty are proposed. Among the methods discussed, the similarity, overlap, pool and language modeling approaches are commonly used. Furthermore, a novel approach, selected pool method is provided, which is immediate following the nature of the task. Experimental results obtained on all the three currently available novelty datasets showed that selected pool is significantly better or no worse than the current methods. Knowledge about the nature of the task also affects the evaluation methodologies. We propose new evaluation measures for Novelty according to the nature of the task, as well as possible directions for future study.",
        "published": "2005-10-19T14:56:48Z",
        "link": "http://arxiv.org/abs/cs/0510054v1",
        "categories": [
            "cs.IR",
            "cs.CL",
            "H.3.3"
        ]
    },
    {
        "title": "Réflexions sur la question fréquentielle en traitement du signal",
        "authors": [
            "Michel Fliess"
        ],
        "summary": "New definitions are suggested for frequencies which may be instantaneous or not. The Heisenberg-Gabor inequality and the Shannon sampling theorem are briefly discussed.",
        "published": "2005-10-26T14:49:47Z",
        "link": "http://arxiv.org/abs/cs/0510084v1",
        "categories": [
            "cs.CE",
            "cs.IR",
            "math-ph",
            "math.MP",
            "math.SP"
        ]
    },
    {
        "title": "Bibliographic Classification using the ADS Databases",
        "authors": [
            "Alberto Accomazzi",
            "Michael J. Kurtz",
            "Guenther Eichhorn",
            "Edwin Henneken",
            "Carolyn S. Grant",
            "Markus Demleitner",
            "Stephen S. Murray"
        ],
        "summary": "We discuss two techniques used to characterize bibliographic records based on their similarity to and relationship with the contents of the NASA Astrophysics Data System (ADS) databases. The first method has been used to classify input text as being relevant to one or more subject areas based on an analysis of the frequency distribution of its individual words. The second method has been used to classify existing records as being relevant to one or more databases based on the distribution of the papers citing them. Both techniques have proven to be valuable tools in assigning new and existing bibliographic records to different disciplines within the ADS databases.",
        "published": "2005-10-31T22:34:34Z",
        "link": "http://arxiv.org/abs/cs/0511002v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "The egalitarian effect of search engines",
        "authors": [
            "Santo Fortunato",
            "Alessandro Flammini",
            "Filippo Menczer",
            "Alessandro Vespignani"
        ],
        "summary": "Search engines have become key media for our scientific, economic, and social activities by enabling people to access information on the Web in spite of its size and complexity. On the down side, search engines bias the traffic of users according to their page-ranking strategies, and some have argued that they create a vicious cycle that amplifies the dominance of established and already popular sites. We show that, contrary to these prior claims and our own intuition, the use of search engines actually has an egalitarian effect. We reconcile theoretical arguments with empirical evidence showing that the combination of retrieval by search engines and search behavior by users mitigates the attraction of popular pages, directing more traffic toward less popular sites, even in comparison to what would be expected from users randomly surfing the Web.",
        "published": "2005-11-01T22:33:48Z",
        "link": "http://arxiv.org/abs/cs/0511005v2",
        "categories": [
            "cs.CY",
            "cs.IR",
            "physics.soc-ph",
            "H.3.3; H.3.4; H.3.5; H.5.4; K.4.m"
        ]
    },
    {
        "title": "How to make the top ten: Approximating PageRank from in-degree",
        "authors": [
            "Santo Fortunato",
            "Marian Boguna",
            "Alessandro Flammini",
            "Filippo Menczer"
        ],
        "summary": "PageRank has become a key element in the success of search engines, allowing to rank the most important hits in the top screen of results. One key aspect that distinguishes PageRank from other prestige measures such as in-degree is its global nature. From the information provider perspective, this makes it difficult or impossible to predict how their pages will be ranked. Consequently a market has emerged for the optimization of search engine results. Here we study the accuracy with which PageRank can be approximated by in-degree, a local measure made freely available by search engines. Theoretical and empirical analyses lead to conclude that given the weak degree correlations in the Web link graph, the approximation can be relatively accurate, giving service and information providers an effective new marketing tool.",
        "published": "2005-11-03T23:01:50Z",
        "link": "http://arxiv.org/abs/cs/0511016v1",
        "categories": [
            "cs.IR",
            "physics.soc-ph",
            "H.3.3; H.3.4; H.3.5; K.4.m"
        ]
    },
    {
        "title": "Entangled messages",
        "authors": [
            "Arindam Mitra"
        ],
        "summary": "It is sometimes necessary to send copies of the same email to different parties, but it is impossible to ensure that if one party reads the message the other parties will bound to read it. We propose an entanglement based scheme where if one party reads the message the other party will bound to read it simultaneously.",
        "published": "2005-12-01T15:40:19Z",
        "link": "http://arxiv.org/abs/cs/0512007v1",
        "categories": [
            "cs.CR",
            "cs.IR"
        ]
    },
    {
        "title": "A Software Framework for Vehicle-Infrastructure Cooperative Applications",
        "authors": [
            "Sebastián Bengochea",
            "Angel Talamona",
            "Michel Parent"
        ],
        "summary": "A growing category of vehicle-infrastructure cooperative (VIC) applications requires telematics software components distributed between an infrastructure-based management center and a number of vehicles. This article presents an approach based on a software framework, focusing on a Telematic Management System (TMS), a component suite aimed to run inside an infrastructure-based operations center, in some cases interacting with legacy systems like Advanced Traffic Management Systems or Vehicle Relationship Management. The TMS framework provides support for modular, flexible, prototyping and implementation of VIC applications. This work has received the support of the European Commission in the context of the projects REACT and CyberCars.",
        "published": "2005-12-08T09:37:49Z",
        "link": "http://arxiv.org/abs/cs/0512032v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Complex Random Vectors and ICA Models: Identifiability, Uniqueness and   Separability",
        "authors": [
            "Jan Eriksson",
            "Visa Koivunen"
        ],
        "summary": "In this paper the conditions for identifiability, separability and uniqueness of linear complex valued independent component analysis (ICA) models are established. These results extend the well-known conditions for solving real-valued ICA problems to complex-valued models. Relevant properties of complex random vectors are described in order to extend the Darmois-Skitovich theorem for complex-valued models. This theorem is used to construct a proof of a theorem for each of the above ICA model concepts. Both circular and noncircular complex random vectors are covered. Examples clarifying the above concepts are presented.",
        "published": "2005-12-15T14:51:36Z",
        "link": "http://arxiv.org/abs/cs/0512063v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "cs.IR",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Reconstructing Websites for the Lazy Webmaster",
        "authors": [
            "Frank McCown",
            "Joan A. Smith",
            "Michael L. Nelson",
            "Johan Bollen"
        ],
        "summary": "Backup or preservation of websites is often not considered until after a catastrophic event has occurred. In the face of complete website loss, \"lazy\" webmasters or concerned third parties may be able to recover some of their website from the Internet Archive. Other pages may also be salvaged from commercial search engine caches. We introduce the concept of \"lazy preservation\"- digital preservation performed as a result of the normal operations of the Web infrastructure (search engines and caches). We present Warrick, a tool to automate the process of website reconstruction from the Internet Archive, Google, MSN and Yahoo. Using Warrick, we have reconstructed 24 websites of varying sizes and composition to demonstrate the feasibility and limitations of website reconstruction from the public Web infrastructure. To measure Warrick's window of opportunity, we have profiled the time required for new Web resources to enter and leave search engine caches.",
        "published": "2005-12-16T16:02:13Z",
        "link": "http://arxiv.org/abs/cs/0512069v1",
        "categories": [
            "cs.IR",
            "cs.CY",
            "H.3.5"
        ]
    },
    {
        "title": "Analyzing and Visualizing the Semantic Coverage of Wikipedia and Its   Authors",
        "authors": [
            "Todd Holloway",
            "Miran Bozicevic",
            "Katy Börner"
        ],
        "summary": "This paper presents a novel analysis and visualization of English Wikipedia data. Our specific interest is the analysis of basic statistics, the identification of the semantic structure and age of the categories in this free online encyclopedia, and the content coverage of its highly productive authors. The paper starts with an introduction of Wikipedia and a review of related work. We then introduce a suite of measures and approaches to analyze and map the semantic structure of Wikipedia. The results show that co-occurrences of categories within individual articles have a power-law distribution, and when mapped reveal the nicely clustered semantic structure of Wikipedia. The results also reveal the content coverage of the article's authors, although the roles these authors play are as varied as the authors themselves. We conclude with a discussion of major results and planned future work.",
        "published": "2005-12-21T19:31:23Z",
        "link": "http://arxiv.org/abs/cs/0512085v1",
        "categories": [
            "cs.IR"
        ]
    },
    {
        "title": "Extending Design by Contract for Aspect-Oriented Programming",
        "authors": [
            "David H. Lorenz",
            "Therapon Skotiniotis"
        ],
        "summary": "Design by Contract (DbC) and runtime enforcement of program assertions enables the construction of more robust software. It also enables the assignment of blame in error reporting. Unfortunately, there is no support for runtime contract enforcement and blame assignment for Aspect-Oriented Programming (AOP). Extending DbC to also cover aspects brings forward a plethora of issues related to the correct order of assertion validation. We show that there is no generally correct execution sequence of object assertions and aspect assertions. A further classification of aspects as agnostic, obedient, or rebellious defines the order of assertion validation that needs to be followed. We describe the application of this classification in a prototyped DbC tool for AOP named Cona, where aspects are used for implementing contracts, and contracts are used for enforcing assertions on aspects.",
        "published": "2005-01-24T20:28:46Z",
        "link": "http://arxiv.org/abs/cs/0501070v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.2, D.2.3, D.2.4"
        ]
    },
    {
        "title": "Optimal Union-Find in Constraint Handling Rules",
        "authors": [
            "Tom Schrijvers",
            "Thom Fruehwirth"
        ],
        "summary": "Constraint Handling Rules (CHR) is a committed-choice rule-based language that was originally intended for writing constraint solvers. In this paper we show that it is also possible to write the classic union-find algorithm and variants in CHR. The programs neither compromise in declarativeness nor efficiency. We study the time complexity of our programs: they match the almost-linear complexity of the best known imperative implementations. This fact is illustrated with experimental results.",
        "published": "2005-01-25T13:28:38Z",
        "link": "http://arxiv.org/abs/cs/0501073v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "cs.DS",
            "cs.PF"
        ]
    },
    {
        "title": "Proving Correctness and Completeness of Normal Programs - a Declarative   Approach",
        "authors": [
            "W. Drabent",
            "M. Milkowska"
        ],
        "summary": "We advocate a declarative approach to proving properties of logic programs. Total correctness can be separated into correctness, completeness and clean termination; the latter includes non-floundering. Only clean termination depends on the operational semantics, in particular on the selection rule. We show how to deal with correctness and completeness in a declarative way, treating programs only from the logical point of view. Specifications used in this approach are interpretations (or theories). We point out that specifications for correctness may differ from those for completeness, as usually there are answers which are neither considered erroneous nor required to be computed.   We present proof methods for correctness and completeness for definite programs and generalize them to normal programs. For normal programs we use the 3-valued completion semantics; this is a standard semantics corresponding to negation as finite failure. The proof methods employ solely the classical 2-valued logic. We use a 2-valued characterization of the 3-valued completion semantics which may be of separate interest. The presented methods are compared with an approach based on operational semantics. We also employ the ideas of this work to generalize a known method of proving termination of normal programs.",
        "published": "2005-01-25T16:53:59Z",
        "link": "http://arxiv.org/abs/cs/0501043v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.3.1; D.1.6; D.2.4"
        ]
    },
    {
        "title": "Structuring quantum effects: superoperators as arrows",
        "authors": [
            "J. K. Vizzotto",
            "T. Altenkirch",
            "A. Sabry"
        ],
        "summary": "We show that the model of quantum computation based on density matrices and superoperators can be decomposed in a pure classical (functional) part and an effectful part modeling probabilities and measurement. The effectful part can be modeled using a generalization of monads called arrows. We express the resulting executable model of quantum computing in the programming language Haskell using its special syntax for arrow computations. The embedding in Haskell is however not perfect: a faithful model of quantum computing requires type capabilities which are not directly expressible in Haskell.",
        "published": "2005-01-25T21:42:01Z",
        "link": "http://arxiv.org/abs/quant-ph/0501151v1",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "A Systematic Aspect-Oriented Refactoring and Testing Strategy, and its   Application to JHotDraw",
        "authors": [
            "Arie van Deursen",
            "Marius Marin",
            "Leon Moonen"
        ],
        "summary": "Aspect oriented programming aims at achieving better modularization for a system's crosscutting concerns in order to improve its key quality attributes, such as evolvability and reusability. Consequently, the adoption of aspect-oriented techniques in existing (legacy) software systems is of interest to remediate software aging. The refactoring of existing systems to employ aspect-orientation will be considerably eased by a systematic approach that will ensure a safe and consistent migration.   In this paper, we propose a refactoring and testing strategy that supports such an approach and consider issues of behavior conservation and (incremental) integration of the aspect-oriented solution with the original system. The strategy is applied to the JHotDraw open source project and illustrated on a group of selected concerns. Finally, we abstract from the case study and present a number of generic refactorings which contribute to an incremental aspect-oriented refactoring process and associate particular types of crosscutting concerns to the model and features of the employed aspect language. The contributions of this paper are both in the area of supporting migration towards aspect-oriented solutions and supporting the development of aspect languages that are better suited for such migrations.",
        "published": "2005-03-05T11:45:00Z",
        "link": "http://arxiv.org/abs/cs/0503015v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.7; D.2.5; D.1.5"
        ]
    },
    {
        "title": "Stabilization of Cooperative Information Agents in Unpredictable   Environment: A Logic Programming Approach",
        "authors": [
            "Phan Minh Dung",
            "Do Duc Hanh",
            "Phan Minh Thang"
        ],
        "summary": "An information agent is viewed as a deductive database consisting of 3 parts: an observation database containing the facts the agent has observed or sensed from its surrounding environment, an input database containing the information the agent has obtained from other agents, and an intensional database which is a set of rules for computing derived information from the information stored in the observation and input databases. Stabilization of a system of information agents represents a capability of the agents to eventually get correct information about their surrounding despite unpredictable environment changes and the incapability of many agents to sense such changes causing them to have temporary incorrect information. We argue that the stabilization of a system of cooperative information agents could be understood as the convergence of the behavior of the whole system toward the behavior of a \"superagent\", who has the sensing and computing capabilities of all agents combined. We show that unfortunately, stabilization is not guaranteed in general, even if the agents are fully cooperative and do not hide any information from each other. We give sufficient conditions for stabilization and discuss the consequences of our results.",
        "published": "2005-03-14T09:07:24Z",
        "link": "http://arxiv.org/abs/cs/0503028v2",
        "categories": [
            "cs.LO",
            "cs.MA",
            "cs.PL",
            "F.4.1; I.2.3; I.2.11"
        ]
    },
    {
        "title": "Optimality in Goal-Dependent Analysis of Sharing",
        "authors": [
            "Gianluca Amato",
            "Francesca Scozzari"
        ],
        "summary": "We face the problems of correctness, optimality and precision for the static analysis of logic programs, using the theory of abstract interpretation. We propose a framework with a denotational, goal-dependent semantics equipped with two unification operators for forward unification (calling a procedure) and backward unification (returning from a procedure). The latter is implemented through a matching operation. Our proposal clarifies and unifies many different frameworks and ideas on static analysis of logic programming in a single, formal setting. On the abstract side, we focus on the domain Sharing by Jacobs and Langen and provide the best correct approximation of all the primitive semantic operators, namely, projection, renaming, forward and backward unification. We show that the abstract unification operators are strictly more precise than those in the literature defined over the same abstract domain. In some cases, our operators are more precise than those developed for more complex domains involving linearity and freeness.   To appear in Theory and Practice of Logic Programming (TPLP)",
        "published": "2005-03-22T20:56:51Z",
        "link": "http://arxiv.org/abs/cs/0503055v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "F.3.2; D.1.6"
        ]
    },
    {
        "title": "Data-Structure Rewriting",
        "authors": [
            "Dominique Duval",
            "Rachid Echahed",
            "Frederic Prost"
        ],
        "summary": "We tackle the problem of data-structure rewriting including pointer redirections. We propose two basic rewrite steps: (i) Local Redirection and Replacement steps the aim of which is redirecting specific pointers determined by means of a pattern, as well as adding new information to an existing data ; and (ii) Global Redirection steps which are aimed to redirect all pointers targeting a node towards another one. We define these two rewriting steps following the double pushout approach. We define first the category of graphs we consider and then define rewrite rules as pairs of graph homomorphisms of the form \"L <- K ->R\". Unfortunately, inverse pushouts (complement pushouts) are not unique in our setting and pushouts do not always exist. Therefore, we define rewriting steps so that a rewrite rule can always be performed once a matching is found.",
        "published": "2005-03-24T09:55:42Z",
        "link": "http://arxiv.org/abs/cs/0503065v1",
        "categories": [
            "cs.PL",
            "cs.DS",
            "D1, D3, E1, F3.3, I1"
        ]
    },
    {
        "title": "Contextual equivalence for higher-order pi-calculus revisited",
        "authors": [
            "Alan Jeffrey",
            "Julian Rathke"
        ],
        "summary": "The higher-order pi-calculus is an extension of the pi-calculus to allow communication of abstractions of processes rather than names alone. It has been studied intensively by Sangiorgi in his thesis where a characterisation of a contextual equivalence for higher-order pi-calculus is provided using labelled transition systems and normal bisimulations. Unfortunately the proof technique used there requires a restriction of the language to only allow finite types. We revisit this calculus and offer an alternative presentation of the labelled transition system and a novel proof technique which allows us to provide a fully abstract characterisation of contextual equivalence using labelled transitions and bisimulations for higher-order pi-calculus with recursive types also.",
        "published": "2005-03-24T13:03:15Z",
        "link": "http://arxiv.org/abs/cs/0503067v5",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Super Object Oriented Programming",
        "authors": [
            "Raju Renjit. G"
        ],
        "summary": "This submission has been withdrawn at the request of the author.",
        "published": "2005-04-04T06:19:43Z",
        "link": "http://arxiv.org/abs/cs/0504008v10",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Incorporating LINQ, State Diagrams Templating and Package Extension Into   Java",
        "authors": [
            "Raju Renjit. G"
        ],
        "summary": "This submission has been withdrawn at the request of the author.",
        "published": "2005-04-07T13:21:15Z",
        "link": "http://arxiv.org/abs/cs/0504025v15",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Mapping Fusion and Synchronized Hyperedge Replacement into Logic   Programming",
        "authors": [
            "Ivan Lanese",
            "Ugo Montanari"
        ],
        "summary": "In this paper we compare three different formalisms that can be used in the area of models for distributed, concurrent and mobile systems. In particular we analyze the relationships between a process calculus, the Fusion Calculus, graph transformations in the Synchronized Hyperedge Replacement with Hoare synchronization (HSHR) approach and logic programming. We present a translation from Fusion Calculus into HSHR (whereas Fusion Calculus uses Milner synchronization) and prove a correspondence between the reduction semantics of Fusion Calculus and HSHR transitions. We also present a mapping from HSHR into a transactional version of logic programming and prove that there is a full correspondence between the two formalisms. The resulting mapping from Fusion Calculus to logic programming is interesting since it shows the tight analogies between the two formalisms, in particular for handling name generation and mobility. The intermediate step in terms of HSHR is convenient since graph transformations allow for multiple, remote synchronizations, as required by Fusion Calculus semantics.",
        "published": "2005-04-13T14:40:28Z",
        "link": "http://arxiv.org/abs/cs/0504050v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.1.1; F.4.1"
        ]
    },
    {
        "title": "A Scalable Stream-Oriented Framework for Cluster Applications",
        "authors": [
            "Tassos S. Argyros",
            "David R. Cheriton"
        ],
        "summary": "This paper presents a stream-oriented architecture for structuring cluster applications. Clusters that run applications based on this architecture can scale to tenths of thousands of nodes with significantly less performance loss or reliability problems. Our architecture exploits the stream nature of the data flow and reduces congestion through load balancing, hides latency behind data pushes and transparently handles node failures. In our ongoing work, we are developing an implementation for this architecture and we are able to run simple data mining applications on a cluster simulator.",
        "published": "2005-04-13T16:37:59Z",
        "link": "http://arxiv.org/abs/cs/0504051v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.NI",
            "cs.OS",
            "cs.PL"
        ]
    },
    {
        "title": "Pluggable AOP: Designing Aspect Mechanisms for Third-party Composition",
        "authors": [
            "Sergei Kojarski",
            "David H. Lorenz"
        ],
        "summary": "Studies of Aspect-Oriented Programming (AOP) usually focus on a language in which a specific aspect extension is integrated with a base language. Languages specified in this manner have a fixed, non-extensible AOP functionality. In this paper we consider the more general case of integrating a base language with a set of domain specific third-party aspect extensions for that language. We present a general mixin-based method for implementing aspect extensions in such a way that multiple, independently developed, dynamic aspect extensions can be subject to third-party composition and work collaboratively.",
        "published": "2005-04-30T20:49:15Z",
        "link": "http://arxiv.org/abs/cs/0505004v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.1.5; D.2.10; D.2.12; D.3.1; D.3.4"
        ]
    },
    {
        "title": "Improving PARMA Trailing",
        "authors": [
            "Tom Schrijvers",
            "Maria Garcia de la Banda",
            "Bart Demoen",
            "Peter J. Stuckey"
        ],
        "summary": "Taylor introduced a variable binding scheme for logic variables in his PARMA system, that uses cycles of bindings rather than the linear chains of bindings used in the standard WAM representation. Both the HAL and dProlog languages make use of the PARMA representation in their Herbrand constraint solvers. Unfortunately, PARMA's trailing scheme is considerably more expensive in both time and space consumption. The aim of this paper is to present several techniques that lower the cost.   First, we introduce a trailing analysis for HAL using the classic PARMA trailing scheme that detects and eliminates unnecessary trailings. The analysis, whose accuracy comes from HAL's determinism and mode declarations, has been integrated in the HAL compiler and is shown to produce space improvements as well as speed improvements. Second, we explain how to modify the classic PARMA trailing scheme to halve its trailing cost. This technique is illustrated and evaluated both in the context of dProlog and HAL. Finally, we explain the modifications needed by the trailing analysis in order to be combined with our modified PARMA trailing scheme. Empirical evidence shows that the combination is more effective than any of the techniques when used in isolation.   To appear in Theory and Practice of Logic Programming.",
        "published": "2005-05-31T08:23:32Z",
        "link": "http://arxiv.org/abs/cs/0505085v1",
        "categories": [
            "cs.PL",
            "cs.PF",
            "D.3.4; D.1.6; D.3.3"
        ]
    },
    {
        "title": "Programming Finite-Domain Constraint Propagators in Action Rules",
        "authors": [
            "Neng-Fa Zhou"
        ],
        "summary": "In this paper, we propose a new language, called AR ({\\it Action Rules}), and describe how various propagators for finite-domain constraints can be implemented in it. An action rule specifies a pattern for agents, an action that the agents can carry out, and an event pattern for events that can activate the agents. AR combines the goal-oriented execution model of logic programming with the event-driven execution model. This hybrid execution model facilitates programming constraint propagators. A propagator for a constraint is an agent that maintains the consistency of the constraint and is activated by the updates of the domain variables in the constraint. AR has a much stronger descriptive power than {\\it indexicals}, the language widely used in the current finite-domain constraint systems, and is flexible for implementing not only interval-consistency but also arc-consistency algorithms. As examples, we present a weak arc-consistency propagator for the {\\tt all\\_distinct} constraint and a hybrid algorithm for n-ary linear equality constraints. B-Prolog has been extended to accommodate action rules. Benchmarking shows that B-Prolog as a CLP(FD) system significantly outperforms other CLP(FD) systems.",
        "published": "2005-06-02T03:49:52Z",
        "link": "http://arxiv.org/abs/cs/0506005v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Fast Recompilation of Object Oriented Modules",
        "authors": [
            "Jerome Collin",
            "Michel Dagenais"
        ],
        "summary": "Once a program file is modified, the recompilation time should be minimized, without sacrificing execution speed or high level object oriented features. The recompilation time is often a problem for the large graphical interactive distributed applications tackled by modern OO languages. A compilation server and fast code generator were developed and integrated with the SRC Modula-3 compiler and Linux ELF dynamic linker. The resulting compilation and recompilation speedups are impressive. The impact of different language features, processor speed, and application size are discussed.",
        "published": "2005-06-10T15:28:00Z",
        "link": "http://arxiv.org/abs/cs/0506035v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Security Policies as Membranes in Systems for Global Computing",
        "authors": [
            "Daniele Gorla",
            "Matthew Hennessy",
            "Vladimiro Sassone"
        ],
        "summary": "We propose a simple global computing framework, whose main concern is code migration. Systems are structured in sites, and each site is divided into two parts: a computing body, and a membrane, which regulates the interactions between the computing body and the external environment. More precisely, membranes are filters which control access to the associated site, and they also rely on the well-established notion of trust between sites. We develop a basic theory to express and enforce security policies via membranes. Initially, these only control the actions incoming agents intend to perform locally. We then adapt the basic theory to encompass more sophisticated policies, where the number of actions an agent wants to perform, and also their order, are considered.",
        "published": "2005-06-14T18:08:10Z",
        "link": "http://arxiv.org/abs/cs/0506061v5",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.2.4; D.3.1; F.3.2; F.3.3; F.4.3"
        ]
    },
    {
        "title": "Improved Inference for Checking Annotations",
        "authors": [
            "Peter J Stuckey",
            "Martin Sulzmann",
            "Jeremy Wazny"
        ],
        "summary": "We consider type inference in the Hindley/Milner system extended with type annotations and constraints with a particular focus on Haskell-style type classes. We observe that standard inference algorithms are incomplete in the presence of nested type annotations. To improve the situation we introduce a novel inference scheme for checking type annotations. Our inference scheme is also incomplete in general but improves over existing implementations as found e.g. in the Glasgow Haskell Compiler (GHC). For certain cases (e.g. Haskell 98) our inference scheme is complete. Our approach has been fully implemented as part of the Chameleon system (experimental version of Haskell).",
        "published": "2005-07-14T08:47:45Z",
        "link": "http://arxiv.org/abs/cs/0507036v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Type Inference for Guarded Recursive Data Types",
        "authors": [
            "Peter J. Stuckey",
            "Martin Sulzmann"
        ],
        "summary": "We consider type inference for guarded recursive data types (GRDTs) -- a recent generalization of algebraic data types. We reduce type inference for GRDTs to unification under a mixed prefix. Thus, we obtain efficient type inference. Inference is incomplete because the set of type constraints allowed to appear in the type system is only a subset of those type constraints generated by type inference. Hence, inference only succeeds if the program is sufficiently type annotated. We present refined procedures to infer types incrementally and to assist the user in identifying which pieces of type information are missing. Additionally, we introduce procedures to test if a type is not principal and to find a principal type if one exists.",
        "published": "2005-07-14T08:58:31Z",
        "link": "http://arxiv.org/abs/cs/0507037v1",
        "categories": [
            "cs.PL",
            "cs.LO"
        ]
    },
    {
        "title": "Proof rules for purely quantum programs",
        "authors": [
            "Yuan Feng",
            "Runyao Duan",
            "Zhengfeng Ji",
            "Mingsheng Ying"
        ],
        "summary": "We apply the notion of quantum predicate proposed by D'Hondt and Panangaden to analyze a purely quantum language fragment which describes the quantum part of a future quantum computer in Knill's architecture. The denotational semantics, weakest precondition semantics, and weakest liberal precondition semantics of this language fragment are introduced. To help reasoning about quantum programs involving quantum loops, we extend proof rules for classical probabilistic programs to our purely quantum programs.",
        "published": "2005-07-18T13:06:13Z",
        "link": "http://arxiv.org/abs/cs/0507043v3",
        "categories": [
            "cs.PL",
            "quant-ph",
            "D.3.1; F.3.1"
        ]
    },
    {
        "title": "Software Libraries and Their Reuse: Entropy, Kolmogorov Complexity, and   Zipf's Law",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "We analyze software reuse from the perspective of information theory and Kolmogorov complexity, assessing our ability to ``compress'' programs by expressing them in terms of software components reused from libraries. A common theme in the software reuse literature is that if we can only get the right environment in place-- the right tools, the right generalizations, economic incentives, a ``culture of reuse'' -- then reuse of software will soar, with consequent improvements in productivity and software quality. The analysis developed in this paper paints a different picture: the extent to which software reuse can occur is an intrinsic property of a problem domain, and better tools and culture can have only marginal impact on reuse rates if the domain is inherently resistant to reuse. We define an entropy parameter $H \\in [0,1]$ of problem domains that measures program diversity, and deduce from this upper bounds on code reuse and the scale of components with which we may work. For ``low entropy'' domains with $H$ near 0, programs are highly similar to one another and the domain is amenable to the Component-Based Software Engineering (CBSE) dream of programming by composing large-scale components. For problem domains with $H$ near 1, programs require substantial quantities of new code, with only a modest proportion of an application comprised of reused, small-scale components. Preliminary empirical results from Unix platforms support some of the predictions of our model.",
        "published": "2005-08-03T04:20:11Z",
        "link": "http://arxiv.org/abs/cs/0508023v3",
        "categories": [
            "cs.SE",
            "cs.IT",
            "cs.PL",
            "math.IT",
            "D.2.8; E.4; D.2.13"
        ]
    },
    {
        "title": "An Operational Foundation for Delimited Continuations in the CPS   Hierarchy",
        "authors": [
            "Malgorzata Biernacka",
            "Dariusz Biernacki",
            "Olivier Danvy"
        ],
        "summary": "We present an abstract machine and a reduction semantics for the lambda-calculus extended with control operators that give access to delimited continuations in the CPS hierarchy. The abstract machine is derived from an evaluator in continuation-passing style (CPS); the reduction semantics (i.e., a small-step operational semantics with an explicit representation of evaluation contexts) is constructed from the abstract machine; and the control operators are the shift and reset family. We also present new applications of delimited continuations in the CPS hierarchy: finding list prefixes and normalization by evaluation for a hierarchical language of units and products.",
        "published": "2005-08-08T13:57:34Z",
        "link": "http://arxiv.org/abs/cs/0508048v4",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.1; F.3.2"
        ]
    },
    {
        "title": "A probabilistic branching bisimulation for quantum processes",
        "authors": [
            "Marie Lalire"
        ],
        "summary": "Full formal descriptions of algorithms making use of quantum principles must take into account both quantum and classical computing components and assemble them so that they communicate and cooperate.Moreover, to model concurrent and distributed quantum computations, as well as quantum communication protocols, quantum to quantum communications which move qubits physically from one place to another must also be taken into account.   Inspired by classical process algebras, which provide a framework for modeling cooperating computations, a process algebraic notation is defined, which provides a homogeneous style to formal descriptions of concurrent and distributed computations comprising both quantum and classical parts.Based upon an operational semantics which makes sure that quantum objects, operations and communications operate according to the postulates of quantum mechanics, a probabilistic branching bisimulation is defined among processes considered as having the same behavior.",
        "published": "2005-08-16T09:37:04Z",
        "link": "http://arxiv.org/abs/quant-ph/0508116v1",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "Proceedings of the 15th Workshop on Logic-based methods in Programming   Environments WLPE'05 -- October 5, 2005 -- Sitges (Barcelona), Spain",
        "authors": [
            "Alexander Serebrenik",
            "Susana Munoz-Hernandez"
        ],
        "summary": "This volume contains papers presented at WLPE 2005, 15th International Workshop on Logic-based methods in Programming Environments.   The aim of the workshop is to provide an informal meeting for the researchers working on logic-based tools for development and analysis of programs. This year we emphasized two aspects: on one hand the presentation, pragmatics and experiences of tools for logic programming environments; on the other one, logic-based environmental tools for programming in general.   The workshop took place in Sitges (Barcelona), Spain as a satellite workshop of the 21th International Conference on Logic Programming (ICLP 2005). This workshop continues the series of successful international workshops on logic programming environments held in Ohio, USA (1989), Eilat, Israel (1990), Paris, France (1991), Washington, USA (1992), Vancouver, Canada (1993), Santa Margherita Ligure, Italy (1994), Portland, USA (1995), Leuven, Belgium and Port Jefferson, USA (1997), Las Cruces, USA (1999), Paphos, Cyprus (2001), Copenhagen, Denmark (2002), Mumbai, India (2003) and Saint Malo, France (2004).   We have received eight submissions (2 from France, 2 Spain-US cooperations, one Spain-Argentina cooperation, one from Japan, one from the United Kingdom and one Sweden-France cooperation). Program committee has decided to accept seven papers. This volume contains revised versions of the accepted papers.   We are grateful to the authors of the papers, the reviewers and the members of the Program Committee for the help and fruitful discussions.",
        "published": "2005-08-17T13:35:20Z",
        "link": "http://arxiv.org/abs/cs/0508078v4",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Extending Prolog with Incomplete Fuzzy Information",
        "authors": [
            "Susana Munoz-Hernandez",
            "Claudio Vaucheret"
        ],
        "summary": "Incomplete information is a problem in many aspects of actual environments. Furthermore, in many sceneries the knowledge is not represented in a crisp way. It is common to find fuzzy concepts or problems with some level of uncertainty. There are not many practical systems which handle fuzziness and uncertainty and the few examples that we can find are used by a minority. To extend a popular system (which many programmers are using) with the ability of combining crisp and fuzzy knowledge representations seems to be an interesting issue.   Our first work (Fuzzy Prolog) was a language that models $\\mathcal{B}([0,1])$-valued Fuzzy Logic. In the Borel algebra, $\\mathcal{B}([0,1])$, truth value is represented using unions of intervals of real numbers. This work was more general in truth value representation and propagation than previous works.   An interpreter for this language using Constraint Logic Programming over Real numbers (CLP(${\\cal R}$)) was implemented and is available in the Ciao system.   Now, we enhance our former approach by using default knowledge to represent incomplete information in Logic Programming. We also provide the implementation of this new framework. This new release of Fuzzy Prolog handles incomplete information, it has a complete semantics (the previous one was incomplete as Prolog) and moreover it is able to combine crisp and fuzzy logic in Prolog programs. Therefore, new Fuzzy Prolog is more expressive to represent real world.   Fuzzy Prolog inherited from Prolog its incompleteness. The incorporation of default reasoning to Fuzzy Prolog removes this problem and requires a richer semantics which we discuss.",
        "published": "2005-08-22T07:41:41Z",
        "link": "http://arxiv.org/abs/cs/0508091v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6"
        ]
    },
    {
        "title": "An Improved Non-Termination Criterion for Binary Constraint Logic   Programs",
        "authors": [
            "Etienne Payet",
            "Fred Mesnard"
        ],
        "summary": "On one hand, termination analysis of logic programs is now a fairly established research topic within the logic programming community. On the other hand, non-termination analysis seems to remain a much less attractive subject. If we divide this line of research into two kinds of approaches: dynamic versus static analysis, this paper belongs to the latter. It proposes a criterion for detecting non-terminating atomic queries with respect to binary CLP clauses, which strictly generalizes our previous works on this subject. We give a generic operational definition and a logical form of this criterion. Then we show that the logical form is correct and complete with respect to the operational definition.",
        "published": "2005-08-24T12:18:11Z",
        "link": "http://arxiv.org/abs/cs/0508106v1",
        "categories": [
            "cs.PL",
            "D.2.6"
        ]
    },
    {
        "title": "Proving or Disproving likely Invariants with Constraint Reasoning",
        "authors": [
            "Tristan Denmat",
            "Arnaud Gotlieb",
            "Mireille Ducasse"
        ],
        "summary": "A program invariant is a property that holds for every execution of the program. Recent work suggest to infer likely-only invariants, via dynamic analysis. A likely invariant is a property that holds for some executions but is not guaranteed to hold for all executions. In this paper, we present work in progress addressing the challenging problem of automatically verifying that likely invariants are actual invariants. We propose a constraint-based reasoning approach that is able, unlike other approaches, to both prove or disprove likely invariants. In the latter case, our approach provides counter-examples. We illustrate the approach on a motivating example where automatically generated likely invariants are verified.",
        "published": "2005-08-24T13:30:59Z",
        "link": "http://arxiv.org/abs/cs/0508108v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.6"
        ]
    },
    {
        "title": "A Generic Framework for the Analysis and Specialization of Logic   Programs",
        "authors": [
            "German Puebla",
            "Elvira Albert",
            "Manuel Hermenegildo"
        ],
        "summary": "The relationship between abstract interpretation and partial deduction has received considerable attention and (partial) integrations have been proposed starting from both the partial deduction and abstract interpretation perspectives. In this work we present what we argue is the first fully described generic algorithm for efficient and precise integration of abstract interpretation and partial deduction. Taking as starting point state-of-the-art algorithms for context-sensitive, polyvariant abstract interpretation and (abstract) partial deduction, we present an algorithm which combines the best of both worlds. Key ingredients include the accurate success propagation inherent to abstract interpretation and the powerful program transformations achievable by partial deduction. In our algorithm, the calls which appear in the analysis graph are not analyzed w.r.t. the original definition of the procedure but w.r.t. specialized definitions of these procedures. Such specialized definitions are obtained by applying both unfolding and abstract executability. Our framework is parametric w.r.t. different control strategies and abstract domains. Different combinations of such parameters correspond to existing algorithms for program analysis and specialization. Simultaneously, our approach opens the door to the efficient computation of strictly more precise results than those achievable by each of the individual techniques. The algorithm is now one of the key components of the CiaoPP analysis and specialization system.",
        "published": "2005-08-24T21:50:59Z",
        "link": "http://arxiv.org/abs/cs/0508111v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6"
        ]
    },
    {
        "title": "On Algorithms and Complexity for Sets with Cardinality Constraints",
        "authors": [
            "Bruno Marnette",
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "Typestate systems ensure many desirable properties of imperative programs, including initialization of object fields and correct use of stateful library interfaces. Abstract sets with cardinality constraints naturally generalize typestate properties: relationships between the typestates of objects can be expressed as subset and disjointness relations on sets, and elements of sets can be represented as sets of cardinality one. Motivated by these applications, this paper presents new algorithms and new complexity results for constraints on sets and their cardinalities. We study several classes of constraints and demonstrate a trade-off between their expressive power and their complexity.   Our first result concerns a quantifier-free fragment of Boolean Algebra with Presburger Arithmetic. We give a nondeterministic polynomial-time algorithm for reducing the satisfiability of sets with symbolic cardinalities to constraints on constant cardinalities, and give a polynomial-space algorithm for the resulting problem.   In a quest for more efficient fragments, we identify several subclasses of sets with cardinality constraints whose satisfiability is NP-hard. Finally, we identify a class of constraints that has polynomial-time satisfiability and entailment problems and can serve as a foundation for efficient program analysis.",
        "published": "2005-08-28T22:25:22Z",
        "link": "http://arxiv.org/abs/cs/0508123v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "Temporal Phylogenetic Networks and Logic Programming",
        "authors": [
            "Esra Erdem",
            "Vladimir Lifschitz",
            "Don Ringe"
        ],
        "summary": "The concept of a temporal phylogenetic network is a mathematical model of evolution of a family of natural languages. It takes into account the fact that languages can trade their characteristics with each other when linguistic communities are in contact, and also that a contact is only possible when the languages are spoken at the same time. We show how computational methods of answer set programming and constraint logic programming can be used to generate plausible conjectures about contacts between prehistoric linguistic communities, and illustrate our approach by applying it to the evolutionary history of Indo-European languages.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2005-08-30T13:04:05Z",
        "link": "http://arxiv.org/abs/cs/0508129v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.PL"
        ]
    },
    {
        "title": "Haskell's overlooked object system",
        "authors": [
            "Oleg Kiselyov",
            "Ralf Laemmel"
        ],
        "summary": "Haskell provides type-class-bounded and parametric polymorphism as opposed to subtype polymorphism of object-oriented languages such as Java and OCaml. It is a contentious question whether Haskell 98 without extensions, or with common extensions, or with new extensions can fully support conventional object-oriented programming with encapsulation, mutable state, inheritance, overriding, statically checked implicit and explicit subtyping, and so on. We systematically substantiate that Haskell 98, with some common extensions, supports all the conventional OO features plus more advanced ones, including first-class lexically scoped classes, implicitly polymorphic classes, flexible multiple inheritance, safe downcasts and safe co-variant arguments. Haskell indeed can support width and depth, structural and nominal subtyping. We address the particular challenge to preserve Haskell's type inference even for objects and object-operating functions. The OO features are introduced in Haskell as the OOHaskell library. OOHaskell lends itself as a sandbox for typed OO language design.",
        "published": "2005-09-10T12:35:20Z",
        "link": "http://arxiv.org/abs/cs/0509027v1",
        "categories": [
            "cs.PL",
            "D.1.5; D.1.1; D.2.3; D.3.3"
        ]
    },
    {
        "title": "Language embeddings that preserve staging and safety",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "We study embeddings of programming languages into one another that preserve what reductions take place at compile-time, i.e., staging. A certain condition -- what we call a `Turing complete kernel' -- is sufficient for a language to be stage-universal in the sense that any language may be embedded in it while preserving staging. A similar line of reasoning yields the notion of safety-preserving embeddings, and a useful characterization of safety-universality. Languages universal with respect to staging and safety are good candidates for realizing domain-specific embedded languages (DSELs) and `active libraries' that provide domain-specific optimizations and safety checks.",
        "published": "2005-09-19T15:30:10Z",
        "link": "http://arxiv.org/abs/cs/0509057v1",
        "categories": [
            "cs.PL",
            "D.3.4"
        ]
    },
    {
        "title": "Semantics of UML 2.0 Activity Diagram for Business Modeling by Means of   Virtual Machine",
        "authors": [
            "Valdis Vitolins",
            "Audris Kalnins"
        ],
        "summary": "The paper proposes a more formalized definition of UML 2.0 Activity Diagram semantics. A subset of activity diagram constructs relevant for business process modeling is considered. The semantics definition is based on the original token flow methodology, but a more constructive approach is used. The Activity Diagram Virtual machine is defined by means of a metamodel, with operations defined by a mix of pseudocode and OCL pre- and postconditions. A formal procedure is described which builds the virtual machine for any activity diagram. The relatively complicated original token movement rules in control nodes and edges are combined into paths from an action to action. A new approach is the use of different (push and pull) engines, which move tokens along the paths. Pull engines are used for paths containing join nodes, where the movement of several tokens must be coordinated. The proposed virtual machine approach makes the activity semantics definition more transparent where the token movement can be easily traced. However, the main benefit of the approach is the possibility to use the defined virtual machine as a basis for UML activity diagram based workflow or simulation engine.",
        "published": "2005-09-28T12:24:42Z",
        "link": "http://arxiv.org/abs/cs/0509089v1",
        "categories": [
            "cs.CE",
            "cs.PL"
        ]
    },
    {
        "title": "Practical Datatype Specializations with Phantom Types and Recursion   Schemes",
        "authors": [
            "Matthew Fluet",
            "Riccardo Pucella"
        ],
        "summary": "Datatype specialization is a form of subtyping that captures program invariants on data structures that are expressed using the convenient and intuitive datatype notation. Of particular interest are structural invariants such as well-formedness. We investigate the use of phantom types for describing datatype specializations. We show that it is possible to express statically-checked specializations within the type system of Standard ML. We also show that this can be done in a way that does not lose useful programming facilities such as pattern matching in case expressions.",
        "published": "2005-10-24T16:27:00Z",
        "link": "http://arxiv.org/abs/cs/0510074v1",
        "categories": [
            "cs.PL",
            "D.1.1; D.3.3; F.3.3"
        ]
    },
    {
        "title": "Semantics and simulation of communication in quantum programming",
        "authors": [
            "Wolfgang Mauerer"
        ],
        "summary": "We present the quantum programming language cQPL which is an extended version of QPL [P. Selinger, Math. Struct. in Comp. Sci. 14(4):527-586, 2004]. It is capable of quantum communication and it can be used to formulate all possible quantum algorithms. Additionally, it possesses a denotational semantics based on a partial order of superoperators and uses fixed points on a generalised Hilbert space to formalise (in addition to all standard features expected from a quantum programming language) the exchange of classical and quantum data between an arbitrary number of participants. Additionally, we present the implementation of a cQPL compiler which generates code for a quantum simulator.",
        "published": "2005-11-15T18:05:35Z",
        "link": "http://arxiv.org/abs/quant-ph/0511145v1",
        "categories": [
            "quant-ph",
            "cs.PL"
        ]
    },
    {
        "title": "Integration of Declarative and Constraint Programming",
        "authors": [
            "Petra Hofstedt",
            "Peter Pepper"
        ],
        "summary": "Combining a set of existing constraint solvers into an integrated system of cooperating solvers is a useful and economic principle to solve hybrid constraint problems. In this paper we show that this approach can also be used to integrate different language paradigms into a unified framework. Furthermore, we study the syntactic, semantic and operational impacts of this idea for the amalgamation of declarative and constraint programming.",
        "published": "2005-11-27T14:58:22Z",
        "link": "http://arxiv.org/abs/cs/0511090v2",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2"
        ]
    },
    {
        "title": "The SL synchronous language, revisited",
        "authors": [
            "Roberto Amadio"
        ],
        "summary": "We revisit the SL synchronous programming model introduced by Boussinot and De Simone (IEEE, Trans. on Soft. Eng., 1996). We discuss an alternative design of the model including thread spawning and recursive definitions and we explore some basic properties of the revised model: determinism, reactivity, CPS translation to a tail recursive form, computational expressivity, and a compositional notion of program equivalence.",
        "published": "2005-11-28T07:20:43Z",
        "link": "http://arxiv.org/abs/cs/0511092v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "A Machine-Independent port of the MPD language run time system to NetBSD",
        "authors": [
            "Ignatios Souvatzis"
        ],
        "summary": "SR (synchronizing resources) is a PASCAL - style language enhanced with constructs for concurrent programming developed at the University of Arizona in the late 1980s. MPD (presented in Gregory Andrews' book about Foundations of Multithreaded, Parallel, and Distributed Programming) is its successor, providing the same language primitives with a different, more C-style, syntax.   The run-time system (in theory, identical, but not designed for sharing) of those languages provides the illusion of a multiprocessor machine on a single Unix-like system or a (local area) network of Unix-like machines.   Chair V of the Computer Science Department of the University of Bonn is operating a laboratory for a practical course in parallel programming consisting of computing nodes running NetBSD/arm, normally used via PVM, MPI etc.   We are considering to offer SR and MPD for this, too. As the original language distributions were only targeted at a few commercial Unix systems, some porting effort is needed. However, some of the porting effort of our earlier SR port should be reusable.   The integrated POSIX threads support of NetBSD-2.0 and later allows us to use library primitives provided for NetBSD's phtread system to implement the primitives needed by the SR run-time system, thus implementing 13 target CPUs at once and automatically making use of SMP on VAX, Alpha, PowerPC, Sparc, 32-bit Intel and 64 bit AMD CPUs.   We'll present some methods used for the impementation and compare some performance values to the traditional implementation.",
        "published": "2005-11-28T13:33:14Z",
        "link": "http://arxiv.org/abs/cs/0511094v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.3.4"
        ]
    },
    {
        "title": "Checking C++ Programs for Dimensional Consistency",
        "authors": [
            "I. Josopait"
        ],
        "summary": "I will present my implementation 'n-units' of physical units into C++ programs. It allows the compiler to check for dimensional consistency.",
        "published": "2005-12-07T15:51:26Z",
        "link": "http://arxiv.org/abs/cs/0512026v1",
        "categories": [
            "cs.PL",
            "D.1.2; I.2.2"
        ]
    },
    {
        "title": "Termination Analysis of General Logic Programs for Moded Queries: A   Dynamic Approach",
        "authors": [
            "Yi-Dong Shen",
            "Danny De Schreye"
        ],
        "summary": "The termination problem of a logic program can be addressed in either a static or a dynamic way. A static approach performs termination analysis at compile time, while a dynamic approach characterizes and tests termination of a logic program by applying a loop checking technique. In this paper, we present a novel dynamic approach to termination analysis for general logic programs with moded queries. We address several interesting questions, including how to formulate an SLDNF-derivation for a moded query, how to characterize an infinite SLDNF-derivation with a moded query, and how to apply a loop checking mechanism to cut infinite SLDNF-derivations for the purpose of termination analysis. The proposed approach is very powerful and useful. It can be used (1) to test if a logic program terminates for a given concrete or moded query, (2) to test if a logic program terminates for all concrete or moded queries, and (3) to find all (most general) concrete/moded queries that are most likely terminating (or non-terminating).",
        "published": "2005-12-14T07:09:50Z",
        "link": "http://arxiv.org/abs/cs/0512055v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6"
        ]
    },
    {
        "title": "Resource Control for Synchronous Cooperative Threads",
        "authors": [
            "Roberto Amadio",
            "Silvano Dal Zilio"
        ],
        "summary": "We develop new methods to statically bound the resources needed for the execution of systems of concurrent, interactive threads. Our study is concerned with a \\emph{synchronous} model of interaction based on cooperative threads whose execution proceeds in synchronous rounds called instants. Our contribution is a system of compositional static analyses to guarantee that each instant terminates and to bound the size of the values computed by the system as a function of the size of its parameters at the beginning of the instant. Our method generalises an approach designed for first-order functional languages that relies on a combination of standard termination techniques for term rewriting systems and an analysis of the size of the computed values based on the notion of quasi-interpretation. We show that these two methods can be combined to obtain an explicit polynomial bound on the resources needed for the execution of the system during an instant. As a second contribution, we introduce a virtual machine and a related bytecode thus producing a precise description of the resources needed for the execution of a system. In this context, we present a suitable control flow analysis that allows to formulte the static analyses for resource control at byte code level.",
        "published": "2005-12-14T13:41:07Z",
        "link": "http://arxiv.org/abs/cs/0512057v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Reactive concurrent programming revisited",
        "authors": [
            "Roberto Amadio",
            "Gerard Boudol",
            "Ilaria Castellani",
            "Frederic Boussinot"
        ],
        "summary": "In this note we revisit the so-called reactive programming style, which evolves from the synchronous programming model of the Esterel language by weakening the assumption that the absence of an event can be detected instantaneously. We review some research directions that have been explored since the emergence of the reactive model ten years ago. We shall also outline some questions that remain to be investigated.",
        "published": "2005-12-14T13:42:17Z",
        "link": "http://arxiv.org/abs/cs/0512058v1",
        "categories": [
            "cs.PL"
        ]
    },
    {
        "title": "Tradeoffs in Metaprogramming",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "The design of metaprogramming languages requires appreciation of the tradeoffs that exist between important language characteristics such as safety properties, expressive power, and succinctness. Unfortunately, such tradeoffs are little understood, a situation we try to correct by embarking on a study of metaprogramming language tradeoffs using tools from computability theory. Safety properties of metaprograms are in general undecidable; for example, the property that a metaprogram always halts and produces a type-correct instance is $\\Pi^0_2$-complete. Although such safety properties are undecidable, they may sometimes be captured by a restricted language, a notion we adapt from complexity theory. We give some sufficient conditions and negative results on when languages capturing properties can exist: there can be no languages capturing total correctness for metaprograms, and no `functional' safety properties above $\\Sigma^0_3$ can be captured. We prove that translating a metaprogram from a general-purpose to a restricted metaprogramming language capturing a property is tantamount to proving that property for the metaprogram. Surprisingly, when one shifts perspective from programming to metaprogramming, the corresponding safety questions do not become substantially harder -- there is no `jump' of Turing degree for typical safety properties.",
        "published": "2005-12-15T21:22:23Z",
        "link": "http://arxiv.org/abs/cs/0512065v1",
        "categories": [
            "cs.PL",
            "D.3.1"
        ]
    },
    {
        "title": "Solving Partial Order Constraints for LPO Termination",
        "authors": [
            "Michael Codish",
            "Vitaly Lagoon",
            "Peter J. Stuckey"
        ],
        "summary": "This paper introduces a new kind of propositional encoding for reasoning about partial orders. The symbols in an unspecified partial order are viewed as variables which take integer values and are interpreted as indices in the order. For a partial order statement on n symbols each index is represented in log2 n propositional variables and partial order constraints between symbols are modeled on the bit representations. We illustrate the application of our approach to determine LPO termination for term rewrite systems. Experimental results are unequivocal, indicating orders of magnitude speedups in comparison with current implementations for LPO termination. The proposed encoding is general and relevant to other applications which involve propositional reasoning about partial orders.",
        "published": "2005-12-16T01:28:25Z",
        "link": "http://arxiv.org/abs/cs/0512067v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SC",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Book review \"The Haskell Road to Logic, Maths and Programming\"",
        "authors": [
            "Ralf Laemmel"
        ],
        "summary": "The textbook by Doets and van Eijck puts the Haskell programming language systematically to work for presenting a major piece of logic and mathematics. The reader is taken through chapters on basic logic, proof recipes, sets and lists, relations and functions, recursion and co-recursion, the number systems, polynomials and power series, ending with Cantor's infinities. The book uses Haskell for the executable and strongly typed manifestation of various mathematical notions at the level of declarative programming. The book adopts a systematic but relaxed mathematical style (definition, example, exercise, ...); the text is very pleasant to read due to a small amount of anecdotal information, and due to the fact that definitions are fluently integrated in the running text. An important goal of the book is to get the reader acquainted with reasoning about programs.",
        "published": "2005-12-24T23:08:25Z",
        "link": "http://arxiv.org/abs/cs/0512096v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.1; F.3.1; G.0"
        ]
    },
    {
        "title": "Implementation of Motzkin-Burger algorithm in Maple",
        "authors": [
            "P. A. Burovsky"
        ],
        "summary": "Subject of this paper is an implementation of a well-known Motzkin-Burger algorithm, which solves the problem of finding the full set of solutions of a system of linear homogeneous inequalities. There exist a number of implementations of this algorithm, but there was no one in Maple, to the best of the author's knowledge.",
        "published": "2005-01-01T04:55:40Z",
        "link": "http://arxiv.org/abs/cs/0501003v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "cs.SC",
            "F.2.2; I.3.5; G.1.6"
        ]
    },
    {
        "title": "A Time-Optimal Delaunay Refinement Algorithm in Two Dimensions",
        "authors": [
            "Sariel Har-Peled",
            "Alper Ungor"
        ],
        "summary": "We propose a new refinement algorithm to generate size-optimal quality-guaranteed Delaunay triangulations in the plane. The algorithm takes $O(n \\log n + m)$ time, where $n$ is the input size and $m$ is the output size. This is the first time-optimal Delaunay refinement algorithm.",
        "published": "2005-01-04T22:30:04Z",
        "link": "http://arxiv.org/abs/cs/0501007v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Improved Approximation Algorithms for Geometric Set Cover",
        "authors": [
            "Kenneth L. Clarkson",
            "Kasturi Varadarajan"
        ],
        "summary": "Given a collection S of subsets of some set U, and M a subset of U, the set cover problem is to find the smallest subcollection C of S such that M is a subset of the union of the sets in C. While the general problem is NP-hard to solve, even approximately, here we consider some geometric special cases, where usually U = R^d. Extending prior results, we show that approximation algorithms with provable performance exist, under a certain general condition: that for a random subset R of S and function f(), there is a decomposition of the portion of U not covered by R into an expected f(|R|) regions, each region of a particular simple form. We show that under this condition, a cover of size O(f(|C|)) can be found. Our proof involves the generalization of shallow cuttings to more general geometric situations. We obtain constant-factor approximation algorithms for covering by unit cubes in R^3, for guarding a one-dimensional terrain, and for covering by similar-sized fat triangles in R^2. We also obtain improved approximation guarantees for fat triangles, of arbitrary size, and for a class of fat objects.",
        "published": "2005-01-20T21:31:22Z",
        "link": "http://arxiv.org/abs/cs/0501045v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Compatible Triangulations and Point Partitions by Series-Triangular   Graphs",
        "authors": [
            "Jeff Danciger",
            "Satyan L. Devadoss",
            "Don Sheehy"
        ],
        "summary": "We introduce series-triangular graph embeddings and show how to partition point sets with them. This result is then used to improve the upper bound on the number of Steiner points needed to obtain compatible triangulations of point sets. The problem is generalized to finding compatible triangulations for more than two point sets and we show that such triangulations can be constructed with only a linear number of Steiner points added to each point set.",
        "published": "2005-02-08T16:19:46Z",
        "link": "http://arxiv.org/abs/cs/0502043v1",
        "categories": [
            "cs.CG",
            "cs.DM"
        ]
    },
    {
        "title": "The Weighted Maximum-Mean Subtree and Other Bicriterion Subtree Problems",
        "authors": [
            "Josiah Carlson",
            "David Eppstein"
        ],
        "summary": "We consider problems in which we are given a rooted tree as input, and must find a subtree with the same root, optimizing some objective function of the nodes in the subtree. When this function is the sum of constant node weights, the problem is trivially solved in linear time. When the objective is the sum of weights that are linear functions of a parameter, we show how to list all optima for all possible parameter values in O(n log n) time; this parametric optimization problem can be used to solve many bicriterion optimizations problems, in which each node has two values xi and yi associated with it, and the objective function is a bivariate function f(SUM(xi),SUM(yi)) of the sums of these two values. A special case, when f is the ratio of the two sums, is the Weighted Maximum-Mean Subtree Problem, or equivalently the Fractional Prize-Collecting Steiner Tree Problem on Trees; for this special case, we provide a linear time algorithm for this problem when all weights are positive, improving a previous O(n log n) solution, and prove that the problem is NP-complete when negative weights are allowed.",
        "published": "2005-03-09T18:16:14Z",
        "link": "http://arxiv.org/abs/cs/0503023v4",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Point set stratification and Delaunay depth",
        "authors": [
            "Manuel Abellanas",
            "Mercè Claverol",
            "Ferran Hurtado"
        ],
        "summary": "In the study of depth functions it is important to decide whether we want such a function to be sensitive to multimodality or not. In this paper we analyze the Delaunay depth function, which is sensitive to multimodality and compare this depth with others, as convex depth and location depth. We study the stratification that Delaunay depth induces in the point set (layers) and in the whole plane (levels), and we develop an algorithm for computing the Delaunay depth contours, associated to a point set in the plane, with running time O(n log^2 n). The depth of a query point p with respect to a data set S in the plane is the depth of p in the union of S and p. When S and p are given in the input the Delaunay depth can be computed in O(n log n), and we prove that this value is optimal.",
        "published": "2005-05-08T16:36:42Z",
        "link": "http://arxiv.org/abs/cs/0505017v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Estimacao Temporal da Deformacao entre Objectos utilizando uma   Metodologia Fisica",
        "authors": [
            "Joao Manuel R. S. Tavares",
            "Raquel R. Pinho"
        ],
        "summary": "In this paper, it is presented a methodology to estimate the deformation involved between two objects attending to its physical properties. This methodology can be used, for example, in Computational Vision or Computer Graphics applications, and consists in physically modeling the objects, by means of the Finite Elements Method, establishing correspondences between some of its data points, by using Modal Matching, and finally, determining the displacement field, that is the intermediate shapes, through the resolution of the Lagrange Dynamic Equilibrium Equation. As in many of the possible applications of the methodology to present, it is necessary to quantify the existing deformation, as well as to estimate only the non rigid component of the involved global deformation. The solutions adopted to satisfy such intentions will be also presented.",
        "published": "2005-05-16T02:20:19Z",
        "link": "http://arxiv.org/abs/cs/0505043v2",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "A Simple Proof of the F{á}ry-Wagner Theorem",
        "authors": [
            "David R. Wood"
        ],
        "summary": "We give a simple proof of the following fundamental result independently due to Fary (1948) and Wagner (1936): Every plane graph has a drawing in which every edge is straight.",
        "published": "2005-05-18T11:27:59Z",
        "link": "http://arxiv.org/abs/cs/0505047v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "An explicit formula for the number of tunnels in digital objects",
        "authors": [
            "Valentin Brimkov",
            "Angelo Maimone",
            "Giorgio Nordo"
        ],
        "summary": "An important concept in digital geometry for computer imagery is that of tunnel. In this paper we obtain a formula for the number of tunnels as a function of the number of the object vertices, pixels, holes, connected components, and 2x2 grid squares. It can be used to test for tunnel-freedom a digital object, in particular a digital curve.",
        "published": "2005-05-31T00:44:16Z",
        "link": "http://arxiv.org/abs/cs/0505084v2",
        "categories": [
            "cs.DM",
            "cs.CG",
            "cs.CV",
            "G.2.1; F.2.2; I.4.6; I.5.1"
        ]
    },
    {
        "title": "An Efficient Approximation Algorithm for Point Pattern Matching Under   Noise",
        "authors": [
            "Vicky Choi",
            "Navin Goyal"
        ],
        "summary": "Point pattern matching problems are of fundamental importance in various areas including computer vision and structural bioinformatics. In this paper, we study one of the more general problems, known as LCP (largest common point set problem): Let $\\PP$ and $\\QQ$ be two point sets in $\\mathbb{R}^3$, and let $\\epsilon \\geq 0$ be a tolerance parameter, the problem is to find a rigid motion $\\mu$ that maximizes the cardinality of subset $\\II$ of $Q$, such that the Hausdorff distance $\\distance(\\PP,\\mu(\\II)) \\leq \\epsilon$. We denote the size of the optimal solution to the above problem by $\\LCP(P,Q)$. The problem is called exact-LCP for $\\epsilon=0$, and \\tolerant-LCP when $\\epsilon>0$ and the minimum interpoint distance is greater than $2\\epsilon$. A $\\beta$-distance-approximation algorithm for tolerant-LCP finds a subset $I \\subseteq \\QQ$ such that $|I|\\geq \\LCP(P,Q)$ and $\\distance(\\PP,\\mu(\\II)) \\leq \\beta \\epsilon$ for some $\\beta \\ge 1$.   This paper has three main contributions. (1) We introduce a new algorithm, called {\\DA}, which gives the fastest known deterministic 4-distance-approximation algorithm for \\tolerant-LCP. (2) For the exact-LCP, when the matched set is required to be large, we give a simple sampling strategy that improves the running times of all known deterministic algorithms, yielding the fastest known deterministic algorithm for this problem. (3) We use expander graphs to speed-up the \\DA algorithm for \\tolerant-LCP when the size of the matched set is required to be large, at the expense of approximation in the matched set size. Our algorithms also work when the transformation $\\mu$ is allowed to be scaling transformation.",
        "published": "2005-06-07T17:41:53Z",
        "link": "http://arxiv.org/abs/cs/0506019v4",
        "categories": [
            "cs.CV",
            "cs.CG"
        ]
    },
    {
        "title": "An O(n log n)-Time Algorithm for the Restricted Scaffold Assignment",
        "authors": [
            "Justin Colannino",
            "Mirela Damian",
            "Ferran Hurtado",
            "John Iacono",
            "Henk Meijer",
            "Suneeta Ramaswami",
            "Godfried Toussaint"
        ],
        "summary": "The assignment problem takes as input two finite point sets S and T and establishes a correspondence between points in S and points in T, such that each point in S maps to exactly one point in T, and each point in T maps to at least one point in S. In this paper we show that this problem has an O(n log n)-time solution, provided that the points in S and T are restricted to lie on a line (linear time, if S and T are presorted).",
        "published": "2005-07-05T19:50:11Z",
        "link": "http://arxiv.org/abs/cs/0507013v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Strictly convex drawings of planar graphs",
        "authors": [
            "Imre Barany",
            "Guenter Rote"
        ],
        "summary": "Every three-connected planar graph with n vertices has a drawing on an O(n^2) x O(n^2) grid in which all faces are strictly convex polygons. These drawings are obtained by perturbing (not strictly) convex drawings on O(n) x O(n) grids. More generally, a strictly convex drawing exists on a grid of size O(W) x O(n^4/W), for any choice of a parameter W in the range n<W<n^2. Tighter bounds are obtained when the faces have fewer sides.   In the proof, we derive an explicit lower bound on the number of primitive vectors in a triangle.",
        "published": "2005-07-11T20:06:06Z",
        "link": "http://arxiv.org/abs/cs/0507030v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Upper Bound on the Number of Vertices of Polyhedra with $0,1$-Constraint   Matrices",
        "authors": [
            "Khaled Elbassioni",
            "Zvi Lotker",
            "Raimund Seidel"
        ],
        "summary": "In this note we show that the maximum number of vertices in any polyhedron $P=\\{x\\in \\mathbb{R}^d : Ax\\leq b\\}$ with $0,1$-constraint matrix $A$ and a real vector $b$ is at most $d!$.",
        "published": "2005-07-14T18:45:36Z",
        "link": "http://arxiv.org/abs/cs/0507038v1",
        "categories": [
            "cs.CG",
            "G.1.6"
        ]
    },
    {
        "title": "The Skip Quadtree: A Simple Dynamic Data Structure for Multidimensional   Data",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Jonathan Z. Sun"
        ],
        "summary": "We present a new multi-dimensional data structure, which we call the skip quadtree (for point data in R^2) or the skip octree (for point data in R^d, with constant d>2). Our data structure combines the best features of two well-known data structures, in that it has the well-defined \"box\"-shaped regions of region quadtrees and the logarithmic-height search and update hierarchical structure of skip lists. Indeed, the bottom level of our structure is exactly a region quadtree (or octree for higher dimensional data). We describe efficient algorithms for inserting and deleting points in a skip quadtree, as well as fast methods for performing point location and approximate range queries.",
        "published": "2005-07-19T20:17:12Z",
        "link": "http://arxiv.org/abs/cs/0507049v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Skip-Webs: Efficient Distributed Data Structures for Multi-Dimensional   Data Sets",
        "authors": [
            "Lars Arge",
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "We present a framework for designing efficient distributed data structures for multi-dimensional data. Our structures, which we call skip-webs, extend and improve previous randomized distributed data structures, including skipnets and skip graphs. Our framework applies to a general class of data querying scenarios, which include linear (one-dimensional) data, such as sorted sets, as well as multi-dimensional data, such as d-dimensional octrees and digital tries of character strings defined over a fixed alphabet. We show how to perform a query over such a set of n items spread among n hosts using O(log n / log log n) messages for one-dimensional data, or O(log n) messages for fixed-dimensional data, while using only O(log n) space per host. We also show how to make such structures dynamic so as to allow for insertions and deletions in O(log n) messages for quadtrees, octrees, and digital tries, and O(log n / log log n) messages for one-dimensional data. Finally, we show how to apply a blocking strategy to skip-webs to further improve message complexity for one-dimensional data when hosts can store more data.",
        "published": "2005-07-19T20:30:33Z",
        "link": "http://arxiv.org/abs/cs/0507050v1",
        "categories": [
            "cs.DC",
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Confluent Layered Drawings",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Jeremy Yu Meng"
        ],
        "summary": "We combine the idea of confluent drawings with Sugiyama style drawings, in order to reduce the edge crossings in the resultant drawings. Furthermore, it is easier to understand the structures of graphs from the mixed style drawings. The basic idea is to cover a layered graph by complete bipartite subgraphs (bicliques), then replace bicliques with tree-like structures. The biclique cover problem is reduced to a special edge coloring problem and solved by heuristic coloring algorithms. Our method can be extended to obtain multi-depth confluent layered drawings.",
        "published": "2005-07-19T22:25:53Z",
        "link": "http://arxiv.org/abs/cs/0507051v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Embeddability of Arrangements of Pseudocircles into the Sphere",
        "authors": [
            "Ronald Ortner"
        ],
        "summary": "An arrangement of pseudocircles is a finite set of oriented closed Jordan curves each two of which cross each other in exactly two points. To describe the combinatorial structure of arrangements on closed orientable surfaces, in (Linhart, Ortner 2004) so-called *intersection schemes* were introduced. Building up on results about the latter, we first clarify the notion of embedding of an arrangement. Once this is done it is shown how the embeddability of an arrangement depends on the embeddability of its subarrangements. The main result presented is that an arrangement of pseudocircles can be embedded into the sphere if and only if all of its subarrangements of four pseudocircles are embeddable into the sphere as well.",
        "published": "2005-08-17T10:30:17Z",
        "link": "http://arxiv.org/abs/math/0508320v1",
        "categories": [
            "math.CO",
            "cs.CG",
            "math.GT",
            "52C35; 52C40; 05C10"
        ]
    },
    {
        "title": "Quantum Algorithm Processors to Reveal Hamiltonian Cycles",
        "authors": [
            "John Robert Burger"
        ],
        "summary": "Quantum computer versus quantum algorithm processor in CMOS are compared to find (in parallel) all Hamiltonian cycles in a graph with m edges and n vertices, each represented by k bits. A quantum computer uses quantum states analogous to CMOS registers. With efficient initialization, number of CMOS registers is proportional to (n-1)! Number of qubits in a quantum computer is approximately proportional to kn+2mn in the approach below. Using CMOS, the bits per register is about proportional to kn, which is less since bits can be irreversibly reset. In either concept, number of gates, or operations to identify Hamiltonian cycles is proportional to kmn. However, a quantum computer needs an additional exponentially large number of operations to accomplish a probabilistic readout. In contrast, CMOS is deterministic and readout is comparable to ordinary memory.",
        "published": "2005-08-25T19:04:22Z",
        "link": "http://arxiv.org/abs/cs/0508116v1",
        "categories": [
            "cs.AR",
            "cs.CG",
            "B.7.1; C.1.2"
        ]
    },
    {
        "title": "Grid Vertex-Unfolding Orthogonal Polyhedra",
        "authors": [
            "Mirela Damian",
            "Robin Flatland",
            "Joseph O'Rourke"
        ],
        "summary": "An edge-unfolding of a polyhedron is produced by cutting along edges and flattening the faces to a *net*, a connected planar piece with no overlaps. A *grid unfolding* allows additional cuts along grid edges induced by coordinate planes passing through every vertex. A vertex-unfolding permits faces in the net to be connected at single vertices, not necessarily along edges. We show that any orthogonal polyhedron of genus zero has a grid vertex-unfolding. (There are orthogonal polyhedra that cannot be vertex-unfolded, so some type of \"gridding\" of the faces is necessary.) For any orthogonal polyhedron P with n vertices, we describe an algorithm that vertex-unfolds P in O(n^2) time. Enroute to explaining this algorithm, we present a simpler vertex-unfolding algorithm that requires a 3 x 1 refinement of the vertex grid.",
        "published": "2005-09-19T00:22:03Z",
        "link": "http://arxiv.org/abs/cs/0509054v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2"
        ]
    },
    {
        "title": "Simultaneous Diagonal Flips in Plane Triangulations",
        "authors": [
            "Prosenjit Bose",
            "Jurek Czyzowicz",
            "Zhicheng Gao",
            "Pat Morin",
            "David R. Wood"
        ],
        "summary": "Simultaneous diagonal flips in plane triangulations are investigated. It is proved that every $n$-vertex triangulation with at least six vertices has a simultaneous flip into a 4-connected triangulation, and that it can be computed in O(n) time. It follows that every triangulation has a simultaneous flip into a Hamiltonian triangulation. This result is used to prove that for any two $n$-vertex triangulations, there exists a sequence of $O(\\log n)$ simultaneous flips to transform one into the other. The total number of edges flipped in this sequence is O(n). The maximum size of a simultaneous flip is then studied. It is proved that every triangulation has a simultaneous flip of at least ${1/3}(n-2)$ edges. On the other hand, every simultaneous flip has at most $n-2$ edges, and there exist triangulations with a maximum simultaneous flip of ${6/7}(n-2)$ edges.",
        "published": "2005-09-21T12:45:55Z",
        "link": "http://arxiv.org/abs/math/0509478v2",
        "categories": [
            "math.CO",
            "cs.CG",
            "05C10"
        ]
    },
    {
        "title": "Planar Earthmover is not in $L_1$",
        "authors": [
            "Assaf Naor",
            "Gideon Schechtman"
        ],
        "summary": "We show that any $L_1$ embedding of the transportation cost (a.k.a. Earthmover) metric on probability measures supported on the grid $\\{0,1,...,n\\}^2\\subseteq \\R^2$ incurs distortion $\\Omega(\\sqrt{\\log n})$. We also use Fourier analytic techniques to construct a simple $L_1$ embedding of this space which has distortion $O(\\log n)$.",
        "published": "2005-09-26T16:26:40Z",
        "link": "http://arxiv.org/abs/cs/0509074v1",
        "categories": [
            "cs.CG",
            "math.FA"
        ]
    },
    {
        "title": "The Hunting of the Bump: On Maximizing Statistical Discrepancy",
        "authors": [
            "Deepak Agarwal",
            "Jeff M. Phillips",
            "Suresh Venkatasubramanian"
        ],
        "summary": "Anomaly detection has important applications in biosurveilance and environmental monitoring. When comparing measured data to data drawn from a baseline distribution, merely, finding clusters in the measured data may not actually represent true anomalies. These clusters may likely be the clusters of the baseline distribution. Hence, a discrepancy function is often used to examine how different measured data is to baseline data within a region. An anomalous region is thus defined to be one with high discrepancy.   In this paper, we present algorithms for maximizing statistical discrepancy functions over the space of axis-parallel rectangles. We give provable approximation guarantees, both additive and relative, and our methods apply to any convex discrepancy function. Our algorithms work by connecting statistical discrepancy to combinatorial discrepancy; roughly speaking, we show that in order to maximize a convex discrepancy function over a class of shapes, one needs only maximize a linear discrepancy function over the same set of shapes.   We derive general discrepancy functions for data generated from a one- parameter exponential family. This generalizes the widely-used Kulldorff scan statistic for data from a Poisson distribution. We present an algorithm running in $O(\\smash[tb]{\\frac{1}{\\epsilon} n^2 \\log^2 n})$ that computes the maximum discrepancy rectangle to within additive error $\\epsilon$, for the Kulldorff scan statistic. Similar results hold for relative error and for discrepancy functions for data coming from Gaussian, Bernoulli, and gamma distributions. Prior to our work, the best known algorithms were exact and ran in time $\\smash[t]{O(n^4)}$.",
        "published": "2005-10-03T02:09:14Z",
        "link": "http://arxiv.org/abs/cs/0510004v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Delta-confluent Drawings",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Jeremy Yu Meng"
        ],
        "summary": "We generalize the tree-confluent graphs to a broader class of graphs called Delta-confluent graphs. This class of graphs and distance-hereditary graphs, a well-known class of graphs, coincide. Some results about the visualization of Delta-confluent graphs are also given.",
        "published": "2005-10-11T00:47:37Z",
        "link": "http://arxiv.org/abs/cs/0510024v1",
        "categories": [
            "cs.CG",
            "F.2.2"
        ]
    },
    {
        "title": "Cubic Partial Cubes from Simplicial Arrangements",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show how to construct a cubic partial cube from any simplicial arrangement of lines or pseudolines in the projective plane. As a consequence, we find nine new infinite families of cubic partial cubes as well as many sporadic examples.",
        "published": "2005-10-12T21:49:04Z",
        "link": "http://arxiv.org/abs/math/0510263v1",
        "categories": [
            "math.CO",
            "cs.CG",
            "math.MG",
            "05C12 (Primary) 05C78, 52C30 (Secondary)"
        ]
    },
    {
        "title": "Deterministic boundary recognition and topology extraction for large   sensor networks",
        "authors": [
            "Alexander Kroeller",
            "Sandor P. Fekete",
            "Dennis Pfisterer",
            "Stefan Fischer"
        ],
        "summary": "We present a new framework for the crucial challenge of self-organization of a large sensor network. The basic scenario can be described as follows: Given a large swarm of immobile sensor nodes that have been scattered in a polygonal region, such as a street network. Nodes have no knowledge of size or shape of the environment or the position of other nodes. Moreover, they have no way of measuring coordinates, geometric distances to other nodes, or their direction. Their only way of interacting with other nodes is to send or to receive messages from any node that is within communication range. The objective is to develop algorithms and protocols that allow self-organization of the swarm into large-scale structures that reflect the structure of the street network, setting the stage for global routing, tracking and guiding algorithms.",
        "published": "2005-10-17T12:23:15Z",
        "link": "http://arxiv.org/abs/cs/0510048v1",
        "categories": [
            "cs.DC",
            "cs.CG",
            "C.2.1; F.2.2"
        ]
    },
    {
        "title": "A pair of trees without a simultaneous geometric embedding in the plane",
        "authors": [
            "Martin Kutz"
        ],
        "summary": "Any planar graph has a crossing-free straight-line drawing in the plane. A simultaneous geometric embedding of two n-vertex graphs is a straight-line drawing of both graphs on a common set of n points, such that the edges withing each individual graph do not cross. We consider simultaneous embeddings of two labeled trees, with predescribed vertex correspondences, and present an instance of such a pair that cannot be embedded. Further we provide an example of a planar graph that cannot be embedded together with a path when vertex correspondences are given.",
        "published": "2005-10-18T18:26:00Z",
        "link": "http://arxiv.org/abs/cs/0510053v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Lower bounds on Locality Sensitive Hashing",
        "authors": [
            "Rajeev Motwani",
            "Assaf Naor",
            "Rina Panigrahy"
        ],
        "summary": "Given a metric space $(X,d_X)$, $c\\ge 1$, $r>0$, and $p,q\\in [0,1]$, a distribution over mappings $\\h:X\\to \\mathbb N$ is called a $(r,cr,p,q)$-sensitive hash family if any two points in $X$ at distance at most $r$ are mapped by $\\h$ to the same value with probability at least $p$, and any two points at distance greater than $cr$ are mapped by $\\h$ to the same value with probability at most $q$. This notion was introduced by Indyk and Motwani in 1998 as the basis for an efficient approximate nearest neighbor search algorithm, and has since been used extensively for this purpose. The performance of these algorithms is governed by the parameter $\\rho=\\frac{\\log(1/p)}{\\log(1/q)}$, and constructing hash families with small $\\rho$ automatically yields improved nearest neighbor algorithms. Here we show that for $X=\\ell_1$ it is impossible to achieve $\\rho\\le \\frac{1}{2c}$. This almost matches the construction of Indyk and Motwani which achieves $\\rho\\le \\frac{1}{c}$.",
        "published": "2005-10-29T15:05:52Z",
        "link": "http://arxiv.org/abs/cs/0510088v2",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "A simple effective method for curvatures estimation on triangular meshes",
        "authors": [
            "Jyh-Yang Wu",
            "Sheng-Gwo Chen",
            "Mei-Hsiu Chi"
        ],
        "summary": "To definite and compute differential invariants, like curvatures, for triangular meshes (or polyhedral surfaces) is a key problem in CAGD and the computer vision. The Gaussian curvature and the mean curvature are determined by the differential of the Gauss map of the underlying surface. The Gauss map assigns to each point in the surface the unit normal vector of the tangent plane to the surface at this point. We follow the ideas developed in Chen and Wu \\cite{Chen2}(2004) and Wu, Chen and Chi\\cite{Wu}(2005) to describe a new and simple approach to estimate the differential of the Gauss map and curvatures from the viewpoint of the gradient and the centroid weights. This will give us a much better estimation of curvatures than Taubin's algorithm \\cite{Taubin} (1995).",
        "published": "2005-10-31T02:41:33Z",
        "link": "http://arxiv.org/abs/cs/0510090v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Ramsey partitions and proximity data structures",
        "authors": [
            "Manor Mendel",
            "Assaf Naor"
        ],
        "summary": "This paper addresses two problems lying at the intersection of geometric analysis and theoretical computer science: The non-linear isomorphic Dvoretzky theorem and the design of good approximate distance oracles for large distortion. We introduce the notion of Ramsey partitions of a finite metric space, and show that the existence of good Ramsey partitions implies a solution to the metric Ramsey problem for large distortion (a.k.a. the non-linear version of the isomorphic Dvoretzky theorem, as introduced by Bourgain, Figiel, and Milman). We then proceed to construct optimal Ramsey partitions, and use them to show that for every e\\in (0,1), any n-point metric space has a subset of size n^{1-e} which embeds into Hilbert space with distortion O(1/e). This result is best possible and improves part of the metric Ramsey theorem of Bartal, Linial, Mendel and Naor, in addition to considerably simplifying its proof. We use our new Ramsey partitions to design the best known approximate distance oracles when the distortion is large, closing a gap left open by Thorup and Zwick. Namely, we show that for any $n$ point metric space X, and k>1, there exists an O(k)-approximate distance oracle whose storage requirement is O(n^{1+1/k}), and whose query time is a universal constant. We also discuss applications of Ramsey partitions to various other geometric data structure problems, such as the design of efficient data structures for approximate ranking.",
        "published": "2005-11-23T20:06:15Z",
        "link": "http://arxiv.org/abs/cs/0511084v3",
        "categories": [
            "cs.DS",
            "cs.CG",
            "math.FA",
            "math.MG"
        ]
    },
    {
        "title": "The Signed Distance Function: A New Tool for Binary Classification",
        "authors": [
            "Erik M. Boczko",
            "Todd R. Young"
        ],
        "summary": "From a geometric perspective most nonlinear binary classification algorithms, including state of the art versions of Support Vector Machine (SVM) and Radial Basis Function Network (RBFN) classifiers, and are based on the idea of reconstructing indicator functions. We propose instead to use reconstruction of the signed distance function (SDF) as a basis for binary classification. We discuss properties of the signed distance function that can be exploited in classification algorithms. We develop simple versions of such classifiers and test them on several linear and nonlinear problems. On linear tests accuracy of the new algorithm exceeds that of standard SVM methods, with an average of 50% fewer misclassifications. Performance of the new methods also matches or exceeds that of standard methods on several nonlinear problems including classification of benchmark diagnostic micro-array data sets.",
        "published": "2005-11-30T14:15:17Z",
        "link": "http://arxiv.org/abs/cs/0511105v1",
        "categories": [
            "cs.LG",
            "cs.CG"
        ]
    },
    {
        "title": "Which n-Venn diagrams can be drawn with convex k-gons?",
        "authors": [
            "Jeremy Carroll",
            "Frank Ruskey",
            "Mark Weston"
        ],
        "summary": "We establish a new lower bound for the number of sides required for the component curves of simple Venn diagrams made from polygons. Specifically, for any n-Venn diagram of convex k-gons, we prove that k >= (2^n - 2 - n) / (n (n-2)). In the process we prove that Venn diagrams of seven curves, simple or not, cannot be formed from triangles. We then give an example achieving the new lower bound of a (simple, symmetric) Venn diagram of seven quadrilaterals. Previously Grunbaum had constructed a 7-Venn diagram of non-convex 5-gons [``Venn Diagrams II'', Geombinatorics 2:25-31, 1992].",
        "published": "2005-11-30T23:30:19Z",
        "link": "http://arxiv.org/abs/cs/0512001v1",
        "categories": [
            "cs.CG",
            "I.3.5"
        ]
    },
    {
        "title": "Points on Computable Curves",
        "authors": [
            "Xiaoyang Gu",
            "Jack H. Lutz",
            "Elvira Mayordomo"
        ],
        "summary": "The ``analyst's traveling salesman theorem'' of geometric measure theory characterizes those subsets of Euclidean space that are contained in curves of finite length. This result, proven for the plane by Jones (1990) and extended to higher-dimensional Euclidean spaces by Okikiolu (1991), says that a bounded set $K$ is contained in some curve of finite length if and only if a certain ``square beta sum'', involving the ``width of $K$'' in each element of an infinite system of overlapping ``tiles'' of descending size, is finite.   In this paper we characterize those {\\it points} of Euclidean space that lie on {\\it computable} curves of finite length by formulating and proving a computable extension of the analyst's traveling salesman theorem. Our extension says that a point in Euclidean space lies on some computable curve of finite length if and only if it is ``permitted'' by some computable ``Jones constriction''. A Jones constriction here is an explicit assignment of a rational cylinder to each of the above-mentioned tiles in such a way that, when the radius of the cylinder corresponding to a tile is used in place of the ``width of $K$'' in each tile, the square beta sum is finite. A point is permitted by a Jones constriction if it is contained in the cylinder assigned to each tile containing the point. The main part of our proof is the construction of a computable curve of finite length traversing all the points permitted by a given Jones constriction. Our construction uses the main ideas of Jones's ``farthest insertion'' construction, but our algorithm for computing the curve must work exclusively with the Jones constriction itself, because it has no direct access to the (typically uncomputable) points permitted by the Jones constriction.",
        "published": "2005-12-10T03:08:39Z",
        "link": "http://arxiv.org/abs/cs/0512042v1",
        "categories": [
            "cs.CC",
            "cs.CG"
        ]
    },
    {
        "title": "Computing shortest non-trivial cycles on orientable surfaces of bounded   genus in almost linear time",
        "authors": [
            "Martin Kutz"
        ],
        "summary": "We present an algorithm that computes a shortest non-contractible and a shortest non-separating cycle on an orientable combinatorial surface of bounded genus in O(n \\log n) time, where n denotes the complexity of the surface. This solves a central open problem in computational topology, improving upon the current-best O(n^{3/2})-time algorithm by Cabello and Mohar (ESA 2005). Our algorithm uses universal-cover constructions to find short cycles and makes extensive use of existing tools from the field.",
        "published": "2005-12-15T17:02:53Z",
        "link": "http://arxiv.org/abs/cs/0512064v1",
        "categories": [
            "cs.CG"
        ]
    },
    {
        "title": "Schwerdtfeger-Fillmore-Springer-Cnops Construction Implemented in GiNaC",
        "authors": [
            "Vladimir V. Kisil"
        ],
        "summary": "This paper presents an implementation of the Schwerdtfeger-Fillmore-Springer-Cnops construction (SFSCc) along with illustrations of its usage. SFSCc linearises the linear-fraction action of the Moebius group in R^n. This has clear advantages in several theoretical and applied fields including engineering. Our implementation is based on the Clifford algebra capacities of the GiNaC computer algebra system (http://www.ginac.de/), which were described in cs.MS/0410044.   The core of this realisation of SFSCc is done for an arbitrary dimension of R^n with a metric given by an arbitrary bilinear form. We also present a subclass for two dimensional cycles (i.e. circles, parabolas and hyperbolas), which add some 2D specific routines including a visualisation to PostScript files through the MetaPost (http://www.tug.org/metapost.html) or Asymptote (http://asymptote.sourceforge.net/) packages.   This software is the backbone of many results published in math.CV/0512416 and we use its applications their for demonstration. The library can be ported (with various level of required changes) to other CAS with Clifford algebras capabilities similar to GiNaC.   There is an ISO image of a Live Debian DVD attached to this paper as an auxiliary file, a copy is stored on Google Drive as well.",
        "published": "2005-12-17T15:09:11Z",
        "link": "http://arxiv.org/abs/cs/0512073v12",
        "categories": [
            "cs.MS",
            "cs.CG",
            "cs.SC",
            "51B25, 51N25, 68U05, 11E88, 68W30"
        ]
    },
    {
        "title": "Data Structures for Halfplane Proximity Queries and Incremental Voronoi   Diagrams",
        "authors": [
            "Boris Aronov",
            "Prosenjit Bose",
            "Erik D. Demaine",
            "Joachim Gudmundsson",
            "John Iacono",
            "Stefan Langerman",
            "Michiel Smid"
        ],
        "summary": "We consider preprocessing a set $S$ of $n$ points in convex position in the plane into a data structure supporting queries of the following form: given a point $q$ and a directed line $\\ell$ in the plane, report the point of $S$ that is farthest from (or, alternatively, nearest to) the point $q$ among all points to the left of line $\\ell$. We present two data structures for this problem. The first data structure uses $O(n^{1+\\varepsilon})$ space and preprocessing time, and answers queries in $O(2^{1/\\varepsilon} \\log n)$ time, for any $0 < \\varepsilon < 1$. The second data structure uses $O(n \\log^3 n)$ space and polynomial preprocessing time, and answers queries in $O(\\log n)$ time. These are the first solutions to the problem with $O(\\log n)$ query time and $o(n^2)$ space.   The second data structure uses a new representation of nearest- and farthest-point Voronoi diagrams of points in convex position. This representation supports the insertion of new points in clockwise order using only $O(\\log n)$ amortized pointer changes, in addition to $O(\\log n)$-time point-location queries, even though every such update may make $\\Theta(n)$ combinatorial changes to the Voronoi diagram. This data structure is the first demonstration that deterministically and incrementally constructed Voronoi diagrams can be maintained in $o(n)$ amortized pointer changes per operation while keeping $O(\\log n)$-time point-location queries.",
        "published": "2005-12-23T04:28:12Z",
        "link": "http://arxiv.org/abs/cs/0512091v3",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Mathematical models of the complex surfaces in simulation and   visualization systems",
        "authors": [
            "Dmitry P. Paukov"
        ],
        "summary": "Modeling, simulation and visualization of three-dimension complex bodies widely use mathematical model of curves and surfaces. The most important curves and surfaces for these purposes are curves and surfaces in Hermite and Bezier forms, splines and NURBS. Article is devoted to survey this way to use geometrical data in various computer graphics systems and adjacent fields.",
        "published": "2005-12-26T14:45:32Z",
        "link": "http://arxiv.org/abs/cs/0512098v1",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "Analytic Definition of Curves and Surfaces by Parabolic Blending",
        "authors": [
            "A. W. Overhauser"
        ],
        "summary": "A procedure for interpolating between specified points of a curve or surface is described. The method guarantees slope continuity at all junctions. A surface panel divided into p x q contiguous patches is completely specified by the coordinates of (p+1) x (q+1) points. Each individual patch, however, depends parametrically on the coordinates of 16 points, allowing shape flexibility and global conformity.",
        "published": "2005-03-22T16:59:56Z",
        "link": "http://arxiv.org/abs/cs/0503054v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "Convexity Analysis of Snake Models Based on Hamiltonian Formulation",
        "authors": [
            "Gilson Antonio Giraldi",
            "Antonio Alberto Fernandes de Oliveira"
        ],
        "summary": "This paper presents a convexity analysis for the dynamic snake model based on the Potential Energy functional and the Hamiltonian formulation of the classical mechanics. First we see the snake model as a dynamical system whose singular points are the borders we seek. Next we show that a necessary condition for a singular point to be an attractor is that the energy functional is strictly convex in a neighborhood of it, that means, if the singular point is a local minimum of the potential energy. As a consequence of this analysis, a local expression relating the dynamic parameters and the rate of convergence arises. Such results link the convexity analysis of the potential energy and the dynamic snake model and point forward to the necessity of a physical quantity whose convexity analysis is related to the dynamic and which incorporate the velocity space. Such a quantity is exactly the (conservative) Hamiltonian of the system.",
        "published": "2005-04-08T16:41:52Z",
        "link": "http://arxiv.org/abs/cs/0504031v1",
        "categories": [
            "cs.CV",
            "cs.GR",
            "I.4; I.4.6;I.4.8"
        ]
    },
    {
        "title": "k-core decomposition: a tool for the visualization of large scale   networks",
        "authors": [
            "José Ignacio Alvarez-Hamelin",
            "Luca Dall'Asta",
            "Alain Barrat",
            "Alessandro Vespignani"
        ],
        "summary": "We use the k-core decomposition to visualize large scale complex networks in two dimensions. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n+e), where 'n' is the size of the network, and 'e' is the number of edges, makes it suitable for the visualization of very large sparse networks. We apply the proposed visualization tool to several real and synthetic graphs, showing its utility in finding specific structural fingerprints of computer generated and real world networks.",
        "published": "2005-04-28T13:53:36Z",
        "link": "http://arxiv.org/abs/cs/0504107v2",
        "categories": [
            "cs.NI",
            "cs.GR"
        ]
    },
    {
        "title": "Estimacao Temporal da Deformacao entre Objectos utilizando uma   Metodologia Fisica",
        "authors": [
            "Joao Manuel R. S. Tavares",
            "Raquel R. Pinho"
        ],
        "summary": "In this paper, it is presented a methodology to estimate the deformation involved between two objects attending to its physical properties. This methodology can be used, for example, in Computational Vision or Computer Graphics applications, and consists in physically modeling the objects, by means of the Finite Elements Method, establishing correspondences between some of its data points, by using Modal Matching, and finally, determining the displacement field, that is the intermediate shapes, through the resolution of the Lagrange Dynamic Equilibrium Equation. As in many of the possible applications of the methodology to present, it is necessary to quantify the existing deformation, as well as to estimate only the non rigid component of the involved global deformation. The solutions adopted to satisfy such intentions will be also presented.",
        "published": "2005-05-16T02:20:19Z",
        "link": "http://arxiv.org/abs/cs/0505043v2",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "Lattice Gas Cellular Automata for Computational Fluid Animation",
        "authors": [
            "Gilson A. Giraldi",
            "Adilson V. Xavier",
            "Antonio L. Apolinario Jr",
            "Paulo S. Rodrigues"
        ],
        "summary": "The past two decades showed a rapid growing of physically-based modeling of fluids for computer graphics applications. In this area, a common top down approach is to model the fluid dynamics by Navier-Stokes equations and apply a numerical techniques such as Finite Differences or Finite Elements for the simulation. In this paper we focus on fluid modeling through Lattice Gas Cellular Automata (LGCA) for computer graphics applications. LGCA are discrete models based on point particles that move on a lattice, according to suitable and simple rules in order to mimic a fully molecular dynamics. By Chapman-Enskog expansion, a known multiscale technique in this area, it can be demonstrated that the Navier-Stokes model can be reproduced by the LGCA technique. Thus, with LGCA we get a fluid model that does not require solution of complicated equations. Therefore, we combine the advantage of the low computational cost of LGCA and its ability to mimic the realistic fluid dynamics to develop a new animating framework for computer graphics applications. In this work, we discuss the theoretical elements of our proposal and show experimental results.",
        "published": "2005-07-05T19:48:09Z",
        "link": "http://arxiv.org/abs/cs/0507012v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "Methods for Analytical Understanding of Agent-Based Modeling of Complex   Systems",
        "authors": [
            "Gilson A. Giraldi",
            "Luis C. da Costa",
            "Adilson V. Xavier",
            "Paulo S. Rodrigues"
        ],
        "summary": "Von Neuman's work on universal machines and the hardware development have allowed the simulation of dynamical systems through a large set of interacting agents. This is a bottom-up approach which tries to derive global properties of a complex system through local interaction rules and agent behaviour. Traditionally, such systems are modeled and simulated through top-down methods based on differential equations. Agent-Based Modeling has the advantage of simplicity and low computational cost. However, unlike differential equations, there is no standard way to express agent behaviour. Besides, it is not clear how to analytically predict the results obtained by the simulation. In this paper we survey some of these methods. For expressing agent behaviour formal methods, like Stochastic Process Algebras have been used. Such approach is useful if the global properties of interest can be expressed as a function of stochastic time series. However, if space variables must be considered, we shall change the focus. In this case, multiscale techniques, based on Chapman-Enskog expansion, was used to establish the connection between the microscopic dynamics and the macroscopic observables. Also, we use data mining techniques,like Principal Component Analysis (PCA), to study agent systems like Cellular Automata. With the help of these tools we will discuss a simple society model, a Lattice Gas Automaton for fluid modeling, and knowledge discovery in CA databases. Besides, we show the capabilities of the NetLogo, a software for agent simulation of complex system and show our experience about.",
        "published": "2005-07-30T12:31:00Z",
        "link": "http://arxiv.org/abs/cs/0508002v1",
        "categories": [
            "cs.GR"
        ]
    },
    {
        "title": "MathPSfrag: Creating Publication-Quality Labels in Mathematica Plots",
        "authors": [
            "J. Grosse"
        ],
        "summary": "This article introduces a Mathematica package providing a graphics export function that automatically replaces Mathematica expressions in a graphic by the corresponding LaTeX constructs and positions them correctly. It thus facilitates the creation of publication-quality Enscapulated PostScript (EPS) graphics.",
        "published": "2005-10-31T09:40:00Z",
        "link": "http://arxiv.org/abs/cs/0510087v1",
        "categories": [
            "cs.GR",
            "I.3.4"
        ]
    },
    {
        "title": "Spatiotemporal sensistivity and visual attention for efficient rendering   of dynamic environments",
        "authors": [
            "Yang Li Hector Yee"
        ],
        "summary": "We present a method to accelerate global illumination computation in dynamic environments by taking advantage of limitations of the human visual system. A model of visual attention is used to locate regions of interest in a scene and to modulate spatiotemporal sensitivity. The method is applied in the form of a spatiotemporal error tolerance map. Perceptual acceleration combined with good sampling protocols provide a global illumination solution feasible for use in animation. Results indicate an order of magnitude improvement in computational speed. The method is adaptable and can also be used in image-based rendering, geometry level of detail selection, realistic image synthesis, video telephony and video compression.",
        "published": "2005-11-08T14:40:47Z",
        "link": "http://arxiv.org/abs/cs/0511032v1",
        "categories": [
            "cs.GR",
            "cs.CV"
        ]
    },
    {
        "title": "A geometry of information, I: Nerves, posets and differential forms",
        "authors": [
            "Jonathan Gratus",
            "Timothy Porter"
        ],
        "summary": "The main theme of this workshop (Dagstuhl seminar 04351) is `Spatial Representation: Continuous vs. Discrete'. Spatial representation has two contrasting but interacting aspects (i) representation of spaces' and (ii) representation by spaces. In this paper, we will examine two aspects that are common to both interpretations of the theme, namely nerve constructions and refinement. Representations change, data changes, spaces change. We will examine the possibility of a `differential geometry' of spatial representations of both types, and in the sequel give an algebra of differential forms that has the potential to handle the dynamical aspect of such a geometry. We will discuss briefly a conjectured class of spaces, generalising the Cantor set which would seem ideal as a test-bed for the set of tools we are developing.",
        "published": "2005-12-02T16:18:11Z",
        "link": "http://arxiv.org/abs/cs/0512010v1",
        "categories": [
            "cs.AI",
            "cs.GR"
        ]
    },
    {
        "title": "Incremental and Transitive Discrete Rotations",
        "authors": [
            "Bertrand Nouvel",
            "Eric Remila"
        ],
        "summary": "A discrete rotation algorithm can be apprehended as a parametric application $f\\_\\alpha$ from $\\ZZ[i]$ to $\\ZZ[i]$, whose resulting permutation ``looks like'' the map induced by an Euclidean rotation. For this kind of algorithm, to be incremental means to compute successively all the intermediate rotate d copies of an image for angles in-between 0 and a destination angle. The di scretized rotation consists in the composition of an Euclidean rotation with a discretization; the aim of this article is to describe an algorithm whic h computes incrementally a discretized rotation. The suggested method uses o nly integer arithmetic and does not compute any sine nor any cosine. More pr ecisely, its design relies on the analysis of the discretized rotation as a step function: the precise description of the discontinuities turns to be th e key ingredient that will make the resulting procedure optimally fast and e xact. A complete description of the incremental rotation process is provided, also this result may be useful in the specification of a consistent set of defin itions for discrete geometry.",
        "published": "2005-12-16T16:12:12Z",
        "link": "http://arxiv.org/abs/cs/0512070v2",
        "categories": [
            "cs.DM",
            "cs.GR"
        ]
    },
    {
        "title": "Mathematical models of the complex surfaces in simulation and   visualization systems",
        "authors": [
            "Dmitry P. Paukov"
        ],
        "summary": "Modeling, simulation and visualization of three-dimension complex bodies widely use mathematical model of curves and surfaces. The most important curves and surfaces for these purposes are curves and surfaces in Hermite and Bezier forms, splines and NURBS. Article is devoted to survey this way to use geometrical data in various computer graphics systems and adjacent fields.",
        "published": "2005-12-26T14:45:32Z",
        "link": "http://arxiv.org/abs/cs/0512098v1",
        "categories": [
            "cs.GR",
            "cs.CG"
        ]
    },
    {
        "title": "Multi-Vehicle Cooperative Control Using Mixed Integer Linear Programming",
        "authors": [
            "Matthew G. Earl",
            "Raffaello D'Andrea"
        ],
        "summary": "We present methods to synthesize cooperative strategies for multi-vehicle control problems using mixed integer linear programming. Complex multi-vehicle control problems are expressed as mixed logical dynamical systems. Optimal strategies for these systems are then solved for using mixed integer linear programming. We motivate the methods on problems derived from an adversarial game between two teams of robots called RoboFlag. We assume the strategy for one team is fixed and governed by state machines. The strategy for the other team is generated using our methods. Finally, we perform an average case computational complexity study on our approach.",
        "published": "2005-01-31T01:03:54Z",
        "link": "http://arxiv.org/abs/cs/0501092v1",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.MA",
            "I.2.9; I.2.8; I.2.11"
        ]
    },
    {
        "title": "The Self-Organization of Speech Sounds",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "summary": "The speech code is a vehicle of language: it defines a set of forms used by a community to carry information. Such a code is necessary to support the linguistic interactions that allow humans to communicate. How then may a speech code be formed prior to the existence of linguistic interactions? Moreover, the human speech code is discrete and compositional, shared by all the individuals of a community but different across communities, and phoneme inventories are characterized by statistical regularities. How can a speech code with these properties form? We try to approach these questions in the paper, using the \"methodology of the artificial\". We build a society of artificial agents, and detail a mechanism that shows the formation of a discrete speech code without pre-supposing the existence of linguistic capacities or of coordinated interactions. The mechanism is based on a low-level model of sensory-motor interactions. We show that the integration of certain very simple and non language-specific neural devices leads to the formation of a speech code that has properties similar to the human speech code. This result relies on the self-organizing properties of a generic coupling between perception and production within agents, and on the interactions between agents. The artificial system helps us to develop better intuitions on how speech might have appeared, by showing how self-organization might have helped natural selection to find speech.",
        "published": "2005-02-22T09:51:16Z",
        "link": "http://arxiv.org/abs/cs/0502086v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE",
            "cs.RO",
            "math.DS"
        ]
    },
    {
        "title": "Geometric Models of Rolling-Shutter Cameras",
        "authors": [
            "Marci Meingast",
            "Christopher Geyer",
            "Shankar Sastry"
        ],
        "summary": "Cameras with rolling shutters are becoming more common as low-power, low-cost CMOS sensors are being used more frequently in cameras. The rolling shutter means that not all scanlines are exposed over the same time interval. The effects of a rolling shutter are noticeable when either the camera or objects in the scene are moving and can lead to systematic biases in projection estimation. We develop a general projection equation for a rolling shutter camera and show how it is affected by different types of camera motion. In the case of fronto-parallel motion, we show how that camera can be modeled as an X-slit camera. We also develop approximate projection equations for a non-zero angular velocity about the optical axis and approximate the projection equation for a constant velocity screw motion. We demonstrate how the rolling shutter effects the projective geometry of the camera and in turn the structure-from-motion.",
        "published": "2005-03-29T00:00:17Z",
        "link": "http://arxiv.org/abs/cs/0503076v1",
        "categories": [
            "cs.CV",
            "cs.RO"
        ]
    },
    {
        "title": "A Decomposition Approach to Multi-Vehicle Cooperative Control",
        "authors": [
            "Matthew Earl",
            "Raffaello D'Andrea"
        ],
        "summary": "We present methods that generate cooperative strategies for multi-vehicle control problems using a decomposition approach. By introducing a set of tasks to be completed by the team of vehicles and a task execution method for each vehicle, we decomposed the problem into a combinatorial component and a continuous component. The continuous component of the problem is captured by task execution, and the combinatorial component is captured by task assignment. In this paper, we present a solver for task assignment that generates near-optimal assignments quickly and can be used in real-time applications. To motivate our methods, we apply them to an adversarial game between two teams of vehicles. One team is governed by simple rules and the other by our algorithms. In our study of this game we found phase transitions, showing that the task assignment problem is most difficult to solve when the capabilities of the adversaries are comparable. Finally, we implement our algorithms in a multi-level architecture with a variable replanning rate at each level to provide feedback on a dynamically changing and uncertain environment.",
        "published": "2005-04-18T03:53:27Z",
        "link": "http://arxiv.org/abs/cs/0504081v1",
        "categories": [
            "cs.RO",
            "I.2.9; I.2.8; I.2.11"
        ]
    },
    {
        "title": "Iterative MILP Methods for Vehicle Control Problems",
        "authors": [
            "Matthew Earl",
            "Raffaello D'Andrea"
        ],
        "summary": "Mixed integer linear programming (MILP) is a powerful tool for planning and control problems because of its modeling capability and the availability of good solvers. However, for large models, MILP methods suffer computationally. In this paper, we present iterative MILP algorithms that address this issue. We consider trajectory generation problems with obstacle avoidance requirements and minimum time trajectory generation problems. The algorithms use fewer binary variables than standard MILP methods and require less computational effort.",
        "published": "2005-05-16T03:54:08Z",
        "link": "http://arxiv.org/abs/cs/0505042v1",
        "categories": [
            "cs.RO",
            "I.2.9; I.2.8; J.2"
        ]
    },
    {
        "title": "A T Step Ahead Optimal Target Detection Algorithm for a Multi Sensor   Surveillance System",
        "authors": [
            "K Madhava Krishna",
            "Henry Hexmoor",
            "Shravan Sogani"
        ],
        "summary": "This paper presents a methodology for optimal target detection in a multi sensor surveillance system. The system consists of mobile sensors that guard a rectangular surveillance zone crisscrossed by moving targets. Targets percolate the surveillance zone in a poisson fashion with uniform velocities. Under these statistics this paper computes a motion strategy for a sensor that maximizes target detections for the next T time steps. A coordination mechanism between sensors ensures that overlapping areas between sensors is reduced. This coordination mechanism is interleaved with the motion strategy computation to reduce detections of the same target by more than one sensor. To avoid an exhaustive search in the joint space of all sensors the coordination mechanism constraints the search by assigning priorities to the sensors. A comparison of this methodology with other multi target tracking schemes verifies its efficacy in maximizing detections. A tabulation of these comparisons is reported in results section of the paper",
        "published": "2005-05-17T13:06:41Z",
        "link": "http://arxiv.org/abs/cs/0505045v1",
        "categories": [
            "cs.MA",
            "cs.RO"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Scouting Red Beds for Uncommon Features with   Geological Significance",
        "authors": [
            "Patrick C. McGuire",
            "Enrique Diaz-Martinez",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose A. Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "The `Cyborg Astrobiologist' (CA) has undergone a second geological field trial, at a red sandstone site in northern Guadalajara, Spain, near Riba de Santiuste. The Cyborg Astrobiologist is a wearable computer and video camera system that has demonstrated a capability to find uncommon interest points in geological imagery in real-time in the field. The first (of three) geological structures that we studied was an outcrop of nearly homogeneous sandstone, which exhibits oxidized-iron impurities in red and and an absence of these iron impurities in white. The white areas in these ``red beds'' have turned white because the iron has been removed by chemical reduction, perhaps by a biological agent. The computer vision system found in one instance several (iron-free) white spots to be uncommon and therefore interesting, as well as several small and dark nodules. The second geological structure contained white, textured mineral deposits on the surface of the sandstone, which were found by the CA to be interesting. The third geological structure was a 50 cm thick paleosol layer, with fossilized root structures of some plants, which were found by the CA to be interesting. A quasi-blind comparison of the Cyborg Astrobiologist's interest points for these images with the interest points determined afterwards by a human geologist shows that the Cyborg Astrobiologist concurred with the human geologist 68% of the time (true positive rate), with a 32% false positive rate and a 32% false negative rate.   (abstract has been abridged).",
        "published": "2005-05-23T09:55:37Z",
        "link": "http://arxiv.org/abs/cs/0505058v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "physics.ins-det",
            "q-bio.NC",
            "I.2.10; I.4.6; I.4.8; I.4.9; I.2.9; I.5.4; I.5.5; J.2; J.3; D.2;\n  D.1.7; D.4.7"
        ]
    },
    {
        "title": "Multi-Modal Human-Machine Communication for Instructing Robot Grasping   Tasks",
        "authors": [
            "P. C. McGuire",
            "J. Fritsch",
            "J. J. Steil",
            "F. Roethling",
            "G. A. Fink",
            "S. Wachsmuth",
            "G. Sagerer",
            "H. Ritter"
        ],
        "summary": "A major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One way of such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable to establish a common focus of attention and be able to use and integrate spoken instructions, visual perceptions, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and an modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects.",
        "published": "2005-05-24T14:53:49Z",
        "link": "http://arxiv.org/abs/cs/0505064v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "H.1.2; I.2.9; I.2.10; I.2.7; H.5.2; H.5.1; I.2.6; I.4.8; I.4.7;\n  I.4.6"
        ]
    },
    {
        "title": "Field geology with a wearable computer: 1st results of the Cyborg   Astrobiologist System",
        "authors": [
            "Patrick C. McGuire",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Markus Oesker",
            "Robert Haschke",
            "Joerg Ontrup",
            "Helge Ritter"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. This work is more of a test of the whole system, rather than of any one part of the system. However, beyond the concept of the system itself, the uncommon map (despite its simplicity) is the main innovative part of the system. The uncommon map helps to determine interest-points in a context-free manner. Overall, the hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner. We use these capabilities for autonomous guidance towards geological points-of-interest.",
        "published": "2005-06-24T10:25:22Z",
        "link": "http://arxiv.org/abs/cs/0506089v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO"
        ]
    },
    {
        "title": "Explorations in engagement for humans and robots",
        "authors": [
            "Candace L. Sidner",
            "Christopher Lee",
            "Cory Kidd",
            "Neal Lesh",
            "Charles Rich"
        ],
        "summary": "This paper explores the concept of engagement, the process by which individuals in an interaction start, maintain and end their perceived connection to one another. The paper reports on one aspect of engagement among human interactors--the effect of tracking faces during an interaction. It also describes the architecture of a robot that can participate in conversational, collaborative interactions with engagement gestures. Finally, the paper reports on findings of experiments with human participants who interacted with a robot when it either performed or did not perform engagement gestures. Results of the human-robot studies indicate that people become engaged with robots: they direct their attention to the robot more often in interactions where engagement gestures are present, and they find interactions more appropriate when engagement gestures are present than when they are not.",
        "published": "2005-07-21T21:56:34Z",
        "link": "http://arxiv.org/abs/cs/0507056v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.RO",
            "I.2.7; I.2.9"
        ]
    },
    {
        "title": "Cybercars : Past, Present and Future of the Technology",
        "authors": [
            "Michel Parent",
            "Arnaud De La Fortelle"
        ],
        "summary": "Automobile has become the dominant transport mode in the world in the last century. In order to meet a continuously growing demand for transport, one solution is to change the control approach for vehicle to full driving automation, which removes the driver from the control loop to improve efficiency and reduce accidents. Recent work shows that there are several realistic paths towards this deployment : driving assistance on passenger cars, automated commercial vehicles on dedicated infrastructures, and new forms of urban transport (car-sharing and cybercars). Cybercars have already been put into operation in Europe, and it seems that this approach could lead the way towards full automation on most urban, and later interurban infrastructures. The European project CyberCars has brought many improvements in the technology needed to operate cybercars over the last three years. A new, larger European project is now being prepared to carry this work further in order to meet more ambitious objectives in terms of safety and efficiency. This paper will present past and present technologies and will focus on the future developments.",
        "published": "2005-10-20T15:03:48Z",
        "link": "http://arxiv.org/abs/cs/0510059v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Applying Evolutionary Optimisation to Robot Obstacle Avoidance",
        "authors": [
            "Olivier Pauplin",
            "Jean Louchet",
            "Evelyne Lutton",
            "Michel Parent"
        ],
        "summary": "This paper presents an artificial evolutionbased method for stereo image analysis and its application to real-time obstacle detection and avoidance for a mobile robot. It uses the Parisian approach, which consists here in splitting the representation of the robot's environment into a large number of simple primitives, the \"flies\", which are evolved following a biologically inspired scheme and give a fast, low-cost solution to the obstacle detection problem in mobile robotics.",
        "published": "2005-10-25T07:07:01Z",
        "link": "http://arxiv.org/abs/cs/0510076v1",
        "categories": [
            "cs.AI",
            "cs.RO"
        ]
    },
    {
        "title": "Effects of Initial Stance of Quadruped Trotting on Walking Stability",
        "authors": [
            "Dongqing He",
            "Peisun Ma"
        ],
        "summary": "It is very important for quadruped walking machine to keep its stability in high speed walking. It has been indicated that moment around the supporting diagonal line of quadruped in trotting gait largely influences walking stability. In this paper, moment around the supporting diagonal line of quadruped in trotting gait is modeled and its effects on body attitude are analyzed. The degree of influence varies with different initial stances of quadruped and we get the optimal initial stance of quadruped in trotting gait with maximal walking stability. Simulation results are presented. Keywords: quadruped, trotting, attitude, walking stability.",
        "published": "2005-11-18T14:39:01Z",
        "link": "http://arxiv.org/abs/cs/0511067v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "An Agent-based Manufacturing Management System for Production and   Logistics within Cross-Company Regional and National Production Networks",
        "authors": [
            "S. Heinrich",
            "H. Durr",
            "T. Hanel",
            "J. Lassig"
        ],
        "summary": "The goal is the development of a simultaneous, dynamic, technological as well as logistical real-time planning and an organizational control of the production by the production units themselves, working in the production network under the use of Multi-Agent-Technology. The design of the multi-agent-based manufacturing management system, the models of the single agents, algorithms for the agent-based, decentralized dispatching of orders, strategies and data management concepts as well as their integration into the SCM, basing on the solution described, will be explained in the following.   Keywords: production engineering and management, dynamic manufacturing planning and control, multi-agentsystems (MAS), supply-chain-management (SCM), e-manufacturing",
        "published": "2005-11-18T14:41:09Z",
        "link": "http://arxiv.org/abs/cs/0511068v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Nonlinear Receding-Horizon Control of Rigid Link Robot Manipulators",
        "authors": [
            "R. Hedjar",
            "P. Boucher"
        ],
        "summary": "The approximate nonlinear receding-horizon control law is used to treat the trajectory tracking control problem of rigid link robot manipulators. The derived nonlinear predictive law uses a quadratic performance index of the predicted tracking error and the predicted control effort. A key feature of this control law is that, for their implementation, there is no need to perform an online optimization, and asymptotic tracking of smooth reference trajectories is guaranteed. It is shown that this controller achieves the positions tracking objectives via link position measurements. The stability convergence of the output tracking error to the origin is proved. To enhance the robustness of the closed loop system with respect to payload uncertainties and viscous friction, an integral action is introduced in the loop. A nonlinear observer is used to estimate velocity. Simulation results for a two-link rigid robot are performed to validate the performance of the proposed controller.   Keywords: receding-horizon control, nonlinear observer, robot manipulators, integral action, robustness.",
        "published": "2005-11-18T14:42:51Z",
        "link": "http://arxiv.org/abs/cs/0511069v1",
        "categories": [
            "cs.RO"
        ]
    },
    {
        "title": "Formal Languages and Algorithms for Similarity based Retrieval from   Sequence Databases",
        "authors": [
            "A. Prasad Sistla"
        ],
        "summary": "The paper considers various formalisms based on Automata, Temporal Logic and Regular Expressions for specifying queries over sequences. Unlike traditional binary semantics, the paper presents a similarity based semantics for thse formalisms. More specifically, a distance measure in the range [0,1] is associated with a sequence, query pair denoting how closely the sequence satisfies the query. These measures are defined using a spectrum of normed vector distance measures. Various distance measures based on the syntax and the traditional semantics of the query are presented. Efficient algorithms for computing these distance measure are presented. These algorithms can be employed for retrieval of sequence from a database that closely satisfy a given.",
        "published": "2005-01-04T01:05:28Z",
        "link": "http://arxiv.org/abs/cs/0501006v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "I.2.4; I.2.3; D.2.4"
        ]
    },
    {
        "title": "Estimating Range Queries using Aggregate Data with Integrity   Constraints: a Probabilistic Approach",
        "authors": [
            "Francesco Buccafurri",
            "Filippo Furfaro",
            "Domenico Sacca'"
        ],
        "summary": "The problem of recovering (count and sum) range queries over multidimensional data only on the basis of aggregate information on such data is addressed. This problem can be formalized as follows. Suppose that a transformation T producing a summary from a multidimensional data set is used. Now, given a data set D, a summary S=T(D) and a range query r on D, the problem consists of studying r by modelling it as a random variable defined over the sample space of all the data sets D' such that T(D) = S. The study of such a random variable, done by the definition of its probability distribution and the computation of its mean value and variance, represents a well-founded, theoretical probabilistic approach for estimating the query only on the basis of the available information (that is the summary S) without assumptions on original data.",
        "published": "2005-01-14T19:45:10Z",
        "link": "http://arxiv.org/abs/cs/0501029v1",
        "categories": [
            "cs.DB",
            "E.4; G.3; H.3"
        ]
    },
    {
        "title": "Maintaining Consistency of Data on the Web",
        "authors": [
            "Martin Bernauer"
        ],
        "summary": "Increasingly more data is becoming available on the Web, estimates speaking of 1 billion documents in 2002. Most of the documents are Web pages whose data is considered to be in XML format, expecting it to eventually replace HTML.   A common problem in designing and maintaining a Web site is that data on a Web page often replicates or derives from other data, the so-called base data, that is usually not contained in the deriving or replicating page. Consequently, replicas and derivations become inconsistent upon modifying base data in a Web page or a relational database. For example, after assigning a thesis to a student and modifying the Web page that describes it in detail, the thesis is still incorrectly contained in the list of offered thesis, missing in the list of ongoing thesis, and missing in the advisor's teaching record.   The thesis presents a solution by proposing a combined approach that provides for maintaining consistency of data in Web pages that (i) replicate data in relational databases, or (ii) replicate or derive from data in Web pages. Upon modifying base data, the modification is immediately pushed to affected Web pages. There, maintenance is performed incrementally by only modifying the affected part of the page instead of re-generating the whole page from scratch.",
        "published": "2005-01-20T14:11:03Z",
        "link": "http://arxiv.org/abs/cs/0501042v3",
        "categories": [
            "cs.DB",
            "cs.DS",
            "E.1; H.2.3; H.2.4"
        ]
    },
    {
        "title": "Relational Algebra as non-Distributive Lattice",
        "authors": [
            "Vadim Tropashko"
        ],
        "summary": "We reduce the set of classic relational algebra operators to two binary operations: natural join and generalized union. We further demonstrate that this set of operators is relationally complete and honors lattice axioms.",
        "published": "2005-01-21T20:07:32Z",
        "link": "http://arxiv.org/abs/cs/0501053v3",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Data Mining for Actionable Knowledge: A Survey",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The data mining process consists of a series of steps ranging from data cleaning, data selection and transformation, to pattern evaluation and visualization. One of the central problems in data mining is to make the mined patterns or knowledge actionable. Here, the term actionable refers to the mined patterns suggest concrete and profitable actions to the decision-maker. That is, the user can do something to bring direct benefits (increase in profits, reduction in cost, improvement in efficiency, etc.) to the organization's advantage. However, there has been written no comprehensive survey available on this topic. The goal of this paper is to fill the void.   In this paper, we first present two frameworks for mining actionable knowledge that are inexplicitly adopted by existing research methods. Then we try to situate some of the research on this topic from two different viewpoints: 1) data mining tasks and 2) adopted framework. Finally, we specify issues that are either not addressed or insufficiently studied yet and conclude the paper.",
        "published": "2005-01-27T12:13:16Z",
        "link": "http://arxiv.org/abs/cs/0501079v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Scientific Data Management in the Coming Decade",
        "authors": [
            "Jim Gray",
            "David T. Liu",
            "Maria Nieto-Santisteban",
            "Alexander S. Szalay",
            "David DeWitt",
            "Gerd Heber"
        ],
        "summary": "This is a thought piece on data-intensive science requirements for databases and science centers. It argues that peta-scale datasets will be housed by science centers that provide substantial storage and processing for scientists who access the data via smart notebooks. Next-generation science instruments and simulations will generate these peta-scale datasets. The need to publish and share data and the need for generic analysis and visualization tools will finally create a convergence on common metadata standards. Database systems will be judged by their support of these metadata standards and by their ability to manage and access peta-scale datasets. The procedural stream-of-bytes-file-centric approach to data analysis is both too cumbersome and too serial for such large datasets. Non-procedural query and analysis of schematized self-describing data is both easier to use and allows much more parallelism.",
        "published": "2005-02-02T03:15:42Z",
        "link": "http://arxiv.org/abs/cs/0502008v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Performance Considerations for Gigabyte per Second Transcontinental   Disk-to-Disk File Transfers",
        "authors": [
            "Peter Kukol",
            "Jim Gray"
        ],
        "summary": "Moving data from CERN to Pasadena at a gigabyte per second using the next generation Internet requires good networking and good disk IO. Ten Gbps Ethernet and OC192 links are in place, so now it is simply a matter of programming. This report describes our preliminary work and measurements in configuring the disk subsystem for this effort. Using 24 SATA disks at each endpoint we are able to locally read and write an NTFS volume is striped across 24 disks at 1.2 GBps. A 32-disk stripe delivers 1.7 GBps. Experiments on higher performance and higher-capacity systems deliver up to 3.5 GBps.",
        "published": "2005-02-02T03:26:40Z",
        "link": "http://arxiv.org/abs/cs/0502009v1",
        "categories": [
            "cs.DB",
            "cs.PF"
        ]
    },
    {
        "title": "TerraServer SAN-Cluster Architecture and Operations Experience",
        "authors": [
            "Tom Barclay",
            "Jim Gray"
        ],
        "summary": "Microsoft TerraServer displays aerial, satellite, and to-pographic images of the earth in a SQL database available via the Internet. It is one of the most popular online at-lases, presenting seventeen terabytes of image data from the United States Geological Survey (USGS). Initially de-ployed in 1998, the system demonstrated the scalability of PC hardware and software - Windows and SQL Server - on a single, mainframe-class processor. In September 2000, the back-end database application was migrated to 4-node active/passive cluster connected to an 18 terabyte Storage Area Network (SAN). The new configuration was designed to achieve 99.99% availability for the back-end application. This paper describes the hardware and software components of the TerraServer Cluster and SAN, and describes our experience in configuring and operating this system for three years. Not surprisingly, the hardware and architecture delivered better than four-9's of availability, but operations mistakes delivered three-9's.",
        "published": "2005-02-02T03:36:30Z",
        "link": "http://arxiv.org/abs/cs/0502010v1",
        "categories": [
            "cs.DC",
            "cs.DB"
        ]
    },
    {
        "title": "Where the Rubber Meets the Sky: Bridging the Gap between Databases and   Science",
        "authors": [
            "Jim Gray",
            "Alexander S. Szalay"
        ],
        "summary": "Scientists in all domains face a data avalanche - both from better instruments and from improved simulations. We believe that computer science tools and computer scientists are in a position to help all the sciences by building tools and developing techniques to manage, analyze, and visualize peta-scale scientific information. This article is summarizes our experiences over the last seven years trying to bridge the gap between database technology and the needs of the astronomy community in building the World-Wide Telescope.",
        "published": "2005-02-02T04:40:55Z",
        "link": "http://arxiv.org/abs/cs/0502011v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Batch is back: CasJobs, serving multi-TB data on the Web",
        "authors": [
            "William OMullane",
            "Nolan Li",
            "Maria Nieto-Santisteban",
            "Alex Szalay",
            "Ani Thakar",
            "Jim Gray"
        ],
        "summary": "The Sloan Digital Sky Survey (SDSS) science database describes over 140 million objects and is over 1.5 TB in size. The SDSS Catalog Archive Server (CAS) provides several levels of query interface to the SDSS data via the SkyServer website. Most queries execute in seconds or minutes. However, some queries can take hours or days, either because they require non-index scans of the largest tables, or because they request very large result sets, or because they represent very complex aggregations of the data. These \"monster queries\" not only take a long time, they also affect response times for everyone else - one or more of them can clog the entire system. To ameliorate this problem, we developed a multi-server multi-queue batch job submission and tracking system for the CAS called CasJobs. The transfer of very large result sets from queries over the network is another serious problem. Statistics suggested that much of this data transfer is unnecessary; users would prefer to store results locally in order to allow further joins and filtering. To allow local analysis, a system was developed that gives users their own personal databases (MyDB) at the server side. Users may transfer data to their MyDB, and then perform further analysis before extracting it to their own machine. MyDB tables also provide a convenient way to share results of queries with collaborators without downloading them. CasJobs is built using SOAP XML Web services and has been in operation since May 2004.",
        "published": "2005-02-17T01:55:35Z",
        "link": "http://arxiv.org/abs/cs/0502072v1",
        "categories": [
            "cs.DC",
            "cs.DB"
        ]
    },
    {
        "title": "How far will you walk to find your shortcut: Space Efficient Synopsis   Construction Algorithms",
        "authors": [
            "Sudipto Guha"
        ],
        "summary": "In this paper we consider the wavelet synopsis construction problem without the restriction that we only choose a subset of coefficients of the original data. We provide the first near optimal algorithm. We arrive at the above algorithm by considering space efficient algorithms for the restricted version of the problem. In this context we improve previous algorithms by almost a linear factor and reduce the required space to almost linear. Our techniques also extend to histogram construction, and improve the space-running time tradeoffs for V-Opt and range query histograms. We believe the idea applies to a broad range of dynamic programs and demonstrate it by showing improvements in a knapsack-like setting seen in construction of Extended Wavelets.",
        "published": "2005-02-18T00:35:58Z",
        "link": "http://arxiv.org/abs/cs/0502075v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "First-order Complete and Computationally Complete Query Languages for   Spatio-Temporal Databases",
        "authors": [
            "Floris Geerts",
            "Sofie Haesevoets",
            "Bart Kuijpers"
        ],
        "summary": "We address a fundamental question concerning spatio-temporal database systems: ``What are exactly spatio-temporal queries?'' We define spatio-temporal queries to be computable mappings that are also generic, meaning that the result of a query may only depend to a limited extent on the actual internal representation of the spatio-temporal data. Genericity is defined as invariance under groups of geometric transformations that preserve certain characteristics of spatio-temporal data (e.g., collinearity, distance, velocity, acceleration, ...). These groups depend on the notions that are relevant in particular spatio-temporal database applications.   These transformations also have the distinctive property that they respect the monotone and unidirectional nature of time.   We investigate different genericity classes with respect to the constraint database model for spatio-temporal databases and we identify sound and complete languages for the first-order and the computable queries in these genericity classes. We distinguish between genericity determined by time-invariant transformations, genericity notions concerning physical quantities and genericity determined by time-dependent transformations.",
        "published": "2005-03-04T15:34:45Z",
        "link": "http://arxiv.org/abs/cs/0503012v2",
        "categories": [
            "cs.DB",
            "H.2.3"
        ]
    },
    {
        "title": "Theory and Practice of Transactional Method Caching",
        "authors": [
            "Daniel Pfeifer",
            "Peter C. Lockemann"
        ],
        "summary": "Nowadays, tiered architectures are widely accepted for constructing large scale information systems. In this context application servers often form the bottleneck for a system's efficiency. An application server exposes an object oriented interface consisting of set of methods which are accessed by potentially remote clients. The idea of method caching is to store results of read-only method invocations with respect to the application server's interface on the client side. If the client invokes the same method with the same arguments again, the corresponding result can be taken from the cache without contacting the server. It has been shown that this approach can considerably improve a real world system's efficiency.   This paper extends the concept of method caching by addressing the case where clients wrap related method invocations in ACID transactions. Demarcating sequences of method calls in this way is supported by many important application server standards. In this context the paper presents an architecture, a theory and an efficient protocol for maintaining full transactional consistency and in particular serializability when using a method cache on the client side. In order to create a protocol for scheduling cached method results, the paper extends a classical transaction formalism. Based on this extension, a recovery protocol and an optimistic serializability protocol are derived. The latter one differs from traditional transactional cache protocols in many essential ways. An efficiency experiment validates the approach: Using the cache a system's performance and scalability are considerably improved.",
        "published": "2005-03-09T17:53:15Z",
        "link": "http://arxiv.org/abs/cs/0503022v2",
        "categories": [
            "cs.DB",
            "H.2.4.o;H.3.4.b;C.4"
        ]
    },
    {
        "title": "Complexity and Approximation of Fixing Numerical Attributes in Databases   Under Integrity Constraints",
        "authors": [
            "L. Bertossi",
            "L. Bravo",
            "E. Franconi",
            "A. Lopatenko"
        ],
        "summary": "Consistent query answering is the problem of computing the answers from a database that are consistent with respect to certain integrity constraints that the database as a whole may fail to satisfy. Those answers are characterized as those that are invariant under minimal forms of restoring the consistency of the database. In this context, we study the problem of repairing databases by fixing integer numerical values at the attribute level with respect to denial and aggregation constraints. We introduce a quantitative definition of database fix, and investigate the complexity of several decision and optimization problems, including DFP, i.e. the existence of fixes within a given distance from the original instance, and CQA, i.e. deciding consistency of answers to aggregate conjunctive queries under different semantics. We provide sharp complexity bounds, identify relevant tractable cases; and introduce approximation algorithms for some of those that are intractable. More specifically, we obtain results like undecidability of existence of fixes for aggregation constraints; MAXSNP-hardness of DFP, but a good approximation algorithm for a relevant special case; and intractability but good approximation for CQA for aggregate queries for one database atom denials (plus built-ins).",
        "published": "2005-03-15T00:03:59Z",
        "link": "http://arxiv.org/abs/cs/0503032v3",
        "categories": [
            "cs.DB",
            "cs.CC"
        ]
    },
    {
        "title": "Mining Top-k Approximate Frequent Patterns",
        "authors": [
            "Zengyou He"
        ],
        "summary": "Frequent pattern (itemset) mining in transactional databases is one of the most well-studied problems in data mining. One obstacle that limits the practical usage of frequent pattern mining is the extremely large number of patterns generated. Such a large size of the output collection makes it difficult for users to understand and use in practice. Even restricting the output to the border of the frequent itemset collection does not help much in alleviating the problem. In this paper we address the issue of overwhelmingly large output size by introducing and studying the following problem: mining top-k approximate frequent patterns. The union of the power sets of these k sets should satisfy the following conditions: (1) including itemsets with larger support as many as possible and (2) including itemsets with smaller support as less as possible. An integrated objective function is designed to combine these two objectives. Consequently, we derive the upper bounds on objective function and present an approximate branch-and-bound method for finding the feasible solution. We give empirical evidence showing that our formulation and approximation methods work well in practice.",
        "published": "2005-03-17T11:01:25Z",
        "link": "http://arxiv.org/abs/cs/0503037v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Integrity Constraints in Trust Management",
        "authors": [
            "Sandro Etalle",
            "William H. Winsborough"
        ],
        "summary": "We introduce the use, monitoring, and enforcement of integrity constraints in trust management-style authorization systems. We consider what portions of the policy state must be monitored to detect violations of integrity constraints. Then we address the fact that not all participants in a trust management system can be trusted to assist in such monitoring, and show how many integrity constraints can be monitored in a conservative manner so that trusted participants detect and report if the system enters a policy state from which evolution in unmonitored portions of the policy could lead to a constraint violation.",
        "published": "2005-03-23T15:20:00Z",
        "link": "http://arxiv.org/abs/cs/0503061v1",
        "categories": [
            "cs.CR",
            "cs.DB",
            "K.6.5; D.4.6"
        ]
    },
    {
        "title": "On the Complexity of Nonrecursive XQuery and Functional Query Languages   on Complex Values",
        "authors": [
            "Christoph Koch"
        ],
        "summary": "This paper studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery.   We show that monad algebra with equality restricted to atomic values is complete for the class TA[2^{O(n)}, O(n)] of problems solvable in linear exponential time with a linear number of alternations. The monotone fragment of monad algebra with atomic value equality but without negation is complete for nondeterministic exponential time. For monad algebra with deep equality, we establish TA[2^{O(n)}, O(n)] lower and exponential-space upper bounds.   Then we study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with ``child'' as the only axis) is exhibited, and it is shown that these languages are expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra w.r.t. combined complexity, and that it is in TC0 if the query is assumed fixed.",
        "published": "2005-03-23T15:36:55Z",
        "link": "http://arxiv.org/abs/cs/0503062v2",
        "categories": [
            "cs.DB",
            "cs.CC",
            "F.4.1, H.2.3, I.7.2"
        ]
    },
    {
        "title": "An Optimization Model for Outlier Detection in Categorical Data",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. Detection of such outliers is important for many applications such as fraud detection and customer migration. Most existing methods are designed for numeric data. They will encounter problems with real-life applications that contain categorical data. In this paper, we formally define the problem of outlier detection in categorical data as an optimization problem from a global viewpoint. Moreover, we present a local-search heuristic based algorithm for efficiently finding feasible solutions. Experimental results on real datasets and large synthetic datasets demonstrate the superiority of our model and algorithm.",
        "published": "2005-03-29T13:31:01Z",
        "link": "http://arxiv.org/abs/cs/0503081v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Monotonic and Nonmonotonic Preference Revision",
        "authors": [
            "Jan Chomicki",
            "Joyce Song"
        ],
        "summary": "We study here preference revision, considering both the monotonic case where the original preferences are preserved and the nonmonotonic case where the new preferences may override the original ones. We use a relational framework in which preferences are represented using binary relations (not necessarily finite). We identify several classes of revisions that preserve order axioms, for example the axioms of strict partial or weak orders. We consider applications of our results to preference querying in relational databases.",
        "published": "2005-03-31T19:43:31Z",
        "link": "http://arxiv.org/abs/cs/0503092v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.3; F.4.1; I.2.3"
        ]
    },
    {
        "title": "A Scalable Stream-Oriented Framework for Cluster Applications",
        "authors": [
            "Tassos S. Argyros",
            "David R. Cheriton"
        ],
        "summary": "This paper presents a stream-oriented architecture for structuring cluster applications. Clusters that run applications based on this architecture can scale to tenths of thousands of nodes with significantly less performance loss or reliability problems. Our architecture exploits the stream nature of the data flow and reduces congestion through load balancing, hides latency behind data pushes and transparently handles node failures. In our ongoing work, we are developing an implementation for this architecture and we are able to run simple data mining applications on a cluster simulator.",
        "published": "2005-04-13T16:37:59Z",
        "link": "http://arxiv.org/abs/cs/0504051v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.NI",
            "cs.OS",
            "cs.PL"
        ]
    },
    {
        "title": "Frequency of occurrence of numbers in the World Wide Web",
        "authors": [
            "S. N. Dorogovtsev",
            "J. F. F. Mendes",
            "J. G. Oliveira"
        ],
        "summary": "The distribution of numbers in human documents is determined by a variety of diverse natural and human factors, whose relative significance can be evaluated by studying the numbers' frequency of occurrence. Although it has been studied since the 1880's, this subject remains poorly understood. Here, we obtain the detailed statistics of numbers in the World Wide Web, finding that their distribution is a heavy-tailed dependence which splits in a set of power-law ones. In particular, we find that the frequency of numbers associated to western calendar years shows an uneven behavior: 2004 represents a `singular critical' point, appearing with a strikingly high frequency; as we move away from it, the decreasing frequency allows us to compare the amounts of existing information on the past and on the future. Moreover, while powers of ten occur extremely often, allowing us to obtain statistics up to the huge 10^127, `non-round' numbers occur in a much more limited range, the variations of their frequencies being dramatically different from standard statistical fluctuations. These findings provide a view of the array of numbers used by humans as a highly non-equilibrium and inhomogeneous system, and shed a new light on an issue that, once fully investigated, could lead to a better understanding of many sociological and psychological phenomena.",
        "published": "2005-04-26T12:47:31Z",
        "link": "http://arxiv.org/abs/physics/0504185v2",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.DB",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "Tight Lower Bounds for Query Processing on Streaming and External Memory   Data",
        "authors": [
            "Martin Grohe",
            "Christoph Koch",
            "Nicole Schweikardt"
        ],
        "summary": "We study a clean machine model for external memory and stream processing. We show that the number of scans of the external data induces a strict hierarchy (as long as work space is sufficiently small, e.g., polylogarithmic in the size of the input). We also show that neither joins nor sorting are feasible if the product of the number $r(n)$ of scans of the external memory and the size $s(n)$ of the internal memory buffers is sufficiently small, e.g., of size $o(\\sqrt[5]{n})$. We also establish tight bounds for the complexity of XPath evaluation and filtering.",
        "published": "2005-04-29T22:28:31Z",
        "link": "http://arxiv.org/abs/cs/0505002v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "H.2.3; I.7.2"
        ]
    },
    {
        "title": "Efficient Management of Short-Lived Data",
        "authors": [
            "Albrecht Schmidt",
            "Christian S. Jensen"
        ],
        "summary": "Motivated by the increasing prominence of loosely-coupled systems, such as mobile and sensor networks, which are characterised by intermittent connectivity and volatile data, we study the tagging of data with so-called expiration times. More specifically, when data are inserted into a database, they may be tagged with time values indicating when they expire, i.e., when they are regarded as stale or invalid and thus are no longer considered part of the database. In a number of applications, expiration times are known and can be assigned at insertion time. We present data structures and algorithms for online management of data tagged with expiration times. The algorithms are based on fully functional, persistent treaps, which are a combination of binary search trees with respect to a primary attribute and heaps with respect to a secondary attribute. The primary attribute implements primary keys, and the secondary attribute stores expiration times in a minimum heap, thus keeping a priority queue of tuples to expire. A detailed and comprehensive experimental study demonstrates the well-behavedness and scalability of the approach as well as its efficiency with respect to a number of competitors.",
        "published": "2005-05-16T14:42:01Z",
        "link": "http://arxiv.org/abs/cs/0505038v1",
        "categories": [
            "cs.DB",
            "H.2; H.2.2"
        ]
    },
    {
        "title": "Consistent query answers on numerical databases under aggregate   constraints",
        "authors": [
            "Sergio Flesca",
            "Filippo Furfaro",
            "Francesco Parisi"
        ],
        "summary": "The problem of extracting consistent information from relational databases violating integrity constraints on numerical data is addressed. In particular, aggregate constraints defined as linear inequalities on aggregate-sum queries on input data are considered. The notion of repair as consistent set of updates at attribute-value level is exploited, and the characterization of several complexity issues related to repairing data and computing consistent query answers is provided.",
        "published": "2005-05-23T14:19:24Z",
        "link": "http://arxiv.org/abs/cs/0505059v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Unified Subspace Outlier Ensemble Framework for Outlier Detection in   High Dimensional Spaces",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. Detection of such outliers is important for many applications such as fraud detection and customer migration. Most such applications are high dimensional domains in which the data may contain hundreds of dimensions. However, the outlier detection problem itself is not well defined and none of the existing definitions are widely accepted, especially in high dimensional space. In this paper, our first contribution is to propose a unified framework for outlier detection in high dimensional spaces from an ensemble-learning viewpoint. In our new framework, the outlying-ness of each data object is measured by fusing outlier factors in different subspaces using a combination function. Accordingly, we show that all existing researches on outlier detection can be regarded as special cases in the unified framework with respect to the set of subspaces considered and the type of combination function used. In addition, to demonstrate the usefulness of the ensemble-learning based outlier detection framework, we developed a very simple and fast algorithm, namely SOE1 (Subspace Outlier Ensemble using 1-dimensional Subspaces) in which only subspaces with one dimension is used for mining outliers from large categorical datasets. The SOE1 algorithm needs only two scans over the dataset and hence is very appealing in real data mining applications. Experimental results on real datasets and large synthetic datasets show that: (1) SOE1 has comparable performance with respect to those state-of-art outlier detection algorithms on identifying true outliers and (2) SOE1 can be an order of magnitude faster than one of the fastest outlier detection algorithms known so far.",
        "published": "2005-05-24T02:41:51Z",
        "link": "http://arxiv.org/abs/cs/0505060v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Summarization Techniques for Pattern Collections in Data Mining",
        "authors": [
            "Taneli Mielikäinen"
        ],
        "summary": "Discovering patterns from data is an important task in data mining. There exist techniques to find large collections of many kinds of patterns from data very efficiently. A collection of patterns can be regarded as a summary of the data. A major difficulty with patterns is that pattern collections summarizing the data well are often very large.   In this dissertation we describe methods for summarizing pattern collections in order to make them also more understandable. More specifically, we focus on the following themes: 1) Quality value simplifications. 2) Pattern orderings. 3) Pattern chains and antichains. 4) Change profiles. 5) Inverse pattern discovery.",
        "published": "2005-05-26T04:41:15Z",
        "link": "http://arxiv.org/abs/cs/0505071v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.DS",
            "E.4; F.2; H.2.8; I.2; I.2.4"
        ]
    },
    {
        "title": "Instance-Independent View Serializability for Semistructured Databases",
        "authors": [
            "Stijn Dekeyser",
            "Jan Hidders",
            "Jan Paredaens",
            "Roel Vercammen"
        ],
        "summary": "Semistructured databases require tailor-made concurrency control mechanisms since traditional solutions for the relational model have been shown to be inadequate. Such mechanisms need to take full advantage of the hierarchical structure of semistructured data, for instance allowing concurrent updates of subtrees of, or even individual elements in, XML documents. We present an approach for concurrency control which is document-independent in the sense that two schedules of semistructured transactions are considered equivalent if they are equivalent on all possible documents. We prove that it is decidable in polynomial time whether two given schedules in this framework are equivalent. This also solves the view serializability for semistructured schedules polynomially in the size of the schedule and exponentially in the number of transactions.",
        "published": "2005-05-26T09:51:46Z",
        "link": "http://arxiv.org/abs/cs/0505074v1",
        "categories": [
            "cs.DB",
            "H.2"
        ]
    },
    {
        "title": "HepToX: Heterogeneous Peer to Peer XML Databases",
        "authors": [
            "Angela Bonifati",
            "Elaine Qing Chang",
            "Terence Ho",
            "Laks V. S. Lakshmanan"
        ],
        "summary": "We study a collection of heterogeneous XML databases maintaining similar and related information, exchanging data via a peer to peer overlay network. In this setting, a mediated global schema is unrealistic. Yet, users/applications wish to query the databases via one peer using its schema. We have recently developed HepToX, a P2P Heterogeneous XML database system. A key idea is that whenever a peer enters the system, it establishes an acquaintance with a small number of peer databases, possibly with different schema. The peer administrator provides correspondences between the local schema and the acquaintance schema using an informal and intuitive notation of arrows and boxes. We develop a novel algorithm that infers a set of precise mapping rules between the schemas from these visual annotations. We pin down a semantics of query translation given such mapping rules, and present a novel query translation algorithm for a simple but expressive fragment of XQuery, that employs the mapping rules in either direction. We show the translation algorithm is correct. Finally, we demonstrate the utility and scalability of our ideas and algorithms with a detailed set of experiments on top of the Emulab, a large scale P2P network emulation testbed.",
        "published": "2005-06-01T01:30:09Z",
        "link": "http://arxiv.org/abs/cs/0506002v1",
        "categories": [
            "cs.DB",
            "H.2.4; H.2.5"
        ]
    },
    {
        "title": "Treillis de concepts et ontologies pour l'interrogation d'un annuaire de   sources de données biologiques (BioRegistry)",
        "authors": [
            "Nizar Messai",
            "Marie-Dominique Devignes",
            "Malika Smaïl-Tabbone",
            "Amedeo Napoli"
        ],
        "summary": "Bioinformatic data sources available on the web are multiple and heterogenous. The lack of documentation and the difficulty of interaction with these data sources require users competence in both informatics and biological fields for an optimal use of sources contents that remain rather under exploited. In this paper we present an approach based on formal concept analysis to classify and search relevant bioinformatic data sources for a given query. It consists in building the concept lattice from the binary relation between bioinformatic data sources and their associated metadata. The concept built from a given query is then merged into the concept lattice. The result is given by the extraction of the set of sources belonging to the extents of the query concept subsumers in the resulting concept lattice. The sources ranking is given by the concept specificity order in the concept lattice. An improvement of the approach consists in automatic query refinement thanks to domain ontologies. Two forms of refinement are possible by generalisation and by specialisation.   -----   Les sources de donn\\'{e}es biologiques disponibles sur le web sont multiples et h\\'{e}t\\'{e}rog\\`{e}nes. L'utilisation optimale de ces ressources n\\'{e}cessite aujourd'hui de la part des utilisateurs des comp\\'{e}tences \\`{a} la fois en informatique et en biologie, du fait du manque de documentation et des difficult\\'{e}s d'interaction avec les sources de donn\\'{e}es. De fait, les contenus de ces ressources restent souvent sous-exploit\\'{e}s. Nous pr\\'{e}sentons ici une approche bas\\'{e}e sur l'analyse de concepts formels, pour organiser et rechercher des sources de donn\\'{e}es biologiques pertinentes pour une requ\\^{e}te donn\\'{e}e. Le travail consiste \\`{a} construire un treillis de concepts \\`{a} partir des m\\'{e}ta-donn\\'{e}es associ\\'{e}es aux sources. Le concept construit \\`{a} partir d'une requ\\^{e}te donn\\'{e}e est alors int\\'{e}gr\\'{e} au treillis. La r\\'{e}ponse \\`{a} la requ\\^{e}te est ensuite fournie par l'extraction des sources de donn\\'{e}es appartenant aux extensions des concepts subsumant le concept requ\\^{e}te dans le treillis. Les sources ainsi retourn\\'{e}es peuvent \\^{e}tre tri\\'{e}es selon l'ordre de sp\\'{e}cificit\\'{e} des concepts dans le treillis. Une proc\\'{e}dure de raffinement de requ\\^{e}te, bas\\'{e}e sur des ontologies du domaine, permet d'am\\'{e}liorer le rappel par g\\'{e}n\\'{e}ralisation ou par sp\\'{e}cialisation",
        "published": "2005-06-06T12:49:53Z",
        "link": "http://arxiv.org/abs/cs/0506017v1",
        "categories": [
            "cs.DB",
            "cs.IR"
        ]
    },
    {
        "title": "Database Reformulation with Integrity Constraints (extended abstract)",
        "authors": [
            "Rada Chirkova",
            "Michael R. Genesereth"
        ],
        "summary": "In this paper we study the problem of reducing the evaluation costs of queries on finite databases in presence of integrity constraints, by designing and materializing views. Given a database schema, a set of queries defined on the schema, a set of integrity constraints, and a storage limit, to find a solution to this problem means to find a set of views that satisfies the storage limit, provides equivalent rewritings of the queries under the constraints (this requirement is weaker than equivalence in the absence of constraints), and reduces the total costs of evaluating the queries. This problem, database reformulation, is important for many applications, including data warehousing and query optimization. We give complexity results and algorithms for database reformulation in presence of constraints, for conjunctive queries, views, and rewritings and for several types of constraints, including functional and inclusion dependencies. To obtain better complexity results, we introduce an unchase technique, which reduces the problem of query equivalence under constraints to equivalence in the absence of constraints without increasing query size.",
        "published": "2005-06-08T22:03:49Z",
        "link": "http://arxiv.org/abs/cs/0506026v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Priority-Based Conflict Resolution in Inconsistent Relational Databases",
        "authors": [
            "Slawomir Staworko",
            "Jan Chomicki"
        ],
        "summary": "We study here the impact of priorities on conflict resolution in inconsistent relational databases. We extend the framework of repairs and consistent query answers. We propose a set of postulates that an extended framework should satisfy and consider two instantiations of the framework: (locally preferred) l-repairs and (globally preferred) g-repairs. We study the relationships between them and the impact each notion of repair has on the computational complexity of repair checking and consistent query answers.",
        "published": "2005-06-14T23:10:47Z",
        "link": "http://arxiv.org/abs/cs/0506063v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "The MammoGrid Virtual Organisation - Federating Distributed Mammograms",
        "authors": [
            "Florida Estrella",
            "Richard McClatchey",
            "Dmitry Rogulin"
        ],
        "summary": "The MammoGrid project aims to deliver a prototype which enables the effective collaboration between radiologists using grid, service-orientation and database solutions. The grid technologies and service-based database management solution provide the platform for integrating diverse and distributed resources, creating what is called a virtual organisation. The MammoGrid Virtual Organisation facilitates the sharing and coordinated access to mammography data, medical imaging software and computing resources of participating hospitals. Hospitals manage their local database of mammograms, but in addition, radiologists who are part of this organisation can share mammograms, reports, results and image analysis software. The MammoGrid Virtual Organisation is a federation of autonomous multi-centres sites which transcends national boundaries. This paper outlines the service-based approach in the creation and management of the federated distributed mammography database and discusses the role of virtual organisations in distributed image analysis.",
        "published": "2005-07-18T12:17:57Z",
        "link": "http://arxiv.org/abs/cs/0507042v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "A Fast Greedy Algorithm for Outlier Mining",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "The task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data. In [38], the problem of outlier detection in categorical data is defined as an optimization problem and a local-search heuristic based algorithm (LSA) is presented. However, as is the case with most iterative type algorithms, the LSA algorithm is still very time-consuming on very large datasets. In this paper, we present a very fast greedy algorithm for mining outliers under the same optimization model. Experimental results on real datasets and large synthetic datasets show that: (1) Our algorithm has comparable performance with respect to those state-of-art outlier detection algorithms on identifying true outliers and (2) Our algorithm can be an order of magnitude faster than LSA algorithm.",
        "published": "2005-07-27T02:14:02Z",
        "link": "http://arxiv.org/abs/cs/0507065v1",
        "categories": [
            "cs.DB",
            "cs.AI"
        ]
    },
    {
        "title": "Conjunctive Query Containment and Answering under Description Logics   Constraints",
        "authors": [
            "Diego Calvanese",
            "Giuseppe De Giacomo",
            "Maurizio Lenzerini"
        ],
        "summary": "Query containment and query answering are two important computational tasks in databases. While query answering amounts to compute the result of a query over a database, query containment is the problem of checking whether for every database, the result of one query is a subset of the result of another query.   In this paper, we deal with unions of conjunctive queries, and we address query containment and query answering under Description Logic constraints. Every such constraint is essentially an inclusion dependencies between concepts and relations, and their expressive power is due to the possibility of using complex expressions, e.g., intersection and difference of relations, special forms of quantification, regular expressions over binary relations, in the specification of the dependencies. These types of constraints capture a great variety of data models, including the relational, the entity-relationship, and the object-oriented model, all extended with various forms of constraints, and also the basic features of the ontology languages used in the context of the Semantic Web.   We present the following results on both query containment and query answering. We provide a method for query containment under Description Logic constraints, thus showing that the problem is decidable, and analyze its computational complexity. We prove that query containment is undecidable in the case where we allow inequalities in the right-hand side query, even for very simple constraints and queries. We show that query answering under Description Logic constraints can be reduced to query containment, and illustrate how such a reduction provides upper bound results with respect to both combined and data complexity.",
        "published": "2005-07-28T08:25:43Z",
        "link": "http://arxiv.org/abs/cs/0507067v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "I.2.4; F.4.1"
        ]
    },
    {
        "title": "Iterative Algorithm for Finding Frequent Patterns in Transactional   Databases",
        "authors": [
            "Gennady P. Berman",
            "Vyacheslav N. Gorshkov",
            "Edward P. MacKerrow",
            "Xidi Wang"
        ],
        "summary": "A high-performance algorithm for searching for frequent patterns (FPs) in transactional databases is presented. The search for FPs is carried out by using an iterative sieve algorithm by computing the set of enclosed cycles. In each inner cycle of level FPs composed of elements are generated. The assigned number of enclosed cycles (the parameter of the problem) defines the maximum length of the desired FPs. The efficiency of the algorithm is produced by (i) the extremely simple logical searching scheme, (ii) the avoidance of recursive procedures, and (iii) the usage of only one-dimensional arrays of integers.",
        "published": "2005-08-26T21:05:44Z",
        "link": "http://arxiv.org/abs/cs/0508120v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "A Fresh Look at the Reliability of Long-term Digital Storage",
        "authors": [
            "Mary Baker",
            "Mehul Shah",
            "David S. H. Rosenthal",
            "Mema Roussopoulos",
            "Petros Maniatis",
            "TJ Giuli",
            "Prashanth Bungale"
        ],
        "summary": "Many emerging Web services, such as email, photo sharing, and web site archives, need to preserve large amounts of quickly-accessible data indefinitely into the future. In this paper, we make the case that these applications' demands on large scale storage systems over long time horizons require us to re-evaluate traditional storage system designs. We examine threats to long-lived data from an end-to-end perspective, taking into account not just hardware and software faults but also faults due to humans and organizations. We present a simple model of long-term storage failures that helps us reason about the various strategies for addressing these threats in a cost-effective manner. Using this model we show that the most important strategies for increasing the reliability of long-term storage are detecting latent faults quickly, automating fault repair to make it faster and cheaper, and increasing the independence of data replicas.",
        "published": "2005-08-31T01:44:35Z",
        "link": "http://arxiv.org/abs/cs/0508130v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "cs.OS"
        ]
    },
    {
        "title": "The Dynamics of Viral Marketing",
        "authors": [
            "Jure Leskovec",
            "Lada A. Adamic",
            "Bernardo A. Huberman"
        ],
        "summary": "We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We analyze how user behavior varies within user communities defined by a recommendation network. Product purchases follow a 'long tail' where a significant share of purchases belongs to rarely sold items. We establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies communities, product and pricing categories for which viral marketing seems to be very effective.",
        "published": "2005-09-05T21:41:15Z",
        "link": "http://arxiv.org/abs/physics/0509039v4",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Business intelligence systems and user's parameters: an application to a   documents' database",
        "authors": [
            "Babajide Afolabi",
            "Odile Thiery"
        ],
        "summary": "This article presents earlier results of our research works in the area of modeling Business Intelligence Systems. The basic idea of this research area is presented first. We then show the necessity of including certain users' parameters in Information systems that are used in Business Intelligence systems in order to integrate a better response from such systems. We identified two main types of attributes that can be missing from a base and we showed why they needed to be included. A user model that is based on a cognitive user evolution is presented. This model when used together with a good definition of the information needs of the user (decision maker) will accelerate his decision making process.",
        "published": "2005-09-28T08:38:15Z",
        "link": "http://arxiv.org/abs/cs/0509088v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Safe Data Sharing and Data Dissemination on Smart Devices",
        "authors": [
            "Luc Bouganim",
            "Cosmin Cremarenco",
            "François Dang Ngoc",
            "Nicolas Dieu",
            "Philippe Pucheral"
        ],
        "summary": "The erosion of trust put in traditional database servers, the growing interest for different forms of data dissemination and the concern for protecting children from suspicious Internet content are different factors that lead to move the access control from servers to clients. Several encryption schemes can be used to serve this purpose but all suffer from a static way of sharing data. In a precedent paper, we devised smarter client-based access control managers exploiting hardware security elements on client devices. The goal pursued is being able to evaluate dynamic and personalized access control rules on a ciphered XML input document, with the benefit of dissociating access rights from encryption. In this demonstration, we validate our solution using a real smart card platform and explain how we deal with the constraints usually met on hardware security elements (small memory and low throughput). Finally, we illustrate the generality of the approach and the easiness of its deployment through two different applications: a collaborative application and a parental control application on video streams.",
        "published": "2005-10-05T12:14:16Z",
        "link": "http://arxiv.org/abs/cs/0510013v1",
        "categories": [
            "cs.CR",
            "cs.DB"
        ]
    },
    {
        "title": "Semantic Optimization Techniques for Preference Queries",
        "authors": [
            "Jan Chomicki"
        ],
        "summary": "Preference queries are relational algebra or SQL queries that contain occurrences of the winnow operator (\"find the most preferred tuples in a given relation\"). Such queries are parameterized by specific preference relations. Semantic optimization techniques make use of integrity constraints holding in the database. In the context of semantic optimization of preference queries, we identify two fundamental properties: containment of preference relations relative to integrity constraints and satisfaction of order axioms relative to integrity constraints. We show numerous applications of those notions to preference query evaluation and optimization. As integrity constraints, we consider constraint-generating dependencies, a class generalizing functional dependencies. We demonstrate that the problems of containment and satisfaction of order axioms can be captured as specific instances of constraint-generating dependency entailment. This makes it possible to formulate necessary and sufficient conditions for the applicability of our techniques as constraint validity problems. We characterize the computational complexity of such problems.",
        "published": "2005-10-14T13:25:34Z",
        "link": "http://arxiv.org/abs/cs/0510036v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.LO"
        ]
    },
    {
        "title": "K-ANMI: A Mutual Information Based Clustering Algorithm for Categorical   Data",
        "authors": [
            "Zengyou He",
            "Xiaofei Xu",
            "Shengchun Deng"
        ],
        "summary": "Clustering categorical data is an integral part of data mining and has attracted much attention recently. In this paper, we present k-ANMI, a new efficient algorithm for clustering categorical data. The k-ANMI algorithm works in a way that is similar to the popular k-means algorithm, and the goodness of clustering in each step is evaluated using a mutual information based criterion (namely, Average Normalized Mutual Information-ANMI) borrowed from cluster ensemble. Experimental results on real datasets show that k-ANMI algorithm is competitive with those state-of-art categorical data clustering algorithms with respect to clustering accuracy.",
        "published": "2005-11-03T01:18:47Z",
        "link": "http://arxiv.org/abs/cs/0511013v1",
        "categories": [
            "cs.AI",
            "cs.DB"
        ]
    },
    {
        "title": "Mining Cellular Automata DataBases throug PCA Models",
        "authors": [
            "Gilson A. Giraldi",
            "Antonio A. F. Oliveira",
            "Leonardo Carvalho"
        ],
        "summary": "Cellular Automata are discrete dynamical systems that evolve following simple and local rules. Despite of its local simplicity, knowledge discovery in CA is a NP problem. This is the main motivation for using data mining techniques for CA study. The Principal Component Analysis (PCA) is a useful tool for data mining because it provides a compact and optimal description of data sets. Such feature have been explored to compute the best subspace which maximizes the projection of the I/O patterns of CA onto the principal axis. The stability of the principal components against the input patterns is the main result of this approach. In this paper we perform such analysis but in the presence of noise which randomly reverses the CA output values with probability $p$. As expected, the number of principal components increases when the pattern size is increased. However, it seems to remain stable when the pattern size is unchanged but the noise intensity gets larger. We describe our experiments and point out further works using KL transform theory and parameter sensitivity analysis.",
        "published": "2005-11-14T10:54:06Z",
        "link": "http://arxiv.org/abs/cs/0511052v1",
        "categories": [
            "cs.DM",
            "cs.DB",
            "F.1.1"
        ]
    },
    {
        "title": "Benefits of InterSite Pre-Processing and Clustering Methods in   E-Commerce Domain",
        "authors": [
            "Sergiu Theodor Chelcea",
            "Alzennyr Da Silva",
            "Yves Lechevallier",
            "Doru Tanasa",
            "Brigitte Trousse"
        ],
        "summary": "This paper presents our preprocessing and clustering analysis on the clickstream dataset proposed for the ECMLPKDD 2005 Discovery Challenge. The main contributions of this article are double. First, after presenting the clickstream dataset, we show how we build a rich data warehouse based an advanced preprocesing. We take into account the intersite aspects in the given ecommerce domain, which offers an interesting data structuration. A preliminary statistical analysis based on time period clickstreams is given, emphasing the importance of intersite user visits in such a context. Secondly, we describe our crossed-clustering method which is applied on data generated from our data warehouse. Our preliminary results are interesting and promising illustrating the benefits of our WUM methods, even if more investigations are needed on the same dataset.",
        "published": "2005-11-30T16:12:38Z",
        "link": "http://arxiv.org/abs/cs/0511106v1",
        "categories": [
            "cs.DB"
        ]
    },
    {
        "title": "Irreducible Frequent Patterns in Transactional Databases",
        "authors": [
            "Gennady P. Berman",
            "Vyacheslav N. Gorshkov",
            "Xidi Wang"
        ],
        "summary": "Irreducible frequent patters (IFPs) are introduced for transactional databases. An IFP is such a frequent pattern (FP),(x1,x2,...xn), the probability of which, P(x1,x2,...xn), cannot be represented as a product of the probabilities of two (or more) other FPs of the smaller lengths. We have developed an algorithm for searching IFPs in transactional databases. We argue that IFPs represent useful tools for characterizing the transactional databases and may have important applications to bio-systems including the immune systems and for improving vaccination strategies. The effectiveness of the IFPs approach has been illustrated in application to a classification problem.",
        "published": "2005-12-13T22:53:17Z",
        "link": "http://arxiv.org/abs/cs/0512054v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "Multipartite Secret Correlations and Bound Information",
        "authors": [
            "Lluis Masanes",
            "Antonio Acin"
        ],
        "summary": "We consider the problem of secret key extraction when $n$ honest parties and an eavesdropper share correlated information. We present a family of probability distributions and give the full characterization of its distillation properties. This formalism allows us to design a rich variety of cryptographic scenarios. In particular, we provide examples of multipartite probability distributions containing non-distillable secret correlations, also known as bound information.",
        "published": "2005-01-05T08:36:21Z",
        "link": "http://arxiv.org/abs/cs/0501008v1",
        "categories": [
            "cs.CR",
            "cs.IT",
            "math.IT",
            "quant-ph"
        ]
    },
    {
        "title": "A simple algorithm for decoding Reed-Solomon codes and its relation to   the Welch-Berlekamp algorithm",
        "authors": [
            "Sergei Fedorenko"
        ],
        "summary": "A simple and natural Gao algorithm for decoding algebraic codes is described. Its relation to the Welch-Berlekamp and Euclidean algorithms is given.",
        "published": "2005-01-06T18:55:37Z",
        "link": "http://arxiv.org/abs/cs/0501011v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Application of Generating Functions and Partial Differential Equations   in Coding Theory",
        "authors": [
            "Milan Bradonjic"
        ],
        "summary": "In this work we have considered formal power series and partial differential equations, and their relationship with Coding Theory. We have obtained the nature of solutions for the partial differential equations for Cycle Poisson Case. The coefficients for this case have been simulated, and the high tendency of growth is shown. In the light of Complex Analysis, the Hadamard Multiplication's Theorem is presented as a new approach to divide the power sums relating to the error probability, each part of which can be analyzed later.",
        "published": "2005-01-10T02:13:38Z",
        "link": "http://arxiv.org/abs/cs/0501015v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the weight distribution of convolutional codes",
        "authors": [
            "Heide Gluesing-Luerssen"
        ],
        "summary": "Detailed information about the weight distribution of a convolutional code is given by the adjacency matrix of the state diagram associated with a controller canonical form of the code. We will show that this matrix is an invariant of the code. Moreover, it will be proven that codes with the same adjacency matrix have the same dimension and the same Forney indices and finally that for one-dimensional binary convolutional codes the adjacency matrix determines the code uniquely up to monomial equivalence.",
        "published": "2005-01-10T14:08:27Z",
        "link": "http://arxiv.org/abs/cs/0501016v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "math.OC"
        ]
    },
    {
        "title": "Public Key Cryptography based on Semigroup Actions",
        "authors": [
            "G. Maze",
            "C. Monico",
            "J. Rosenthal"
        ],
        "summary": "A generalization of the original Diffie-Hellman key exchange in $(\\Z/p\\Z)^*$ found a new depth when Miller and Koblitz suggested that such a protocol could be used with the group over an elliptic curve. In this paper, we propose a further vast generalization where abelian semigroups act on finite sets. We define a Diffie-Hellman key exchange in this setting and we illustrate how to build interesting semigroup actions using finite (simple) semirings. The practicality of the proposed extensions rely on the orbit sizes of the semigroup actions and at this point it is an open question how to compute the sizes of these orbits in general and also if there exists a square root attack in general. In Section 2 a concrete practical semigroup action built from simple semirings is presented. It will require further research to analyse this system.",
        "published": "2005-01-10T19:47:13Z",
        "link": "http://arxiv.org/abs/cs/0501017v4",
        "categories": [
            "cs.CR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "No-cloning principal can alone provide security",
        "authors": [
            "Arindam Mitra"
        ],
        "summary": "Existing quantum key distribution schemes need the support of classical authentication scheme to ensure security. This is a conceptual drawback of quantum cryptography. It is pointed out that quantum cryptosystem does not need any support of classical cryptosystem to ensure security. No-cloning principal can alone provide security in communication. Even no-cloning principle itself can help to authenticate each bit of information. It implies that quantum password need not to be a secret password.",
        "published": "2005-01-12T07:28:13Z",
        "link": "http://arxiv.org/abs/cs/0501023v12",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An Empirical Study of MDL Model Selection with Infinite Parametric   Complexity",
        "authors": [
            "Steven de Rooij",
            "Peter Grunwald"
        ],
        "summary": "Parametric complexity is a central concept in MDL model selection. In practice it often turns out to be infinite, even for quite simple models such as the Poisson and Geometric families. In such cases, MDL model selection as based on NML and Bayesian inference based on Jeffreys' prior can not be used. Several ways to resolve this problem have been proposed. We conduct experiments to compare and evaluate their behaviour on small sample sizes.   We find interestingly poor behaviour for the plug-in predictive code; a restricted NML model performs quite well but it is questionable if the results validate its theoretical motivation. The Bayesian model with the improper Jeffreys' prior is the most dependable.",
        "published": "2005-01-14T15:50:28Z",
        "link": "http://arxiv.org/abs/cs/0501028v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "E.3; G.4"
        ]
    },
    {
        "title": "Simple Rate-1/3 Convolutional and Tail-Biting Quantum Error-Correcting   Codes",
        "authors": [
            "G. David Forney, Jr.",
            "Saikat Guha"
        ],
        "summary": "Simple rate-1/3 single-error-correcting unrestricted and CSS-type quantum convolutional codes are constructed from classical self-orthogonal $\\F_4$-linear and $\\F_2$-linear convolutional codes, respectively. These quantum convolutional codes have higher rate than comparable quantum block codes or previous quantum convolutional codes, and are simple to decode. A block single-error-correcting [9, 3, 3] tail-biting code is derived from the unrestricted convolutional code, and similarly a [15, 5, 3] CSS-type block code from the CSS-type convolutional code.",
        "published": "2005-01-18T22:14:53Z",
        "link": "http://arxiv.org/abs/quant-ph/0501099v2",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Thermodynamics of used punched tape: A weak and a strong equivalence   principle",
        "authors": [
            "Tommaso Toffoli"
        ],
        "summary": "We study the repeated use of a monotonic recording medium--such as punched tape or photographic plate--where marks can be added at any time but never erased. (For practical purposes, also the electromagnetic \"ether\" falls into this class.) Our emphasis is on the case where the successive users act independently and selfishly, but not maliciously; typically, the \"first user\" would be a blind natural process tending to degrade the recording medium, and the \"second user\" a human trying to make the most of whatever capacity is left.   To what extent is a length of used tape \"equivalent\"--for information transmission purposes--to a shorter length of virgin tape? Can we characterize a piece of used tape by an appropriate \"effective length\" and forget all other details? We identify two equivalence principles. The weak principle is exact, but only holds for a sequence of infinitesimal usage increments. The strong principle holds for any amount of incremental usage, but is only approximate; nonetheless, it is quite accurate even in the worst case and is virtually exact over most of the range--becoming exact in the limit of heavily used tape.   The fact that strong equivalence does not hold exactly, but then it does almost exactly, comes as a bit of a surprise.",
        "published": "2005-01-21T04:17:50Z",
        "link": "http://arxiv.org/abs/cs/0501046v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Impact of Channel Estimation Errors on Multiuser Detection via the   Replica Method",
        "authors": [
            "Husheng Li",
            "H. V. Poor"
        ],
        "summary": "For practical wireless DS-CDMA systems, channel estimation is imperfect due to noise and interference. In this paper, the impact of channel estimation errors on multiuser detection (MUD) is analyzed under the framework of the replica method. System performance is obtained in the large system limit for optimal MUD, linear MUD and turbo MUD, and is validated by numerical results for finite systems.",
        "published": "2005-01-21T16:57:35Z",
        "link": "http://arxiv.org/abs/cs/0501047v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Low Complexity Joint Iterative Equalization and Multiuser Detection in   Dispersive DS-CDMA Channels",
        "authors": [
            "Husheng Li",
            "H. V. Poor"
        ],
        "summary": "Communications in dispersive direct-sequence code-division multiple-access (DS-CDMA) channels suffer from intersymbol and multiple-access interference, which can significantly impair performance. Joint maximum \\textit{a posteriori} probability (MAP) equalization and multiuser detection with error control decoding can be used to mitigate this interference and to achieve the optimal bit error rate. Unfortunately, such optimal detection typically requires prohibitive computational complexity. This problem is addressed in this paper through the development of a reduced state trellis search detection algorithm, based on decision feedback from channel decoders. The performance of this algorithm is analyzed in the large-system limit. This analysis and simulations show that this low-complexity algorithm can obtain near-optimal performance under moderate signal-to-noise ratio and attains larger system load capacity than parallel interference cancellation.",
        "published": "2005-01-21T17:06:17Z",
        "link": "http://arxiv.org/abs/cs/0501048v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Performance Evaluation of Impulse Radio UWB Systems with Pulse-Based   Polarity Randomization",
        "authors": [
            "Sinan Gezici",
            "Hisashi Kobayashi",
            "H. Vincent Poor",
            "Andreas F. Molisch"
        ],
        "summary": "In this paper, the performance of a binary phase shift keyed random time-hopping impulse radio system with pulse-based polarity randomization is analyzed. Transmission over frequency-selective channels is considered and the effects of inter-frame interference and multiple access interference on the performance of a generic Rake receiver are investigated for both synchronous and asynchronous systems. Closed form (approximate) expressions for the probability of error that are valid for various Rake combining schemes are derived. The asynchronous system is modelled as a chip-synchronous system with uniformly distributed timing jitter for the transmitted pulses of interfering users. This model allows the analytical technique developed for the synchronous case to be extended to the asynchronous case. An approximate closed-form expression for the probability of bit error, expressed in terms of the autocorrelation function of the transmitted pulse, is derived for the asynchronous case. Then, transmission over an additive white Gaussian noise channel is studied as a special case, and the effects of multiple-access interference is investigated for both synchronous and asynchronous systems. The analysis shows that the chip-synchronous assumption can result in over-estimating the error probability, and the degree of over-estimation mainly depends on the autocorrelation function of the ultra-wideband pulse and the signal-to-interference-plus-noise-ratio of the system. Simulations studies support the approximate analysis.",
        "published": "2005-01-21T17:42:01Z",
        "link": "http://arxiv.org/abs/cs/0501049v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Energy-Efficient Joint Estimation in Sensor Networks: Analog vs. Digital",
        "authors": [
            "Shuguang Cui",
            "Jinjun Xiao",
            "Zhi-Quan Luo",
            "Andrea Goldsmith",
            "H. Vincent Poor"
        ],
        "summary": "Sensor networks in which energy is a limited resource so that energy consumption must be minimized for the intended application are considered. In this context, an energy-efficient method for the joint estimation of an unknown analog source under a given distortion constraint is proposed. The approach is purely analog, in which each sensor simply amplifies and forwards the noise-corrupted analog bservation to the fusion center for joint estimation. The total transmission power across all the sensor nodes is minimized while satisfying a distortion requirement on the joint estimate. The energy efficiency of this analog approach is compared with previously proposed digital approaches with and without coding. It is shown in our simulation that the analog approach is more energy-efficient than the digital system without coding, and in some cases outperforms the digital system with optimal coding.",
        "published": "2005-01-21T18:58:04Z",
        "link": "http://arxiv.org/abs/cs/0501050v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Capacity of Multiple Antenna Systems in Rician Fading",
        "authors": [
            "Sudharman K. Jayaweera",
            "H. Vincent Poor"
        ],
        "summary": "The effect of Rician-ness on the capacity of multiple antenna systems is investigated under the assumption that channel state information (CSI) is available only at the receiver. The average-power-constrained capacity of such systems is considered under two different assumptions on the knowledge about the fading available at the transmitter: the case in which the transmitter has no knowledge of fading at all, and the case in which the transmitter has knowledge of the distribution of the fading process but not the instantaneous CSI. The exact capacity is given for the former case while capacity bounds are derived for the latter case. A new signalling scheme is also proposed for the latter case and it is shown that by exploiting the knowledge of Rician-ness at the transmitter via this signalling scheme, significant capacity gain can be achieved. The derived capacity bounds are evaluated explicitly to provide numerical results in some representative situations.",
        "published": "2005-01-21T19:33:51Z",
        "link": "http://arxiv.org/abs/cs/0501051v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Stochastic Differential Games in a Non-Markovian Setting",
        "authors": [
            "Erhan Bayraktar",
            "H. Vincent Poor"
        ],
        "summary": "Stochastic differential games are considered in a non-Markovian setting. Typically, in stochastic differential games the modulating process of the diffusion equation describing the state flow is taken to be Markovian. Then Nash equilibria or other types of solution such as Pareto equilibria are constructed using Hamilton-Jacobi-Bellman (HJB) equations. But in a non-Markovian setting the HJB method is not applicable. To examine the non-Markovian case, this paper considers the situation in which the modulating process is a fractional Brownian motion. Fractional noise calculus is used for such models to find the Nash equilibria explicitly. Although fractional Brownian motion is taken as the modulating process because of its versatility in modeling in the fields of finance and networks, the approach in this paper has the merit of being applicable to more general Gaussian stochastic differential games with only slight conceptual modifications. This work has applications in finance to stock price modeling which incorporates the effect of institutional investors, and to stochastic differential portfolio games in markets in which the stock prices follow diffusions modulated with fractional Brownian motion.",
        "published": "2005-01-21T20:36:39Z",
        "link": "http://arxiv.org/abs/cs/0501052v2",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT"
        ]
    },
    {
        "title": "A Large Deviations Approach to Sensor Scheduling for Detection of   Correlated Random Fields",
        "authors": [
            "Youngchul Sung",
            "Lang Tong",
            "H. Vincent Poor"
        ],
        "summary": "The problem of scheduling sensor transmissions for the detection of correlated random fields using spatially deployed sensors is considered. Using the large deviations principle, a closed-form expression for the error exponent of the miss probability is given as a function of the sensor spacing and signal-to-noise ratio (SNR). It is shown that the error exponent has a distinct characteristic: at high SNR, the error exponent is monotonically increasing with respect to sensor spacing, while at low SNR there is an optimal spacing for scheduled sensors.",
        "published": "2005-01-21T23:14:31Z",
        "link": "http://arxiv.org/abs/cs/0501056v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Concavity of the auxiliary function appearing in quantum reliability   function an classical-quantum channels",
        "authors": [
            "Jun Ichi Fujii",
            "Ritsuo Nakamoto",
            "Kenjiro Yanagi"
        ],
        "summary": "Concavity of the auxiliary function which appears in the random coding exponent as the lower bound of the quantum reliability function for general quantum states is proven for s between 0 and 1.",
        "published": "2005-01-22T01:22:10Z",
        "link": "http://arxiv.org/abs/cs/0501057v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Estimation of the Number of Sources in Unbalanced Arrays via Information   Theoretic Criteria",
        "authors": [
            "Eran Fishler",
            "H. Vincent Poor"
        ],
        "summary": "Estimating the number of sources impinging on an array of sensors is a well known and well investigated problem. A common approach for solving this problem is to use an information theoretic criterion, such as Minimum Description Length (MDL) or the Akaike Information Criterion (AIC). The MDL estimator is known to be a consistent estimator, robust against deviations from the Gaussian assumption, and non-robust against deviations from the point source and/or temporally or spatially white additive noise assumptions. Over the years several alternative estimation algorithms have been proposed and tested. Usually, these algorithms are shown, using computer simulations, to have improved performance over the MDL estimator, and to be robust against deviations from the assumed spatial model. Nevertheless, these robust algorithms have high computational complexity, requiring several multi-dimensional searches.   In this paper, motivated by real life problems, a systematic approach toward the problem of robust estimation of the number of sources using information theoretic criteria is taken. An MDL type estimator that is robust against deviation from assumption of equal noise level across the array is studied. The consistency of this estimator, even when deviations from the equal noise level assumption occur, is proven. A novel low-complexity implementation method avoiding the need for multi-dimensional searches is presented as well, making this estimator a favorable choice for practical applications.",
        "published": "2005-01-22T02:53:20Z",
        "link": "http://arxiv.org/abs/cs/0501058v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Primitive Quantum BCH Codes over Finite Fields",
        "authors": [
            "Salah Aly",
            "Andreas Klappenecker",
            "Pradeep Kiran Sarvepalli"
        ],
        "summary": "An attractive feature of BCH codes is that one can infer valuable information from their design parameters (length, size of the finite field, and designed distance), such as bounds on the minimum distance and dimension of the code. In this paper, it is shown that one can also deduce from the design parameters whether or not a primitive, narrow-sense BCH contains its Euclidean or Hermitian dual code. This information is invaluable in the construction of quantum BCH codes. A new proof is provided for the dimension of BCH codes with small designed distance, and simple bounds on the minimum distance of such codes and their duals are derived as a consequence. These results allow us to derive the parameters of two families of primitive quantum BCH codes as a function of their design parameters.",
        "published": "2005-01-22T06:53:24Z",
        "link": "http://arxiv.org/abs/quant-ph/0501126v2",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Optimal and Suboptimal Finger Selection Algorithms for MMSE Rake   Receivers in Impulse Radio Ultra-Wideband Systems",
        "authors": [
            "Sinan Gezici",
            "Mung Chiang",
            "H. Vincent Poor",
            "Hisashi Kobayashi"
        ],
        "summary": "Convex relaxations of the optimal finger selection algorithm are proposed for a minimum mean square error (MMSE) Rake receiver in an impulse radio ultra-wideband system. First, the optimal finger selection problem is formulated as an integer programming problem with a non-convex objective function. Then, the objective function is approximated by a convex function and the integer programming problem is solved by means of constraint relaxation techniques. The proposed algorithms are suboptimal due to the approximate objective function and the constraint relaxation steps. However, they can be used in conjunction with the conventional finger selection algorithm, which is suboptimal on its own since it ignores the correlation between multipath components, to obtain performances reasonably close to that of the optimal scheme that cannot be implemented in practice due to its complexity. The proposed algorithms leverage convexity of the optimization problem formulations, which is the watershed between `easy' and `difficult' optimization problems.",
        "published": "2005-01-22T16:51:26Z",
        "link": "http://arxiv.org/abs/cs/0501061v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On The Tradeoff Between Two Types of Processing Gain",
        "authors": [
            "Eran Fishler",
            "H. Vincent Poor"
        ],
        "summary": "One of the features characterizing almost every multiple access (MA) communication system is the processing gain. Through the use of spreading sequences, the processing gain of Random CDMA systems (RCDMA), is devoted to both bandwidth expansion and orthogonalization of the signals transmitted by different users. Another type of multiple access system is Impulse Radio (IR). In many aspects, IR systems are similar to time division multiple access (TDMA) systems, and the processing gain of IR systems represents the ratio between the actual transmission time and the total time between two consecutive ransmissions (on-plus-off to on ratio). While CDMA systems, which constantly excite the channel, rely on spreading sequences to orthogonalize the signals transmitted by different users, IR systems transmit a series of short pulses and the orthogonalization between the signals transmitted by different users is achieved by the fact that most of the pulses do not collide with each other at the receiver.   In this paper, a general class of MA communication systems that use both types of processing gain is presented, and both IR and RCDMA systems are demonstrated to be two special cases of this more general class of systems. The bit error rate (BER) of several receivers as a function of the ratio between the two types of processing gain is analyzed and compared under the constraint that the total processing gain of the system is large and fixed. It is demonstrated that in non inter-symbol interference (ISI) channels there is no tradeoff between the two types of processing gain. However, in ISI channels a tradeoff between the two types of processing gain exists. In addition, the sub-optimality of RCDMA systems in frequency selective channels is established.",
        "published": "2005-01-22T17:11:00Z",
        "link": "http://arxiv.org/abs/cs/0501062v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Bandit Problems with Side Observations",
        "authors": [
            "Chih-Chun Wang",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "An extension of the traditional two-armed bandit problem is considered, in which the decision maker has access to some side information before deciding which arm to pull. At each time t, before making a selection, the decision maker is able to observe a random variable X_t that provides some information on the rewards to be obtained. The focus is on finding uniformly good rules (that minimize the growth rate of the inferior sampling time) and on quantifying how much the additional information helps. Various settings are considered and for each setting, lower bounds on the achievable inferior sampling time are developed and asymptotically optimal adaptive schemes achieving these lower bounds are constructed.",
        "published": "2005-01-22T22:07:18Z",
        "link": "http://arxiv.org/abs/cs/0501063v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "A Non-Cooperative Power Control Game for Multi-Carrier CDMA Systems",
        "authors": [
            "Farhad Meshkati",
            "Mung Chiang",
            "Stuart C. Schwartz",
            "H. Vincent Poor",
            "Narayan B. Mandayam"
        ],
        "summary": "In this work, a non-cooperative power control game for multi-carrier CDMA systems is proposed. In the proposed game, each user needs to decide how much power to transmit over each carrier to maximize its overall utility. The utility function considered here measures the number of reliable bits transmitted per joule of energy consumed. It is shown that the user's utility is maximized when the user transmits only on the carrier with the best \"effective channel\". The existence and uniqueness of Nash equilibrium for the proposed game are investigated and the properties of equilibrium are studied. Also, an iterative and distributed algorithm for reaching the equilibrium (if it exists) is presented. It is shown that the proposed approach results in a significant improvement in the total utility achieved at equilibrium compared to the case in which each user maximizes its utility over each carrier independently.",
        "published": "2005-01-22T22:18:06Z",
        "link": "http://arxiv.org/abs/cs/0501064v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Arbitrage in Fractal Modulated Markets When the Volatility is Stochastic",
        "authors": [
            "Erhan Bayraktar",
            "H. Vincent Poor"
        ],
        "summary": "In this paper an arbitrage strategy is constructed for the modified Black-Scholes model driven by fractional Brownian motion or by a time changed fractional Brownian motion, when the volatility is stochastic. This latter property allows the heavy tailedness of the log returns of the stock prices to be also accounted for in addition to the long range dependence introduced by the fractional Brownian motion. Work has been done previously on this problem for the case with constant `volatility' and without a time change; here these results are extended to the case of stochastic volatility models when the modulator is fractional Brownian motion or a time change of it. (Volatility in fractional Black-Scholes models does not carry the same meaning as in the classic Black-Scholes framework, which is made clear in the text.)   Since fractional Brownian motion is not a semi-martingale, the Black-Scholes differential equation is not well-defined sense for arbitrary predictable volatility processes. However, it is shown here that any almost surely continuous and adapted process having zero quadratic variation can act as an integrator over functions of the integrator and over the family of continuous adapted semi-martingales. Moreover it is shown that the integral also has zero quadratic variation, and therefore that the integral itself can be an integrator. This property of the integral is crucial in developing the arbitrage strategy. Since fractional Brownian motion and a time change of fractional Brownian motion have zero quadratic variation, these results are applicable to these cases in particular. The appropriateness of fractional Brownian motion as a means of modeling stock price returns is discussed as well.",
        "published": "2005-01-22T23:51:01Z",
        "link": "http://arxiv.org/abs/cs/0501054v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT"
        ]
    },
    {
        "title": "Consistency Problems for Jump-Diffusion Models",
        "authors": [
            "Erhan Bayraktar",
            "Li Chen",
            "H. Vincent Poor"
        ],
        "summary": "In this paper consistency problems for multi-factor jump-diffusion models, where the jump parts follow multivariate point processes are examined. First the gap between jump-diffusion models and generalized Heath-Jarrow-Morton (HJM) models is bridged. By applying the drift condition for a generalized arbitrage-free HJM model, the consistency condition for jump-diffusion models is derived. Then we consider a case in which the forward rate curve has a separable structure, and obtain a specific version of the general consistency condition. In particular, a necessary and sufficient condition for a jump-diffusion model to be affine is provided. Finally the Nelson-Siegel type of forward curve structures is discussed. It is demonstrated that under regularity condition, there exists no jump-diffusion model consistent with the Nelson-Siegel curves.",
        "published": "2005-01-23T00:12:44Z",
        "link": "http://arxiv.org/abs/cs/0501055v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT"
        ]
    },
    {
        "title": "The Noncoherent Rician Fading Channel -- Part I : Structure of the   Capacity-Achieving Input",
        "authors": [
            "Mustafa Cenk Gursoy",
            "H. Vincent Poor",
            "Sergio Verdu"
        ],
        "summary": "Transmission of information over a discrete-time memoryless Rician fading channel is considered where neither the receiver nor the transmitter knows the fading coefficients. First the structure of the capacity-achieving input signals is investigated when the input is constrained to have limited peakedness by imposing either a fourth moment or a peak constraint. When the input is subject to second and fourth moment limitations, it is shown that the capacity-achieving input amplitude distribution is discrete with a finite number of mass points in the low-power regime. A similar discrete structure for the optimal amplitude is proven over the entire SNR range when there is only a peak power constraint. The Rician fading with phase-noise channel model, where there is phase uncertainty in the specular component, is analyzed. For this model it is shown that, with only an average power constraint, the capacity-achieving input amplitude is discrete with a finite number of levels. For the classical average power limited Rician fading channel, it is proven that the optimal input amplitude distribution has bounded support.",
        "published": "2005-01-24T07:26:49Z",
        "link": "http://arxiv.org/abs/cs/0501066v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The Noncoherent Rician Fading Channel -- Part II : Spectral Efficiency   in the Low-Power Regime",
        "authors": [
            "Mustafa Cenk Gursoy",
            "H. Vincent Poor",
            "Sergio Verdu"
        ],
        "summary": "Transmission of information over a discrete-time memoryless Rician fading channel is considered where neither the receiver nor the transmitter knows the fading coefficients. The spectral-efficiency/bit-energy tradeoff in the low-power regime is examined when the input has limited peakedness. It is shown that if a fourth moment input constraint is imposed or the input peak-to-average power ratio is limited, then in contrast to the behavior observed in average power limited channels, the minimum bit energy is not always achieved at zero spectral efficiency. The low-power performance is also characterized when there is a fixed peak limit that does not vary with the average power. A new signaling scheme that overlays phase-shift keying on on-off keying is proposed and shown to be optimally efficient in the low-power regime.",
        "published": "2005-01-24T07:34:37Z",
        "link": "http://arxiv.org/abs/cs/0501067v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Capacity Regions and Optimal Power Allocation for Groupwise Multiuser   Detection",
        "authors": [
            "C. Comaniciu",
            "H. V. Poor"
        ],
        "summary": "In this paper, optimal power allocation and capacity regions are derived for GSIC (groupwise successive interference cancellation) systems operating in multipath fading channels, under imperfect channel estimation conditions. It is shown that the impact of channel estimation errors on the system capacity is two-fold: it affects the receivers' performance within a group of users, as well as the cancellation performance (through cancellation errors). An iterative power allocation algorithm is derived, based on which it can be shown that the total required received power is minimized when the groups are ordered according to their cancellation errors, and the first detected group has the smallest cancellation error.   Performace/complexity tradeoff issues are also discussed by directly comparing the system capacity for different implementations: GSIC with linear minimum-mean-square error (LMMSE) receivers within the detection groups, GSIC with matched filter receivers, multicode LMMSE systems, and simple all matched filter receivers systems.",
        "published": "2005-01-24T21:20:05Z",
        "link": "http://arxiv.org/abs/cs/0501071v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "C.2.1"
        ]
    },
    {
        "title": "A generalized skew information and uncertainty relation",
        "authors": [
            "Kenjiro Yanagi",
            "Shigeru Furuichi",
            "Ken Kuriyama"
        ],
        "summary": "A generalized skew information is defined and a generalized uncertainty relation is established with the help of a trace inequality which was recently proven by J.I.Fujii. In addition, we prove the trace inequality conjectured by S.Luo and Z.Zhang. Finally we point out that Theorem 1 in {\\it S.Luo and Q.Zhang, IEEE Trans.IT, Vol.50, pp.1778-1782 (2004)} is incorrect in general, by giving a simple counter-example.",
        "published": "2005-01-26T00:36:45Z",
        "link": "http://arxiv.org/abs/quant-ph/0501152v2",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Tree Search Method for Iterative Decoding of Underdetermined Multiuser   Systems",
        "authors": [
            "Adriel Kind",
            "Alex Grant"
        ],
        "summary": "Application of the turbo principle to multiuser decoding results in an exchange of probability distributions between two sets of constraints. Firstly, constraints imposed by the multiple-access channel, and secondly, individual constraints imposed by each users' error control code. A-posteriori probability computation for the first set of constraints is prohibitively complex for all but a small number of users. Several lower complexity approaches have been proposed in the literature. One class of methods is based on linear filtering (e.g. LMMSE). A more recent approach is to compute approximations to the posterior probabilities by marginalising over a subset of sequences (list detection). Most of the list detection methods are restricted to non-singular systems. In this paper, we introduce a transformation that permits application of standard tree-search methods to underdetermined systems. We find that the resulting tree-search based receiver outperforms existing methods.",
        "published": "2005-01-28T00:58:34Z",
        "link": "http://arxiv.org/abs/cs/0501081v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Group-Theoretic Approach to the WSSUS Pulse Design Problem",
        "authors": [
            "Peter Jung",
            "Gerhard Wunder"
        ],
        "summary": "We consider the pulse design problem in multicarrier transmission where the pulse shapes are adapted to the second order statistics of the WSSUS channel. Even though the problem has been addressed by many authors analytical insights are rather limited. First we show that the problem is equivalent to the pure state channel fidelity in quantum information theory. Next we present a new approach where the original optimization functional is related to an eigenvalue problem for a pseudo differential operator by utilizing unitary representations of the Weyl--Heisenberg group.A local approximation of the operator for underspread channels is derived which implicitly covers the concepts of pulse scaling and optimal phase space displacement. The problem is reformulated as a differential equation and the optimal pulses occur as eigenstates of the harmonic oscillator Hamiltonian. Furthermore this operator--algebraic approach is extended to provide exact solutions for different classes of scattering environments.",
        "published": "2005-01-28T14:10:14Z",
        "link": "http://arxiv.org/abs/cs/0501082v4",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Space Frequency Codes from Spherical Codes",
        "authors": [
            "Oliver Henkel"
        ],
        "summary": "A new design method for high rate, fully diverse ('spherical') space frequency codes for MIMO-OFDM systems is proposed, which works for arbitrary numbers of antennas and subcarriers. The construction exploits a differential geometric connection between spherical codes and space time codes. The former are well studied e.g. in the context of optimal sequence design in CDMA systems, while the latter serve as basic building blocks for space frequency codes. In addition a decoding algorithm with moderate complexity is presented. This is achieved by a lattice based construction of spherical codes, which permits lattice decoding algorithms and thus offers a substantial reduction of complexity.",
        "published": "2005-01-28T21:37:37Z",
        "link": "http://arxiv.org/abs/cs/0501085v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Information estimations and analysis of structures",
        "authors": [
            "Alexander Shaydurov"
        ],
        "summary": "In this paper have written the results of the information analysis of structures. The obtained information estimation (IE) are based on an entropy measure of C. Shannon. Obtained IE is univalent both for the non-isomorphic and for the isomorphic graphs, algorithmically, it is asymptotically steady and has vector character. IE can be used for the solution of the problems ranking of structures by the preference, the evaluation of the structurization of subject area, the solution of the problems of structural optimization. Information estimations and method of the information analysis of structures it can be used in many fields of knowledge (Electrical Systems and Circuit, Image recognition, Computer technology, Databases and Bases of knowledge, Organic chemistry, Biology and others) and it can be base for the structure calculus.",
        "published": "2005-01-30T18:03:51Z",
        "link": "http://arxiv.org/abs/cs/0501088v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4,H.1.1"
        ]
    },
    {
        "title": "Stochastic Iterative Decoders",
        "authors": [
            "Chris Winstead",
            "Anthony Rapley",
            "Vincent C. Gaudet",
            "Christian Schlegel"
        ],
        "summary": "This paper presents a stochastic algorithm for iterative error control decoding. We show that the stochastic decoding algorithm is an approximation of the sum-product algorithm. When the code's factor graph is a tree, as with trellises, the algorithm approaches maximum a-posteriori decoding. We also demonstrate a stochastic approximations to the alternative update rule known as successive relaxation. Stochastic decoders have very simple digital implementations which have almost no RAM requirements. We present example stochastic decoders for a trellis-based Hamming code, and for a Block Turbo code constructed from Hamming codes.",
        "published": "2005-01-30T19:57:09Z",
        "link": "http://arxiv.org/abs/cs/0501090v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A complexity-regularized quantization approach to nonlinear   dimensionality reduction",
        "authors": [
            "Maxim Raginsky"
        ],
        "summary": "We consider the problem of nonlinear dimensionality reduction: given a training set of high-dimensional data whose ``intrinsic'' low dimension is assumed known, find a feature extraction map to low-dimensional space, a reconstruction map back to high-dimensional space, and a geometric description of the dimension-reduced data as a smooth manifold. We introduce a complexity-regularized quantization approach for fitting a Gaussian mixture model to the training set via a Lloyd algorithm. Complexity regularization controls the trade-off between adaptation to the local shape of the underlying manifold and global geometric consistency. The resulting mixture model is used to design the feature extraction and reconstruction maps and to define a Riemannian metric on the low-dimensional data. We also sketch a proof of consistency of our scheme for the purposes of estimating the unknown underlying pdf of high-dimensional data.",
        "published": "2005-01-31T00:08:29Z",
        "link": "http://arxiv.org/abs/cs/0501091v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Some Extensions of Gallager's Method to General Sources and Channels",
        "authors": [
            "Shengtian Yang",
            "Peiliang Qiu"
        ],
        "summary": "The Gallager bound is well known in the area of channel coding. However, most discussions about it mainly focus on its applications to memoryless channels. We show in this paper that the bounds obtained by Gallager's method are very tight even for general sources and channels that are defined in the information-spectrum theory. Our method is mainly based on the estimations of error exponents in those bounds, and by these estimations we proved the direct part of the Slepian-Wolf theorem and channel coding theorem for general sources and channels.",
        "published": "2005-02-01T02:19:01Z",
        "link": "http://arxiv.org/abs/cs/0502001v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Asymptotic Log-loss of Prequential Maximum Likelihood Codes",
        "authors": [
            "Peter Grunwald",
            "Steven de Rooij"
        ],
        "summary": "We analyze the Dawid-Rissanen prequential maximum likelihood codes relative to one-parameter exponential family models M. If data are i.i.d. according to an (essentially) arbitrary P, then the redundancy grows at rate c/2 ln n. We show that c=v1/v2, where v1 is the variance of P, and v2 is the variance of the distribution m* in M that is closest to P in KL divergence. This shows that prequential codes behave quite differently from other important universal codes such as the 2-part MDL, Shtarkov and Bayes codes, for which c=1. This behavior is undesirable in an MDL model selection setting.",
        "published": "2005-02-01T13:42:49Z",
        "link": "http://arxiv.org/abs/cs/0502004v1",
        "categories": [
            "cs.LG",
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Estimating mutual information and multi--information in large networks",
        "authors": [
            "Noam Slonim",
            "Gurinder S. Atwal",
            "Gasper Tkacik",
            "William Bialek"
        ],
        "summary": "We address the practical problems of estimating the information relations that characterize large networks. Building on methods developed for analysis of the neural code, we show that reliable estimates of mutual information can be obtained with manageable computational effort. The same methods allow estimation of higher order, multi--information terms. These ideas are illustrated by analyses of gene expression, financial markets, and consumer preferences. In each case, information theoretic measures correlate with independent, intuitive measures of the underlying structures in the system.",
        "published": "2005-02-03T21:11:54Z",
        "link": "http://arxiv.org/abs/cs/0502017v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Idempotents, Mattson-Solomon Polynomials and Binary LDPC codes",
        "authors": [
            "R. Horan",
            "C. Tjhai",
            "M. Tomlinson",
            "M. Ambroze",
            "M. Ahmed"
        ],
        "summary": "We show how to construct an algorithm to search for binary idempotents which may be used to construct binary LDPC codes. The algorithm, which allows control of the key properties of sparseness, code rate and minimum distance, is constructed in the Mattson-Solomon domain. Some of the new codes, found by using this technique, are displayed.",
        "published": "2005-02-04T10:08:07Z",
        "link": "http://arxiv.org/abs/cs/0502024v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "H.1.1"
        ]
    },
    {
        "title": "Pseudo-Codewords of Cycle Codes via Zeta Functions",
        "authors": [
            "Ralf Koetter",
            "Wen-Ching W. Li",
            "Pascal O. Vontobel",
            "Judy L. Walker"
        ],
        "summary": "Cycle codes are a special case of low-density parity-check (LDPC) codes and as such can be decoded using an iterative message-passing decoding algorithm on the associated Tanner graph. The existence of pseudo-codewords is known to cause the decoding algorithm to fail in certain instances. In this paper, we draw a connection between pseudo-codewords of cycle codes and the so-called edge zeta function of the associated normal graph and show how the Newton polyhedron of the zeta function equals the fundamental cone of the code, which plays a crucial role in characterizing the performance of iterative decoding algorithms.",
        "published": "2005-02-06T03:26:37Z",
        "link": "http://arxiv.org/abs/cs/0502033v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Near Maximum-Likelihood Performance of Some New Cyclic Codes Constructed   in the Finite-Field Transform Domain",
        "authors": [
            "C. Tjhai",
            "M. Tomlinson",
            "R. Horan",
            "M. Ambroze",
            "M. Ahmed"
        ],
        "summary": "It is shown that some well-known and some new cyclic codes with orthogonal parity-check equations can be constructed in the finite-field transform domain. It is also shown that, for some binary linear cyclic codes, the performance of the iterative decoder can be improved by substituting some of the dual code codewords in the parity-check matrix with other dual code codewords formed from linear combinations. This technique can bring the performance of a code closer to its maximum-likelihood performance, which can be derived from the erroneous decoded codeword whose euclidean distance with the respect to the received block is smaller than that of the correct codeword. For (63,37), (93,47) and (105,53) cyclic codes, the maximum-likelihood performance is realised with this technique.",
        "published": "2005-02-07T08:50:24Z",
        "link": "http://arxiv.org/abs/cs/0502035v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Improved Iterative Decoding for Perpendicular Magnetic Recording",
        "authors": [
            "E. Papagiannis",
            "C. Tjhai",
            "M. Ahmed",
            "M. Ambroze",
            "M. Tomlinson"
        ],
        "summary": "An algorithm of improving the performance of iterative decoding on perpendicular magnetic recording is presented. This algorithm follows on the authors' previous works on the parallel and serial concatenated turbo codes and low-density parity-check codes. The application of this algorithm with signal-to-noise ratio mismatch technique shows promising results in the presence of media noise. We also show that, compare to the standard iterative decoding algorithm, an improvement of within one order of magnitude can be achieved.",
        "published": "2005-02-07T11:23:53Z",
        "link": "http://arxiv.org/abs/cs/0502036v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "GF(2^m) Low-Density Parity-Check Codes Derived from Cyclotomic Cosets",
        "authors": [
            "C. Tjhai",
            "M. Tomlinson",
            "R. Horan",
            "M. Ambroze",
            "M. Ahmed"
        ],
        "summary": "Based on the ideas of cyclotomic cosets, idempotents and Mattson-Solomon polynomials, we present a new method to construct GF(2^m), where m>0 cyclic low-density parity-check codes. The construction method produces the dual code idempotent which is used to define the parity-check matrix of the low-density parity-check code. An interesting feature of this construction method is the ability to increment the code dimension by adding more idempotents and so steadily decrease the sparseness of the parity-check matrix. We show that the constructed codes can achieve performance very close to the sphere-packing-bound constrained for binary transmission.",
        "published": "2005-02-07T11:37:56Z",
        "link": "http://arxiv.org/abs/cs/0502037v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Unified Large System Analysis of MMSE and Adaptive Least Squares   Receivers for a class of Random Matrix Channels",
        "authors": [
            "Matthew J. M. Peacock",
            "Iain B. Collings",
            "Michael L. Honig"
        ],
        "summary": "We present a unified large system analysis of linear receivers for a class of random matrix channels. The technique unifies the analysis of both the minimum-mean-squared-error (MMSE) receiver and the adaptive least-squares (ALS) receiver, and also uses a common approach for both random i.i.d. and random orthogonal precoding. We derive expressions for the asymptotic signal-to-interference-plus-noise (SINR) of the MMSE receiver, and both the transient and steady-state SINR of the ALS receiver, trained using either i.i.d. data sequences or orthogonal training sequences. The results are in terms of key system parameters, and allow for arbitrary distributions of the power of each of the data streams and the eigenvalues of the channel correlation matrix. In the case of the ALS receiver, we allow a diagonal loading constant and an arbitrary data windowing function. For i.i.d. training sequences and no diagonal loading, we give a fundamental relationship between the transient/steady-state SINR of the ALS and the MMSE receivers. We demonstrate that for a particular ratio of receive to transmit dimensions and window shape, all channels which have the same MMSE SINR have an identical transient ALS SINR response. We demonstrate several applications of the results, including an optimization of information throughput with respect to training sequence length in coded block transmission.",
        "published": "2005-02-08T04:31:30Z",
        "link": "http://arxiv.org/abs/cs/0502042v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Generalised Bent Criteria for Boolean Functions (I)",
        "authors": [
            "Constanza Riera",
            "Matthew G. Parker"
        ],
        "summary": "Generalisations of the bent property of a boolean function are presented, by proposing spectral analysis with respect to a well-chosen set of local unitary transforms. Quadratic boolean functions are related to simple graphs and it is shown that the orbit generated by successive Local Complementations on a graph can be found within the transform spectra under investigation. The flat spectra of a quadratic boolean function are related to modified versions of its associated adjacency matrix.",
        "published": "2005-02-09T15:42:17Z",
        "link": "http://arxiv.org/abs/cs/0502049v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Generalised Bent Criteria for Boolean Functions (II)",
        "authors": [
            "Constanza Riera",
            "George Petrides",
            "Matthew G. Parker"
        ],
        "summary": "In the first part of this paper [16], some results on how to compute the flat spectra of Boolean constructions w.r.t. the transforms {I,H}^n, {H,N}^n and {I,H,N}^n were presented, and the relevance of Local Complementation to the quadratic case was indicated. In this second part, the results are applied to develop recursive formulae for the numbers of flat spectra of some structural quadratics. Observations are made as to the generalised Bent properties of boolean functions of algebraic degree greater than two, and the number of flat spectra w.r.t. {I,H,N}^n are computed for some of them.",
        "published": "2005-02-09T15:55:14Z",
        "link": "http://arxiv.org/abs/cs/0502050v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A low-cost time-hopping impulse radio system for high data rate   transmission",
        "authors": [
            "Andreas F. Molisch",
            "Ye Geoffrey Li",
            "Yves-Paul Nakache",
            "Philip Orlik",
            "Makoto Miyake",
            "Yunnan Wu",
            "Sinan Gezici",
            "Harry Sheng",
            "S. Y. Kung",
            "H. Kobayashi",
            "H. Vincent Poor",
            "Alexander Haimovich",
            "Jinyun Zhang"
        ],
        "summary": "We present an efficient, low-cost implementation of time-hopping impulse radio that fulfills the spectral mask mandated by the FCC and is suitable for high-data-rate, short-range communications. Key features are: (i) all-baseband implementation that obviates the need for passband components, (ii) symbol-rate (not chip rate) sampling, A/D conversion, and digital signal processing, (iii) fast acquisition due to novel search algorithms, (iv) spectral shaping that can be adapted to accommodate different spectrum regulations and interference environments. Computer simulations show that this system can provide 110Mbit/s at 7-10m distance, as well as higher data rates at shorter distances under FCC emissions limits. Due to the spreading concept of time-hopping impulse radio, the system can sustain multiple simultaneous users, and can suppress narrowband interference effectively.",
        "published": "2005-02-10T03:49:25Z",
        "link": "http://arxiv.org/abs/cs/0502053v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On quasi-cyclic interleavers for parallel turbo codes",
        "authors": [
            "Joseph Boutros",
            "Gilles Zémor"
        ],
        "summary": "We present an interleaving scheme that yields quasi-cyclic turbo codes. We prove that randomly chosen members of this family yield with probability almost 1 turbo codes with asymptotically optimum minimum distance, i.e. growing as a logarithm of the interleaver size. These interleavers are also very practical in terms of memory requirements and their decoding error probabilities for small block lengths compare favorably with previous interleaving schemes.",
        "published": "2005-02-10T15:26:15Z",
        "link": "http://arxiv.org/abs/cs/0502055v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E4"
        ]
    },
    {
        "title": "Nonlinear MMSE Multiuser Detection Based on Multivariate Gaussian   Approximation",
        "authors": [
            "Peng Hui Tan",
            "Lars K. Rasmussen"
        ],
        "summary": "In this paper, a class of nonlinear MMSE multiuser detectors are derived based on a multivariate Gaussian approximation of the multiple access interference. This approach leads to expressions identical to those describing the probabilistic data association (PDA) detector, thus providing an alternative analytical justification for this structure. A simplification to the PDA detector based on approximating the covariance matrix of the multivariate Gaussian distribution is suggested, resulting in a soft interference cancellation scheme. Corresponding multiuser soft-input, soft-output detectors delivering extrinsic log-likelihood ratios are derived for application in iterative multiuser decoders. Finally, a large system performance analysis is conducted for the simplified PDA, showing that the bit error rate performance of this detector can be accurately predicted and related to the replica method analysis for the optimal detector. Methods from statistical neuro-dynamics are shown to provide a closely related alternative large system prediction. Numerical results demonstrate that for large systems, the bit error rate is accurately predicted by the analysis and found to be close to optimal performance.",
        "published": "2005-02-14T14:44:19Z",
        "link": "http://arxiv.org/abs/cs/0502063v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Strong Asymptotic Assertions for Discrete MDL in Regression and   Classification",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "We study the properties of the MDL (or maximum penalized complexity) estimator for Regression and Classification, where the underlying model class is countable. We show in particular a finite bound on the Hellinger losses under the only assumption that there is a \"true\" model contained in the class. This implies almost sure convergence of the predictive distribution to the true one at a fast rate. It corresponds to Solomonoff's central theorem of universal induction, however with a bound that is exponentially larger.",
        "published": "2005-02-15T16:26:36Z",
        "link": "http://arxiv.org/abs/math/0502315v1",
        "categories": [
            "math.ST",
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.PR",
            "stat.TH"
        ]
    },
    {
        "title": "Analysis of Second-order Statistics Based Semi-blind Channel Estimation   in CDMA Channels",
        "authors": [
            "Husheng Li",
            "H. Vincent Poor"
        ],
        "summary": "The performance of second order statistics (SOS) based semi-blind channel estimation in long-code DS-CDMA systems is analyzed. The covariance matrix of SOS estimates is obtained in the large system limit, and is used to analyze the large-sample performance of two SOS based semi-blind channel estimation algorithms. A notion of blind estimation efficiency is also defined and is examined via simulation results.",
        "published": "2005-02-17T01:48:34Z",
        "link": "http://arxiv.org/abs/cs/0502071v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Achievable Information Rates of Finite-State Input   Two-Dimensional Channels with Memory",
        "authors": [
            "Ori Shental",
            "Noam Shental",
            "Shlomo Shamai"
        ],
        "summary": "The achievable information rate of finite-state input two-dimensional (2-D) channels with memory is an open problem, which is relevant, e.g., for inter-symbol-interference (ISI) channels and cellular multiple-access channels. We propose a method for simulation-based computation of such information rates. We first draw a connection between the Shannon-theoretic information rate and the statistical mechanics notion of free energy. Since the free energy of such systems is intractable, we approximate it using the cluster variation method, implemented via generalized belief propagation. The derived, fully tractable, algorithm is shown to provide a practically accurate estimate of the information rate. In our experimental study we calculate the information rates of 2-D ISI channels and of hexagonal Wyner cellular networks with binary inputs, for which formerly only bounds were known.",
        "published": "2005-02-18T08:29:47Z",
        "link": "http://arxiv.org/abs/cs/0502077v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Multilevel expander codes",
        "authors": [
            "Alexander Barg",
            "Gilles Zemor"
        ],
        "summary": "We define multilevel codes on bipartite graphs that have properties analogous to multilevel serial concatenations. A decoding algorithm is described that corrects a proportion of errors equal to half the Blokh-Zyablov bound on the minimum distance. The error probability of this algorithm has exponent similar to that of serially concatenated multilevel codes.",
        "published": "2005-02-19T02:48:34Z",
        "link": "http://arxiv.org/abs/cs/0502079v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Sensor Configuration and Activation for Field Detection in Large Sensor   Arrays",
        "authors": [
            "Youngchul Sung",
            "Lang Tong",
            "H. Vincent Poor"
        ],
        "summary": "The problems of sensor configuration and activation for the detection of correlated random fields using large sensor arrays are considered. Using results that characterize the large-array performance of sensor networks in this application, the detection capabilities of different sensor configurations are analyzed and compared. The dependence of the optimal choice of configuration on parameters such as sensor signal-to-noise ratio (SNR), field correlation, etc., is examined, yielding insights into the most effective choices for sensor selection and activation in various operating regimes.",
        "published": "2005-02-19T17:04:59Z",
        "link": "http://arxiv.org/abs/cs/0502080v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E. 4; H.11"
        ]
    },
    {
        "title": "Impulse Radio Systems with Multiple Types of Ultra-Wideband Pulses",
        "authors": [
            "Sinan Gezici",
            "Zafer Sahinoglu",
            "Hisashi Kobayashi",
            "H. Vincent Poor"
        ],
        "summary": "Spectral properties and performance of multi-pulse impulse radio ultra-wideband systems with pulse-based polarity randomization are analyzed. Instead of a single type of pulse transmitted in each frame, multiple types of pulses are considered, which is shown to reduce the effects of multiple-access interference. First, the spectral properties of a multi-pulse impulse radio system is investigated. It is shown that the power spectral density is the average of spectral contents of different pulse shapes. Then, approximate closed-form expressions for bit error probability of a multi-pulse impulse radio system are derived for RAKE receivers in asynchronous multiuser environments. The theoretical and simulation results indicate that impulse radio systems that are more robust against multiple-access interference than a \"classical\" impulse radio system can be designed with multiple types of ultra-wideband pulses.",
        "published": "2005-02-22T00:58:18Z",
        "link": "http://arxiv.org/abs/cs/0502083v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Typicality of the Linear Code Among the LDPC Coset Code Ensemble",
        "authors": [
            "Chih-Chun Wang",
            "H. Vincent Poor",
            "Sanjeev R. Kulkarni"
        ],
        "summary": "Density evolution (DE) is one of the most powerful analytical tools for low-density parity-check (LDPC) codes on memoryless binary-input/symmetric-output channels. The case of non-symmetric channels is tackled either by the LDPC coset code ensemble (a channel symmetrizing argument) or by the generalized DE for linear codes on non-symmetric channels. Existing simulations show that the bit error rate performances of these two different approaches are nearly identical. This paper explains this phenomenon by proving that as the minimum check node degree $d_c$ becomes sufficiently large, the performance discrepancy of the linear and the coset LDPC codes is theoretically indistinguishable. This typicality of linear codes among the LDPC coset code ensemble provides insight into the concentration theorem of LDPC coset codes.",
        "published": "2005-02-22T02:47:45Z",
        "link": "http://arxiv.org/abs/cs/0502084v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A New Non-Iterative Decoding Algorithm for the Erasure Channel :   Comparisons with Enhanced Iterative Methods",
        "authors": [
            "J. Cai",
            "C. Tjhai",
            "M. Tomlinson",
            "M. Ambroze",
            "M. Ahmed"
        ],
        "summary": "This paper investigates decoding of binary linear block codes over the binary erasure channel (BEC). Of the current iterative decoding algorithms on this channel, we review the Recovery Algorithm and the Guess Algorithm. We then present a Multi-Guess Algorithm extended from the Guess Algorithm and a new algorithm -- the In-place Algorithm. The Multi-Guess Algorithm can push the limit to break the stopping sets. However, the performance of the Guess and the Multi-Guess Algorithm depend on the parity-check matrix of the code. Simulations show that we can decrease the frame error rate by several orders of magnitude using the Guess and the Multi-Guess Algorithms when the parity-check matrix of the code is sparse. The In-place Algorithm can obtain better performance even if the parity check matrix is dense. We consider the application of these algorithms in the implementation of multicast and broadcast techniques on the Internet. Using these algorithms, a user does not have to wait until the entire transmission has been received.",
        "published": "2005-03-02T14:08:04Z",
        "link": "http://arxiv.org/abs/cs/0503006v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Duality Bounds on the Cut-Off Rate with Applications to Ricean Fading",
        "authors": [
            "Amos Lapidoth",
            "Natalia Miliou"
        ],
        "summary": "We propose a technique to derive upper bounds on Gallager's cost-constrained random coding exponent function. Applying this technique to the non-coherent peak-power or average-power limited discrete time memoryless Ricean fading channel, we obtain the high signal-to-noise ratio (SNR) expansion of this channel's cut-off rate. At high SNR the gap between channel capacity and the cut-off rate approaches a finite limit. This limit is approximately 0.26 nats per channel-use for zero specular component (Rayleigh) fading and approaches 0.39 nats per channel-use for very large specular components.   We also compute the asymptotic cut-off rate of a Rayleigh fading channel when the receiver has access to some partial side information concerning the fading. It is demonstrated that the cut-off rate does not utilize the side information as efficiently as capacity, and that the high SNR gap between the two increases to infinity as the imperfect side information becomes more and more precise.",
        "published": "2005-03-08T14:47:44Z",
        "link": "http://arxiv.org/abs/cs/0503019v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Authentication with Distortion Criteria",
        "authors": [
            "Emin Martinian",
            "Gregory W. Wornell",
            "Brian Chen"
        ],
        "summary": "In a variety of applications, there is a need to authenticate content that has experienced legitimate editing in addition to potential tampering attacks. We develop one formulation of this problem based on a strict notion of security, and characterize and interpret the associated information-theoretic performance limits. The results can be viewed as a natural generalization of classical approaches to traditional authentication. Additional insights into the structure of such systems and their behavior are obtained by further specializing the results to Bernoulli and Gaussian cases. The associated systems are shown to be substantially better in terms of performance and/or security than commonly advocated approaches based on data hiding and digital watermarking. Finally, the formulation is extended to obtain efficient layered authentication system constructions.",
        "published": "2005-03-12T21:13:21Z",
        "link": "http://arxiv.org/abs/cs/0503027v2",
        "categories": [
            "cs.IT",
            "cs.CR",
            "cs.MM",
            "math.IT"
        ]
    },
    {
        "title": "On the Scalability of Cooperative Time Synchronization in   Pulse-Connected Networks",
        "authors": [
            "An-swol Hu",
            "Sergio D. Servetto"
        ],
        "summary": "The problem of time synchronization in dense wireless networks is considered. Well established synchronization techniques suffer from an inherent scalability problem in that synchronization errors grow with an increasing number of hops across the network. In this work, a model for communication in wireless networks is first developed, and then the model is used to define a new time synchronization mechanism. A salient feature of the proposed method is that, in the regime of asymptotically dense networks, it can average out all random errors and maintain global synchronization in the sense that all nodes in the multi-hop network can see identical timing signals. This is irrespective of the distance separating any two nodes.",
        "published": "2005-03-14T22:53:57Z",
        "link": "http://arxiv.org/abs/cs/0503031v2",
        "categories": [
            "cs.IT",
            "math.IT",
            "nlin.AO"
        ]
    },
    {
        "title": "On a Kronecker products sum distance bounds",
        "authors": [
            "Armen Grigoryants"
        ],
        "summary": "A binary linear error correcting codes represented by two code families Kronecker products sum are considered. The dimension and distance of new code is investigated. Upper and lower bounds of distance are obtained. Some examples are given. It is shown that some classic constructions are the private cases of considered one. The subclass of codes with equal lower and upper distance bounds is allocated.",
        "published": "2005-03-17T19:54:35Z",
        "link": "http://arxiv.org/abs/cs/0503038v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Uplink Throughput in a Single-Macrocell/Single-Microcell CDMA System,   with Application to Data Access Points",
        "authors": [
            "Shalinee Kishore",
            "Stuart C. Schwartz",
            "Larry J. Greenstein",
            "H. Vincent Poor"
        ],
        "summary": "This paper studies a two-tier CDMA system in which the microcell base is converted into a data access point (DAP), i.e., a limited-range base station that provides high-speed access to one user at a time. The microcell (or DAP) user operates on the same frequency as the macrocell users and has the same chip rate. However, it adapts its spreading factor, and thus its data rate, in accordance with interference conditions. By contrast, the macrocell serves multiple simultaneous data users, each with the same fixed rate. The achieveable throughput for individual microcell users is examined and a simple, accurate approximation for its probability distribution is presented. Computations for average throughputs, both per-user and total, are also presented. The numerical results highlight the impact of a desensitivity parameter used in the base-selection process.",
        "published": "2005-03-18T16:03:45Z",
        "link": "http://arxiv.org/abs/cs/0503040v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Soft Handoff and Uplink Capacity in a Two-Tier CDMA System",
        "authors": [
            "Shalinee Kishore",
            "Larry J. Greenstein",
            "H. Vincent Poor",
            "Stuart C. Schwartz"
        ],
        "summary": "This paper examines the effect of soft handoff on the uplink user capacity of a CDMA system consisting of a single macrocell in which a single hotspot microcell is embedded. The users of these two base stations operate over the same frequency band. In the soft handoff scenario studied here, both macrocell and microcell base stations serve each system user and the two received copies of a desired user's signal are summed using maximal ratio combining. Exact and approximate analytical methods are developed to compute uplink user capacity. Simulation results demonstrate a 20% increase in user capacity compared to hard handoff. In addition, simple, approximate methods are presented for estimating soft handoff capacity and are shown to be quite accurate.",
        "published": "2005-03-18T16:24:04Z",
        "link": "http://arxiv.org/abs/cs/0503041v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Uplink User Capacity in a CDMA System with Hotspot Microcells: Effects   of Finite Transmit Power and Dispersion",
        "authors": [
            "Shalinee Kishore",
            "Larry J. Greenstein",
            "H. Vincent Poor",
            "Stuart C. Schwartz"
        ],
        "summary": "This paper examines the uplink user capacity in a two-tier code division multiple access (CDMA) system with hotspot microcells when user terminal power is limited and the wireless channel is finitely-dispersive. A finitely-dispersive channel causes variable fading of the signal power at the output of the RAKE receiver. First, a two-cell system composed of one macrocell and one embedded microcell is studied and analytical methods are developed to estimate the user capacity as a function of a dimensionless parameter that depends on the transmit power constraint and cell radius. Next, novel analytical methods are developed to study the effect of variable fading, both with and without transmit power constraints. Finally, the analytical methods are extended to estimate uplink user capacity for multicell CDMA systems, composed of multiple macrocells and multiple embedded microcells. In all cases, the analysis-based estimates are compared with and confirmed by simulation results.",
        "published": "2005-03-18T17:18:16Z",
        "link": "http://arxiv.org/abs/cs/0503042v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On Multiflows in Random Unit-Disk Graphs, and the Capacity of Some   Wireless Networks",
        "authors": [
            "Christina Peraki",
            "Sergio D. Servetto"
        ],
        "summary": "We consider the capacity problem for wireless networks. Networks are modeled as random unit-disk graphs, and the capacity problem is formulated as one of finding the maximum value of a multicommodity flow. In this paper, we develop a proof technique based on which we are able to obtain a tight characterization of the solution to the linear program associated with the multiflow problem, to within constants independent of network size. We also use this proof method to analyze network capacity for a variety of transmitter/receiver architectures, for which we obtain some conclusive results. These results contain as a special case (and strengthen) those of Gupta and Kumar for random networks, for which a new derivation is provided using only elementary counting and discrete probability tools.",
        "published": "2005-03-21T06:20:19Z",
        "link": "http://arxiv.org/abs/cs/0503047v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Zeta-Dimension",
        "authors": [
            "David Doty",
            "Xiaoyang Gu",
            "Jack H. Lutz",
            "Elvira Mayordomo",
            "Philippe Moser"
        ],
        "summary": "The zeta-dimension of a set A of positive integers is the infimum s such that the sum of the reciprocals of the s-th powers of the elements of A is finite.   Zeta-dimension serves as a fractal dimension on the positive integers that extends naturally usefully to discrete lattices such as the set of all integer lattice points in d-dimensional space.   This paper reviews the origins of zeta-dimension (which date to the eighteenth and nineteenth centuries) and develops its basic theory, with particular attention to its relationship with algorithmic information theory. New results presented include extended connections between zeta-dimension and classical fractal dimensions, a gale characterization of zeta-dimension, and a theorem on the zeta-dimensions of pointwise sums and products of sets of positive integers.",
        "published": "2005-03-22T05:58:43Z",
        "link": "http://arxiv.org/abs/cs/0503052v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Stopping Distance and the Stopping Redundancy of Codes",
        "authors": [
            "Moshe Schwartz",
            "Alexander Vardy"
        ],
        "summary": "It is now well known that the performance of a linear code $C$ under iterative decoding on a binary erasure channel (and other channels) is determined by the size of the smallest stopping set in the Tanner graph for $C$. Several recent papers refer to this parameter as the \\emph{stopping distance} $s$ of $C$. This is somewhat of a misnomer since the size of the smallest stopping set in the Tanner graph for $C$ depends on the corresponding choice of a parity-check matrix. It is easy to see that $s \\le d$, where $d$ is the minimum Hamming distance of $C$, and we show that it is always possible to choose a parity-check matrix for $C$ (with sufficiently many dependent rows) such that $s = d$. We thus introduce a new parameter, termed the \\emph{stopping redundancy} of $C$, defined as the minimum number of rows in a parity-check matrix $H$ for $C$ such that the corresponding stopping distance $s(H)$ attains its largest possible value, namely $s(H) = d$.   We then derive general bounds on the stopping redundancy of linear codes. We also examine several simple ways of constructing codes from other codes, and study the effect of these constructions on the stopping redundancy. Specifically, for the family of binary Reed-Muller codes (of all orders), we prove that their stopping redundancy is at most a constant times their conventional redundancy. We show that the stopping redundancies of the binary and ternary extended Golay codes are at most 35 and 22, respectively. Finally, we provide upper and lower bounds on the stopping redundancy of MDS codes.",
        "published": "2005-03-23T04:56:27Z",
        "link": "http://arxiv.org/abs/cs/0503058v2",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Randomly Spread CDMA: Asymptotics via Statistical Physics",
        "authors": [
            "Dongning Guo",
            "Sergio Verdu"
        ],
        "summary": "This paper studies randomly spread code-division multiple access (CDMA) and multiuser detection in the large-system limit using the replica method developed in statistical physics. Arbitrary input distributions and flat fading are considered. A generic multiuser detector in the form of the posterior mean estimator is applied before single-user decoding. The generic detector can be particularized to the matched filter, decorrelator, linear MMSE detector, the jointly or the individually optimal detector, and others. It is found that the detection output for each user, although in general asymptotically non-Gaussian conditioned on the transmitted symbol, converges as the number of users go to infinity to a deterministic function of a \"hidden\" Gaussian statistic independent of the interferers. Thus the multiuser channel can be decoupled: Each user experiences an equivalent single-user Gaussian channel, whose signal-to-noise ratio suffers a degradation due to the multiple-access interference. The uncoded error performance (e.g., symbol-error-rate) and the mutual information can then be fully characterized using the degradation factor, also known as the multiuser efficiency, which can be obtained by solving a pair of coupled fixed-point equations identified in this paper. Based on a general linear vector channel model, the results are also applicable to MIMO channels such as in multiantenna systems.",
        "published": "2005-03-24T00:07:42Z",
        "link": "http://arxiv.org/abs/cs/0503063v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Minimum-Cost Multicast over Coded Packet Networks",
        "authors": [
            "Desmond S. Lun",
            "Niranjan Ratnakar",
            "Muriel Medard",
            "Ralf Koetter",
            "David R. Karger",
            "Tracey Ho",
            "Ebad Ahmed",
            "Fang Zhao"
        ],
        "summary": "We consider the problem of establishing minimum-cost multicast connections over coded packet networks, i.e. packet networks where the contents of outgoing packets are arbitrary, causal functions of the contents of received packets. We consider both wireline and wireless packet networks as well as both static multicast (where membership of the multicast group remains constant for the duration of the connection) and dynamic multicast (where membership of the multicast group changes in time, with nodes joining and leaving the group).   For static multicast, we reduce the problem to a polynomial-time solvable optimization problem, and we present decentralized algorithms for solving it. These algorithms, when coupled with existing decentralized schemes for constructing network codes, yield a fully decentralized approach for achieving minimum-cost multicast. By contrast, establishing minimum-cost static multicast connections over routed packet networks is a very difficult problem even using centralized computation, except in the special cases of unicast and broadcast connections.   For dynamic multicast, we reduce the problem to a dynamic programming problem and apply the theory of dynamic programming to suggest how it may be solved.",
        "published": "2005-03-24T05:02:19Z",
        "link": "http://arxiv.org/abs/cs/0503064v2",
        "categories": [
            "cs.IT",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "Improved message passing for inference in densely connected systems",
        "authors": [
            "Juan P. Neirotti",
            "David Saad"
        ],
        "summary": "An improved inference method for densely connected systems is presented. The approach is based on passing condensed messages between variables, representing macroscopic averages of microscopic messages. We extend previous work that showed promising results in cases where the solution space is contiguous to cases where fragmentation occurs. We apply the method to the signal detection problem of Code Division Multiple Access (CDMA) for demonstrating its potential. A highly efficient practical algorithm is also derived on the basis of insight gained from the analysis.",
        "published": "2005-03-24T21:27:37Z",
        "link": "http://arxiv.org/abs/cs/0503070v1",
        "categories": [
            "cs.IT",
            "cond-mat.dis-nn",
            "math.IT"
        ]
    },
    {
        "title": "Consistency in Models for Distributed Learning under Communication   Constraints",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "Motivated by sensor networks and other distributed settings, several models for distributed learning are presented. The models differ from classical works in statistical pattern recognition by allocating observations of an independent and identically distributed (i.i.d.) sampling process amongst members of a network of simple learning agents. The agents are limited in their ability to communicate to a central fusion center and thus, the amount of information available for use in classification or regression is constrained. For several basic communication models in both the binary classification and regression frameworks, we question the existence of agent decision rules and fusion rules that result in a universally consistent ensemble. The answers to this question present new issues to consider with regard to universal consistency. Insofar as these models present a useful picture of distributed scenarios, this paper addresses the issue of whether or not the guarantees provided by Stone's Theorem in centralized environments hold in distributed settings.",
        "published": "2005-03-26T05:13:51Z",
        "link": "http://arxiv.org/abs/cs/0503071v2",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "G.3; I.2.6; I.2.11; I.5.1"
        ]
    },
    {
        "title": "Distributed Learning in Wireless Sensor Networks",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "The problem of distributed or decentralized detection and estimation in applications such as wireless sensor networks has often been considered in the framework of parametric models, in which strong assumptions are made about a statistical description of nature. In certain applications, such assumptions are warranted and systems designed from these models show promise. However, in other scenarios, prior knowledge is at best vague and translating such knowledge into a statistical model is undesirable. Applications such as these pave the way for a nonparametric study of distributed detection and estimation. In this paper, we review recent work of the authors in which some elementary models for distributed learning are considered. These models are in the spirit of classical work in nonparametric statistics and are applicable to wireless sensor networks.",
        "published": "2005-03-26T05:42:06Z",
        "link": "http://arxiv.org/abs/cs/0503072v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "C.2.1; I.2.6; G3"
        ]
    },
    {
        "title": "Space-time databases modeling global semantic networks",
        "authors": [
            "A. A. Prikhod'ko",
            "N. A. Prikhod'ko"
        ],
        "summary": "This paper represents an approach to creating global knowledge systems, using new philosophy and infrastructure of global distributed semantic network (frame knowledge representation system) based on the space-time database construction. The main idea of the space-time database environment introduced in the paper is to bind a document (an information frame, a knowledge) to a special kind of entity, that we call permanent entity, -- an object without history and evolution, described by a \"point\" in the generalized, informational space-time (not an evolving object in the real space having history). For documents (information) it means that document content is unchangeable, and documents are absolutely persistent. This approach leads to new knowledge representation and retreival techniques. We discuss the way of applying the concept to a global distributed scientific library and scientific workspace. Some practical aspects of the work are elaborated by the open IT project at http://sourceforge.net/projects/gil/.",
        "published": "2005-03-29T20:18:42Z",
        "link": "http://arxiv.org/abs/cs/0503079v1",
        "categories": [
            "cs.IT",
            "cs.IR",
            "math.IT",
            "E.1; H.1.1; H.3.4; H.3.7"
        ]
    },
    {
        "title": "Dynamic Shannon Coding",
        "authors": [
            "Travis Gagie"
        ],
        "summary": "We present a new algorithm for dynamic prefix-free coding, based on Shannon coding. We give a simple analysis and prove a better upper bound on the length of the encoding produced than the corresponding bound for dynamic Huffman coding. We show how our algorithm can be modified for efficient length-restricted coding, alphabetic coding and coding with unequal letter costs.",
        "published": "2005-03-30T13:51:46Z",
        "link": "http://arxiv.org/abs/cs/0503085v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "General non-asymptotic and asymptotic formulas in channel resolvability   and identification capacity and their application to wire-tap channel",
        "authors": [
            "Masahito Hayashi"
        ],
        "summary": "Several non-asymptotic formulas are established in channel resolvability and identification capacity, and they are applied to wire-tap channel. By using these formulas, the $\\epsilon$ capacities of the above three problems are considered in the most general setting, where no structural assumptions such as the stationary memoryless property are made on a channel. As a result, we solve an open problem proposed in Han & Verdu and Han. Moreover, we obtain lower bounds of the exponents of error probability and the wire-tapper's information in wire-tap channel.",
        "published": "2005-03-31T05:16:29Z",
        "link": "http://arxiv.org/abs/cs/0503088v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Second order asymptotics in fixed-length source coding and intrinsic   randomness",
        "authors": [
            "Masahito Hayashi"
        ],
        "summary": "Second order asymptotics of fixed-length source coding and intrinsic randomness is discussed with a constant error constraint. There was a difference between optimal rates of fixed-length source coding and intrinsic randomness, which never occurred in the first order asymptotics. In addition, the relation between uniform distribution and compressed data is discussed based on this fact. These results are valid for general information sources as well as independent and identical distributions. A universal code attaining the second order optimal rate is also constructed.",
        "published": "2005-03-31T05:21:39Z",
        "link": "http://arxiv.org/abs/cs/0503089v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On Self-Dual Quantum Codes, Graphs, and Boolean Functions",
        "authors": [
            "Lars Eirik Danielsen"
        ],
        "summary": "A short introduction to quantum error correction is given, and it is shown that zero-dimensional quantum codes can be represented as self-dual additive codes over GF(4) and also as graphs. We show that graphs representing several such codes with high minimum distance can be described as nested regular graphs having minimum regular vertex degree and containing long cycles. Two graphs correspond to equivalent quantum codes if they are related by a sequence of local complementations. We use this operation to generate orbits of graphs, and thus classify all inequivalent self-dual additive codes over GF(4) of length up to 12, where previously only all codes of length up to 9 were known. We show that these codes can be interpreted as quadratic Boolean functions, and we define non-quadratic quantum codes, corresponding to Boolean functions of higher degree. We look at various cryptographic properties of Boolean functions, in particular the propagation criteria. The new aperiodic propagation criterion (APC) and the APC distance are then defined. We show that the distance of a zero-dimensional quantum code is equal to the APC distance of the corresponding Boolean function. Orbits of Boolean functions with respect to the {I,H,N}^n transform set are generated. We also study the peak-to-average power ratio with respect to the {I,H,N}^n transform set (PAR_IHN), and prove that PAR_IHN of a quadratic Boolean function is related to the size of the maximum independent set over the corresponding orbit of graphs. A construction technique for non-quadratic Boolean functions with low PAR_IHN is proposed. It is finally shown that both PAR_IHN and APC distance can be interpreted as partial entanglement measures.",
        "published": "2005-03-31T13:00:39Z",
        "link": "http://arxiv.org/abs/quant-ph/0503236v1",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Multiple Description Quantization via Gram-Schmidt Orthogonalization",
        "authors": [
            "Jun Chen",
            "Chao Tian",
            "Toby Berger",
            "Sheila Hemami"
        ],
        "summary": "The multiple description (MD) problem has received considerable attention as a model of information transmission over unreliable channels. A general framework for designing efficient multiple description quantization schemes is proposed in this paper. We provide a systematic treatment of the El Gamal-Cover (EGC) achievable MD rate-distortion region, and show that any point in the EGC region can be achieved via a successive quantization scheme along with quantization splitting. For the quadratic Gaussian case, the proposed scheme has an intrinsic connection with the Gram-Schmidt orthogonalization, which implies that the whole Gaussian MD rate-distortion region is achievable with a sequential dithered lattice-based quantization scheme as the dimension of the (optimal) lattice quantizers becomes large. Moreover, this scheme is shown to be universal for all i.i.d. smooth sources with performance no worse than that for an i.i.d. Gaussian source with the same variance and asymptotically optimal at high resolution. A class of low-complexity MD scalar quantizers in the proposed general framework also is constructed and is illustrated geometrically; the performance is analyzed in the high resolution regime, which exhibits a noticeable improvement over the existing MD scalar quantization schemes.",
        "published": "2005-04-01T19:53:32Z",
        "link": "http://arxiv.org/abs/cs/0504003v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Fast Codes for Large Alphabets",
        "authors": [
            "Boris Ryabko",
            "Jaakko Astola",
            "Karen Egiazarian"
        ],
        "summary": "We address the problem of constructing a fast lossless code in the case when the source alphabet is large. The main idea of the new scheme may be described as follows. We group letters with small probabilities in subsets (acting as super letters) and use time consuming coding for these subsets only, whereas letters in the subsets have the same code length and therefore can be coded fast. The described scheme can be applied to sources with known and unknown statistics.",
        "published": "2005-04-03T04:09:27Z",
        "link": "http://arxiv.org/abs/cs/0504005v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Using Information Theory Approach to Randomness Testing",
        "authors": [
            "B. Ya. Ryabko",
            "V. A. Monarev"
        ],
        "summary": "We address the problem of detecting deviations of binary sequence from randomness,which is very important for random number (RNG) and pseudorandom number generators (PRNG). Namely, we consider a null hypothesis $H_0$ that a given bit sequence is generated by Bernoulli source with equal probabilities of 0 and 1 and the alternative hypothesis $H_1$ that the sequence is generated by a stationary and ergodic source which differs from the source under $H_0$. We show that data compression methods can be used as a basis for such testing and describe two new tests for randomness, which are based on ideas of universal coding. Known statistical tests and suggested ones are applied for testing PRNGs. Those experiments show that the power of the new tests is greater than of many known algorithms.",
        "published": "2005-04-03T06:39:27Z",
        "link": "http://arxiv.org/abs/cs/0504006v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Reversible Fault-Tolerant Logic",
        "authors": [
            "P. Oscar Boykin",
            "Vwani P. Roychowdhury"
        ],
        "summary": "It is now widely accepted that the CMOS technology implementing irreversible logic will hit a scaling limit beyond 2016, and that the increased power dissipation is a major limiting factor. Reversible computing can potentially require arbitrarily small amounts of energy. Recently several nano-scale devices which have the potential to scale, and which naturally perform reversible logic, have emerged. This paper addresses several fundamental issues that need to be addressed before any nano-scale reversible computing systems can be realized, including reliability and performance trade-offs and architecture optimization. Many nano-scale devices will be limited to only near neighbor interactions, requiring careful optimization of circuits. We provide efficient fault-tolerant (FT) circuits when restricted to both 2D and 1D. Finally, we compute bounds on the entropy (and hence, heat) generated by our FT circuits and provide quantitative estimates on how large can we make our circuits before we lose any advantage over irreversible computing.",
        "published": "2005-04-04T21:44:42Z",
        "link": "http://arxiv.org/abs/cs/0504010v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "quant-ph"
        ]
    },
    {
        "title": "Average Coset Weight Distribution of Combined LDPC Matrix Ensemble",
        "authors": [
            "Tadashi Wadayama"
        ],
        "summary": "In this paper, the average coset weight distribution (ACWD) of structured ensembles of LDPC (Low-density Parity-Check) matrix, which is called combined ensembles, is discussed. A combined ensemble is composed of a set of simpler ensembles such as a regular bipartite ensemble. Two classes of combined ensembles have prime importance; a stacked ensemble and a concatenated ensemble, which consists of set of stacked matrices and concatenated matrices, respectively. The ACWD formulas of these ensembles is shown in this paper. Such formulas are key tools to evaluate the ACWD of a complex combined ensemble.   From the ACWD of an ensemble, we can obtain some detailed properties of a code (e.g., weight of coset leaders) which is not available from an average weight distribution. Moreover, it is shown that the analysis based on the ACWD is indispensable to evaluate the average weight distribution of some classes of combined ensembles.",
        "published": "2005-04-05T05:51:47Z",
        "link": "http://arxiv.org/abs/cs/0504011v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Pseudocodewords of Tanner graphs",
        "authors": [
            "Christine A. Kelley",
            "Deepak Sridhara"
        ],
        "summary": "This papers presents a detailed analysis of pseudocodewords of Tanner graphs. Pseudocodewords arising on the iterative decoder's computation tree are distinguished from pseudocodewords arising on finite degree lifts. Lower bounds on the minimum pseudocodeword weight are presented for the BEC, BSC, and AWGN channel. Some structural properties of pseudocodewords are examined, and pseudocodewords and graph properties that are potentially problematic with min-sum iterative decoding are identified. An upper bound on the minimum degree lift needed to realize a particular irreducible lift-realizable pseudocodeword is given in terms of its maximal component, and it is shown that all irreducible lift-realizable pseudocodewords have components upper bounded by a finite value $t$ that is dependent on the graph structure. Examples and different Tanner graph representations of individual codes are examined and the resulting pseudocodeword distributions and iterative decoding performances are analyzed. The results obtained provide some insights in relating the structure of the Tanner graph to the pseudocodeword distribution and suggest ways of designing Tanner graphs with good minimum pseudocodeword weight.",
        "published": "2005-04-05T12:30:07Z",
        "link": "http://arxiv.org/abs/cs/0504013v4",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Network Information Flow with Correlated Sources",
        "authors": [
            "Joao Barros",
            "Sergio D. Servetto"
        ],
        "summary": "In this paper, we consider a network communications problem in which multiple correlated sources must be delivered to a single data collector node, over a network of noisy independent point-to-point channels. We prove that perfect reconstruction of all the sources at the sink is possible if and only if, for all partitions of the network nodes into two subsets S and S^c such that the sink is always in S^c, we have that H(U_S|U_{S^c}) < \\sum_{i\\in S,j\\in S^c} C_{ij}. Our main finding is that in this setup a general source/channel separation theorem holds, and that Shannon information behaves as a classical network flow, identical in nature to the flow of water in pipes. At first glance, it might seem surprising that separation holds in a fairly general network situation like the one we study. A closer look, however, reveals that the reason for this is that our model allows only for independent point-to-point channels between pairs of nodes, and not multiple-access and/or broadcast channels, for which separation is well known not to hold. This ``information as flow'' view provides an algorithmic interpretation for our results, among which perhaps the most important one is the optimality of implementing codes using a layered protocol stack.",
        "published": "2005-04-05T13:10:34Z",
        "link": "http://arxiv.org/abs/cs/0504014v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Design of Block Transceivers with Decision Feedback Detection",
        "authors": [
            "Fang Xu",
            "Tim Davidson",
            "Jian-Kang Zhang",
            "K. Max Wong"
        ],
        "summary": "This paper presents a method for jointly designing the transmitter-receiver pair in a block-by-block communication system that employs (intra-block) decision feedback detection. We provide closed-form expressions for transmitter-receiver pairs that simultaneously minimize the arithmetic mean squared error (MSE) at the decision point (assuming perfect feedback), the geometric MSE, and the bit error rate of a uniformly bit-loaded system at moderate-to-high signal-to-noise ratios. Separate expressions apply for the ``zero-forcing'' and ``minimum MSE'' (MMSE) decision feedback structures. In the MMSE case, the proposed design also maximizes the Gaussian mutual information and suggests that one can approach the capacity of the block transmission system using (independent instances of) the same (Gaussian) code for each element of the block. Our simulation studies indicate that the proposed transceivers perform significantly better than standard transceivers, and that they retain their performance advantages in the presence of error propagation.",
        "published": "2005-04-05T13:51:29Z",
        "link": "http://arxiv.org/abs/cs/0504015v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Shortened Array Codes of Large Girth",
        "authors": [
            "Olgica Milenkovic",
            "Navin Kashyap",
            "David Leyba"
        ],
        "summary": "One approach to designing structured low-density parity-check (LDPC) codes with large girth is to shorten codes with small girth in such a manner that the deleted columns of the parity-check matrix contain all the variables involved in short cycles. This approach is especially effective if the parity-check matrix of a code is a matrix composed of blocks of circulant permutation matrices, as is the case for the class of codes known as array codes. We show how to shorten array codes by deleting certain columns of their parity-check matrices so as to increase their girth. The shortening approach is based on the observation that for array codes, and in fact for a slightly more general class of LDPC codes, the cycles in the corresponding Tanner graph are governed by certain homogeneous linear equations with integer coefficients. Consequently, we can selectively eliminate cycles from an array code by only retaining those columns from the parity-check matrix of the original code that are indexed by integer sequences that do not contain solutions to the equations governing those cycles. We provide Ramsey-theoretic estimates for the maximum number of columns that can be retained from the original parity-check matrix with the property that the sequence of their indices avoid solutions to various types of cycle-governing equations. This translates to estimates of the rate penalty incurred in shortening a code to eliminate cycles. Simulation results show that for the codes considered, shortening them to increase the girth can lead to significant gains in signal-to-noise ratio in the case of communication over an additive white Gaussian noise channel.",
        "published": "2005-04-05T22:32:25Z",
        "link": "http://arxiv.org/abs/cs/0504016v2",
        "categories": [
            "cs.DM",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A new SISO algorithm with application to turbo equalization",
        "authors": [
            "Marcin Sikora",
            "Daniel J. Costello Jr"
        ],
        "summary": "In this paper we propose a new soft-input soft-output equalization algorithm, offering very good performance/complexity tradeoffs. It follows the structure of the BCJR algorithm, but dynamically constructs a simplified trellis during the forward recursion. In each trellis section, only the M states with the strongest forward metric are preserved, similar to the M-BCJR algorithm. Unlike the M-BCJR, however, the remaining states are not deleted, but rather merged into the surviving states. The new algorithm compares favorably with the reduced-state BCJR algorithm, offering better performance and more flexibility, particularly for systems with higher order modulations.",
        "published": "2005-04-06T01:26:55Z",
        "link": "http://arxiv.org/abs/cs/0504017v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The Viterbi Algorithm: A Personal History",
        "authors": [
            "G. David Forney Jr"
        ],
        "summary": "The story of the Viterbi algorithm (VA) is told from a personal perspective. Applications both within and beyond communications are discussed. In brief summary, the VA has proved to be an extremely important algorithm in a surprising variety of fields.",
        "published": "2005-04-06T15:59:31Z",
        "link": "http://arxiv.org/abs/cs/0504020v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Near Perfect Decoding of LDPC Codes",
        "authors": [
            "Xiaofei Huang"
        ],
        "summary": "Cooperative optimization is a new way for finding global optima of complicated functions of many variables. It has some important properties not possessed by any conventional optimization methods. It has been successfully applied in solving many large scale optimization problems in image processing, computer vision, and computational chemistry. This paper shows the application of this optimization principle in decoding LDPC codes, which is another hard combinatorial optimization problem. In our experiments, it significantly out-performed the sum-product algorithm, the best known method for decoding LDPC codes. Compared to the sum-product algorithm, our algorithm reduced the error rate further by three fold, improved the speed by six times, and lowered error floors dramatically in the decoding.",
        "published": "2005-04-06T17:19:24Z",
        "link": "http://arxiv.org/abs/cs/0504021v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On Extrinsic Information of Good Codes Operating Over Discrete   Memoryless Channels",
        "authors": [
            "Michael Peleg",
            "Amichai Sanderovich",
            "Shlomo Shamai"
        ],
        "summary": "We show that the Extrinsic Information about the coded bits of any good (capacity achieving) code operating over a wide class of discrete memoryless channels (DMC) is zero when channel capacity is below the code rate and positive constant otherwise, that is, the Extrinsic Information Transfer (EXIT) chart is a step function of channel quality, for any capacity achieving code. It follows that, for a common class of iterative receivers where the error correcting decoder must operate at first iteration at rate above capacity (such as in turbo equalization, turbo channel estimation, parallel and serial concatenated coding and the like), classical good codes which achieve capacity over the DMC are not effective and should be replaced by different new ones. Another meaning of the results is that a good code operating at rate above channel capacity falls apart into its individual transmitted symbols in the sense that all the information about a coded transmitted symbol is contained in the corresponding received symbol and no information about it can be inferred from the other received symbols. The binary input additive white Gaussian noise channel is treated in part 1 of this report. Part 2 extends the results to the symmetric binary channel and to the binary erasure channel and provides an heuristic extension to wider class of channel models.",
        "published": "2005-04-07T17:35:05Z",
        "link": "http://arxiv.org/abs/cs/0504028v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Sufficient conditions for convergence of the Sum-Product Algorithm",
        "authors": [
            "Joris M. Mooij",
            "Hilbert J. Kappen"
        ],
        "summary": "We derive novel conditions that guarantee convergence of the Sum-Product algorithm (also known as Loopy Belief Propagation or simply Belief Propagation) to a unique fixed point, irrespective of the initial messages. The computational complexity of the conditions is polynomial in the number of variables. In contrast with previously existing conditions, our results are directly applicable to arbitrary factor graphs (with discrete variables) and are shown to be valid also in the case of factors containing zeros, under some additional conditions. We compare our bounds with existing ones, numerically and, if possible, analytically. For binary variables with pairwise interactions, we derive sufficient conditions that take into account local evidence (i.e., single variable factors) and the type of pair interactions (attractive or repulsive). It is shown empirically that this bound outperforms existing bounds.",
        "published": "2005-04-08T15:11:04Z",
        "link": "http://arxiv.org/abs/cs/0504030v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "I.2.3; F.2.1"
        ]
    },
    {
        "title": "Critical Point for Maximum Likelihood Decoding of Linear Block Codes",
        "authors": [
            "Marc Fossorier"
        ],
        "summary": "In this letter, the SNR value at which the error performance curve of a soft decision maximum likelihood decoder reaches the slope corresponding to the code minimum distance is determined for a random code. Based on this value, referred to as the critical point, new insight about soft bounded distance decoding of random-like codes (and particularly Reed-Solomon codes) is provided.",
        "published": "2005-04-10T00:40:46Z",
        "link": "http://arxiv.org/abs/cs/0504032v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Entropy Rate of Pattern Processes",
        "authors": [
            "George M. Gemelos",
            "Tsachy Weissman"
        ],
        "summary": "We study the entropy rate of pattern sequences of stochastic processes, and its relationship to the entropy rate of the original process. We give a complete characterization of this relationship for i.i.d. processes over arbitrary alphabets, stationary ergodic processes over discrete alphabets, and a broad family of stationary ergodic processes over uncountable alphabets. For cases where the entropy rate of the pattern process is infinite, we characterize the possible growth rate of the block entropy.",
        "published": "2005-04-12T19:47:53Z",
        "link": "http://arxiv.org/abs/cs/0504046v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Pushdown dimension",
        "authors": [
            "David Doty",
            "Jared Nichols"
        ],
        "summary": "This paper develops the theory of pushdown dimension and explores its relationship with finite-state dimension. Pushdown dimension is trivially bounded above by finite-state dimension for all sequences, since a pushdown gambler can simulate any finite-state gambler. We show that for every rational 0 < d < 1, there exists a sequence with finite-state dimension d whose pushdown dimension is at most d/2. This establishes a quantitative analogue of the well-known fact that pushdown automata decide strictly more languages than finite automata.",
        "published": "2005-04-12T22:13:12Z",
        "link": "http://arxiv.org/abs/cs/0504047v4",
        "categories": [
            "cs.IT",
            "cs.CC",
            "math.IT"
        ]
    },
    {
        "title": "Bounds on the Entropy of Patterns of I.I.D. Sequences",
        "authors": [
            "Gil I. Shamir"
        ],
        "summary": "Bounds on the entropy of patterns of sequences generated by independently identically distributed (i.i.d.) sources are derived. A pattern is a sequence of indices that contains all consecutive integer indices in increasing order of first occurrence. If the alphabet of a source that generated a sequence is unknown, the inevitable cost of coding the unknown alphabet symbols can be exploited to create the pattern of the sequence. This pattern can in turn be compressed by itself. The bounds derived here are functions of the i.i.d. source entropy, alphabet size, and letter probabilities. It is shown that for large alphabets, the pattern entropy must decrease from the i.i.d. one. The decrease is in many cases more significant than the universal coding redundancy bounds derived in prior works. The pattern entropy is confined between two bounds that depend on the arrangement of the letter probabilities in the probability space. For very large alphabets whose size may be greater than the coded pattern length, all low probability letters are packed into one symbol. The pattern entropy is upper and lower bounded in terms of the i.i.d. entropy of the new packed alphabet. Correction terms, which are usually negligible, are provided for both upper and lower bounds.",
        "published": "2005-04-13T03:22:15Z",
        "link": "http://arxiv.org/abs/cs/0504049v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Universal Minimax Discrete Denoising under Channel Uncertainty",
        "authors": [
            "George Gemelos",
            "Styrmir Sigurjonsson",
            "Tsachy Weissman"
        ],
        "summary": "The goal of a denoising algorithm is to recover a signal from its noise-corrupted observations. Perfect recovery is seldom possible and performance is measured under a given single-letter fidelity criterion. For discrete signals corrupted by a known discrete memoryless channel, the DUDE was recently shown to perform this task asymptotically optimally, without knowledge of the statistical properties of the source. In the present work we address the scenario where, in addition to the lack of knowledge of the source statistics, there is also uncertainty in the channel characteristics. We propose a family of discrete denoisers and establish their asymptotic optimality under a minimax performance criterion which we argue is appropriate for this setting. As we show elsewhere, the proposed schemes can also be implemented computationally efficiently.",
        "published": "2005-04-13T19:20:05Z",
        "link": "http://arxiv.org/abs/cs/0504060v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Prediction of Large Alphabet Processes and Its Application to Adaptive   Source Coding",
        "authors": [
            "Boris Ryabko",
            "Jaakko Astola"
        ],
        "summary": "The problem of predicting a sequence $x_1,x_2,...$ generated by a discrete source with unknown statistics is considered. Each letter $x_{t+1}$ is predicted using information on the word $x_1x_2... x_t$ only. In fact, this problem is a classical problem which has received much attention. Its history can be traced back to Laplace. We address the problem where each $x_i$ belongs to some large (or even infinite) alphabet. A method is presented for which the precision is greater than for known algorithms, where precision is estimated by the Kullback-Leibler divergence. The results can readily be translated to results about adaptive coding.",
        "published": "2005-04-17T16:32:03Z",
        "link": "http://arxiv.org/abs/cs/0504079v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Performance of Gaussian Signalling in Non Coherent Rayleigh Fading   Channels",
        "authors": [
            "Rasika Perera",
            "Tony Pollock",
            "Thushara Abhayapala"
        ],
        "summary": "The mutual information of a discrete time memoryless Rayleigh fading channel is considered, where neither the transmitter nor the receiver has the knowledge of the channel state information except the fading statistics. We present the mutual information of this channel in closed form when the input distribution is complex Gaussian, and derive a lower bound in terms of the capacity of the corresponding non fading channel and the capacity when the perfect channel state information is known at the receiver.",
        "published": "2005-04-18T03:26:49Z",
        "link": "http://arxiv.org/abs/cs/0504080v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Capacity per Unit Energy of Fading Channels with a Peak Constraint",
        "authors": [
            "Vignesh Sethuraman",
            "Bruce Hajek"
        ],
        "summary": "A discrete-time single-user scalar channel with temporally correlated Rayleigh fading is analyzed. There is no side information at the transmitter or the receiver. A simple expression is given for the capacity per unit energy, in the presence of a peak constraint. The simple formula of Verdu for capacity per unit cost is adapted to a channel with memory, and is used in the proof. In addition to bounding the capacity of a channel with correlated fading, the result gives some insight into the relationship between the correlation in the fading process and the channel capacity. The results are extended to a channel with side information, showing that the capacity per unit energy is one nat per Joule, independently of the peak power constraint.   A continuous-time version of the model is also considered. The capacity per unit energy subject to a peak constraint (but no bandwidth constraint) is given by an expression similar to that for discrete time, and is evaluated for Gauss-Markov and Clarke fading channels.",
        "published": "2005-04-18T16:29:14Z",
        "link": "http://arxiv.org/abs/cs/0504085v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Probabilistic Upper Bound on Differential Entropy",
        "authors": [
            "Joseph DeStefano",
            "Erik Learned-Miller"
        ],
        "summary": "A novel, non-trivial, probabilistic upper bound on the entropy of an unknown one-dimensional distribution, given the support of the distribution and a sample from that distribution, is presented. No knowledge beyond the support of the unknown distribution is required, nor is the distribution required to have a density. Previous distribution-free bounds on the cumulative distribution function of a random variable given a sample of that variable are used to construct the bound. A simple, fast, and intuitive algorithm for computing the entropy bound from a sample is provided.",
        "published": "2005-04-21T18:25:09Z",
        "link": "http://arxiv.org/abs/cs/0504091v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The Capacity of Random Ad hoc Networks under a Realistic Link Layer   Model",
        "authors": [
            "Vivek P. Mhatre",
            "Catherine P. Rosenberg"
        ],
        "summary": "The problem of determining asymptotic bounds on the capacity of a random ad hoc network is considered. Previous approaches assumed a threshold-based link layer model in which a packet transmission is successful if the SINR at the receiver is greater than a fixed threshold. In reality, the mapping from SINR to packet success probability is continuous. Hence, over each hop, for every finite SINR, there is a non-zero probability of packet loss. With this more realistic link model, it is shown that for a broad class of routing and scheduling schemes, a fixed fraction of hops on each route have a fixed non-zero packet loss probability. In a large network, a packet travels an asymptotically large number of hops from source to destination. Consequently, it is shown that the cumulative effect of per-hop packet loss results in a per-node throughput of only O(1/n) (instead of Theta(1/sqrt{n log{n}})) as shown previously for the threshold-based link model).   A scheduling scheme is then proposed to counter this effect. The proposed scheme improves the link SINR by using conservative spatial reuse, and improves the per-node throughput to O(1/(K_n sqrt{n log{n}})), where each cell gets a transmission opportunity at least once every K_n slots, and K_n tends to infinity as n tends to infinity.",
        "published": "2005-04-25T01:44:12Z",
        "link": "http://arxiv.org/abs/cs/0504099v1",
        "categories": [
            "cs.IT",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "Spectral Orbits and Peak-to-Average Power Ratio of Boolean Functions   with respect to the {I,H,N}^n Transform",
        "authors": [
            "Lars Eirik Danielsen",
            "Matthew G. Parker"
        ],
        "summary": "We enumerate the inequivalent self-dual additive codes over GF(4) of blocklength n, thereby extending the sequence A090899 in The On-Line Encyclopedia of Integer Sequences from n = 9 to n = 12. These codes have a well-known interpretation as quantum codes. They can also be represented by graphs, where a simple graph operation generates the orbits of equivalent codes. We highlight the regularity and structure of some graphs that correspond to codes with high distance. The codes can also be interpreted as quadratic Boolean functions, where inequivalence takes on a spectral meaning. In this context we define PAR_IHN, peak-to-average power ratio with respect to the {I,H,N}^n transform set. We prove that PAR_IHN of a Boolean function is equivalent to the the size of the maximum independent set over the associated orbit of graphs. Finally we propose a construction technique to generate Boolean functions with low PAR_IHN and algebraic degree higher than 2.",
        "published": "2005-04-25T11:32:25Z",
        "link": "http://arxiv.org/abs/cs/0504102v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A DNA Sequence Compression Algorithm Based on LUT and LZ77",
        "authors": [
            "Sheng Bao",
            "Shi Chen",
            "Zhiqiang Jing",
            "Ran Ren"
        ],
        "summary": "This article introduces a new DNA sequence compression algorithm which is based on LUT and LZ77 algorithm. Combined a LUT-based pre-coding routine and LZ77 compression routine,this algorithm can approach a compression ratio of 1.9bits \\slash base and even lower.The biggest advantage of this algorithm is fast execution, small memory occupation and easy implementation.",
        "published": "2005-04-25T14:57:19Z",
        "link": "http://arxiv.org/abs/cs/0504100v6",
        "categories": [
            "cs.IT",
            "math.IT",
            "J.3; E.4"
        ]
    },
    {
        "title": "On the Classification of All Self-Dual Additive Codes over GF(4) of   Length up to 12",
        "authors": [
            "Lars Eirik Danielsen",
            "Matthew G. Parker"
        ],
        "summary": "We consider additive codes over GF(4) that are self-dual with respect to the Hermitian trace inner product. Such codes have a well-known interpretation as quantum codes and correspond to isotropic systems. It has also been shown that these codes can be represented as graphs, and that two codes are equivalent if and only if the corresponding graphs are equivalent with respect to local complementation and graph isomorphism. We use these facts to classify all codes of length up to 12, where previously only all codes of length up to 9 were known. We also classify all extremal Type II codes of length 14. Finally, we find that the smallest Type I and Type II codes with trivial automorphism group have length 9 and 12, respectively.",
        "published": "2005-04-25T16:06:16Z",
        "link": "http://arxiv.org/abs/math/0504522v4",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "94B60 (Primary) 05C90 (Secondary)"
        ]
    },
    {
        "title": "On the Wyner-Ziv problem for individual sequences",
        "authors": [
            "Neri Merhav",
            "Jacob Ziv"
        ],
        "summary": "We consider a variation of the Wyner-Ziv problem pertaining to lossy compression of individual sequences using finite-state encoders and decoders. There are two main results in this paper. The first characterizes the relationship between the performance of the best $M$-state encoder-decoder pair to that of the best block code of size $\\ell$ for every input sequence, and shows that the loss of the latter relative to the former (in terms of both rate and distortion) never exceeds the order of $(\\log M)/\\ell$, independently of the input sequence. Thus, in the limit of large $M$, the best rate-distortion performance of every infinite source sequence can be approached universally by a sequence of block codes (which are also implementable by finite-state machines). While this result assumes an asymptotic regime where the number of states is fixed, and only the length $n$ of the input sequence grows without bound, we then consider the case where the number of states $M=M_n$ is allowed to grow concurrently with $n$. Our second result is then about the critical growth rate of $M_n$ such that the rate-distortion performance of $M_n$-state encoder-decoder pairs can still be matched by a universal code. We show that this critical growth rate of $M_n$ is linear in $n$.",
        "published": "2005-05-04T11:05:58Z",
        "link": "http://arxiv.org/abs/cs/0505010v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Shannon cipher system with a capacity-limited key-distribution   channel",
        "authors": [
            "Neri Merhav"
        ],
        "summary": "We consider the Shannon cipher system in a setting where the secret key is delivered to the legitimate receiver via a channel with limited capacity. For this setting, we characterize the achievable region in the space of three figures of merit: the security (measured in terms of the equivocation), the compressibility of the cryptogram, and the distortion associated with the reconstruction of the plaintext source. Although lossy reconstruction of the plaintext does not rule out the option that the (noisy) decryption key would differ, to a certain extent, from the encryption key, we show, nevertheless, that the best strategy is to strive for perfect match between the two keys, by applying reliable channel coding to the key bits, and to control the distortion solely via rate-distortion coding of the plaintext source before the encryption. In this sense, our result has a flavor similar to that of the classical source-channel separation theorem. Some variations and extensions of this model are discussed as well.",
        "published": "2005-05-05T07:44:32Z",
        "link": "http://arxiv.org/abs/cs/0505012v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Asymptotic Capacity Results for Non-Stationary Time-Variant Channels   Using Subspace Projections",
        "authors": [
            "Thomas Zemen",
            "Stefan M. Moser"
        ],
        "summary": "In this paper we deal with a single-antenna discrete-time flat-fading channel. The fading process is assumed to be stationary for the duration of a single data block. From block to block the fading process is allowed to be non-stationary. The number of scatterers bounds the rank of the channels covariance matrix. The signal-to-noise ratio (SNR), the user velocity, and the data block-length define the usable rank of the time-variant channel subspace. The usable channel subspace grows with the SNR. This growth in dimensionality must be taken into account for asymptotic capacity results in the high-SNR regime. Using results from the theory of time-concentrated and band-limited sequences we are able to define an SNR threshold below which the capacity grows logarithmically. Above this threshold the capacity grows double-logarithmically.",
        "published": "2005-05-10T09:28:32Z",
        "link": "http://arxiv.org/abs/cs/0505020v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Collaborative Beamforming for Distributed Wireless Ad Hoc Sensor   Networks",
        "authors": [
            "Hideki Ochiai",
            "Patrick Mitran",
            "H. Vincent Poor",
            "Vahid Tarokh"
        ],
        "summary": "The performance of collaborative beamforming is analyzed using the theory of random arrays. The statistical average and distribution of the beampattern of randomly generated phased arrays is derived in the framework of wireless ad hoc sensor networks. Each sensor node is assumed to have a single isotropic antenna and nodes in the cluster collaboratively transmit the signal such that the signal in the target direction is coherently added in the far- eld region. It is shown that with N sensor nodes uniformly distributed over a disk, the directivity can approach N, provided that the nodes are located sparsely enough. The distribution of the maximum sidelobe peak is also studied. With the application to ad hoc networks in mind, two scenarios, closed-loop and open-loop, are considered. Associated with these scenarios, the effects of phase jitter and location estimation errors on the average beampattern are also analyzed.",
        "published": "2005-05-10T13:09:35Z",
        "link": "http://arxiv.org/abs/cs/0505022v1",
        "categories": [
            "cs.IT",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "Broadcast Channels with Cooperating Decoders",
        "authors": [
            "Ron Dabora",
            "Sergio D. Servetto"
        ],
        "summary": "We consider the problem of communicating over the general discrete memoryless broadcast channel (BC) with partially cooperating receivers. In our setup, receivers are able to exchange messages over noiseless conference links of finite capacities, prior to decoding the messages sent from the transmitter. In this paper we formulate the general problem of broadcast with cooperation. We first find the capacity region for the case where the BC is physically degraded. Then, we give achievability results for the general broadcast channel, for both the two independent messages case and the single common message case.",
        "published": "2005-05-11T19:37:51Z",
        "link": "http://arxiv.org/abs/cs/0505032v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Separating a Real-Life Nonlinear Image Mixture",
        "authors": [
            "Luis B. Almeida"
        ],
        "summary": "When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation.   This paper addresses a difficult version of this problem, corresponding to the use of \"onion skin\" paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement.",
        "published": "2005-05-16T19:31:36Z",
        "link": "http://arxiv.org/abs/cs/0505044v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Optimum Signal Linear Detector in the Discrete Wavelet Transform-Domain",
        "authors": [
            "Ignacio Melgar",
            "Jaime Gomez",
            "Juan Seijas"
        ],
        "summary": "The problem of known signal detection in Additive White Gaussian Noise is considered. In this paper a new detection algorithm based on Discrete Wavelet Transform pre-processing and threshold comparison is introduced. Current approaches described in [7] use the maximum value obtained in the wavelet domain for decision. Here, we use all available information in the wavelet domain with excellent results. Detector performance is presented in Probability of detection curves for a fixed probability of false alarm.",
        "published": "2005-05-18T11:03:18Z",
        "link": "http://arxiv.org/abs/cs/0505046v1",
        "categories": [
            "cs.IT",
            "cs.IR",
            "math.IT"
        ]
    },
    {
        "title": "Fading-Resilient Super-Orthogonal Space-Time Signal Sets: Can Good   Constellations Survive in Fading?",
        "authors": [
            "Dumitru Mihai Ionescu",
            "Zhiyuan Yan"
        ],
        "summary": "In this correspondence, first-tier indirect (direct) discernible constellation expansions are defined for generalized orthogonal designs. The expanded signal constellation, leading to so-called super-orthogonal codes, allows the achievement of coding gains in addition to diversity gains enabled by orthogonal designs. Conditions that allow the shape of an expanded multidimensional constellation to be preserved at the channel output, on an instantaneous basis, are derived. It is further shown that, for such constellations, the channel alters neither the relative distances nor the angles between signal points in the expanded signal constellation.",
        "published": "2005-05-18T21:16:27Z",
        "link": "http://arxiv.org/abs/cs/0505049v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "The Partition Weight Enumerator of MDS Codes and its Applications",
        "authors": [
            "Mostafa El-Khamy",
            "Robert J. McEliece"
        ],
        "summary": "A closed form formula of the partition weight enumerator of maximum distance separable (MDS) codes is derived for an arbitrary number of partitions. Using this result, some properties of MDS codes are discussed. The results are extended for the average binary image of MDS codes in finite fields of characteristic two. As an application, we study the multiuser error probability of Reed Solomon codes.",
        "published": "2005-05-21T01:21:27Z",
        "link": "http://arxiv.org/abs/cs/0505054v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Text Compression and Superfast Searching",
        "authors": [
            "Udayan Khurana",
            "Anirudh Koul"
        ],
        "summary": "In this paper, a new compression scheme for text is presented. The same is efficient in giving high compression ratios and enables super fast searching within the compressed text. Typical compression ratios of 70-80% and reducing the search time by 80-85% are the features of this paper. Till now, a trade-off between high ratios and searchability within compressed text has been seen. In this paper, we show that greater the compression, faster the search. This finds applicability in so many places where data as natural language text is present.",
        "published": "2005-05-23T07:04:49Z",
        "link": "http://arxiv.org/abs/cs/0505056v1",
        "categories": [
            "cs.IR",
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Improved Bounds on the Parity-Check Density and Achievable Rates of   Binary Linear Block Codes with Applications to LDPC Codes",
        "authors": [
            "Gil Wiechman",
            "Igal Sason"
        ],
        "summary": "We derive bounds on the asymptotic density of parity-check matrices and the achievable rates of binary linear block codes transmitted over memoryless binary-input output-symmetric (MBIOS) channels. The lower bounds on the density of arbitrary parity-check matrices are expressed in terms of the gap between the rate of these codes for which reliable communication is achievable and the channel capacity, and the bounds are valid for every sequence of binary linear block codes. These bounds address the question, previously considered by Sason and Urbanke, of how sparse can parity-check matrices of binary linear block codes be as a function of the gap to capacity. Similarly to a previously reported bound by Sason and Urbanke, the new lower bounds on the parity-check density scale like the log of the inverse of the gap to capacity, but their tightness is improved (except for a binary symmetric/erasure channel, where they coincide with the previous bound). The new upper bounds on the achievable rates of binary linear block codes tighten previously reported bounds by Burshtein et al., and therefore enable to obtain tighter upper bounds on the thresholds of sequences of binary linear block codes under ML decoding. The bounds are applied to low-density parity-check (LDPC) codes, and the improvement in their tightness is exemplified numerically. The upper bounds on the achievable rates enable to assess the inherent loss in performance of various iterative decoding algorithms as compared to optimal ML decoding. The lower bounds on the asymptotic parity-check density are helpful in assessing the inherent tradeoff between the asymptotic performance of LDPC codes and their decoding complexity (per iteration) under message-passing decoding.",
        "published": "2005-05-23T07:59:33Z",
        "link": "http://arxiv.org/abs/cs/0505057v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "HLA and HIV Infection Progression: Application of the Minimum   Description Length Principle to Statistical Genetics",
        "authors": [
            "Peter T. Hraber",
            "Bette T. Korber",
            "Steven Wolinsky",
            "Henry A. Erlich",
            "Elizabeth A. Trachtenberg",
            "Thomas B. Kepler"
        ],
        "summary": "The minimum description length (MDL) principle states that the best model to account for some data minimizes the sum of the lengths, in bits, of the descriptions of the model and the residual error. The description length is thus a criterion for model selection. Description-length analysis of HLA alleles from the Chicago MACS cohort enables classification of alleles associated with plasma HIV RNA, an indicator of infection progression. Progression variation is most strongly associated with HLA-B. Individuals without B58s supertype alleles average viral RNA levels 3.6-fold greater than individuals with them.",
        "published": "2005-05-26T16:48:05Z",
        "link": "http://arxiv.org/abs/q-bio/0505050v1",
        "categories": [
            "q-bio.QM",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Parity-Check Density and Achievable Rates of LDPC Codes",
        "authors": [
            "Gil Wiechman",
            "Igal Sason"
        ],
        "summary": "The paper introduces new bounds on the asymptotic density of parity-check matrices and the achievable rates under ML decoding of binary linear block codes transmitted over memoryless binary-input output-symmetric channels. The lower bounds on the parity-check density are expressed in terms of the gap between the channel capacity and the rate of the codes for which reliable communication is achievable, and are valid for every sequence of binary linear block codes. The bounds address the question, previously considered by Sason and Urbanke, of how sparse can parity-check matrices of binary linear block codes be as a function of the gap to capacity. The new upper bounds on the achievable rates of binary linear block codes tighten previously reported bounds by Burshtein et al., and therefore enable to obtain tighter upper bounds on the thresholds of sequences of binary linear block codes under ML decoding. The bounds are applied to low-density parity-check (LDPC) codes, and the improvement in their tightness is exemplified numerically.",
        "published": "2005-05-30T06:29:58Z",
        "link": "http://arxiv.org/abs/cs/0505078v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Diagnosis of weaknesses in modern error correction codes: a physics   approach",
        "authors": [
            "M. G. Stepanov",
            "V. Chernyak",
            "M. Chertkov",
            "B. Vasic"
        ],
        "summary": "One of the main obstacles to the wider use of the modern error-correction codes is that, due to the complex behavior of their decoding algorithms, no systematic method which would allow characterization of the Bit-Error-Rate (BER) is known. This is especially true at the weak noise where many systems operate and where coding performance is difficult to estimate because of the diminishingly small number of errors. We show how the instanton method of physics allows one to solve the problem of BER analysis in the weak noise range by recasting it as a computationally tractable minimization problem.",
        "published": "2005-06-01T22:16:20Z",
        "link": "http://arxiv.org/abs/cond-mat/0506037v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Approximate MAP Decoding on Tail-Biting Trellises",
        "authors": [
            "A. S. Madhu",
            "Priti Shankar"
        ],
        "summary": "We propose two approximate algorithms for MAP decoding on tail-biting trellises. The algorithms work on a subset of nodes of the tail-biting trellis, judiciously selected. We report the results of simulations on an AWGN channel using the approximate algorithms on tail-biting trellises for the $(24,12)$ Extended Golay Code and a rate 1/2 convolutional code with memory 6.",
        "published": "2005-06-03T06:58:47Z",
        "link": "http://arxiv.org/abs/cs/0506009v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the dimensions of certain LDPC codes based on q-regular bipartite   graphs",
        "authors": [
            "Peter Sin",
            "Qing Xiang"
        ],
        "summary": "An explicit construction of a family of binary LDPC codes called LU(3,q), where q is a power of a prime, was recently given. A conjecture was made for the dimensions of these codes when q is odd. The conjecture is proved in this note. The proof involves the geometry of a 4-dimensional symplectic vector space and the action of the symplectic group and its subgroups.",
        "published": "2005-06-05T11:44:10Z",
        "link": "http://arxiv.org/abs/cs/0506011v3",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "A Non-Cooperative Power Control Game in Delay-Constrained   Multiple-Access Networks",
        "authors": [
            "Farhad Meshkati",
            "H. Vincent Poor",
            "Stuart C. Schwartz"
        ],
        "summary": "A game-theoretic approach for studying power control in multiple-access networks with transmission delay constraints is proposed. A non-cooperative power control game is considered in which each user seeks to choose a transmit power that maximizes its own utility while satisfying the user's delay requirements. The utility function measures the number of reliable bits transmitted per joule of energy and the user's delay constraint is modeled as an upper bound on the delay outage probability. The Nash equilibrium for the proposed game is derived, and its existence and uniqueness are proved. Using a large-system analysis, explicit expressions for the utilities achieved at equilibrium are obtained for the matched filter, decorrelating and minimum mean square error multiuser detectors. The effects of delay constraints on the users' utilities (in bits/Joule) and network capacity (i.e., the maximum number of users that can be supported) are quantified.",
        "published": "2005-06-05T17:47:23Z",
        "link": "http://arxiv.org/abs/cs/0506012v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the existence and characterization of the maxent distribution under   general moment inequality constraints",
        "authors": [
            "Prakash Ishwar",
            "Pierre Moulin"
        ],
        "summary": "A broad set of sufficient conditions that guarantees the existence of the maximum entropy (maxent) distribution consistent with specified bounds on certain generalized moments is derived. Most results in the literature are either focused on the minimum cross-entropy distribution or apply only to distributions with a bounded-volume support or address only equality constraints. The results of this work hold for general moment inequality constraints for probability distributions with possibly unbounded support, and the technical conditions are explicitly on the underlying generalized moment functions. An analytical characterization of the maxent distribution is also derived using results from the theory of constrained optimization in infinite-dimensional normed linear spaces. Several auxiliary results of independent interest pertaining to certain properties of convex coercive functions are also presented.",
        "published": "2005-06-05T20:02:08Z",
        "link": "http://arxiv.org/abs/cs/0506013v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Compressing Probability Distributions",
        "authors": [
            "Travis Gagie"
        ],
        "summary": "We show how to store good approximations of probability distributions in small space.",
        "published": "2005-06-06T12:28:33Z",
        "link": "http://arxiv.org/abs/cs/0506016v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "On the Achievable Diversity-Multiplexing Tradeoffs in Half-Duplex   Cooperative Channels",
        "authors": [
            "Kambiz Azarian",
            "Hesham El Gamal",
            "Philip Schniter"
        ],
        "summary": "In this paper, we propose novel cooperative transmission protocols for delay limited coherent fading channels consisting of N (half-duplex and single-antenna) partners and one cell site. In our work, we differentiate between the relay, cooperative broadcast (down-link), and cooperative multiple-access (up-link) channels. For the relay channel, we investigate two classes of cooperation schemes; namely, Amplify and Forward (AF) protocols and Decode and Forward (DF) protocols. For the first class, we establish an upper bound on the achievable diversity-multiplexing tradeoff with a single relay. We then construct a new AF protocol that achieves this upper bound. The proposed algorithm is then extended to the general case with N-1 relays where it is shown to outperform the space-time coded protocol of Laneman and Worenell without requiring decoding/encoding at the relays. For the class of DF protocols, we develop a dynamic decode and forward (DDF) protocol that achieves the optimal tradeoff for multiplexing gains 0 < r < 1/N. Furthermore, with a single relay, the DDF protocol is shown to dominate the class of AF protocols for all multiplexing gains. The superiority of the DDF protocol is shown to be more significant in the cooperative broadcast channel. The situation is reversed in the cooperative multiple-access channel where we propose a new AF protocol that achieves the optimal tradeoff for all multiplexing gains. A distinguishing feature of the proposed protocols in the three scenarios is that they do not rely on orthogonal subspaces, allowing for a more efficient use of resources. In fact, using our results one can argue that the sub-optimality of previously proposed protocols stems from their use of orthogonal subspaces rather than the half-duplex constraint.",
        "published": "2005-06-07T02:30:28Z",
        "link": "http://arxiv.org/abs/cs/0506018v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "On the Throughput-Delay Tradeoff in Cellular Multicast",
        "authors": [
            "Praveen Kumar Gopala",
            "Hesham El Gamal"
        ],
        "summary": "In this paper, we adopt a cross layer design approach for analyzing the throughput-delay tradeoff of the multicast channel in a single cell system. To illustrate the main ideas, we start with the single group case, i.e., pure multicast, where a common information stream is requested by all the users. We consider three classes of scheduling algorithms with progressively increasing complexity. The first class strives for minimum complexity by resorting to a static scheduling strategy along with memoryless decoding. Our analysis for this class of scheduling algorithms reveals the existence of a static scheduling policy that achieves the optimal scaling law of the throughput at the expense of a delay that increases exponentially with the number of users. The second scheduling policy resorts to a higher complexity incremental redundancy encoding/decoding strategy to achieve a superior throughput-delay tradeoff. The third, and most complex, scheduling strategy benefits from the cooperation between the different users to minimize the delay while achieving the optimal scaling law of the throughput. In particular, the proposed cooperative multicast strategy is shown to simultaneously achieve the optimal scaling laws of both throughput and delay. Then, we generalize our scheduling algorithms to exploit the multi-group diversity available when different information streams are requested by different subsets of the user population. Finally, we discuss the effect of the potential gains of equipping the base station with multi-transmit antennas and present simulation results that validate our theoretical claims.",
        "published": "2005-06-07T21:03:53Z",
        "link": "http://arxiv.org/abs/cs/0506020v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Asymptotics of Discrete MDL for Online Prediction",
        "authors": [
            "Jan Poland",
            "Marcus Hutter"
        ],
        "summary": "Minimum Description Length (MDL) is an important principle for induction and prediction, with strong relations to optimal Bayesian learning. This paper deals with learning non-i.i.d. processes by means of two-part MDL, where the underlying model class is countable. We consider the online learning framework, i.e. observations come in one by one, and the predictor is allowed to update his state of mind after each time step. We identify two ways of predicting by MDL for this setup, namely a static} and a dynamic one. (A third variant, hybrid MDL, will turn out inferior.) We will prove that under the only assumption that the data is generated by a distribution contained in the model class, the MDL predictions converge to the true values almost surely. This is accomplished by proving finite bounds on the quadratic, the Hellinger, and the Kullback-Leibler loss of the MDL learner, which are however exponentially worse than for Bayesian prediction. We demonstrate that these bounds are sharp, even for model classes containing only Bernoulli distributions. We show how these bounds imply regret bounds for arbitrary loss functions. Our results apply to a wide range of setups, namely sequence prediction, pattern classification, regression, and universal induction in the sense of Algorithmic Information Theory among others.",
        "published": "2005-06-08T09:07:23Z",
        "link": "http://arxiv.org/abs/cs/0506022v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "math.ST",
            "stat.TH",
            "I.2.6; E.4; G.3"
        ]
    },
    {
        "title": "Dynamic Asymmetric Communication",
        "authors": [
            "Travis Gagie"
        ],
        "summary": "We show how any dynamic instantaneous compression algorithm can be converted to an asymmetric communication protocol, with which a server with high bandwidth can help clients with low bandwidth send it messages. Unlike previous authors, we do not assume the server knows the messages' distribution, and our protocols are the first to use only one round of communication for each message.",
        "published": "2005-06-08T22:02:37Z",
        "link": "http://arxiv.org/abs/cs/0506025v2",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Neyman-Pearson Detection of Gauss-Markov Signals in Noise: Closed-Form   Error Exponent and Properties",
        "authors": [
            "Youngchul Sung",
            "Lang Tong",
            "H. Vincent Poor"
        ],
        "summary": "The performance of Neyman-Pearson detection of correlated stochastic signals using noisy observations is investigated via the error exponent for the miss probability with a fixed level. Using the state-space structure of the signal and observation model, a closed-form expression for the error exponent is derived, and the connection between the asymptotic behavior of the optimal detector and that of the Kalman filter is established. The properties of the error exponent are investigated for the scalar case. It is shown that the error exponent has distinct characteristics with respect to correlation strength: for signal-to-noise ratio (SNR) >1 the error exponent decreases monotonically as the correlation becomes stronger, whereas for SNR <1 there is an optimal correlation that maximizes the error exponent for a given SNR.",
        "published": "2005-06-08T22:43:07Z",
        "link": "http://arxiv.org/abs/cs/0506028v2",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "A Unified Framework for Tree Search Decoding : Rediscovering the   Sequential Decoder",
        "authors": [
            "Arul D. Murugan",
            "Hesham El Gamal",
            "Mohamed Oussama Damen",
            "Giuseppe Caire"
        ],
        "summary": "We consider receiver design for coded transmission over linear Gaussian channels. We restrict ourselves to the class of lattice codes and formulate the joint detection and decoding problem as a closest lattice point search (CLPS). Here, a tree search framework for solving the CLPS is adopted. In our framework, the CLPS algorithm decomposes into the preprocessing and tree search stages. The role of the preprocessing stage is to expose the tree structure in a form {\\em matched} to the search stage. We argue that the minimum mean square error decision feedback (MMSE-DFE) frontend is instrumental for solving the joint detection and decoding problem in a single search stage. It is further shown that MMSE-DFE filtering allows for using lattice reduction methods to reduce complexity, at the expense of a marginal performance loss, and solving under-determined linear systems. For the search stage, we present a generic method, based on the branch and bound (BB) algorithm, and show that it encompasses all existing sphere decoders as special cases. The proposed generic algorithm further allows for an interesting classification of tree search decoders, sheds more light on the structural properties of all known sphere decoders, and inspires the design of more efficient decoders. In particular, an efficient decoding algorithm that resembles the well known Fano sequential decoder is identified. The excellent performance-complexity tradeoff achieved by the proposed MMSE-Fano decoder is established via simulation results and analytical arguments in several MIMO and ISI scenarios.",
        "published": "2005-06-09T07:48:59Z",
        "link": "http://arxiv.org/abs/cs/0506029v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Entropy and Quantum Kolmogorov Complexity: A Quantum Brudno's Theorem",
        "authors": [
            "Fabio Benatti",
            "Tyll Krueger",
            "Markus Mueller",
            "Rainer Siegmund-Schultze",
            "Arleta Szkola"
        ],
        "summary": "In classical information theory, entropy rate and Kolmogorov complexity per symbol are related by a theorem of Brudno. In this paper, we prove a quantum version of this theorem, connecting the von Neumann entropy rate and two notions of quantum Kolmogorov complexity, both based on the shortest qubit descriptions of qubit strings that, run by a universal quantum Turing machine, reproduce them as outputs.",
        "published": "2005-06-10T14:27:43Z",
        "link": "http://arxiv.org/abs/quant-ph/0506080v3",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math-ph",
            "math.DS",
            "math.IT",
            "math.MP"
        ]
    },
    {
        "title": "Non prefix-free codes for constrained sequences",
        "authors": [
            "Marco Dalai",
            "Riccardo Leonardi"
        ],
        "summary": "In this paper we consider the use of variable length non prefix-free codes for coding constrained sequences of symbols. We suppose to have a Markov source where some state transitions are impossible, i.e. the stochastic matrix associated with the Markov chain has some null entries. We show that classic Kraft inequality is not a necessary condition, in general, for unique decodability under the above hypothesis and we propose a relaxed necessary inequality condition. This allows, in some cases, the use of non prefix-free codes that can give very good performance, both in terms of compression and computational efficiency. Some considerations are made on the relation between the proposed approach and other existing coding paradigms.",
        "published": "2005-06-10T16:27:48Z",
        "link": "http://arxiv.org/abs/cs/0506036v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Tradeoff Between Source and Channel Coding for Erasure Channels",
        "authors": [
            "Sriram N. Kizhakkemadam",
            "Panos Papamichalis",
            "Mandyam Srinath",
            "Dinesh Rajan"
        ],
        "summary": "In this paper, we investigate the optimal tradeoff between source and channel coding for channels with bit or packet erasure. Upper and Lower bounds on the optimal channel coding rate are computed to achieve minimal end-to-end distortion. The bounds are calculated based on a combination of sphere packing, straight line and expurgated error exponents and also high rate vector quantization theory. By modeling a packet erasure channel in terms of an equivalent bit erasure channel, we obtain bounds on the packet size for a specified limit on the distortion.",
        "published": "2005-06-10T22:49:27Z",
        "link": "http://arxiv.org/abs/cs/0506037v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Antenna array geometry and coding performance",
        "authors": [
            "Weijun Zhu",
            "Heechoon Lee",
            "Daniel Liu",
            "Michael P. Fitz"
        ],
        "summary": "This paper provides details about experiments in realistic, urban, and frequency flat channels with space-time coding that specifically examines the impact of the number of receive antennas and the design criteria for code selection on the performance. Also the performance characteristics are examined of the coded modulations in the presence of finite size array geometries. This paper gives some insight into which of the theories are most useful in realistic deployments.",
        "published": "2005-06-11T02:38:26Z",
        "link": "http://arxiv.org/abs/cs/0506039v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Fixed-Length Coding Algorithm for DNA Sequence Compression",
        "authors": [
            "Jie Liu",
            "Sheng Bao",
            "Zhiqiang Jing",
            "Shi Chen"
        ],
        "summary": "While achieving a compression ratio of 2.0 bits/base, the new algorithm codes non-N bases in fixed length. It dramatically reduces the time of coding and decoding than previous DNA compression algorithms and some universal compression programs.",
        "published": "2005-06-12T08:49:07Z",
        "link": "http://arxiv.org/abs/cs/0506040v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "J.3; E.4"
        ]
    },
    {
        "title": "Tree-Based Construction of LDPC Codes",
        "authors": [
            "Deepak Sridhara",
            "Christine Kelley",
            "Joachim Rosenthal"
        ],
        "summary": "We present a construction of LDPC codes that have minimum pseudocodeword weight equal to the minimum distance, and perform well with iterative decoding. The construction involves enumerating a d-regular tree for a fixed number of layers and employing a connection algorithm based on mutually orthogonal Latin squares to close the tree. Methods are presented for degrees d=p^s and d = p^s+1, for p a prime, -- one of which includes the well-known finite-geometry-based LDPC codes.",
        "published": "2005-06-12T09:00:09Z",
        "link": "http://arxiv.org/abs/cs/0506042v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Decision Feedback Based Scheme for Slepian-Wolf Coding of sources with   Hidden Markov Correlation",
        "authors": [
            "Krishna R. Narayanan",
            "Kapil Bhattad"
        ],
        "summary": "We consider the problem of compression of two memoryless binary sources, the correlation between which is defined by a Hidden Markov Model (HMM). We propose a Decision Feedback (DF) based scheme which when used with low density parity check codes results in compression close to the Slepian Wolf limits.",
        "published": "2005-06-12T15:18:07Z",
        "link": "http://arxiv.org/abs/cs/0506045v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Minimal Network Coding for Multicast",
        "authors": [
            "Kapil Bhattad",
            "Niranjan Ratnakar",
            "Ralf Koetter",
            "Krishna R. Narayanan"
        ],
        "summary": "We give an information flow interpretation for multicasting using network coding. This generalizes the fluid model used to represent flows to a single receiver. Using the generalized model, we present a decentralized algorithm to minimize the number of packets that undergo network coding. We also propose a decentralized algorithm to construct capacity achieving multicast codes when the processing at some nodes is restricted to routing. The proposed algorithms can be coupled with existing decentralized schemes to achieve minimum cost muticast.",
        "published": "2005-06-12T15:20:05Z",
        "link": "http://arxiv.org/abs/cs/0506044v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Decision Feedback Based Scheme for Slepian-Wolf Coding of sources with   Hidden Markov Correlation",
        "authors": [
            "Krishna R. Narayanan",
            "Kapil Bhattad"
        ],
        "summary": "We consider the problem of compression of two memoryless binary sources, the correlation between which is defined by a Hidden Markov Model (HMM). We propose a Decision Feedback (DF) based scheme which when used with low density parity check codes results in compression close to the Slepian Wolf limits.",
        "published": "2005-06-13T13:17:21Z",
        "link": "http://arxiv.org/abs/cs/0506043v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Comments on `Bit Interleaved Coded Modulation'",
        "authors": [
            "Vignesh Sethuraman",
            "Bruce Hajek"
        ],
        "summary": "Caire, Taricco and Biglieri presented a detailed analysis of bit interleaved coded modulation, a simple and popular technique used to improve system performance, especially in the context of fading channels. They derived an upper bound to the probability of error, called the expurgated bound. In this correspondence, the proof of the expurgated bound is shown to be flawed. A new upper bound is also derived. It is not known whether the original expurgated bound is valid for the important special case of square QAM with Gray labeling, but the new bound is very close to, and slightly tighter than, the original bound for a numerical example.",
        "published": "2005-06-13T16:38:01Z",
        "link": "http://arxiv.org/abs/cs/0506052v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Analysis on Transmit Antenna Selection for Spatial Multiplexing Systems:   A Geometrical Approach",
        "authors": [
            "Hongyuan Zhang",
            "Huaiyu Dai",
            "Quan Zhou",
            "Brian L. Hughes"
        ],
        "summary": "Recently, the remarkable potential of a multiple-input multiple-output (MIMO) wireless communication system was unveiled for its ability to provide spatial diversity or multiplexing gains. For MIMO diversity schemes, it is already known that. by the optimal antenna selection maximizing the post-processing signal-to-noise ratio, the diversity order of the full system can be maintained. On the other hand, the diversity order achieved by antenna selection in spatial multiplexing systems, especially those exploiting practical coding and decoding schemes, has not been rigorously analyzed thus far. In this paper, from a geometric standpoint, we propose a new framework for theoretically analyzing the diversity order achieved by transmit antenna selection for separately encoded spatial multiplexing systems with linear and decision-feedback receivers. We rigorously show that a diversity order of (Nt-1)(Nr-1) can be achieved for an Nr by Nt SM system when L=2 antennas are selected from the transmit side; while for L>2 scenarios, we give bounds for the achievable diversity order and show that the optimal diversity order is at least (Nt-L+1)(Nr-L+1) . Furthermore, the same geometrical approach can be used to evaluate the diversity-multiplexing tradeoff curves for the considered spatial multiplexing systems with transmit antenna selection.",
        "published": "2005-06-13T16:45:37Z",
        "link": "http://arxiv.org/abs/cs/0506053v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Large Alphabets and Incompressibility",
        "authors": [
            "Travis Gagie"
        ],
        "summary": "We briefly survey some concepts related to empirical entropy -- normal numbers, de Bruijn sequences and Markov processes -- and investigate how well it approximates Kolmogorov complexity. Our results suggest $\\ell$th-order empirical entropy stops being a reasonable complexity metric for almost all strings of length $m$ over alphabets of size $n$ about when $n^\\ell$ surpasses $m$.",
        "published": "2005-06-14T03:08:57Z",
        "link": "http://arxiv.org/abs/cs/0506056v3",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "An MSE Based Ttransfer Chart to Analyze Iterative Decoding Schemes",
        "authors": [
            "Kapil Bhattad",
            "Krishna Narayanan"
        ],
        "summary": "An alternative to extrinsic information transfer (EXIT) charts called mean squared error (MSE) charts that use a measure related to the MSE instead of mutual information is proposed. Using the relationship between mutual information and minimum mean squared error (MMSE), a relationship between the rate of any code and the area under a plot of MSE versus signal to noise ratio (SNR) is obtained, when the log likelihood ratios (LLR) can be assumed to be from a Gaussian channel. Using this result, a theoretical justification is provided for designing concatenated codes by matching the EXIT charts of the inner and outer decoders, when the LLRs are Gaussian which is typically assumed for code design using EXIT charts. Finally, for the special case of AWGN channel it is shown that any capacity achieving code has an EXIT curve that is flat. This extends Ashikhmin et als results for erasure channels to the Gaussian channel.",
        "published": "2005-06-14T07:56:14Z",
        "link": "http://arxiv.org/abs/cs/0506058v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Fast directional continuous spherical wavelet transform algorithms",
        "authors": [
            "J. D. McEwen",
            "M. P. Hobson",
            "D. J. Mortlock",
            "A. N. Lasenby"
        ],
        "summary": "We describe the construction of a spherical wavelet analysis through the inverse stereographic projection of the Euclidean planar wavelet framework, introduced originally by Antoine and Vandergheynst and developed further by Wiaux et al. Fast algorithms for performing the directional continuous wavelet analysis on the unit sphere are presented. The fast directional algorithm, based on the fast spherical convolution algorithm developed by Wandelt and Gorski, provides a saving of O(sqrt(Npix)) over a direct quadrature implementation for Npix pixels on the sphere, and allows one to perform a directional spherical wavelet analysis of a 10^6 pixel map on a personal computer.",
        "published": "2005-06-14T15:01:53Z",
        "link": "http://arxiv.org/abs/astro-ph/0506308v3",
        "categories": [
            "astro-ph",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A CDMA multiuser detection algorithm based on survey propagation",
        "authors": [
            "Yoshiyuki Kabashima"
        ],
        "summary": "A computationally tractable CDMA multiuser detection algorithm is developed based on survey propagation.",
        "published": "2005-06-15T01:40:57Z",
        "link": "http://arxiv.org/abs/cs/0506062v4",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Optimal multiple assignments based on integer programming in secret   sharing schemes with general access structures",
        "authors": [
            "Mitsugu Iwamoto",
            "Hirosuke Yamamoto",
            "Hirohisa Ogawa"
        ],
        "summary": "It is known that for any general access structure, a secret sharing scheme (SSS) can be constructed from an (m,m)-threshold scheme by using the so-called cumulative map or from a (t,m)-threshold SSS by a modified cumulative map. However, such constructed SSSs are not efficient generally. In this paper, we propose a new method to construct a SSS from a $(t,m)$-threshold scheme for any given general access structure. In the proposed method, integer programming is used to distribute optimally the shares of (t,m)-threshold scheme to each participant of the general access structure. From the optimality, it can always attain lower coding rate than the cumulative maps except the cases that they give the optimal distribution. The same method is also applied to construct SSSs for incomplete access structures and/or ramp access structures.",
        "published": "2005-06-15T06:07:42Z",
        "link": "http://arxiv.org/abs/cs/0506064v1",
        "categories": [
            "cs.CR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Strongly secure ramp secret sharing schemes for general access   structures",
        "authors": [
            "Mitsugu Iwamoto",
            "Hirosuke Yamamoto"
        ],
        "summary": "Ramp secret sharing (SS) schemes can be classified into strong ramp SS schemes and weak ramp SS schemes. The strong ramp SS schemes do not leak out any part of a secret explicitly even in the case where some information about the secret leaks from a non-qualified set of shares, and hence, they are more desirable than weak ramp SS schemes. However, it is not known how to construct the strong ramp SS schemes in the case of general access structures. In this paper, it is shown that a strong ramp SS scheme can always be constructed from a SS scheme with plural secrets for any feasible general access structure. As a byproduct, it is pointed out that threshold ramp SS schemes based on Shamir's polynomial interpolation method are {\\em not} always strong.",
        "published": "2005-06-15T06:36:17Z",
        "link": "http://arxiv.org/abs/cs/0506065v1",
        "categories": [
            "cs.CR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Performance Analysis of Algebraic Soft Decoding of Reed-Solomon Codes   over Binary Symmetric and Erasure Channels",
        "authors": [
            "Jing Jiang",
            "Krishna R. Narayanan"
        ],
        "summary": "In this paper, we characterize the decoding region of algebraic soft decoding (ASD) of Reed-Solomon (RS) codes over erasure channels and binary symmetric channel (BSC). Optimal multiplicity assignment strategies (MAS) are investigated and tight bounds are derived to show the ASD can significantly outperform conventional Berlekamp Massey (BM) decoding over these channels for a wide code rate range. The analysis technique can also be extended to other channel models, e.g., RS coded modulation over erasure channels.",
        "published": "2005-06-17T02:20:52Z",
        "link": "http://arxiv.org/abs/cs/0506072v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Iterative Soft Input Soft Output Decoding of Reed-Solomon Codes by   Adapting the Parity Check Matrix",
        "authors": [
            "Jing Jiang",
            "Krishna R. Narayanan"
        ],
        "summary": "An iterative algorithm is presented for soft-input-soft-output (SISO) decoding of Reed-Solomon (RS) codes. The proposed iterative algorithm uses the sum product algorithm (SPA) in conjunction with a binary parity check matrix of the RS code. The novelty is in reducing a submatrix of the binary parity check matrix that corresponds to less reliable bits to a sparse nature before the SPA is applied at each iteration. The proposed algorithm can be geometrically interpreted as a two-stage gradient descent with an adaptive potential function. This adaptive procedure is crucial to the convergence behavior of the gradient descent algorithm and, therefore, significantly improves the performance. Simulation results show that the proposed decoding algorithm and its variations provide significant gain over hard decision decoding (HDD) and compare favorably with other popular soft decision decoding methods.",
        "published": "2005-06-17T02:46:09Z",
        "link": "http://arxiv.org/abs/cs/0506073v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Stability of Scheduled Multi-access Communication over Quasi-static Flat   Fading Channels with Random Coding and Independent Decoding",
        "authors": [
            "KCV Kalyanarama Sesha Sayee",
            "Utpal Mukherji"
        ],
        "summary": "The stability of scheduled multiaccess communication with random coding and independent decoding of messages is investigated. The number of messages that may be scheduled for simultaneous transmission is limited to a given maximum value, and the channels from transmitters to receiver are quasi-static, flat, and have independent fades. Requests for message transmissions are assumed to arrive according to an i.i.d. arrival process. Then, we show the following: (1) in the limit of large message alphabet size, the stability region has an interference limited information-theoretic capacity interpretation, (2) state-independent scheduling policies achieve this asymptotic stability region, and (3) in the asymptotic limit corresponding to immediate access, the stability region for non-idling scheduling policies is shown to be identical irrespective of received signal powers.",
        "published": "2005-06-20T07:51:33Z",
        "link": "http://arxiv.org/abs/cs/0506077v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Maxwell Construction: The Hidden Bridge between Iterative and Maximum a   Posteriori Decoding",
        "authors": [
            "Cyril Measson",
            "Andrea Montanari",
            "Ruediger Urbanke"
        ],
        "summary": "There is a fundamental relationship between belief propagation and maximum a posteriori decoding. A decoding algorithm, which we call the Maxwell decoder, is introduced and provides a constructive description of this relationship. Both, the algorithm itself and the analysis of the new decoder are reminiscent of the Maxwell construction in thermodynamics. This paper investigates in detail the case of transmission over the binary erasure channel, while the extension to general binary memoryless channels is discussed in a companion paper.",
        "published": "2005-06-21T09:20:19Z",
        "link": "http://arxiv.org/abs/cs/0506083v1",
        "categories": [
            "cs.IT",
            "cond-mat.dis-nn",
            "math.IT"
        ]
    },
    {
        "title": "Large System Decentralized Detection Performance Under Communication   Constraints",
        "authors": [
            "Sudharman K. Jayaweera"
        ],
        "summary": "The problem of decentralized detection in a sensor network subjected to a total average power constraint and all nodes sharing a common bandwidth is investigated. The bandwidth constraint is taken into account by assuming non-orthogonal communication between sensors and the data fusion center via direct-sequence code-division multiple-access (DS-CDMA). In the case of large sensor systems and random spreading, the asymptotic decentralized detection performance is derived assuming independent and identically distributed (iid) sensor observations via random matrix theory. The results show that, even under both power and bandwidth constraints, it is better to combine many not-so-good local decisions rather than relying on one (or a few) very-good local decisions.",
        "published": "2005-06-22T22:30:30Z",
        "link": "http://arxiv.org/abs/cs/0506086v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Primal-dual distance bounds of linear codes with application to   cryptography",
        "authors": [
            "Ryutaroh Matsumoto",
            "Kaoru Kurosawa",
            "Toshiya Itoh",
            "Toshimitsu Konno",
            "Tomohiko Uyematsu"
        ],
        "summary": "Let $N(d,d^\\perp)$ denote the minimum length $n$ of a linear code $C$ with $d$ and $d^{\\bot}$, where $d$ is the minimum Hamming distance of $C$ and $d^{\\bot}$ is the minimum Hamming distance of $C^{\\bot}$. In this paper, we show a lower bound and an upper bound on $N(d,d^\\perp)$. Further, for small values of $d$ and $d^\\perp$, we determine $N(d,d^\\perp)$ and give a generator matrix of the optimum linear code. This problem is directly related to the design method of cryptographic Boolean functions suggested by Kurosawa et al.",
        "published": "2005-06-24T07:35:51Z",
        "link": "http://arxiv.org/abs/cs/0506087v2",
        "categories": [
            "cs.IT",
            "cs.CR",
            "math.IT"
        ]
    },
    {
        "title": "An Alternative to Huffman's Algorithm for Constructing Variable-Length   Codes",
        "authors": [
            "Olivier Rioul"
        ],
        "summary": "This paper has been withdrawn by the author.",
        "published": "2005-06-24T09:53:17Z",
        "link": "http://arxiv.org/abs/cs/0506088v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The theoretical capacity of the Parity Source Coder",
        "authors": [
            "Stefano Ciliberti",
            "Marc Mezard"
        ],
        "summary": "The Parity Source Coder is a protocol for data compression which is based on a set of parity checks organized in a sparse random network. We consider here the case of memoryless unbiased binary sources. We show that the theoretical capacity saturate the Shannon limit at large K. We also find that the first corrections to the leading behavior are exponentially small, so that the behavior at finite K is very close to the optimal one.",
        "published": "2005-06-24T14:17:37Z",
        "link": "http://arxiv.org/abs/cond-mat/0506652v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A New Construction for LDPC Codes using Permutation Polynomials over   Integer Rings",
        "authors": [
            "Oscar Y. Takeshita"
        ],
        "summary": "A new construction is proposed for low density parity check (LDPC) codes using quadratic permutation polynomials over finite integer rings. The associated graphs for the new codes have both algebraic and pseudo-random nature, and the new codes are quasi-cyclic. Graph isomorphisms and automorphisms are identified and used in an efficient search for good codes. Graphs with girth as large as 12 were found. Upper bounds on the minimum Hamming distance are found both analytically and algorithmically. The bounds indicate that the minimum distance grows with block length. Near-codewords are one of the causes for error floors in LDPC codes; the new construction provides a good framework for studying near-codewords in LDPC codes. Nine example codes are given, and computer simulation results show the excellent error performance of these codes. Finally, connections are made between this new LDPC construction and turbo codes using interleavers generated by quadratic permutation polynomials.",
        "published": "2005-06-24T15:37:04Z",
        "link": "http://arxiv.org/abs/cs/0506091v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On Maximum Contention-Free Interleavers and Permutation Polynomials over   Integer Rings",
        "authors": [
            "Oscar Y. Takeshita"
        ],
        "summary": "An interleaver is a critical component for the channel coding performance of turbo codes. Algebraic constructions are of particular interest because they admit analytical designs and simple, practical hardware implementation. Contention-free interleavers have been recently shown to be suitable for parallel decoding of turbo codes. In this correspondence, it is shown that permutation polynomials generate maximum contention-free interleavers, i.e., every factor of the interleaver length becomes a possible degree of parallel processing of the decoder. Further, it is shown by computer simulations that turbo codes using these interleavers perform very well for the 3rd Generation Partnership Project (3GPP) standard.",
        "published": "2005-06-24T20:30:06Z",
        "link": "http://arxiv.org/abs/cs/0506093v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Universal Codes as a Basis for Nonparametric Testing of Serial   Independence for Time Series",
        "authors": [
            "Boris Ryabko",
            "Jaakko Astola"
        ],
        "summary": "We consider a stationary and ergodic source $p$ generated symbols $x_1 ... x_t$ from some finite set $A$ and a null hypothesis $H_0$ that $p$ is Markovian source with memory (or connectivity) not larger than $m, (m >= 0).$ The alternative hypothesis $H_1$ is that the sequence is generated by a stationary and ergodic source, which differs from the source under $H_0$. In particular, if $m= 0$ we have the null hypothesis $H_0$ that the sequence is generated by Bernoully source (or the hypothesis that $x_1 ...x_t$ are independent.) Some new tests which are based on universal codes and universal predictors, are suggested.",
        "published": "2005-06-26T06:06:36Z",
        "link": "http://arxiv.org/abs/cs/0506094v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On $m$-dimensional toric codes",
        "authors": [
            "John Little",
            "Ryan Schwarz"
        ],
        "summary": "Toric codes are a class of $m$-dimensional cyclic codes introduced recently by J. Hansen. They may be defined as evaluation codes obtained from monomials corresponding to integer lattice points in an integral convex polytope $P \\subseteq \\R^m$. As such, they are in a sense a natural extension of Reed-Solomon codes. Several authors have used intersection theory on toric surfaces to derive bounds on the minimum distance of some toric codes with $m = 2$. In this paper, we will provide a more elementary approach that applies equally well to many toric codes for all $m \\ge 2$. Our methods are based on a sort of multivariate generalization of Vandermonde determinants that has also been used in the study of multivariate polynomial interpolation. We use these Vandermonde determinants to determine the minimum distance of toric codes from rectangular polytopes and simplices. We also prove a general result showing that if there is a unimodular integer affine transformation taking one polytope $P_1$ to a second polytope $P_2$, then the corresponding toric codes are monomially equivalent (hence have the same parameters). We use this to begin a classification of two-dimensional toric codes with small dimension.",
        "published": "2005-06-29T21:09:37Z",
        "link": "http://arxiv.org/abs/cs/0506102v2",
        "categories": [
            "cs.IT",
            "math.AC",
            "math.AG",
            "math.IT"
        ]
    },
    {
        "title": "Asymptotically Optimal Tree-based Group Key Management Schemes",
        "authors": [
            "Hideyuki Sakai",
            "Hirosuke Yamamoto"
        ],
        "summary": "In key management schemes that realize secure multicast communications encrypted by group keys on a public network, tree structures are often used to update the group keys efficiently. Selcuk and Sidhu have proposed an efficient scheme which updates dynamically the tree structures based on the withdrawal probabilities of members. In this paper, it is shown that Selcuk-Sidhu scheme is asymptotically optimal for the cost of withdrawal. Furthermore, a new key management scheme, which takes account of key update costs of joining in addition to withdrawal, is proposed. It is proved that the proposed scheme is also asymptotically optimal, and it is shown by simulation that it can attain good performance for nonasymptotic cases.",
        "published": "2005-07-01T01:07:00Z",
        "link": "http://arxiv.org/abs/cs/0507001v1",
        "categories": [
            "cs.IT",
            "cs.CR",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "The Three Node Wireless Network: Achievable Rates and Cooperation   Strategies",
        "authors": [
            "Lifeng Lai",
            "Ke Liu",
            "Hesham El Gamal"
        ],
        "summary": "We consider a wireless network composed of three nodes and limited by the half-duplex and total power constraints. This formulation encompasses many of the special cases studied in the literature and allows for capturing the common features shared by them. Here, we focus on three special cases, namely 1) Relay Channel, 2) Multicast Channel, and 3) Conference Channel. These special cases are judicially chosen to reflect varying degrees of complexity while highlighting the common ground shared by the different variants of the three node wireless network. For the relay channel, we propose a new cooperation scheme that exploits the wireless feedback gain. This scheme combines the benefits of decode-and-forward and compress-and-forward strategies and avoids the idealistic feedback assumption adopted in earlier works. Our analysis of the achievable rate of this scheme reveals the diminishing feedback gain at both the low and high signal-to-noise ratio regimes. Inspired by the proposed feedback strategy, we identify a greedy cooperation framework applicable to both the multicast and conference channels. Our performance analysis reveals several nice properties of the proposed greedy approach and the central role of cooperative source-channel coding in exploiting the receiver side information in the wireless network setting. Our proofs for the cooperative multicast with side-information rely on novel nested and independent binning encoders along with a list decoder.",
        "published": "2005-07-01T02:36:13Z",
        "link": "http://arxiv.org/abs/cs/0507002v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An End-to-End Probabilistic Network Calculus with Moment Generating   Functions",
        "authors": [
            "Markus Fidler"
        ],
        "summary": "Network calculus is a min-plus system theory for performance evaluation of queuing networks. Its elegance stems from intuitive convolution formulas for concatenation of deterministic servers. Recent research dispenses with the worst-case assumptions of network calculus to develop a probabilistic equivalent that benefits from statistical multiplexing. Significant achievements have been made, owing for example to the theory of effective bandwidths, however, the outstanding scalability set up by concatenation of deterministic servers has not been shown.   This paper establishes a concise, probabilistic network calculus with moment generating functions. The presented work features closed-form, end-to-end, probabilistic performance bounds that achieve the objective of scaling linearly in the number of servers in series. The consistent application of moment generating functions put forth in this paper utilizes independence beyond the scope of current statistical multiplexing of flows. A relevant additional gain is demonstrated for tandem servers with independent cross-traffic.",
        "published": "2005-07-03T20:02:01Z",
        "link": "http://arxiv.org/abs/cs/0507004v2",
        "categories": [
            "cs.IT",
            "cs.PF",
            "math.IT"
        ]
    },
    {
        "title": "A Genetic Algorithm Based Finger Selection Scheme for UWB MMSE Rake   Receivers",
        "authors": [
            "Sinan Gezici",
            "Mung Chiang",
            "H. Vincent Poor",
            "Hisashi Kobayashi"
        ],
        "summary": "Due to a large number of multipath components in a typical ultra wideband (UWB) system, selective Rake (SRake) receivers, which combine energy from a subset of multipath components, are commonly employed. In order to optimize system performance, an optimal selection of multipath components to be employed at fingers of an SRake receiver needs to be considered. In this paper, this finger selection problem is investigated for a minimum mean square error (MMSE) UWB SRake receiver. Since the optimal solution is NP hard, a genetic algorithm (GA) based iterative scheme is proposed, which can achieve near-optimal performance after a reasonable number of iterations. Simulation results are presented to compare the performance of the proposed finger selection algorithm with those of the conventional and optimal schemes.",
        "published": "2005-07-04T05:08:37Z",
        "link": "http://arxiv.org/abs/cs/0507005v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Two-Step Time of Arrival Estimation Algorithm for Impulse Radio Ultra   Wideband Systems",
        "authors": [
            "Sinan Gezici",
            "Zafer Sahinoglu",
            "Andreas F. Molisch",
            "Hisashi Kobayashi",
            "H. Vincent Poor"
        ],
        "summary": "High time resolution of ultra wideband (UWB) signals facilitates very precise positioning capabilities based on time-of-arrival (TOA) measurements. Although the theoretical lower bound for TOA estimation can be achieved by the maximum likelihood principle, it is impractical due to the need for extremely high-rate sampling and the presence of large number of multipath components. On the other hand, the conventional correlation-based algorithm, which serially searches possible signal delays, takes a very long time to estimate the TOA of a received UWB signal. Moreover, the first signal path does not always have the strongest correlation output. Therefore, first path detection algorithms need to be considered. In this paper, a data-aided two-step TOA estimation algorithm is proposed. In order to speed up the estimation process, the first step estimates the rough TOA of the received signal based on received signal energy. Then, in the second step, the arrival time of the first signal path is estimated by considering a hypothesis testing approach. The proposed scheme uses low-rate correlation outputs, and is able to perform accurate TOA estimation in reasonable time intervals. The simulation results are presented to analyze the performance of the estimator.",
        "published": "2005-07-04T14:32:27Z",
        "link": "http://arxiv.org/abs/cs/0507006v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Utility-Based Approach to Power Control and Receiver Design in   Wireless Data Networks",
        "authors": [
            "Farhad Meshkati",
            "H. Vincent Poor",
            "Stuart C. Schwartz",
            "Narayan B. Mandayam"
        ],
        "summary": "In this work, the cross-layer design problem of joint multiuser detection and power control is studied using a game-theoretic approach. The uplink of a direct-sequence code division multiple access (DS-CDMA) data network is considered and a non-cooperative game is proposed in which users in the network are allowed to choose their uplink receivers as well as their transmit powers to maximize their own utilities. The utility function measures the number of reliable bits transmitted by the user per joule of energy consumed. Focusing on linear receivers, the Nash equilibrium for the proposed game is derived. It is shown that the equilibrium is one where the powers are SIR-balanced with the minimum mean square error (MMSE) detector as the receiver. In addition, this framework is used to study power control games for the matched filter, the decorrelator, and the MMSE detector; and the receivers' performance is compared in terms of the utilities achieved at equilibrium (in bits/Joule). The optimal cooperative solution is also discussed and compared with the non-cooperative approach. Extensions of the results to the case of multiple receive antennas are also presented. In addition, an admission control scheme based on maximizing the total utility in the network is proposed.",
        "published": "2005-07-05T15:43:12Z",
        "link": "http://arxiv.org/abs/cs/0507011v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Duality between Packings and Coverings of the Hamming Space",
        "authors": [
            "Gérard Cohen",
            "Alexander Vardy"
        ],
        "summary": "We investigate the packing and covering densities of linear and nonlinear binary codes, and establish a number of duality relationships between the packing and covering problems. Specifically, we prove that if almost all codes (in the class of linear or nonlinear codes) are good packings, then only a vanishing fraction of codes are good coverings, and vice versa: if almost all codes are good coverings, then at most a vanishing fraction of codes are good packings. We also show that any specific maximal binary code is either a good packing or a good covering, in a certain well-defined sense.",
        "published": "2005-07-06T13:01:43Z",
        "link": "http://arxiv.org/abs/cs/0507015v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT",
            "E.2; G.2.1"
        ]
    },
    {
        "title": "Optimal and Suboptimal Detection of Gaussian Signals in Noise:   Asymptotic Relative Efficiency",
        "authors": [
            "Youngchul Sung",
            "Lang Tong",
            "H. Vincent Poor"
        ],
        "summary": "The performance of Bayesian detection of Gaussian signals using noisy observations is investigated via the error exponent for the average error probability. Under unknown signal correlation structure or limited processing capability it is reasonable to use the simple quadratic detector that is optimal in the case of an independent and identically distributed (i.i.d.) signal. Using the large deviations principle, the performance of this detector (which is suboptimal for non-i.i.d. signals) is compared with that of the optimal detector for correlated signals via the asymptotic relative efficiency defined as the ratio between sample sizes of two detectors required for the same performance in the large-sample-size regime. The effects of SNR on the ARE are investigated. It is shown that the asymptotic efficiency of the simple quadratic detector relative to the optimal detector converges to one as the SNR increases without bound for any bounded spectrum, and that the simple quadratic detector performs as well as the optimal detector for a wide range of the correlation values at high SNR.",
        "published": "2005-07-06T19:05:09Z",
        "link": "http://arxiv.org/abs/cs/0507018v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "On Hilberg's Law and Its Links with Guiraud's Law",
        "authors": [
            "Łukasz Dȩbowski"
        ],
        "summary": "Hilberg (1990) supposed that finite-order excess entropy of a random human text is proportional to the square root of the text length. Assuming that Hilberg's hypothesis is true, we derive Guiraud's law, which states that the number of word types in a text is greater than proportional to the square root of the text length. Our derivation is based on some mathematical conjecture in coding theory and on several experiments suggesting that words can be defined approximately as the nonterminals of the shortest context-free grammar for the text. Such operational definition of words can be applied even to texts deprived of spaces, which do not allow for Mandelbrot's ``intermittent silence'' explanation of Zipf's and Guiraud's laws. In contrast to Mandelbrot's, our model assumes some probabilistic long-memory effects in human narration and might be capable of explaining Menzerath's law.",
        "published": "2005-07-07T18:53:14Z",
        "link": "http://arxiv.org/abs/cs/0507022v1",
        "categories": [
            "cs.CL",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Hard Problems of Algebraic Geometry Codes",
        "authors": [
            "Qi Cheng"
        ],
        "summary": "The minimum distance is one of the most important combinatorial characterizations of a code. The maximum likelihood decoding problem is one of the most important algorithmic problems of a code. While these problems are known to be hard for general linear codes, the techniques used to prove their hardness often rely on the construction of artificial codes. In general, much less is known about the hardness of the specific classes of natural linear codes. In this paper, we show that both problems are   NP-hard for algebraic geometry codes. We achieve this by reducing a well-known NP-complete problem to these problems using a randomized algorithm. The family of codes in the reductions are based on elliptic curves. They have positive rates, but the alphabet sizes are exponential in the block lengths.",
        "published": "2005-07-08T16:22:52Z",
        "link": "http://arxiv.org/abs/cs/0507026v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "The error-floor of LDPC codes in the Laplacian channel",
        "authors": [
            "M. G. Stepanov",
            "M. Chertkov"
        ],
        "summary": "We analyze the performance of Low-Density-Parity-Check codes in the error-floor domain where the Signal-to-Noise-Ratio, s, is large, s >> 1. We describe how the instanton method of theoretical physics, recently adapted to coding theory, solves the problem of characterizing the error-floor domain in the Laplacian channel. An example of the (155,64,20) LDPC code with four iterations (each iteration consisting of two semi-steps: from bits-to-checks and from checks-to-bits) of the min-sum decoding is discussed. A generalized computational tree analysis is devised to explain the rational structure of the leading instantons. The asymptotic for the symbol Bit-Error-Rate in the error-floor domain is comprised of individual instanton contributions, each estimated as ~ \\exp(-l_{inst;L} s), where the effective distances, l_{inst;L}, of the the leading instantons are 7.6, 8.0 and 8.0 respectively. (The Hamming distance of the code is 20.) The analysis shows that the instantons are distinctly different from the ones found for the same coding/decoding scheme performing over the Gaussian channel. We validate instanton results against direct simulations and offer an explanation for remarkable performance of the instanton approximation not only in the extremal, s -> \\infty, limit but also at the moderate s values of practical interest.",
        "published": "2005-07-12T02:02:23Z",
        "link": "http://arxiv.org/abs/cs/0507031v2",
        "categories": [
            "cs.IT",
            "cond-mat.dis-nn",
            "math.IT"
        ]
    },
    {
        "title": "Introduction to Quantum Message Space",
        "authors": [
            "R. D. Ogden"
        ],
        "summary": "This paper develops the quantum analog of the message ensemble of classical information theory as developed by Shannon and Khinchin. The principal mathematical tool is harmonic analysis on the free group with two generators.",
        "published": "2005-07-12T04:37:07Z",
        "link": "http://arxiv.org/abs/cs/0507032v2",
        "categories": [
            "cs.IT",
            "math.IT",
            "math.OA",
            "quant-ph",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Analyticity of Entropy Rate of Hidden Markov Chains",
        "authors": [
            "Guangyue Han",
            "Brian Marcus"
        ],
        "summary": "We prove that under mild positivity assumptions the entropy rate of a hidden Markov chain varies analytically as a function of the underlying Markov chain parameters. A general principle to determine the domain of analyticity is stated. An example is given to estimate the radius of convergence for the entropy rate. We then show that the positivity assumptions can be relaxed, and examples are given for the relaxed conditions. We study a special class of hidden Markov chains in more detail: binary hidden Markov chains with an unambiguous symbol, and we give necessary and sufficient conditions for analyticity of the entropy rate for this case. Finally, we show that under the positivity assumptions the hidden Markov chain {\\em itself} varies analytically, in a strong sense, as a function of the underlying Markov chain parameters.",
        "published": "2005-07-12T14:24:10Z",
        "link": "http://arxiv.org/abs/math/0507235v4",
        "categories": [
            "math.PR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Distributed Regression in Sensor Networks: Training Distributively with   Alternating Projections",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "Wireless sensor networks (WSNs) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing. The problem of distributed or decentralized estimation has often been considered in the context of parametric models. However, the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models. In this paper, a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of WSN applications including field estimation. Here, starting with the standard regularized kernel least-squares estimator, a message-passing algorithm for distributed estimation in WSNs is derived. The algorithm can be viewed as an instantiation of the successive orthogonal projection (SOP) algorithm. Various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach.",
        "published": "2005-07-18T00:45:12Z",
        "link": "http://arxiv.org/abs/cs/0507039v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Monotone Conditional Complexity Bounds on Future Prediction Errors",
        "authors": [
            "Alexey Chernov",
            "Marcus Hutter"
        ],
        "summary": "We bound the future loss when predicting any (computably) stochastic sequence online. Solomonoff finitely bounded the total deviation of his universal predictor M from the true distribution m by the algorithmic complexity of m. Here we assume we are at a time t>1 and already observed x=x_1...x_t. We bound the future prediction performance on x_{t+1}x_{t+2}... by a new variant of algorithmic complexity of m given x, plus the complexity of the randomness deficiency of x. The new complexity is monotone in its condition in the sense that this complexity can only decrease if the condition is prolonged. We also briefly discuss potential generalizations to Bayesian model classes and to classification problems.",
        "published": "2005-07-18T12:34:53Z",
        "link": "http://arxiv.org/abs/cs/0507041v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.IT",
            "math.IT",
            "I.2.6; E.4; G.3; F.1.3"
        ]
    },
    {
        "title": "The Entropy of a Binary Hidden Markov Process",
        "authors": [
            "O. Zuk",
            "I. Kanter",
            "E. Domany"
        ],
        "summary": "The entropy of a binary symmetric Hidden Markov Process is calculated as an expansion in the noise parameter epsilon. We map the problem onto a one-dimensional Ising model in a large field of random signs and calculate the expansion coefficients up to second order in epsilon. Using a conjecture we extend the calculation to 11th order and discuss the convergence of the resulting series.",
        "published": "2005-07-23T17:15:49Z",
        "link": "http://arxiv.org/abs/cs/0507060v1",
        "categories": [
            "cs.IT",
            "cond-mat.stat-mech",
            "math.IT",
            "math.ST",
            "stat.TH"
        ]
    },
    {
        "title": "On parity check collections for iterative erasure decoding that correct   all correctable erasure patterns of a given size",
        "authors": [
            "Henk D. L. Hollmann",
            "Ludo M. G. M. Tolhuizen"
        ],
        "summary": "Recently there has been interest in the construction of small parity check sets for iterative decoding of the Hamming code with the property that each uncorrectable (or stopping) set of size three is the support of a codeword and hence uncorrectable anyway. Here we reformulate and generalise the problem, and improve on this construction. First we show that a parity check collection that corrects all correctable erasure patterns of size m for the r-th order Hamming code (i.e, the Hamming code with codimension r) provides for all codes of codimension $r$ a corresponding ``generic'' parity check collection with this property. This leads naturally to a necessary and sufficient condition on such generic parity check collections. We use this condition to construct a generic parity check collection for codes of codimension r correcting all correctable erasure patterns of size at most m, for all r and m <= r, thus generalising the known construction for m=3. Then we discussoptimality of our construction and show that it can be improved for m>=3 and r large enough. Finally we discuss some directions for further research.",
        "published": "2005-07-28T11:55:28Z",
        "link": "http://arxiv.org/abs/cs/0507068v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "Dimensions of Copeland-Erdos Sequences",
        "authors": [
            "Xiaoyang Gu",
            "Jack H. Lutz",
            "Philippe Moser"
        ],
        "summary": "The base-$k$ {\\em Copeland-Erd\\\"os sequence} given by an infinite set $A$ of positive integers is the infinite sequence $\\CE_k(A)$ formed by concatenating the base-$k$ representations of the elements of $A$ in numerical order. This paper concerns the following four quantities.   The {\\em finite-state dimension} $\\dimfs (\\CE_k(A))$, a finite-state version of classical Hausdorff dimension introduced in 2001.   The {\\em finite-state strong dimension} $\\Dimfs(\\CE_k(A))$, a finite-state version of classical packing dimension introduced in 2004. This is a dual of $\\dimfs(\\CE_k(A))$ satisfying $\\Dimfs(\\CE_k(A))$ $\\geq \\dimfs(\\CE_k(A))$.   The {\\em zeta-dimension} $\\Dimzeta(A)$, a kind of discrete fractal dimension discovered many times over the past few decades.   The {\\em lower zeta-dimension} $\\dimzeta(A)$, a dual of $\\Dimzeta(A)$ satisfying $\\dimzeta(A)\\leq \\Dimzeta(A)$.   We prove the following.   $\\dimfs(\\CE_k(A))\\geq \\dimzeta(A)$. This extends the 1946 proof by Copeland and Erd\\\"os that the sequence $\\CE_k(\\mathrm{PRIMES})$ is Borel normal.   $\\Dimfs(\\CE_k(A))\\geq \\Dimzeta(A)$.   These bounds are tight in the strong sense that these four quantities can have (simultaneously) any four values in $[0,1]$ satisfying the four above-mentioned inequalities.",
        "published": "2005-07-30T04:30:05Z",
        "link": "http://arxiv.org/abs/cs/0508001v1",
        "categories": [
            "cs.CC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The accurate optimal-success/error-rate calculations applied to the   realizations of the reliable and short-period integer ambiguity resolution in   carrier-phase GPS/GNSS positioning",
        "authors": [
            "Kentaro Kondo"
        ],
        "summary": "The maximum-marginal-a-posteriori success rate of statistical decision under multivariate Gaussian error distribution on an integer lattice is almost rigorously calculated by using union-bound approximation and Monte Carlo integration. These calculations are applied to the revelation of the various possible realizations of the reliable and short-period integer ambiguity resolution in precise carrier-phase relative positioning by GPS/GNSS. The theoretical foundation and efficient methodology are systematically developed, and two types of the enhancement of union-bound approximation are proposed and examined.   The results revealed include an extremely high reliability under the condition of accurate carrier-phase measurements and a large number of visible satellites, its heavy degradation caused by the slight amount of differentiated ionospheric delays due to the nonvanishing baseline length between rover and reference receivers, and the advantages of the use of the multiple carrier frequencies. The succeeding initialization of the integer ambiguities is shown to overcome the disadvantageous condition of the nonvanishing baseline length effectively due to the reasonably assumed temporal and spatial constancy of differentiated ionospheric delays.",
        "published": "2005-08-02T06:07:58Z",
        "link": "http://arxiv.org/abs/cs/0508008v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "n-Channel Asymmetric Multiple-Description Lattice Vector Quantization",
        "authors": [
            "Jan Ostergaard",
            "Richard Heusdens",
            "Jesper Jensen"
        ],
        "summary": "We present analytical expressions for optimal entropy-constrained multiple-description lattice vector quantizers which, under high-resolutions assumptions, minimize the expected distortion for given packet-loss probabilities. We consider the asymmetric case where packet-loss probabilities and side entropies are allowed to be unequal and find optimal quantizers for any number of descriptions in any dimension. We show that the normalized second moments of the side-quantizers are given by that of an $L$-dimensional sphere independent of the choice of lattices. Furthermore, we show that the optimal bit-distribution among the descriptions is not unique. In fact, within certain limits, bits can be arbitrarily distributed.",
        "published": "2005-08-02T07:52:14Z",
        "link": "http://arxiv.org/abs/cs/0508012v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Relations between the Local Weight Distributions of a Linear Block Code,   Its Extended Code, and Its Even Weight Subcode",
        "authors": [
            "Kenji Yasunaga",
            "Toru Fujiwara"
        ],
        "summary": "Relations between the local weight distributions of a binary linear code, its extended code, and its even weight subcode are presented. In particular, for a code of which the extended code is transitive invariant and contains only codewords with weight multiples of four, the local weight distribution can be obtained from that of the extended code. Using the relations, the local weight distributions of the $(127,k)$ primitive BCH codes for $k\\leq50$, the $(127,64)$ punctured third-order Reed-Muller, and their even weight subcodes are obtained from the local weight distribution of the $(128,k)$ extended primitive BCH codes for $k\\leq50$ and the $(128,64)$ third-order Reed-Muller code. We also show an approach to improve an algorithm for computing the local weight distribution proposed before.",
        "published": "2005-08-02T08:25:57Z",
        "link": "http://arxiv.org/abs/cs/0508013v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Chosen-ciphertext attack on noncommutative Polly Cracker",
        "authors": [
            "Stanislav Bulygin"
        ],
        "summary": "We propose a chosen-ciphertext attack on recently presented noncommutative variant of the well-known Polly Cracker cryptosystem. We show that if one chooses parameters for this noncommutative Polly Cracker as initially proposed, than the system should be claimed as insecure.",
        "published": "2005-08-02T09:48:43Z",
        "link": "http://arxiv.org/abs/cs/0508015v2",
        "categories": [
            "cs.IT",
            "cs.CR",
            "math.IT"
        ]
    },
    {
        "title": "Spectral Factorization, Whitening- and Estimation Filter -- Stability,   Smoothness Properties and FIR Approximation Behavior",
        "authors": [
            "Holger Boche",
            "Volker Pohl"
        ],
        "summary": "A Wiener filter can be interpreted as a cascade of a whitening- and an estimation filter. This paper gives a detailed investigates of the properties of these two filters. Then the practical consequences for the overall Wiener filter are ascertained. It is shown that if the given spectral densities are smooth (Hoelder continuous) functions, the resulting Wiener filter will always be stable and can be approximated arbitrarily well by a finite impulse response (FIR) filter. Moreover, the smoothness of the spectral densities characterizes how fast the FIR filter approximates the desired filter characteristic. If on the other hand the spectral densities are continuous but not smooth enough, the resulting Wiener filter may not be stable.",
        "published": "2005-08-02T15:41:40Z",
        "link": "http://arxiv.org/abs/cs/0508018v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Minimal Pseudo-Codewords of Codes from Finite Geometries",
        "authors": [
            "Pascal O. Vontobel",
            "Roxana Smarandache",
            "Negar Kiyavash",
            "Jason Teutsch",
            "Dejan Vukobratovic"
        ],
        "summary": "In order to understand the performance of a code under maximum-likelihood (ML) decoding, it is crucial to know the minimal codewords. In the context of linear programming (LP) decoding, it turns out to be necessary to know the minimal pseudo-codewords. This paper studies the minimal codewords and minimal pseudo-codewords of some families of codes derived from projective and Euclidean planes. Although our numerical results are only for codes of very modest length, they suggest that these code families exhibit an interesting property. Namely, all minimal pseudo-codewords that are not multiples of a minimal codeword have an AWGNC pseudo-weight that is strictly larger than the minimum Hamming weight of the code. This observation has positive consequences not only for LP decoding but also for iterative decoding.",
        "published": "2005-08-02T16:34:05Z",
        "link": "http://arxiv.org/abs/cs/0508019v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "The Benefit of Thresholding in LP Decoding of LDPC Codes",
        "authors": [
            "Jon Feldman",
            "Ralf Koetter",
            "Pascal O. Vontobel"
        ],
        "summary": "Consider data transmission over a binary-input additive white Gaussian noise channel using a binary low-density parity-check code. We ask the following question: Given a decoder that takes log-likelihood ratios as input, does it help to modify the log-likelihood ratios before decoding? If we use an optimal decoder then it is clear that modifying the log-likelihoods cannot possibly help the decoder's performance, and so the answer is \"no.\" However, for a suboptimal decoder like the linear programming decoder, the answer might be \"yes\": In this paper we prove that for certain interesting classes of low-density parity-check codes and large enough SNRs, it is advantageous to truncate the log-likelihood ratios before passing them to the linear programming decoder.",
        "published": "2005-08-02T16:52:11Z",
        "link": "http://arxiv.org/abs/cs/0508014v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Capacity Gain from Transmitter and Receiver Cooperation",
        "authors": [
            "Chris T. K. Ng",
            "Andrea J. Goldsmith"
        ],
        "summary": "Capacity gain from transmitter and receiver cooperation are compared in a relay network where the cooperating nodes are close together. When all nodes have equal average transmit power along with full channel state information (CSI), it is proved that transmitter cooperation outperforms receiver cooperation, whereas the opposite is true when power is optimally allocated among the nodes but only receiver phase CSI is available. In addition, when the nodes have equal average power with receiver phase CSI only, cooperation is shown to offer no capacity improvement over a non-cooperative scheme with the same average network power. When the system is under optimal power allocation with full CSI, the decode-and-forward transmitter cooperation rate is close to its cut-set capacity upper bound, and outperforms compress-and-forward receiver cooperation. Moreover, it is shown that full CSI is essential in transmitter cooperation, while optimal power allocation is essential in receiver cooperation.",
        "published": "2005-08-02T18:03:14Z",
        "link": "http://arxiv.org/abs/cs/0508020v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Matrix Construction Using Cyclic Shifts of a Column",
        "authors": [
            "Andrew Z Tirkel",
            "Tom E Hall"
        ],
        "summary": "This paper describes the synthesis of matrices with good correlation, from cyclic shifts of pseudonoise columns. Optimum matrices result whenever the shift sequence satisfies the constant difference property. Known shift sequences with the constant (or almost constant) difference property are: Quadratic (Polynomial) and Reciprocal Shift modulo prime, Exponential Shift, Legendre Shift, Zech Logarithm Shift, and the shift sequences of some m-arrays. We use these shift sequences to produce arrays for watermarking of digital images. Matrices can also be unfolded into long sequences by diagonal unfolding (with no deterioration in correlation) or row-by-row unfolding, with some degradation in correlation.",
        "published": "2005-08-02T21:17:01Z",
        "link": "http://arxiv.org/abs/cs/0508022v1",
        "categories": [
            "cs.DM",
            "cs.CR",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Software Libraries and Their Reuse: Entropy, Kolmogorov Complexity, and   Zipf's Law",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "We analyze software reuse from the perspective of information theory and Kolmogorov complexity, assessing our ability to ``compress'' programs by expressing them in terms of software components reused from libraries. A common theme in the software reuse literature is that if we can only get the right environment in place-- the right tools, the right generalizations, economic incentives, a ``culture of reuse'' -- then reuse of software will soar, with consequent improvements in productivity and software quality. The analysis developed in this paper paints a different picture: the extent to which software reuse can occur is an intrinsic property of a problem domain, and better tools and culture can have only marginal impact on reuse rates if the domain is inherently resistant to reuse. We define an entropy parameter $H \\in [0,1]$ of problem domains that measures program diversity, and deduce from this upper bounds on code reuse and the scale of components with which we may work. For ``low entropy'' domains with $H$ near 0, programs are highly similar to one another and the domain is amenable to the Component-Based Software Engineering (CBSE) dream of programming by composing large-scale components. For problem domains with $H$ near 1, programs require substantial quantities of new code, with only a modest proportion of an application comprised of reused, small-scale components. Preliminary empirical results from Unix platforms support some of the predictions of our model.",
        "published": "2005-08-03T04:20:11Z",
        "link": "http://arxiv.org/abs/cs/0508023v3",
        "categories": [
            "cs.SE",
            "cs.IT",
            "cs.PL",
            "math.IT",
            "D.2.8; E.4; D.2.13"
        ]
    },
    {
        "title": "New Codes for OFDM with Low PMEPR",
        "authors": [
            "Kai-Uwe Schmidt",
            "Adolf Finger"
        ],
        "summary": "In this paper new codes for orthogonal frequency-division multiplexing (OFDM) with tightly controlled peak-to-mean envelope power ratio (PMEPR) are proposed. We identify a new family of sequences occuring in complementary sets and show that such sequences form subsets of a new generalization of the Reed--Muller codes. Contrarily to previous constructions we present a compact description of such codes, which makes them suitable even for larger block lengths. We also show that some previous constructions just occur as special cases in our construction.",
        "published": "2005-08-03T07:22:41Z",
        "link": "http://arxiv.org/abs/cs/0508024v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Signature coding for OR channel with asynchronous access",
        "authors": [
            "Sándor Győri"
        ],
        "summary": "Signature coding for multiple access OR channel is considered. We prove that in block asynchronous case the upper bound on the minimum code length asymptotically is the same as in the case of synchronous access.",
        "published": "2005-08-03T10:47:00Z",
        "link": "http://arxiv.org/abs/cs/0508025v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Simple Maximum-Likelihood Decoding of Generalized First-order   Reed-Muller Codes",
        "authors": [
            "Kai-Uwe Schmidt",
            "Adolf Finger"
        ],
        "summary": "An efficient decoder for the generalized first-order Reed-Muller code RM_q(1,m) is essential for the decoding of various block-coding schemes for orthogonal frequency-division multiplexing with reduced peak-to-mean power ratio. We present an efficient and simple maximum-likelihood decoding algorithm for RM_q(1,m). It is shown that this algorithm has lower complexity than other previously known maximum-likelihood decoders for RM_q(1,m).",
        "published": "2005-08-03T12:46:11Z",
        "link": "http://arxiv.org/abs/cs/0508026v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Expectation maximization as message passing",
        "authors": [
            "J. Dauwels",
            "S. Korl",
            "H. -A. Loeliger"
        ],
        "summary": "Based on prior work by Eckford, it is shown how expectation maximization (EM) may be viewed, and used, as a message passing algorithm in factor graphs.",
        "published": "2005-08-03T16:09:00Z",
        "link": "http://arxiv.org/abs/cs/0508027v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "Terminated LDPC Convolutional Codes with Thresholds Close to Capacity",
        "authors": [
            "Michael Lentmaier",
            "Arvind Sridharan",
            "Kamil Sh. Zigangirov",
            "Daniel J. Costello Jr"
        ],
        "summary": "An ensemble of LDPC convolutional codes with parity-check matrices composed of permutation matrices is considered. The convergence of the iterative belief propagation based decoder for terminated convolutional codes in the ensemble is analyzed for binary-input output-symmetric memoryless channels using density evolution techniques. We observe that the structured irregularity in the Tanner graph of the codes leads to significantly better thresholds when compared to corresponding LDPC block codes.",
        "published": "2005-08-03T22:27:03Z",
        "link": "http://arxiv.org/abs/cs/0508030v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Capacity Theorems for Quantum Multiple Access Channels",
        "authors": [
            "Jon Yard",
            "Igor Devetak",
            "Patrick Hayden"
        ],
        "summary": "We consider quantum channels with two senders and one receiver. For an arbitrary such channel, we give multi-letter characterizations of two different two-dimensional capacity regions. The first region characterizes the rates at which it is possible for one sender to send classical information while the other sends quantum information. The second region gives the rates at which each sender can send quantum information. We give an example of a channel for which each region has a single-letter description, concluding with a characterization of the rates at which each user can simultaneously send classical and quantum information.",
        "published": "2005-08-04T01:49:14Z",
        "link": "http://arxiv.org/abs/cs/0508031v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "quant-ph"
        ]
    },
    {
        "title": "Channel combining and splitting for cutoff rate improvement",
        "authors": [
            "Erdal Arikan"
        ],
        "summary": "The cutoff rate $R_0(W)$ of a discrete memoryless channel (DMC) $W$ is often used as a figure of merit, alongside the channel capacity $C(W)$. Given a channel $W$ consisting of two possibly correlated subchannels $W_1$, $W_2$, the capacity function always satisfies $C(W_1)+C(W_2) \\le C(W)$, while there are examples for which $R_0(W_1)+R_0(W_2) > R_0(W)$. This fact that cutoff rate can be ``created'' by channel splitting was noticed by Massey in his study of an optical modulation system modeled as a $M$'ary erasure channel. This paper demonstrates that similar gains in cutoff rate can be achieved for general DMC's by methods of channel combining and splitting. Relation of the proposed method to Pinsker's early work on cutoff rate improvement and to Imai-Hirakawa multi-level coding are also discussed.",
        "published": "2005-08-04T07:33:32Z",
        "link": "http://arxiv.org/abs/cs/0508034v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Codes for error detection, good or not good",
        "authors": [
            "Irina Naydenova",
            "Torleiv Klove"
        ],
        "summary": "Linear codes for error detection on a q-ary symmetric channel are studied. It is shown that for given dimension k and minimum distance d, there exists a value \\mu(d,k) such that if C is a code of length n >= \\mu(d,k), then neither C nor its dual are good for error detection. For d >> k or k << d good approximations for \\mu(d,k) are given. A generalization to non-linear codes is also given.",
        "published": "2005-08-04T12:48:03Z",
        "link": "http://arxiv.org/abs/cs/0508035v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Tight Bounds on the Redundancy of Huffman Codes",
        "authors": [
            "Soheil Mohajer",
            "Payam Pakzad",
            "Ali Kakhbod"
        ],
        "summary": "In this paper we study the redundancy of Huffman codes. In particular, we consider sources for which the probability of one of the source symbols is known. We prove a conjecture of Ye and Yeung regarding the upper bound on the redundancy of such Huffman codes, which yields in a tight upper bound. We also derive a tight lower bound for the redundancy under the same assumption.   We further apply the method introduced in this paper to other related problems. It is shown that several other previously known bounds with different constraints follow immediately from our results.",
        "published": "2005-08-04T19:29:18Z",
        "link": "http://arxiv.org/abs/cs/0508039v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Bounds on the Capacity of the Blockwise Noncoherent APSK-AWGN Channels",
        "authors": [
            "Daniel C. Cunha",
            "Jaime Portugheis"
        ],
        "summary": "Capacity of M-ary Amplitude and Phase-Shift Keying(M-APSK) over an Additive White Gaussian Noise(AWGN) channel that also introduces an unknown carrier phase rotation is considered. The phase remains constant over a block of L symbols and it is independent from block to block. Aiming to design codes with equally probable symbols, uniformly distributed channel inputs are assumed. Based on results of Peleg and Shamai for M-ary Phase Shift Keying(M-PSK) modulation, easily computable upper and lower bounds on the effective M-APSK capacity are derived. For moderate M and L and a broad range of Signal-to-Noise Ratios(SNR's), the bounds come close together. As in the case of M-PSK modulation, for large L the coherent capacity is approached.",
        "published": "2005-08-04T21:14:02Z",
        "link": "http://arxiv.org/abs/cs/0508040v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Sequential Predictions based on Algorithmic Complexity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "This paper studies sequence prediction based on the monotone Kolmogorov complexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is extremely close to Solomonoff's universal prior M, the latter being an excellent predictor in deterministic as well as probabilistic environments, where performance is measured in terms of convergence of posteriors or losses. Despite this closeness to M, it is difficult to assess the prediction quality of m, since little is known about the closeness of their posteriors, which are the important quantities for prediction. We show that for deterministic computable environments, the \"posterior\" and losses of m converge, but rapid convergence could only be shown on-sequence; the off-sequence convergence can be slow. In probabilistic environments, neither the posterior nor the losses converge, in general.",
        "published": "2005-08-05T10:16:16Z",
        "link": "http://arxiv.org/abs/cs/0508043v1",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.IT",
            "G.3; G.1.2; I.2.6; E.4"
        ]
    },
    {
        "title": "Relaxation Bounds on the Minimum Pseudo-Weight of Linear Block Codes",
        "authors": [
            "Panu Chaichanavong",
            "Paul H. Siegel"
        ],
        "summary": "Just as the Hamming weight spectrum of a linear block code sheds light on the performance of a maximum likelihood decoder, the pseudo-weight spectrum provides insight into the performance of a linear programming decoder. Using properties of polyhedral cones, we find the pseudo-weight spectrum of some short codes. We also present two general lower bounds on the minimum pseudo-weight. The first bound is based on the column weight of the parity-check matrix. The second bound is computed by solving an optimization problem. In some cases, this bound is more tractable to compute than previously known bounds and thus can be applied to longer codes.",
        "published": "2005-08-07T19:32:27Z",
        "link": "http://arxiv.org/abs/cs/0508046v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Further Results on Coding for Reliable Communication over Packet   Networks",
        "authors": [
            "Desmond S. Lun",
            "Muriel Medard",
            "Ralf Koetter",
            "Michelle Effros"
        ],
        "summary": "In \"On Coding for Reliable Communication over Packet Networks\" (Lun, Medard, and Effros, Proc. 42nd Annu. Allerton Conf. Communication, Control, and Computing, 2004), a capacity-achieving coding scheme for unicast or multicast over lossy wireline or wireless packet networks is presented. We extend that paper's results in two ways: First, we extend the network model to allow packets received on a link to arrive according to any process with an average rate, as opposed to the assumption of Poisson traffic with i.i.d. losses that was previously made. Second, in the case of Poisson traffic with i.i.d. losses, we derive error exponents that quantify the rate at which the probability of error decays with coding delay.",
        "published": "2005-08-08T02:09:13Z",
        "link": "http://arxiv.org/abs/cs/0508047v1",
        "categories": [
            "cs.IT",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "Nonbinary stabilizer codes over finite fields",
        "authors": [
            "Avanti Ketkar",
            "Andreas Klappenecker",
            "Santosh Kumar",
            "Pradeep Kiran Sarvepalli"
        ],
        "summary": "One formidable difficulty in quantum communication and computation is to protect information-carrying quantum states against undesired interactions with the environment. In past years, many good quantum error-correcting codes had been derived as binary stabilizer codes. Fault-tolerant quantum computation prompted the study of nonbinary quantum codes, but the theory of such codes is not as advanced as that of binary quantum codes. This paper describes the basic theory of stabilizer codes over finite fields. The relation between stabilizer codes and general quantum codes is clarified by introducing a Galois theory for these objects. A characterization of nonbinary stabilizer codes over GF(q) in terms of classical codes over GF(q^2) is provided that generalizes the well-known notion of additive codes over GF(4) of the binary case. This paper derives lower and upper bounds on the minimum distance of stabilizer codes, gives several code constructions, and derives numerous families of stabilizer codes, including quantum Hamming codes, quadratic residue codes, quantum Melas codes, quantum BCH codes, and quantum character codes. The puncturing theory by Rains is generalized to additive codes that are not necessarily pure. Bounds on the maximal length of maximum distance separable stabilizer codes are given. A discussion of open problems concludes this paper.",
        "published": "2005-08-08T21:09:07Z",
        "link": "http://arxiv.org/abs/quant-ph/0508070v2",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Characterizations of Pseudo-Codewords of LDPC Codes",
        "authors": [
            "Ralf Koetter",
            "Wen-Ching W. Li",
            "Pascal O. Vontobel",
            "Judy L. Walker"
        ],
        "summary": "An important property of high-performance, low complexity codes is the existence of highly efficient algorithms for their decoding. Many of the most efficient, recent graph-based algorithms, e.g. message passing algorithms and decoding based on linear programming, crucially depend on the efficient representation of a code in a graphical model. In order to understand the performance of these algorithms, we argue for the characterization of codes in terms of a so called fundamental cone in Euclidean space which is a function of a given parity check matrix of a code, rather than of the code itself. We give a number of properties of this fundamental cone derived from its connection to unramified covers of the graphical models on which the decoding algorithms operate. For the class of cycle codes, these developments naturally lead to a characterization of the fundamental polytope as the Newton polytope of the Hashimoto edge zeta function of the underlying graph.",
        "published": "2005-08-09T00:10:03Z",
        "link": "http://arxiv.org/abs/cs/0508049v3",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "Duality between channel capacity and rate distortion with two-sided   state information",
        "authors": [
            "T. M. Cover",
            "M. Chiang"
        ],
        "summary": "We show that the duality between channel capacity and data compression is retained when state information is available to the sender, to the receiver, to both, or to neither. We present a unified theory for eight special cases of channel capacity and rate distortion with state information, which also extends existing results to arbitrary pairs of independent and identically distributed (i.i.d.) correlated state information available at the sender and at the receiver, respectively. In particular, the resulting general formula for channel capacity assumes the same form as the generalized Wyner Ziv rate distortion function.",
        "published": "2005-08-09T03:01:18Z",
        "link": "http://arxiv.org/abs/cs/0508050v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Cluster Variation Method in Statistical Physics and Probabilistic   Graphical Models",
        "authors": [
            "Alessandro Pelizzola"
        ],
        "summary": "The cluster variation method (CVM) is a hierarchy of approximate variational techniques for discrete (Ising--like) models in equilibrium statistical mechanics, improving on the mean--field approximation and the Bethe--Peierls approximation, which can be regarded as the lowest level of the CVM. In recent years it has been applied both in statistical physics and to inference and optimization problems formulated in terms of probabilistic graphical models.   The foundations of the CVM are briefly reviewed, and the relations with similar techniques are discussed. The main properties of the method are considered, with emphasis on its exactness for particular models and on its asymptotic properties.   The problem of the minimization of the variational free energy, which arises in the CVM, is also addressed, and recent results about both provably convergent and message-passing algorithms are discussed.",
        "published": "2005-08-09T09:19:34Z",
        "link": "http://arxiv.org/abs/cond-mat/0508216v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Trellis-Based Equalization for Sparse ISI Channels Revisited",
        "authors": [
            "Jan Mietzner",
            "Sabah Badri-Hoeher",
            "Ingmar Land",
            "Peter A. Hoeher"
        ],
        "summary": "Sparse intersymbol-interference (ISI) channels are encountered in a variety of high-data-rate communication systems. Such channels have a large channel memory length, but only a small number of significant channel coefficients. In this paper, trellis-based equalization of sparse ISI channels is revisited. Due to the large channel memory length, the complexity of maximum-likelihood detection, e.g., by means of the Viterbi algorithm (VA), is normally prohibitive. In the first part of the paper, a unified framework based on factor graphs is presented for complexity reduction without loss of optimality. In this new context, two known reduced-complexity algorithms for sparse ISI channels are recapitulated: The multi-trellis VA (M-VA) and the parallel-trellis VA (P-VA). It is shown that the M-VA, although claimed, does not lead to a reduced computational complexity. The P-VA, on the other hand, leads to a significant complexity reduction, but can only be applied for a certain class of sparse channels. In the second part of the paper, a unified approach is investigated to tackle general sparse channels: It is shown that the use of a linear filter at the receiver renders the application of standard reduced-state trellis-based equalizer algorithms feasible, without significant loss of optimality. Numerical results verify the efficiency of the proposed receiver structure.",
        "published": "2005-08-10T09:37:52Z",
        "link": "http://arxiv.org/abs/cs/0508051v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Sensing Capacity for Markov Random Fields",
        "authors": [
            "Yaron Rachlin",
            "Rohit Negi",
            "Pradeep Khosla"
        ],
        "summary": "This paper computes the sensing capacity of a sensor network, with sensors of limited range, sensing a two-dimensional Markov random field, by modeling the sensing operation as an encoder. Sensor observations are dependent across sensors, and the sensor network output across different states of the environment is neither identically nor independently distributed. Using a random coding argument, based on the theory of types, we prove a lower bound on the sensing capacity of the network, which characterizes the ability of the sensor network to distinguish among environments with Markov structure, to within a desired accuracy.",
        "published": "2005-08-10T20:13:49Z",
        "link": "http://arxiv.org/abs/cs/0508054v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "DNA Codes that Avoid Secondary Structures",
        "authors": [
            "Olgica Milenkovic",
            "Navin Kashyap"
        ],
        "summary": "In this paper, we consider the problem of designing DNA sequences (codewords) for DNA storage systems and DNA computing that are unlikely to fold back onto themselves to form undesirable secondary structures. The paper addresses both the issue of enumerating the sequences with such properties and the problem of practical code construction.",
        "published": "2005-08-10T21:41:38Z",
        "link": "http://arxiv.org/abs/cs/0508055v1",
        "categories": [
            "cs.DM",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Very Simple Chaitin Machines for Concrete AIT",
        "authors": [
            "Michael Stay"
        ],
        "summary": "In 1975, Chaitin introduced his celebrated Omega number, the halting probability of a universal Chaitin machine, a universal Turing machine with a prefix-free domain. The Omega number's bits are {\\em algorithmically random}--there is no reason the bits should be the way they are, if we define ``reason'' to be a computable explanation smaller than the data itself. Since that time, only {\\em two} explicit universal Chaitin machines have been proposed, both by Chaitin himself.   Concrete algorithmic information theory involves the study of particular universal Turing machines, about which one can state theorems with specific numerical bounds, rather than include terms like O(1). We present several new tiny Chaitin machines (those with a prefix-free domain) suitable for the study of concrete algorithmic information theory. One of the machines, which we call Keraia, is a binary encoding of lambda calculus based on a curried lambda operator. Source code is included in the appendices.   We also give an algorithm for restricting the domain of blank-endmarker machines to a prefix-free domain over an alphabet that does not include the endmarker; this allows one to take many universal Turing machines and construct universal Chaitin machines from them.",
        "published": "2005-08-11T06:57:48Z",
        "link": "http://arxiv.org/abs/cs/0508056v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "On the Performance of Turbo Codes in Quasi-Static Fading Channels",
        "authors": [
            "M. R. D. Rodrigues",
            "I. Chatzigeorgiou",
            "I. J. Wassell",
            "R. Carrasco"
        ],
        "summary": "In this paper, we investigate in detail the performance of turbo codes in quasi-static fading channels both with and without antenna diversity. First, we develop a simple and accurate analytic technique to evaluate the performance of turbo codes in quasi-static fading channels. The proposed analytic technique relates the frame error rate of a turbo code to the iterative decoder convergence threshold, rather than to the turbo code distance spectrum. Subsequently, we compare the performance of various turbo codes in quasi-static fading channels. We show that, in contrast to the situation in the AWGN channel, turbo codes with different interleaver sizes or turbo codes based on RSC codes with different constraint lengths and generator polynomials exhibit identical performance. Moreover, we also compare the performance of turbo codes and convolutional codes in quasi-static fading channels under the condition of identical decoding complexity. In particular, we show that turbo codes do not outperform convolutional codes in quasi-static fading channels with no antenna diversity; and that turbo codes only outperform convolutional codes in quasi-static fading channels with antenna diversity.",
        "published": "2005-08-11T10:14:52Z",
        "link": "http://arxiv.org/abs/cs/0508057v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Entropy coding with Variable Length Re-writing Systems",
        "authors": [
            "Herve Jegou",
            "Christine Guillemot"
        ],
        "summary": "This paper describes a new set of block source codes well suited for data compression. These codes are defined by sets of productions rules of the form a.l->b, where a in A represents a value from the source alphabet A and l, b are -small- sequences of bits. These codes naturally encompass other Variable Length Codes (VLCs) such as Huffman codes. It is shown that these codes may have a similar or even a shorter mean description length than Huffman codes for the same encoding and decoding complexity. A first code design method allowing to preserve the lexicographic order in the bit domain is described. The corresponding codes have the same mean description length (mdl) as Huffman codes from which they are constructed. Therefore, they outperform from a compression point of view the Hu-Tucker codes designed to offer the lexicographic property in the bit domain. A second construction method allows to obtain codes such that the marginal bit probability converges to 0.5 as the sequence length increases and this is achieved even if the probability distribution function is not known by the encoder.",
        "published": "2005-08-11T15:41:19Z",
        "link": "http://arxiv.org/abs/cs/0508058v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Algorithms for Discrete Denoising Under Channel Uncertainty",
        "authors": [
            "George Gemelos",
            "Styrmir Sigurjonsson",
            "Tsachy Weissman"
        ],
        "summary": "The goal of a denoising algorithm is to reconstruct a signal from its noise-corrupted observations. Perfect reconstruction is seldom possible and performance is measured under a given fidelity criterion. In a recent work, the authors addressed the problem of denoising unknown discrete signals corrupted by a discrete memoryless channel when the channel, rather than being completely known, is only known to lie in some uncertainty set of possible channels. A sequence of denoisers was derived for this case and shown to be asymptotically optimal with respect to a worst-case criterion argued most relevant to this setting. In the present work we address the implementation and complexity of this denoiser for channels parametrized by a scalar, establishing its practicality. We show that for symmetric channels, the problem can be mapped into a convex optimization problem, which can be solved efficiently. We also present empirical results suggesting the potential of these schemes to do well in practice. A key component of our schemes is an estimator of the subset of channels in the uncertainty set that are feasible in the sense of being able to give rise to the noise-corrupted signal statistics for some channel input distribution. We establish the efficiency of this estimator, both algorithmically and experimentally. We also present a modification of the recently developed discrete universal denoiser (DUDE) that assumes a channel based on the said estimator, and show that, in practice, the resulting scheme performs well. For concreteness, we focus on the binary alphabet case and binary symmetric channels, but also discuss the extensions of the algorithms to general finite alphabets and to general channels parameterized by a scalar.",
        "published": "2005-08-11T22:17:56Z",
        "link": "http://arxiv.org/abs/cs/0508060v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Decoding of Expander Codes at Rates Close to Capacity",
        "authors": [
            "Alexei Ashikhmin",
            "Vitaly Skachek"
        ],
        "summary": "The decoding error probability of codes is studied as a function of their block length. It is shown that the existence of codes with a polynomially small decoding error probability implies the existence of codes with an exponentially small decoding error probability. Specifically, it is assumed that there exists a family of codes of length N and rate R=(1-\\epsilon)C (C is a capacity of a binary symmetric channel), whose decoding probability decreases polynomially in 1/N. It is shown that if the decoding probability decreases sufficiently fast, but still only polynomially fast in 1/N, then there exists another such family of codes whose decoding error probability decreases exponentially fast in N. Moreover, if the decoding time complexity of the assumed family of codes is polynomial in N and 1/\\epsilon, then the decoding time complexity of the presented family is linear in N and polynomial in 1/\\epsilon. These codes are compared to the recently presented codes of Barg and Zemor, ``Error Exponents of Expander Codes,'' IEEE Trans. Inform. Theory, 2002, and ``Concatenated Codes: Serial and Parallel,'' IEEE Trans. Inform. Theory, 2005. It is shown that the latter families can not be tuned to have exponentially decaying (in N) error probability, and at the same time to have decoding time complexity linear in N and polynomial in 1/\\epsilon.",
        "published": "2005-08-12T09:36:50Z",
        "link": "http://arxiv.org/abs/cs/0508062v2",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Layered Orthogonal Lattice Detector for Two Transmit Antenna   Communications",
        "authors": [
            "Massimiliano Siti",
            "Michael P. Fitz"
        ],
        "summary": "A novel detector for multiple-input multiple-output (MIMO) communications is presented. The algorithm belongs to the class of the lattice detectors, i.e. it finds a reduced complexity solution to the problem of finding the closest vector to the received observations. The algorithm achieves optimal maximum-likelihood (ML) performance in case of two transmit antennas, at the same time keeping a complexity much lower than the exhaustive search-based ML detection technique. Also, differently from the state-of-art lattice detector (namely sphere decoder), the proposed algorithm is suitable for a highly parallel hardware architecture and for a reliable bit soft-output information generation, thus making it a promising option for real-time high-data rate transmission.",
        "published": "2005-08-12T19:35:11Z",
        "link": "http://arxiv.org/abs/cs/0508064v3",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Lossy source encoding via message-passing and decimation over   generalized codewords of LDGM codes",
        "authors": [
            "Martin J. Wainwright",
            "Elitza Maneva"
        ],
        "summary": "We describe message-passing and decimation approaches for lossy source coding using low-density generator matrix (LDGM) codes. In particular, this paper addresses the problem of encoding a Bernoulli(0.5) source: for randomly generated LDGM codes with suitably irregular degree distributions, our methods yield performance very close to the rate distortion limit over a range of rates. Our approach is inspired by the survey propagation (SP) algorithm, originally developed by Mezard et al. for solving random satisfiability problems. Previous work by Maneva et al. shows how SP can be understood as belief propagation (BP) for an alternative representation of satisfiability problems. In analogy to this connection, our approach is to define a family of Markov random fields over generalized codewords, from which local message-passing rules can be derived in the standard way. The overall source encoding method is based on message-passing, setting a subset of bits to their preferred values (decimation), and reducing the code.",
        "published": "2005-08-15T02:00:55Z",
        "link": "http://arxiv.org/abs/cs/0508068v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "MAP estimation via agreement on (hyper)trees: Message-passing and linear   programming",
        "authors": [
            "Martin J. Wainwright",
            "Tommi S. Jaakkola",
            "Alan S. Willsky"
        ],
        "summary": "We develop and analyze methods for computing provably optimal {\\em maximum a posteriori} (MAP) configurations for a subclass of Markov random fields defined on graphs with cycles. By decomposing the original distribution into a convex combination of tree-structured distributions, we obtain an upper bound on the optimal value of the original problem (i.e., the log probability of the MAP assignment) in terms of the combined optimal values of the tree problems. We prove that this upper bound is tight if and only if all the tree distributions share an optimal configuration in common. An important implication is that any such shared configuration must also be a MAP configuration for the original distribution. Next we develop two approaches to attempting to obtain tight upper bounds: (a) a {\\em tree-relaxed linear program} (LP), which is derived from the Lagrangian dual of the upper bounds; and (b) a {\\em tree-reweighted max-product message-passing algorithm} that is related to but distinct from the max-product algorithm. In this way, we establish a connection between a certain LP relaxation of the mode-finding problem, and a reweighted form of the max-product (min-sum) message-passing algorithm.",
        "published": "2005-08-15T14:32:21Z",
        "link": "http://arxiv.org/abs/cs/0508070v1",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT"
        ]
    },
    {
        "title": "On Achievable Rates and Complexity of LDPC Codes for Parallel Channels   with Application to Puncturing",
        "authors": [
            "Igal Sason",
            "Gil Wiechman"
        ],
        "summary": "This paper considers the achievable rates and decoding complexity of low-density parity-check (LDPC) codes over statistically independent parallel channels. The paper starts with the derivation of bounds on the conditional entropy of the transmitted codeword given the received sequence at the output of the parallel channels; the component channels are considered to be memoryless, binary-input, and output-symmetric (MBIOS). These results serve for the derivation of an upper bound on the achievable rates of ensembles of LDPC codes under optimal maximum-likelihood (ML) decoding when their transmission takes place over parallel MBIOS channels. The paper relies on the latter bound for obtaining upper bounds on the achievable rates of ensembles of randomly and intentionally punctured LDPC codes over MBIOS channels. The paper also provides a lower bound on the decoding complexity (per iteration) of ensembles of LDPC codes under message-passing iterative decoding over parallel MBIOS channels; the bound is given in terms of the gap between the rate of these codes for which reliable communication is achievable and the channel capacity. The paper presents a diagram which shows interconnections between the theorems introduced in this paper and some other previously reported results. The setting which serves for the derivation of the bounds on the achievable rates and decoding complexity is general, and the bounds can be applied to other scenarios which can be treated as different forms of communication over parallel channels.",
        "published": "2005-08-16T05:51:47Z",
        "link": "http://arxiv.org/abs/cs/0508072v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Throughput and Delay in Random Wireless Networks with Restricted   Mobility",
        "authors": [
            "James Mammen",
            "Devavrat Shah"
        ],
        "summary": "Grossglauser and Tse (2001) introduced a mobile random network model where each node moves independently on a unit disk according to a stationary uniform distribution and showed that a throughput of $\\Theta(1)$ is achievable. El Gamal, Mammen, Prabhakar and Shah (2004) showed that the delay associated with this throughput scales as $\\Theta(n\\log n)$, when each node moves according to an independent random walk. In a later work, Diggavi, Grossglauser and Tse (2002) considered a random network on a sphere with a restricted mobility model, where each node moves along a randomly chosen great circle on the unit sphere. They showed that even with this one-dimensional restriction on mobility, constant throughput scaling is achievable. Thus, this particular mobility restriction does not affect the throughput scaling. This raises the question whether this mobility restriction affects the delay scaling.   This paper studies the delay scaling at $\\Theta(1)$ throughput for a random network with restricted mobility. First, a variant of the scheme presented by Diggavi, Grossglauser and Tse (2002) is presented and it is shown to achieve $\\Theta(1)$ throughput using different (and perhaps simpler) techniques. The exact order of delay scaling for this scheme is determined, somewhat surprisingly, to be of $\\Theta(n\\log n)$, which is the same as that without the mobility restriction. Thus, this particular mobility restriction \\emph{does not} affect either the maximal throughput scaling or the corresponding delay scaling of the network. This happens because under this 1-D restriction, each node is in the proximity of every other node in essentially the same manner as without this restriction.",
        "published": "2005-08-17T00:07:45Z",
        "link": "http://arxiv.org/abs/cs/0508074v1",
        "categories": [
            "cs.IT",
            "cs.NI",
            "math.IT",
            "C.2.1"
        ]
    },
    {
        "title": "Complexity of Networks",
        "authors": [
            "Russell K. Standish"
        ],
        "summary": "Network or graph structures are ubiquitous in the study of complex systems. Often, we are interested in complexity trends of these system as it evolves under some dynamic. An example might be looking at the complexity of a food web as species enter an ecosystem via migration or speciation, and leave via extinction.   In this paper, a complexity measure of networks is proposed based on the {\\em complexity is information content} paradigm. To apply this paradigm to any object, one must fix two things: a representation language, in which strings of symbols from some alphabet describe, or stand for the objects being considered; and a means of determining when two such descriptions refer to the same object. With these two things set, the information content of an object can be computed in principle from the number of equivalent descriptions describing a particular object.   I propose a simple representation language for undirected graphs that can be encoded as a bitstring, and equivalence is a topological equivalence. I also present an algorithm for computing the complexity of an arbitrary undirected network.",
        "published": "2005-08-17T00:51:41Z",
        "link": "http://arxiv.org/abs/cs/0508075v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "G.2.2"
        ]
    },
    {
        "title": "Myopic Coding in Multiple Relay Channels",
        "authors": [
            "Lawrence Ong",
            "Mehul Motani"
        ],
        "summary": "In this paper, we investigate achievable rates for data transmission from sources to sinks through multiple relay networks. We consider myopic coding, a constrained communication strategy in which each node has only a local view of the network, meaning that nodes can only transmit to and decode from neighboring nodes. We compare this with omniscient coding, in which every node has a global view of the network and all nodes can cooperate. Using Gaussian channels as examples, we find that when the nodes transmit at low power, the rates achievable with two-hop myopic coding are as large as that under omniscient coding in a five-node multiple relay channel and close to that under omniscient coding in a six-node multiple relay channel. These results suggest that we may do local coding and cooperation without compromising much on the transmission rate. Practically, myopic coding schemes are more robust to topology changes because encoding and decoding at a node are not affected when there are changes at remote nodes. Furthermore, myopic coding mitigates the high computational complexity and large buffer/memory requirements of omniscient coding.",
        "published": "2005-08-17T03:55:26Z",
        "link": "http://arxiv.org/abs/cs/0508076v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Families of unitary matrices achieving full diversity",
        "authors": [
            "Frederique Oggier",
            "Emmanuel Lequeu"
        ],
        "summary": "This paper presents an algebraic construction of families of unitary matrices that achieve full diversity. They are obtained as subsets of cyclic division algebras.",
        "published": "2005-08-17T09:40:35Z",
        "link": "http://arxiv.org/abs/cs/0508077v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A General Framework for Codes Involving Redundancy Minimization",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "A framework with two scalar parameters is introduced for various problems of finding a prefix code minimizing a coding penalty function. The framework encompasses problems previously proposed by Huffman, Campbell, Nath, and Drmota and Szpankowski, shedding light on the relationships among these problems. In particular, Nath's range of problems can be seen as bridging the minimum average redundancy problem of Huffman with the minimum maximum pointwise redundancy problem of Drmota and Szpankowski. Using this framework, two linear-time Huffman-like algorithms are devised for the minimum maximum pointwise redundancy problem, the only one in the framework not previously solved with a Huffman-like algorithm. Both algorithms provide solutions common to this problem and a subrange of Nath's problems, the second algorithm being distinguished by its ability to find the minimum variance solution among all solutions common to the minimum maximum pointwise redundancy and Nath problems. Simple redundancy bounds are also presented.",
        "published": "2005-08-18T20:22:45Z",
        "link": "http://arxiv.org/abs/cs/0508083v2",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Source Coding for Quasiarithmetic Penalties",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Huffman coding finds a prefix code that minimizes mean codeword length for a given probability distribution over a finite number of items. Campbell generalized the Huffman problem to a family of problems in which the goal is to minimize not mean codeword length but rather a generalized mean known as a quasiarithmetic or quasilinear mean. Such generalized means have a number of diverse applications, including applications in queueing. Several quasiarithmetic-mean problems have novel simple redundancy bounds in terms of a generalized entropy. A related property involves the existence of optimal codes: For ``well-behaved'' cost functions, optimal codes always exist for (possibly infinite-alphabet) sources having finite generalized entropy. Solving finite instances of such problems is done by generalizing an algorithm for finding length-limited binary codes to a new algorithm for finding optimal binary codes for any quasiarithmetic mean with a convex cost function. This algorithm can be performed using quadratic time and linear space, and can be extended to other penalty functions, some of which are solvable with similar space and time complexity, and others of which are solvable with slightly greater complexity. This reduces the computational complexity of a problem involving minimum delay in a queue, allows combinations of previously considered problems to be optimized, and greatly expands the space of problems solvable in quadratic time and linear space. The algorithm can be extended for purposes such as breaking ties among possibly different optimal codes, as with bottom-merge Huffman coding.",
        "published": "2005-08-18T20:29:04Z",
        "link": "http://arxiv.org/abs/cs/0508084v5",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "On the Asymptotic Performance of Iterative Decoders for Product Codes",
        "authors": [
            "Moshe Schwartz",
            "Paul H. Siegel",
            "Alexander Vardy"
        ],
        "summary": "We consider hard-decision iterative decoders for product codes over the erasure channel, which employ repeated rounds of decoding rows and columns alternatingly. We derive the exact asymptotic probability of decoding failure as a function of the error-correction capabilities of the row and column codes, the number of decoding rounds, and the channel erasure probability. We examine both the case of codes capable of correcting a constant amount of errors, and the case of codes capable of correcting a constant fraction of their length.",
        "published": "2005-08-20T09:30:13Z",
        "link": "http://arxiv.org/abs/cs/0508085v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT",
            "E.4"
        ]
    },
    {
        "title": "Special Cases of Encodings by Generalized Adaptive Codes",
        "authors": [
            "Dragos Trinca"
        ],
        "summary": "Adaptive (variable-length) codes associate variable-length codewords to symbols being encoded depending on the previous symbols in the input data string. This class of codes has been presented in [Dragos Trinca, cs.DS/0505007] as a new class of non-standard variable-length codes. Generalized adaptive codes (GA codes, for short) have been also presented in [Dragos Trinca, cs.DS/0505007] not only as a new class of non-standard variable-length codes, but also as a natural generalization of adaptive codes of any order. This paper is intended to continue developing the theory of variable-length codes by establishing several interesting connections between adaptive codes and other classes of codes. The connections are discussed not only from a theoretical point of view (by proving new results), but also from an applicative one (by proposing several applications). First, we prove that adaptive Huffman encodings and Lempel-Ziv encodings are particular cases of encodings by GA codes. Second, we show that any (n,1,m) convolutional code satisfying certain conditions can be modelled as an adaptive code of order m. Third, we describe a cryptographic scheme based on the connection between adaptive codes and convolutional codes, and present an insightful analysis of this scheme. Finally, we conclude by generalizing adaptive codes to (p,q)-adaptive codes, and discussing connections between adaptive codes and time-varying codes.",
        "published": "2005-08-21T08:22:49Z",
        "link": "http://arxiv.org/abs/cs/0508088v1",
        "categories": [
            "cs.IT",
            "math.IT",
            "D.3.1; E.4; F.4.3"
        ]
    },
    {
        "title": "Performance of PPM Multipath Synchronization in the Limit of Large   Bandwidth",
        "authors": [
            "Dana Porrat",
            "Urbashi Mitra"
        ],
        "summary": "The acquisition, or synchronization, of the multipath profile for an ultrawideband pulse position modulation (PPM) communication systems is considered. Synchronization is critical for the proper operation of PPM based For the multipath channel, it is assumed that channel gains are known, but path delays are unknown. In the limit of large bandwidth, W, it is assumed that the number of paths, L, grows. The delay spread of the channel, M, is proportional to the bandwidth. The rate of growth of L versus M determines whether synchronization can occur. It is shown that if L/sqrt(M) --> 0, then the maximum likelihood synchronizer cannot acquire any of the paths and alternatively if L/M --> 0, the maximum likelihood synchronizer is guaranteed to miss at least one path.",
        "published": "2005-08-22T13:46:54Z",
        "link": "http://arxiv.org/abs/cs/0508093v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Conference Key Agreement and Quantum Sharing of Classical Secrets with   Noisy GHZ States",
        "authors": [
            "Kai Chen",
            "Hoi-Kwong Lo"
        ],
        "summary": "We propose a wide class of distillation schemes for multi-partite entangled states that are CSS-states. Our proposal provides not only superior efficiency, but also new insights on the connection between CSS-states and bipartite graph states. We then consider the applications of our distillation schemes for two cryptographic tasks--namely, (a) conference key agreement and (b) quantum sharing of classical secrets. In particular, we construct ``prepare-and-measure'' protocols. Also we study the yield of those protocols and the threshold value of the fidelity above which the protocols can function securely. Surprisingly, our protocols will function securely even when the initial state does not violate the standard Bell-inequalities for GHZ states. Experimental realization involving only bi-partite entanglement is also suggested.",
        "published": "2005-08-22T14:07:14Z",
        "link": "http://arxiv.org/abs/cs/0508094v1",
        "categories": [
            "cs.IT",
            "cs.CR",
            "math.IT"
        ]
    },
    {
        "title": "Capacity of Ultra Wide Band Wireless Ad Hoc Networks",
        "authors": [
            "Rohit Negi",
            "Arjunan Rajeswaran"
        ],
        "summary": "Throughput capacity is a critical parameter for the design and evaluation of ad-hoc wireless networks. Consider n identical randomly located nodes, on a unit area, forming an ad-hoc wireless network. Assuming a fixed per node transmission capability of T bits per second at a fixed range, it has been shown that the uniform throughput capacity per node r(n) is Theta((T)/(sqrt{n log n})), a decreasing function of node density n.   However an alternate communication model may also be considered, with each node constrained to a maximum transmit power P_0 and capable of utilizing W Hz of bandwidth. Under the limiting case W rightarrow infinity, such as in Ultra Wide Band (UWB) networks, the uniform throughput per node is O ((n log n)^{(alpha-1}/2}) (upper bound) and Omega((n^{(alpha-1)/2})/((log n)^{(alpha +1)/2})) (achievable lower bound).   These bounds demonstrate that throughput increases with node density $n$, in contrast to previously published results. This is the result of the large bandwidth, and the assumed power and rate adaptation, which alleviate interference. Thus, the effect of physical layer properties on the capacity of ad hoc wireless networks is demonstrated. Further, the promise of UWB as a physical layer technology for ad-hoc networks is justified.",
        "published": "2005-08-22T19:27:34Z",
        "link": "http://arxiv.org/abs/cs/0508095v1",
        "categories": [
            "cs.IT",
            "cs.NI",
            "math.IT"
        ]
    },
    {
        "title": "On Multiple User Channels with Causal State Information at the   Transmitters",
        "authors": [
            "Styrmir Sigurjonsson",
            "Young-Han Kim"
        ],
        "summary": "We extend Shannon's result on the capacity of channels with state information to multiple user channels. More specifically, we characterize the capacity (region) of degraded broadcast channels and physically degraded relay channels where the channel state information is causally available at the transmitters. We also obtain inner and outer bounds on the capacity region for multiple access channels with causal state information at the transmitters.",
        "published": "2005-08-22T21:36:02Z",
        "link": "http://arxiv.org/abs/cs/0508096v1",
        "categories": [
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "An Explicit Construction of Universally Decodable Matrices",
        "authors": [
            "Pascal O. Vontobel",
            "Ashwin Ganesan"
        ],
        "summary": "Universally decodable matrices can be used for coding purposes when transmitting over slow fading channels. These matrices are parameterized by positive integers $L$ and $n$ and a prime power $q$. Based on Pascal's triangle we give an explicit construction of universally decodable matrices for any non-zero integers $L$ and $n$ and any prime power $q$ where $L \\leq q+1$. This is the largest set of possible parameter values since for any list of universally decodable matrices the value $L$ is upper bounded by $q+1$, except for the trivial case $n = 1$. For the proof of our construction we use properties of Hasse derivatives, and it turns out that our construction has connections to Reed-Solomon codes, Reed-Muller codes, and so-called repeated-root cyclic codes. Additionally, we show how universally decodable matrices can be modified so that they remain universally decodable matrices.",
        "published": "2005-08-23T02:13:48Z",
        "link": "http://arxiv.org/abs/cs/0508098v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "Search Process and Probabilistic Bifix Approach",
        "authors": [
            "Dragana Bajic",
            "Cedomir Stefanovic",
            "Dejan Vukobratovic"
        ],
        "summary": "An analytical approach to a search process is a mathematical prerequisite for digital synchronization acquisition analysis and optimization. A search is performed for an arbitrary set of sequences within random but not equiprobable L-ary data. This paper derives in detail an expression for probability distribution function, from which other statistical parameters - expected value and variance - can be obtained. The probabilistic nature of (cross-) bifix indicators is shown and application examples are outlined, ranging beyond the usual telecommunication field.",
        "published": "2005-08-23T08:51:45Z",
        "link": "http://arxiv.org/abs/cs/0508099v1",
        "categories": [
            "cs.IT",
            "cs.CV",
            "math.IT"
        ]
    },
    {
        "title": "Maximum Weight Matching via Max-Product Belief Propagation",
        "authors": [
            "Mohsen Bayati",
            "Devavrat Shah",
            "Mayank Sharma"
        ],
        "summary": "Max-product \"belief propagation\" is an iterative, local, message-passing algorithm for finding the maximum a posteriori (MAP) assignment of a discrete probability distribution specified by a graphical model. Despite the spectacular success of the algorithm in many application areas such as iterative decoding, computer vision and combinatorial optimization which involve graphs with many cycles, theoretical results about both correctness and convergence of the algorithm are known in few cases (Weiss-Freeman Wainwright, Yeddidia-Weiss-Freeman, Richardson-Urbanke}.   In this paper we consider the problem of finding the Maximum Weight Matching (MWM) in a weighted complete bipartite graph. We define a probability distribution on the bipartite graph whose MAP assignment corresponds to the MWM. We use the max-product algorithm for finding the MAP of this distribution or equivalently, the MWM on the bipartite graph. Even though the underlying bipartite graph has many short cycles, we find that surprisingly, the max-product algorithm always converges to the correct MAP assignment as long as the MAP assignment is unique. We provide a bound on the number of iterations required by the algorithm and evaluate the computational cost of the algorithm. We find that for a graph of size $n$, the computational cost of the algorithm scales as $O(n^3)$, which is the same as the computational cost of the best known algorithm. Finally, we establish the precise relation between the max-product algorithm and the celebrated {\\em auction} algorithm proposed by Bertsekas. This suggests possible connections between dual algorithm and max-product algorithm for discrete optimization problems.",
        "published": "2005-08-23T18:37:50Z",
        "link": "http://arxiv.org/abs/cs/0508101v2",
        "categories": [
            "cs.IT",
            "cs.AI",
            "math.IT",
            "G.2.2; G.3; I.2.6"
        ]
    },
    {
        "title": "A Generalised Hadamard Transform",
        "authors": [
            "K. J. Horadam"
        ],
        "summary": "A Generalised Hadamard Transform for multi-phase or multilevel signals is introduced, which includes the Fourier, Generalised, Discrete Fourier, Walsh-Hadamard and Reverse Jacket Transforms. The jacket construction is formalised and shown to admit a tensor product decomposition. Primary matrices under this decomposition are identified. New examples of primary jacket matrices of orders 8 and 12 are presented.",
        "published": "2005-08-24T01:03:34Z",
        "link": "http://arxiv.org/abs/cs/0508104v1",
        "categories": [
            "cs.IT",
            "cs.DM",
            "math.IT"
        ]
    },
    {
        "title": "Portfolio selection using neural networks",
        "authors": [
            "Alberto Fernandez",
            "Sergio Gomez"
        ],
        "summary": "In this paper we apply a heuristic method based on artificial neural networks in order to trace out the efficient frontier associated to the portfolio selection problem. We consider a generalization of the standard Markowitz mean-variance model which includes cardinality and bounding constraints. These constraints ensure the investment in a given number of different assets and limit the amount of capital to be invested in each asset. We present some experimental results obtained with the neural network heuristic and we compare them to those obtained with three previous heuristic methods.",
        "published": "2005-01-03T18:55:47Z",
        "link": "http://arxiv.org/abs/cs/0501005v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Spike timing precision and neural error correction: local behavior",
        "authors": [
            "Michael Stiber"
        ],
        "summary": "The effects of spike timing precision and dynamical behavior on error correction in spiking neurons were investigated. Stationary discharges -- phase locked, quasiperiodic, or chaotic -- were induced in a simulated neuron by presenting pacemaker presynaptic spike trains across a model of a prototypical inhibitory synapse. Reduced timing precision was modeled by jittering presynaptic spike times. Aftereffects of errors -- in this communication, missed presynaptic spikes -- were determined by comparing postsynaptic spike times between simulations identical except for the presence or absence of errors. Results show that the effects of an error vary greatly depending on the ongoing dynamical behavior. In the case of phase lockings, a high degree of presynaptic spike timing precision can provide significantly faster error recovery. For non-locked behaviors, isolated missed spikes can have little or no discernible aftereffects (or even serve to paradoxically reduce uncertainty in postsynaptic spike timing), regardless of presynaptic imprecision. This suggests two possible categories of error correction: high-precision locking with rapid recovery and low-precision non-locked with error immunity.",
        "published": "2005-01-14T13:25:42Z",
        "link": "http://arxiv.org/abs/q-bio/0501021v1",
        "categories": [
            "q-bio.NC",
            "cs.NE",
            "math.DS"
        ]
    },
    {
        "title": "Neural network ensembles: Evaluation of aggregation algorithms",
        "authors": [
            "P. M. Granitto",
            "P. F. Verdes",
            "H. A. Ceccatto"
        ],
        "summary": "Ensembles of artificial neural networks show improved generalization capabilities that outperform those of single networks. However, for aggregation to be effective, the individual networks must be as accurate and diverse as possible. An important problem is, then, how to tune the aggregate members in order to have an optimal compromise between these two conflicting conditions. We present here an extensive evaluation of several algorithms for ensemble construction, including new proposals and comparing them with standard methods in the literature. We also discuss a potential problem with sequential aggregation algorithms: the non-frequent but damaging selection through their heuristics of particularly bad ensemble members. We introduce modified algorithms that cope with this problem by allowing individual weighting of aggregate members. Our algorithms and their weighted modifications are favorably tested against other methods in the literature, producing a sensible improvement in performance on most of the standard statistical databases used as benchmarks.",
        "published": "2005-02-01T21:22:24Z",
        "link": "http://arxiv.org/abs/cs/0502006v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Identification of complex systems in the basis of wavelets",
        "authors": [
            "Alexander Shaydurov"
        ],
        "summary": "In this paper is proposed the method of the identification of complex dynamic systems. Method can be used for the identification of linear and nonlinear complex dynamic systems for the determined or stochastic signals at the inputs and the outputs. It is proposed to use a basis of wavelets for obtaining the impulse transient function (ITF) of system. ITF is considered in the form of surface in the 3D space. Are given the results of experiments on the identification of systems in the basis of wavelets.",
        "published": "2005-02-02T00:12:45Z",
        "link": "http://arxiv.org/abs/cs/0502007v1",
        "categories": [
            "cs.CE",
            "cs.NE",
            "J.2, J.6"
        ]
    },
    {
        "title": "Population Sizing for Genetic Programming Based Upon Decision Making",
        "authors": [
            "K. Sastry",
            "U. -M. O'Reilly",
            "D. E. Goldberg"
        ],
        "summary": "This paper derives a population sizing relationship for genetic programming (GP). Following the population-sizing derivation for genetic algorithms in Goldberg, Deb, and Clark (1992), it considers building block decision making as a key facet. The analysis yields a GP-unique relationship because it has to account for bloat and for the fact that GP solutions often use subsolution multiple times. The population-sizing relationship depends upon tree size, solution complexity, problem difficulty and building block expression probability. The relationship is used to analyze and empirically investigate population sizing for three model GP problems named ORDER, ON-OFF and LOUD. These problems exhibit bloat to differing extents and differ in whether their solutions require the use of a building block multiple times.",
        "published": "2005-02-04T03:58:26Z",
        "link": "http://arxiv.org/abs/cs/0502020v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Oiling the Wheels of Change: The Role of Adaptive Automatic Problem   Decomposition in Non--Stationary Environments",
        "authors": [
            "H. A. Abbass",
            "K. Sastry",
            "D. E. Goldberg"
        ],
        "summary": "Genetic algorithms (GAs) that solve hard problems quickly, reliably and accurately are called competent GAs. When the fitness landscape of a problem changes overtime, the problem is called non--stationary, dynamic or time--variant problem. This paper investigates the use of competent GAs for optimizing non--stationary optimization problems. More specifically, we use an information theoretic approach based on the minimum description length principle to adaptively identify regularities and substructures that can be exploited to respond quickly to changes in the environment. We also develop a special type of problems with bounded difficulties to test non--stationary optimization problems. The results provide new insights into non-stationary optimization problems and show that a search algorithm which automatically identifies and exploits possible decompositions is more robust and responds quickly to changes than a simple genetic algorithm.",
        "published": "2005-02-04T04:15:15Z",
        "link": "http://arxiv.org/abs/cs/0502021v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Sub-Structural Niching in Non-Stationary Environments",
        "authors": [
            "K. Sastry",
            "H. A. Abbass",
            "D. E. Goldberg"
        ],
        "summary": "Niching enables a genetic algorithm (GA) to maintain diversity in a population. It is particularly useful when the problem has multiple optima where the aim is to find all or as many as possible of these optima. When the fitness landscape of a problem changes overtime, the problem is called non--stationary, dynamic or time--variant problem. In these problems, niching can maintain useful solutions to respond quickly, reliably and accurately to a change in the environment. In this paper, we present a niching method that works on the problem substructures rather than the whole solution, therefore it has less space complexity than previously known niching mechanisms. We show that the method is responding accurately when environmental changes occur.",
        "published": "2005-02-04T04:29:15Z",
        "link": "http://arxiv.org/abs/cs/0502022v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Sub-structural Niching in Estimation of Distribution Algorithms",
        "authors": [
            "K. Sastry",
            "H. A. Abbass",
            "D. E. Goldberg",
            "D. D. Johnson"
        ],
        "summary": "We propose a sub-structural niching method that fully exploits the problem decomposition capability of linkage-learning methods such as the estimation of distribution algorithms and concentrate on maintaining diversity at the sub-structural level. The proposed method consists of three key components: (1) Problem decomposition and sub-structure identification, (2) sub-structure fitness estimation, and (3) sub-structural niche preservation. The sub-structural niching method is compared to restricted tournament selection (RTS)--a niching method used in hierarchical Bayesian optimization algorithm--with special emphasis on sustained preservation of multiple global solutions of a class of boundedly-difficult, additively-separable multimodal problems. The results show that sub-structural niching successfully maintains multiple global optima over large number of generations and does so with significantly less population than RTS. Additionally, the market share of each of the niche is much closer to the expected level in sub-structural niching when compared to RTS.",
        "published": "2005-02-04T04:46:04Z",
        "link": "http://arxiv.org/abs/cs/0502023v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Multiobjective hBOA, Clustering, and Scalability",
        "authors": [
            "Martin Pelikan",
            "Kumara Sastry",
            "David E. Goldberg"
        ],
        "summary": "This paper describes a scalable algorithm for solving multiobjective decomposable problems by combining the hierarchical Bayesian optimization algorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and clustering in the objective space. It is first argued that for good scalability, clustering or some other form of niching in the objective space is necessary and the size of each niche should be approximately equal. Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and clustering in the objective space. The algorithm mohBOA differs from the multiobjective variants of BOA and hBOA proposed in the past by including clustering in the objective space and allocating an approximately equally sized portion of the population to each cluster. The algorithm mohBOA is shown to scale up well on a number of problems on which standard multiobjective evolutionary algorithms perform poorly.",
        "published": "2005-02-07T05:26:13Z",
        "link": "http://arxiv.org/abs/cs/0502034v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; I.2.6; G.1.6; I.5.3"
        ]
    },
    {
        "title": "Scalability of Genetic Programming and Probabilistic Incremental Program   Evolution",
        "authors": [
            "Radovan Ondas",
            "Martin Pelikan",
            "Kumara Sastry"
        ],
        "summary": "This paper discusses scalability of standard genetic programming (GP) and the probabilistic incremental program evolution (PIPE). To investigate the need for both effective mixing and linkage learning, two test problems are considered: ORDER problem, which is rather easy for any recombination-based GP, and TRAP or the deceptive trap problem, which requires the algorithm to learn interactions among subsets of terminals. The scalability results show that both GP and PIPE scale up polynomially with problem size on the simple ORDER problem, but they both scale up exponentially on the deceptive problem. This indicates that while standard recombination is sufficient when no interactions need to be considered, for some problems linkage learning is necessary. These results are in agreement with the lessons learned in the domain of binary-string genetic algorithms (GAs). Furthermore, the paper investigates the effects of introducing utnnecessary and irrelevant primitives on the performance of GP and PIPE.",
        "published": "2005-02-07T19:40:01Z",
        "link": "http://arxiv.org/abs/cs/0502029v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.8; I.2.6; G.1.6"
        ]
    },
    {
        "title": "Particle Swarm Optimization: An efficient method for tracing periodic   orbits in 3D galactic potentials",
        "authors": [
            "Ch. Skokos",
            "K. E. Parsopoulos",
            "P. A. Patsis",
            "M. N. Vrahatis"
        ],
        "summary": "We propose the Particle Swarm Optimization (PSO) as an alternative method for locating periodic orbits in a three--dimensional (3D) model of barred galaxies. We develop an appropriate scheme that transforms the problem of finding periodic orbits into the problem of detecting global minimizers of a function, which is defined on the Poincar\\'{e} Surface of Section (PSS) of the Hamiltonian system. By combining the PSO method with deflection techniques, we succeeded in tracing systematically several periodic orbits of the system. The method succeeded in tracing the initial conditions of periodic orbits in cases where Newton iterative techniques had difficulties. In particular, we found families of 2D and 3D periodic orbits associated with the inner 8:1 to 12:1 resonances, between the radial 4:1 and corotation resonances of our 3D Ferrers bar model. The main advantages of the proposed algorithm is its simplicity, its ability to work using function values solely, as well as its ability to locate many periodic orbits per run at a given Jacobian constant.",
        "published": "2005-02-08T15:51:56Z",
        "link": "http://arxiv.org/abs/astro-ph/0502164v1",
        "categories": [
            "astro-ph",
            "cs.NA",
            "cs.NE",
            "math.NA",
            "nlin.CD"
        ]
    },
    {
        "title": "Decomposable Problems, Niching, and Scalability of Multiobjective   Estimation of Distribution Algorithms",
        "authors": [
            "Kumara Sastry",
            "Martin Pelikan",
            "David E. Goldberg"
        ],
        "summary": "The paper analyzes the scalability of multiobjective estimation of distribution algorithms (MOEDAs) on a class of boundedly-difficult additively-separable multiobjective optimization problems. The paper illustrates that even if the linkage is correctly identified, massive multimodality of the search problems can easily overwhelm the nicher and lead to exponential scale-up. Facetwise models are subsequently used to propose a growth rate of the number of differing substructures between the two objectives to avoid the niching method from being overwhelmed and lead to polynomial scalability of MOEDAs.",
        "published": "2005-02-12T22:29:45Z",
        "link": "http://arxiv.org/abs/cs/0502057v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Learning intrinsic excitability in medium spiny neurons",
        "authors": [
            "Gabriele Scheler"
        ],
        "summary": "We present an unsupervised, local activation-dependent learning rule for intrinsic plasticity (IP) which affects the composition of ion channel conductances for single neurons in a use-dependent way. We use a single-compartment conductance-based model for medium spiny striatal neurons in order to show the effects of parametrization of individual ion channels on the neuronal activation function. We show that parameter changes within the physiological ranges are sufficient to create an ensemble of neurons with significantly different activation functions. We emphasize that the effects of intrinsic neuronal variability on spiking behavior require a distributed mode of synaptic input and can be eliminated by strongly correlated input. We show how variability and adaptivity in ion channel conductances can be utilized to store patterns without an additional contribution by synaptic plasticity (SP). The adaptation of the spike response may result in either \"positive\" or \"negative\" pattern learning. However, read-out of stored information depends on a distributed pattern of synaptic activity to let intrinsic variability determine spike response. We briefly discuss the implications of this conditional memory on learning and addiction.",
        "published": "2005-02-21T02:42:32Z",
        "link": "http://arxiv.org/abs/q-bio/0502023v4",
        "categories": [
            "q-bio.NC",
            "cs.NE"
        ]
    },
    {
        "title": "The Self-Organization of Speech Sounds",
        "authors": [
            "Pierre-Yves Oudeyer"
        ],
        "summary": "The speech code is a vehicle of language: it defines a set of forms used by a community to carry information. Such a code is necessary to support the linguistic interactions that allow humans to communicate. How then may a speech code be formed prior to the existence of linguistic interactions? Moreover, the human speech code is discrete and compositional, shared by all the individuals of a community but different across communities, and phoneme inventories are characterized by statistical regularities. How can a speech code with these properties form? We try to approach these questions in the paper, using the \"methodology of the artificial\". We build a society of artificial agents, and detail a mechanism that shows the formation of a discrete speech code without pre-supposing the existence of linguistic capacities or of coordinated interactions. The mechanism is based on a low-level model of sensory-motor interactions. We show that the integration of certain very simple and non language-specific neural devices leads to the formation of a speech code that has properties similar to the human speech code. This result relies on the self-organizing properties of a generic coupling between perception and production within agents, and on the interactions between agents. The artificial system helps us to develop better intuitions on how speech might have appeared, by showing how self-organization might have helped natural selection to find speech.",
        "published": "2005-02-22T09:51:16Z",
        "link": "http://arxiv.org/abs/cs/0502086v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.NE",
            "cs.RO",
            "math.DS"
        ]
    },
    {
        "title": "Self-Replicating Strands that Self-Assemble into User-Specified Meshes",
        "authors": [
            "Robert Ewaschuk",
            "Peter Turney"
        ],
        "summary": "It has been argued that a central objective of nanotechnology is to make products inexpensively, and that self-replication is an effective approach to very low-cost manufacturing. The research presented here is intended to be a step towards this vision. In previous work (JohnnyVon 1.0), we simulated machines that bonded together to form self-replicating strands. There were two types of machines (called types 0 and 1), which enabled strands to encode arbitrary bit strings. However, the information encoded in the strands had no functional role in the simulation. The information was replicated without being interpreted, which was a significant limitation for potential manufacturing applications. In the current work (JohnnyVon 2.0), the information in a strand is interpreted as instructions for assembling a polygonal mesh. There are now four types of machines and the information encoded in a strand determines how it folds. A strand may be in an unfolded state, in which the bonds are straight (although they flex slightly due to virtual forces acting on the machines), or in a folded state, in which the bond angles depend on the types of machines. By choosing the sequence of machine types in a strand, the user can specify a variety of polygonal shapes. A simulation typically begins with an initial unfolded seed strand in a soup of unbonded machines. The seed strand replicates by bonding with free machines in the soup. The child strands fold into the encoded polygonal shape, and then the polygons drift together and bond to form a mesh. We demonstrate that a variety of polygonal meshes can be manufactured in the simulation, by simply changing the sequence of machine types in the seed.",
        "published": "2005-02-22T16:53:15Z",
        "link": "http://arxiv.org/abs/cs/0502087v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "Property analysis of symmetric travelling salesman problem instances   acquired through evolution",
        "authors": [
            "J. I. van Hemert"
        ],
        "summary": "We show how an evolutionary algorithm can successfully be used to evolve a set of difficult to solve symmetric travelling salesman problem instances for two variants of the Lin-Kernighan algorithm. Then we analyse the instances in those sets to guide us towards deferring general knowledge about the efficiency of the two variants in relation to structural properties of the symmetric travelling sale sman problem.",
        "published": "2005-02-28T16:40:34Z",
        "link": "http://arxiv.org/abs/cs/0502096v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "G1.6;I.2.8"
        ]
    },
    {
        "title": "Obtaining Membership Functions from a Neuron Fuzzy System extended by   Kohonen Network",
        "authors": [
            "Angelo Luis Pagliosa",
            "Claudio Cesar de Sa",
            "Fernando D. Sasse"
        ],
        "summary": "This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network (NFN-MK), an hybrid computational model that combines fuzzy system technique and artificial neural networks. Its main task consists in the automatic generation of membership functions, in particular, triangle forms, aiming a dynamic modeling of a system. The model is tested by simulating real systems, here represented by a nonlinear mathematical function. Comparison with the results obtained by traditional neural networks, and correlated studies of neurofuzzy systems applied in system identification area, shows that the NFN-MK model has a similar performance, despite its greater simplicity.",
        "published": "2005-03-29T19:40:14Z",
        "link": "http://arxiv.org/abs/cs/0503078v1",
        "categories": [
            "cs.NE",
            "C.1.3; I.2.6"
        ]
    },
    {
        "title": "Fitness Uniform Deletion: A Simple Way to Preserve Diversity",
        "authors": [
            "Shane Legg",
            "Marcus Hutter"
        ],
        "summary": "A commonly experienced problem with population based optimisation methods is the gradual decline in population diversity that tends to occur over time. This can slow a system's progress or even halt it completely if the population converges on a local optimum from which it cannot escape. In this paper we present the Fitness Uniform Deletion Scheme (FUDS), a simple but somewhat unconventional approach to this problem. Under FUDS the deletion operation is modified to only delete those individuals which are \"common\" in the sense that there exist many other individuals of similar fitness in the population. This makes it impossible for the population to collapse to a collection of highly related individuals with similar fitness. Our experimental results on a range of optimisation problems confirm this, in particular for deceptive optimisation problems the performance is significantly more robust to variation in the selection intensity.",
        "published": "2005-04-11T10:42:41Z",
        "link": "http://arxiv.org/abs/cs/0504035v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.M"
        ]
    },
    {
        "title": "Learning Polynomial Networks for Classification of Clinical   Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "We describe a polynomial network technique developed for learning to classify clinical electroencephalograms (EEGs) presented by noisy features. Using an evolutionary strategy implemented within Group Method of Data Handling, we learn classification models which are comprehensively described by sets of short-term polynomials. The polynomial models were learnt to classify the EEGs recorded from Alzheimer and healthy patients and recognize the EEG artifacts. Comparing the performances of our technique and some machine learning methods we conclude that our technique can learn well-suited polynomial models which experts can find easy-to-understand.",
        "published": "2005-04-11T17:11:29Z",
        "link": "http://arxiv.org/abs/cs/0504041v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Learning Multi-Class Neural-Network Models from Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult",
            "Burkhart Scheidt",
            "Valery Kuriakin"
        ],
        "summary": "We describe a new algorithm for learning multi-class neural-network models from large-scale clinical electroencephalograms (EEGs). This algorithm trains hidden neurons separately to classify all the pairs of classes. To find best pairwise classifiers, our algorithm searches for input variables which are relevant to the classification problem. Despite patient variability and heavily overlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns correctly classified 80.8% of the training and 80.1% of the testing examples. Additionally, the neural-network model provides a probabilistic interpretation of decisions.",
        "published": "2005-04-13T13:22:49Z",
        "link": "http://arxiv.org/abs/cs/0504052v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "A Neural-Network Technique for Recognition of Filaments in Solar Images",
        "authors": [
            "V. V. Zharkova",
            "V. Schetinin"
        ],
        "summary": "We describe a new neural-network technique developed for an automated recognition of solar filaments visible in the hydrogen H-alpha line full disk spectroheliograms. This technique allows neural networks learn from a few image fragments labelled manually to recognize the single filaments depicted on a local background. The trained network is able to recognize filaments depicted on the backgrounds with variations in brightness caused by atmospherics distortions. Despite the difference in backgrounds in our experiments the neural network has properly recognized filaments in the testing image fragments. Using a parabolic activation function we extend this technique to recognize multiple solar filaments which may appear in one fragment.",
        "published": "2005-04-13T13:28:15Z",
        "link": "http://arxiv.org/abs/cs/0504053v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Learning from Web: Review of Approaches",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "Knowledge discovery is defined as non-trivial extraction of implicit, previously unknown and potentially useful information from given data. Knowledge extraction from web documents deals with unstructured, free-format documents whose number is enormous and rapidly growing. The artificial neural networks are well suitable to solve a problem of knowledge discovery from web documents because trained networks are able more accurately and easily to classify the learning and testing examples those represent the text mining domain. However, the neural networks that consist of large number of weighted connections and activation units often generate the incomprehensible and hard-to-understand models of text classification. This problem may be also addressed to most powerful recurrent neural networks that employ the feedback links from hidden or output units to their input units. Due to feedback links, recurrent neural networks are able take into account of a context in document. To be useful for data mining, self-organizing neural network techniques of knowledge extraction have been explored and developed. Self-organization principles were used to create an adequate neural-network structure and reduce a dimensionality of features used to describe text documents. The use of these principles seems interesting because ones are able to reduce a neural-network redundancy and considerably facilitate the knowledge representation.",
        "published": "2005-04-13T13:40:38Z",
        "link": "http://arxiv.org/abs/cs/0504054v1",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "A Learning Algorithm for Evolving Cascade Neural Networks",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is described. An ECNN starts to learn with one input node and then adding new inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly minimal number of input and hidden neurons as well as connections. The algorithm was successfully applied to classify artifacts and normal segments in clinical electroencephalograms (EEGs). The EEG segments were visually labeled by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing segments. It is slightly better than a standard fully connected neural network.",
        "published": "2005-04-13T13:57:56Z",
        "link": "http://arxiv.org/abs/cs/0504055v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity",
        "authors": [
            "V. Schetinin"
        ],
        "summary": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.",
        "published": "2005-04-13T13:59:55Z",
        "link": "http://arxiv.org/abs/cs/0504056v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Diagnostic Rule Extraction Using Neural Networks",
        "authors": [
            "Vitaly Schetinin",
            "Anatoly Brazhnikov"
        ],
        "summary": "The neural networks have trained on incomplete sets that a doctor could collect. Trained neural networks have correctly classified all the presented instances. The number of intervals entered for encoding the quantitative variables is equal two. The number of features as well as the number of neurons and layers in trained neural networks was minimal. Trained neural networks are adequately represented as a set of logical formulas that more comprehensible and easy-to-understand. These formulas are as the syndrome-complexes, which may be easily tabulated and represented as a diagnostic table that the doctors usually use. Decision rules provide the evaluations of their confidence in which interested a doctor. Conducted clinical researches have shown that iagnostic decisions produced by symbolic rules have coincided with the doctor's conclusions.",
        "published": "2005-04-13T14:03:02Z",
        "link": "http://arxiv.org/abs/cs/0504057v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Polynomial Neural Networks Learnt to Classify EEG Signals",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "A neural network based technique is presented, which is able to successfully extract polynomial classification rules from labeled electroencephalogram (EEG) signals. To represent the classification rules in an analytical form, we use the polynomial neural networks trained by a modified Group Method of Data Handling (GMDH). The classification rules were extracted from clinical EEG data that were recorded from an Alzheimer patient and the sudden death risk patients. The third data is EEG recordings that include the normal and artifact segments. These EEG data were visually identified by medical experts. The extracted polynomial rules verified on the testing EEG data allow to correctly classify 72% of the risk group patients and 96.5% of the segments. These rules performs slightly better than standard feedforward neural networks.",
        "published": "2005-04-13T14:06:32Z",
        "link": "http://arxiv.org/abs/cs/0504058v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Neural Network Decision Tree for Learning Concepts from EEG Data",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "To learn the multi-class conceptions from the electroencephalogram (EEG) data we developed a neural network decision tree (DT), that performs the linear tests, and a new training algorithm. We found that the known methods fail inducting the classification models when the data are presented by the features some of them are irrelevant, and the classes are heavily overlapped. To train the DT, our algorithm exploits a bottom up search of the features that provide the best classification accuracy of the linear tests. We applied the developed algorithm to induce the DT from the large EEG dataset consisted of 65 patients belonging to 16 age groups. In these recordings each EEG segment was represented by 72 calculated features. The DT correctly classified 80.8% of the training and 80.1% of the testing examples. Correspondingly it correctly classified 89.2% and 87.7% of the EEG recordings.",
        "published": "2005-04-13T14:28:48Z",
        "link": "http://arxiv.org/abs/cs/0504059v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "An Evolving Cascade Neural Network Technique for Cleaning Sleep   Electroencephalograms",
        "authors": [
            "Vitaly Schetinin"
        ],
        "summary": "Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable of selecting informative features are described. The ECNN initially learns with one input node and then evolves by adding new inputs as well as new hidden neurons. The resultant ECNN has a near minimal number of hidden neurons and inputs. The algorithm is successfully used for training ECNN to recognise artefacts in sleep electroencephalograms (EEGs) which were visually labelled by EEG-viewers. In our experiments, the ECNN outperforms the standard neural-network as well as evolutionary techniques.",
        "published": "2005-04-14T10:36:54Z",
        "link": "http://arxiv.org/abs/cs/0504067v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Self-Organization of the Neuron Collective of Optimal Complexity",
        "authors": [
            "V. Schetinin",
            "A. Kostunin"
        ],
        "summary": "The optimal complexity of neural networks is achieved when the self-organization principles is used to eliminate the contradictions existing in accordance with the K. Godel theorem about incompleteness of the systems based on axiomatics. The principle of S. Beer exterior addition the Heuristic Group Method of Data Handling by A. Ivakhnenko realized is used.",
        "published": "2005-04-14T10:45:06Z",
        "link": "http://arxiv.org/abs/cs/0504068v1",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "A Neural-Network Technique to Learn Concepts from Electroencephalograms",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "A new technique is presented developed to learn multi-class concepts from clinical electroencephalograms. A desired concept is represented as a neuronal computational model consisting of the input, hidden, and output neurons. In this model the hidden neurons learn independently to classify the electroencephalogram segments presented by spectral and statistical features. This technique has been applied to the electroencephalogram data recorded from 65 sleeping healthy newborns in order to learn a brain maturation concept of newborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these data have been used for learning and testing the concept, respectively. As a result, the concept has correctly classified 80.1% of the testing segments or 87.7% of the 65 records.",
        "published": "2005-04-14T10:47:38Z",
        "link": "http://arxiv.org/abs/cs/0504069v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "The Combined Technique for Detection of Artifacts in Clinical   Electroencephalograms of Sleeping Newborns",
        "authors": [
            "Vitaly Schetinin",
            "Joachim Schult"
        ],
        "summary": "In this paper we describe a new method combining the polynomial neural network and decision tree techniques in order to derive comprehensible classification rules from clinical electroencephalograms (EEGs) recorded from sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement, muscle and noise artifacts and as a consequence some EEG features are irrelevant to classification problems. Combining the polynomial network and decision tree techniques, we discover comprehensible classification rules whilst also attempting to keep their classification error down. This technique is shown to outperform a number of commonly used machine learning technique applied to automatically recognize artifacts in the sleep EEGs.",
        "published": "2005-04-14T10:49:55Z",
        "link": "http://arxiv.org/abs/cs/0504070v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ]
    },
    {
        "title": "A New Kind of Hopfield Networks for Finding Global Optimum",
        "authors": [
            "Xiaofei Huang"
        ],
        "summary": "The Hopfield network has been applied to solve optimization problems over decades. However, it still has many limitations in accomplishing this task. Most of them are inherited from the optimization algorithms it implements. The computation of a Hopfield network, defined by a set of difference equations, can easily be trapped into one local optimum or another, sensitive to initial conditions, perturbations, and neuron update orders. It doesn't know how long it will take to converge, as well as if the final solution is a global optimum, or not. In this paper, we present a Hopfield network with a new set of difference equations to fix those problems. The difference equations directly implement a new powerful optimization algorithm.",
        "published": "2005-04-30T16:13:32Z",
        "link": "http://arxiv.org/abs/cs/0505003v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Visual Character Recognition using Artificial Neural Networks",
        "authors": [
            "Shashank Araokar"
        ],
        "summary": "The recognition of optical characters is known to be one of the earliest applications of Artificial Neural Networks, which partially emulate human thinking in the domain of artificial intelligence. In this paper, a simplified neural approach to recognition of optical or visual characters is portrayed and discussed. The document is expected to serve as a resource for learners and amateur investigators in pattern recognition, neural networking and related disciplines.",
        "published": "2005-05-07T20:56:58Z",
        "link": "http://arxiv.org/abs/cs/0505016v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Artificial Neural Networks and their Applications",
        "authors": [
            "Nitin Malik"
        ],
        "summary": "The Artificial Neural network is a functional imitation of simplified model of the biological neurons and their goal is to construct useful computers for real world problems. The ANN applications have increased dramatically in the last few years fired by both theoretical and practical applications in a wide variety of applications. A brief theory of ANN is presented and potential areas are identified and future trends are discussed.",
        "published": "2005-05-10T06:37:31Z",
        "link": "http://arxiv.org/abs/cs/0505019v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Distant generalization by feedforward neural networks",
        "authors": [
            "Artur Rataj"
        ],
        "summary": "This paper discusses the notion of generalization of training samples over long distances in the input space of a feedforward neural network. Such a generalization might occur in various ways, that differ in how great the contribution of different training features should be.   The structure of a neuron in a feedforward neural network is analyzed and it is concluded, that the actual performance of the discussed generalization in such neural networks may be problematic -- while such neural networks might be capable for such a distant generalization, a random and spurious generalization may occur as well.   To illustrate the differences in generalizing of the same function by different learning machines, results given by the support vector machines are also presented.",
        "published": "2005-05-10T11:36:35Z",
        "link": "http://arxiv.org/abs/cs/0505021v3",
        "categories": [
            "cs.NE",
            "I.2.6"
        ]
    },
    {
        "title": "Characterizing Self-Developing Biological Neural Networks: A First Step   Towards their Application To Computing Systems",
        "authors": [
            "Hugues Berry",
            "Olivier Temam"
        ],
        "summary": "Carbon nanotubes are often seen as the only alternative technology to silicon transistors. While they are the most likely short-term one, other longer-term alternatives should be studied as well. While contemplating biological neurons as an alternative component may seem preposterous at first sight, significant recent progress in CMOS-neuron interface suggests this direction may not be unrealistic; moreover, biological neurons are known to self-assemble into very large networks capable of complex information processing tasks, something that has yet to be achieved with other emerging technologies. The first step to designing computing systems on top of biological neurons is to build an abstract model of self-assembled biological neural networks, much like computer architects manipulate abstract models of transistors and circuits. In this article, we propose a first model of the structure of biological neural networks. We provide empirical evidence that this model matches the biological neural networks found in living organisms, and exhibits the small-world graph structure properties commonly found in many large and self-organized systems, including biological neural networks. More importantly, we extract the simple local rules and characteristics governing the growth of such networks, enabling the development of potentially large but realistic biological neural networks, as would be needed for complex information processing/computing tasks. Based on this model, future work will be targeted to understanding the evolution and learning properties of such networks, and how they can be used to build computing systems.",
        "published": "2005-05-10T19:51:16Z",
        "link": "http://arxiv.org/abs/q-bio/0505021v1",
        "categories": [
            "q-bio.NC",
            "cs.AR",
            "cs.NE",
            "nlin.AO"
        ]
    },
    {
        "title": "Separating a Real-Life Nonlinear Image Mixture",
        "authors": [
            "Luis B. Almeida"
        ],
        "summary": "When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation.   This paper addresses a difficult version of this problem, corresponding to the use of \"onion skin\" paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement.",
        "published": "2005-05-16T19:31:36Z",
        "link": "http://arxiv.org/abs/cs/0505044v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Sub-Optimum Signal Linear Detector Using Wavelets and Support Vector   Machines",
        "authors": [
            "Jaime Gomez",
            "Ignacio Melgar",
            "Juan Seijas",
            "Diego Andina"
        ],
        "summary": "The problem of known signal detection in Additive White Gaussian Noise is considered. In previous work, a new detection scheme was introduced by the authors, and it was demonstrated that optimum performance cannot be reached in a real implementation. In this paper we analyse Support Vector Machines (SVM) as an alternative, evaluating the results in terms of Probability of detection curves for a fixed Probability of false alarm.",
        "published": "2005-05-20T14:54:40Z",
        "link": "http://arxiv.org/abs/cs/0505051v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Upgrading Pulse Detection with Time Shift Properties Using Wavelets and   Support Vector Machines",
        "authors": [
            "Jaime Gomez",
            "Ignacio Melgar",
            "Juan Seijas"
        ],
        "summary": "Current approaches in pulse detection use domain transformations so as to concentrate frequency related information that can be distinguishable from noise. In real cases we do not know when the pulse will begin, so we need a time search process in which time windows are scheduled and analysed. Each window can contain the pulsed signal (either complete or incomplete) and / or noise. In this paper a simple search process will be introduced, allowing the algorithm to process more information, upgrading the capabilities in terms of probability of detection (Pd) and probability of false alarm (Pfa).",
        "published": "2005-05-20T15:01:20Z",
        "link": "http://arxiv.org/abs/cs/0505052v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Wavelet Time Shift Properties Integration with Support Vector Machines",
        "authors": [
            "Jaime Gomez",
            "Ignacio Melgar",
            "Juan Seijas"
        ],
        "summary": "This paper presents a short evaluation about the integration of information derived from wavelet non-linear-time-invariant (non-LTI) projection properties using Support Vector Machines (SVM). These properties may give additional information for a classifier trying to detect known patterns hidden by noise. In the experiments we present a simple electromagnetic pulsed signal recognition scheme, where some improvement is achieved with respect to previous work. SVMs are used as a tool for information integration, exploiting some unique properties not easily found in neural networks.",
        "published": "2005-05-20T15:06:40Z",
        "link": "http://arxiv.org/abs/cs/0505053v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "A dissipative particle swarm optimization",
        "authors": [
            "Xiao-Feng Xie",
            "Wen-Jun Zhang",
            "Zhi-Lian Yang"
        ],
        "summary": "A dissipative particle swarm optimization is developed according to the self-organization of dissipative structure. The negative entropy is introduced to construct an opening dissipative system that is far-from-equilibrium so as to driving the irreversible evolution process with better fitness. The testing of two multimodal functions indicates it improves the performance effectively",
        "published": "2005-05-24T14:54:06Z",
        "link": "http://arxiv.org/abs/cs/0505065v2",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Optimizing semiconductor devices by self-organizing particle swarm",
        "authors": [
            "Xiao-Feng Xie",
            "Wen-Jun Zhang",
            "De-Chun Bi"
        ],
        "summary": "A self-organizing particle swarm is presented. It works in dissipative state by employing the small inertia weight, according to experimental analysis on a simplified model, which with fast convergence. Then by recognizing and replacing inactive particles according to the process deviation information of device parameters, the fluctuation is introduced so as to driving the irreversible evolution process with better fitness. The testing on benchmark functions and an application example for device optimization with designed fitness function indicates it improves the performance effectively.",
        "published": "2005-05-25T01:28:18Z",
        "link": "http://arxiv.org/abs/cs/0505067v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Handling equality constraints by adaptive relaxing rule for swarm   algorithms",
        "authors": [
            "Xiao-Feng Xie",
            "Wen-Jun Zhang",
            "De-Chun Bi"
        ],
        "summary": "The adaptive constraints relaxing rule for swarm algorithms to handle with the problems with equality constraints is presented. The feasible space of such problems may be similiar to ridge function class, which is hard for applying swarm algorithms. To enter the solution space more easily, the relaxed quasi feasible space is introduced and shrinked adaptively. The experimental results on benchmark functions are compared with the performance of other algorithms, which show its efficiency.",
        "published": "2005-05-25T01:32:30Z",
        "link": "http://arxiv.org/abs/cs/0505068v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Handling boundary constraints for numerical optimization by particle   swarm flying in periodic search space",
        "authors": [
            "Wen-Jun Zhang",
            "Xiao-Feng Xie",
            "De-Chun Bi"
        ],
        "summary": "The periodic mode is analyzed together with two conventional boundary handling modes for particle swarm. By providing an infinite space that comprises periodic copies of original search space, it avoids possible disorganizing of particle swarm that is induced by the undesired mutations at the boundary. The results on benchmark functions show that particle swarm with periodic mode is capable of improving the search performance significantly, by compared with that of conventional modes and other algorithms.",
        "published": "2005-05-25T01:36:07Z",
        "link": "http://arxiv.org/abs/cs/0505069v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "SWAF: Swarm Algorithm Framework for Numerical Optimization",
        "authors": [
            "Xiao-Feng Xie",
            "Wen-Jun Zhang"
        ],
        "summary": "A swarm algorithm framework (SWAF), realized by agent-based modeling, is presented to solve numerical optimization problems. Each agent is a bare bones cognitive architecture, which learns knowledge by appropriately deploying a set of simple rules in fast and frugal heuristics. Two essential categories of rules, the generate-and-test and the problem-formulation rules, are implemented, and both of the macro rules by simple combination and subsymbolic deploying of multiple rules among them are also studied. Experimental results on benchmark problems are presented, and performance comparison between SWAF and other existing algorithms indicates that it is efficiently.",
        "published": "2005-05-25T01:39:55Z",
        "link": "http://arxiv.org/abs/cs/0505070v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "The Hyper-Cortex of Human Collective-Intelligence Systems",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "Individual-intelligence research, from a neurological perspective, discusses the hierarchical layers of the cortex as a structure that performs conceptual abstraction and specification. This theory has been used to explain how motor-cortex regions responsible for different behavioral modalities such as writing and speaking can be utilized to express the same general concept represented higher in the cortical hierarchy. For example, the concept of a dog, represented across a region of high-level cortical-neurons, can either be written or spoken about depending on the individual's context. The higher-layer cortical areas project down the hierarchy, sending abstract information to specific regions of the motor-cortex for contextual implementation. In this paper, this idea is expanded to incorporate collective-intelligence within a hyper-cortical construct. This hyper-cortex is a multi-layered network used to represent abstract collective concepts. These ideas play an important role in understanding how collective-intelligence systems can be engineered to handle problem abstraction and solution specification. Finally, a collection of common problems in the scientific community are solved using an artificial hyper-cortex generated from digital-library metadata.",
        "published": "2005-06-08T21:33:44Z",
        "link": "http://arxiv.org/abs/cs/0506024v3",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.DL",
            "cs.NE"
        ]
    },
    {
        "title": "Framework for Hopfield Network based Adaptive routing - A design level   approach for adaptive routing phenomena with Artificial Neural Network",
        "authors": [
            "R. Shankar"
        ],
        "summary": "Routing, as a basic phenomena, by itself, has got umpteen scopes to analyse, discuss and arrive at an optimal solution for the technocrats over years. Routing is analysed based on many factors; few key constraints that decide the factors are communication medium, time dependency, information source nature. Parametric routing has become the requirement of the day, with some kind of adaptation to the underlying network environment. Satellite constellations, particularly LEO satellite constellations have become a reality in operational to have a non-breaking voice/data communication around the world.Routing in these constellations has to be treated in a non conventional way, taking their network geometry into consideration. One of the efficient methods of optimization is putting Neural Networks to use. Few Artificial Neural Network models are very much suitable for the adaptive control mechanism, by their nature of network arrangement. One such efficient model is Hopfield Network model.   This paper is an attempt to design a framework for the Hopfield Network based adaptive routing phenomena in satellite constellations.",
        "published": "2005-06-10T05:30:41Z",
        "link": "http://arxiv.org/abs/cs/0506032v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Dynamical Neural Network: Information and Topology",
        "authors": [
            "David Dominguez",
            "Kostadin Koroutchev",
            "Eduardo Serrano",
            "Francisco B. Rodriguez"
        ],
        "summary": "A neural network works as an associative memory device if it has large storage capacity and the quality of the retrieval is good enough. The learning and attractor abilities of the network both can be measured by the mutual information (MI), between patterns and retrieval states. This paper deals with a search for an optimal topology, of a Hebb network, in the sense of the maximal MI. We use small-world topology. The connectivity $\\gamma$ ranges from an extremely diluted to the fully connected network; the randomness $\\omega$ ranges from purely local to completely random neighbors. It is found that, while stability implies an optimal $MI(\\gamma,\\omega)$ at $\\gamma_{opt}(\\omega)\\to 0$, for the dynamics, the optimal topology holds at certain $\\gamma_{opt}>0$ whenever $0\\leq\\omega<0.3$.",
        "published": "2005-06-20T14:54:41Z",
        "link": "http://arxiv.org/abs/cs/0506078v1",
        "categories": [
            "cs.IR",
            "cs.NE"
        ]
    },
    {
        "title": "Transmitting a signal by amplitude modulation in a chaotic network",
        "authors": [
            "B. Cessac",
            "J. A. Sepulchre"
        ],
        "summary": "We discuss the ability of a network with non linear relays and chaotic dynamics to transmit signals, on the basis of a linear response theory developed by Ruelle \\cite{Ruelle} for dissipative systems. We show in particular how the dynamics interfere with the graph topology to produce an effective transmission network, whose topology depends on the signal, and cannot be directly read on the ``wired'' network. This leads one to reconsider notions such as ``hubs''. Then, we show examples where, with a suitable choice of the carrier frequency (resonance), one can transmit a signal from a node to another one by amplitude modulation, \\textit{in spite of chaos}. Also, we give an example where a signal, transmitted to any node via different paths, can only be recovered by a couple of \\textit{specific} nodes. This opens the possibility for encoding data in a way such that the recovery of the signal requires the knowledge of the carrier frequency \\textit{and} can be performed only at some specific node.",
        "published": "2005-06-28T13:36:34Z",
        "link": "http://arxiv.org/abs/nlin/0506061v1",
        "categories": [
            "nlin.CD",
            "cond-mat.stat-mech",
            "cs.NE"
        ]
    },
    {
        "title": "Neuromodulation Influences Synchronization and Intrinsic Read-out",
        "authors": [
            "Gabriele Scheler"
        ],
        "summary": "Background: The roles of neuromodulation in a neural network, such as in a cortical microcolumn, are still incompletely understood. Neuromodulation influences neural processing by presynaptic and postsynaptic regulation of synaptic efficacy. Neuromodulation also affects ion channels and intrinsic excitability. Methods: Synaptic efficacy modulation is an effective way to rapidly alter network density and topology. We alter network topology and density to measure the effect on spike synchronization. We also operate with differently parameterized neuron models which alter the neurons intrinsic excitability, i.e., activation function. Results: We find that (a) fast synaptic efficacy modulation influences the amount of correlated spiking in a network. Also, (b) synchronization in a network influences the read-out of intrinsic properties. Highly synchronous input drives neurons, such that differences in intrinsic properties disappear, while asynchronous input lets intrinsic properties determine output behavior. Thus, altering network topology can alter the balance between intrinsically vs. synaptically driven network activity. Conclusion: We conclude that neuromodulation may allow a network to shift between a more synchronized transmission mode and a more asynchronous intrinsic read-out mode. This has significant implications for our understanding of the flexibility of cortical computations.",
        "published": "2005-07-25T21:01:32Z",
        "link": "http://arxiv.org/abs/q-bio/0507037v4",
        "categories": [
            "q-bio.NC",
            "cs.NE",
            "nlin.AO",
            "68T05, 91E40"
        ]
    },
    {
        "title": "Long-term neuronal behavior caused by two synaptic modification   mechanisms",
        "authors": [
            "Xi Shen",
            "Philippe De Wilde"
        ],
        "summary": "We report the first results of simulating the coupling of neuronal, astrocyte, and cerebrovascular activity. It is suggested that the dynamics of the system is different from systems that only include neurons. In the neuron-vascular coupling, distribution of synapse strengths affects neuronal behavior and thus balance of the blood flow; oscillations are induced in the neuron-to-astrocyte coupling.",
        "published": "2005-08-26T10:38:36Z",
        "link": "http://arxiv.org/abs/cs/0508117v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Accurate and robust image superresolution by neural processing of local   image representations",
        "authors": [
            "Carlos Miravet",
            "Francisco B. Rodriguez"
        ],
        "summary": "Image superresolution involves the processing of an image sequence to generate a still image with higher resolution. Classical approaches, such as bayesian MAP methods, require iterative minimization procedures, with high computational costs. Recently, the authors proposed a method to tackle this problem, based on the use of a hybrid MLP-PNN architecture. In this paper, we present a novel superresolution method, based on an evolution of this concept, to incorporate the use of local image models. A neural processing stage receives as input the value of model coefficients on local windows. The data dimensionality is firstly reduced by application of PCA. An MLP, trained on synthetic sequences with various amounts of noise, estimates the high-resolution image data. The effect of varying the dimension of the network input space is examined, showing a complex, structured behavior. Quantitative results are presented showing the accuracy and robustness of the proposed method.",
        "published": "2005-10-03T19:42:55Z",
        "link": "http://arxiv.org/abs/cs/0510008v1",
        "categories": [
            "cs.CV",
            "cs.NE",
            "I.4.5; I.2.6; I.5.1"
        ]
    },
    {
        "title": "Does a Plane Imitate a Bird? Does Computer Vision Have to Follow   Biological Paradigms?",
        "authors": [
            "Emanuel Diamant"
        ],
        "summary": "We posit a new paradigm for image information processing. For the last 25 years, this task was usually approached in the frame of Treisman's two-stage paradigm [1]. The latter supposes an unsupervised, bottom-up directed process of preliminary information pieces gathering at the lower processing stages and a supervised, top-down directed process of information pieces binding and grouping at the higher stages. It is acknowledged that these sub-processes interact and intervene between them in a tricky and a complicated manner. Notwithstanding the prevalence of this paradigm in biological and computer vision, we nevertheless propose to replace it with a new one, which we would like to designate as a two-part paradigm. In it, information contained in an image is initially extracted in an independent top-down manner by one part of the system, and then it is examined and interpreted by another, separate system part. We argue that the new paradigm seems to be more plausible than its forerunner. We provide evidence from human attention vision studies and insights of Kolmogorov's complexity theory to support our arguments. We also provide some reasons in favor of separate image interpretation issues.",
        "published": "2005-11-04T15:08:47Z",
        "link": "http://arxiv.org/abs/cs/0511022v1",
        "categories": [
            "cs.NE"
        ]
    },
    {
        "title": "Discrete Network Dynamics. Part 1: Operator Theory",
        "authors": [
            "Stephen Luttrell"
        ],
        "summary": "An operator algebra implementation of Markov chain Monte Carlo algorithms for simulating Markov random fields is proposed. It allows the dynamics of networks whose nodes have discrete state spaces to be specified by the action of an update operator that is composed of creation and annihilation operators. This formulation of discrete network dynamics has properties that are similar to those of a quantum field theory of bosons, which allows reuse of many conceptual and theoretical structures from QFT. The equilibrium behaviour of one of these generalised MRFs and of the adaptive cluster expansion network (ACEnet) are shown to be equivalent, which provides a way of unifying these two theories.",
        "published": "2005-11-07T19:09:01Z",
        "link": "http://arxiv.org/abs/cs/0511027v1",
        "categories": [
            "cs.NE",
            "F.1.2; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Dimensions of Neural-symbolic Integration - A Structured Survey",
        "authors": [
            "Sebastian Bader",
            "Pascal Hitzler"
        ],
        "summary": "Research on integrated neural-symbolic systems has made significant progress in the recent past. In particular the understanding of ways to deal with symbolic knowledge within connectionist systems (also called artificial neural networks) has reached a critical mass which enables the community to strive for applicable implementations and use cases. Recent work has covered a great variety of logics used in artificial intelligence and provides a multitude of techniques for dealing with them within the context of artificial neural networks. We present a comprehensive survey of the field of neural-symbolic integration, including a new classification of system according to their architectures and abilities.",
        "published": "2005-11-10T10:00:46Z",
        "link": "http://arxiv.org/abs/cs/0511042v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.NE"
        ]
    },
    {
        "title": "The use of the GARP genetic algorithm and internet grid computing in the   Lifemapper world atlas of species biodiversity",
        "authors": [
            "David R. B. Stockwell",
            "James H. Beach",
            "Aimee Stewart",
            "Gregory Vorontsov",
            "David Vieglais",
            "Ricardo Scachetti Pereira"
        ],
        "summary": "Lifemapper (http://www.lifemapper.org) is a predictive electronic atlas of the Earth's biological biodiversity. Using a screensaver version of the GARP genetic algorithm for modeling species distributions, Lifemapper harnesses vast computing resources through 'volunteers' PCs similar to SETI@home, to develop models of the distribution of the worlds fauna and flora. The Lifemapper project's primary goal is to provide an up to date and comprehensive database of species maps and prediction models (i.e. a fauna and flora of the world) using available data on species' locations. The models are developed using specimen data from distributed museum collections and an archive of geospatial environmental correlates. A central server maintains a dynamic archive of species maps and models for research, outreach to the general community, and feedback to museum data providers. This paper is a case study in the role, use and justification of a genetic algorithm in development of large-scale environmental informatics infrastructure.",
        "published": "2005-11-28T17:54:03Z",
        "link": "http://arxiv.org/abs/q-bio/0511045v1",
        "categories": [
            "q-bio.QM",
            "cs.DC",
            "cs.NE",
            "q-bio.OT"
        ]
    },
    {
        "title": "On Self-Regulated Swarms, Societal Memory, Speed and Dynamics",
        "authors": [
            "Vitorino Ramos",
            "Carlos Fernandes",
            "Agostinho C. Rosa"
        ],
        "summary": "We propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of our dynamic search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control problems, measuring reaction speeds and performance. Final comparisons were made with standard Genetic Algorithms (GAs), Bacterial Foraging strategies (BFOA), as well as with recent Co-Evolutionary approaches. SRS's were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found. One of the most interesting illustrate that the present SRS collective swarm of bio-inspired ant-like agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system.",
        "published": "2005-12-01T03:10:09Z",
        "link": "http://arxiv.org/abs/cs/0512002v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "I.2.11; G.1.6; I.2.9"
        ]
    },
    {
        "title": "DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation   framework",
        "authors": [
            "Anthony Mouraud",
            "Didier Puzenat",
            "Hélène Paugam-Moisy"
        ],
        "summary": "In a Spiking Neural Networks (SNN), spike emissions are sparsely and irregularly distributed both in time and in the network architecture. Since a current feature of SNNs is a low average activity, efficient implementations of SNNs are usually based on an Event-Driven Simulation (EDS). On the other hand, simulations of large scale neural networks can take advantage of distributing the neurons on a set of processors (either workstation cluster or parallel computer). This article presents DAMNED, a large scale SNN simulation framework able to gather the benefits of EDS and parallel computing. Two levels of parallelism are combined: Distributed mapping of the neural topology, at the network level, and local multithreaded allocation of resources for simultaneous processing of events, at the neuron level. Based on the causality of events, a distributed solution is proposed for solving the complex problem of scheduling without synchronization barrier.",
        "published": "2005-12-05T06:57:39Z",
        "link": "http://arxiv.org/abs/cs/0512018v2",
        "categories": [
            "cs.NE",
            "cs.LG"
        ]
    },
    {
        "title": "Amazing geometry of genetic space or are genetic algorithms convergent?",
        "authors": [
            "Marek W. Gutowski"
        ],
        "summary": "There is no proof yet of convergence of Genetic Algorithms. We do not supply it too. Instead, we present some thoughts and arguments to convince the Reader, that Genetic Algorithms are essentially bound for success. For this purpose, we consider only the crossover operators, single- or multiple-point, together with selection procedure. We also give a proof that the soft selection is superior to other selection schemes.",
        "published": "2005-12-05T11:12:42Z",
        "link": "http://arxiv.org/abs/cs/0512019v1",
        "categories": [
            "cs.NE",
            "cs.DM",
            "cs.SE",
            "F.2.2; G.1.6; G.4; I.1.2"
        ]
    },
    {
        "title": "Evolving Stochastic Learning Algorithm Based on Tsallis Entropic Index",
        "authors": [
            "Aristoklis D. Anastasiadis",
            "George D. Magoulas"
        ],
        "summary": "In this paper, inspired from our previous algorithm, which was based on the theory of Tsallis statistical mechanics, we develop a new evolving stochastic learning algorithm for neural networks. The new algorithm combines deterministic and stochastic search steps by employing a different adaptive stepsize for each network weight, and applies a form of noise that is characterized by the nonextensive entropic index q, regulated by a weight decay term. The behavior of the learning algorithm can be made more stochastic or deterministic depending on the trade off between the temperature T and the q values. This is achieved by introducing a formula that defines a time--dependent relationship between these two important learning parameters. Our experimental study verifies that there are indeed improvements in the convergence speed of this new evolving stochastic learning algorithm, which makes learning faster than using the original Hybrid Learning Scheme (HLS). In addition, experiments are conducted to explore the influence of the entropic index q and temperature T on the convergence speed and stability of the proposed method.",
        "published": "2005-12-09T20:30:02Z",
        "link": "http://arxiv.org/abs/cs/0512037v2",
        "categories": [
            "cs.NE",
            "cs.AI"
        ]
    },
    {
        "title": "Evolino for recurrent support vector machines",
        "authors": [
            "Juergen Schmidhuber",
            "Matteo Gagliolo",
            "Daan Wierstra",
            "Faustino Gomez"
        ],
        "summary": "Traditional Support Vector Machines (SVMs) need pre-wired finite time windows to predict and classify time series. They do not have an internal state necessary to deal with sequences involving arbitrary long-term dependencies. Here we introduce a new class of recurrent, truly sequential SVM-like devices with internal adaptive states, trained by a novel method called EVOlution of systems with KErnel-based outputs (Evoke), an instance of the recent Evolino class of methods. Evoke evolves recurrent neural networks to detect and represent temporal dependencies while using quadratic programming/support vector regression to produce precise outputs. Evoke is the first SVM-based mechanism learning to classify a context-sensitive language. It also outperforms recent state-of-the-art gradient-based recurrent neural networks (RNNs) on various time series prediction tasks.",
        "published": "2005-12-15T15:05:22Z",
        "link": "http://arxiv.org/abs/cs/0512062v1",
        "categories": [
            "cs.NE",
            "F.1.1; I.2.6"
        ]
    },
    {
        "title": "\"Going back to our roots\": second generation biocomputing",
        "authors": [
            "Jon Timmis",
            "Martyn Amos",
            "Wolfgang Banzhaf",
            "Andy Tyrrell"
        ],
        "summary": "Researchers in the field of biocomputing have, for many years, successfully \"harvested and exploited\" the natural world for inspiration in developing systems that are robust, adaptable and capable of generating novel and even \"creative\" solutions to human-defined problems. However, in this position paper we argue that the time has now come for a reassessment of how we exploit biology to generate new computational systems. Previous solutions (the \"first generation\" of biocomputing techniques), whilst reasonably effective, are crude analogues of actual biological systems. We believe that a new, inherently inter-disciplinary approach is needed for the development of the emerging \"second generation\" of bio-inspired methods. This new modus operandi will require much closer interaction between the engineering and life sciences communities, as well as a bidirectional flow of concepts, applications and expertise. We support our argument by examining, in this new light, three existing areas of biocomputing (genetic programming, artificial immune systems and evolvable hardware), as well as an emerging area (natural genetic engineering) which may provide useful pointers as to the way forward.",
        "published": "2005-12-16T16:42:25Z",
        "link": "http://arxiv.org/abs/cs/0512071v1",
        "categories": [
            "cs.AI",
            "cs.NE"
        ]
    },
    {
        "title": "Enhancing Histograms by Tree-Like Bucket Indices",
        "authors": [
            "Francesco Buccafurri",
            "Gianluca Lax",
            "Domenico Sacca'",
            "Luigi Pontieri",
            "Domenico Rosaci"
        ],
        "summary": "Histograms are used to summarize the contents of relations into a number of buckets for the estimation of query result sizes. Several techniques (e.g., MaxDiff and V-Optimal) have been proposed in the past for determining bucket boundaries which provide accurate estimations. However, while search strategies for optimal bucket boundaries are rather sophisticated, no much attention has been paid for estimating queries inside buckets and all of the above techniques adopt naive methods for such an estimation. This paper focuses on the problem of improving the estimation inside a bucket once its boundaries have been fixed. The proposed technique is based on the addition, to each bucket, of 32-bit additional information (organized into a 4-level tree index), storing approximate cumulative frequencies at 7 internal intervals of the bucket. Both theoretical analysis and experimental results show that, among a number of alternative ways to organize the additional information, the 4-level tree index provides the best frequency estimation inside a bucket. The index is later added to two well-known histograms, MaxDiff and V-Optimal, obtaining the non-obvious result that despite the spatial cost of 4LT which reduces the number of allowed buckets once the storage space has been fixed, the original methods are strongly improved in terms of accuracy.",
        "published": "2005-01-11T10:15:31Z",
        "link": "http://arxiv.org/abs/cs/0501020v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Maintaining Consistency of Data on the Web",
        "authors": [
            "Martin Bernauer"
        ],
        "summary": "Increasingly more data is becoming available on the Web, estimates speaking of 1 billion documents in 2002. Most of the documents are Web pages whose data is considered to be in XML format, expecting it to eventually replace HTML.   A common problem in designing and maintaining a Web site is that data on a Web page often replicates or derives from other data, the so-called base data, that is usually not contained in the deriving or replicating page. Consequently, replicas and derivations become inconsistent upon modifying base data in a Web page or a relational database. For example, after assigning a thesis to a student and modifying the Web page that describes it in detail, the thesis is still incorrectly contained in the list of offered thesis, missing in the list of ongoing thesis, and missing in the advisor's teaching record.   The thesis presents a solution by proposing a combined approach that provides for maintaining consistency of data in Web pages that (i) replicate data in relational databases, or (ii) replicate or derive from data in Web pages. Upon modifying base data, the modification is immediately pushed to affected Web pages. There, maintenance is performed incrementally by only modifying the affected part of the page instead of re-generating the whole page from scratch.",
        "published": "2005-01-20T14:11:03Z",
        "link": "http://arxiv.org/abs/cs/0501042v3",
        "categories": [
            "cs.DB",
            "cs.DS",
            "E.1; H.2.3; H.2.4"
        ]
    },
    {
        "title": "Improved Approximation Algorithms for Geometric Set Cover",
        "authors": [
            "Kenneth L. Clarkson",
            "Kasturi Varadarajan"
        ],
        "summary": "Given a collection S of subsets of some set U, and M a subset of U, the set cover problem is to find the smallest subcollection C of S such that M is a subset of the union of the sets in C. While the general problem is NP-hard to solve, even approximately, here we consider some geometric special cases, where usually U = R^d. Extending prior results, we show that approximation algorithms with provable performance exist, under a certain general condition: that for a random subset R of S and function f(), there is a decomposition of the portion of U not covered by R into an expected f(|R|) regions, each region of a particular simple form. We show that under this condition, a cover of size O(f(|C|)) can be found. Our proof involves the generalization of shallow cuttings to more general geometric situations. We obtain constant-factor approximation algorithms for covering by unit cubes in R^3, for guarding a one-dimensional terrain, and for covering by similar-sized fat triangles in R^2. We also obtain improved approximation guarantees for fat triangles, of arbitrary size, and for a class of fat objects.",
        "published": "2005-01-20T21:31:22Z",
        "link": "http://arxiv.org/abs/cs/0501045v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Optimal Union-Find in Constraint Handling Rules",
        "authors": [
            "Tom Schrijvers",
            "Thom Fruehwirth"
        ],
        "summary": "Constraint Handling Rules (CHR) is a committed-choice rule-based language that was originally intended for writing constraint solvers. In this paper we show that it is also possible to write the classic union-find algorithm and variants in CHR. The programs neither compromise in declarativeness nor efficiency. We study the time complexity of our programs: they match the almost-linear complexity of the best known imperative implementations. This fact is illustrated with experimental results.",
        "published": "2005-01-25T13:28:38Z",
        "link": "http://arxiv.org/abs/cs/0501073v1",
        "categories": [
            "cs.PL",
            "cs.CC",
            "cs.DS",
            "cs.PF"
        ]
    },
    {
        "title": "On the asymptotic behavior of some Algorithms",
        "authors": [
            "Philippe Robert"
        ],
        "summary": "A simple approach is presented to study the asymptotic behavior of some algorithms with an underlying tree structure. It is shown that some asymptotic oscillating behaviors can be precisely analyzed without resorting to complex analysis techniques as it is usually done in this context. A new explicit representation of periodic functions involved is obtained at the same time.",
        "published": "2005-02-03T08:25:09Z",
        "link": "http://arxiv.org/abs/cs/0502014v1",
        "categories": [
            "cs.DS",
            "math.CA",
            "math.PR"
        ]
    },
    {
        "title": "On Dynamic Range Reporting in One Dimension",
        "authors": [
            "Christian Worm Mortensen",
            "Rasmus Pagh",
            "Mihai Patrascu"
        ],
        "summary": "We consider the problem of maintaining a dynamic set of integers and answering queries of the form: report a point (equivalently, all points) in a given interval. Range searching is a natural and fundamental variant of integer search, and can be solved using predecessor search. However, for a RAM with w-bit words, we show how to perform updates in O(lg w) time and answer queries in O(lglg w) time. The update time is identical to the van Emde Boas structure, but the query time is exponentially faster. Existing lower bounds show that achieving our query time for predecessor search requires doubly-exponentially slower updates. We present some arguments supporting the conjecture that our solution is optimal.   Our solution is based on a new and interesting recursion idea which is \"more extreme\" that the van Emde Boas recursion. Whereas van Emde Boas uses a simple recursion (repeated halving) on each path in a trie, we use a nontrivial, van Emde Boas-like recursion on every such path. Despite this, our algorithm is quite clean when seen from the right angle. To achieve linear space for our data structure, we solve a problem which is of independent interest. We develop the first scheme for dynamic perfect hashing requiring sublinear space. This gives a dynamic Bloomier filter (an approximate storage scheme for sparse vectors) which uses low space. We strengthen previous lower bounds to show that these results are optimal.",
        "published": "2005-02-05T23:22:37Z",
        "link": "http://arxiv.org/abs/cs/0502032v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Logarithmic Lower Bounds in the Cell-Probe Model",
        "authors": [
            "Mihai Patrascu",
            "Erik D. Demaine"
        ],
        "summary": "We develop a new technique for proving cell-probe lower bounds on dynamic data structures. This technique enables us to prove an amortized randomized Omega(lg n) lower bound per operation for several data structural problems on n elements, including partial sums, dynamic connectivity among disjoint paths (or a forest or a graph), and several other dynamic graph problems (by simple reductions). Such a lower bound breaks a long-standing barrier of Omega(lg n / lglg n) for any dynamic language membership problem. It also establishes the optimality of several existing data structures, such as Sleator and Tarjan's dynamic trees. We also prove the first Omega(log_B n) lower bound in the external-memory model without assumptions on the data structure (such as the comparison model). Our lower bounds also give a query-update trade-off curve matched, e.g., by several data structures for dynamic connectivity in graphs. We also prove matching upper and lower bounds for partial sums when parameterized by the word size and the maximum additive change in an update.",
        "published": "2005-02-08T03:03:55Z",
        "link": "http://arxiv.org/abs/cs/0502041v2",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "Improved Tag Set Design and Multiplexing Algorithms for Universal Arrays",
        "authors": [
            "Ion I. Mandoiu",
            "Claudia Prajescu",
            "Dragos Trinca"
        ],
        "summary": "In this paper we address two optimization problems arising in the design of genomic assays based on universal tag arrays. First, we address the universal array tag set design problem. For this problem, we extend previous formulations to incorporate antitag-to-antitag hybridization constraints in addition to constraints on antitag-to-tag hybridization specificity, establish a constructive upper bound on the maximum number of tags satisfying the extended constraints, and propose a simple greedy tag selection algorithm. Second, we give methods for improving the multiplexing rate in large-scale genomic assays by combining primer selection with tag assignment. Experimental results on simulated data show that this integrated optimization leads to reductions of up to 50% in the number of required arrays.",
        "published": "2005-02-10T20:20:53Z",
        "link": "http://arxiv.org/abs/cs/0502054v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Individual displacements in hashing with coalesced chains",
        "authors": [
            "Svante Janson"
        ],
        "summary": "We study the asymptotic distribution of the displacements in hashing with coalesced chains, for both late-insertion and early-insertion. Asymptotic formulas for means and variances follow. The method uses Poissonization and some stochastic calculus.",
        "published": "2005-02-11T09:28:04Z",
        "link": "http://arxiv.org/abs/math/0502232v1",
        "categories": [
            "math.PR",
            "cs.DS",
            "68P10 (Primary); 60F05, 60G35, 68W40 (Secondary)"
        ]
    },
    {
        "title": "Highly Scalable Algorithms for Robust String Barcoding",
        "authors": [
            "Bhaskar DasGupta",
            "Kishori M. Konwar",
            "Ion I. Mandoiu",
            "Alex A. Shvartsman"
        ],
        "summary": "String barcoding is a recently introduced technique for genomic-based identification of microorganisms. In this paper we describe the engineering of highly scalable algorithms for robust string barcoding. Our methods enable distinguisher selection based on whole genomic sequences of hundreds of microorganisms of up to bacterial size on a well-equipped workstation, and can be easily parallelized to further extend the applicability range to thousands of bacterial size genomes. Experimental results on both randomly generated and NCBI genomic data show that whole-genome based selection results in a number of distinguishers nearly matching the information theoretic lower bounds for the problem.",
        "published": "2005-02-14T22:19:52Z",
        "link": "http://arxiv.org/abs/cs/0502065v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Bidimensionality, Map Graphs, and Grid Minors",
        "authors": [
            "Erik D. Demaine",
            "MohammadTaghi Hajiaghayi"
        ],
        "summary": "In this paper we extend the theory of bidimensionality to two families of graphs that do not exclude fixed minors: map graphs and power graphs. In both cases we prove a polynomial relation between the treewidth of a graph in the family and the size of the largest grid minor. These bounds improve the running times of a broad class of fixed-parameter algorithms. Our novel technique of using approximate max-min relations between treewidth and size of grid minors is powerful, and we show how it can also be used, e.g., to prove a linear relation between the treewidth of a bounded-genus graph and the treewidth of its dual.",
        "published": "2005-02-16T19:01:50Z",
        "link": "http://arxiv.org/abs/cs/0502070v1",
        "categories": [
            "cs.DM",
            "cs.DS"
        ]
    },
    {
        "title": "A note on the Burrows-Wheeler transformation",
        "authors": [
            "Maxime Crochemore",
            "Jacques Désarménien",
            "Dominique Perrin"
        ],
        "summary": "We relate the Burrows-Wheeler transformation with a result in combinatorics on words known as the Gessel-Reutenauer transformation.",
        "published": "2005-02-17T07:06:28Z",
        "link": "http://arxiv.org/abs/cs/0502073v1",
        "categories": [
            "cs.DS",
            "ACM E.4 ; ACM G.2.1"
        ]
    },
    {
        "title": "How far will you walk to find your shortcut: Space Efficient Synopsis   Construction Algorithms",
        "authors": [
            "Sudipto Guha"
        ],
        "summary": "In this paper we consider the wavelet synopsis construction problem without the restriction that we only choose a subset of coefficients of the original data. We provide the first near optimal algorithm. We arrive at the above algorithm by considering space efficient algorithms for the restricted version of the problem. In this context we improve previous algorithms by almost a linear factor and reduce the required space to almost linear. Our techniques also extend to histogram construction, and improve the space-running time tradeoffs for V-Opt and range query histograms. We believe the idea applies to a broad range of dynamic programs and demonstrate it by showing improvements in a knapsack-like setting seen in construction of Extended Wavelets.",
        "published": "2005-02-18T00:35:58Z",
        "link": "http://arxiv.org/abs/cs/0502075v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "The Weighted Maximum-Mean Subtree and Other Bicriterion Subtree Problems",
        "authors": [
            "Josiah Carlson",
            "David Eppstein"
        ],
        "summary": "We consider problems in which we are given a rooted tree as input, and must find a subtree with the same root, optimizing some objective function of the nodes in the subtree. When this function is the sum of constant node weights, the problem is trivially solved in linear time. When the objective is the sum of weights that are linear functions of a parameter, we show how to list all optima for all possible parameter values in O(n log n) time; this parametric optimization problem can be used to solve many bicriterion optimizations problems, in which each node has two values xi and yi associated with it, and the objective function is a bivariate function f(SUM(xi),SUM(yi)) of the sums of these two values. A special case, when f is the ratio of the two sums, is the Weighted Maximum-Mean Subtree Problem, or equivalently the Fractional Prize-Collecting Steiner Tree Problem on Trees; for this special case, we provide a linear time algorithm for this problem when all weights are positive, improving a previous O(n log n) solution, and prove that the problem is NP-complete when negative weights are allowed.",
        "published": "2005-03-09T18:16:14Z",
        "link": "http://arxiv.org/abs/cs/0503023v4",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Exact and Approximation Algorithms for DNA Tag Set Design",
        "authors": [
            "Ion I. Mandoiu",
            "Dragos Trinca"
        ],
        "summary": "In this paper we propose new solution methods for designing tag sets for use in universal DNA arrays. First, we give integer linear programming formulations for two previous formalizations of the tag set design problem, and show that these formulations can be solved to optimality for instance sizes of practical interest by using general purpose optimization packages. Second, we note the benefits of periodic tags, and establish an interesting connection between the tag design problem and the problem of packing the maximum number of vertex-disjoint directed cycles in a given graph. We show that combining a simple greedy cycle packing algorithm with a previously proposed alphabetic tree search strategy yields an increase of over 40% in the number of tags compared to previous methods.",
        "published": "2005-03-23T02:36:14Z",
        "link": "http://arxiv.org/abs/cs/0503057v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Data-Structure Rewriting",
        "authors": [
            "Dominique Duval",
            "Rachid Echahed",
            "Frederic Prost"
        ],
        "summary": "We tackle the problem of data-structure rewriting including pointer redirections. We propose two basic rewrite steps: (i) Local Redirection and Replacement steps the aim of which is redirecting specific pointers determined by means of a pattern, as well as adding new information to an existing data ; and (ii) Global Redirection steps which are aimed to redirect all pointers targeting a node towards another one. We define these two rewriting steps following the double pushout approach. We define first the category of graphs we consider and then define rewrite rules as pairs of graph homomorphisms of the form \"L <- K ->R\". Unfortunately, inverse pushouts (complement pushouts) are not unique in our setting and pushouts do not always exist. Therefore, we define rewriting steps so that a rewrite rule can always be performed once a matching is found.",
        "published": "2005-03-24T09:55:42Z",
        "link": "http://arxiv.org/abs/cs/0503065v1",
        "categories": [
            "cs.PL",
            "cs.DS",
            "D1, D3, E1, F3.3, I1"
        ]
    },
    {
        "title": "How to Simulate Billiards and Similar Systems",
        "authors": [
            "Boris D. Lubachevsky"
        ],
        "summary": "An N-component continuous-time dynamic system is considered whose components evolve autonomously all the time except for in discrete asynchronous instances of pairwise interactions. Examples include chaotically colliding billiard balls and combat models. A new efficient serial event-driven algorithm is described for simulating such systems. Rather than maintaining and updating the global state of the system, the algorithm tries to examine only essential events, i.e., component interactions. The events are processed in a non-decreasing order of time; new interactions are scheduled on the basis of the examined interactions using preintegrated equations of the evolutions of the components. If the components are distributed uniformly enough in the evolution space, so that this space can be subdivided into small sectors such that only O(1) sectors and O(1)$components are in the neighborhood of a sector, then the algorithm spends time O (log N) for processing an event which is the asymptotical minimum. The algorithm uses a simple strategy for handling data: only two states are maintained for each simulated component. Fast data access in this strategy assures the practical efficiency of the algorithm. It works noticeably faster than other algorithms proposed for this model.   Key phrases: collision detection, dense packing, molecular dynamics, hard spheres, granular flow",
        "published": "2005-03-26T16:15:36Z",
        "link": "http://arxiv.org/abs/cond-mat/0503627v2",
        "categories": [
            "cond-mat.mtrl-sci",
            "cs.DS",
            "math.DS"
        ]
    },
    {
        "title": "Optimization of Partial Search",
        "authors": [
            "Vladimir Korepin"
        ],
        "summary": "Quantum Grover search algorithm can find a target item in a database faster than any classical algorithm. One can trade accuracy for speed and find a part of the database (a block) containing the target item even faster, this is partial search. A partial search algorithm was recently suggested by Grover and Radhakrishnan. Here we optimize it. Efficiency of the search algorithm is measured by number of queries to the oracle. The author suggests new version of Grover-Radhakrishnan algorithm which uses minimal number of queries to the oracle. The algorithm can run on the same hardware which is used for the usual Grover algorithm.",
        "published": "2005-03-31T13:50:47Z",
        "link": "http://arxiv.org/abs/quant-ph/0503238v2",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Quantum search algorithms",
        "authors": [
            "Andris Ambainis"
        ],
        "summary": "We review some of quantum algorithms for search problems: Grover's search algorithm, its generalization to amplitude amplification, the applications of amplitude amplification to various problems and the recent quantum algorithms based on quantum walks.",
        "published": "2005-04-03T19:03:49Z",
        "link": "http://arxiv.org/abs/quant-ph/0504012v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Correlation Clustering with a Fixed Number of Clusters",
        "authors": [
            "Ioannis Giotis",
            "Venkatesan Guruswami"
        ],
        "summary": "We continue the investigation of problems concerning correlation clustering or clustering with qualitative information, which is a clustering formulation that has been studied recently. The basic setup here is that we are given as input a complete graph on n nodes (which correspond to nodes to be clustered) whose edges are labeled + (for similar pairs of items) and - (for dissimilar pairs of items). Thus we have only as input qualitative information on similarity and no quantitative distance measure between items. The quality of a clustering is measured in terms of its number of agreements, which is simply the number of edges it correctly classifies, that is the sum of number of - edges whose endpoints it places in different clusters plus the number of + edges both of whose endpoints it places within the same cluster.   In this paper, we study the problem of finding clusterings that maximize the number of agreements, and the complementary minimization version where we seek clusterings that minimize the number of disagreements. We focus on the situation when the number of clusters is stipulated to be a small constant k. Our main result is that for every k, there is a polynomial time approximation scheme for both maximizing agreements and minimizing disagreements. (The problems are NP-hard for every k >= 2.) The main technical work is for the minimization version, as the PTAS for maximizing agreements follows along the lines of the property tester for Max k-CUT.   In contrast, when the number of clusters is not specified, the problem of minimizing disagreements was shown to be APX-hard, even though the maximization version admits a PTAS.",
        "published": "2005-04-06T22:36:03Z",
        "link": "http://arxiv.org/abs/cs/0504023v1",
        "categories": [
            "cs.DS",
            "F.2.2; G.1.2; G.1.6"
        ]
    },
    {
        "title": "Searching Monotone Multi-dimensional Arrays",
        "authors": [
            "Yongxi Cheng",
            "Xiaoming Sun",
            "Yiqun Lisa Yin"
        ],
        "summary": "In this paper we investigate the problem of searching monotone multi-dimensional arrays. We generalize Linial and Saks' search algorithm \\cite{LS1} for monotone 3-dimensional arrays to $d$-dimensions with $d\\geq 4$. Our new search algorithm is asymptotically optimal for $d=4$.",
        "published": "2005-04-07T15:58:18Z",
        "link": "http://arxiv.org/abs/cs/0504026v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Disaster Management in Scale-Free Networks: Recovery from and Protection   Against Intentional Attacks",
        "authors": [
            "Behnam A. Rezaei",
            "Nima Sarshar",
            "P. Oscar Boykin",
            "Vwani P. Roychowdhury"
        ],
        "summary": "Susceptibility of scale free Power Law (PL) networks to attacks has been traditionally studied in the context of what may be termed as {\\em instantaneous attacks}, where a randomly selected set of nodes and edges are deleted while the network is kept {\\em static}. In this paper, we shift the focus to the study of {\\em progressive} and instantaneous attacks on {\\em reactive} grown and random PL networks, which can respond to attacks and take remedial steps. In the process, we present several techniques that managed networks can adopt to minimize the damages during attacks, and also to efficiently recover from the aftermath of successful attacks. For example, we present (i) compensatory dynamics that minimize the damages inflicted by targeted progressive attacks, such as linear-preferential deletions of nodes in grown PL networks; the resulting dynamic naturally leads to the emergence of networks with PL degree distributions with exponential cutoffs; (ii) distributed healing algorithms that can scale the maximum degree of nodes in a PL network using only local decisions, and (iii) efficient means of creating giant connected components in a PL network that has been fragmented by attacks on a large number of high-degree nodes. Such targeted attacks are considered to be a major vulnerability of PL networks; however, our results show that the introduction of only a small number of random edges, through a {\\em reverse percolation} process, can restore connectivity, which in turn allows restoration of other topological properties of the original network. Thus, the scale-free nature of the networks can itself be effectively utilized for protection and recovery purposes.",
        "published": "2005-04-07T20:57:16Z",
        "link": "http://arxiv.org/abs/cond-mat/0504185v3",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS",
            "cs.NI",
            "physics.data-an",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Fast Distributed Algorithms for Computing Separable Functions",
        "authors": [
            "Damon Mosk-Aoyama",
            "Devavrat Shah"
        ],
        "summary": "The problem of computing functions of values at the nodes in a network in a totally distributed manner, where nodes do not have unique identities and make decisions based only on local information, has applications in sensor, peer-to-peer, and ad-hoc networks. The task of computing separable functions, which can be written as linear combinations of functions of individual variables, is studied in this context. Known iterative algorithms for averaging can be used to compute the normalized values of such functions, but these algorithms do not extend in general to the computation of the actual values of separable functions.   The main contribution of this paper is the design of a distributed randomized algorithm for computing separable functions. The running time of the algorithm is shown to depend on the running time of a minimum computation algorithm used as a subroutine. Using a randomized gossip mechanism for minimum computation as the subroutine yields a complete totally distributed algorithm for computing separable functions. For a class of graphs with small spectral gap, such as grid graphs, the time used by the algorithm to compute averages is of a smaller order than the time required by a known iterative averaging scheme.",
        "published": "2005-04-08T06:49:29Z",
        "link": "http://arxiv.org/abs/cs/0504029v4",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Incremental Medians via Online Bidding",
        "authors": [
            "Marek Chrobak",
            "Claire Kenyon",
            "John Noga",
            "Neal E. Young"
        ],
        "summary": "In the k-median problem we are given sets of facilities and customers, and distances between them. For a given set F of facilities, the cost of serving a customer u is the minimum distance between u and a facility in F. The goal is to find a set F of k facilities that minimizes the sum, over all customers, of their service costs.   Following Mettu and Plaxton, we study the incremental medians problem, where k is not known in advance, and the algorithm produces a nested sequence of facility sets where the kth set has size k. The algorithm is c-cost-competitive if the cost of each set is at most c times the cost of the optimum set of size k. We give improved incremental algorithms for the metric version: an 8-cost-competitive deterministic algorithm, a 2e ~ 5.44-cost-competitive randomized algorithm, a (24+epsilon)-cost-competitive, poly-time deterministic algorithm, and a (6e+epsilon ~ .31)-cost-competitive, poly-time randomized algorithm.   The algorithm is s-size-competitive if the cost of the kth set is at most the minimum cost of any set of size k, and has size at most s k. The optimal size-competitive ratios for this problem are 4 (deterministic) and e (randomized). We present the first poly-time O(log m)-size-approximation algorithm for the offline problem and first poly-time O(log m)-size-competitive algorithm for the incremental problem.   Our proofs reduce incremental medians to the following online bidding problem: faced with an unknown threshold T, an algorithm submits \"bids\" until it submits a bid that is at least the threshold. It pays the sum of all its bids. We prove that folklore algorithms for online bidding are optimally competitive.",
        "published": "2005-04-27T00:07:32Z",
        "link": "http://arxiv.org/abs/cs/0504103v3",
        "categories": [
            "cs.DS",
            "G.1.6; G.2.2; F.2.2"
        ]
    },
    {
        "title": "The reverse greedy algorithm for the metric k-median problem",
        "authors": [
            "Marek Chrobak",
            "Claire Kenyon",
            "Neal E. Young"
        ],
        "summary": "The Reverse Greedy algorithm (RGreedy) for the k-median problem works as follows. It starts by placing facilities on all nodes. At each step, it removes a facility to minimize the resulting total distance from the customers to the remaining facilities. It stops when k facilities remain. We prove that, if the distance function is metric, then the approximation ratio of RGreedy is between ?(log n/ log log n) and O(log n).",
        "published": "2005-04-27T19:36:08Z",
        "link": "http://arxiv.org/abs/cs/0504104v2",
        "categories": [
            "cs.DS",
            "G.1.6; G.2.2; F.2.2"
        ]
    },
    {
        "title": "Computing finite-dimensional bipartite quantum separability",
        "authors": [
            "Lawrence M. Ioannou"
        ],
        "summary": "Ever since entanglement was identified as a computational and cryptographic resource, effort has been made to find an efficient way to tell whether a given density matrix represents an unentangled, or separable, state. Essentially, this is the quantum separability problem.   Chapters 1 to 3 motivate a new interior-point algorithm which, given the expected values of a subset of an orthogonal basis of observables of an otherwise unknown quantum state, searches for an entanglement witness in the span of the subset of observables. When all the expected values are known, the algorithm solves the separability problem. In Chapter 4, I give the motivation for the algorithm and show how it can be used in a particular physical scenario to detect entanglement (or decide separability) of an unknown quantum state using as few quantum resources as possible. I then explain the intuitive idea behind the algorithm and relate it to the standard algorithms of its kind. I end the chapter with a comparison of the complexities of the algorithms surveyed in Chapter 3. Finally, in Chapter 5, I present the details of the algorithm and discuss its performance relative to standard methods.",
        "published": "2005-04-29T16:42:54Z",
        "link": "http://arxiv.org/abs/cs/0504110v3",
        "categories": [
            "cs.DS",
            "quant-ph"
        ]
    },
    {
        "title": "Defragmenting the Module Layout of a Partially Reconfigurable Device",
        "authors": [
            "Jan van der Veen",
            "Sandor P. Fekete",
            "Ali Ahmadinia",
            "Christophe Bobda",
            "Frank Hannig",
            "Juergen Teich"
        ],
        "summary": "Modern generations of field-programmable gate arrays (FPGAs) allow for partial reconfiguration. In an online context, where the sequence of modules to be loaded on the FPGA is unknown beforehand, repeated insertion and deletion of modules leads to progressive fragmentation of the available space, making defragmentation an important issue. We address this problem by propose an online and an offline component for the defragmentation of the available space. We consider defragmenting the module layout on a reconfigurable device. This corresponds to solving a two-dimensional strip packing problem. Problems of this type are NP-hard in the strong sense, and previous algorithmic results are rather limited. Based on a graph-theoretic characterization of feasible packings, we develop a method that can solve two-dimensional defragmentation instances of practical size to optimality. Our approach is validated for a set of benchmark instances.",
        "published": "2005-05-02T01:10:04Z",
        "link": "http://arxiv.org/abs/cs/0505005v1",
        "categories": [
            "cs.AR",
            "cs.DS",
            "B.7; C.5; C.3"
        ]
    },
    {
        "title": "Adaptive Codes: A New Class of Non-standard Variable-length Codes",
        "authors": [
            "Dragos Trinca"
        ],
        "summary": "We introduce a new class of non-standard variable-length codes, called adaptive codes. This class of codes associates a variable-length codeword to the symbol being encoded depending on the previous symbols in the input data string. An efficient algorithm for constructing adaptive codes of order one is presented. Then, we introduce a natural generalization of adaptive codes, called GA codes.",
        "published": "2005-05-02T09:40:02Z",
        "link": "http://arxiv.org/abs/cs/0505007v1",
        "categories": [
            "cs.DS",
            "E.4; F.4.3"
        ]
    },
    {
        "title": "Human being is a living random number generator",
        "authors": [
            "Arindam Mitra"
        ],
        "summary": "General wisdom is, mathematical operation is needed to generate number by numbers. It is pointed out that without any mathematical operation true random numbers can be generated by numbers through algorithmic process. It implies that human brain itself is a living true random number generator. Human brain can meet the enormous human demand of true random numbers.",
        "published": "2005-05-03T15:42:24Z",
        "link": "http://arxiv.org/abs/cs/0505009v17",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Complex Mean and Variance of Linear Regression Model for High-Noised   Systems by Kriging",
        "authors": [
            "Tomasz Suslo"
        ],
        "summary": "The aim of the paper is to derive the complex-valued least-squares estimator for bias-noise mean and variance.",
        "published": "2005-05-07T12:11:56Z",
        "link": "http://arxiv.org/abs/cs/0505015v4",
        "categories": [
            "cs.NA",
            "cs.DS"
        ]
    },
    {
        "title": "The Generic Multiple-Precision Floating-Point Addition With Exact   Rounding (as in the MPFR Library)",
        "authors": [
            "Vincent Lefèvre"
        ],
        "summary": "We study the multiple-precision addition of two positive floating-point numbers in base 2, with exact rounding, as specified in the MPFR library, i.e. where each number has its own precision. We show how the best possible complexity (up to a constant factor that depends on the implementation) can be obtain.",
        "published": "2005-05-11T14:22:54Z",
        "link": "http://arxiv.org/abs/cs/0505027v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "A linear memory algorithm for Baum-Welch training",
        "authors": [
            "Istvan Miklos",
            "Irmtraud M. Meyer"
        ],
        "summary": "Background: Baum-Welch training is an expectation-maximisation algorithm for training the emission and transition probabilities of hidden Markov models in a fully automated way.   Methods and results: We introduce a linear space algorithm for Baum-Welch training. For a hidden Markov model with M states, T free transition and E free emission parameters, and an input sequence of length L, our new algorithm requires O(M) memory and O(L M T_max (T + E)) time for one Baum-Welch iteration, where T_max is the maximum number of states that any state is connected to. The most memory efficient algorithm until now was the checkpointing algorithm with O(log(L) M) memory and O(log(L) L M T_max) time requirement. Our novel algorithm thus renders the memory requirement completely independent of the length of the training sequences. More generally, for an n-hidden Markov model and n input sequences of length L, the memory requirement of O(log(L) L^(n-1) M) is reduced to O(L^(n-1) M) memory while the running time is changed from O(log(L) L^n M T_max + L^n (T + E)) to O(L^n M T_max (T + E)).   Conclusions: For the large class of hidden Markov models used for example in gene prediction, whose number of states does not scale with the length of the input sequence, our novel algorithm can thus be both faster and more memory-efficient than any of the existing algorithms.",
        "published": "2005-05-11T16:45:58Z",
        "link": "http://arxiv.org/abs/cs/0505028v3",
        "categories": [
            "cs.LG",
            "cs.DS",
            "q-bio.QM",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Estudo e Implementacao de Algoritmos de Roteamento sobre Grafos em um   Sistema de Informacoes Geograficas",
        "authors": [
            "Rudini M. Sampaio",
            "Horacio H. Yanasse"
        ],
        "summary": "This article presents an implementation of a graphical software with various algorithms in Operations research, like minimum path, minimum tree, chinese postman problem and travelling salesman.",
        "published": "2005-05-11T18:50:32Z",
        "link": "http://arxiv.org/abs/cs/0505031v1",
        "categories": [
            "cs.MS",
            "cs.DS"
        ]
    },
    {
        "title": "Improved Combinatorial Group Testing Algorithms for Real-World Problem   Sizes",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Daniel S. Hirschberg"
        ],
        "summary": "We study practically efficient methods for performing combinatorial group testing. We present efficient non-adaptive and two-stage combinatorial group testing algorithms, which identify the at most d items out of a given set of n items that are defective, using fewer tests for all practical set sizes. For example, our two-stage algorithm matches the information theoretic lower bound for the number of tests in a combinatorial group testing regimen.",
        "published": "2005-05-18T20:25:16Z",
        "link": "http://arxiv.org/abs/cs/0505048v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "EAH: A New Encoder based on Adaptive Variable-length Codes",
        "authors": [
            "Dragos Trinca"
        ],
        "summary": "Adaptive variable-length codes associate a variable-length codeword to the symbol being encoded depending on the previous symbols in the input string. This class of codes has been recently presented in [Dragos Trinca, arXiv:cs.DS/0505007] as a new class of non-standard variable-length codes. New algorithms for data compression, based on adaptive variable-length codes of order one and Huffman's algorithm, have been recently presented in [Dragos Trinca, ITCC 2004]. In this paper, we extend the work done so far by the following contributions: first, we propose an improved generalization of these algorithms, called EAHn. Second, we compute the entropy bounds for EAHn, using the well-known bounds for Huffman's algorithm. Third, we discuss implementation details and give reports of experimental results obtained on some well-known corpora. Finally, we describe a parallel version of EAHn using the PRAM model of computation.",
        "published": "2005-05-24T06:53:33Z",
        "link": "http://arxiv.org/abs/cs/0505061v1",
        "categories": [
            "cs.DS",
            "E.4; F.4.3"
        ]
    },
    {
        "title": "Decision Sort and its Parallel Implementation",
        "authors": [
            "Udayan Khuarana"
        ],
        "summary": "In this paper, a sorting technique is presented that takes as input a data set whose primary key domain is known to the sorting algorithm, and works with an time efficiency of O(n+k), where k is the primary key domain. It is shown that the algorithm has applicability over a wide range of data sets. Later, a parallel formulation of the same is proposed and its effectiveness is argued. Though this algorithm is applicable over a wide range of general data sets, it finds special application (much superior to others) in places where sorting information that arrives in parts and in cases where input data is huge in size.",
        "published": "2005-05-24T15:41:27Z",
        "link": "http://arxiv.org/abs/cs/0505066v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Summarization Techniques for Pattern Collections in Data Mining",
        "authors": [
            "Taneli Mielikäinen"
        ],
        "summary": "Discovering patterns from data is an important task in data mining. There exist techniques to find large collections of many kinds of patterns from data very efficiently. A collection of patterns can be regarded as a summary of the data. A major difficulty with patterns is that pattern collections summarizing the data well are often very large.   In this dissertation we describe methods for summarizing pattern collections in order to make them also more understandable. More specifically, we focus on the following themes: 1) Quality value simplifications. 2) Pattern orderings. 3) Pattern chains and antichains. 4) Change profiles. 5) Inverse pattern discovery.",
        "published": "2005-05-26T04:41:15Z",
        "link": "http://arxiv.org/abs/cs/0505071v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.DS",
            "E.4; F.2; H.2.8; I.2; I.2.4"
        ]
    },
    {
        "title": "On Searching a Table Consistent with Division Poset",
        "authors": [
            "Yongxi Cheng",
            "Xi Chen",
            "Yiqun Lisa Yin"
        ],
        "summary": "Suppose $P_n=\\{1,2,...,n\\}$ is a partially ordered set with the partial order defined by divisibility, that is, for any two distinct elements $i,j\\in P_n$ satisfying $i$ divides $j$, $i<_{P_n} j$. A table $A_n=\\{a_i|i=1,2,...,n\\}$ of distinct real numbers is said to be \\emph{consistent} with $P_n$, provided for any two distinct elements $i,j\\in \\{1,2,...,n\\}$ satisfying $i$ divides $j$, $a_i< a_j$. Given an real number $x$, we want to determine whether $x\\in A_n$, by comparing $x$ with as few entries of $A_n$ as possible. In this paper we investigate the complexity $\\tau(n)$, measured in the number of comparisons, of the above search problem. We present a $\\frac{55n}{72}+O(\\ln^2 n)$ search algorithm for $A_n$ and prove a lower bound $({3/4}+{17/2160})n+O(1)$ on $\\tau(n)$ by using an adversary argument.",
        "published": "2005-05-26T17:45:48Z",
        "link": "http://arxiv.org/abs/cs/0505075v2",
        "categories": [
            "cs.DM",
            "cs.DS",
            "G.2.1; F.2.2"
        ]
    },
    {
        "title": "Efficient Approximation of Convex Recolorings",
        "authors": [
            "Shlomo Moran",
            "Sagi Snir"
        ],
        "summary": "A coloring of a tree is convex if the vertices that pertain to any color induce a connected subtree; a partial coloring (which assigns colors to some of the vertices) is convex if it can be completed to a convex (total) coloring. Convex coloring of trees arise in areas such as phylogenetics, linguistics, etc. eg, a perfect phylogenetic tree is one in which the states of each character induce a convex coloring of the tree. Research on perfect phylogeny is usually focused on finding a tree so that few predetermined partial colorings of its vertices are convex.   When a coloring of a tree is not convex, it is desirable to know \"how far\" it is from a convex one. In [19], a natural measure for this distance, called the recoloring distance was defined: the minimal number of color changes at the vertices needed to make the coloring convex. This can be viewed as minimizing the number of \"exceptional vertices\" w.r.t. to a closest convex coloring. The problem was proved to be NP-hard even for colored string.   In this paper we continue the work of [19], and present a 2-approximation algorithm of convex recoloring of strings whose running time O(cn), where c is the number of colors and n is the size of the input, and an O(cn^2)-time 3-approximation algorithm for convex recoloring of trees.",
        "published": "2005-05-27T23:16:48Z",
        "link": "http://arxiv.org/abs/cs/0505077v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Sorting a Low-Entropy Sequence",
        "authors": [
            "Travis Gagie"
        ],
        "summary": "We give the first sorting algorithm with bounds in terms of higher-order entropies: let $S$ be a sequence of length $m$ containing $n$ distinct elements and let (H_\\ell (S)) be the $\\ell$th-order empirical entropy of $S$, with (n^{\\ell + 1} \\log n \\in O (m)); our algorithm sorts $S$ using ((H_\\ell (S) + O (1)) m) comparisons.",
        "published": "2005-06-08T22:15:18Z",
        "link": "http://arxiv.org/abs/cs/0506027v1",
        "categories": [
            "cs.DS",
            "E.4; E.5"
        ]
    },
    {
        "title": "Computing minimal models, stable models and answer sets",
        "authors": [
            "Z. Lonc",
            "M. Truszczynski"
        ],
        "summary": "We propose and study algorithms to compute minimal models, stable models and answer sets of t-CNF theories, and normal and disjunctive t-programs. We are especially interested in algorithms with non-trivial worst-case performance bounds. The bulk of the paper is concerned with the classes of 2- and 3-CNF theories, and normal and disjunctive 2- and 3-programs, for which we obtain significantly stronger results than those implied by our general considerations. We show that one can find all minimal models of 2-CNF theories and all answer sets of disjunctive 2-programs in time O(m 1.4422..^n). Our main results concern computing stable models of normal 3-programs, minimal models of 3-CNF theories and answer sets of disjunctive 3-programs. We design algorithms that run in time O(m 1.6701..^n), in the case of the first problem, and in time O(mn^2 2.2782..^n), in the case of the latter two. All these bounds improve by exponential factors the best algorithms known previously. We also obtain closely related upper bounds on the number of minimal models, stable models and answer sets a t-CNF theory, a normal t-program or a disjunctive t-program may have.   To appear in Theory and Practice of Logic Programming (TPLP).",
        "published": "2005-06-30T01:51:41Z",
        "link": "http://arxiv.org/abs/cs/0506104v1",
        "categories": [
            "cs.LO",
            "cs.DS",
            "D.1.6"
        ]
    },
    {
        "title": "Quantum Complexity of Testing Group Commutativity",
        "authors": [
            "Frederic Magniez",
            "Ashwin Nayak"
        ],
        "summary": "We consider the problem of testing the commutativity of a black-box group specified by its k generators. The complexity (in terms of k) of this problem was first considered by Pak, who gave a randomized algorithm involving O(k) group operations. We construct a quite optimal quantum algorithm for this problem whose complexity is in O (k^{2/3}). The algorithm uses and highlights the power of the quantization method of Szegedy. For the lower bound of Omega(k^{2/3}), we give a reduction from a special case of Element Distinctness to our problem. Along the way, we prove the optimality of the algorithm of Pak for the randomized model.",
        "published": "2005-06-30T12:23:10Z",
        "link": "http://arxiv.org/abs/quant-ph/0506265v4",
        "categories": [
            "quant-ph",
            "cs.DS"
        ]
    },
    {
        "title": "Isomorphism of graphs-a polynomial test",
        "authors": [
            "Moshe Schwartz"
        ],
        "summary": "An explicit algorithm is presented for testing whether two non-directed graphs are isomorphic or not. It is shown that for a graph of n vertices, the number of n independent operations needed for the test is polynomial in n. A proof that the algorithm actually performs the test is presented.",
        "published": "2005-07-06T11:35:42Z",
        "link": "http://arxiv.org/abs/cs/0507014v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Inferring AS Relationships: Dead End or Lively Beginning?",
        "authors": [
            "Xenofontas Dimitropoulos",
            "Dmitri Krioukov",
            "Bradley Huffaker",
            "kc claffy",
            "George Riley"
        ],
        "summary": "Recent techniques for inferring business relationships between ASs have yielded maps that have extremely few invalid BGP paths in the terminology of Gao. However, some relationships inferred by these newer algorithms are incorrect, leading to the deduction of unrealistic AS hierarchies. We investigate this problem and discover what causes it. Having obtained such insight, we generalize the problem of AS relationship inference as a multiobjective optimization problem with node-degree-based corrections to the original objective function of minimizing the number of invalid paths. We solve the generalized version of the problem using the semidefinite programming relaxation of the MAX2SAT problem. Keeping the number of invalid paths small, we obtain a more veracious solution than that yielded by recent heuristics.",
        "published": "2005-07-19T09:32:16Z",
        "link": "http://arxiv.org/abs/cs/0507047v1",
        "categories": [
            "cs.NI",
            "cs.DS"
        ]
    },
    {
        "title": "Skip-Webs: Efficient Distributed Data Structures for Multi-Dimensional   Data Sets",
        "authors": [
            "Lars Arge",
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "We present a framework for designing efficient distributed data structures for multi-dimensional data. Our structures, which we call skip-webs, extend and improve previous randomized distributed data structures, including skipnets and skip graphs. Our framework applies to a general class of data querying scenarios, which include linear (one-dimensional) data, such as sorted sets, as well as multi-dimensional data, such as d-dimensional octrees and digital tries of character strings defined over a fixed alphabet. We show how to perform a query over such a set of n items spread among n hosts using O(log n / log log n) messages for one-dimensional data, or O(log n) messages for fixed-dimensional data, while using only O(log n) space per host. We also show how to make such structures dynamic so as to allow for insertions and deletions in O(log n) messages for quadtrees, octrees, and digital tries, and O(log n / log log n) messages for one-dimensional data. Finally, we show how to apply a blocking strategy to skip-webs to further improve message complexity for one-dimensional data when hosts can store more data.",
        "published": "2005-07-19T20:30:33Z",
        "link": "http://arxiv.org/abs/cs/0507050v1",
        "categories": [
            "cs.DC",
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Confluent Layered Drawings",
        "authors": [
            "David Eppstein",
            "Michael T. Goodrich",
            "Jeremy Yu Meng"
        ],
        "summary": "We combine the idea of confluent drawings with Sugiyama style drawings, in order to reduce the edge crossings in the resultant drawings. Furthermore, it is easier to understand the structures of graphs from the mixed style drawings. The basic idea is to cover a layered graph by complete bipartite subgraphs (bicliques), then replace bicliques with tree-like structures. The biclique cover problem is reduced to a special edge coloring problem and solved by heuristic coloring algorithms. Our method can be extended to obtain multi-depth confluent layered drawings.",
        "published": "2005-07-19T22:25:53Z",
        "link": "http://arxiv.org/abs/cs/0507051v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Nonrepetitive Paths and Cycles in Graphs with Application to Sudoku",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We provide a simple linear time transformation from a directed or undirected graph with labeled edges to an unlabeled digraph, such that paths in the input graph in which no two consecutive edges have the same label correspond to paths in the transformed graph and vice versa. Using this transformation, we provide efficient algorithms for finding paths and cycles with no two consecutive equal labels. We also consider related problems where the paths and cycles are required to be simple; we find efficient algorithms for the undirected case of these problems but show the directed case to be NP-complete. We apply our path and cycle finding algorithms in a program for generating and solving Sudoku puzzles, and show experimentally that they lead to effective puzzle-solving rules that may also be of interest to human Sudoku puzzle solvers.",
        "published": "2005-07-20T15:58:30Z",
        "link": "http://arxiv.org/abs/cs/0507053v1",
        "categories": [
            "cs.DS",
            "cs.AI",
            "F.2.2; I.2.1"
        ]
    },
    {
        "title": "A New Approach for Boundary Recognition in Geometric Sensor Networks",
        "authors": [
            "Sandor P. Fekete",
            "Michael Kaufmann",
            "Alexander Kroeller",
            "Katharina Lehmann"
        ],
        "summary": "We describe a new approach for dealing with the following central problem in the self-organization of a geometric sensor network: Given a polygonal region R, and a large, dense set of sensor nodes that are scattered uniformly at random in R. There is no central control unit, and nodes can only communicate locally by wireless radio to all other nodes that are within communication radius r, without knowing their coordinates or distances to other nodes. The objective is to develop a simple distributed protocol that allows nodes to identify themselves as being located near the boundary of R and form connected pieces of the boundary. We give a comparison of several centrality measures commonly used in the analysis of social networks and show that restricted stress centrality is particularly suited for geometric networks; we provide mathematical as well as experimental evidence for the quality of this measure.",
        "published": "2005-08-01T19:44:53Z",
        "link": "http://arxiv.org/abs/cs/0508006v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "C.2.1; F.2.2; G.3"
        ]
    },
    {
        "title": "Multicommodity Flow Algorithms for Buffered Global Routing",
        "authors": [
            "Christoph Albrecht",
            "Andrew B. Kahng",
            "Ion I. Mandoiu",
            "Alexander Zelikovsky"
        ],
        "summary": "In this paper we describe a new algorithm for buffered global routing according to a prescribed buffer site map. Specifically, we describe a provably good multi-commodity flow based algorithm that finds a global routing minimizing buffer and wire congestion subject to given constraints on routing area (wirelength and number of buffers) and sink delays. Our algorithm allows computing the tradeoff curve between routing area and wire/buffer congestion under any combination of delay and capacity constraints, and simultaneously performs buffer/wire sizing, as well as layer and pin assignment. Experimental results show that near-optimal results are obtained with a practical runtime.",
        "published": "2005-08-06T12:44:09Z",
        "link": "http://arxiv.org/abs/cs/0508045v1",
        "categories": [
            "cs.DS",
            "B.7.2; F.2.2"
        ]
    },
    {
        "title": "On a Duality between Metrics and $Σ$-Proximities",
        "authors": [
            "P. Yu. Chebotarev",
            "E. V. Shamis"
        ],
        "summary": ": In studies of discrete structures, functions are frequently used that express proximity, but are not metrics. We consider a class of such functions that is characterized by a normalization condition and an inequality that plays the same role as the triangle inequality does for metrics. We show that the introduced functions, named $\\Sigma$-proximities, are in a definite sense dual to metrics: there exists a natural one-to-one correspondence between metrics and $\\Sigma$-proximities defined on the same finite set; in contrast to metrics, $\\Sigma$-proximities measure {\\it comparative} proximity; the closer the objects, the greater the $\\Sigma$-proximity; diagonal entries of the $\\Sigma$-proximity matrix characterize the ``centrality'' of elements. The results are extended to arbitrary infinite sets.",
        "published": "2005-08-10T14:25:41Z",
        "link": "http://arxiv.org/abs/math/0508183v1",
        "categories": [
            "math.MG",
            "cs.DS",
            "math.CO",
            "46F10; 54E40; 15A51"
        ]
    },
    {
        "title": "Extending Utility Representations of Partial Orders",
        "authors": [
            "Pavel Chebotarev"
        ],
        "summary": "The problem is considered as to whether a monotone function defined on a subset P of a Euclidean space can be strictly monotonically extended to the whole space. It is proved that this is the case if and only if the function is {\\em separably increasing}. Explicit formulas are given for a class of extensions which involves an arbitrary bounded increasing function. Similar results are obtained for monotone functions that represent strict partial orders on arbitrary abstract sets X. The special case where P is a Pareto subset is considered.",
        "published": "2005-08-11T12:51:11Z",
        "link": "http://arxiv.org/abs/math/0508199v1",
        "categories": [
            "math.OC",
            "cs.DS",
            "math.FA",
            "91B16, 26B40, 51M04"
        ]
    },
    {
        "title": "The Symmetric Traveling Salesman Problem",
        "authors": [
            "Howard Kleiman"
        ],
        "summary": "Let M be an nXn symetric matrix, n, even, T, an upper bound for T_OPT, an optimal tour, sigma_T, the smaller-valued perfect matching obtained from alternate edges of T expressed as a product of 2-cycles. Applying the modified Floyd-Warshall algorithm to (sigma_T)^-1M^-, we construct acceptable and 2-circuit cycles some sets of which may yield circuits that can be patched into tours. We obtain necessary and sufficient conditions for a set, S, of cycles to yield circuits that may be patched into a tour.Assume that the following (Condition A)is valid: If (sigma_T)s = T*, |T*|<T, then all cycles of s have values less than |T| - |sigma_T|.Let SFWOPT),S(OPT)be the respective sets of cycles yielding T_FWOPT, T_OPT. Given Condition(A), using F-W, we can always obtain S(FWOPT). Using Condition A but not F-W, S_OPT is always obtainable from a subset of the cycles obtained.",
        "published": "2005-08-11T22:08:27Z",
        "link": "http://arxiv.org/abs/math/0508212v3",
        "categories": [
            "math.CO",
            "cs.DS",
            "05"
        ]
    },
    {
        "title": "A General Framework for Codes Involving Redundancy Minimization",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "A framework with two scalar parameters is introduced for various problems of finding a prefix code minimizing a coding penalty function. The framework encompasses problems previously proposed by Huffman, Campbell, Nath, and Drmota and Szpankowski, shedding light on the relationships among these problems. In particular, Nath's range of problems can be seen as bridging the minimum average redundancy problem of Huffman with the minimum maximum pointwise redundancy problem of Drmota and Szpankowski. Using this framework, two linear-time Huffman-like algorithms are devised for the minimum maximum pointwise redundancy problem, the only one in the framework not previously solved with a Huffman-like algorithm. Both algorithms provide solutions common to this problem and a subrange of Nath's problems, the second algorithm being distinguished by its ability to find the minimum variance solution among all solutions common to the minimum maximum pointwise redundancy and Nath problems. Simple redundancy bounds are also presented.",
        "published": "2005-08-18T20:22:45Z",
        "link": "http://arxiv.org/abs/cs/0508083v2",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Source Coding for Quasiarithmetic Penalties",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Huffman coding finds a prefix code that minimizes mean codeword length for a given probability distribution over a finite number of items. Campbell generalized the Huffman problem to a family of problems in which the goal is to minimize not mean codeword length but rather a generalized mean known as a quasiarithmetic or quasilinear mean. Such generalized means have a number of diverse applications, including applications in queueing. Several quasiarithmetic-mean problems have novel simple redundancy bounds in terms of a generalized entropy. A related property involves the existence of optimal codes: For ``well-behaved'' cost functions, optimal codes always exist for (possibly infinite-alphabet) sources having finite generalized entropy. Solving finite instances of such problems is done by generalizing an algorithm for finding length-limited binary codes to a new algorithm for finding optimal binary codes for any quasiarithmetic mean with a convex cost function. This algorithm can be performed using quadratic time and linear space, and can be extended to other penalty functions, some of which are solvable with similar space and time complexity, and others of which are solvable with slightly greater complexity. This reduces the computational complexity of a problem involving minimum delay in a queue, allows combinations of previously considered problems to be optimized, and greatly expands the space of problems solvable in quadratic time and linear space. The algorithm can be extended for purposes such as breaking ties among possibly different optimal codes, as with bottom-merge Huffman coding.",
        "published": "2005-08-18T20:29:04Z",
        "link": "http://arxiv.org/abs/cs/0508084v5",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "High-performance BWT-based Encoders",
        "authors": [
            "Dragos Trinca"
        ],
        "summary": "In 1994, Burrows and Wheeler developed a data compression algorithm which performs significantly better than Lempel-Ziv based algorithms. Since then, a lot of work has been done in order to improve their algorithm, which is based on a reversible transformation of the input string, called BWT (the Burrows-Wheeler transformation). In this paper, we propose a compression scheme based on BWT, MTF (move-to-front coding), and a version of the algorithms presented in [Dragos Trinca, ITCC-2004].",
        "published": "2005-08-21T05:47:00Z",
        "link": "http://arxiv.org/abs/cs/0508086v1",
        "categories": [
            "cs.DS",
            "E.4; F.4.3"
        ]
    },
    {
        "title": "Modelling the Eulerian Path Problem using a String Matching Framework",
        "authors": [
            "Dragos Trinca"
        ],
        "summary": "The well-known Eulerian path problem can be solved in polynomial time (more exactly, there exists a linear time algorithm for this problem). In this paper, we model the problem using a string matching framework, and then initiate an algorithmic study on a variant of this problem, called the (2,1)-STRING-MATCH problem (which is actually a generalization of the Eulerian path problem). Then, we present a polynomial-time algorithm for the (2,1)-STRING-MATCH problem, which is the most important result of this paper. Specifically, we get a lower bound of Omega(n), and an upper bound of O(n^{2}).",
        "published": "2005-08-21T06:08:40Z",
        "link": "http://arxiv.org/abs/cs/0508087v1",
        "categories": [
            "cs.DS",
            "E.1; F.2.2; G.2.2"
        ]
    },
    {
        "title": "Modelling the EAH Data Compression Algorithm using Graph Theory",
        "authors": [
            "Dragos Trinca"
        ],
        "summary": "Adaptive codes associate variable-length codewords to symbols being encoded depending on the previous symbols in the input data string. This class of codes has been introduced in [Dragos Trinca, cs.DS/0505007] as a new class of non-standard variable-length codes. New algorithms for data compression, based on adaptive codes of order one, have been presented in [Dragos Trinca, ITCC-2004], where we have behaviorally shown that for a large class of input data strings, these algorithms substantially outperform the Lempel-Ziv universal data compression algorithm. EAH has been introduced in [Dragos Trinca, cs.DS/0505061], as an improved generalization of these algorithms. In this paper, we present a translation of the EAH algorithm into the graph theory.",
        "published": "2005-08-21T19:32:39Z",
        "link": "http://arxiv.org/abs/cs/0508089v1",
        "categories": [
            "cs.DS",
            "E.4; G.2.2"
        ]
    },
    {
        "title": "Translating the EAH Data Compression Algorithm into Automata Theory",
        "authors": [
            "Dragos Trinca"
        ],
        "summary": "Adaptive codes have been introduced in [Dragos Trinca, cs.DS/0505007] as a new class of non-standard variable-length codes. These codes associate variable-length codewords to symbols being encoded depending on the previous symbols in the input data string. A new data compression algorithm, called EAH, has been introduced in [Dragos Trinca, cs.DS/0505061], where we have behaviorally shown that for a large class of input data strings, this algorithm substantially outperforms the well-known Lempel-Ziv universal data compression algorithm. In this paper, we translate the EAH encoder into automata theory.",
        "published": "2005-08-21T19:56:31Z",
        "link": "http://arxiv.org/abs/cs/0508090v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Tightness of LP via Max-product Belief Propagation",
        "authors": [
            "Sujay Sanghavi",
            "Devavrat Shah"
        ],
        "summary": "We investigate the question of tightness of linear programming (LP) relaxation for finding a maximum weight independent set (MWIS) in sparse random weighted graphs. We show that an edge-based LP relaxation is asymptotically tight for Erdos-Renyi graph $G(n,c/n)$ for $c \\leq 2e$ and random regular graph $G(n,r)$ for $r\\leq 4$ when node weights are i.i.d. with exponential distribution of mean 1. We establish these results, through a precise relation between the tightness of LP relaxation and convergence of the max-product belief propagation algorithm. We believe that this novel method of understanding structural properties of combinatorial problems through properties of iterative procedure such as the max-product should be of interest in its own right.",
        "published": "2005-08-23T01:08:06Z",
        "link": "http://arxiv.org/abs/cs/0508097v2",
        "categories": [
            "cs.DS",
            "cs.DM"
        ]
    },
    {
        "title": "Streaming and Sublinear Approximation of Entropy and Information   Distances",
        "authors": [
            "Sudipto Guha",
            "Andrew McGregor",
            "Suresh Venkatasubramanian"
        ],
        "summary": "In many problems in data mining and machine learning, data items that need to be clustered or classified are not points in a high-dimensional space, but are distributions (points on a high dimensional simplex). For distributions, natural measures of distance are not the $\\ell_p$ norms and variants, but information-theoretic measures like the Kullback-Leibler distance, the Hellinger distance, and others. Efficient estimation of these distances is a key component in algorithms for manipulating distributions. Thus, sublinear resource constraints, either in time (property testing) or space (streaming) are crucial.   We start by resolving two open questions regarding property testing of distributions. Firstly, we show a tight bound for estimating bounded, symmetric f-divergences between distributions in a general property testing (sublinear time) framework (the so-called combined oracle model). This yields optimal algorithms for estimating such well known distances as the Jensen-Shannon divergence and the Hellinger distance. Secondly, we close a $(\\log n)/H$ gap between upper and lower bounds for estimating entropy $H$ in this model. In a stream setting (sublinear space), we give the first algorithm for estimating the entropy of a distribution. Our algorithm runs in polylogarithmic space and yields an asymptotic constant factor approximation scheme. We also provide other results along the space/time/approximation tradeoff curve.",
        "published": "2005-08-27T23:10:52Z",
        "link": "http://arxiv.org/abs/cs/0508122v2",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "A Sorting Algorithm Based on Calculation",
        "authors": [
            "Sheng Bao",
            "De-Shun Zheng"
        ],
        "summary": "This article introduces an adaptive sorting algorithm that can relocate elements accurately by substituting their values into a function which we name it the guessing function. We focus on building this function which is the mapping relationship between record values and their corresponding sorted locations essentially. The time complexity of this algorithm O(n),when records distributed uniformly. Additionally, similar approach can be used in the searching algorithm.",
        "published": "2005-08-29T14:22:57Z",
        "link": "http://arxiv.org/abs/cs/0508125v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "The Dynamics of Viral Marketing",
        "authors": [
            "Jure Leskovec",
            "Lada A. Adamic",
            "Bernardo A. Huberman"
        ],
        "summary": "We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We analyze how user behavior varies within user communities defined by a recommendation network. Product purchases follow a 'long tail' where a significant share of purchases belongs to rarely sold items. We establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies communities, product and pricing categories for which viral marketing seems to be very effective.",
        "published": "2005-09-05T21:41:15Z",
        "link": "http://arxiv.org/abs/physics/0509039v4",
        "categories": [
            "physics.soc-ph",
            "cond-mat.stat-mech",
            "cs.DB",
            "cs.DS"
        ]
    },
    {
        "title": "Optimal Prefix Codes with Fewer Distinct Codeword Lengths are Faster to   Construct",
        "authors": [
            "Ahmed Belal",
            "Amr Elmasry"
        ],
        "summary": "A new method for constructing minimum-redundancy binary prefix codes is described. Our method does not explicitly build a Huffman tree; instead it uses a property of optimal prefix codes to compute the codeword lengths corresponding to the input weights. Let $n$ be the number of weights and $k$ be the number of distinct codeword lengths as produced by the algorithm for the optimum codes. The running time of our algorithm is $O(k \\cdot n)$. Following our previous work in \\cite{be}, no algorithm can possibly construct optimal prefix codes in $o(k \\cdot n)$ time. When the given weights are presorted our algorithm performs $O(9^k \\cdot \\log^{2k}{n})$ comparisons.",
        "published": "2005-09-06T10:58:22Z",
        "link": "http://arxiv.org/abs/cs/0509015v4",
        "categories": [
            "cs.DS",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Sampling to estimate arbitrary subset sums",
        "authors": [
            "Nick Duffield",
            "Carsten Lund",
            "Mikkel Thorup"
        ],
        "summary": "Starting with a set of weighted items, we want to create a generic sample of a certain size that we can later use to estimate the total weight of arbitrary subsets. For this purpose, we propose priority sampling which tested on Internet data performed better than previous methods by orders of magnitude.   Priority sampling is simple to define and implement: we consider a steam of items i=0,...,n-1 with weights w_i. For each item i, we generate a random number r_i in (0,1) and create a priority q_i=w_i/r_i. The sample S consists of the k highest priority items. Let t be the (k+1)th highest priority. Each sampled item i in S gets a weight estimate W_i=max{w_i,t}, while non-sampled items get weight estimate W_i=0.   Magically, it turns out that the weight estimates are unbiased, that is, E[W_i]=w_i, and by linearity of expectation, we get unbiased estimators over any subset sum simply by adding the sampled weight estimates from the subset. Also, we can estimate the variance of the estimates, and surpricingly, there is no co-variance between different weight estimates W_i and W_j.   We conjecture an extremely strong near-optimality; namely that for any weight sequence, there exists no specialized scheme for sampling k items with unbiased estimators that gets smaller total variance than priority sampling with k+1 items. Very recently Mario Szegedy has settled this conjecture.",
        "published": "2005-09-09T21:47:52Z",
        "link": "http://arxiv.org/abs/cs/0509026v1",
        "categories": [
            "cs.DS",
            "C.2.3; E.1; F.2; G.3; H.3"
        ]
    },
    {
        "title": "On the Worst-case Performance of the Sum-of-Squares Algorithm for Bin   Packing",
        "authors": [
            "Janos Csirik",
            "David S. Johnson",
            "Claire Kenyon"
        ],
        "summary": "The Sum of Squares algorithm for bin packing was defined in [2] and studied in great detail in [1], where it was proved that its worst case performance ratio is at most 3. In this note, we improve the asymptotic worst case bound to 2.7777...",
        "published": "2005-09-12T14:49:48Z",
        "link": "http://arxiv.org/abs/cs/0509031v1",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Algorithms for Max Hamming Exact Satisfiability",
        "authors": [
            "Vilhelm Dahllof"
        ],
        "summary": "We here study Max Hamming XSAT, ie, the problem of finding two XSAT models at maximum Hamming distance. By using a recent XSAT solver as an auxiliary function, an O(1.911^n) time algorithm can be constructed, where n is the number of variables. This upper time bound can be further improved to O(1.8348^n) by introducing a new kind of branching, more directly suited for finding models at maximum Hamming distance. The techniques presented here are likely to be of practical use as well as of theoretical value, proving that there are non-trivial algorithms for maximum Hamming distance problems.",
        "published": "2005-09-14T09:04:20Z",
        "link": "http://arxiv.org/abs/cs/0509038v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Guarantees for the Success Frequency of an Algorithm for Finding   Dodgson-Election Winners",
        "authors": [
            "Christopher M. Homan",
            "Lane A. Hemaspaandra"
        ],
        "summary": "In the year 1876 the mathematician Charles Dodgson, who wrote fiction under the now more famous name of Lewis Carroll, devised a beautiful voting system that has long fascinated political scientists. However, determining the winner of a Dodgson election is known to be complete for the \\Theta_2^p level of the polynomial hierarchy. This implies that unless P=NP no polynomial-time solution to this problem exists, and unless the polynomial hierarchy collapses to NP the problem is not even in NP. Nonetheless, we prove that when the number of voters is much greater than the number of candidates--although the number of voters may still be polynomial in the number of candidates--a simple greedy algorithm very frequently finds the Dodgson winners in such a way that it ``knows'' that it has found them, and furthermore the algorithm never incorrectly declares a nonwinner to be a winner.",
        "published": "2005-09-19T21:59:24Z",
        "link": "http://arxiv.org/abs/cs/0509061v3",
        "categories": [
            "cs.DS",
            "cs.MA",
            "F.2.2; I.2.8; J.4"
        ]
    },
    {
        "title": "Fast and Compact Regular Expression Matching",
        "authors": [
            "Philip Bille",
            "Martin Farach-Colton"
        ],
        "summary": "We study 4 problems in string matching, namely, regular expression matching, approximate regular expression matching, string edit distance, and subsequence indexing, on a standard word RAM model of computation that allows logarithmic-sized words to be manipulated in constant time. We show how to improve the space and/or remove a dependency on the alphabet size for each problem using either an improved tabulation technique of an existing algorithm or by combining known algorithms in a new way.",
        "published": "2005-09-22T13:30:20Z",
        "link": "http://arxiv.org/abs/cs/0509069v3",
        "categories": [
            "cs.DS",
            "F.2.2; F.2.0; F.1.1"
        ]
    },
    {
        "title": "Evolutionary Trees and the Ising Model on the Bethe Lattice: a Proof of   Steel's Conjecture",
        "authors": [
            "Constantinos Daskalakis",
            "Elchanan Mossel",
            "Sebastien Roch"
        ],
        "summary": "A major task of evolutionary biology is the reconstruction of phylogenetic trees from molecular data. The evolutionary model is given by a Markov chain on a tree. Given samples from the leaves of the Markov chain, the goal is to reconstruct the leaf-labelled tree.   It is well known that in order to reconstruct a tree on $n$ leaves, sample sequences of length $\\Omega(\\log n)$ are needed. It was conjectured by M. Steel that for the CFN/Ising evolutionary model, if the mutation probability on all edges of the tree is less than $p^{\\ast} = (\\sqrt{2}-1)/2^{3/2}$, then the tree can be recovered from sequences of length $O(\\log n)$. The value $p^{\\ast}$ is given by the transition point for the extremality of the free Gibbs measure for the Ising model on the binary tree. Steel's conjecture was proven by the second author in the special case where the tree is \"balanced.\" The second author also proved that if all edges have mutation probability larger than $p^{\\ast}$ then the length needed is $n^{\\Omega(1)}$. Here we show that Steel's conjecture holds true for general trees by giving a reconstruction algorithm that recovers the tree from $O(\\log n)$-length sequences when the mutation probabilities are discretized and less than $p^\\ast$. Our proof and results demonstrate that extremality of the free Gibbs measure on the infinite binary tree, which has been studied before in probability, statistical physics and computer science, determines how distinguishable are Gibbs measures on finite binary trees.",
        "published": "2005-09-23T20:22:09Z",
        "link": "http://arxiv.org/abs/math/0509575v3",
        "categories": [
            "math.PR",
            "cs.CE",
            "cs.DS",
            "math.CA",
            "math.CO",
            "math.ST",
            "q-bio.PE",
            "stat.TH"
        ]
    },
    {
        "title": "Brownian Functionals in Physics and Computer Science",
        "authors": [
            "Satya N. Majumdar"
        ],
        "summary": "This is a brief review on Brownian functionals in one dimension and their various applications, a contribution to the special issue ``The Legacy of Albert Einstein\" of Current Science. After a brief description of Einstein's original derivation of the diffusion equation, this article provides a pedagogical introduction to the path integral methods leading to the derivation of the celebrated Feynman-Kac formula. The usefulness of this technique in calculating the statistical properties of Brownian functionals is illustrated with several examples in physics and probability theory, with particular emphasis on applications in computer science. The statistical properties of \"first-passage Brownian functionals\" and their applications are also discussed.",
        "published": "2005-10-04T07:11:35Z",
        "link": "http://arxiv.org/abs/cond-mat/0510064v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.DS"
        ]
    },
    {
        "title": "Partial fillup and search time in LC tries",
        "authors": [
            "Svante Janson",
            "Wojciech Szpankowski"
        ],
        "summary": "Andersson and Nilsson introduced in 1993 a level-compressed trie (in short: LC trie) in which a full subtree of a node is compressed to a single node of degree being the size of the subtree. Recent experimental results indicated a 'dramatic improvement' when full subtrees are replaced by partially filled subtrees. In this paper, we provide a theoretical justification of these experimental results showing, among others, a rather moderate improvement of the search time over the original LC tries. For such an analysis, we assume that n strings are generated independently by a binary memoryless source with p denoting the probability of emitting a 1. We first prove that the so called alpha-fillup level (i.e., the largest level in a trie with alpha fraction of nodes present at this level) is concentrated on two values with high probability. We give these values explicitly up to O(1), and observe that the value of alpha (strictly between 0 and 1) does not affect the leading term.   This result directly yields the typical depth (search time) in the alpha-LC tries with p not equal to 1/2, which turns out to be C loglog n for an explicitly given constant C (depending on p but not on alpha). This should be compared with recently found typical depth in the original LC tries which is C' loglog n for a larger constant C'. The search time in alpha-LC tries is thus smaller but of the same order as in the original LC tries.",
        "published": "2005-10-06T10:04:16Z",
        "link": "http://arxiv.org/abs/cs/0510017v1",
        "categories": [
            "cs.DS",
            "math.PR",
            "E.1"
        ]
    },
    {
        "title": "Entropy based Nearest Neighbor Search in High Dimensions",
        "authors": [
            "Rina Panigrahy"
        ],
        "summary": "In this paper we study the problem of finding the approximate nearest neighbor of a query point in the high dimensional space, focusing on the Euclidean space. The earlier approaches use locality-preserving hash functions (that tend to map nearby points to the same value) to construct several hash tables to ensure that the query point hashes to the same bucket as its nearest neighbor in at least one table. Our approach is different -- we use one (or a few) hash table and hash several randomly chosen points in the neighborhood of the query point showing that at least one of them will hash to the bucket containing its nearest neighbor. We show that the number of randomly chosen points in the neighborhood of the query point $q$ required depends on the entropy of the hash value $h(p)$ of a random point $p$ at the same distance from $q$ at its nearest neighbor, given $q$ and the locality preserving hash function $h$ chosen randomly from the hash family. Precisely, we show that if the entropy $I(h(p)|q,h) = M$ and $g$ is a bound on the probability that two far-off points will hash to the same bucket, then we can find the approximate nearest neighbor in $O(n^\\rho)$ time and near linear $\\tilde O(n)$ space where $\\rho = M/\\log(1/g)$. Alternatively we can build a data structure of size $\\tilde O(n^{1/(1-\\rho)})$ to answer queries in $\\tilde O(d)$ time. By applying this analysis to the locality preserving hash functions in and adjusting the parameters we show that the $c$ nearest neighbor can be computed in time $\\tilde O(n^\\rho)$ and near linear space where $\\rho \\approx 2.06/c$ as $c$ becomes large.",
        "published": "2005-10-07T00:55:06Z",
        "link": "http://arxiv.org/abs/cs/0510019v2",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Phase Transition in the Aldous-Shields Model of Growing Trees",
        "authors": [
            "David S. Dean",
            "Satya N. Majumdar"
        ],
        "summary": "We study analytically the late time statistics of the number of particles in a growing tree model introduced by Aldous and Shields. In this model, a cluster grows in continuous time on a binary Cayley tree, starting from the root, by absorbing new particles at the empty perimeter sites at a rate proportional to c^{-l} where c is a positive parameter and l is the distance of the perimeter site from the root. For c=1, this model corresponds to random binary search trees and for c=2 it corresponds to digital search trees in computer science. By introducing a backward Fokker-Planck approach, we calculate the mean and the variance of the number of particles at large times and show that the variance undergoes a `phase transition' at a critical value c=sqrt{2}. While for c>sqrt{2} the variance is proportional to the mean and the distribution is normal, for c<sqrt{2} the variance is anomalously large and the distribution is non-Gaussian due to the appearance of extreme fluctuations. The model is generalized to one where growth occurs on a tree with $m$ branches and, in this more general case, we show that the critical point occurs at c=sqrt{m}.",
        "published": "2005-10-17T09:35:58Z",
        "link": "http://arxiv.org/abs/cond-mat/0510429v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.DS",
            "math.PR"
        ]
    },
    {
        "title": "Fast Monte-Carlo Low Rank Approximations for Matrices",
        "authors": [
            "Shmuel Friedland",
            "Mostafa Kaveh",
            "Amir Niknejad",
            "Hossein Zare"
        ],
        "summary": "In many applications, it is of interest to approximate data, given by mxn matrix A, by a matrix B of at most rank k, which is much smaller than m and n. The best approximation is given by singular value decomposition, which is too time consuming for very large m and n. We present here a Monte Carlo algorithm for iteratively computing a k-rank approximation to the data consisting of mxn matrix A. Each iteration involves the reading of O(k) of columns or rows of A. The complexity of our algorithm is O(kmn). Our algorithm, distinguished from other known algorithms, guarantees that each iteration is a better k-rank approximation than the previous iteration. We believe that this algorithm will have many applications in data mining, data storage and data analysis.",
        "published": "2005-10-26T18:41:46Z",
        "link": "http://arxiv.org/abs/math/0510573v1",
        "categories": [
            "math.NA",
            "cs.DS"
        ]
    },
    {
        "title": "Balanced Allocation on Graphs",
        "authors": [
            "K. Kenthapadi",
            "R. Panigrahy"
        ],
        "summary": "In this paper, we study the two choice balls and bins process when balls are not allowed to choose any two random bins, but only bins that are connected by an edge in an underlying graph. We show that for $n$ balls and $n$ bins, if the graph is almost regular with degree $n^\\epsilon$, where $\\epsilon$ is not too small, the previous bounds on the maximum load continue to hold. Precisely, the maximum load is $\\log \\log n + O(1/\\epsilon) + O(1)$. For general $\\Delta$-regular graphs, we show that the maximum load is $\\log\\log n + O(\\frac{\\log n}{\\log (\\Delta/\\log^4 n)}) + O(1)$ and also provide an almost matching lower bound of $\\log \\log n + \\frac{\\log n}{\\log (\\Delta \\log n)}$.   V{\\\"o}cking [Voc99] showed that the maximum bin size with $d$ choice load balancing can be further improved to $O(\\log\\log n /d)$ by breaking ties to the left. This requires $d$ random bin choices. We show that such bounds can be achieved by making only two random accesses and querying $d/2$ contiguous bins in each access. By grouping a sequence of $n$ bins into $2n/d$ groups, each of $d/2$ consecutive bins, if each ball chooses two groups at random and inserts the new ball into the least-loaded bin in the lesser loaded group, then the maximum load is $O(\\log\\log n/d)$ with high probability.",
        "published": "2005-10-27T21:59:21Z",
        "link": "http://arxiv.org/abs/cs/0510086v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Optimal Prefix Codes for Infinite Alphabets with Nonlinear Costs",
        "authors": [
            "Michael B. Baer"
        ],
        "summary": "Let $P = \\{p(i)\\}$ be a measure of strictly positive probabilities on the set of nonnegative integers. Although the countable number of inputs prevents usage of the Huffman algorithm, there are nontrivial $P$ for which known methods find a source code that is optimal in the sense of minimizing expected codeword length. For some applications, however, a source code should instead minimize one of a family of nonlinear objective functions, $\\beta$-exponential means, those of the form $\\log_a \\sum_i p(i) a^{n(i)}$, where $n(i)$ is the length of the $i$th codeword and $a$ is a positive constant. Applications of such minimizations include a novel problem of maximizing the chance of message receipt in single-shot communications ($a<1$) and a previously known problem of minimizing the chance of buffer overflow in a queueing system ($a>1$). This paper introduces methods for finding codes optimal for such exponential means. One method applies to geometric distributions, while another applies to distributions with lighter tails. The latter algorithm is applied to Poisson distributions and both are extended to alphabetic codes, as well as to minimizing maximum pointwise redundancy. The aforementioned application of minimizing the chance of buffer overflow is also considered.",
        "published": "2005-11-01T07:00:11Z",
        "link": "http://arxiv.org/abs/cs/0511003v3",
        "categories": [
            "cs.IT",
            "cs.DS",
            "math.IT",
            "E.4; H.1.1"
        ]
    },
    {
        "title": "Pbit and other list sorting algorithms",
        "authors": [
            "David S. Płaneta"
        ],
        "summary": "Pbit, besides its simplicity, is definitely the fastest list sorting algorithm. It considerably surpasses all already known methods. Among many advantages, it is stable, linear and be made to run in place. I will compare Pbit with algorithm described by Donald E. Knuth in the third volume of ''The Art of Computer Programming'' and other (QuickerSort, MergeSort) list sorting algorithms.",
        "published": "2005-11-04T01:52:02Z",
        "link": "http://arxiv.org/abs/cs/0511020v2",
        "categories": [
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "The Linear Arrangement Problem Parameterized Above Guaranteed Value",
        "authors": [
            "G. Gutin",
            "A. Rafiey",
            "S. Szeider",
            "A. Yeo"
        ],
        "summary": "A linear arrangement (LA) is an assignment of distinct integers to the vertices of a graph. The cost of an LA is the sum of lengths of the edges of the graph, where the length of an edge is defined as the absolute value of the difference of the integers assigned to its ends. For many application one hopes to find an LA with small cost. However, it is a classical NP-complete problem to decide whether a given graph $G$ admits an LA of cost bounded by a given integer. Since every edge of $G$ contributes at least one to the cost of any LA, the problem becomes trivially fixed-parameter tractable (FPT) if parameterized by the upper bound of the cost. Fernau asked whether the problem remains FPT if parameterized by the upper bound of the cost minus the number of edges of the given graph; thus whether the problem is FPT ``parameterized above guaranteed value.'' We answer this question positively by deriving an algorithm which decides in time $O(m+n+5.88^k)$ whether a given graph with $m$ edges and $n$ vertices admits an LA of cost at most $m+k$ (the algorithm computes such an LA if it exists). Our algorithm is based on a procedure which generates a problem kernel of linear size in linear time for a connected graph $G$. We also prove that more general parameterized LA problems stated by Serna and Thilikos are not FPT, unless P=NP.",
        "published": "2005-11-07T17:47:55Z",
        "link": "http://arxiv.org/abs/cs/0511030v3",
        "categories": [
            "cs.DS",
            "cs.CC",
            "F.2.2"
        ]
    },
    {
        "title": "Various Solutions to the Firing Squad Synchronization Problems",
        "authors": [
            "J. Gruska",
            "S. La Torre",
            "M. Napoli",
            "M. Parente"
        ],
        "summary": "We present different classes of solutions to the Firing Squad Synchronization Problem on networks of different shapes. The nodes are finite state processors that work at unison discrete steps. The networks considered are the line, the ring and the square. For all of these models we have considered one and two-way communication modes and also constrained the quantity of information that adjacent processors can exchange each step. We are given a particular time expressed as a function of the number of nodes of the network, $f(n)$ and present synchronization algorithms in time $n^2$, $n \\log n$, $n\\sqrt n$, $2^n$. The solutions are presented as {\\em signals} that are used as building blocks to compose new solutions for all times expressed by polynomials with nonnegative coefficients.",
        "published": "2005-11-12T06:44:20Z",
        "link": "http://arxiv.org/abs/cs/0511044v1",
        "categories": [
            "cs.DS",
            "cs.CC"
        ]
    },
    {
        "title": "A polynomial-time heuristic for Circuit-SAT",
        "authors": [
            "Francesco Capasso"
        ],
        "summary": "In this paper is presented an heuristic that, in polynomial time and space in the input dimension, determines if a circuit describes a tautology or a contradiction. If the circuit is neither a tautology nor a contradiction, then the heuristic finds an assignment to the circuit inputs such that the circuit is satisfied.",
        "published": "2005-11-18T20:23:46Z",
        "link": "http://arxiv.org/abs/cs/0511071v4",
        "categories": [
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Approximating Clustering of Fingerprint Vectors with Missing Values",
        "authors": [
            "Paola Bonizzoni",
            "Gianluca Della Vedova",
            "Riccardo Dondi"
        ],
        "summary": "The problem of clustering fingerprint vectors is an interesting problem in Computational Biology that has been proposed in (Figureroa et al. 2004). In this paper we show some improvements in closing the gaps between the known lower bounds and upper bounds on the approximability of some variants of the biological problem. Namely we are able to prove that the problem is APX-hard even when each fingerprint contains only two unknown position. Moreover we have studied some variants of the orginal problem, and we give two 2-approximation algorithm for the IECMV and OECMV problems when the number of unknown entries for each vector is at most a constant.",
        "published": "2005-11-23T10:32:47Z",
        "link": "http://arxiv.org/abs/cs/0511082v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Ramsey partitions and proximity data structures",
        "authors": [
            "Manor Mendel",
            "Assaf Naor"
        ],
        "summary": "This paper addresses two problems lying at the intersection of geometric analysis and theoretical computer science: The non-linear isomorphic Dvoretzky theorem and the design of good approximate distance oracles for large distortion. We introduce the notion of Ramsey partitions of a finite metric space, and show that the existence of good Ramsey partitions implies a solution to the metric Ramsey problem for large distortion (a.k.a. the non-linear version of the isomorphic Dvoretzky theorem, as introduced by Bourgain, Figiel, and Milman). We then proceed to construct optimal Ramsey partitions, and use them to show that for every e\\in (0,1), any n-point metric space has a subset of size n^{1-e} which embeds into Hilbert space with distortion O(1/e). This result is best possible and improves part of the metric Ramsey theorem of Bartal, Linial, Mendel and Naor, in addition to considerably simplifying its proof. We use our new Ramsey partitions to design the best known approximate distance oracles when the distortion is large, closing a gap left open by Thorup and Zwick. Namely, we show that for any $n$ point metric space X, and k>1, there exists an O(k)-approximate distance oracle whose storage requirement is O(n^{1+1/k}), and whose query time is a universal constant. We also discuss applications of Ramsey partitions to various other geometric data structure problems, such as the design of efficient data structures for approximate ranking.",
        "published": "2005-11-23T20:06:15Z",
        "link": "http://arxiv.org/abs/cs/0511084v3",
        "categories": [
            "cs.DS",
            "cs.CG",
            "math.FA",
            "math.MG"
        ]
    },
    {
        "title": "Parameter Estimation of Hidden Diffusion Processes: Particle Filter vs.   Modified Baum-Welch Algorithm",
        "authors": [
            "A. Benabdallah",
            "G. Radons"
        ],
        "summary": "We propose a new method for the estimation of parameters of hidden diffusion processes. Based on parametrization of the transition matrix, the Baum-Welch algorithm is improved. The algorithm is compared to the particle filter in application to the noisy periodic systems. It is shown that the modified Baum-Welch algorithm is capable of estimating the system parameters with better accuracy than particle filters.",
        "published": "2005-11-30T20:23:19Z",
        "link": "http://arxiv.org/abs/cs/0511108v1",
        "categories": [
            "cs.DS",
            "cs.LG",
            "F.2.1; J.2"
        ]
    },
    {
        "title": "A linear-time algorithm for finding the longest segment which scores   above a given threshold",
        "authors": [
            "Miklós Csűrös"
        ],
        "summary": "This paper describes a linear-time algorithm that finds the longest stretch in a sequence of real numbers (``scores'') in which the sum exceeds an input parameter. The algorithm also solves the problem of finding the longest interval in which the average of the scores is above a fixed threshold. The problem originates from molecular sequence analysis: for instance, the algorithm can be employed to identify long GC-rich regions in DNA sequences. The algorithm can also be used to trim low-quality ends of shotgun sequences in a preprocessing step of whole-genome assembly.",
        "published": "2005-12-04T04:28:00Z",
        "link": "http://arxiv.org/abs/cs/0512016v2",
        "categories": [
            "cs.DS",
            "cs.CE",
            "F.2.2; G.2; J.3"
        ]
    },
    {
        "title": "The Poster Session of SSS 2005",
        "authors": [
            "Brahim Hamid",
            "Ted Herman",
            "Morten Mjelde"
        ],
        "summary": "This technical report documents the poster session of SSS 2005, the Symposium on Self-Stabilizing Systems published by Springer as LNCS volume 3764. The poster session included five presentations. Two of these presentations are summarized in brief abstracts contained in this technical report.",
        "published": "2005-12-05T22:51:11Z",
        "link": "http://arxiv.org/abs/cs/0512021v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.4.5"
        ]
    },
    {
        "title": "A polynomial algorithm for the k-cluster problem on interval graphs",
        "authors": [
            "George B. Mertzios"
        ],
        "summary": "This paper deals with the problem of finding, for a given graph and a given natural number k, a subgraph of k nodes with a maximum number of edges. This problem is known as the k-cluster problem and it is NP-hard on general graphs as well as on chordal graphs. In this paper, it is shown that the k-cluster problem is solvable in polynomial time on interval graphs. In particular, we present two polynomial time algorithms for the class of proper interval graphs and the class of general interval graphs, respectively. Both algorithms are based on a matrix representation for interval graphs. In contrast to representations used in most of the previous work, this matrix representation does not make use of the maximal cliques in the investigated graph.",
        "published": "2005-12-11T23:13:44Z",
        "link": "http://arxiv.org/abs/cs/0512046v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Irreducible Frequent Patterns in Transactional Databases",
        "authors": [
            "Gennady P. Berman",
            "Vyacheslav N. Gorshkov",
            "Xidi Wang"
        ],
        "summary": "Irreducible frequent patters (IFPs) are introduced for transactional databases. An IFP is such a frequent pattern (FP),(x1,x2,...xn), the probability of which, P(x1,x2,...xn), cannot be represented as a product of the probabilities of two (or more) other FPs of the smaller lengths. We have developed an algorithm for searching IFPs in transactional databases. We argue that IFPs represent useful tools for characterizing the transactional databases and may have important applications to bio-systems including the immune systems and for improving vaccination strategies. The effectiveness of the IFPs approach has been illustrated in application to a classification problem.",
        "published": "2005-12-13T22:53:17Z",
        "link": "http://arxiv.org/abs/cs/0512054v1",
        "categories": [
            "cs.DS",
            "cs.DB"
        ]
    },
    {
        "title": "High-Throughput SNP Genotyping by SBE/SBH",
        "authors": [
            "Ion I. Mandoiu",
            "Claudia Prajescu"
        ],
        "summary": "Despite much progress over the past decade, current Single Nucleotide Polymorphism (SNP) genotyping technologies still offer an insufficient degree of multiplexing when required to handle user-selected sets of SNPs. In this paper we propose a new genotyping assay architecture combining multiplexed solution-phase single-base extension (SBE) reactions with sequencing by hybridization (SBH) using universal DNA arrays such as all $k$-mer arrays. In addition to PCR amplification of genomic DNA, SNP genotyping using SBE/SBH assays involves the following steps: (1) Synthesizing primers complementing the genomic sequence immediately preceding SNPs of interest; (2) Hybridizing these primers with the genomic DNA; (3) Extending each primer by a single base using polymerase enzyme and dideoxynucleotides labeled with 4 different fluorescent dyes; and finally (4) Hybridizing extended primers to a universal DNA array and determining the identity of the bases that extend each primer by hybridization pattern analysis. Our contributions include a study of multiplexing algorithms for SBE/SBH genotyping assays and preliminary experimental results showing the achievable tradeoffs between the number of array probes and primer length on one hand and the number of SNPs that can be assayed simultaneously on the other. Simulation results on datasets both randomly generated and extracted from the NCBI dbSNP database suggest that the SBE/SBH architecture provides a flexible and cost-effective alternative to genotyping assays currently used in the industry, enabling genotyping of up to hundreds of thousands of user-specified SNPs per assay.",
        "published": "2005-12-14T18:01:51Z",
        "link": "http://arxiv.org/abs/cs/0512052v1",
        "categories": [
            "cs.DS",
            "q-bio.GN",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Distributed Navigation Algorithms for Sensor Networks",
        "authors": [
            "Chiranjeeb Buragohain",
            "Divyakant Agrawal",
            "Subhash Suri"
        ],
        "summary": "We propose efficient distributed algorithms to aid navigation of a user through a geographic area covered by sensors. The sensors sense the level of danger at their locations and we use this information to find a safe path for the user through the sensor field. Traditional distributed navigation algorithms rely upon flooding the whole network with packets to find an optimal safe path. To reduce the communication expense, we introduce the concept of a skeleton graph which is a sparse subset of the true sensor network communication graph. Using skeleton graphs we show that it is possible to find approximate safe paths with much lower communication cost. We give tight theoretical guarantees on the quality of our approximation and by simulation, show the effectiveness of our algorithms in realistic sensor network situations.",
        "published": "2005-12-14T22:36:53Z",
        "link": "http://arxiv.org/abs/cs/0512060v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Matching Subsequences in Trees",
        "authors": [
            "Philip Bille",
            "Inge Li Goertz"
        ],
        "summary": "Given two rooted, labeled trees $P$ and $T$ the tree path subsequence problem is to determine which paths in $P$ are subsequences of which paths in $T$. Here a path begins at the root and ends at a leaf. In this paper we propose this problem as a useful query primitive for XML data, and provide new algorithms improving the previously best known time and space bounds.",
        "published": "2005-12-15T10:28:04Z",
        "link": "http://arxiv.org/abs/cs/0512061v3",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "EqRank: Theme Evolution in Citation Graphs",
        "authors": [
            "G. B. Pivovarov",
            "S. E. Trunov"
        ],
        "summary": "Time evolution of the classification scheme generated by the EqRank algorithm is studied with hep-th citation graph as an example. Intuitive expectations about evolution of an adequate classification scheme for a growing set of objects are formulated. Evolution compliant with these expectations is called natural. It is demonstrated that EqRank yields a naturally evolving classification scheme. We conclude that EqRank can be used as a means to detect new scientific themes, and to track their development.",
        "published": "2005-12-20T14:01:45Z",
        "link": "http://arxiv.org/abs/cs/0512080v1",
        "categories": [
            "cs.DS",
            "cs.DL"
        ]
    },
    {
        "title": "De Dictionariis Dynamicis Pauco Spatio Utentibus",
        "authors": [
            "Erik D. Demaine",
            "Friedhelm Meyer auf der Heide",
            "Rasmus Pagh",
            "Mihai Patrascu"
        ],
        "summary": "We develop dynamic dictionaries on the word RAM that use asymptotically optimal space, up to constant factors, subject to insertions and deletions, and subject to supporting perfect-hashing queries and/or membership queries, each operation in constant time with high probability. When supporting only membership queries, we attain the optimal space bound of Theta(n lg(u/n)) bits, where n and u are the sizes of the dictionary and the universe, respectively. Previous dictionaries either did not achieve this space bound or had time bounds that were only expected and amortized. When supporting perfect-hashing queries, the optimal space bound depends on the range {1,2,...,n+t} of hashcodes allowed as output. We prove that the optimal space bound is Theta(n lglg(u/n) + n lg(n/(t+1))) bits when supporting only perfect-hashing queries, and it is Theta(n lg(u/n) + n lg(n/(t+1))) bits when also supporting membership queries. All upper bounds are new, as is the Omega(n lg(n/(t+1))) lower bound.",
        "published": "2005-12-20T23:01:41Z",
        "link": "http://arxiv.org/abs/cs/0512081v1",
        "categories": [
            "cs.DS"
        ]
    },
    {
        "title": "Data Structures for Halfplane Proximity Queries and Incremental Voronoi   Diagrams",
        "authors": [
            "Boris Aronov",
            "Prosenjit Bose",
            "Erik D. Demaine",
            "Joachim Gudmundsson",
            "John Iacono",
            "Stefan Langerman",
            "Michiel Smid"
        ],
        "summary": "We consider preprocessing a set $S$ of $n$ points in convex position in the plane into a data structure supporting queries of the following form: given a point $q$ and a directed line $\\ell$ in the plane, report the point of $S$ that is farthest from (or, alternatively, nearest to) the point $q$ among all points to the left of line $\\ell$. We present two data structures for this problem. The first data structure uses $O(n^{1+\\varepsilon})$ space and preprocessing time, and answers queries in $O(2^{1/\\varepsilon} \\log n)$ time, for any $0 < \\varepsilon < 1$. The second data structure uses $O(n \\log^3 n)$ space and polynomial preprocessing time, and answers queries in $O(\\log n)$ time. These are the first solutions to the problem with $O(\\log n)$ query time and $o(n^2)$ space.   The second data structure uses a new representation of nearest- and farthest-point Voronoi diagrams of points in convex position. This representation supports the insertion of new points in clockwise order using only $O(\\log n)$ amortized pointer changes, in addition to $O(\\log n)$-time point-location queries, even though every such update may make $\\Theta(n)$ combinatorial changes to the Voronoi diagram. This data structure is the first demonstration that deterministically and incrementally constructed Voronoi diagrams can be maintained in $o(n)$ amortized pointer changes per operation while keeping $O(\\log n)$-time point-location queries.",
        "published": "2005-12-23T04:28:12Z",
        "link": "http://arxiv.org/abs/cs/0512091v3",
        "categories": [
            "cs.CG",
            "cs.DS"
        ]
    },
    {
        "title": "Collaborative tagging as a tripartite network",
        "authors": [
            "R. Lambiotte",
            "M. Ausloos"
        ],
        "summary": "We describe online collaborative communities by tripartite networks, the nodes being persons, items and tags. We introduce projection methods in order to uncover the structures of the networks, i.e. communities of users, genre families...   To do so, we focus on the correlations between the nodes, depending on their profiles, and use percolation techniques that consist in removing less correlated links and observing the shaping of disconnected islands. The structuring of the network is visualised by using a tree representation. The notion of diversity in the system is also discussed.",
        "published": "2005-12-23T13:38:57Z",
        "link": "http://arxiv.org/abs/cs/0512090v2",
        "categories": [
            "cs.DS",
            "cs.DL"
        ]
    },
    {
        "title": "Fedora: An Architecture for Complex Objects and their Relationships",
        "authors": [
            "Carl Lagoze",
            "Sandy Payette",
            "Edwin Shin",
            "Chris Wilper"
        ],
        "summary": "The Fedora architecture is an extensible framework for the storage, management, and dissemination of complex objects and the relationships among them. Fedora accommodates the aggregation of local and distributed content into digital objects and the association of services with objects. This al-lows an object to have several accessible representations, some of them dy-namically produced. The architecture includes a generic RDF-based relation-ship model that represents relationships among objects and their components. Queries against these relationships are supported by an RDF triple store. The architecture is implemented as a web service, with all aspects of the complex object architecture and related management functions exposed through REST and SOAP interfaces. The implementation is available as open-source soft-ware, providing the foundation for a variety of end-user applications for digital libraries, archives, institutional repositories, and learning object systems.",
        "published": "2005-01-07T17:57:05Z",
        "link": "http://arxiv.org/abs/cs/0501012v6",
        "categories": [
            "cs.DL",
            "cs.MM",
            "H.3.7"
        ]
    },
    {
        "title": "Clustering SPIRES with EqRank",
        "authors": [
            "G. B. Pivovarov",
            "S. E. Trunov"
        ],
        "summary": "SPIRES is the largest database of scientific papers in the subject field of high energy and nuclear physics. It contains information on the citation graph of more than half a million of papers (vertexes of the citation graph). We outline the EqRank algorithm designed to cluster vertexes of directed graphs, and present the results of EqRank application to the SPIRES citation graph. The hierarchical clustering of SPIRES yielded by EqRank is used to set up a web service, which is also outlined.",
        "published": "2005-01-11T05:00:03Z",
        "link": "http://arxiv.org/abs/cs/0501019v1",
        "categories": [
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "An Information Network Overlay Architecture for the NSDL",
        "authors": [
            "Carl Lagoze",
            "Dean B. Krafft",
            "Susan Jesuroga",
            "Tim Cornwell",
            "Ellen J. Cramer",
            "Eddie Shin"
        ],
        "summary": "We describe the underlying data model and implementation of a new architecture for the National Science Digital Library (NSDL) by the Core Integration Team (CI). The architecture is based on the notion of an information network overlay. This network, implemented as a graph of digital objects in a Fedora repository, allows the representation of multiple information entities and their relationships. The architecture provides the framework for contextualization and reuse of resources, which we argue is essential for the utility of the NSDL as a tool for teaching and learning.",
        "published": "2005-01-27T19:49:11Z",
        "link": "http://arxiv.org/abs/cs/0501080v2",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Orchestrating Metadata Enhancement Services: Introducing Lenny",
        "authors": [
            "Jon Phipps",
            "Diane I. Hillmann",
            "Gordon Paynter"
        ],
        "summary": "Harvested metadata often suffers from uneven quality to the point that utility is compromised. Although some aggregators have developed methods for evaluating and repairing specific metadata problems, it has been unclear how these methods might be scaled into services that can be used within an automated production environment. The National Science Digital Library (NSDL), as part of its work with INFOMINE, has developed a model of ser-vice interaction that enables loosely-coupled third party services to provide metadata enhancements to a central repository, with interactions orchestrated by a centralized software application.",
        "published": "2005-01-28T18:44:23Z",
        "link": "http://arxiv.org/abs/cs/0501083v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "aDORe: a modular, standards-based Digital Object Repository",
        "authors": [
            "Herbert Van de Sompel",
            "Jeroen Bekaert",
            "Xiaoming Liu",
            "Luda Balakireva",
            "Thorsten Schwander"
        ],
        "summary": "This paper describes the aDORe repository architecture, designed and implemented for ingesting, storing, and accessing a vast collection of Digital Objects at the Research Library of the Los Alamos National Laboratory. The aDORe architecture is highly modular and standards-based. In the architecture, the MPEG-21 Digital Item Declaration Language is used as the XML-based format to represent Digital Objects that can consist of multiple datastreams as Open Archival Information System Archival Information Packages (OAIS AIPs).Through an ingestion process, these OAIS AIPs are stored in a multitude of autonomous repositories. A Repository Index keeps track of the creation and location of all the autonomous repositories, whereas an Identifier Locator registers in which autonomous repository a given Digital Object or OAIS AIP resides. A front-end to the complete environment, the OAI-PMH Federator, is introduced for requesting OAIS Dissemination Information Packages (OAIS DIPs). These OAIS DIPs can be the stored OAIS AIPs themselves, or transformations thereof. This front-end allows OAI-PMH harvesters to recurrently and selectively collect batches of OAIS DIPs from aDORe, and hence to create multiple, parallel services using the collected objects. Another front-end, the OpenURL Resolver, is introduced for requesting OAIS Result Sets. An OAIS Result Set is a dissemination of an individual Digital Object or of its constituent datastreams. Both front-ends make use of an MPEG-21 Digital Item Processing Engine to apply services to OAIS AIPs, Digital Objects, or constituent datastreams that were specified in a dissemination request.",
        "published": "2005-02-04T22:09:14Z",
        "link": "http://arxiv.org/abs/cs/0502028v1",
        "categories": [
            "cs.DL",
            "H 3.7"
        ]
    },
    {
        "title": "Co-Authorship Networks in the Digital Library Research Community",
        "authors": [
            "Xiaoming Liu",
            "Johan Bollen",
            "Michael L. Nelson",
            "Herbert Van de Sompel"
        ],
        "summary": "The field of digital libraries (DLs) coalesced in 1994: the first digital library conferences were held that year, awareness of the World Wide Web was accelerating, and the National Science Foundation awarded $24 Million (U.S.) for the Digital Library Initiative (DLI). In this paper we examine the state of the DL domain after a decade of activity by applying social network analysis to the co-authorship network of the past ACM, IEEE, and joint ACM/IEEE digital library conferences. We base our analysis on a common binary undirectional network model to represent the co-authorship network, and from it we extract several established network measures. We also introduce a weighted directional network model to represent the co-authorship network, for which we define $AuthorRank$ as an indicator of the impact of an individual author in the network. The results are validated against conference program committee members in the same period. The results show clear advantages of PageRank and AuthorRank over degree, closeness and betweenness centrality metrics. We also investigate the amount and nature of international participation in Joint Conference on Digital Libraries (JCDL).",
        "published": "2005-02-12T00:08:57Z",
        "link": "http://arxiv.org/abs/cs/0502056v2",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Toward alternative metrics of journal impact: A comparison of download   and citation data",
        "authors": [
            "Johan Bollen",
            "Herbert Van de Sompel",
            "Joan Smith",
            "Rick Luce"
        ],
        "summary": "We generated networks of journal relationships from citation and download data, and determined journal impact rankings from these networks using a set of social network centrality metrics. The resulting journal impact rankings were compared to the ISI IF. Results indicate that, although social network metrics and ISI IF rankings deviate moderately for citation-based journal networks, they differ considerably for journal networks derived from download data. We believe the results represent a unique aspect of general journal impact that is not captured by the ISI IF. These results furthermore raise questions regarding the validity of the ISI IF as the sole assessment of journal impact, and suggest the possibility of devising impact metrics based on usage information in general.",
        "published": "2005-03-03T05:14:47Z",
        "link": "http://arxiv.org/abs/cs/0503007v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "File-based storage of Digital Objects and constituent datastreams:   XMLtapes and Internet Archive ARC files",
        "authors": [
            "Xiaoming Liu",
            "Lyudmila Balakireva",
            "Patrick Hochstenbach",
            "Herbert Van de Sompel"
        ],
        "summary": "This paper introduces the write-once/read-many XMLtape/ARC storage approach for Digital Objects and their constituent datastreams. The approach combines two interconnected file-based storage mechanisms that are made accessible in a protocol-based manner. First, XML-based representations of multiple Digital Objects are concatenated into a single file named an XMLtape. An XMLtape is a valid XML file; its format definition is independent of the choice of the XML-based complex object format by which Digital Objects are represented. The creation of indexes for both the identifier and the creation datetime of the XML-based representation of the Digital Objects facilitates OAI-PMH-based access to Digital Objects stored in an XMLtape. Second, ARC files, as introduced by the Internet Archive, are used to contain the constituent datastreams of the Digital Objects in a concatenated manner. An index for the identifier of the datastream facilitates OpenURL-based access to an ARC file. The interconnection between XMLtapes and ARC files is provided by conveying the identifiers of ARC files associated with an XMLtape as administrative information in the XMLtape, and by including OpenURL references to constituent datastreams of a Digital Object in the XML-based representation of that Digital Object.",
        "published": "2005-03-07T23:17:23Z",
        "link": "http://arxiv.org/abs/cs/0503016v2",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "The Effect of Use and Access on Citations",
        "authors": [
            "Michael J. Kurtz",
            "Guenther Eichhorn",
            "Alberto Accomazzi",
            "Carolyn Grant",
            "Markus Demleitner",
            "Edwin Henneken",
            "Stephen S. Murray"
        ],
        "summary": "It has been shown (S. Lawrence, 2001, Nature, 411, 521) that journal articles which have been posted without charge on the internet are more heavily cited than those which have not been. Using data from the NASA Astrophysics Data System (ads.harvard.edu) and from the ArXiv e-print archive at Cornell University (arXiv.org) we examine the causes of this effect.",
        "published": "2005-03-14T16:44:37Z",
        "link": "http://arxiv.org/abs/cs/0503029v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "mod_oai: An Apache Module for Metadata Harvesting",
        "authors": [
            "Michael L. Nelson",
            "Herbert Van de Sompel",
            "Xiaoming Liu",
            "Terry L. Harrison",
            "Nathan McFarland"
        ],
        "summary": "We describe mod_oai, an Apache 2.0 module that implements the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH). OAIPMH is the de facto standard for metadata exchange in digital libraries and allows repositories to expose their contents in a structured, application-neutral format with semantics optimized for accurate incremental harvesting. Current implementations of OAI-PMH are either separate applications that access an existing repository, or are built-in to repository software packages. mod_oai is different in that it optimizes harvesting web content by building OAI-PMH capability into the Apache server. We discuss the implications of adding harvesting capability to an Apache server and describe our initial experimental results accessing a departmental web site using both web crawling and OAIPMH harvesting techniques.",
        "published": "2005-03-24T14:38:26Z",
        "link": "http://arxiv.org/abs/cs/0503069v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Scientific impact quantity and quality: Analysis of two sources of   bibliographic data",
        "authors": [
            "Richard K. Belew"
        ],
        "summary": "Attempts to understand the consequence of any individual scientist's activity within the long-term trajectory of science is one of the most difficult questions within the philosophy of science. Because scientific publications play such as central role in the modern enterprise of science, bibliometric techniques which measure the ``impact'' of an individual publication as a function of the number of citations it receives from subsequent authors have provided some of the most useful empirical data on this question. Until recently, Thompson/ISI has provided the only source of large-scale ``inverted'' bibliographic data of the sort required for impact analysis. In the end of 2004, Google introduced a new service, GoogleScholar, making much of this same data available. Here we analyze 203 publications, collectively cited by more than 4000 other publications. We show surprisingly good agreement between data citation counts provided by the two services. Data quality across the systems is analyzed, and potentially useful complementarities between are considered. The additional robustness offered by multiple sources of such data promises to increase the utility of these measurements as open citation protocols and open access increase their impact on electronic scientific publication practices.",
        "published": "2005-04-11T13:52:55Z",
        "link": "http://arxiv.org/abs/cs/0504036v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.3; H.3.7; H.5.4"
        ]
    },
    {
        "title": "The Convergence of Digital-Libraries and the Peer-Review Process",
        "authors": [
            "Marko A. Rodriguez",
            "Johan Bollen",
            "Herbert Van de Sompel"
        ],
        "summary": "Pre-print repositories have seen a significant increase in use over the past fifteen years across multiple research domains. Researchers are beginning to develop applications capable of using these repositories to assist the scientific community above and beyond the pure dissemination of information. The contribution set forth by this paper emphasizes a deconstructed publication model in which the peer-review process is mediated by an OAI-PMH peer-review service. This peer-review service uses a social-network algorithm to determine potential reviewers for a submitted manuscript and for weighting the relative influence of each participating reviewer's evaluations. This paper also suggests a set of peer-review specific metadata tags that can accompany a pre-print's existing metadata record. The combinations of these contributions provide a unique repository-centric peer-review model that fits within the widely deployed OAI-PMH framework.",
        "published": "2005-04-18T14:28:12Z",
        "link": "http://arxiv.org/abs/cs/0504084v3",
        "categories": [
            "cs.DL",
            "cs.CY",
            "H.3.7"
        ]
    },
    {
        "title": "The OAI Data-Provider Registration and Validation Service",
        "authors": [
            "Simeon Warner"
        ],
        "summary": "I present a summary of recent use of the Open Archives Initiative (OAI) registration and validation services for data-providers. The registration service has seen a steady stream of registrations since its launch in 2002, and there are now over 220 registered repositories. I examine the validation logs to produce a breakdown of reasons why repositories fail validation. This breakdown highlights some common problems and will be used to guide work to improve the validation service.",
        "published": "2005-06-03T17:15:04Z",
        "link": "http://arxiv.org/abs/cs/0506010v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "The Hyper-Cortex of Human Collective-Intelligence Systems",
        "authors": [
            "Marko A. Rodriguez"
        ],
        "summary": "Individual-intelligence research, from a neurological perspective, discusses the hierarchical layers of the cortex as a structure that performs conceptual abstraction and specification. This theory has been used to explain how motor-cortex regions responsible for different behavioral modalities such as writing and speaking can be utilized to express the same general concept represented higher in the cortical hierarchy. For example, the concept of a dog, represented across a region of high-level cortical-neurons, can either be written or spoken about depending on the individual's context. The higher-layer cortical areas project down the hierarchy, sending abstract information to specific regions of the motor-cortex for contextual implementation. In this paper, this idea is expanded to incorporate collective-intelligence within a hyper-cortical construct. This hyper-cortex is a multi-layered network used to represent abstract collective concepts. These ideas play an important role in understanding how collective-intelligence systems can be engineered to handle problem abstraction and solution specification. Finally, a collection of common problems in the scientific community are solved using an artificial hyper-cortex generated from digital-library metadata.",
        "published": "2005-06-08T21:33:44Z",
        "link": "http://arxiv.org/abs/cs/0506024v3",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.DL",
            "cs.NE"
        ]
    },
    {
        "title": "Dictionaries merger for text expansion in question answering",
        "authors": [
            "Bernard Jacquemin"
        ],
        "summary": "This paper presents an original way to add new data in a reference dictionary from several other lexical resources, without loosing any consistence. This operation is carried in order to get lexical information classified by the sense of the entry. This classification makes it possible to enrich utterances (in QA: the queries) following the meaning, and to reduce noise. An analysis of the experienced problems shows the interest of this method, and insists on the points that have to be tackled.",
        "published": "2005-06-12T16:36:47Z",
        "link": "http://arxiv.org/abs/cs/0506046v1",
        "categories": [
            "cs.DL",
            "H.3; H.5"
        ]
    },
    {
        "title": "Exploitation de dictionnaires électroniques pour la   désambiguïsation sémantique lexicale",
        "authors": [
            "Caroline Brun",
            "Bernard Jacquemin",
            "Frédérique Segond"
        ],
        "summary": "This paper presents a lexical disambiguation system, initially developed for English and now adapted to French. This system associates a word with its meaning in a given context using electronic dictionaries as semantically annotated corpora in order to extract semantic disambiguation rules. We describe the rule extraction and application process as well as the evaluation of the system. The results for French give us insight information on some possible improvments of the nature and content of lexical resources adapted for disambiguation in this framework.",
        "published": "2005-06-12T16:48:33Z",
        "link": "http://arxiv.org/abs/cs/0506049v1",
        "categories": [
            "cs.DL",
            "H.3; H.4; H.5"
        ]
    },
    {
        "title": "Adapting CBPP platforms for instructional use",
        "authors": [
            "Robert Milson",
            "Aaron Krowne"
        ],
        "summary": "Commons based peer-production (CBPP) is the de-centralized, net-based approach to the creation and dissemination of information resources. Underlying every CBPP system is a virtual community brought together by an internet tool (such as a web site) and structured by a specific collaboration protocol. In this talk we will argue that the value of such platforms can be leveraged by adapting them for pedagogical purposes.   We report on one such recent adaptation. The Noosphere system is a web-based collaboration environment that underlies the popular Planetmath website, a collaboratively written encyclopedia of mathematics licensed under the GNU Free Documentation License (FDL). Recently, the system was used to host a graduate-level mathematics course at Dalhousie University, in Halifax, Canada. The course consisted of regular lectures and assignment problems. The students in the course collaborated on a set of course notes, encapsulating the lecture content and giving solutions of assigned problems. The successful outcome of this experiment demonstrated that a dedicated Noosphere system is well suited for classroom applications. We argue that this ``proof of concept'' experience also strongly suggests that every successful CBPP platform possesses latent pedagogical value.",
        "published": "2005-07-10T19:05:26Z",
        "link": "http://arxiv.org/abs/cs/0507028v1",
        "categories": [
            "cs.DL",
            "cs.HC",
            "H.3.7; J.2"
        ]
    },
    {
        "title": "Representing Digital Assets using MPEG-21 Digital Item Declaration",
        "authors": [
            "Jeroen Bekaert",
            "Herbert Van de Sompel"
        ],
        "summary": "Various XML-based approaches aimed at representing compound digital assets have emerged over the last several years. Approaches that are of specific relevance to the digital library community include the Metadata Encoding and Transmission Standard (METS), the IMS Content Packaging XML Binding, and the XML Formatted Data Units (XFDU) developed by CCSDS Panel 2. The MPEG-21 Digital Item Declaration (MPEG-21 DID) is another standard specifying the representation of digital assets in XML that, so far, has received little attention in the digital library community. This article gives a brief insight into the MPEG-21 standardization effort, highlights the major characteristics of the MPEG-21 DID Abstract Model, and describes the MPEG-21 Digital Item Declaration Language (MPEG-21 DIDL), an XML syntax for the representation of digital assets based on the MPEG-21 DID Abstract Model. Also, it briefly demonstrates the potential relevance of MPEG-21 DID to the digital library community by describing its use in the aDORe repository environment at the Research Library of the Los Alamos National Laboratory (LANL) for the representation of digital assets.",
        "published": "2005-08-13T03:09:25Z",
        "link": "http://arxiv.org/abs/cs/0508065v1",
        "categories": [
            "cs.DL",
            "cs.AR"
        ]
    },
    {
        "title": "Can Small Museums Develop Compelling, Educational and Accessible Web   Resources? The Case of Accademia Carrara",
        "authors": [
            "Silvia Filippini-Fantoni",
            "Jonathan P. Bowen"
        ],
        "summary": "Due to the lack of budget, competence, personnel and time, small museums are often unable to develop compelling, educational and accessible web resources for their permanent collections or temporary exhibitions. In an attempt to prove that investing in these types of resources can be very fruitful even for small institutions, we will illustrate the case of Accademia Carrara, a museum in Bergamo, northern Italy, which, for a current temporary exhibition on Cezanne and Renoir's masterpieces from the Paul Guillaume collection, developed a series of multimedia applications, including an accessible website, rich in content and educational material [www.cezannerenoir.it].",
        "published": "2005-08-13T14:46:16Z",
        "link": "http://arxiv.org/abs/cs/0508066v1",
        "categories": [
            "cs.MM",
            "cs.CY",
            "cs.DL",
            "cs.IR",
            "H.3.5; H.3.7; H.4.3; H.5.1; H.5.2; H.5.3; H.5.4; K.3.1; K.4.0"
        ]
    },
    {
        "title": "Copyright and Promotion: Oxymoron or Opportunity?",
        "authors": [
            "Teresa Numerico",
            "Jonathan P. Bowen"
        ],
        "summary": "Copyright in the cultural sphere can act as a barrier to the dissemination of high-quality information. On the other hand it protects works of art that might not be made available otherwise. This dichotomy makes the area of copyright difficult, especially when it applies to the digital arena of the web where copying is so easy and natural. Here we present a snapshot of the issues for online copyright, with particular emphasis on the relevance to cultural institutions. We concentrate on Europe and the US; as an example we include a special section dedicated to the situation in Italy.",
        "published": "2005-08-13T14:58:07Z",
        "link": "http://arxiv.org/abs/cs/0508067v1",
        "categories": [
            "cs.CY",
            "cs.DL",
            "H.3.5; H.3.7; K.3.1; K.4.1; K.4.2; K.5.1; K.5.2"
        ]
    },
    {
        "title": "The Structure of Collaborative Tagging Systems",
        "authors": [
            "Scott Golder",
            "Bernardo A. Huberman"
        ],
        "summary": "Collaborative tagging describes the process by which many users add metadata in the form of keywords to shared content. Recently, collaborative tagging has grown in popularity on the web, on sites that allow users to tag bookmarks, photographs and other content. In this paper we analyze the structure of collaborative tagging systems as well as their dynamical aspects. Specifically, we discovered regularities in user activity, tag frequencies, kinds of tags used, bursts of popularity in bookmarking and a remarkable stability in the relative proportions of tags within a given url. We also present a dynamical model of collaborative tagging that predicts these stable patterns and relates them to imitation and shared knowledge.",
        "published": "2005-08-18T18:34:19Z",
        "link": "http://arxiv.org/abs/cs/0508082v1",
        "categories": [
            "cs.DL",
            "cs.CY"
        ]
    },
    {
        "title": "A Fresh Look at the Reliability of Long-term Digital Storage",
        "authors": [
            "Mary Baker",
            "Mehul Shah",
            "David S. H. Rosenthal",
            "Mema Roussopoulos",
            "Petros Maniatis",
            "TJ Giuli",
            "Prashanth Bungale"
        ],
        "summary": "Many emerging Web services, such as email, photo sharing, and web site archives, need to preserve large amounts of quickly-accessible data indefinitely into the future. In this paper, we make the case that these applications' demands on large scale storage systems over long time horizons require us to re-evaluate traditional storage system designs. We examine threats to long-lived data from an end-to-end perspective, taking into account not just hardware and software faults but also faults due to humans and organizations. We present a simple model of long-term storage failures that helps us reason about the various strategies for addressing these threats in a cost-effective manner. Using this model we show that the most important strategies for increasing the reliability of long-term storage are detecting latent faults quickly, automating fault repair to make it faster and cheaper, and increasing the independence of data replicas.",
        "published": "2005-08-31T01:44:35Z",
        "link": "http://arxiv.org/abs/cs/0508130v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "cs.OS"
        ]
    },
    {
        "title": "Lattices for Dynamic, Hierarchic & Overlapping Categorization: the Case   of Epistemic Communities",
        "authors": [
            "Camille Roth",
            "Paul Bourgine"
        ],
        "summary": "We present a method for hierarchic categorization and taxonomy evolution description. We focus on the structure of epistemic communities (ECs), or groups of agents sharing common knowledge concerns. Introducing a formal framework based on Galois lattices, we categorize ECs in an automated and hierarchically structured way and propose criteria for selecting the most relevant epistemic communities - for instance, ECs gathering a certain proportion of agents and thus prototypical of major fields. This process produces a manageable, insightful taxonomy of the community. Then, the longitudinal study of these static pictures makes possible an historical description. In particular, we capture stylized facts such as field progress, decline, specialization, interaction (merging or splitting), and paradigm emergence. The detection of such patterns in social networks could fruitfully be applied to other contexts.",
        "published": "2005-09-04T18:15:40Z",
        "link": "http://arxiv.org/abs/nlin/0509007v1",
        "categories": [
            "nlin.AO",
            "cs.AI",
            "cs.DL",
            "cs.IR"
        ]
    },
    {
        "title": "Requirements for Digital Preservation Systems: A Bottom-Up Approach",
        "authors": [
            "David S. H. Rosenthal",
            "Thomas S. Robertson",
            "Tom Lipkis",
            "Vicky Reich",
            "Seth Morabito"
        ],
        "summary": "The field of digital preservation is being defined by a set of standards developed top-down, starting with an abstract reference model (OAIS) and gradually adding more specific detail. Systems claiming conformance to these standards are entering production use. Work is underway to certify that systems conform to requirements derived from OAIS.   We complement these requirements derived top-down by presenting an alternate, bottom-up view of the field. The fundamental goal of these systems is to ensure that the information they contain remains accessible for the long term. We develop a parallel set of requirements based on observations of how existing systems handle this task, and on an analysis of the threats to achieving the goal. On this basis we suggest disclosures that systems should provide as to how they satisfy their goals.",
        "published": "2005-09-06T19:26:08Z",
        "link": "http://arxiv.org/abs/cs/0509018v2",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "On the genre-fication of Music: a percolation approach (long version)",
        "authors": [
            "R. Lambiotte",
            "M. Ausloos"
        ],
        "summary": "In this paper, we analyze web-downloaded data on people sharing their music library. By attributing to each music group usual music genres (Rock, Pop...), and analysing correlations between music groups of different genres with percolation-idea based methods, we probe the reality of these subdivisions and construct a music genre cartography, with a tree representation. We also show the diversity of music genres with Shannon entropy arguments, and discuss an alternative objective way to classify music, that is based on the complex structure of the groups audience. Finally, a link is drawn with the theory of hidden variables in complex networks.",
        "published": "2005-09-15T13:00:51Z",
        "link": "http://arxiv.org/abs/physics/0509134v2",
        "categories": [
            "physics.soc-ph",
            "cs.DL"
        ]
    },
    {
        "title": "Folksonomy as a Complex Network",
        "authors": [
            "Kaikai Shen",
            "Lide Wu"
        ],
        "summary": "Folksonomy is an emerging technology that works to classify the information over WWW through tagging the bookmarks, photos or other web-based contents. It is understood to be organized by every user while not limited to the authors of the contents and the professional editors. This study surveyed the folksonomy as a complex network. The result indicates that the network, which is composed of the tags from the folksonomy, displays both properties of small world and scale-free. However, the statistics only shows a local and static slice of the vast body of folksonomy which is still evolving.",
        "published": "2005-09-23T13:27:18Z",
        "link": "http://arxiv.org/abs/cs/0509072v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Representing Digital Assets for Long-Term Preservation using MPEG-21 DID",
        "authors": [
            "Jeroen Bekaert",
            "Xiaoming Liu",
            "Herbert Van de Sompel"
        ],
        "summary": "Various efforts aimed at representing digital assets have emerged from several communities over the last years, including the Metadata Encoding and Transmission Standard (METS), the IMS Content Packaging (IMS-CP) XML Binding and the XML Formatted Data Units (XFDU). The MPEG-21 Digital Item Declaration (MPEG-21 DID) is another approach that can be used for the representation of digital assets in XML. This paper will explore the potential of the MPEG-21 DID in a Digital Preservation context, by looking at the core building blocks of the OAIS Information Model and the way in which they map to the MPEG-21 DID abstract model and the MPEG-21 DIDL XML syntax.",
        "published": "2005-09-27T19:05:39Z",
        "link": "http://arxiv.org/abs/cs/0509084v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Access Interfaces for Open Archival Information Systems based on the   OAI-PMH and the OpenURL Framework for Context-Sensitive Services",
        "authors": [
            "Jeroen Bekaert",
            "Herbert Van de Sompel"
        ],
        "summary": "In recent years, a variety of digital repository and archival systems have been developed and adopted. All of these systems aim at hosting a variety of compound digital assets and at providing tools for storing, managing and accessing those assets. This paper will focus on the definition of common and standardized access interfaces that could be deployed across such diverse digital respository and archival systems. The proposed interfaces are based on the two formal specifications that have recently emerged from the Digital Library community: The Open Archive Initiative Protocol for Metadata Harvesting (OAI-PMH) and the NISO OpenURL Framework for Context-Sensitive Services (OpenURL Standard). As will be described, the former allows for the retrieval of batches of XML-based representations of digital assets, while the latter facilitates the retrieval of disseminations of a specific digital asset or of one or more of its constituents. The core properties of the proposed interfaces are explained in terms of the Reference Model for an Open Archival Information System (OAIS).",
        "published": "2005-09-28T14:57:38Z",
        "link": "http://arxiv.org/abs/cs/0509090v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "Telling Great Stories: An NSDL Content and Communications System for   Aggregation, Display, and Distribution of News and Features",
        "authors": [
            "Carol Minton Morris"
        ],
        "summary": "Education digital libraries contain cataloged resources as well as contextual information about innovations in the use of educational technology, exemplar stories about community activities, and news from various user communities that include teachers, students, scholars, and developers. Long-standing library traditions of service, preservation, democratization of knowledge, rich discourse, equal access, and fair use are evident in library communications models that both pull in and push out contextual information from multiple sources integrated with editorial production processes. This paper argues that a dynamic narrative flow [1] is enabled by effective management of complex content and communications in a decentralized web-based education digital library making publishing objects such as aggregations of resources, or selected parts of objects [4] accessible through a Content and Communications System. Providing services that encourage patrons to reuse, reflect out, and contribute resources back [5] to the Library increases the reach and impact of the National Science Digital Library (NSDL). This system is a model for distributed content development and effective communications for education digital libraries in general.",
        "published": "2005-09-28T16:28:19Z",
        "link": "http://arxiv.org/abs/cs/0509094v2",
        "categories": [
            "cs.DL",
            "cs.SE",
            "D.2.10"
        ]
    },
    {
        "title": "Bibliographic Classification using the ADS Databases",
        "authors": [
            "Alberto Accomazzi",
            "Michael J. Kurtz",
            "Guenther Eichhorn",
            "Edwin Henneken",
            "Carolyn S. Grant",
            "Markus Demleitner",
            "Stephen S. Murray"
        ],
        "summary": "We discuss two techniques used to characterize bibliographic records based on their similarity to and relationship with the contents of the NASA Astrophysics Data System (ADS) databases. The first method has been used to classify input text as being relevant to one or more subject areas based on an analysis of the frequency distribution of its individual words. The second method has been used to classify existing records as being relevant to one or more databases based on the distribution of the papers citing them. Both techniques have proven to be valuable tools in assigning new and existing bibliographic records to different disciplines within the ADS databases.",
        "published": "2005-10-31T22:34:34Z",
        "link": "http://arxiv.org/abs/cs/0511002v1",
        "categories": [
            "cs.IR",
            "cs.DL"
        ]
    },
    {
        "title": "The Availability and Persistence of Web References in D-Lib Magazine",
        "authors": [
            "Frank McCown",
            "Sheffan Chan",
            "Michael L. Nelson",
            "Johan Bollen"
        ],
        "summary": "We explore the availability and persistence of URLs cited in articles published in D-Lib Magazine. We extracted 4387 unique URLs referenced in 453 articles published from July 1995 to August 2004. The availability was checked three times a week for 25 weeks from September 2004 to February 2005. We found that approximately 28% of those URLs failed to resolve initially, and 30% failed to resolve at the last check. A majority of the unresolved URLs were due to 404 (page not found) and 500 (internal server error) errors. The content pointed to by the URLs was relatively stable; only 16% of the content registered more than a 1 KB change during the testing period. We explore possible factors which may cause a URL to fail by examining its age, path depth, top-level domain and file extension. Based on the data collected, we found the half-life of a URL referenced in a D-Lib Magazine article is approximately 10 years. We also found that URLs were more likely to be unavailable if they pointed to resources in the .net, .edu or country-specific top-level domain, used non-standard ports (i.e., not port 80), or pointed to resources with uncommon or deprecated extensions (e.g., .shtml, .ps, .txt).",
        "published": "2005-11-21T15:56:17Z",
        "link": "http://arxiv.org/abs/cs/0511077v1",
        "categories": [
            "cs.DL",
            "H.3.m"
        ]
    },
    {
        "title": "Dynamic Web File Format Transformations with Grace",
        "authors": [
            "Daniel S. Swaney",
            "Frank McCown",
            "Michael L. Nelson"
        ],
        "summary": "Web accessible content stored in obscure, unpopular or obsolete formats represents a significant problem for digital preservation. The file formats that encode web content represent the implicit and explicit choices of web site maintainers at a particular point in time. Older file formats that have fallen out of favor are obviously a problem, but so are new file formats that have not yet been fully supported by browsers. Often browsers use plug-in software for displaying old and new formats, but plug-ins can be difficult to find, install and replicate across all environments that one may use. We introduce Grace, an http proxy server that transparently converts browser-incompatible and obsolete web content into web content that a browser is able to display without the use of plug-ins. Grace is configurable on a per user basis and can be expanded to provide an array of conversion services. We illustrate how the Grace prototype transforms several image formats (XBM, PNG with various alpha channels, and JPEG 2000) so they are viewable in Internet Explorer.",
        "published": "2005-12-16T17:21:57Z",
        "link": "http://arxiv.org/abs/cs/0512068v1",
        "categories": [
            "cs.DL"
        ]
    },
    {
        "title": "EqRank: Theme Evolution in Citation Graphs",
        "authors": [
            "G. B. Pivovarov",
            "S. E. Trunov"
        ],
        "summary": "Time evolution of the classification scheme generated by the EqRank algorithm is studied with hep-th citation graph as an example. Intuitive expectations about evolution of an adequate classification scheme for a growing set of objects are formulated. Evolution compliant with these expectations is called natural. It is demonstrated that EqRank yields a naturally evolving classification scheme. We conclude that EqRank can be used as a means to detect new scientific themes, and to track their development.",
        "published": "2005-12-20T14:01:45Z",
        "link": "http://arxiv.org/abs/cs/0512080v1",
        "categories": [
            "cs.DS",
            "cs.DL"
        ]
    },
    {
        "title": "Collaborative tagging as a tripartite network",
        "authors": [
            "R. Lambiotte",
            "M. Ausloos"
        ],
        "summary": "We describe online collaborative communities by tripartite networks, the nodes being persons, items and tags. We introduce projection methods in order to uncover the structures of the networks, i.e. communities of users, genre families...   To do so, we focus on the correlations between the nodes, depending on their profiles, and use percolation techniques that consist in removing less correlated links and observing the shaping of disconnected islands. The structuring of the network is visualised by using a tree representation. The notion of diversity in the system is also discussed.",
        "published": "2005-12-23T13:38:57Z",
        "link": "http://arxiv.org/abs/cs/0512090v2",
        "categories": [
            "cs.DS",
            "cs.DL"
        ]
    },
    {
        "title": "Enabling Agents to Dynamically Select Protocols for Interactions",
        "authors": [
            "Jose Ghislain Quenum Samir Aknine"
        ],
        "summary": "in this paper we describe a method which allows agents to dynamically select protocols and roles when they need to execute collaborative tasks",
        "published": "2005-01-18T17:08:05Z",
        "link": "http://arxiv.org/abs/cs/0501036v2",
        "categories": [
            "cs.MA",
            "cs.SE"
        ]
    },
    {
        "title": "Under-approximation of the Greatest Fixpoint in Real-Time System   Verification",
        "authors": [
            "Farn Wang"
        ],
        "summary": "Techniques for the efficient successive under-approximation of the greatest fixpoint in TCTL formulas can be useful in fast refutation of inevitability properties and vacuity checking. We first give an integrated algorithmic framework for both under and over-approximate model-checking. We design the {\\em NZF (Non-Zeno Fairness) predicate}, with a greatest fixpoint formulation, as a unified framework for the evaluation of formulas like $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then prove the correctness of a new formulation for the characterization of the NZF predicate based on zone search and the least fixpoint evaluation. The new formulation then leads to the design of an evaluation algorithm, with the capability of successive under-approximation, for $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then present techniques to efficiently search for the zones and to speed up the under-approximate evaluation of those three formulas. Our experiments show that the techniques have significantly enhanced the verification performance against several benchmarks over exact model-checking.",
        "published": "2005-01-22T11:02:01Z",
        "link": "http://arxiv.org/abs/cs/0501059v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "Under-approximation of the Greatest Fixpoints in Real-Time System   Verification",
        "authors": [
            "Farn Wang"
        ],
        "summary": "Techniques for the efficient successive under-approximation of the greatest fixpoint in TCTL formulas can be useful in fast refutation of inevitability properties and vacuity checking. We first give an integrated algorithmic framework for both under and over-approximate model-checking. We design the {\\em NZF (Non-Zeno Fairness) predicate}, with a greatest fixpoint formulation, as a unified framework for the evaluation of formulas like $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then prove the correctness of a new formulation for the characterization of the NZF predicate based on zone search and the least fixpoint evaluation. The new formulation then leads to the design of an evaluation algorithm, with the capability of successive under-approximation, for $\\exists\\pfrr\\eta_1$, $\\exists\\pfrr\\pevt\\eta_1$, and $\\exists\\pevt\\pfrr\\eta_1$. We then present techniques to efficiently search for the zones and to speed up the under-approximate evaluation of those three formulas. Our experiments show that the techniques have significantly enhanced the verification performance against several benchmarks over exact model-checking.",
        "published": "2005-01-22T11:03:49Z",
        "link": "http://arxiv.org/abs/cs/0501060v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "Extending Design by Contract for Aspect-Oriented Programming",
        "authors": [
            "David H. Lorenz",
            "Therapon Skotiniotis"
        ],
        "summary": "Design by Contract (DbC) and runtime enforcement of program assertions enables the construction of more robust software. It also enables the assignment of blame in error reporting. Unfortunately, there is no support for runtime contract enforcement and blame assignment for Aspect-Oriented Programming (AOP). Extending DbC to also cover aspects brings forward a plethora of issues related to the correct order of assertion validation. We show that there is no generally correct execution sequence of object assertions and aspect assertions. A further classification of aspects as agnostic, obedient, or rebellious defines the order of assertion validation that needs to be followed. We describe the application of this classification in a prototyped DbC tool for AOP named Cona, where aspects are used for implementing contracts, and contracts are used for enforcing assertions on aspects.",
        "published": "2005-01-24T20:28:46Z",
        "link": "http://arxiv.org/abs/cs/0501070v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.2, D.2.3, D.2.4"
        ]
    },
    {
        "title": "EPspectra: A Formal Toolkit for Developing DSP Software Applications",
        "authors": [
            "Hahnsang Kim",
            "Theirry Turletti",
            "Amar Bouali"
        ],
        "summary": "The software approach to developing Digital Signal Processing (DSP) applications brings some great features such as flexibility, re-usability of resources and easy upgrading of applications. However, it requires long and tedious tests and verification phases because of the increasing complexity of the software. This implies the need of a software programming environment capable of putting together DSP modules and providing facilities to debug, verify and validate the code. The objective of the work is to provide such facilities as simulation and verification for developing DSP software applications. This led us to develop an extension toolkit, Epspectra, built upon Pspectra, one of the first toolkits available to design basic software radio applications on standard PC workstations. In this paper, we first present Epspectra, an Esterel-based extension of Pspectra that makes the design and implementation of portable DSP applications easier. It allows drastic reduction of testing and verification time while requiring relatively little expertise in formal verification methods. Second, we demonstrate the use of Epspectra, taking as an example the radio interface part of a GSM base station. We also present the verification procedures for the three safety properties of the implementation programs which have complex control-paths. These have to obey strict scheduling rules. In addition, Epspectra achieves the verification of the targeted application since the same model is used for the executable code generation and for the formal verification.",
        "published": "2005-02-04T15:11:26Z",
        "link": "http://arxiv.org/abs/cs/0502025v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "D.2.4, F.3.1, F.4.0, F.4.1, F.4.2"
        ]
    },
    {
        "title": "Testing Systems of Concurrent Black-boxes--an Automata-Theoretic and   Decompositional Approach",
        "authors": [
            "Gaoyan Xie",
            "Zhe Dang"
        ],
        "summary": "The global testing problem studied in this paper is to seek a definite answer to whether a system of concurrent black-boxes has an observable behavior in a given finite (but could be huge) set \"Bad\". We introduce a novel approach to solve the problem that does not require integration testing. Instead, in our approach, the global testing problem is reduced to testing individual black-boxes in the system one by one in some given order. Using an automata-theoretic approach, test sequences for each individual black-box are generated from the system's description as well as the test results of black-boxes prior to this black-box in the given order. In contrast to the conventional compositional/modular verification/testing approaches, our approach is essentially decompositional.   Also, our technique is complete, sound, and can be carried out automatically. Our experiment results show that the total number of tests needed to solve the global testing problem is substantially small even for an extremely large \"Bad\".",
        "published": "2005-02-08T01:15:47Z",
        "link": "http://arxiv.org/abs/cs/0502040v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Local and Global Analysis: Complementary Activities for Increasing the   Effectiveness of Requirements Verification and Validation",
        "authors": [
            "Lester Lobo",
            "James D. Arthur"
        ],
        "summary": "This paper presents a unique approach to connecting requirements engineering (RE) activities into a process framework that can be employed to obtain quality requirements with reduced expenditures of effort and cost. We propose a two-phase model that is novel in that it introduces the concept of verification and validation (V&V) early in the requirements life cycle. In the first phase, we perform V&V immediately following the elicitation of requirements for each individually distinct system function. Because the first phase focuses on capturing smaller sets of related requirements iteratively, each corresponding V&V activity is better focused for detecting and correcting errors in each requirement set. In the second phase, a complementary verification activity is initiated; the corresponding focus is on the quality of linkages between requirements sets rather than on the requirements within the sets. Consequently, this approach reduces the effort in verification and enhances the focus on the verification task. Our approach, unlike other models, has a minimal time delay between the elicitation of requirements and the execution of the V&V activities. Because of this short time gap, the stakeholders have a clearer recollection of the requirements, their context and rationale; this enhances the stakeholder feedback. Furthermore, our model includes activities that closely align with the effective RE processes employed in the software industry. Thus, our approach facilitates a better understanding of the flow of requirements, and provides guidance for the implementation of the RE process.",
        "published": "2005-03-02T00:21:11Z",
        "link": "http://arxiv.org/abs/cs/0503002v1",
        "categories": [
            "cs.SE",
            "D.2.1; D.2.9"
        ]
    },
    {
        "title": "An Objectives-Driven Process for Selecting Methods to Support   Requirements Engineering Activities",
        "authors": [
            "Lester Lobo",
            "James D. Arthur"
        ],
        "summary": "This paper presents a framework that guides the requirements engineer in the implementation and execution of an effective requirements generation process. We achieve this goal by providing a well-defined requirements engineering model and a criteria based process for optimizing method selection for attendant activities. Our model, unlike other models, addresses the complete requirements generation process and consists of activities defined at more adequate levels of abstraction. Additionally, activity objectives are identified and explicitly stated - not implied as in the current models. Activity objectives are crucial as they drive the selection of methods for each activity. Our model also incorporates a unique approach to verification and validation that enhances quality and reduces the cost of generating requirements. To assist in the selection of methods, we have mapped commonly used methods to activities based on their objectives. In addition, we have identified method selection criteria and prescribed a reduced set of methods that optimize these criteria for each activity defined by our requirements generation process. Thus, the defined approach assists in the task of selecting methods by using selection criteria to reduce a large collection of potential methods to a smaller, manageable set. The model and the set of methods, taken together, provide the much needed guidance for the effective implementation and execution of the requirements generation process.",
        "published": "2005-03-02T00:46:46Z",
        "link": "http://arxiv.org/abs/cs/0503003v1",
        "categories": [
            "cs.SE",
            "D.2.1; D.2.9"
        ]
    },
    {
        "title": "Effective Requirements Generation: Synchronizing Early Verification &   Validation, Methods and Method Selection Criteria",
        "authors": [
            "Lester Lobo",
            "James D. Arthur"
        ],
        "summary": "This paper presents an approach for the implementation and execution of an effective requirements generation process. We achieve this goal by providing a well-defined requirements engineering model that includes verification and validation (V&V), and analysis. In addition, we identify focused activity objectives and map popular methods to lower-level activities, and define a criterion based process for optimizing method selection for attendant activities. Our model, unlike other models, addresses the complete requirements generation process and consists of activities defined at more adequate levels of abstraction. Furthermore, our model also incorporates a unique approach to V&V that enhances quality and reduces the cost of generating requirements. Additionally, activity objectives are identified and explicitly stated - not implied as in the current models. To assist in the selection of an appropriate set of methods, we have mapped commonly used methods to activities based on their objectives. Finally, we have identified method selection criteria and prescribed a reduced set of methods that optimize these criteria for each activity in our model. Thus, our approach assists in the task of selecting methods by using selection criteria to reduce a large collection of potential methods to a smaller, manageable set. The model, clear mapping of methods to activity objectives, and the criteria based process, taken together, provide the much needed guidance for the effective implementation and execution of the requirements generation process.",
        "published": "2005-03-02T01:05:32Z",
        "link": "http://arxiv.org/abs/cs/0503004v1",
        "categories": [
            "cs.SE",
            "D.2.1; D.2.9"
        ]
    },
    {
        "title": "A Systematic Aspect-Oriented Refactoring and Testing Strategy, and its   Application to JHotDraw",
        "authors": [
            "Arie van Deursen",
            "Marius Marin",
            "Leon Moonen"
        ],
        "summary": "Aspect oriented programming aims at achieving better modularization for a system's crosscutting concerns in order to improve its key quality attributes, such as evolvability and reusability. Consequently, the adoption of aspect-oriented techniques in existing (legacy) software systems is of interest to remediate software aging. The refactoring of existing systems to employ aspect-orientation will be considerably eased by a systematic approach that will ensure a safe and consistent migration.   In this paper, we propose a refactoring and testing strategy that supports such an approach and consider issues of behavior conservation and (incremental) integration of the aspect-oriented solution with the original system. The strategy is applied to the JHotDraw open source project and illustrated on a group of selected concerns. Finally, we abstract from the case study and present a number of generic refactorings which contribute to an incremental aspect-oriented refactoring process and associate particular types of crosscutting concerns to the model and features of the employed aspect language. The contributions of this paper are both in the area of supporting migration towards aspect-oriented solutions and supporting the development of aspect languages that are better suited for such migrations.",
        "published": "2005-03-05T11:45:00Z",
        "link": "http://arxiv.org/abs/cs/0503015v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.7; D.2.5; D.1.5"
        ]
    },
    {
        "title": "Systematic Method for Path-Complete White Box Testing",
        "authors": [
            "Hanna Makaruk",
            "Robert Owczarek",
            "Nikita Sakhanenko"
        ],
        "summary": "A systematic, language-independent method of finding a minimal set of paths covering the code of a sequential program is proposed for application in White Box testing. Execution of all paths from the set ensures also statement coverage. Execution fault marks problematic areas of the code. The method starts from a UML activity diagram of a program. The diagram is transformed into a directed graph: graph's nodes substitute decision and action points; graph's directed edges substitute action arrows.   The number of independent paths equals easy-to-compute cyclomatic complexity of the graph. Association of a vector to each path creates a path vector space. Independence of the paths is equivalent to linear independence of the vectors. It is sufficient to test any base of the path space to complete the procedure. An effective algorithm for choosing the base paths is presented.",
        "published": "2005-03-21T21:26:48Z",
        "link": "http://arxiv.org/abs/cs/0503050v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "A Survey of Reverse Engineering and Program Comprehension",
        "authors": [
            "Michael L. Nelson"
        ],
        "summary": "Reverse engineering has been a standard practice in the hardware community for some time. It has only been within the last ten years that reverse engineering, or \"program comprehension\", has grown into the current sub-discipline of software engineering. Traditional software engineering is primarily focused on the development and design of new software. However, most programmers work on software that other people have designed and developed. Up to 50% of a software maintainers time can be spent determining the intent of source code. The growing demand to reevaluate and reimplement legacy software systems, brought on by the proliferation of clientserver and World Wide Web technologies, has underscored the need for reverse engineering tools and techniques. This paper introduces the terminology of reverse engineering and gives some of the obstacles that make reverse engineering difficult. Although reverse engineering remains heavily dependent on the human component, a number of automated tools are presented that aid the reverse engineer.",
        "published": "2005-03-24T13:55:53Z",
        "link": "http://arxiv.org/abs/cs/0503068v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Prototype of Fault Adaptive Embedded Software for Large-Scale Real-Time   Systems",
        "authors": [
            "Derek Messie",
            "Mina Jung",
            "Jae C. Oh",
            "Shweta Shetty",
            "Steven Nordstrom",
            "Michael Haney"
        ],
        "summary": "This paper describes a comprehensive prototype of large-scale fault adaptive embedded software developed for the proposed Fermilab BTeV high energy physics experiment. Lightweight self-optimizing agents embedded within Level 1 of the prototype are responsible for proactive and reactive monitoring and mitigation based on specified layers of competence. The agents are self-protecting, detecting cascading failures using a distributed approach. Adaptive, reconfigurable, and mobile objects for reliablility are designed to be self-configuring to adapt automatically to dynamically changing environments. These objects provide a self-healing layer with the ability to discover, diagnose, and react to discontinuities in real-time processing. A generic modeling environment was developed to facilitate design and implementation of hardware resource specifications, application data flow, and failure mitigation strategies. Level 1 of the planned BTeV trigger system alone will consist of 2500 DSPs, so the number of components and intractable fault scenarios involved make it impossible to design an `expert system' that applies traditional centralized mitigative strategies based on rules capturing every possible system state. Instead, a distributed reactive approach is implemented using the tools and methodologies developed by the Real-Time Embedded Systems group.",
        "published": "2005-04-29T14:59:23Z",
        "link": "http://arxiv.org/abs/cs/0504109v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Pluggable AOP: Designing Aspect Mechanisms for Third-party Composition",
        "authors": [
            "Sergei Kojarski",
            "David H. Lorenz"
        ],
        "summary": "Studies of Aspect-Oriented Programming (AOP) usually focus on a language in which a specific aspect extension is integrated with a base language. Languages specified in this manner have a fixed, non-extensible AOP functionality. In this paper we consider the more general case of integrating a base language with a set of domain specific third-party aspect extensions for that language. We present a general mixin-based method for implementing aspect extensions in such a way that multiple, independently developed, dynamic aspect extensions can be subject to third-party composition and work collaboratively.",
        "published": "2005-04-30T20:49:15Z",
        "link": "http://arxiv.org/abs/cs/0505004v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.1.5; D.2.10; D.2.12; D.3.1; D.3.4"
        ]
    },
    {
        "title": "A General Methodology for Designing Self-Organizing Systems",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "Our technologies complexify our environments. Thus, new technologies need to deal with more and more complexity. Several efforts have been made to deal with this complexity using the concept of self-organization. However, in order to promote its use and understanding, we must first have a pragmatic understanding of complexity and self-organization. This paper presents a conceptual framework for speaking about self-organizing systems. The aim is to provide a methodology useful for designing and controlling systems developed to solve complex problems. First, practical notions of complexity and self-organization are given. Then, starting from the agent metaphor, a conceptual framework is presented. This provides formal ways of speaking about \"satisfaction\" of elements and systems. The main premise of the methodology claims that reducing the \"friction\" or \"interference\" of interactions between elements of a system will result in a higher \"satisfaction\" of the system, i.e. better performance. The methodology discusses different ways in which this can be achieved. A case study on self-organizing traffic lights illustrates the ideas presented in the paper.",
        "published": "2005-05-03T19:23:35Z",
        "link": "http://arxiv.org/abs/nlin/0505009v3",
        "categories": [
            "nlin.AO",
            "cs.GL",
            "cs.SE",
            "physics.soc-ph"
        ]
    },
    {
        "title": "Automated Improvement for Component Reuse",
        "authors": [
            "Muthu Ramachandran"
        ],
        "summary": "Software component reuse is the key to significant gains in productivity. However, the major problem is the lack of identifying and developing potentially reusable components. This paper concentrates on our approach to the development of reusable software components. A prototype tool has been developed, known as the Reuse Assessor and Improver System (RAIS) which can interactively identify, analyse, assess, and modify abstractions, attributes and architectures that support reuse. Practical and objective reuse guidelines are used to represent reuse knowledge and to do domain analysis. It takes existing components, provides systematic reuse assessment which is based on reuse advice and analysis, and produces components that are improved for reuse. Our work on guidelines has been extended to a large scale industrial application.",
        "published": "2005-05-11T18:23:19Z",
        "link": "http://arxiv.org/abs/cs/0505029v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Scouting Red Beds for Uncommon Features with   Geological Significance",
        "authors": [
            "Patrick C. McGuire",
            "Enrique Diaz-Martinez",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose A. Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "The `Cyborg Astrobiologist' (CA) has undergone a second geological field trial, at a red sandstone site in northern Guadalajara, Spain, near Riba de Santiuste. The Cyborg Astrobiologist is a wearable computer and video camera system that has demonstrated a capability to find uncommon interest points in geological imagery in real-time in the field. The first (of three) geological structures that we studied was an outcrop of nearly homogeneous sandstone, which exhibits oxidized-iron impurities in red and and an absence of these iron impurities in white. The white areas in these ``red beds'' have turned white because the iron has been removed by chemical reduction, perhaps by a biological agent. The computer vision system found in one instance several (iron-free) white spots to be uncommon and therefore interesting, as well as several small and dark nodules. The second geological structure contained white, textured mineral deposits on the surface of the sandstone, which were found by the CA to be interesting. The third geological structure was a 50 cm thick paleosol layer, with fossilized root structures of some plants, which were found by the CA to be interesting. A quasi-blind comparison of the Cyborg Astrobiologist's interest points for these images with the interest points determined afterwards by a human geologist shows that the Cyborg Astrobiologist concurred with the human geologist 68% of the time (true positive rate), with a 32% false positive rate and a 32% false negative rate.   (abstract has been abridged).",
        "published": "2005-05-23T09:55:37Z",
        "link": "http://arxiv.org/abs/cs/0505058v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "physics.ins-det",
            "q-bio.NC",
            "I.2.10; I.4.6; I.4.8; I.4.9; I.2.9; I.5.4; I.5.5; J.2; J.3; D.2;\n  D.1.7; D.4.7"
        ]
    },
    {
        "title": "The Workshop - Implementing Well Structured Enterprise Applications",
        "authors": [
            "A. Wiesmaier",
            "V. Karatsiolis",
            "M. Lippert",
            "J. Buchmann"
        ],
        "summary": "We specify an abstraction layer to be used between an enterprise application and the utilized enterprise framework (like J2EE or .NET). This specification is called the Workshop. It provides an intuitive metaphor supporting the programmer in designing easy understandable code. We present an implementation of this specification. It is based upon the J2EE framework and is called the JWorkshop. As a proof of concept we implement a special certification authority called the Key Authority based upon the JWorkshop. The mentioned certification authority runs very successfully in a variety of different real world projects.",
        "published": "2005-06-13T13:25:14Z",
        "link": "http://arxiv.org/abs/cs/0506050v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Measuring Woody: The Size of Debian 3.0",
        "authors": [
            "Juan Jose Amor",
            "Gregorio Robles",
            "Jesus Gonzalez-Barahona"
        ],
        "summary": "Debian is possibly the largest free software distribution, with well over 4,500 source packages in the latest stable release (Debian 3.0) and more than 8,000 source packages in the release currently in preparation. However, we wish to know what these numbers mean. In this paper, we use David A. Wheeler's SLOCCount system to determine the number of physical source lines of code (SLOC) of Debian 3.0 (aka woody). We show that Debian 3.0 includes more than 105,000,000 physical SLOC (almost twice than Red Hat 9, released about 8 months later), showing that the Debian development model (based on the work of a large group of voluntary developers spread around the world) is at least as capable as other development methods (like the more centralized one, based on the work of employees, used by Red Hat or Microsoft) to manage distributions of this size.   It is also shown that if Debian had been developed using traditional proprietary methods, the COCOMO model estimates that its cost would be close to $6.1 billion USD to develop Debian 3.0. In addition, we offer both an analysis of the programming languages used in the distribution (C amounts for about 65%, C++ for about 12%, Shell for about 8% and LISP is around 4%, with many others to follow), and the largest packages (The Linux kernel, Mozilla, XFree86, PM3, etc.)",
        "published": "2005-06-15T21:48:06Z",
        "link": "http://arxiv.org/abs/cs/0506067v1",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Software Architecture Overview",
        "authors": [
            "Andre Adrian"
        ],
        "summary": "What is Software Architecture? The rules, paradigmen, pattern that help to construct, build and test a serious piece of software. It is the practical experience boiled down to abstract level. Software Architecture builds on System Engineering and the scientific method as established by Galileo Galilei: Measure what you can and make measureable what you can not. The experiment (test) is more important then the deduction. Pieces of information about software architecture are all over the internet. This paper uses citation as much as possible. The aim is to bring together an overview, not to rephrase the wording.",
        "published": "2005-07-24T09:43:27Z",
        "link": "http://arxiv.org/abs/cs/0507061v1",
        "categories": [
            "cs.SE",
            "C.0; C.5"
        ]
    },
    {
        "title": "Software Libraries and Their Reuse: Entropy, Kolmogorov Complexity, and   Zipf's Law",
        "authors": [
            "Todd L. Veldhuizen"
        ],
        "summary": "We analyze software reuse from the perspective of information theory and Kolmogorov complexity, assessing our ability to ``compress'' programs by expressing them in terms of software components reused from libraries. A common theme in the software reuse literature is that if we can only get the right environment in place-- the right tools, the right generalizations, economic incentives, a ``culture of reuse'' -- then reuse of software will soar, with consequent improvements in productivity and software quality. The analysis developed in this paper paints a different picture: the extent to which software reuse can occur is an intrinsic property of a problem domain, and better tools and culture can have only marginal impact on reuse rates if the domain is inherently resistant to reuse. We define an entropy parameter $H \\in [0,1]$ of problem domains that measures program diversity, and deduce from this upper bounds on code reuse and the scale of components with which we may work. For ``low entropy'' domains with $H$ near 0, programs are highly similar to one another and the domain is amenable to the Component-Based Software Engineering (CBSE) dream of programming by composing large-scale components. For problem domains with $H$ near 1, programs require substantial quantities of new code, with only a modest proportion of an application comprised of reused, small-scale components. Preliminary empirical results from Unix platforms support some of the predictions of our model.",
        "published": "2005-08-03T04:20:11Z",
        "link": "http://arxiv.org/abs/cs/0508023v3",
        "categories": [
            "cs.SE",
            "cs.IT",
            "cs.PL",
            "math.IT",
            "D.2.8; E.4; D.2.13"
        ]
    },
    {
        "title": "Proceedings of the 15th Workshop on Logic-based methods in Programming   Environments WLPE'05 -- October 5, 2005 -- Sitges (Barcelona), Spain",
        "authors": [
            "Alexander Serebrenik",
            "Susana Munoz-Hernandez"
        ],
        "summary": "This volume contains papers presented at WLPE 2005, 15th International Workshop on Logic-based methods in Programming Environments.   The aim of the workshop is to provide an informal meeting for the researchers working on logic-based tools for development and analysis of programs. This year we emphasized two aspects: on one hand the presentation, pragmatics and experiences of tools for logic programming environments; on the other one, logic-based environmental tools for programming in general.   The workshop took place in Sitges (Barcelona), Spain as a satellite workshop of the 21th International Conference on Logic Programming (ICLP 2005). This workshop continues the series of successful international workshops on logic programming environments held in Ohio, USA (1989), Eilat, Israel (1990), Paris, France (1991), Washington, USA (1992), Vancouver, Canada (1993), Santa Margherita Ligure, Italy (1994), Portland, USA (1995), Leuven, Belgium and Port Jefferson, USA (1997), Las Cruces, USA (1999), Paphos, Cyprus (2001), Copenhagen, Denmark (2002), Mumbai, India (2003) and Saint Malo, France (2004).   We have received eight submissions (2 from France, 2 Spain-US cooperations, one Spain-Argentina cooperation, one from Japan, one from the United Kingdom and one Sweden-France cooperation). Program committee has decided to accept seven papers. This volume contains revised versions of the accepted papers.   We are grateful to the authors of the papers, the reviewers and the members of the Program Committee for the help and fruitful discussions.",
        "published": "2005-08-17T13:35:20Z",
        "link": "http://arxiv.org/abs/cs/0508078v4",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE",
            "D.2.6; D.1.6"
        ]
    },
    {
        "title": "Extending Prolog with Incomplete Fuzzy Information",
        "authors": [
            "Susana Munoz-Hernandez",
            "Claudio Vaucheret"
        ],
        "summary": "Incomplete information is a problem in many aspects of actual environments. Furthermore, in many sceneries the knowledge is not represented in a crisp way. It is common to find fuzzy concepts or problems with some level of uncertainty. There are not many practical systems which handle fuzziness and uncertainty and the few examples that we can find are used by a minority. To extend a popular system (which many programmers are using) with the ability of combining crisp and fuzzy knowledge representations seems to be an interesting issue.   Our first work (Fuzzy Prolog) was a language that models $\\mathcal{B}([0,1])$-valued Fuzzy Logic. In the Borel algebra, $\\mathcal{B}([0,1])$, truth value is represented using unions of intervals of real numbers. This work was more general in truth value representation and propagation than previous works.   An interpreter for this language using Constraint Logic Programming over Real numbers (CLP(${\\cal R}$)) was implemented and is available in the Ciao system.   Now, we enhance our former approach by using default knowledge to represent incomplete information in Logic Programming. We also provide the implementation of this new framework. This new release of Fuzzy Prolog handles incomplete information, it has a complete semantics (the previous one was incomplete as Prolog) and moreover it is able to combine crisp and fuzzy logic in Prolog programs. Therefore, new Fuzzy Prolog is more expressive to represent real world.   Fuzzy Prolog inherited from Prolog its incompleteness. The incorporation of default reasoning to Fuzzy Prolog removes this problem and requires a richer semantics which we discuss.",
        "published": "2005-08-22T07:41:41Z",
        "link": "http://arxiv.org/abs/cs/0508091v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6"
        ]
    },
    {
        "title": "A Tracer Driver for Versatile Dynamic Analyses of Constraint Logic   Programs",
        "authors": [
            "Ludovic Langevine",
            "Mireille Ducasse"
        ],
        "summary": "Programs with constraints are hard to debug. In this paper, we describe a general architecture to help develop new debugging tools for constraint programming. The possible tools are fed by a single general-purpose tracer. A tracer-driver is used to adapt the actual content of the trace, according to the needs of the tool. This enables the tools and the tracer to communicate in a client-server scheme. Each tool describes its needs of execution data thanks to event patterns. The tracer driver scrutinizes the execution according to these event patterns and sends only the data that are relevant to the connected tools. Experimental measures show that this approach leads to good performance in the context of constraint logic programming, where a large variety of tools exists and the trace is potentially huge.",
        "published": "2005-08-24T11:30:26Z",
        "link": "http://arxiv.org/abs/cs/0508105v1",
        "categories": [
            "cs.SE",
            "D.2; D.2.5; D.2.6"
        ]
    },
    {
        "title": "Proving or Disproving likely Invariants with Constraint Reasoning",
        "authors": [
            "Tristan Denmat",
            "Arnaud Gotlieb",
            "Mireille Ducasse"
        ],
        "summary": "A program invariant is a property that holds for every execution of the program. Recent work suggest to infer likely-only invariants, via dynamic analysis. A likely invariant is a property that holds for some executions but is not guaranteed to hold for all executions. In this paper, we present work in progress addressing the challenging problem of automatically verifying that likely invariants are actual invariants. We propose a constraint-based reasoning approach that is able, unlike other approaches, to both prove or disprove likely invariants. In the latter case, our approach provides counter-examples. We illustrate the approach on a motivating example where automatically generated likely invariants are verified.",
        "published": "2005-08-24T13:30:59Z",
        "link": "http://arxiv.org/abs/cs/0508108v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.6"
        ]
    },
    {
        "title": "Enhancing the Alloy Analyzer with Patterns of Analysis",
        "authors": [
            "William Heaven",
            "Alessandra Russo"
        ],
        "summary": "Formal techniques have been shown to be useful in the development of correct software. But the level of expertise required of practitioners of these techniques prohibits their widespread adoption. Formal techniques need to be tailored to the commercial software developer. Alloy is a lightweight specification language supported by the Alloy Analyzer (AA), a tool based on off-the-shelf SAT technology. The tool allows a user to check interactively whether given properties are consistent or valid with respect to a high-level specification, providing an environment in which the correctness of such a specification may be established. However, Alloy is not particularly suited to expressing program specifications and the feedback provided by AA can be misleading where the specification under analysis or the property being checked contains inconsistencies. In this paper, we address these two shortcomings. Firstly, we present a lightweight language called \"Loy\", tailored to the specification of object-oriented programs. An encoding of Loy into Alloy is provided so that AA can be used for automated analysis of Loy program specifications. Secondly, we present some \"patterns of analysis\" that guide a developer through the analysis of a Loy specification in order to establish its correctness before implementation.",
        "published": "2005-08-24T15:35:07Z",
        "link": "http://arxiv.org/abs/cs/0508109v1",
        "categories": [
            "cs.SE",
            "cs.LO"
        ]
    },
    {
        "title": "A Generic Framework for the Analysis and Specialization of Logic   Programs",
        "authors": [
            "German Puebla",
            "Elvira Albert",
            "Manuel Hermenegildo"
        ],
        "summary": "The relationship between abstract interpretation and partial deduction has received considerable attention and (partial) integrations have been proposed starting from both the partial deduction and abstract interpretation perspectives. In this work we present what we argue is the first fully described generic algorithm for efficient and precise integration of abstract interpretation and partial deduction. Taking as starting point state-of-the-art algorithms for context-sensitive, polyvariant abstract interpretation and (abstract) partial deduction, we present an algorithm which combines the best of both worlds. Key ingredients include the accurate success propagation inherent to abstract interpretation and the powerful program transformations achievable by partial deduction. In our algorithm, the calls which appear in the analysis graph are not analyzed w.r.t. the original definition of the procedure but w.r.t. specialized definitions of these procedures. Such specialized definitions are obtained by applying both unfolding and abstract executability. Our framework is parametric w.r.t. different control strategies and abstract domains. Different combinations of such parameters correspond to existing algorithms for program analysis and specialization. Simultaneously, our approach opens the door to the efficient computation of strictly more precise results than those achievable by each of the individual techniques. The algorithm is now one of the key components of the CiaoPP analysis and specialization system.",
        "published": "2005-08-24T21:50:59Z",
        "link": "http://arxiv.org/abs/cs/0508111v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6"
        ]
    },
    {
        "title": "On Algorithms and Complexity for Sets with Cardinality Constraints",
        "authors": [
            "Bruno Marnette",
            "Viktor Kuncak",
            "Martin Rinard"
        ],
        "summary": "Typestate systems ensure many desirable properties of imperative programs, including initialization of object fields and correct use of stateful library interfaces. Abstract sets with cardinality constraints naturally generalize typestate properties: relationships between the typestates of objects can be expressed as subset and disjointness relations on sets, and elements of sets can be represented as sets of cardinality one. Motivated by these applications, this paper presents new algorithms and new complexity results for constraints on sets and their cardinalities. We study several classes of constraints and demonstrate a trade-off between their expressive power and their complexity.   Our first result concerns a quantifier-free fragment of Boolean Algebra with Presburger Arithmetic. We give a nondeterministic polynomial-time algorithm for reducing the satisfiability of sets with symbolic cardinalities to constraints on constant cardinalities, and give a polynomial-space algorithm for the resulting problem.   In a quest for more efficient fragments, we identify several subclasses of sets with cardinality constraints whose satisfiability is NP-hard. Finally, we identify a class of constraints that has polynomial-time satisfiability and entailment problems and can serve as a foundation for efficient program analysis.",
        "published": "2005-08-28T22:25:22Z",
        "link": "http://arxiv.org/abs/cs/0508123v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SE"
        ]
    },
    {
        "title": "A Model-driven Approach for Grid Services Engineering",
        "authors": [
            "David Manset",
            "Richard McClatchey",
            "Flavio Oquendo",
            "Herve Verjus"
        ],
        "summary": "As a consequence to the hype of Grid computing, such systems have seldom been designed using formal techniques. The complexity and rapidly growing demand around Grid technologies has favour the use of classical development techniques, resulting in no guidelines or rules and unstructured engineering processes. This paper advocates a formal approach to Grid applications development in an effort to contribute to the rigorous development of Grids software architectures. This approach addresses cross-platform interoperability and quality of service; the model-driven paradigm is applied to a formal architecture-centric engineering method in order to benefit from the formal semantic description power in addition to model-based transformations. The result of such a novel combined concept promotes the re-use of design models and eases developments in Grid computing by providing an adapted development process and ensuring correctness at each design step.",
        "published": "2005-09-21T17:39:19Z",
        "link": "http://arxiv.org/abs/cs/0509066v2",
        "categories": [
            "cs.SE",
            "cs.DC",
            "D.2.11"
        ]
    },
    {
        "title": "Telling Great Stories: An NSDL Content and Communications System for   Aggregation, Display, and Distribution of News and Features",
        "authors": [
            "Carol Minton Morris"
        ],
        "summary": "Education digital libraries contain cataloged resources as well as contextual information about innovations in the use of educational technology, exemplar stories about community activities, and news from various user communities that include teachers, students, scholars, and developers. Long-standing library traditions of service, preservation, democratization of knowledge, rich discourse, equal access, and fair use are evident in library communications models that both pull in and push out contextual information from multiple sources integrated with editorial production processes. This paper argues that a dynamic narrative flow [1] is enabled by effective management of complex content and communications in a decentralized web-based education digital library making publishing objects such as aggregations of resources, or selected parts of objects [4] accessible through a Content and Communications System. Providing services that encourage patrons to reuse, reflect out, and contribute resources back [5] to the Library increases the reach and impact of the National Science Digital Library (NSDL). This system is a model for distributed content development and effective communications for education digital libraries in general.",
        "published": "2005-09-28T16:28:19Z",
        "link": "http://arxiv.org/abs/cs/0509094v2",
        "categories": [
            "cs.DL",
            "cs.SE",
            "D.2.10"
        ]
    },
    {
        "title": "Diophantus' 20th Problem and Fermat's Last Theorem for n=4:   Formalization of Fermat's Proofs in the Coq Proof Assistant",
        "authors": [
            "David Delahaye",
            "Micaela Mayero"
        ],
        "summary": "We present the proof of Diophantus' 20th problem (book VI of Diophantus' Arithmetica), which consists in wondering if there exist right triangles whose sides may be measured as integers and whose surface may be a square. This problem was negatively solved by Fermat in the 17th century, who used the \"wonderful\" method (ipse dixit Fermat) of infinite descent. This method, which is, historically, the first use of induction, consists in producing smaller and smaller non-negative integer solutions assuming that one exists; this naturally leads to a reductio ad absurdum reasoning because we are bounded by zero. We describe the formalization of this proof which has been carried out in the Coq proof assistant. Moreover, as a direct and no less historical application, we also provide the proof (by Fermat) of Fermat's last theorem for n=4, as well as the corresponding formalization made in Coq.",
        "published": "2005-10-04T08:53:10Z",
        "link": "http://arxiv.org/abs/cs/0510011v1",
        "categories": [
            "cs.LO",
            "cs.SE",
            "math.NT"
        ]
    },
    {
        "title": "From General Systems to Soft Systems to Soft Computing: Applications for   Large and Complex Real World Systems",
        "authors": [
            "Prashant"
        ],
        "summary": "This is article is taken out.",
        "published": "2005-11-03T23:29:55Z",
        "link": "http://arxiv.org/abs/cs/0511018v2",
        "categories": [
            "cs.SE"
        ]
    },
    {
        "title": "Logarithmic growth dynamics in software networks",
        "authors": [
            "Sergi Valverde",
            "Ricard V. Sole"
        ],
        "summary": "In a recent paper, Krapivsky and Redner (Phys. Rev. E, 71 (2005) 036118) proposed a new growing network model with new nodes being attached to a randomly selected node, as well to all ancestors of the target node. The model leads to a sparse graph with an average degree growing logarithmically with the system size. Here we present compeling evidence for software networks being the result of a similar class of growing dynamics. The predicted pattern of network growth, as well as the stationary in- and out-degree distributions are consistent with the model. Our results confirm the view of large-scale software topology being generated through duplication-rewiring mechanisms. Implications of these findings are outlined.",
        "published": "2005-11-07T19:12:11Z",
        "link": "http://arxiv.org/abs/physics/0511064v1",
        "categories": [
            "physics.soc-ph",
            "cond-mat.dis-nn",
            "cs.SE"
        ]
    },
    {
        "title": "Amazing geometry of genetic space or are genetic algorithms convergent?",
        "authors": [
            "Marek W. Gutowski"
        ],
        "summary": "There is no proof yet of convergence of Genetic Algorithms. We do not supply it too. Instead, we present some thoughts and arguments to convince the Reader, that Genetic Algorithms are essentially bound for success. For this purpose, we consider only the crossover operators, single- or multiple-point, together with selection procedure. We also give a proof that the soft selection is superior to other selection schemes.",
        "published": "2005-12-05T11:12:42Z",
        "link": "http://arxiv.org/abs/cs/0512019v1",
        "categories": [
            "cs.NE",
            "cs.DM",
            "cs.SE",
            "F.2.2; G.1.6; G.4; I.1.2"
        ]
    },
    {
        "title": "Stochastic Differential Games in a Non-Markovian Setting",
        "authors": [
            "Erhan Bayraktar",
            "H. Vincent Poor"
        ],
        "summary": "Stochastic differential games are considered in a non-Markovian setting. Typically, in stochastic differential games the modulating process of the diffusion equation describing the state flow is taken to be Markovian. Then Nash equilibria or other types of solution such as Pareto equilibria are constructed using Hamilton-Jacobi-Bellman (HJB) equations. But in a non-Markovian setting the HJB method is not applicable. To examine the non-Markovian case, this paper considers the situation in which the modulating process is a fractional Brownian motion. Fractional noise calculus is used for such models to find the Nash equilibria explicitly. Although fractional Brownian motion is taken as the modulating process because of its versatility in modeling in the fields of finance and networks, the approach in this paper has the merit of being applicable to more general Gaussian stochastic differential games with only slight conceptual modifications. This work has applications in finance to stock price modeling which incorporates the effect of institutional investors, and to stochastic differential portfolio games in markets in which the stock prices follow diffusions modulated with fractional Brownian motion.",
        "published": "2005-01-21T20:36:39Z",
        "link": "http://arxiv.org/abs/cs/0501052v2",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT"
        ]
    },
    {
        "title": "Arbitrage in Fractal Modulated Markets When the Volatility is Stochastic",
        "authors": [
            "Erhan Bayraktar",
            "H. Vincent Poor"
        ],
        "summary": "In this paper an arbitrage strategy is constructed for the modified Black-Scholes model driven by fractional Brownian motion or by a time changed fractional Brownian motion, when the volatility is stochastic. This latter property allows the heavy tailedness of the log returns of the stock prices to be also accounted for in addition to the long range dependence introduced by the fractional Brownian motion. Work has been done previously on this problem for the case with constant `volatility' and without a time change; here these results are extended to the case of stochastic volatility models when the modulator is fractional Brownian motion or a time change of it. (Volatility in fractional Black-Scholes models does not carry the same meaning as in the classic Black-Scholes framework, which is made clear in the text.)   Since fractional Brownian motion is not a semi-martingale, the Black-Scholes differential equation is not well-defined sense for arbitrary predictable volatility processes. However, it is shown here that any almost surely continuous and adapted process having zero quadratic variation can act as an integrator over functions of the integrator and over the family of continuous adapted semi-martingales. Moreover it is shown that the integral also has zero quadratic variation, and therefore that the integral itself can be an integrator. This property of the integral is crucial in developing the arbitrage strategy. Since fractional Brownian motion and a time change of fractional Brownian motion have zero quadratic variation, these results are applicable to these cases in particular. The appropriateness of fractional Brownian motion as a means of modeling stock price returns is discussed as well.",
        "published": "2005-01-22T23:51:01Z",
        "link": "http://arxiv.org/abs/cs/0501054v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT"
        ]
    },
    {
        "title": "Consistency Problems for Jump-Diffusion Models",
        "authors": [
            "Erhan Bayraktar",
            "Li Chen",
            "H. Vincent Poor"
        ],
        "summary": "In this paper consistency problems for multi-factor jump-diffusion models, where the jump parts follow multivariate point processes are examined. First the gap between jump-diffusion models and generalized Heath-Jarrow-Morton (HJM) models is bridged. By applying the drift condition for a generalized arbitrage-free HJM model, the consistency condition for jump-diffusion models is derived. Then we consider a case in which the forward rate curve has a separable structure, and obtain a specific version of the general consistency condition. In particular, a necessary and sufficient condition for a jump-diffusion model to be affine is provided. Finally the Nelson-Siegel type of forward curve structures is discussed. It is demonstrated that under regularity condition, there exists no jump-diffusion model consistent with the Nelson-Siegel curves.",
        "published": "2005-01-23T00:12:44Z",
        "link": "http://arxiv.org/abs/cs/0501055v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "math.IT"
        ]
    },
    {
        "title": "Identification of complex systems in the basis of wavelets",
        "authors": [
            "Alexander Shaydurov"
        ],
        "summary": "In this paper is proposed the method of the identification of complex dynamic systems. Method can be used for the identification of linear and nonlinear complex dynamic systems for the determined or stochastic signals at the inputs and the outputs. It is proposed to use a basis of wavelets for obtaining the impulse transient function (ITF) of system. ITF is considered in the form of surface in the 3D space. Are given the results of experiments on the identification of systems in the basis of wavelets.",
        "published": "2005-02-02T00:12:45Z",
        "link": "http://arxiv.org/abs/cs/0502007v1",
        "categories": [
            "cs.CE",
            "cs.NE",
            "J.2, J.6"
        ]
    },
    {
        "title": "Scientific Data Management in the Coming Decade",
        "authors": [
            "Jim Gray",
            "David T. Liu",
            "Maria Nieto-Santisteban",
            "Alexander S. Szalay",
            "David DeWitt",
            "Gerd Heber"
        ],
        "summary": "This is a thought piece on data-intensive science requirements for databases and science centers. It argues that peta-scale datasets will be housed by science centers that provide substantial storage and processing for scientists who access the data via smart notebooks. Next-generation science instruments and simulations will generate these peta-scale datasets. The need to publish and share data and the need for generic analysis and visualization tools will finally create a convergence on common metadata standards. Database systems will be judged by their support of these metadata standards and by their ability to manage and access peta-scale datasets. The procedural stream-of-bytes-file-centric approach to data analysis is both too cumbersome and too serial for such large datasets. Non-procedural query and analysis of schematized self-describing data is both easier to use and allows much more parallelism.",
        "published": "2005-02-02T03:15:42Z",
        "link": "http://arxiv.org/abs/cs/0502008v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Where the Rubber Meets the Sky: Bridging the Gap between Databases and   Science",
        "authors": [
            "Jim Gray",
            "Alexander S. Szalay"
        ],
        "summary": "Scientists in all domains face a data avalanche - both from better instruments and from improved simulations. We believe that computer science tools and computer scientists are in a position to help all the sciences by building tools and developing techniques to manage, analyze, and visualize peta-scale scientific information. This article is summarizes our experiences over the last seven years trying to bridge the gap between database technology and the needs of the astronomy community in building the World-Wide Telescope.",
        "published": "2005-02-02T04:40:55Z",
        "link": "http://arxiv.org/abs/cs/0502011v1",
        "categories": [
            "cs.DB",
            "cs.CE"
        ]
    },
    {
        "title": "Can Computer Algebra be Liberated from its Algebraic Yoke ?",
        "authors": [
            "R. Barrere"
        ],
        "summary": "So far, the scope of computer algebra has been needlessly restricted to exact algebraic methods. Its possible extension to approximate analytical methods is discussed. The entangled roles of functional analysis and symbolic programming, especially the functional and transformational paradigms, are put forward. In the future, algebraic algorithms could constitute the core of extended symbolic manipulation systems including primitives for symbolic approximations.",
        "published": "2005-02-03T17:28:01Z",
        "link": "http://arxiv.org/abs/cs/0502015v1",
        "categories": [
            "cs.SC",
            "cs.CE",
            "G.4; I.1; I.6"
        ]
    },
    {
        "title": "Learning nonsingular phylogenies and hidden Markov models",
        "authors": [
            "Elchanan Mossel",
            "Sébastien Roch"
        ],
        "summary": "In this paper we study the problem of learning phylogenies and hidden Markov models. We call a Markov model nonsingular if all transition matrices have determinants bounded away from 0 (and 1). We highlight the role of the nonsingularity condition for the learning problem. Learning hidden Markov models without the nonsingularity condition is at least as hard as learning parity with noise, a well-known learning problem conjectured to be computationally hard. On the other hand, we give a polynomial-time algorithm for learning nonsingular phylogenies and hidden Markov models.",
        "published": "2005-02-18T01:31:53Z",
        "link": "http://arxiv.org/abs/cs/0502076v2",
        "categories": [
            "cs.LG",
            "cs.CE",
            "math.PR",
            "math.ST",
            "q-bio.PE",
            "stat.TH",
            "60J10, 60J20, 68T05, 92B10 (Primary)"
        ]
    },
    {
        "title": "Self-Replicating Strands that Self-Assemble into User-Specified Meshes",
        "authors": [
            "Robert Ewaschuk",
            "Peter Turney"
        ],
        "summary": "It has been argued that a central objective of nanotechnology is to make products inexpensively, and that self-replication is an effective approach to very low-cost manufacturing. The research presented here is intended to be a step towards this vision. In previous work (JohnnyVon 1.0), we simulated machines that bonded together to form self-replicating strands. There were two types of machines (called types 0 and 1), which enabled strands to encode arbitrary bit strings. However, the information encoded in the strands had no functional role in the simulation. The information was replicated without being interpreted, which was a significant limitation for potential manufacturing applications. In the current work (JohnnyVon 2.0), the information in a strand is interpreted as instructions for assembling a polygonal mesh. There are now four types of machines and the information encoded in a strand determines how it folds. A strand may be in an unfolded state, in which the bonds are straight (although they flex slightly due to virtual forces acting on the machines), or in a folded state, in which the bond angles depend on the types of machines. By choosing the sequence of machine types in a strand, the user can specify a variety of polygonal shapes. A simulation typically begins with an initial unfolded seed strand in a soup of unbonded machines. The seed strand replicates by bonding with free machines in the soup. The child strands fold into the encoded polygonal shape, and then the polygons drift together and bond to form a mesh. We demonstrate that a variety of polygonal meshes can be manufactured in the simulation, by simply changing the sequence of machine types in the seed.",
        "published": "2005-02-22T16:53:15Z",
        "link": "http://arxiv.org/abs/cs/0502087v1",
        "categories": [
            "cs.NE",
            "cs.CE",
            "cs.MA",
            "I.6.3; I.6.8; J.2; J.3"
        ]
    },
    {
        "title": "The Peculiarities of Nonstationary Formation of Inhomogeneous Structures   of Charged Particles in the Electrodiffusion Processes",
        "authors": [
            "P. Nefyodov",
            "V. Reztsov",
            "O. Riabinina"
        ],
        "summary": "In this paper the distribution of charged particles is constructed under the approximation of ambipolar diffusion. The results of mathematical modelling in two-dimensional case taking into account the velocities of the system are presented.",
        "published": "2005-03-30T07:00:29Z",
        "link": "http://arxiv.org/abs/cs/0503084v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Dynamic Simulation of Construction Machinery: Towards an Operator Model",
        "authors": [
            "Reno Filla",
            "Allan Ericsson",
            "Jan-Ove Palmberg"
        ],
        "summary": "In dynamic simulation of complete wheel loaders, one interesting aspect, specific for the working task, is the momentary power distribution between drive train and hydraulics, which is balanced by the operator.   This paper presents the initial results to a simulation model of a human operator. Rather than letting the operator model follow a predefined path with control inputs at given points, it follows a collection of general rules that together describe the machine's working cycle in a generic way. The advantage of this is that the working task description and the operator model itself are independent of the machine's technical parameters. Complete sub-system characteristics can thus be changed without compromising the relevance and validity of the simulation. Ultimately, this can be used to assess a machine's total performance, fuel efficiency and operability already in the concept phase of the product development process.",
        "published": "2005-03-30T20:52:11Z",
        "link": "http://arxiv.org/abs/cs/0503087v4",
        "categories": [
            "cs.CE",
            "I.6.3; I.6.5; J.6"
        ]
    },
    {
        "title": "Virtual Observatory: From Concept to Implementation",
        "authors": [
            "S. G. Djorgovski",
            "R. Williams"
        ],
        "summary": "We review the origins of the Virtual Observatory (VO) concept, and the current status of the efforts in this field. VO is the response of the astronomical community to the challenges posed by the modern massive and complex data sets. It is a framework in which information technology is harnessed to organize, maintain, and explore the rich information content of the exponentially growing data sets, and to enable a qualitatively new science to be done with them. VO will become a complete, open, distributed, web-based framework for astronomy of the early 21st century. A number of significant efforts worldwide are now striving to convert this vision into reality. The technological and methodological challenges posed by the information-rich astronomy are also common to many other fields. We see a fundamental change in the way all science is done, driven by the information technology revolution.",
        "published": "2005-04-01T00:57:22Z",
        "link": "http://arxiv.org/abs/astro-ph/0504006v1",
        "categories": [
            "astro-ph",
            "cs.CE"
        ]
    },
    {
        "title": "Point process model of 1/f noise versus a sum of Lorentzians",
        "authors": [
            "B. Kaulakys",
            "V. Gontis",
            "M. Alaburda"
        ],
        "summary": "We present a simple point process model of $1/f^{\\beta}$ noise, covering different values of the exponent $\\beta$. The signal of the model consists of pulses or events. The interpulse, interevent, interarrival, recurrence or waiting times of the signal are described by the general Langevin equation with the multiplicative noise and stochastically diffuse in some interval resulting in the power-law distribution. Our model is free from the requirement of a wide distribution of relaxation times and from the power-law forms of the pulses. It contains only one relaxation rate and yields $1/f^ {\\beta}$ spectra in a wide range of frequency. We obtain explicit expressions for the power spectra and present numerical illustrations of the model. Further we analyze the relation of the point process model of $1/f$ noise with the Bernamont-Surdin-McWhorter model, representing the signals as a sum of the uncorrelated components. We show that the point process model is complementary to the model based on the sum of signals with a wide-range distribution of the relaxation times. In contrast to the Gaussian distribution of the signal intensity of the sum of the uncorrelated components, the point process exhibits asymptotically a power-law distribution of the signal intensity. The developed multiplicative point process model of $1/f^{\\beta}$ noise may be used for modeling and analysis of stochastic processes in different systems with the power-law distribution of the intensity of pulsing signals.",
        "published": "2005-04-01T14:16:25Z",
        "link": "http://arxiv.org/abs/cond-mat/0504025v1",
        "categories": [
            "cond-mat.stat-mech",
            "astro-ph",
            "cond-mat.dis-nn",
            "cs.CE",
            "math.ST",
            "nlin.AO",
            "physics.data-an",
            "q-bio.NC",
            "stat.TH"
        ]
    },
    {
        "title": "A Short Proof that Phylogenetic Tree Reconstruction by Maximum   Likelihood is Hard",
        "authors": [
            "S. Roch"
        ],
        "summary": "Maximum likelihood is one of the most widely used techniques to infer evolutionary histories. Although it is thought to be intractable, a proof of its hardness has been lacking. Here, we give a short proof that computing the maximum likelihood tree is NP-hard by exploiting a connection between likelihood and parsimony observed by Tuffley and Steel.",
        "published": "2005-04-19T08:36:17Z",
        "link": "http://arxiv.org/abs/math/0504378v2",
        "categories": [
            "math.PR",
            "cs.CC",
            "cs.CE",
            "math.ST",
            "q-bio.PE",
            "stat.TH"
        ]
    },
    {
        "title": "Modelling investment in artificial stock markets: Analytical and   Numerical Results",
        "authors": [
            "Roberto da Silva",
            "Alexandre Tavares Baraviera",
            "Silvio R. Dahmen"
        ],
        "summary": "In this article we study the behavior of a group of economic agents in the context of cooperative game theory, interacting according to rules based on the Potts Model with suitable modifications. Each agent can be thought of as belonging to a chain, where agents can only interact with their nearest neighbors (periodic boundary conditions are imposed). Each agent can invest an amount &#963;_{i}=0,...,q-1. Using the transfer matrix method we study analytically, among other things, the behavior of the investment as a function of a control parameter (denoted &#946;) for the cases q=2 and 3. For q>3 numerical evaluation of eigenvalues and high precision numerical derivatives are used in order to assess this information.",
        "published": "2005-04-29T20:38:42Z",
        "link": "http://arxiv.org/abs/cs/0505001v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Data Mining on Crash Simulation Data",
        "authors": [
            "A. Kuhlmann",
            "R. -M. Vetter",
            "Ch. Luebbing",
            "C. -A. Thole"
        ],
        "summary": "The work presented in this paper is part of the cooperative research project AUTO-OPT carried out by twelve partners from the automotive industries. One major work package concerns the application of data mining methods in the area of automotive design. Suitable methods for data preparation and data analysis are developed. The objective of the work is the re-use of data stored in the crash-simulation department at BMW in order to gain deeper insight into the interrelations between the geometric variations of the car during its design and its performance in crash testing. In this paper a method for data analysis of finite element models and results from crash simulation is proposed and application to recent data from the industrial partner BMW is demonstrated. All necessary steps from data pre-processing to re-integration into the working environment of the engineer are covered.",
        "published": "2005-05-02T15:27:45Z",
        "link": "http://arxiv.org/abs/cs/0505008v1",
        "categories": [
            "cs.IR",
            "cs.CE",
            "H.2.8; D.2.2"
        ]
    },
    {
        "title": "Parallel Programming with Matrix Distributed Processing",
        "authors": [
            "Massimo Di Pierro"
        ],
        "summary": "Matrix Distributed Processing (MDP) is a C++ library for fast development of efficient parallel algorithms. It constitues the core of FermiQCD. MDP enables programmers to focus on algorithms, while parallelization is dealt with automatically and transparently. Here we present a brief overview of MDP and examples of applications in Computer Science (Cellular Automata), Engineering (PDE Solver) and Physics (Ising Model).",
        "published": "2005-05-09T16:03:21Z",
        "link": "http://arxiv.org/abs/hep-lat/0505005v1",
        "categories": [
            "hep-lat",
            "cs.CE",
            "physics.comp-ph"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Scouting Red Beds for Uncommon Features with   Geological Significance",
        "authors": [
            "Patrick C. McGuire",
            "Enrique Diaz-Martinez",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose A. Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "The `Cyborg Astrobiologist' (CA) has undergone a second geological field trial, at a red sandstone site in northern Guadalajara, Spain, near Riba de Santiuste. The Cyborg Astrobiologist is a wearable computer and video camera system that has demonstrated a capability to find uncommon interest points in geological imagery in real-time in the field. The first (of three) geological structures that we studied was an outcrop of nearly homogeneous sandstone, which exhibits oxidized-iron impurities in red and and an absence of these iron impurities in white. The white areas in these ``red beds'' have turned white because the iron has been removed by chemical reduction, perhaps by a biological agent. The computer vision system found in one instance several (iron-free) white spots to be uncommon and therefore interesting, as well as several small and dark nodules. The second geological structure contained white, textured mineral deposits on the surface of the sandstone, which were found by the CA to be interesting. The third geological structure was a 50 cm thick paleosol layer, with fossilized root structures of some plants, which were found by the CA to be interesting. A quasi-blind comparison of the Cyborg Astrobiologist's interest points for these images with the interest points determined afterwards by a human geologist shows that the Cyborg Astrobiologist concurred with the human geologist 68% of the time (true positive rate), with a 32% false positive rate and a 32% false negative rate.   (abstract has been abridged).",
        "published": "2005-05-23T09:55:37Z",
        "link": "http://arxiv.org/abs/cs/0505058v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "physics.ins-det",
            "q-bio.NC",
            "I.2.10; I.4.6; I.4.8; I.4.9; I.2.9; I.5.4; I.5.5; J.2; J.3; D.2;\n  D.1.7; D.4.7"
        ]
    },
    {
        "title": "Sparse Covariance Selection via Robust Maximum Likelihood Estimation",
        "authors": [
            "Onureena Banerjee",
            "Alexandre d'Aspremont",
            "Laurent El Ghaoui"
        ],
        "summary": "We address a problem of covariance selection, where we seek a trade-off between a high likelihood against the number of non-zero elements in the inverse covariance matrix. We solve a maximum likelihood problem with a penalty term given by the sum of absolute values of the elements of the inverse covariance matrix, and allow for imposing bounds on the condition number of the solution. The problem is directly amenable to now standard interior-point algorithms for convex optimization, but remains challenging due to its size. We first give some results on the theoretical computational complexity of the problem, by showing that a recent methodology for non-smooth convex optimization due to Nesterov can be applied to this problem, to greatly improve on the complexity estimate given by interior-point algorithms. We then examine two practical algorithms aimed at solving large-scale, noisy (hence dense) instances: one is based on a block-coordinate descent approach, where columns and rows are updated sequentially, another applies a dual version of Nesterov's method.",
        "published": "2005-06-08T21:08:38Z",
        "link": "http://arxiv.org/abs/cs/0506023v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "F.2.1; G.1.3; G.1.6; G.3; J.3"
        ]
    },
    {
        "title": "An Event-driven Operator Model for Dynamic Simulation of Construction   Machinery",
        "authors": [
            "Reno Filla"
        ],
        "summary": "Prediction and optimisation of a wheel loader's dynamic behaviour is a challenge due to tightly coupled, non-linear subsystems of different technical domains. Furthermore, a simulation regarding performance, efficiency, and operability cannot be limited to the machine itself, but has to include operator, environment, and work task. This paper presents some results of our approach to an event-driven simulation model of a human operator. Describing the task and the operator model independently of the machine's technical parameters, gives the possibility to change whole sub-system characteristics without compromising the relevance and validity of the simulation.",
        "published": "2005-06-10T10:35:14Z",
        "link": "http://arxiv.org/abs/cs/0506033v4",
        "categories": [
            "cs.CE",
            "I.6.3; I.6.5; J.6"
        ]
    },
    {
        "title": "A Taxonomy of Data Grids for Distributed Data Sharing, Management and   Processing",
        "authors": [
            "Srikumar Venugopal",
            "Rajkumar Buyya",
            "Kotagiri Ramamohanarao"
        ],
        "summary": "Data Grids have been adopted as the platform for scientific communities that need to share, access, transport, process and manage large data collections distributed worldwide. They combine high-end computing technologies with high-performance networking and wide-area storage management techniques. In this paper, we discuss the key concepts behind Data Grids and compare them with other data sharing and distribution paradigms such as content delivery networks, peer-to-peer networks and distributed databases. We then provide comprehensive taxonomies that cover various aspects of architecture, data transportation, data replication and resource allocation and scheduling. Finally, we map the proposed taxonomy to various Data Grid systems not only to validate the taxonomy but also to identify areas for future exploration. Through this taxonomy, we aim to categorise existing systems to better understand their goals and their methodology. This would help evaluate their applicability for solving similar problems. This taxonomy also provides a \"gap analysis\" of this area through which researchers can potentially identify new issues for investigation. Finally, we hope that the proposed taxonomy and mapping also helps to provide an easy way for new practitioners to understand this complex area of research.",
        "published": "2005-06-10T10:59:37Z",
        "link": "http://arxiv.org/abs/cs/0506034v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "A.1; C.2.4; J.2"
        ]
    },
    {
        "title": "Comparison of two different implementations of a   finite-difference-method for first-order pde in mathematica and matlab",
        "authors": [
            "Heiko Herrmann",
            "Gunnar Rueckner"
        ],
        "summary": "In this article two implementations of a symmetric finite difference algorithm for a first-order partial differential equation are discussed. The considered partial differential equation discribes the time evolution of the crack length distribution of microcracks in brittle materia.",
        "published": "2005-06-13T15:26:16Z",
        "link": "http://arxiv.org/abs/cs/0506051v2",
        "categories": [
            "cs.CE",
            "cs.DM"
        ]
    },
    {
        "title": "Field geology with a wearable computer: 1st results of the Cyborg   Astrobiologist System",
        "authors": [
            "Patrick C. McGuire",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Markus Oesker",
            "Robert Haschke",
            "Joerg Ontrup",
            "Helge Ritter"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. This work is more of a test of the whole system, rather than of any one part of the system. However, beyond the concept of the system itself, the uncommon map (despite its simplicity) is the main innovative part of the system. The uncommon map helps to determine interest-points in a context-free manner. Overall, the hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner. We use these capabilities for autonomous guidance towards geological points-of-interest.",
        "published": "2005-06-24T10:25:22Z",
        "link": "http://arxiv.org/abs/cs/0506089v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO"
        ]
    },
    {
        "title": "Comparison of Resampling Schemes for Particle Filtering",
        "authors": [
            "Randal Douc",
            "Olivier Cappé",
            "Eric Moulines"
        ],
        "summary": "This contribution is devoted to the comparison of various resampling approaches that have been proposed in the literature on particle filtering. It is first shown using simple arguments that the so-called residual and stratified methods do yield an improvement over the basic multinomial resampling approach. A simple counter-example showing that this property does not hold true for systematic resampling is given. Finally, some results on the large-sample behavior of the simple bootstrap filter algorithm are given. In particular, a central limit theorem is established for the case where resampling is performed using the residual approach.",
        "published": "2005-07-08T15:14:51Z",
        "link": "http://arxiv.org/abs/cs/0507025v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "ReacProc: A Tool to Process Reactions Describing Particle Interactions",
        "authors": [
            "A. S. Siver"
        ],
        "summary": "ReacProc is a program written in C/C++ programming language which can be used (1) to check out of reactions describing particles interactions against conservation laws and (2) to reduce input reaction into some canonical form. A table with particles properties is available within ReacProc package.",
        "published": "2005-07-21T14:17:47Z",
        "link": "http://arxiv.org/abs/cs/0507055v2",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Investigations of Process Damping Forces in Metal Cutting",
        "authors": [
            "Emily Stone",
            "Suhail Ahmed",
            "Abe Askari",
            "Hong Tat"
        ],
        "summary": "Using finite element software developed for metal cutting by Third Wave Systems we investigate the forces involved in chatter, a self-sustained oscillation of the cutting tool. The phenomena is decomposed into a vibrating tool cutting a flat surface work piece, and motionless tool cutting a work piece with a wavy surface. While cutting the wavy surface, the shearplane was seen to oscillate in advance of the oscillation of the depth of cut, as were the cutting, thrust, and shear plane forces. The vibrating tool was used to investigate process damping through the interaction of the relief face of the tool and the workpiece. Crushing forces are isolated and compared to the contact length between the tool and workpiece. We found that the wavelength dependence of the forces depended on the relative size of the wavelength to the length of the relief face of the tool. The results indicate that the damping force from crushing will be proportional to the cutting speed for short tools, and inversely proportional for long tools.",
        "published": "2005-08-23T19:52:38Z",
        "link": "http://arxiv.org/abs/cs/0508102v1",
        "categories": [
            "cs.CE",
            "I.6.3; I.6.4"
        ]
    },
    {
        "title": "Long-term neuronal behavior caused by two synaptic modification   mechanisms",
        "authors": [
            "Xi Shen",
            "Philippe De Wilde"
        ],
        "summary": "We report the first results of simulating the coupling of neuronal, astrocyte, and cerebrovascular activity. It is suggested that the dynamics of the system is different from systems that only include neurons. In the neuron-vascular coupling, distribution of synapse strengths affects neuronal behavior and thus balance of the blood flow; oscillations are induced in the neuron-to-astrocyte coupling.",
        "published": "2005-08-26T10:38:36Z",
        "link": "http://arxiv.org/abs/cs/0508117v1",
        "categories": [
            "cs.NE",
            "cs.CE"
        ]
    },
    {
        "title": "Component Based Programming in Scientific Computing: The Viable Approach",
        "authors": [
            "Zsolt I. Lázár",
            "Jouke R. Heringa",
            "Bazil Pârv",
            "Simon W. de Leeuw"
        ],
        "summary": "Computational scientists are facing a new era where the old ways of developing and reusing code have to be left behind and a few daring steps are to be made towards new horizons. The present work analyzes the needs that drive this change, the factors that contribute to the inertia of the community and slow the transition, the status and perspective of present attempts, the principle, practical and technical problems that are to be addressed in the short and long run.",
        "published": "2005-08-31T21:57:04Z",
        "link": "http://arxiv.org/abs/cs/0509002v1",
        "categories": [
            "cs.CE",
            "D.2.13; D.2.12; D.2.11; D.2.9; D.2.6"
        ]
    },
    {
        "title": "COMODI: Architecture for a Component-Based Scientific Computing System",
        "authors": [
            "Zsolt I. Lázár",
            "Lehel István Kovács",
            "Bazil Pârv"
        ],
        "summary": "The COmputational MODule Integrator (COMODI) is an initiative aiming at a component based framework, component developer tool and component repository for scientific computing. We identify the main ingredients to a solution that would be sufficiently appealing to scientists and engineers to consider alternatives to their deeply rooted programming traditions. The overall structure of the complete solution is sketched with special emphasis on the Component Developer Tool standing at the basis of COMODI.",
        "published": "2005-08-31T22:48:40Z",
        "link": "http://arxiv.org/abs/cs/0509003v1",
        "categories": [
            "cs.CE",
            "D.2.13; D.2.12; D.2.11; D.2.9; D.2.6"
        ]
    },
    {
        "title": "Kriging Scenario For Capital Markets",
        "authors": [
            "T. Suslo"
        ],
        "summary": "An introduction to numerical statistics.",
        "published": "2005-09-05T08:04:06Z",
        "link": "http://arxiv.org/abs/cs/0509012v5",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "Traders imprint themselves by adaptively updating their own avatar",
        "authors": [
            "Gilles Daniel",
            "Lev Muchnik",
            "Sorin Solomon"
        ],
        "summary": "Simulations of artificial stock markets were considered as early as 1964 and multi-agent ones were introduced as early as 1989. Starting the early 90's, collaborations of economists and physicists produced increasingly realistic simulation platforms. Currently, the market stylized facts are easily reproduced and one has now to address the realistic details of the Market Microstructure and of the Traders Behaviour. This calls for new methods and tools capable of bridging smoothly between simulations and experiments in economics.   We propose here the following Avatar-Based Method (ABM). The subjects implement and maintain their Avatars (programs encoding their personal decision making procedures) on NatLab, a market simulation platform. Once these procedures are fed in a computer edible format, they can be operationally used as such without the need for belabouring, interpreting or conceptualising them. Thus ABM short-circuits the usual behavioural economics experiments that search for the psychological mechanisms underlying the subjects behaviour. Finally, ABM maintains a level of objectivity close to the classical behaviourism while extending its scope to subjects' decision making mechanisms.   We report on experiments where Avatars designed and maintained by humans from different backgrounds (including real traders) compete in a continuous double-auction market. We hope this unbiased way of capturing the adaptive evolution of real subjects behaviour may lead to a new kind of behavioural economics experiments with a high degree of reliability, analysability and reproducibility.",
        "published": "2005-09-06T17:02:52Z",
        "link": "http://arxiv.org/abs/cs/0509017v1",
        "categories": [
            "cs.MA",
            "cs.CE"
        ]
    },
    {
        "title": "Projecting the Forward Rate Flow onto a Finite Dimensional Manifold",
        "authors": [
            "Erhan Bayraktar",
            "Li Chen",
            "H. Vincent Poor"
        ],
        "summary": "Given a Heath-Jarrow-Morton (HJM) interest rate model $\\mathcal{M}$ and a parametrized family of finite dimensional forward rate curves $\\mathcal{G}$, this paper provides a technique for projecting the infinite dimensional forward rate curve $r_{t}$ given by $\\mathcal{M}$ onto the finite dimensional manifold $\\mathcal{G}$.The Stratonovich dynamics of the projected finite dimensional forward curve are derived and it is shown that, under the regularity conditions, the given Stratonovich differential equation has a unique strong solution. Moreover, this projection leads to an efficient algorithm for implicit parametric estimation of the infinite dimensional HJM model. The feasibility of this method is demonstrated by applying the generalized method of moments.",
        "published": "2005-09-10T14:01:02Z",
        "link": "http://arxiv.org/abs/cs/0509028v1",
        "categories": [
            "cs.CE",
            "cs.IT",
            "math.IT",
            "G.3"
        ]
    },
    {
        "title": "Quickest detection of a minimum of disorder times",
        "authors": [
            "Erhan Bayraktar",
            "H. Vincent Poor"
        ],
        "summary": "A multi-source quickest detection problem is considered. Assume there are two independent Poisson processes $X^{1}$ and $X^{2}$ with disorder times $\\theta_{1}$ and $\\theta_{2}$, respectively; that is, the intensities of $X^1$ and $X^2$ change at random unobservable times $\\theta_1$ and $\\theta_2$, respectively. $\\theta_1$ and $\\theta_2$ are independent of each other and are exponentially distributed. Define $\\theta \\triangleq \\theta_1 \\wedge \\theta_2=\\min\\{\\theta_{1},\\theta_{2}\\}$ . For any stopping time $\\tau$ that is measurable with respect to the filtration generated by the observations define a penalty function of the form \\[ R_{\\tau}=\\mathbb{P}(\\tau<\\theta)+c \\mathbb{E}[(\\tau-\\theta)^{+}], \\] where $c>0$ and $(\\tau-\\theta)^{+}$ is the positive part of $\\tau-\\theta$. It is of interest to find a stopping time $\\tau$ that minimizes the above performance index. Since both observations $X^{1}$ and $X^{2}$ reveal information about the disorder time $\\theta$, even this simple problem is more involved than solving the disorder problems for $X^{1}$ and $X^{2}$ separately. This problem is formulated in terms of a three dimensional sufficient statistic, and the corresponding optimal stopping problem is examined. A two dimensional optimal stopping problem whose optimal stopping time turns out to coincide with the optimal stopping time of the original problem for some range of parameters is also solved. The value function of this problem serves as a tight upper bound for the original problem's value function. The two solutions are characterized by iterating suitable functional operators.",
        "published": "2005-09-10T14:24:44Z",
        "link": "http://arxiv.org/abs/cs/0509029v4",
        "categories": [
            "cs.CE",
            "cs.IT",
            "math.IT",
            "G.3"
        ]
    },
    {
        "title": "Underwater Hacker Missile Wars: A Cryptography and Engineering Contest",
        "authors": [
            "Joshua Holden",
            "Richard Layton",
            "Laurence Merkle",
            "Tina Hudson"
        ],
        "summary": "For a recent student conference, the authors developed a day-long design problem and competition suitable for engineering, mathematics and science undergraduates. The competition included a cryptography problem, for which a workshop was run during the conference. This paper describes the competition, focusing on the cryptography problem and the workshop. Notes from the workshop and code for the computer programs are made available via the Internet. The results of a personal self-evaluation (PSE) are described.",
        "published": "2005-09-18T22:11:56Z",
        "link": "http://arxiv.org/abs/cs/0509053v1",
        "categories": [
            "cs.CR",
            "cs.CE"
        ]
    },
    {
        "title": "Evolutionary Trees and the Ising Model on the Bethe Lattice: a Proof of   Steel's Conjecture",
        "authors": [
            "Constantinos Daskalakis",
            "Elchanan Mossel",
            "Sebastien Roch"
        ],
        "summary": "A major task of evolutionary biology is the reconstruction of phylogenetic trees from molecular data. The evolutionary model is given by a Markov chain on a tree. Given samples from the leaves of the Markov chain, the goal is to reconstruct the leaf-labelled tree.   It is well known that in order to reconstruct a tree on $n$ leaves, sample sequences of length $\\Omega(\\log n)$ are needed. It was conjectured by M. Steel that for the CFN/Ising evolutionary model, if the mutation probability on all edges of the tree is less than $p^{\\ast} = (\\sqrt{2}-1)/2^{3/2}$, then the tree can be recovered from sequences of length $O(\\log n)$. The value $p^{\\ast}$ is given by the transition point for the extremality of the free Gibbs measure for the Ising model on the binary tree. Steel's conjecture was proven by the second author in the special case where the tree is \"balanced.\" The second author also proved that if all edges have mutation probability larger than $p^{\\ast}$ then the length needed is $n^{\\Omega(1)}$. Here we show that Steel's conjecture holds true for general trees by giving a reconstruction algorithm that recovers the tree from $O(\\log n)$-length sequences when the mutation probabilities are discretized and less than $p^\\ast$. Our proof and results demonstrate that extremality of the free Gibbs measure on the infinite binary tree, which has been studied before in probability, statistical physics and computer science, determines how distinguishable are Gibbs measures on finite binary trees.",
        "published": "2005-09-23T20:22:09Z",
        "link": "http://arxiv.org/abs/math/0509575v3",
        "categories": [
            "math.PR",
            "cs.CE",
            "cs.DS",
            "math.CA",
            "math.CO",
            "math.ST",
            "q-bio.PE",
            "stat.TH"
        ]
    },
    {
        "title": "Semantics of UML 2.0 Activity Diagram for Business Modeling by Means of   Virtual Machine",
        "authors": [
            "Valdis Vitolins",
            "Audris Kalnins"
        ],
        "summary": "The paper proposes a more formalized definition of UML 2.0 Activity Diagram semantics. A subset of activity diagram constructs relevant for business process modeling is considered. The semantics definition is based on the original token flow methodology, but a more constructive approach is used. The Activity Diagram Virtual machine is defined by means of a metamodel, with operations defined by a mix of pseudocode and OCL pre- and postconditions. A formal procedure is described which builds the virtual machine for any activity diagram. The relatively complicated original token movement rules in control nodes and edges are combined into paths from an action to action. A new approach is the use of different (push and pull) engines, which move tokens along the paths. Pull engines are used for paths containing join nodes, where the movement of several tokens must be coordinated. The proposed virtual machine approach makes the activity semantics definition more transparent where the token movement can be easily traced. However, the main benefit of the approach is the possibility to use the defined virtual machine as a basis for UML activity diagram based workflow or simulation engine.",
        "published": "2005-09-28T12:24:42Z",
        "link": "http://arxiv.org/abs/cs/0509089v1",
        "categories": [
            "cs.CE",
            "cs.PL"
        ]
    },
    {
        "title": "A Market Test for the Positivity of Arrow-Debreu Prices",
        "authors": [
            "Alexandre d'Aspremont"
        ],
        "summary": "We derive tractable necessary and sufficient conditions for the absence of buy-and-hold arbitrage opportunities in a perfectly liquid, one period market. We formulate the positivity of Arrow-Debreu prices as a generalized moment problem to show that this no arbitrage condition is equivalent to the positive semidefiniteness of matrices formed by the market price of tradeable securities and their products. We apply this result to a market with multiple assets and basket call options.",
        "published": "2005-10-11T13:40:17Z",
        "link": "http://arxiv.org/abs/cs/0510027v2",
        "categories": [
            "cs.CE",
            "J.4"
        ]
    },
    {
        "title": "COMODI: On the Graphical User Interface",
        "authors": [
            "Zsolt I. Lázár",
            "Andreea Fanea",
            "Dragoş Petraşcu",
            "Vladiela Ciobotariu-Boer",
            "Bazil Pârv"
        ],
        "summary": "We propose a series of features for the graphical user interface (GUI) of the COmputational MOdule Integrator (COMODI) \\cite{Synasc05a}\\cite{COMODI}. In view of the special requirements that a COMODI type of framework for scientific computing imposes and inspiring from existing solutions that provide advanced graphical visual programming environments, we identify those elements and associated behaviors that will have to find their way into the first release of COMODI.",
        "published": "2005-10-14T01:03:55Z",
        "link": "http://arxiv.org/abs/cs/0510034v1",
        "categories": [
            "cs.HC",
            "cs.CE",
            "cs.MS",
            "D.2.13; D.2.12; D.2.11; D.2.9; D.2.6"
        ]
    },
    {
        "title": "Réflexions sur la question fréquentielle en traitement du signal",
        "authors": [
            "Michel Fliess"
        ],
        "summary": "New definitions are suggested for frequencies which may be instantaneous or not. The Heisenberg-Gabor inequality and the Shannon sampling theorem are briefly discussed.",
        "published": "2005-10-26T14:49:47Z",
        "link": "http://arxiv.org/abs/cs/0510084v1",
        "categories": [
            "cs.CE",
            "cs.IR",
            "math-ph",
            "math.MP",
            "math.SP"
        ]
    },
    {
        "title": "Heat kernel expansion for a family of stochastic volatility models :   delta-geometry",
        "authors": [
            "Bourgade Paul",
            "Croissant Olivier"
        ],
        "summary": "In this paper, we study a family of stochastic volatility processes; this family features a mean reversion term for the volatility and a double CEV-like exponent that generalizes SABR and Heston's models. We derive approximated closed form formulas for the digital prices, the local and implied volatilities. Our formulas are efficient for small maturities.   Our method is based on differential geometry, especially small time diffusions on riemanian spaces. This geometrical point of view can be extended to other processes, and is very accurate to produce variate smiles for small maturities and small moneyness.",
        "published": "2005-11-04T18:31:49Z",
        "link": "http://arxiv.org/abs/cs/0511024v1",
        "categories": [
            "cs.CE"
        ]
    },
    {
        "title": "A linear-time algorithm for finding the longest segment which scores   above a given threshold",
        "authors": [
            "Miklós Csűrös"
        ],
        "summary": "This paper describes a linear-time algorithm that finds the longest stretch in a sequence of real numbers (``scores'') in which the sum exceeds an input parameter. The algorithm also solves the problem of finding the longest interval in which the average of the scores is above a fixed threshold. The problem originates from molecular sequence analysis: for instance, the algorithm can be employed to identify long GC-rich regions in DNA sequences. The algorithm can also be used to trim low-quality ends of shotgun sequences in a preprocessing step of whole-genome assembly.",
        "published": "2005-12-04T04:28:00Z",
        "link": "http://arxiv.org/abs/cs/0512016v2",
        "categories": [
            "cs.DS",
            "cs.CE",
            "F.2.2; G.2; J.3"
        ]
    },
    {
        "title": "Complex Random Vectors and ICA Models: Identifiability, Uniqueness and   Separability",
        "authors": [
            "Jan Eriksson",
            "Visa Koivunen"
        ],
        "summary": "In this paper the conditions for identifiability, separability and uniqueness of linear complex valued independent component analysis (ICA) models are established. These results extend the well-known conditions for solving real-valued ICA problems to complex-valued models. Relevant properties of complex random vectors are described in order to extend the Darmois-Skitovich theorem for complex-valued models. This theorem is used to construct a proof of a theorem for each of the above ICA model concepts. Both circular and noncircular complex random vectors are covered. Examples clarifying the above concepts are presented.",
        "published": "2005-12-15T14:51:36Z",
        "link": "http://arxiv.org/abs/cs/0512063v1",
        "categories": [
            "cs.IT",
            "cs.CE",
            "cs.IR",
            "cs.LG",
            "math.IT"
        ]
    },
    {
        "title": "ADF95: Tool for automatic differentiation of a FORTRAN code designed for   large numbers of independent variables",
        "authors": [
            "Christian W. Straka"
        ],
        "summary": "ADF95 is a tool to automatically calculate numerical first derivatives for any mathematical expression as a function of user defined independent variables. Accuracy of derivatives is achieved within machine precision. ADF95 may be applied to any FORTRAN 77/90/95 conforming code and requires minimal changes by the user. It provides a new derived data type that holds the value and derivatives and applies forward differencing by overloading all FORTRAN operators and intrinsic functions. An efficient indexing technique leads to a reduced memory usage and a substantially increased performance gain over other available tools with operator overloading. This gain is especially pronounced for sparse systems with large number of independent variables. A wide class of numerical simulations, e.g., those employing implicit solvers, can profit from ADF95.",
        "published": "2005-03-04T19:20:04Z",
        "link": "http://arxiv.org/abs/cs/0503014v1",
        "categories": [
            "cs.MS",
            "D.1.5; G1.7; G1.8; G.4; J.2"
        ]
    },
    {
        "title": "TeXmacs-maxima interface",
        "authors": [
            "A. G. Grozin"
        ],
        "summary": "This tutorial presents features of the new and improved TeXmacs-maxima interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or later).",
        "published": "2005-04-11T16:39:16Z",
        "link": "http://arxiv.org/abs/cs/0504039v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "hep-ph"
        ]
    },
    {
        "title": "Estudo e Implementacao de Algoritmos de Roteamento sobre Grafos em um   Sistema de Informacoes Geograficas",
        "authors": [
            "Rudini M. Sampaio",
            "Horacio H. Yanasse"
        ],
        "summary": "This article presents an implementation of a graphical software with various algorithms in Operations research, like minimum path, minimum tree, chinese postman problem and travelling salesman.",
        "published": "2005-05-11T18:50:32Z",
        "link": "http://arxiv.org/abs/cs/0505031v1",
        "categories": [
            "cs.MS",
            "cs.DS"
        ]
    },
    {
        "title": "A Maple Package for Computing Groebner Bases for Linear Recurrence   Relations",
        "authors": [
            "Vladimir P. Gerdt",
            "Daniel Robertz"
        ],
        "summary": "A Maple package for computing Groebner bases of linear difference ideals is described. The underlying algorithm is based on Janet and Janet-like monomial divisions associated with finite difference operators. The package can be used, for example, for automatic generation of difference schemes for linear partial differential equations and for reduction of multiloop Feynman integrals. These two possible applications are illustrated by simple examples of the Laplace equation and a one-loop scalar integral of propagator type",
        "published": "2005-09-22T15:45:35Z",
        "link": "http://arxiv.org/abs/cs/0509070v2",
        "categories": [
            "cs.SC",
            "cs.MS",
            "I.1.4"
        ]
    },
    {
        "title": "COMODI: On the Graphical User Interface",
        "authors": [
            "Zsolt I. Lázár",
            "Andreea Fanea",
            "Dragoş Petraşcu",
            "Vladiela Ciobotariu-Boer",
            "Bazil Pârv"
        ],
        "summary": "We propose a series of features for the graphical user interface (GUI) of the COmputational MOdule Integrator (COMODI) \\cite{Synasc05a}\\cite{COMODI}. In view of the special requirements that a COMODI type of framework for scientific computing imposes and inspiring from existing solutions that provide advanced graphical visual programming environments, we identify those elements and associated behaviors that will have to find their way into the first release of COMODI.",
        "published": "2005-10-14T01:03:55Z",
        "link": "http://arxiv.org/abs/cs/0510034v1",
        "categories": [
            "cs.HC",
            "cs.CE",
            "cs.MS",
            "D.2.13; D.2.12; D.2.11; D.2.9; D.2.6"
        ]
    },
    {
        "title": "Numerical resolution of some BVP using Bernstein polynomials",
        "authors": [
            "Gianluca Argentini"
        ],
        "summary": "In this work we present a method, based on the use of Bernstein polynomials, for the numerical resolution of some boundary values problems. The computations have not need of particular approximations of derivatives, such as finite differences, or particular techniques, such as finite elements. Also, the method doesn't require the use of matrices, as in resolution of linear algebraic systems, nor the use of like-Newton algorithms, as in resolution of non linear sets of equations. An initial equation is resolved only once, then the method is based on iterated evaluations of appropriate polynomials.",
        "published": "2005-10-18T09:55:42Z",
        "link": "http://arxiv.org/abs/cs/0510051v1",
        "categories": [
            "cs.NA",
            "cs.MS",
            "math.CA",
            "physics.comp-ph",
            "G.1.7"
        ]
    },
    {
        "title": "PURRS: Towards Computer Algebra Support for Fully Automatic Worst-Case   Complexity Analysis",
        "authors": [
            "Roberto Bagnara",
            "Andrea Pescetti",
            "Alessandro Zaccagnini",
            "Enea Zaffanella"
        ],
        "summary": "Fully automatic worst-case complexity analysis has a number of applications in computer-assisted program manipulation. A classical and powerful approach to complexity analysis consists in formally deriving, from the program syntax, a set of constraints expressing bounds on the resources required by the program, which are then solved, possibly applying safe approximations. In several interesting cases, these constraints take the form of recurrence relations. While techniques for solving recurrences are known and implemented in several computer algebra systems, these do not completely fulfill the needs of fully automatic complexity analysis: they only deal with a somewhat restricted class of recurrence relations, or sometimes require user intervention, or they are restricted to the computation of exact solutions that are often so complex to be unmanageable, and thus useless in practice. In this paper we briefly describe PURRS, a system and software library aimed at providing all the computer algebra services needed by applications performing or exploiting the results of worst-case complexity analyses. The capabilities of the system are illustrated by means of examples derived from the analysis of programs written in a domain-specific functional programming language for real-time embedded systems.",
        "published": "2005-12-14T09:54:01Z",
        "link": "http://arxiv.org/abs/cs/0512056v1",
        "categories": [
            "cs.MS",
            "cs.CC"
        ]
    },
    {
        "title": "Schwerdtfeger-Fillmore-Springer-Cnops Construction Implemented in GiNaC",
        "authors": [
            "Vladimir V. Kisil"
        ],
        "summary": "This paper presents an implementation of the Schwerdtfeger-Fillmore-Springer-Cnops construction (SFSCc) along with illustrations of its usage. SFSCc linearises the linear-fraction action of the Moebius group in R^n. This has clear advantages in several theoretical and applied fields including engineering. Our implementation is based on the Clifford algebra capacities of the GiNaC computer algebra system (http://www.ginac.de/), which were described in cs.MS/0410044.   The core of this realisation of SFSCc is done for an arbitrary dimension of R^n with a metric given by an arbitrary bilinear form. We also present a subclass for two dimensional cycles (i.e. circles, parabolas and hyperbolas), which add some 2D specific routines including a visualisation to PostScript files through the MetaPost (http://www.tug.org/metapost.html) or Asymptote (http://asymptote.sourceforge.net/) packages.   This software is the backbone of many results published in math.CV/0512416 and we use its applications their for demonstration. The library can be ported (with various level of required changes) to other CAS with Clifford algebras capabilities similar to GiNaC.   There is an ISO image of a Live Debian DVD attached to this paper as an auxiliary file, a copy is stored on Google Drive as well.",
        "published": "2005-12-17T15:09:11Z",
        "link": "http://arxiv.org/abs/cs/0512073v12",
        "categories": [
            "cs.MS",
            "cs.CG",
            "cs.SC",
            "51B25, 51N25, 68U05, 11E88, 68W30"
        ]
    },
    {
        "title": "Computations with one and two real algebraic numbers",
        "authors": [
            "Ioannis Z. Emiris",
            "Elias P. Tsigaridas"
        ],
        "summary": "We present algorithmic and complexity results concerning computations with one and two real algebraic numbers, as well as real solving of univariate polynomials and bivariate polynomial systems with integer coefficients using Sturm-Habicht sequences.   Our main results, in the univariate case, concern the problems of real root isolation (Th. 19) and simultaneous inequalities (Cor.26) and in the bivariate, the problems of system real solving (Th.42), sign evaluation (Th. 37) and simultaneous inequalities (Cor. 43).",
        "published": "2005-12-18T16:58:35Z",
        "link": "http://arxiv.org/abs/cs/0512072v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "I.1.2"
        ]
    },
    {
        "title": "Implementation of Motzkin-Burger algorithm in Maple",
        "authors": [
            "P. A. Burovsky"
        ],
        "summary": "Subject of this paper is an implementation of a well-known Motzkin-Burger algorithm, which solves the problem of finding the full set of solutions of a system of linear homogeneous inequalities. There exist a number of implementations of this algorithm, but there was no one in Maple, to the best of the author's knowledge.",
        "published": "2005-01-01T04:55:40Z",
        "link": "http://arxiv.org/abs/cs/0501003v1",
        "categories": [
            "cs.CG",
            "cs.CC",
            "cs.SC",
            "F.2.2; I.3.5; G.1.6"
        ]
    },
    {
        "title": "Generalized Laplace transformations and integration of hyperbolic   systems of linear partial differential equations",
        "authors": [
            "Sergey P. Tsarev"
        ],
        "summary": "We give a new procedure for generalized factorization and construction of the complete solution of strictly hyperbolic linear partial differential equations or strictly hyperbolic systems of such equations in the plane. This procedure generalizes the classical theory of Laplace transformations of second-order equations in the plane.",
        "published": "2005-01-15T15:57:19Z",
        "link": "http://arxiv.org/abs/cs/0501030v1",
        "categories": [
            "cs.SC",
            "math.AP",
            "nlin.SI"
        ]
    },
    {
        "title": "Efficient Computation of the Characteristic Polynomial",
        "authors": [
            "Jean-Guillaume Dumas",
            "Clément Pernet",
            "Zhendong Wan"
        ],
        "summary": "This article deals with the computation of the characteristic polynomial of dense matrices over small finite fields and over the integers. We first present two algorithms for the finite fields: one is based on Krylov iterates and Gaussian elimination. We compare it to an improvement of the second algorithm of Keller-Gehrig. Then we show that a generalization of Keller-Gehrig's third algorithm could improve both complexity and computational time. We use these results as a basis for the computation of the characteristic polynomial of integer matrices. We first use early termination and Chinese remaindering for dense matrices. Then a probabilistic approach, based on integer minimal polynomial and Hensel factorization, is particularly well suited to sparse and/or structured matrices.",
        "published": "2005-01-25T13:16:16Z",
        "link": "http://arxiv.org/abs/cs/0501074v2",
        "categories": [
            "cs.SC",
            "F.2.4; B.2.4; G.4"
        ]
    },
    {
        "title": "Can Computer Algebra be Liberated from its Algebraic Yoke ?",
        "authors": [
            "R. Barrere"
        ],
        "summary": "So far, the scope of computer algebra has been needlessly restricted to exact algebraic methods. Its possible extension to approximate analytical methods is discussed. The entangled roles of functional analysis and symbolic programming, especially the functional and transformational paradigms, are put forward. In the future, algebraic algorithms could constitute the core of extended symbolic manipulation systems including primitives for symbolic approximations.",
        "published": "2005-02-03T17:28:01Z",
        "link": "http://arxiv.org/abs/cs/0502015v1",
        "categories": [
            "cs.SC",
            "cs.CE",
            "G.4; I.1; I.6"
        ]
    },
    {
        "title": "The complexity of computing the Hilbert polynomial of smooth   equidimensional complex projective varieties",
        "authors": [
            "Peter Buergisser",
            "Martin Lotz"
        ],
        "summary": "We continue the study of counting complexity begun in [Buergisser, Cucker 04] and [Buergisser, Cucker, Lotz 05] by proving upper and lower bounds on the complexity of computing the Hilbert polynomial of a homogeneous ideal. We show that the problem of computing the Hilbert polynomial of a smooth equidimensional complex projective variety can be reduced in polynomial time to the problem of counting the number of complex common zeros of a finite set of multivariate polynomials. Moreover, we prove that the more general problem of computing the Hilbert polynomial of a homogeneous ideal is polynomial space hard. This implies polynomial space lower bounds for both the problems of computing the rank and the Euler characteristic of cohomology groups of coherent sheaves on projective space, improving the #P-lower bound of Bach (JSC 1999).",
        "published": "2005-02-08T17:10:00Z",
        "link": "http://arxiv.org/abs/cs/0502044v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1; F.1"
        ]
    },
    {
        "title": "An hybrid system approach to nonlinear optimal control problems",
        "authors": [
            "Jean-Guillaume Luc Dumas",
            "Aude Rondepierre"
        ],
        "summary": "We consider a nonlinear ordinary differential equation and want to control its behavior so that it reaches a target by minimizing a cost function. Our approach is to use hybrid systems to solve this problem: the complex dynamic is replaced by piecewise affine approximations which allow an analytical resolution. The sequence of affine models then forms a sequence of states of a hybrid automaton. Given a sequence of states, we introduce an hybrid approximation of the nonlinear controllable domain and propose a new algorithm computing a controllable, piecewise convex approximation. The same way the nonlinear optimal control problem is replaced by an hybrid piecewise affine one. Stating a hybrid maximum principle suitable to our hybrid model, we deduce the global structure of the hybrid optimal control steering the system to the target.",
        "published": "2005-02-08T20:49:45Z",
        "link": "http://arxiv.org/abs/math/0502172v3",
        "categories": [
            "math.OC",
            "cs.SC",
            "49J15"
        ]
    },
    {
        "title": "Approximation of dynamical systems using S-systems theory : application   to biological systems",
        "authors": [
            "Laurent Tournier"
        ],
        "summary": "In this paper we propose a new symbolic-numeric algorithm to find positive equilibria of a n-dimensional dynamical system. This algorithm implies a symbolic manipulation of ODE in order to give a local approximation of differential equations with power-law dynamics (S-systems). A numerical calculus is then needed to converge towards an equilibrium, giving at the same time a S-system approximating the initial system around this equilibrium. This algorithm is applied to a real biological example in 14 dimensions which is a subsystem of a metabolic pathway in Arabidopsis Thaliana.",
        "published": "2005-03-03T06:32:17Z",
        "link": "http://arxiv.org/abs/cs/0503008v1",
        "categories": [
            "cs.SC",
            "math.DS",
            "q-bio.MN"
        ]
    },
    {
        "title": "Tensor manipulation in GPL Maxima",
        "authors": [
            "Viktor Toth"
        ],
        "summary": "GPL Maxima is an open-source computer algebra system based on DOE-MACSYMA. GPL Maxima included two tensor manipulation packages from DOE-MACSYMA, but these were in various states of disrepair. One of the two packages, CTENSOR, implemented component-based tensor manipulation; the other, ITENSOR, treated tensor symbols as opaque, manipulating them based on their index properties. The present paper describes the state in which these packages were found, the steps that were needed to make the packages fully functional again, and the new functionality that was implemented to make them more versatile. A third package, ATENSOR, was also implemented; fully compatible with the identically named package in the commercial version of MACSYMA, ATENSOR implements abstract tensor algebras.",
        "published": "2005-03-26T14:41:49Z",
        "link": "http://arxiv.org/abs/cs/0503073v2",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "TeXmacs-maxima interface",
        "authors": [
            "A. G. Grozin"
        ],
        "summary": "This tutorial presents features of the new and improved TeXmacs-maxima interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or later).",
        "published": "2005-04-11T16:39:16Z",
        "link": "http://arxiv.org/abs/cs/0504039v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "hep-ph"
        ]
    },
    {
        "title": "On Compatibility of Discrete Relations",
        "authors": [
            "Vladimir V. Kornyak"
        ],
        "summary": "An approach to compatibility analysis of systems of discrete relations is proposed. Unlike the Groebner basis technique, the proposed scheme is not based on the polynomial ring structure. It uses more primitive set-theoretic and topological concepts and constructions. We illustrate the approach by application to some two-state cellular automata. In the two-state case the Groebner basis method is also applicable, and we compare both approaches.",
        "published": "2005-04-15T13:08:21Z",
        "link": "http://arxiv.org/abs/math-ph/0504048v3",
        "categories": [
            "math-ph",
            "cs.SC",
            "math.AC",
            "math.MP",
            "nlin.CG"
        ]
    },
    {
        "title": "Computing the Rank and a Small Nullspace Basis of a Polynomial Matrix",
        "authors": [
            "Arne Storjohann",
            "Gilles Villard"
        ],
        "summary": "We reduce the problem of computing the rank and a nullspace basis of a univariate polynomial matrix to polynomial matrix multiplication. For an input n x n matrix of degree d over a field K we give a rank and nullspace algorithm using about the same number of operations as for multiplying two matrices of dimension n and degree d. If the latter multiplication is done in MM(n,d)=softO(n^omega d) operations, with omega the exponent of matrix multiplication over K, then the algorithm uses softO(MM(n,d)) operations in K. The softO notation indicates some missing logarithmic factors. The method is randomized with Las Vegas certification. We achieve our results in part through a combination of matrix Hensel high-order lifting and matrix minimal fraction reconstruction, and through the computation of minimal or small degree vectors in the nullspace seen as a K[x]-module",
        "published": "2005-05-11T18:29:00Z",
        "link": "http://arxiv.org/abs/cs/0505030v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1; F.2.1"
        ]
    },
    {
        "title": "Methods for the construction of generators of algebraic curvature   tensors",
        "authors": [
            "Bernd Fiedler"
        ],
        "summary": "We demonstrate the use of several tools from Algebraic Combinatorics such as Young tableaux, symmetry operators, the Littlewood-Richardson rule and discrete Fourier transforms of symmetric groups in investigations of algebraic curvature tensors.",
        "published": "2005-07-20T16:18:29Z",
        "link": "http://arxiv.org/abs/math/0507410v1",
        "categories": [
            "math.CO",
            "cs.SC",
            "math.DG",
            "53B20; 15A72; 05E10; 16D60; 05-04"
        ]
    },
    {
        "title": "Asymptotically fast polynomial matrix algorithms for multivariable   systems",
        "authors": [
            "Claude-Pierre Jeannerod",
            "Gilles Villard"
        ],
        "summary": "We present the asymptotically fastest known algorithms for some basic problems on univariate polynomial matrices: rank, nullspace, determinant, generic inverse, reduced form. We show that they essentially can be reduced to two computer algebra techniques, minimal basis computations and matrix fraction expansion/reconstruction, and to polynomial matrix multiplication. Such reductions eventually imply that all these problems can be solved in about the same amount of time as polynomial matrix multiplication.",
        "published": "2005-08-25T13:52:56Z",
        "link": "http://arxiv.org/abs/cs/0508113v1",
        "categories": [
            "cs.SC",
            "cs.CC",
            "I.1; F.2.1"
        ]
    },
    {
        "title": "Finding Liouvillian first integrals of rational ODEs of any order in   finite terms",
        "authors": [
            "Yuri N. Kosovtsov"
        ],
        "summary": "It is known, due to Mordukhai-Boltovski, Ritt, Prelle, Singer, Christopher and others, that if a given rational ODE has a Liouvillian first integral then the corresponding integrating factor of the ODE must be of a very special form of a product of powers and exponents of irreducible polynomials. These results lead to a partial algorithm for finding Liouvillian first integrals. However, there are two main complications on the way to obtaining polynomials in the integrating factor form. First of all, one has to find an upper bound for the degrees of the polynomials in the product above, an unsolved problem, and then the set of coefficients for each of the polynomials by the computationally-intensive method of undetermined parameters. As a result, this approach was implemented in CAS only for first and relatively simple second order ODEs. We propose an algebraic method for finding polynomials of the integrating factors for rational ODEs of any order, based on examination of the resultants of the polynomials in the numerator and the denominator of the right-hand side of such equation. If both the numerator and the denominator of the right-hand side of such ODE are not constants, the method can determine in finite terms an explicit expression of an integrating factor if the ODE permits integrating factors of the above mentioned form and then the Liouvillian first integral. The tests of this procedure based on the proposed method, implemented in Maple in the case of rational integrating factors, confirm the consistence and efficiency of the method.",
        "published": "2005-08-31T12:40:58Z",
        "link": "http://arxiv.org/abs/math-ph/0508065v2",
        "categories": [
            "math-ph",
            "cs.SC",
            "math.CA",
            "math.MP",
            "nlin.SI",
            "34A05; 34A34; 34A35"
        ]
    },
    {
        "title": "A formally verified proof of the prime number theorem",
        "authors": [
            "Jeremy Avigad",
            "Kevin Donnelly",
            "David Gray",
            "Paul Raff"
        ],
        "summary": "The prime number theorem, established by Hadamard and de la Vall'ee Poussin independently in 1896, asserts that the density of primes in the positive integers is asymptotic to 1 / ln x. Whereas their proofs made serious use of the methods of complex analysis, elementary proofs were provided by Selberg and Erd\"os in 1948. We describe a formally verified version of Selberg's proof, obtained using the Isabelle proof assistant.",
        "published": "2005-09-09T15:47:35Z",
        "link": "http://arxiv.org/abs/cs/0509025v3",
        "categories": [
            "cs.AI",
            "cs.LO",
            "cs.SC",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "A Maple Package for Computing Groebner Bases for Linear Recurrence   Relations",
        "authors": [
            "Vladimir P. Gerdt",
            "Daniel Robertz"
        ],
        "summary": "A Maple package for computing Groebner bases of linear difference ideals is described. The underlying algorithm is based on Janet and Janet-like monomial divisions associated with finite difference operators. The package can be used, for example, for automatic generation of difference schemes for linear partial differential equations and for reduction of multiloop Feynman integrals. These two possible applications are illustrated by simple examples of the Laplace equation and a one-loop scalar integral of propagator type",
        "published": "2005-09-22T15:45:35Z",
        "link": "http://arxiv.org/abs/cs/0509070v2",
        "categories": [
            "cs.SC",
            "cs.MS",
            "I.1.4"
        ]
    },
    {
        "title": "Computing the Kalman form",
        "authors": [
            "Clément Pernet",
            "Aude Rondepierre",
            "Gilles Villard"
        ],
        "summary": "We present two algorithms for the computation of the Kalman form of a linear control system. The first one is based on the technique developed by Keller-Gehrig for the computation of the characteristic polynomial. The cost is a logarithmic number of matrix multiplications. To our knowledge, this improves the best previously known algebraic complexity by an order of magnitude. Then we also present a cubic algorithm proven to more efficient in practice.",
        "published": "2005-10-05T14:16:15Z",
        "link": "http://arxiv.org/abs/cs/0510014v4",
        "categories": [
            "cs.SC",
            "math.OC"
        ]
    },
    {
        "title": "Stationary or static space-times and Young tableaux",
        "authors": [
            "Bernd Fiedler"
        ],
        "summary": "Algebraic curvature tensors possess generators which can be formed from symmetric or alternating tensors S, A or tensors \\theta with an irreducible (2,1)-symmetry. In differential geometry examples of curvature formulas are known which contain generators on the basis of S or A realized by differentiable tensor fields in a natural way. We show that certain curvature formulas for stationary or static space-times contain such differentiable realizations of generators based on \\theta. The tensor \\theta is connected with the timelike Killing vector field of the space-time. \\theta lies in a special symmetry class from the infinite family of irreducible (2,1)-symmetry classes. We determine characteristics of this class. In particular, this class allows a maximal reduction of the length of the curvature formulas. We use a projection formalism by Vladimirov, Young symmetrizers and Littlewood-Richardson products. Computer calculations were carried out by means of the packages Ricci and PERMS.",
        "published": "2005-10-14T16:28:48Z",
        "link": "http://arxiv.org/abs/math/0510304v1",
        "categories": [
            "math.DG",
            "cs.SC",
            "gr-qc",
            "math.CO",
            "53B20; 83C20; 05E10; 16D60; 05-04"
        ]
    },
    {
        "title": "Feynman graphs and related Hopf algebras",
        "authors": [
            "Gérard Henry Edmond Duchamp",
            "Pawel Blasiak",
            "Andrzej Horzela",
            "Karol A. Penson",
            "Allan I. Solomon"
        ],
        "summary": "In a recent series of communications we have shown that the reordering problem of bosons leads to certain combinatorial structures. These structures may be associated with a certain graphical description. In this paper, we show that there is a Hopf Algebra structure associated with this problem which is, in a certain sense, unique.",
        "published": "2005-10-15T06:11:56Z",
        "link": "http://arxiv.org/abs/cs/0510041v1",
        "categories": [
            "cs.SC",
            "cs.DM",
            "math-ph",
            "math.CO",
            "math.MP",
            "quant-ph",
            "G.2.1"
        ]
    },
    {
        "title": "Towards a diagrammatic modeling of the LinBox C++ linear algebra library",
        "authors": [
            "Jean-Guillaume Dumas",
            "Dominique Duval"
        ],
        "summary": "We propose a new diagrammatic modeling language, DML. The paradigm used is that of the category theory and in particular of the pushout tool. We show that most of the object-oriented structures can be described with this tool and have many examples in C++, ranging from virtual inheritance and polymorphism to template genericity. With this powerful tool, we propose a quite simple description of the C++ LinBox library. This library has been designed for efficiency and genericity and therefore makes heavy usage of complex template and polymorphic mecanism. Be reverse engineering, we are able to describe in a simple manner the complex structure of archetypes in LinBox.",
        "published": "2005-10-20T15:07:18Z",
        "link": "http://arxiv.org/abs/cs/0510057v1",
        "categories": [
            "cs.SC",
            "D.2.12; I.6.5; D.1.5; D.3.3; G.4"
        ]
    },
    {
        "title": "ParFORM: recent development",
        "authors": [
            "M. Tentyukov",
            "J. A. M. Vermaseren",
            "H. M. Staudenmaier"
        ],
        "summary": "We report on the status of our project of parallelization of the symbolic manipulation program FORM. We have now parallel versions of FORM running on Cluster- or SMP-architectures. These versions can be used to run arbitrary FORM programs in parallel.",
        "published": "2005-10-31T14:19:41Z",
        "link": "http://arxiv.org/abs/cs/0510093v1",
        "categories": [
            "cs.SC",
            "I.1; I.1.2; I.1.4"
        ]
    },
    {
        "title": "Fast (Multi-)Evaluation of Linearly Recurrent Sequences: Improvements   and Applications",
        "authors": [
            "Martin Ziegler"
        ],
        "summary": "For a linearly recurrent vector sequence P[n+1] = A(n) * P[n], consider the problem of calculating either the n-th term P[n] or L<=n arbitrary terms P[n_1],...,P[n_L], both for the case of constant coefficients A(n)=A and for a matrix A(n) with entries polynomial in n. We improve and extend known algorithms for this problem and present new applications for it. Specifically it turns out that for instance * any family (p_n) of classical orthogonal polynomials admits evaluation at given x within O(n^{1/2} log n) operations INDEPENDENT of the family (p_n) under consideration. * For any L indices n_1,...,n_L <= n, the values p_{n_i}(x) can be calculated simultaneously using O(n^{1/2} log n + L log(n/L)) arithmetic operations; again this running time bound holds uniformly. * Every hypergeometric (or, more generally, holonomic) function admits approximate evaluation up to absolute error e>0 within O((log(1/e)^{1/2} loglog(1/e)) -- as opposed to O(log(1/e)) -- arithmetic steps. * Given m and a polynomial p of degree d over a field of characteristic zero, the coefficient of p^m to term X^n can be computed within O(d^2 M(n^{1/2})) steps where M(n) denotes the cost of multiplying two degree-n polynomials. * The same time bound holds for the joint calculation of any L<=n^{1/2} desired coefficients of p^m to terms X^{n_i}, n_1,...,n_L <= n.",
        "published": "2005-11-08T16:46:18Z",
        "link": "http://arxiv.org/abs/cs/0511033v1",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "An introspective algorithm for the integer determinant",
        "authors": [
            "Jean-Guillaume Dumas",
            "Anna Urbanska"
        ],
        "summary": "We present an algorithm computing the determinant of an integer matrix A. The algorithm is introspective in the sense that it uses several distinct algorithms that run in a concurrent manner. During the course of the algorithm partial results coming from distinct methods can be combined. Then, depending on the current running time of each method, the algorithm can emphasize a particular variant. With the use of very fast modular routines for linear algebra, our implementation is an order of magnitude faster than other existing implementations. Moreover, we prove that the expected complexity of our algorithm is only O(n^3 log^{2.5}(n ||A||)) bit operations in the dense case and O(Omega n^{1.5} log^2(n ||A||) + n^{2.5}log^3(n||A||)) in the sparse case, where ||A|| is the largest entry in absolute value of the matrix and Omega is the cost of matrix-vector multiplication in the case of a sparse matrix.",
        "published": "2005-11-17T14:53:41Z",
        "link": "http://arxiv.org/abs/cs/0511066v5",
        "categories": [
            "cs.SC"
        ]
    },
    {
        "title": "Solving Partial Order Constraints for LPO Termination",
        "authors": [
            "Michael Codish",
            "Vitaly Lagoon",
            "Peter J. Stuckey"
        ],
        "summary": "This paper introduces a new kind of propositional encoding for reasoning about partial orders. The symbols in an unspecified partial order are viewed as variables which take integer values and are interpreted as indices in the order. For a partial order statement on n symbols each index is represented in log2 n propositional variables and partial order constraints between symbols are modeled on the bit representations. We illustrate the application of our approach to determine LPO termination for term rewrite systems. Experimental results are unequivocal, indicating orders of magnitude speedups in comparison with current implementations for LPO termination. The proposed encoding is general and relevant to other applications which involve propositional reasoning about partial orders.",
        "published": "2005-12-16T01:28:25Z",
        "link": "http://arxiv.org/abs/cs/0512067v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "cs.SC",
            "F.3.1; F.4.1"
        ]
    },
    {
        "title": "Schwerdtfeger-Fillmore-Springer-Cnops Construction Implemented in GiNaC",
        "authors": [
            "Vladimir V. Kisil"
        ],
        "summary": "This paper presents an implementation of the Schwerdtfeger-Fillmore-Springer-Cnops construction (SFSCc) along with illustrations of its usage. SFSCc linearises the linear-fraction action of the Moebius group in R^n. This has clear advantages in several theoretical and applied fields including engineering. Our implementation is based on the Clifford algebra capacities of the GiNaC computer algebra system (http://www.ginac.de/), which were described in cs.MS/0410044.   The core of this realisation of SFSCc is done for an arbitrary dimension of R^n with a metric given by an arbitrary bilinear form. We also present a subclass for two dimensional cycles (i.e. circles, parabolas and hyperbolas), which add some 2D specific routines including a visualisation to PostScript files through the MetaPost (http://www.tug.org/metapost.html) or Asymptote (http://asymptote.sourceforge.net/) packages.   This software is the backbone of many results published in math.CV/0512416 and we use its applications their for demonstration. The library can be ported (with various level of required changes) to other CAS with Clifford algebras capabilities similar to GiNaC.   There is an ISO image of a Live Debian DVD attached to this paper as an auxiliary file, a copy is stored on Google Drive as well.",
        "published": "2005-12-17T15:09:11Z",
        "link": "http://arxiv.org/abs/cs/0512073v12",
        "categories": [
            "cs.MS",
            "cs.CG",
            "cs.SC",
            "51B25, 51N25, 68U05, 11E88, 68W30"
        ]
    },
    {
        "title": "Computations with one and two real algebraic numbers",
        "authors": [
            "Ioannis Z. Emiris",
            "Elias P. Tsigaridas"
        ],
        "summary": "We present algorithmic and complexity results concerning computations with one and two real algebraic numbers, as well as real solving of univariate polynomials and bivariate polynomial systems with integer coefficients using Sturm-Habicht sequences.   Our main results, in the univariate case, concern the problems of real root isolation (Th. 19) and simultaneous inequalities (Cor.26) and in the bivariate, the problems of system real solving (Th.42), sign evaluation (Th. 37) and simultaneous inequalities (Cor. 43).",
        "published": "2005-12-18T16:58:35Z",
        "link": "http://arxiv.org/abs/cs/0512072v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "I.1.2"
        ]
    },
    {
        "title": "A Survey of Fault-Tolerance and Fault-Recovery Techniques in Parallel   Systems",
        "authors": [
            "Michael Treaster"
        ],
        "summary": "Supercomputing systems today often come in the form of large numbers of commodity systems linked together into a computing cluster. These systems, like any distributed system, can have large numbers of independent hardware components cooperating or collaborating on a computation. Unfortunately, any of this vast number of components can fail at any time, resulting in potentially erroneous output. In order to improve the robustness of supercomputing applications in the presence of failures, many techniques have been developed to provide resilience to these kinds of system faults. This survey provides an overview of these various fault-tolerance techniques.",
        "published": "2005-01-01T03:32:51Z",
        "link": "http://arxiv.org/abs/cs/0501002v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Large-scale lattice Boltzmann simulations of complex fluids: advances   through the advent of computational grids",
        "authors": [
            "J. Harting",
            "J. Chin",
            "M. Venturoli",
            "P. V. Coveney"
        ],
        "summary": "During the last two years the RealityGrid project has allowed us to be one of the few scientific groups involved in the development of computational grids. Since smoothly working production grids are not yet available, we have been able to substantially influence the direction of software development and grid deployment within the project. In this paper we review our results from large scale three-dimensional lattice Boltzmann simulations performed over the last two years. We describe how the proactive use of computational steering and advanced job migration and visualization techniques enabled us to do our scientific work more efficiently. The projects reported on in this paper are studies of complex fluid flows under shear or in porous media, as well as large-scale parameter searches, and studies of the self-organisation of liquid cubic mesophases.   Movies are available at http://www.ica1.uni-stuttgart.de/~jens/pub/05/05-PhilTransReview.html",
        "published": "2005-01-11T18:31:49Z",
        "link": "http://arxiv.org/abs/cs/0501021v1",
        "categories": [
            "cs.DC",
            "cond-mat.other",
            "cond-mat.soft",
            "physics.flu-dyn"
        ]
    },
    {
        "title": "A Statistical Theory of Chord under Churn",
        "authors": [
            "Supriya Krishnamurthy",
            "Sameh El-Ansary",
            "Erik Aurell",
            "Seif Haridi"
        ],
        "summary": "Most earlier studies of Distributed Hash Tables (DHTs) under churn have either depended on simulations as the primary investigation tool, or on establishing bounds for DHTs to function. In this paper, we present a complete analytical study of churn using a master-equation-based approach, used traditionally in non-equilibrium statistical mechanics to describe steady-state or transient phenomena. Simulations are used to verify all theoretical predictions. We demonstrate the application of our methodology to the Chord system. For any rate of churn and stabilization rates, and any system size, we accurately predict the fraction of failed or incorrect successor and finger pointers and show how we can use these quantities to predict the performance and consistency of lookups under churn. We also discuss briefly how churn may actually be of different 'types' and the implications this will have for the functioning of DHTs in general.",
        "published": "2005-01-24T16:22:25Z",
        "link": "http://arxiv.org/abs/cs/0501069v1",
        "categories": [
            "cs.NI",
            "cond-mat.stat-mech",
            "cs.DC",
            "I.6;G.3;E.1"
        ]
    },
    {
        "title": "Performance of an Operating High Energy Physics Data Grid: D0SAR-Grid",
        "authors": [
            "B. Abbott",
            "P. Baringer",
            "T. Bolton",
            "Z. Greenwood",
            "E. Gregores",
            "H. Kim",
            "C. Leangsuksun",
            "D. Meyer",
            "N. Mondal",
            "S. Novaes",
            "B. Quinn",
            "H. Severini",
            "P. Skubic",
            "J. Snow",
            "M. Sosebee",
            "J. Yu"
        ],
        "summary": "The D0 experiment at Fermilab's Tevatron will record several petabytes of data over the next five years in pursuing the goals of understanding nature and searching for the origin of mass. Computing resources required to analyze these data far exceed capabilities of any one institution. Moreover, the widely scattered geographical distribution of D0 collaborators poses further serious difficulties for optimal use of human and computing resources. These difficulties will exacerbate in future high energy physics experiments, like the LHC. The computing grid has long been recognized as a solution to these problems. This technology is being made a more immediate reality to end users in D0 by developing a grid in the D0 Southern Analysis Region (D0SAR), D0SAR-Grid, using all available resources within it and a home-grown local task manager, McFarm. We will present the architecture in which the D0SAR-Grid is implemented, the use of technology and the functionality of the grid, and the experience from operating the grid in simulation, reprocessing and data analyses for a currently running HEP experiment.",
        "published": "2005-01-31T18:14:24Z",
        "link": "http://arxiv.org/abs/physics/0501164v1",
        "categories": [
            "physics.data-an",
            "cs.DC",
            "physics.ins-det"
        ]
    },
    {
        "title": "Shawn: A new approach to simulating wireless sensor networks",
        "authors": [
            "Alexander Kroeller",
            "Dennis Pfisterer",
            "Carsten Buschmann",
            "Sandor P. Fekete",
            "Stefan Fischer"
        ],
        "summary": "We consider the simulation of wireless sensor networks (WSN) using a new approach. We present Shawn, an open-source discrete-event simulator that has considerable differences to all other existing simulators. Shawn is very powerful in simulating large scale networks with an abstract point of view. It is, to the best of our knowledge, the first simulator to support generic high-level algorithms as well as distributed protocols on exactly the same underlying networks.",
        "published": "2005-02-01T12:23:26Z",
        "link": "http://arxiv.org/abs/cs/0502003v1",
        "categories": [
            "cs.DC",
            "cs.PF",
            "D.4.7, D.4.8"
        ]
    },
    {
        "title": "TerraServer SAN-Cluster Architecture and Operations Experience",
        "authors": [
            "Tom Barclay",
            "Jim Gray"
        ],
        "summary": "Microsoft TerraServer displays aerial, satellite, and to-pographic images of the earth in a SQL database available via the Internet. It is one of the most popular online at-lases, presenting seventeen terabytes of image data from the United States Geological Survey (USGS). Initially de-ployed in 1998, the system demonstrated the scalability of PC hardware and software - Windows and SQL Server - on a single, mainframe-class processor. In September 2000, the back-end database application was migrated to 4-node active/passive cluster connected to an 18 terabyte Storage Area Network (SAN). The new configuration was designed to achieve 99.99% availability for the back-end application. This paper describes the hardware and software components of the TerraServer Cluster and SAN, and describes our experience in configuring and operating this system for three years. Not surprisingly, the hardware and architecture delivered better than four-9's of availability, but operations mistakes delivered three-9's.",
        "published": "2005-02-02T03:36:30Z",
        "link": "http://arxiv.org/abs/cs/0502010v1",
        "categories": [
            "cs.DC",
            "cs.DB"
        ]
    },
    {
        "title": "When Database Systems Meet the Grid",
        "authors": [
            "Maria A. Nieto-Santisteban",
            "Alexander S. Szalay",
            "Aniruddha R. Thakar",
            "William J. O'Mullane",
            "Jim Gray",
            "James Annis"
        ],
        "summary": "We illustrate the benefits of combining database systems and Grid technologies for data-intensive applications. Using a cluster of SQL servers, we reimplemented an existing Grid application that finds galaxy clusters in a large astronomical database. The SQL implementation runs an order of magnitude faster than the earlier Tcl-C-file-based implementation. We discuss why and how Grid applications can take advantage of database systems.",
        "published": "2005-02-03T21:43:50Z",
        "link": "http://arxiv.org/abs/cs/0502018v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Price-Anticipating Resource Allocation Mechanism for Distributed   Shared Clusters",
        "authors": [
            "Michal Feldman",
            "Kevin Lai",
            "Li Zhang"
        ],
        "summary": "In this paper we formulate the fixed budget resource allocation game to understand the performance of a distributed market-based resource allocation system. Multiple users decide how to distribute their budget (bids) among multiple machines according to their individual preferences to maximize their individual utility. We look at both the efficiency and the fairness of the allocation at the equilibrium, where fairness is evaluated through the measures of utility uniformity and envy-freeness. We show analytically and through simulations that despite being highly decentralized, such a system converges quickly to an equilibrium and unlike the social optimum that achieves high efficiency but poor fairness, the proposed allocation scheme achieves a nice balance of high degrees of efficiency and fairness at the equilibrium.",
        "published": "2005-02-04T02:48:41Z",
        "link": "http://arxiv.org/abs/cs/0502019v1",
        "categories": [
            "cs.DC",
            "cs.GT",
            "C.2.4; D.4.1; D.4.7; K.6.0"
        ]
    },
    {
        "title": "Efficient Parallel Simulations of Asynchronous Cellular Arrays",
        "authors": [
            "Boris D. Lubachevsky"
        ],
        "summary": "A definition for a class of asynchronous cellular arrays is proposed. An example of such asynchrony would be independent Poisson arrivals of cell iterations. The Ising model in the continuous time formulation of Glauber falls into this class. Also proposed are efficient parallel algorithms for simulating these asynchronous cellular arrays. In the algorithms, one or several cells are assigned to a processing element (PE), local times for different PEs can be different. Although the standard serial algorithm by Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller can simulate such arrays, it is usually believed to be without an efficient parallel counterpart. However, the proposed parallel algorithms contradict this belief proving to be both efficient and able to perform the same task as the standard algorithm. The results of experiments with the new algorithms are encouraging: the speed-up is greater than 16 using 25 PEs on a shared memory MIMD bus computer, and greater than 1900 using 2**14 PEs on a SIMD computer. The algorithm by Bortz, Kalos, and Lebowitz can be incorporated in the proposed parallel algorithms, further contributing to speed-up. [In this paper I invented the update-cites-of-local-time-minima parallel simulation scheme. Now the scheme is becoming popular. Many misprints of the original 1987 Complex Systems publication are corrected here.-B.L.]",
        "published": "2005-02-08T00:32:33Z",
        "link": "http://arxiv.org/abs/cs/0502039v1",
        "categories": [
            "cs.DC",
            "cond-mat.mtrl-sci",
            "cs.PF",
            "D.1.3; D.4.1; D.4.8; I.6.8"
        ]
    },
    {
        "title": "A Semantic Grid-based E-Learning Framework (SELF)",
        "authors": [
            "Zaheer Abbas",
            "Muhammad Umer",
            "Mohammed Odeh",
            "Richard McClatchey",
            "Arshad Ali",
            "Farooq Ahmad"
        ],
        "summary": "E-learning can be loosely defined as a wide set of applications and processes, which uses available electronic media (and tools) to deliver vocational education and training. With its increasing recognition as an ubiquitous mode of instruction and interaction in the academic as well as corporate world, the need for a scaleable and realistic model is becoming important. In this paper we introduce SELF; a Semantic grid-based E-Learning Framework. SELF aims to identify the key-enablers in a practical grid-based E-learning environment and to minimize technological reworking by proposing a well-defined interaction plan among currently available tools and technologies. We define a dichotomy with E-learning specific application layers on top and semantic grid-based support layers underneath. We also map the latest open and freeware technologies with various components in SELF.",
        "published": "2005-02-09T17:22:21Z",
        "link": "http://arxiv.org/abs/cs/0502051v1",
        "categories": [
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Koordinatenfreies Lokationsbewusstsein (Localization without   Coordinates)",
        "authors": [
            "Alexander Kroeller",
            "Sandor P. Fekete",
            "Carsten Buschmann",
            "Stefan Fischer",
            "Dennis Pfisterer"
        ],
        "summary": "Localization is one of the fundamental issues in sensor networks. It is almost always assumed that it must be solved by assigning coordinates to the nodes. This article discusses positioning algorithms from a theoretical, practical and simulative point of view, and identifies difficulties and limitations. Ideas for more abstract means of location awareness are presented and the resulting possible improvements for applications are shown. Nodes with certain topological or environmental properties are clustered, and the neighborhood structure of the clusters is modeled as a graph. Eines der fundamentalen Probleme in Sensornetzwerken besteht darin, ein Bewusstsein fuer die Position eines Knotens im Netz zu entwickeln. Dabei wird fast immer davon ausgegangen, dass dies durch die Zuweisung von Koordinaten zu erfolgen hat. In diesem Artikel wird auf theoretischer, praktischer und simulativer Ebene ein kritischer Blick auf entsprechende Verfahren geworfen, und es werden Grenzen aufgezeigt. Es wird ein Ansatz vorgestellt, mit dem in der Zukunft eine abstrakte Form von Lokationsbewusstsein etabliert werden kann, und es wird gezeigt, wie Anwendungen dadurch verbessert werden koennen. Er basiert auf einer graphenbasierten Modellierung des Netzes: Knoten mit bestimmten topologischen oder Umwelteigenschaften werden zu Clustern zusammengefasst, und Clusternachbarschaften dann als Graphen modelliert.",
        "published": "2005-02-15T16:35:13Z",
        "link": "http://arxiv.org/abs/cs/0502069v1",
        "categories": [
            "cs.DC",
            "C.2.2; F.2; G.2.2"
        ]
    },
    {
        "title": "Batch is back: CasJobs, serving multi-TB data on the Web",
        "authors": [
            "William OMullane",
            "Nolan Li",
            "Maria Nieto-Santisteban",
            "Alex Szalay",
            "Ani Thakar",
            "Jim Gray"
        ],
        "summary": "The Sloan Digital Sky Survey (SDSS) science database describes over 140 million objects and is over 1.5 TB in size. The SDSS Catalog Archive Server (CAS) provides several levels of query interface to the SDSS data via the SkyServer website. Most queries execute in seconds or minutes. However, some queries can take hours or days, either because they require non-index scans of the largest tables, or because they request very large result sets, or because they represent very complex aggregations of the data. These \"monster queries\" not only take a long time, they also affect response times for everyone else - one or more of them can clog the entire system. To ameliorate this problem, we developed a multi-server multi-queue batch job submission and tracking system for the CAS called CasJobs. The transfer of very large result sets from queries over the network is another serious problem. Statistics suggested that much of this data transfer is unnecessary; users would prefer to store results locally in order to allow further joins and filtering. To allow local analysis, a system was developed that gives users their own personal databases (MyDB) at the server side. Users may transfer data to their MyDB, and then perform further analysis before extracting it to their own machine. MyDB tables also provide a convenient way to share results of queries with collaborators without downloading them. CasJobs is built using SOAP XML Web services and has been in operation since May 2004.",
        "published": "2005-02-17T01:55:35Z",
        "link": "http://arxiv.org/abs/cs/0502072v1",
        "categories": [
            "cs.DC",
            "cs.DB"
        ]
    },
    {
        "title": "The QuarkNet/Grid Collaborative Learning e-Lab",
        "authors": [
            "M. Bardeen",
            "E. Gilbert",
            "T. Jordan",
            "P. Neywoda",
            "E. Quigg",
            "M. Wilde",
            "Y. Zhao"
        ],
        "summary": "We describe a case study that uses grid computing techniques to support the collaborative learning of high school students investigating cosmic rays. Students gather and upload science data to our e-Lab portal. They explore those data using techniques from the GriPhyN collaboration. These techniques include virtual data transformations, workflows, metadata cataloging and indexing, data product provenance and persistence, as well as job planners. Students use web browsers and a custom interface that extends the GriPhyN Chiron portal to perform all of these tasks. They share results in the form of online posters and ask each other questions in this asynchronous environment. Students can discover and extend the research of other students, modeling the processes of modern large-scale scientific collaborations. Also, the e-Lab portal provides tools for teachers to guide student work throughout an investigation. http://quarknet.uchicago.edu/elab/cosmic",
        "published": "2005-02-22T20:11:40Z",
        "link": "http://arxiv.org/abs/cs/0502089v1",
        "categories": [
            "cs.DC",
            "physics.ed-ph"
        ]
    },
    {
        "title": "UNICORE - From Project Results to Production Grids",
        "authors": [
            "A. Streit",
            "D. Erwin",
            "Th. Lippert",
            "D. Mallmann",
            "R. Menday",
            "M. Rambadt",
            "M. Riedel",
            "M. Romberg",
            "B. Schuller",
            "Ph. Wieder"
        ],
        "summary": "The UNICORE Grid-technology provides a seamless, secure and intuitive access to distributed Grid resources. In this paper we present the recent evolution from project results to production Grids. At the beginning UNICORE was developed as a prototype software in two projects funded by the German research ministry (BMBF). Over the following years, in various European-funded projects, UNICORE evolved to a full-grown and well-tested Grid middleware system, which today is used in daily production at many supercomputing centers worldwide. Beyond this production usage, the UNICORE technology serves as a solid basis in many European and International research projects, which use existing UNICORE components to implement advanced features, high level services, and support for applications from a growing range of domains. In order to foster these ongoing developments, UNICORE is available as open source under BSD licence at SourceForge, where new releases are published on a regular basis. This paper is a review of the UNICORE achievements so far and gives a glimpse on the UNICORE roadmap.",
        "published": "2005-02-24T14:13:56Z",
        "link": "http://arxiv.org/abs/cs/0502090v1",
        "categories": [
            "cs.DC",
            "cs.OS"
        ]
    },
    {
        "title": "Online Permutation Routing in Partitioned Optical Passive Star Networks",
        "authors": [
            "Alessandro Mei",
            "Romeo Rizzi"
        ],
        "summary": "This paper establishes the state of the art in both deterministic and randomized online permutation routing in the POPS network. Indeed, we show that any permutation can be routed online on a POPS network either with $O(\\frac{d}{g}\\log g)$ deterministic slots, or, with high probability, with $5c\\lceil d/g\\rceil+o(d/g)+O(\\log\\log g)$ randomized slots, where constant $c=\\exp (1+e^{-1})\\approx 3.927$. When $d=\\Theta(g)$, that we claim to be the \"interesting\" case, the randomized algorithm is exponentially faster than any other algorithm in the literature, both deterministic and randomized ones. This is true in practice as well. Indeed, experiments show that it outperforms its rivals even starting from as small a network as a POPS(2,2), and the gap grows exponentially with the size of the network. We can also show that, under proper hypothesis, no deterministic algorithm can asymptotically match its performance.",
        "published": "2005-02-26T00:07:43Z",
        "link": "http://arxiv.org/abs/cs/0502093v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Prédiction de Performances pour les Communications Collectives",
        "authors": [
            "Luiz Angelo Barchet-Estefanel",
            "Grégory Mounié"
        ],
        "summary": "Des travaux r\\'{e}cents visent l'optimisation des op\\'{e}rations de communication collective dans les environnements de type grille de calcul. La solution la plus r\\'{e}pandue est la s\\'{e}paration des communications internes et externes \\`{a} chaque grappe, mais cela n'exclut pas le d\\'{e}coupage des communications en plusieurs couches, pratique efficace d\\'{e}montr\\'{e}e par Karonis et al. [10]. Dans les deux cas, la pr\\'{e}diction des performances est un facteur essentiel, soit pour le r\\'{e}glage fin des param\\`{e}tres de communication, soit pour le calcul de la distribution et de la hi\\'{e}rarchie des communications. Pour cela, il est tr\\`{e}s important d'avoir des mod\\`{e}les pr\\'{e}cis des communications collectives, lesquels seront utilis\\'{e}s pour pr\\'{e}dire ces performances. Cet article d\\'{e}crit notre exp\\'{e}rience sur la mod\\'{e}lisation des op\\'{e}rations de communication collective. Nous pr\\'{e}sentons des mod\\`{e}les de communication pour diff\\'{e}rents patrons de communication collective comme un vers plusieurs, un vers plusieurs personnalis\\'{e} et plusieurs vers plusieurs. Pour \\'{e}valuer la pr\\'{e}cision des mod\\`{e}les, nous comparons les pr\\'{e}dictions obtenues avec les r\\'{e}sultats des exp\\'{e}rimentations effectu\\'{e}es sur deux environnements r\\'{e}seaux diff\\'{e}rents, Fast Ethernet et Myrinet.",
        "published": "2005-03-04T17:09:23Z",
        "link": "http://arxiv.org/abs/cs/0503013v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Taxonomy of Workflow Management Systems for Grid Computing",
        "authors": [
            "Jia Yu",
            "Rajkumar Buyya"
        ],
        "summary": "With the advent of Grid and application technologies, scientists and engineers are building more and more complex applications to manage and process large data sets, and execute scientific experiments on distributed resources. Such application scenarios require means for composing and executing complex workflows. Therefore, many efforts have been made towards the development of workflow management systems for Grid computing. In this paper, we propose a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. We also survey several representative Grid workflow systems developed by various projects world-wide to demonstrate the comprehensiveness of the taxonomy. The taxonomy not only highlights the design and engineering similarities and differences of state-of-the-art in Grid workflow systems, but also identifies the areas that need further research.",
        "published": "2005-03-11T01:13:07Z",
        "link": "http://arxiv.org/abs/cs/0503025v2",
        "categories": [
            "cs.DC",
            "D.1"
        ]
    },
    {
        "title": "Contextual Constraint Modeling in Grid Application Workflows",
        "authors": [
            "G. E. Graham",
            "M. Anzar Afaq",
            "David Evans",
            "Gerald Guglielmo",
            "Eric Wicklund",
            "Peter Love"
        ],
        "summary": "This paper introduces a new mechanism for specifying constraints in distributed workflows. By introducing constraints in a contextual form, it is shown how different people and groups within collaborative communities can cooperatively constrain workflows. A comparison with existing state-of-the-art workflow systems is made. These ideas are explored in practice with an illustrative example from High Energy Physics.",
        "published": "2005-03-19T01:28:48Z",
        "link": "http://arxiv.org/abs/cs/0503045v1",
        "categories": [
            "cs.DC",
            "D.1.3; D.2.12"
        ]
    },
    {
        "title": "Fast Distributed Algorithms for Computing Separable Functions",
        "authors": [
            "Damon Mosk-Aoyama",
            "Devavrat Shah"
        ],
        "summary": "The problem of computing functions of values at the nodes in a network in a totally distributed manner, where nodes do not have unique identities and make decisions based only on local information, has applications in sensor, peer-to-peer, and ad-hoc networks. The task of computing separable functions, which can be written as linear combinations of functions of individual variables, is studied in this context. Known iterative algorithms for averaging can be used to compute the normalized values of such functions, but these algorithms do not extend in general to the computation of the actual values of separable functions.   The main contribution of this paper is the design of a distributed randomized algorithm for computing separable functions. The running time of the algorithm is shown to depend on the running time of a minimum computation algorithm used as a subroutine. Using a randomized gossip mechanism for minimum computation as the subroutine yields a complete totally distributed algorithm for computing separable functions. For a class of graphs with small spectral gap, such as grid graphs, the time used by the algorithm to compute averages is of a smaller order than the time required by a known iterative averaging scheme.",
        "published": "2005-04-08T06:49:29Z",
        "link": "http://arxiv.org/abs/cs/0504029v4",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Resource Management Services for a Grid Analysis Environment",
        "authors": [
            "Arshad Ali",
            "Ashiq Anjum",
            "Tahir Azim",
            "Julian Bunn",
            "Atif Mehmood",
            "Richard McClatchey",
            "Harvey Newman",
            "Waqas ur Rehman",
            "Conrad Steenberg",
            "Michael Thomas",
            "Frank van Lingen",
            "Ian Willers",
            "Muhammad Adeel Zafar"
        ],
        "summary": "Selecting optimal resources for submitting jobs on a computational Grid or accessing data from a data grid is one of the most important tasks of any Grid middleware. Most modern Grid software today satisfies this responsibility and gives a best-effort performance to solve this problem. Almost all decisions regarding scheduling and data access are made by the software automatically, giving users little or no control over the entire process. To solve this problem, a more interactive set of services and middleware is desired that provides users more information about Grid weather, and gives them more control over the decision making process. This paper presents a set of services that have been developed to provide more interactive resource management capabilities within the Grid Analysis Environment (GAE) being developed collaboratively by Caltech, NUST and several other institutes. These include a steering service, a job monitoring service and an estimator service that have been designed and written using a common Grid-enabled Web Services framework named Clarens. The paper also presents a performance analysis of the developed services to show that they have indeed resulted in a more interactive and powerful system for user-centric Grid-enabled physics analysis.",
        "published": "2005-04-10T11:59:25Z",
        "link": "http://arxiv.org/abs/cs/0504033v1",
        "categories": [
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "Heterogeneous Relational Databases for a Grid-enabled Analysis   Environment",
        "authors": [
            "Arshad Ali",
            "Ashiq Anjum",
            "Tahir Azim",
            "Julian Bunn",
            "Saima Iqbal",
            "Richard McClatchey",
            "Harvey Newman",
            "S. Yousaf Shah",
            "Tony Solomonides",
            "Conrad Steenberg",
            "Michael Thomas",
            "Frank van Lingen",
            "Ian Willers"
        ],
        "summary": "Grid based systems require a database access mechanism that can provide seamless homogeneous access to the requested data through a virtual data access system, i.e. a system which can take care of tracking the data that is stored in geographically distributed heterogeneous databases. This system should provide an integrated view of the data that is stored in the different repositories by using a virtual data access mechanism, i.e. a mechanism which can hide the heterogeneity of the backend databases from the client applications. This paper focuses on accessing data stored in disparate relational databases through a web service interface, and exploits the features of a Data Warehouse and Data Marts. We present a middleware that enables applications to access data stored in geographically distributed relational databases without being aware of their physical locations and underlying schema. A web service interface is provided to enable applications to access this middleware in a language and platform independent way. A prototype implementation was created based on Clarens [4], Unity [7] and POOL [8]. This ability to access the data stored in the distributed relational databases transparently is likely to be a very powerful one for Grid users, especially the scientific community wishing to collate and analyze data distributed over the Grid.",
        "published": "2005-04-10T12:05:03Z",
        "link": "http://arxiv.org/abs/cs/0504034v1",
        "categories": [
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "JClarens: A Java Framework for Developing and Deploying Web Services for   Grid Computing",
        "authors": [
            "Michael Thomas",
            "Conrad Steenberg",
            "Frank van Lingen",
            "Harvey Newman",
            "Julian Bunn",
            "Arshad Ali",
            "Richard McClatchey",
            "Ashiq Anjum",
            "Tahir Azim",
            "Waqas ur Rehman",
            "Faisal Khan",
            "Jang Uk In"
        ],
        "summary": "High Energy Physics (HEP) and other scientific communities have adopted Service Oriented Architectures (SOA) as part of a larger Grid computing effort. This effort involves the integration of many legacy applications and programming libraries into a SOA framework. The Grid Analysis Environment (GAE) is such a service oriented architecture based on the Clarens Grid Services Framework and is being developed as part of the Compact Muon Solenoid (CMS) experiment at the Large Hadron Collider (LHC) at European Laboratory for Particle Physics (CERN). Clarens provides a set of authorization, access control, and discovery services, as well as XMLRPC and SOAP access to all deployed services. Two implementations of the Clarens Web Services Framework (Python and Java) offer integration possibilities for a wide range of programming languages. This paper describes the Java implementation of the Clarens Web Services Framework called JClarens. and several web services of interest to the scientific and Grid community that have been deployed using JClarens.",
        "published": "2005-04-11T21:45:07Z",
        "link": "http://arxiv.org/abs/cs/0504044v1",
        "categories": [
            "cs.DC",
            "H2.4;J.3"
        ]
    },
    {
        "title": "A Scalable Stream-Oriented Framework for Cluster Applications",
        "authors": [
            "Tassos S. Argyros",
            "David R. Cheriton"
        ],
        "summary": "This paper presents a stream-oriented architecture for structuring cluster applications. Clusters that run applications based on this architecture can scale to tenths of thousands of nodes with significantly less performance loss or reliability problems. Our architecture exploits the stream nature of the data flow and reduces congestion through load balancing, hides latency behind data pushes and transparently handles node failures. In our ongoing work, we are developing an implementation for this architecture and we are able to run simple data mining applications on a cluster simulator.",
        "published": "2005-04-13T16:37:59Z",
        "link": "http://arxiv.org/abs/cs/0504051v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.NI",
            "cs.OS",
            "cs.PL"
        ]
    },
    {
        "title": "Wikis in Tuple Spaces",
        "authors": [
            "G Gordon Worley III"
        ],
        "summary": "We consider storing the pages of a wiki in a tuple space and the effects this might have on the wiki experience. In particular, wiki pages are stored in tuples with a few identifying values such as title, author, revision date, content, etc. and pages are retrieved by sending the tuple space templates, such as one that gives the title but nothing else, leaving the tuple space to resolve to a single tuple. We use a tuple space wiki to avoid deadlocks, infinite loops, and wasted efforts when page edit contention arises and examine how a tuple space wiki changes the wiki experience.",
        "published": "2005-04-27T23:04:35Z",
        "link": "http://arxiv.org/abs/cs/0504105v1",
        "categories": [
            "cs.DC",
            "cs.MM"
        ]
    },
    {
        "title": "SafeMPI - Extending MPI for Byzantine Error Detection on Parallel   Clusters",
        "authors": [
            "Dmitry Mogilevsky",
            "Sean Keller"
        ],
        "summary": "Modern high-performance computing relies heavily on the use of commodity processors arranged together in clusters. These clusters consist of individual nodes (typically off-the-shelf single or dual processor machines) connected together with a high speed interconnect. Using cluster computation has many benefits, but also carries the liability of being failure prone due to the sheer number of components involved. Many effective solutions have been proposed to aid failure recovery in clusters, their one significant downside being the failure models they support. Most of the work in the area has focused on detecting and correcting fail-stop errors. We propose a system that will also detect more general error models, such as Byzantine errors, thus allowing existing failure recovery methods to handle them correctly.",
        "published": "2005-05-31T21:05:03Z",
        "link": "http://arxiv.org/abs/cs/0506001v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A batch scheduler with high level components",
        "authors": [
            "Nicolas Capit",
            "Georges Da Costa",
            "Yiannis Georgiou",
            "Guillaume Huard",
            "Cyrille Martin",
            "Grégory Mounié",
            "Pierre Neyron",
            "Olivier Richard"
        ],
        "summary": "In this article we present the design choices and the evaluation of a batch scheduler for large clusters, named OAR. This batch scheduler is based upon an original design that emphasizes on low software complexity by using high level tools. The global architecture is built upon the scripting language Perl and the relational database engine Mysql. The goal of the project OAR is to prove that it is possible today to build a complex system for ressource management using such tools without sacrificing efficiency and scalability. Currently, our system offers most of the important features implemented by other batch schedulers such as priority scheduling (by queues), reservations, backfilling and some global computing support. Despite the use of high level tools, our experiments show that our system has performances close to other systems. Furthermore, OAR is currently exploited for the management of 700 nodes (a metropolitan GRID) and has shown good efficiency and robustness.",
        "published": "2005-06-02T13:04:14Z",
        "link": "http://arxiv.org/abs/cs/0506006v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "A Taxonomy of Data Grids for Distributed Data Sharing, Management and   Processing",
        "authors": [
            "Srikumar Venugopal",
            "Rajkumar Buyya",
            "Kotagiri Ramamohanarao"
        ],
        "summary": "Data Grids have been adopted as the platform for scientific communities that need to share, access, transport, process and manage large data collections distributed worldwide. They combine high-end computing technologies with high-performance networking and wide-area storage management techniques. In this paper, we discuss the key concepts behind Data Grids and compare them with other data sharing and distribution paradigms such as content delivery networks, peer-to-peer networks and distributed databases. We then provide comprehensive taxonomies that cover various aspects of architecture, data transportation, data replication and resource allocation and scheduling. Finally, we map the proposed taxonomy to various Data Grid systems not only to validate the taxonomy but also to identify areas for future exploration. Through this taxonomy, we aim to categorise existing systems to better understand their goals and their methodology. This would help evaluate their applicability for solving similar problems. This taxonomy also provides a \"gap analysis\" of this area through which researchers can potentially identify new issues for investigation. Finally, we hope that the proposed taxonomy and mapping also helps to provide an easy way for new practitioners to understand this complex area of research.",
        "published": "2005-06-10T10:59:37Z",
        "link": "http://arxiv.org/abs/cs/0506034v1",
        "categories": [
            "cs.DC",
            "cs.CE",
            "A.1; C.2.4; J.2"
        ]
    },
    {
        "title": "A low-cost parallel implementation of direct numerical simulation of   wall turbulence",
        "authors": [
            "Paolo Luchini",
            "Maurizio Quadrio"
        ],
        "summary": "A numerical method for the direct numerical simulation of incompressible wall turbulence in rectangular and cylindrical geometries is presented. The distinctive feature resides in its design being targeted towards an efficient distributed-memory parallel computing on commodity hardware. The adopted discretization is spectral in the two homogeneous directions; fourth-order accurate, compact finite-difference schemes over a variable-spacing mesh in the wall-normal direction are key to our parallel implementation. The parallel algorithm is designed in such a way as to minimize data exchange among the computing machines, and in particular to avoid taking a global transpose of the data during the pseudo-spectral evaluation of the non-linear terms. The computing machines can then be connected to each other through low-cost network devices. The code is optimized for memory requirements, which can moreover be subdivided among the computing nodes. The layout of a simple, dedicated and optimized computing system based on commodity hardware is described. The performance of the numerical method on this computing system is evaluated and compared with that of other codes described in the literature, as well as with that of the same code implementing a commonly employed strategy for the pseudo-spectral calculation.",
        "published": "2005-06-18T11:34:16Z",
        "link": "http://arxiv.org/abs/physics/0506155v1",
        "categories": [
            "physics.flu-dyn",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "A Flexible Thread Scheduler for Hierarchical Multiprocessor Machines",
        "authors": [
            "Samuel Thibault"
        ],
        "summary": "With the current trend of multiprocessor machines towards more and more hierarchical architectures, exploiting the full computational power requires careful distribution of execution threads and data so as to limit expensive remote memory accesses. Existing multi-threaded libraries provide only limited facilities to let applications express distribution indications, so that programmers end up with explicitly distributing tasks according to the underlying architecture, which is difficult and not portable. In this article, we present: (1) a model for dynamically expressing the structure of the computation; (2) a scheduler interpreting this model so as to make judicious hierarchical distribution decisions; (3) an implementation within the Marcel user-level thread library. We experimented our proposal on a scientific application running on a ccNUMA Bull NovaScale with 16 Intel Itanium II processors; results show a 30% gain compared to a classical scheduler, and are similar to what a handmade scheduler achieves in a non-portable way.",
        "published": "2005-06-27T14:32:50Z",
        "link": "http://arxiv.org/abs/cs/0506097v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Papillon: Greedy Routing in Rings",
        "authors": [
            "Ittai Abraham",
            "Dahlia Malkhi",
            "Gurmeet Singh Manku"
        ],
        "summary": "We study {\\sc greedy} routing over $n$ nodes placed in a ring, with the \\emph{distance} between two nodes defined to be the clockwise or the absolute distance between them along the ring. Such graphs arise in the context of modeling social networks and in routing networks for peer-to-peer systems. We construct the first network over $n$ nodes in which {\\sc greedy} routing takes $O(\\log n / \\log d)$ hops in the worst-case, with $d$ out-going links per node. Our result has the first asymptotically optimal greedy routing complexity. Previous constructions required $O(\\frac{\\log^2 n}{d})$ hops.",
        "published": "2005-07-14T00:34:42Z",
        "link": "http://arxiv.org/abs/cs/0507034v1",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "Distributed Regression in Sensor Networks: Training Distributively with   Alternating Projections",
        "authors": [
            "Joel B. Predd",
            "Sanjeev R. Kulkarni",
            "H. Vincent Poor"
        ],
        "summary": "Wireless sensor networks (WSNs) have attracted considerable attention in recent years and motivate a host of new challenges for distributed signal processing. The problem of distributed or decentralized estimation has often been considered in the context of parametric models. However, the success of parametric methods is limited by the appropriateness of the strong statistical assumptions made by the models. In this paper, a more flexible nonparametric model for distributed regression is considered that is applicable in a variety of WSN applications including field estimation. Here, starting with the standard regularized kernel least-squares estimator, a message-passing algorithm for distributed estimation in WSNs is derived. The algorithm can be viewed as an instantiation of the successive orthogonal projection (SOP) algorithm. Various practical aspects of the algorithm are discussed and several numerical simulations validate the potential of the approach.",
        "published": "2005-07-18T00:45:12Z",
        "link": "http://arxiv.org/abs/cs/0507039v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "The MammoGrid Virtual Organisation - Federating Distributed Mammograms",
        "authors": [
            "Florida Estrella",
            "Richard McClatchey",
            "Dmitry Rogulin"
        ],
        "summary": "The MammoGrid project aims to deliver a prototype which enables the effective collaboration between radiologists using grid, service-orientation and database solutions. The grid technologies and service-based database management solution provide the platform for integrating diverse and distributed resources, creating what is called a virtual organisation. The MammoGrid Virtual Organisation facilitates the sharing and coordinated access to mammography data, medical imaging software and computing resources of participating hospitals. Hospitals manage their local database of mammograms, but in addition, radiologists who are part of this organisation can share mammograms, reports, results and image analysis software. The MammoGrid Virtual Organisation is a federation of autonomous multi-centres sites which transcends national boundaries. This paper outlines the service-based approach in the creation and management of the federated distributed mammography database and discusses the role of virtual organisations in distributed image analysis.",
        "published": "2005-07-18T12:17:57Z",
        "link": "http://arxiv.org/abs/cs/0507042v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "H.2.4; J.3"
        ]
    },
    {
        "title": "Skip-Webs: Efficient Distributed Data Structures for Multi-Dimensional   Data Sets",
        "authors": [
            "Lars Arge",
            "David Eppstein",
            "Michael T. Goodrich"
        ],
        "summary": "We present a framework for designing efficient distributed data structures for multi-dimensional data. Our structures, which we call skip-webs, extend and improve previous randomized distributed data structures, including skipnets and skip graphs. Our framework applies to a general class of data querying scenarios, which include linear (one-dimensional) data, such as sorted sets, as well as multi-dimensional data, such as d-dimensional octrees and digital tries of character strings defined over a fixed alphabet. We show how to perform a query over such a set of n items spread among n hosts using O(log n / log log n) messages for one-dimensional data, or O(log n) messages for fixed-dimensional data, while using only O(log n) space per host. We also show how to make such structures dynamic so as to allow for insertions and deletions in O(log n) messages for quadtrees, octrees, and digital tries, and O(log n / log log n) messages for one-dimensional data. Finally, we show how to apply a blocking strategy to skip-webs to further improve message complexity for one-dimensional data when hosts can store more data.",
        "published": "2005-07-19T20:30:33Z",
        "link": "http://arxiv.org/abs/cs/0507050v1",
        "categories": [
            "cs.DC",
            "cs.CG",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Reliable Data Storage in Distributed Hash Tables",
        "authors": [
            "Matthew Leslie"
        ],
        "summary": "Distributed Hash Tables offer a resilient lookup service for unstable distributed environments. Resilient data storage, however, requires additional data replication and maintenance algorithms. These algorithms can have an impact on both the performance and the scalability of the system. In this paper, we describe the goals and design space of these replication algorithms.   We examine an existing replication algorithm, and present a new analysis of its reliability. We then present a new dynamic replication algorithm which can operate in unstable environments. We give several possible replica placement strategies for this algorithm, and show how they impact reliability and performance.   Finally we compare all replication algorithms through simulation, showing quantitatively the difference between their bandwidth use, fault tolerance and performance.",
        "published": "2005-07-29T16:54:00Z",
        "link": "http://arxiv.org/abs/cs/0507072v3",
        "categories": [
            "cs.DC",
            "cs.NI"
        ]
    },
    {
        "title": "A New Approach for Boundary Recognition in Geometric Sensor Networks",
        "authors": [
            "Sandor P. Fekete",
            "Michael Kaufmann",
            "Alexander Kroeller",
            "Katharina Lehmann"
        ],
        "summary": "We describe a new approach for dealing with the following central problem in the self-organization of a geometric sensor network: Given a polygonal region R, and a large, dense set of sensor nodes that are scattered uniformly at random in R. There is no central control unit, and nodes can only communicate locally by wireless radio to all other nodes that are within communication radius r, without knowing their coordinates or distances to other nodes. The objective is to develop a simple distributed protocol that allows nodes to identify themselves as being located near the boundary of R and form connected pieces of the boundary. We give a comparison of several centrality measures commonly used in the analysis of social networks and show that restricted stress centrality is particularly suited for geometric networks; we provide mathematical as well as experimental evidence for the quality of this measure.",
        "published": "2005-08-01T19:44:53Z",
        "link": "http://arxiv.org/abs/cs/0508006v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "C.2.1; F.2.2; G.3"
        ]
    },
    {
        "title": "Energy Optimal Data Propagation in Wireless Sensor Networks",
        "authors": [
            "Pierre Leone",
            "Olivier Powell",
            "Jose Rolim"
        ],
        "summary": "We propose an algorithm which produces a randomized strategy reaching optimal data propagation in wireless sensor networks (WSN).In [6] and [8], an energy balanced solution is sought using an approximation algorithm. Our algorithm improves by (a) when an energy-balanced solution does not exist, it still finds an optimal solution (whereas previous algorithms did not consider this case and provide no useful solution) (b) instead of being an approximation algorithm, it finds the exact solution in one pass. We also provide a rigorous proof of the optimality of our solution.",
        "published": "2005-08-10T15:54:13Z",
        "link": "http://arxiv.org/abs/cs/0508052v1",
        "categories": [
            "cs.DC",
            "C.2.1"
        ]
    },
    {
        "title": "On the cascade rollback synchronization",
        "authors": [
            "Anatoli Manita",
            "Francois Simonot"
        ],
        "summary": "We consider a cascade model of $N$ different processors performing a distributed parallel simulation. The main goal of the study is to show that the long-time dynamics of the system has a cluster behavior. To attack this problem we combine two methods: stochastic comparison and Foster-Lyapunov functions.",
        "published": "2005-08-26T11:19:44Z",
        "link": "http://arxiv.org/abs/math/0508533v1",
        "categories": [
            "math.PR",
            "cs.DC",
            "60J27 (Primary); 68M14, 68M20 (Secondary)"
        ]
    },
    {
        "title": "Coding Schemes for Line Networks",
        "authors": [
            "Payam Pakzad",
            "Christina Fragouli",
            "Amin Shokrollahi"
        ],
        "summary": "We consider a simple network, where a source and destination node are connected with a line of erasure channels. It is well known that in order to achieve the min-cut capacity, the intermediate nodes are required to process the information. We propose coding schemes for this setting, and discuss each scheme in terms of complexity, delay, achievable rate, memory requirement, and adaptability to unknown channel parameters. We also briefly discuss how these schemes can be extended to more general networks.",
        "published": "2005-08-29T09:47:20Z",
        "link": "http://arxiv.org/abs/cs/0508124v1",
        "categories": [
            "cs.IT",
            "cs.DC",
            "cs.NI",
            "math.IT",
            "H.1.1"
        ]
    },
    {
        "title": "A Model-driven Approach for Grid Services Engineering",
        "authors": [
            "David Manset",
            "Richard McClatchey",
            "Flavio Oquendo",
            "Herve Verjus"
        ],
        "summary": "As a consequence to the hype of Grid computing, such systems have seldom been designed using formal techniques. The complexity and rapidly growing demand around Grid technologies has favour the use of classical development techniques, resulting in no guidelines or rules and unstructured engineering processes. This paper advocates a formal approach to Grid applications development in an effort to contribute to the rigorous development of Grids software architectures. This approach addresses cross-platform interoperability and quality of service; the model-driven paradigm is applied to a formal architecture-centric engineering method in order to benefit from the formal semantic description power in addition to model-based transformations. The result of such a novel combined concept promotes the re-use of design models and eases developments in Grid computing by providing an adapted development process and ensuring correctness at each design step.",
        "published": "2005-09-21T17:39:19Z",
        "link": "http://arxiv.org/abs/cs/0509066v2",
        "categories": [
            "cs.SE",
            "cs.DC",
            "D.2.11"
        ]
    },
    {
        "title": "Decomposing Solution Sets of Polynomial Systems: A New Parallel   Monodromy Breakup Algorithm",
        "authors": [
            "Anton Leykin",
            "Jan Verschelde"
        ],
        "summary": "We consider the numerical irreducible decomposition of a positive dimensional solution set of a polynomial system into irreducible factors. Path tracking techniques computing loops around singularities connect points on the same irreducible components. The computation of a linear trace for each factor certifies the decomposition. This factorization method exhibits a good practical performance on solution sets of relative high degrees.   Using the same concepts of monodromy and linear trace, we present a new monodromy breakup algorithm. It shows a better performance than the old method which requires construction of permutations of witness points in order to break up the solution set. In contrast, the new algorithm assumes a finer approach allowing us to avoid tracking unnecessary homotopy paths.   As we designed the serial algorithm keeping in mind distributed computing, an additional advantage is that its parallel version can be easily built. Synchronization issues resulted in a performance loss of the straightforward parallel version of the old algorithm. Our parallel implementation of the new approach bypasses these issues, therefore, exhibiting a better performance, especially on solution sets of larger degree.",
        "published": "2005-09-21T20:55:09Z",
        "link": "http://arxiv.org/abs/cs/0509067v1",
        "categories": [
            "cs.DC",
            "cs.NA",
            "math.AG"
        ]
    },
    {
        "title": "State-Based Control of Fuzzy Discrete Event Systems",
        "authors": [
            "Yongzhi Cao",
            "Mingsheng Ying",
            "Guoqing Chen"
        ],
        "summary": "To effectively represent possibility arising from states and dynamics of a system, fuzzy discrete event systems as a generalization of conventional discrete event systems have been introduced recently. Supervisory control theory based on event feedback has been well established for such systems. Noting that the system state description, from the viewpoint of specification, seems more convenient, we investigate the state-based control of fuzzy discrete event systems in this paper. We first present an approach to finding all fuzzy states that are reachable by controlling the system. After introducing the notion of controllability for fuzzy states, we then provide a necessary and sufficient condition for a set of fuzzy states to be controllable. We also find that event-based control and state-based control are not equivalent and further discuss the relationship between them. Finally, we examine the possibility of driving a fuzzy discrete event system under control from a given initial state to a prescribed set of fuzzy states and then keeping it there indefinitely.",
        "published": "2005-09-30T12:24:31Z",
        "link": "http://arxiv.org/abs/cs/0509099v3",
        "categories": [
            "cs.DM",
            "cs.DC",
            "G.3; I.6.8"
        ]
    },
    {
        "title": "Acceleration of adaptive optics simulations using programmable logic",
        "authors": [
            "A. G. Basden",
            "F. Assemat",
            "T. Butterley",
            "D. Geng",
            "C. D. Saunter",
            "R. W. Wilson"
        ],
        "summary": "Numerical Simulation is an essential part of the design and optimisation of astronomical adaptive optics systems. Simulations of adaptive optics are computationally expensive and the problem scales rapidly with telescope aperture size, as the required spatial order of the correcting system increases. Practical realistic simulations of AO systems for extremely large telescopes are beyond the capabilities of all but the largest of modern parallel supercomputers. Here we describe a more cost effective approach through the use of hardware acceleration using field programmable gate arrays. By transferring key parts of the simulation into programmable logic, large increases in computational bandwidth can be expected. We show that the calculation of wavefront sensor image centroids can be accelerated by a factor of four by transferring the algorithm into hardware. Implementing more demanding parts of the adaptive optics simulation in hardware will lead to much greater performance improvements, of up to 1000 times.",
        "published": "2005-10-03T12:37:03Z",
        "link": "http://arxiv.org/abs/astro-ph/0510041v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Defining a Comprehensive Threat Model for High Performance Computational   Clusters",
        "authors": [
            "Dmitry Mogilevsky",
            "Adam Lee",
            "William Yurcik"
        ],
        "summary": "Over the past decade, high performance computational (HPC) clusters have become mainstream in academic and industrial settings as accessible means of computation. Throughout their proliferation, HPC security has been a secondary concern to performance. It is evident, however, that ensuring HPC security presents different challenges than the ones faced when dealing with traditional networks. To design suitable security measures for high performance computing, it is necessary to first realize the threats faced by such an environment. This task can be accomplished by the means of constructing a comprehensive threat model. To our knowledge, no such threat model exists with regards to Cluster Computing. In this paper, we explore the unique challenges of securing HPCs and propose a threat model based on the classical Confidentiality, Integrity and Availability security principles.",
        "published": "2005-10-16T23:32:52Z",
        "link": "http://arxiv.org/abs/cs/0510046v1",
        "categories": [
            "cs.CR",
            "cs.DC"
        ]
    },
    {
        "title": "Deterministic boundary recognition and topology extraction for large   sensor networks",
        "authors": [
            "Alexander Kroeller",
            "Sandor P. Fekete",
            "Dennis Pfisterer",
            "Stefan Fischer"
        ],
        "summary": "We present a new framework for the crucial challenge of self-organization of a large sensor network. The basic scenario can be described as follows: Given a large swarm of immobile sensor nodes that have been scattered in a polygonal region, such as a street network. Nodes have no knowledge of size or shape of the environment or the position of other nodes. Moreover, they have no way of measuring coordinates, geometric distances to other nodes, or their direction. Their only way of interacting with other nodes is to send or to receive messages from any node that is within communication range. The objective is to develop algorithms and protocols that allow self-organization of the swarm into large-scale structures that reflect the structure of the street network, setting the stage for global routing, tracking and guiding algorithms.",
        "published": "2005-10-17T12:23:15Z",
        "link": "http://arxiv.org/abs/cs/0510048v1",
        "categories": [
            "cs.DC",
            "cs.CG",
            "C.2.1; F.2.2"
        ]
    },
    {
        "title": "A new authentication protocol for revocable anonymity in ad-hoc networks",
        "authors": [
            "Adam Wierzbicki",
            "Aneta Zwierko",
            "Zbigniew Kotulski"
        ],
        "summary": "This paper describes a new protocol for authentication in ad-hoc networks. The protocol has been designed to meet specialized requirements of ad-hoc networks, such as lack of direct communication between nodes or requirements for revocable anonymity. At the same time, a ad-hoc authentication protocol must be resistant to spoofing, eavesdropping and playback, and man-in-the-middle attacks. The article analyzes existing authentication methods based on the Public Key Infrastructure, and finds that they have several drawbacks in ad-hoc networks. Therefore, a new authentication protocol, basing on established cryptographic primitives (Merkle's puzzles and zero-knowledge proofs) is proposed. The protocol is studied for a model ad-hoc chat application that provides private conversations.",
        "published": "2005-10-22T11:51:30Z",
        "link": "http://arxiv.org/abs/cs/0510065v1",
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI",
            "C.2.0; C.2.4"
        ]
    },
    {
        "title": "Using the Parallel Virtual Machine for Everyday Analysis",
        "authors": [
            "M. S. Noble",
            "J. C. Houck",
            "J. E. Davis",
            "A. Young",
            "M. Nowak"
        ],
        "summary": "A review of the literature reveals that while parallel computing is sometimes employed by astronomers for custom, large-scale calculations, no package fosters the routine application of parallel methods to standard problems in astronomical data analysis. This paper describes our attempt to close that gap by wrapping the Parallel Virtual Machine (PVM) as a scriptable S-Lang module. Using PVM within ISIS, the Interactive Spectral Interpretation System, we've distributed a number of representive calculations over a network of 25+ CPUs to achieve dramatic reductions in execution times. We discuss how the approach applies to a wide class of modeling problems, outline our efforts to make it more transparent for common use, and note its growing importance in the context of the large, multi-wavelength datasets used in modern analysis.",
        "published": "2005-10-24T15:17:36Z",
        "link": "http://arxiv.org/abs/astro-ph/0510688v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "Virtual Environments for multiphysics code validation on Computing Grids",
        "authors": [
            "Toan Nguyen",
            "Lizhe Wang",
            "Vittorio Selmin"
        ],
        "summary": "We advocate in this paper the use of grid-based infrastructures that are designed for seamless approaches to the numerical expert users, i.e., the multiphysics applications designers. It relies on sophisticated computing environments based on computing grids, connecting heterogeneous computing resources: mainframes, PC-clusters and workstations running multiphysics codes and utility software, e.g., visualization tools. The approach is based on concepts defined by the HEAVEN* consortium. HEAVEN is a European scientific consortium including industrial partners from the aerospace, telecommunication and software industries, as well as academic research institutes. Currently, the HEAVEN consortium works on a project that aims to create advanced services platforms. It is intended to enable \"virtual private grids\" supporting various environments for users manipulating a suitable high-level interface. This will become the basis for future generalized services allowing the integration of various services without the need to deploy specific grid infrastructures.",
        "published": "2005-10-26T12:26:57Z",
        "link": "http://arxiv.org/abs/cs/0510081v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "Big Science with a Small Budget: Non-Embarrassingly Parallel   Applications in a Non-Dedicated Network of Workstations",
        "authors": [
            "Angel de Vicente",
            "Nayra Rodriguez"
        ],
        "summary": "Many astronomers and astrophysicists require large computing resources for their research, which are usually obtained via dedicated (and expensive) parallel machines. Depending on the type of the problem to be solved, an alternative solution can be provided by creating dynamically a computer cluster out of non-dedicated workstations using the Condor High Throughput Computing System and the Master-Worker (MW) framework. As an example of this we show in this paper how a radiative transfer application previously coded with MPI is solved using this solution without the need for dedicated machines.",
        "published": "2005-10-31T14:55:57Z",
        "link": "http://arxiv.org/abs/cs/0510094v1",
        "categories": [
            "cs.DC",
            "astro-ph"
        ]
    },
    {
        "title": "Gradient Based Routing in Wireless Sensor Networks: a Mixed Strategy",
        "authors": [
            "Olivier Powell",
            "Aubin Jarry",
            "Pierre Leone",
            "Jose Rolim"
        ],
        "summary": "We show how recent theoretical advances for data-propagation in Wireless Sensor Networks (WSNs) can be combined to improve gradient-based routing (GBR) in Wireless Sensor Networks. We propose a mixed-strategy of direct transmission and multi-hop propagation of data which improves the lifespan of WSNs by reaching better energy-load-balancing amongst sensor nodes.",
        "published": "2005-11-23T18:13:18Z",
        "link": "http://arxiv.org/abs/cs/0511083v1",
        "categories": [
            "cs.DC",
            "C.2.1"
        ]
    },
    {
        "title": "A Machine-Independent port of the MPD language run time system to NetBSD",
        "authors": [
            "Ignatios Souvatzis"
        ],
        "summary": "SR (synchronizing resources) is a PASCAL - style language enhanced with constructs for concurrent programming developed at the University of Arizona in the late 1980s. MPD (presented in Gregory Andrews' book about Foundations of Multithreaded, Parallel, and Distributed Programming) is its successor, providing the same language primitives with a different, more C-style, syntax.   The run-time system (in theory, identical, but not designed for sharing) of those languages provides the illusion of a multiprocessor machine on a single Unix-like system or a (local area) network of Unix-like machines.   Chair V of the Computer Science Department of the University of Bonn is operating a laboratory for a practical course in parallel programming consisting of computing nodes running NetBSD/arm, normally used via PVM, MPI etc.   We are considering to offer SR and MPD for this, too. As the original language distributions were only targeted at a few commercial Unix systems, some porting effort is needed. However, some of the porting effort of our earlier SR port should be reusable.   The integrated POSIX threads support of NetBSD-2.0 and later allows us to use library primitives provided for NetBSD's phtread system to implement the primitives needed by the SR run-time system, thus implementing 13 target CPUs at once and automatically making use of SMP on VAX, Alpha, PowerPC, Sparc, 32-bit Intel and 64 bit AMD CPUs.   We'll present some methods used for the impementation and compare some performance values to the traditional implementation.",
        "published": "2005-11-28T13:33:14Z",
        "link": "http://arxiv.org/abs/cs/0511094v1",
        "categories": [
            "cs.DC",
            "cs.PL",
            "D.1.3; D.3.4"
        ]
    },
    {
        "title": "The use of the GARP genetic algorithm and internet grid computing in the   Lifemapper world atlas of species biodiversity",
        "authors": [
            "David R. B. Stockwell",
            "James H. Beach",
            "Aimee Stewart",
            "Gregory Vorontsov",
            "David Vieglais",
            "Ricardo Scachetti Pereira"
        ],
        "summary": "Lifemapper (http://www.lifemapper.org) is a predictive electronic atlas of the Earth's biological biodiversity. Using a screensaver version of the GARP genetic algorithm for modeling species distributions, Lifemapper harnesses vast computing resources through 'volunteers' PCs similar to SETI@home, to develop models of the distribution of the worlds fauna and flora. The Lifemapper project's primary goal is to provide an up to date and comprehensive database of species maps and prediction models (i.e. a fauna and flora of the world) using available data on species' locations. The models are developed using specimen data from distributed museum collections and an archive of geospatial environmental correlates. A central server maintains a dynamic archive of species maps and models for research, outreach to the general community, and feedback to museum data providers. This paper is a case study in the role, use and justification of a genetic algorithm in development of large-scale environmental informatics infrastructure.",
        "published": "2005-11-28T17:54:03Z",
        "link": "http://arxiv.org/abs/q-bio/0511045v1",
        "categories": [
            "q-bio.QM",
            "cs.DC",
            "cs.NE",
            "q-bio.OT"
        ]
    },
    {
        "title": "On Ants, Bacteria and Dynamic Environments",
        "authors": [
            "Vitorino Ramos",
            "Carlos Fernandes",
            "Agostinho C. Rosa"
        ],
        "summary": "Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective swarm intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Then, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed.",
        "published": "2005-12-01T04:52:00Z",
        "link": "http://arxiv.org/abs/cs/0512005v1",
        "categories": [
            "cs.DC"
        ]
    },
    {
        "title": "The Poster Session of SSS 2005",
        "authors": [
            "Brahim Hamid",
            "Ted Herman",
            "Morten Mjelde"
        ],
        "summary": "This technical report documents the poster session of SSS 2005, the Symposium on Self-Stabilizing Systems published by Springer as LNCS volume 3764. The poster session included five presentations. Two of these presentations are summarized in brief abstracts contained in this technical report.",
        "published": "2005-12-05T22:51:11Z",
        "link": "http://arxiv.org/abs/cs/0512021v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.4.5"
        ]
    },
    {
        "title": "Distributed Navigation Algorithms for Sensor Networks",
        "authors": [
            "Chiranjeeb Buragohain",
            "Divyakant Agrawal",
            "Subhash Suri"
        ],
        "summary": "We propose efficient distributed algorithms to aid navigation of a user through a geographic area covered by sensors. The sensors sense the level of danger at their locations and we use this information to find a safe path for the user through the sensor field. Traditional distributed navigation algorithms rely upon flooding the whole network with packets to find an optimal safe path. To reduce the communication expense, we introduce the concept of a skeleton graph which is a sparse subset of the true sensor network communication graph. Using skeleton graphs we show that it is possible to find approximate safe paths with much lower communication cost. We give tight theoretical guarantees on the quality of our approximation and by simulation, show the effectiveness of our algorithms in realistic sensor network situations.",
        "published": "2005-12-14T22:36:53Z",
        "link": "http://arxiv.org/abs/cs/0512060v1",
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.DS"
        ]
    },
    {
        "title": "Flat Holonomies on Automata Networks",
        "authors": [
            "Gene Itkis",
            "Leonid A. Levin"
        ],
        "summary": "We consider asynchronous networks of identical finite (independent of network's size or topology) automata. Our automata drive any network from any initial configuration of states, to a coherent one in which it can carry efficiently any computations implementable on synchronous properly initialized networks of the same size.   A useful data structure on such networks is a partial orientation of its edges. It needs to be flat, i.e. have null holonomy (no excess of up or down edges in any cycle). It also needs to be centered, i.e. have a unique node with no down edges.   There are (interdependent) self-stabilizing asynchronous finite automata protocols assuring flat centered orientation. Such protocols may vary in assorted efficiency parameters and it is desirable to have each replaceable with any alternative, responsible for a simple limited task. We describe an efficient reduction of any computational task to any such set of protocols compliant with our interface conditions.",
        "published": "2005-12-20T11:35:10Z",
        "link": "http://arxiv.org/abs/cs/0512077v7",
        "categories": [
            "cs.DC",
            "cs.DM"
        ]
    },
    {
        "title": "Advances towards a General-Purpose Societal-Scale Human-Collective   Problem-Solving Engine",
        "authors": [
            "Marko Rodriguez"
        ],
        "summary": "Human collective intelligence has proved itself as an important factor in a society's ability to accomplish large-scale behavioral feats. As societies have grown in population-size, individuals have seen a decrease in their ability to activeily participate in the problem-solving processes of the group. Representative decision-making structures have been used as a modern solution to society's inadequate information-processing infrastructure. With computer and network technologies being further embedded within the fabric of society, the implementation of a general-purpose societal-scale human-collective problem-solving engine is envisioned as a means of furthering the collective-intelligence potential of society. This paper provides both a novel framework for creating collective intelligence systems and a method for implementing a representative and expertise system based on social-network theory.",
        "published": "2005-01-03T15:11:07Z",
        "link": "http://arxiv.org/abs/cs/0501004v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "H.1.2; H.4.2; H.5.3"
        ]
    },
    {
        "title": "Weighted Automata in Text and Speech Processing",
        "authors": [
            "Mehryar Mohri",
            "Fernando Pereira",
            "Michael Riley"
        ],
        "summary": "Finite-state automata are a very effective tool in natural language processing. However, in a variety of applications and especially in speech precessing, it is necessary to consider more general machines in which arcs are assigned weights or costs. We briefly describe some of the main theoretical and algorithmic aspects of these machines. In particular, we describe an efficient composition algorithm for weighted transducers, and give examples illustrating the value of determinization and minimization algorithms for weighted automata.",
        "published": "2005-03-29T04:59:50Z",
        "link": "http://arxiv.org/abs/cs/0503077v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "F.1.1; F.4.3; H.5.2"
        ]
    },
    {
        "title": "SWiM: A Simple Window Mover",
        "authors": [
            "Tony Chang",
            "Damon Cook",
            "Ramona Su"
        ],
        "summary": "As computers become more ubiquitous, traditional two-dimensional interfaces must be replaced with interfaces based on a three-dimensional metaphor. However, these interfaces must still be as simple and functional as their two-dimensional predecessors. This paper introduces SWiM, a new interface for moving application windows between various screens, such as wall displays, laptop monitors, and desktop displays, in a three-dimensional physical environment. SWiM was designed based on the results of initial \"paper and pencil\" user tests of three possible interfaces. The results of these tests led to a map-like interface where users select the destination display for their application from various icons. If the destination is a mobile display it is not displayed on the map. Instead users can select the screen's name from a list of all possible destination displays. User testing of SWiM was conducted to discover whether it is easy to learn and use. Users that were asked to use SWiM without any instructions found the interface as intuitive to use as users who were given a demonstration. The results show that SWiM combines simplicity and functionality to create an interface that is easy to learn and easy to use.",
        "published": "2005-05-04T16:17:54Z",
        "link": "http://arxiv.org/abs/cs/0505011v1",
        "categories": [
            "cs.HC"
        ]
    },
    {
        "title": "The Cyborg Astrobiologist: Scouting Red Beds for Uncommon Features with   Geological Significance",
        "authors": [
            "Patrick C. McGuire",
            "Enrique Diaz-Martinez",
            "Jens Ormo",
            "Javier Gomez-Elvira",
            "Jose A. Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Helge Ritter",
            "Robert Haschke",
            "Markus Oesker",
            "Joerg Ontrup"
        ],
        "summary": "The `Cyborg Astrobiologist' (CA) has undergone a second geological field trial, at a red sandstone site in northern Guadalajara, Spain, near Riba de Santiuste. The Cyborg Astrobiologist is a wearable computer and video camera system that has demonstrated a capability to find uncommon interest points in geological imagery in real-time in the field. The first (of three) geological structures that we studied was an outcrop of nearly homogeneous sandstone, which exhibits oxidized-iron impurities in red and and an absence of these iron impurities in white. The white areas in these ``red beds'' have turned white because the iron has been removed by chemical reduction, perhaps by a biological agent. The computer vision system found in one instance several (iron-free) white spots to be uncommon and therefore interesting, as well as several small and dark nodules. The second geological structure contained white, textured mineral deposits on the surface of the sandstone, which were found by the CA to be interesting. The third geological structure was a 50 cm thick paleosol layer, with fossilized root structures of some plants, which were found by the CA to be interesting. A quasi-blind comparison of the Cyborg Astrobiologist's interest points for these images with the interest points determined afterwards by a human geologist shows that the Cyborg Astrobiologist concurred with the human geologist 68% of the time (true positive rate), with a 32% false positive rate and a 32% false negative rate.   (abstract has been abridged).",
        "published": "2005-05-23T09:55:37Z",
        "link": "http://arxiv.org/abs/cs/0505058v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO",
            "cs.SE",
            "physics.ins-det",
            "q-bio.NC",
            "I.2.10; I.4.6; I.4.8; I.4.9; I.2.9; I.5.4; I.5.5; J.2; J.3; D.2;\n  D.1.7; D.4.7"
        ]
    },
    {
        "title": "Multi-Modal Human-Machine Communication for Instructing Robot Grasping   Tasks",
        "authors": [
            "P. C. McGuire",
            "J. Fritsch",
            "J. J. Steil",
            "F. Roethling",
            "G. A. Fink",
            "S. Wachsmuth",
            "G. Sagerer",
            "H. Ritter"
        ],
        "summary": "A major challenge for the realization of intelligent robots is to supply them with cognitive abilities in order to allow ordinary users to program them easily and intuitively. One way of such programming is teaching work tasks by interactive demonstration. To make this effective and convenient for the user, the machine must be capable to establish a common focus of attention and be able to use and integrate spoken instructions, visual perceptions, and non-verbal clues like gestural commands. We report progress in building a hybrid architecture that combines statistical methods, neural networks, and finite state machines into an integrated system for instructing grasping tasks by man-machine interaction. The system combines the GRAVIS-robot for visual attention and gestural instruction with an intelligent interface for speech recognition and linguistic interpretation, and an modality fusion module to allow multi-modal task-oriented man-machine communication with respect to dextrous robot manipulation of objects.",
        "published": "2005-05-24T14:53:49Z",
        "link": "http://arxiv.org/abs/cs/0505064v1",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO",
            "H.1.2; I.2.9; I.2.10; I.2.7; H.5.2; H.5.1; I.2.6; I.4.8; I.4.7;\n  I.4.6"
        ]
    },
    {
        "title": "Field geology with a wearable computer: 1st results of the Cyborg   Astrobiologist System",
        "authors": [
            "Patrick C. McGuire",
            "Javier Gomez-Elvira",
            "Jose Antonio Rodriguez-Manfredi",
            "Eduardo Sebastian-Martinez",
            "Jens Ormo",
            "Enrique Diaz-Martinez",
            "Markus Oesker",
            "Robert Haschke",
            "Joerg Ontrup",
            "Helge Ritter"
        ],
        "summary": "We present results from the first geological field tests of the `Cyborg Astrobiologist', which is a wearable computer and video camcorder system that we are using to test and train a computer-vision system towards having some of the autonomous decision-making capabilities of a field-geologist. The Cyborg Astrobiologist platform has thus far been used for testing and development of these algorithms and systems: robotic acquisition of quasi-mosaics of images, real-time image segmentation, and real-time determination of interesting points in the image mosaics. This work is more of a test of the whole system, rather than of any one part of the system. However, beyond the concept of the system itself, the uncommon map (despite its simplicity) is the main innovative part of the system. The uncommon map helps to determine interest-points in a context-free manner. Overall, the hardware and software systems function reliably, and the computer-vision algorithms are adequate for the first field tests. In addition to the proof-of-concept aspect of these field tests, the main result of these field tests is the enumeration of those issues that we can improve in the future, including: dealing with structural shadow and microtexture, and also, controlling the camera's zoom lens in an intelligent manner. Nonetheless, despite these and other technical inadequacies, this Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer and its computer-vision algorithms, has demonstrated its ability of finding genuinely interesting points in real-time in the geological scenery, and then gathering more information about these interest points in an automated manner. We use these capabilities for autonomous guidance towards geological points-of-interest.",
        "published": "2005-06-24T10:25:22Z",
        "link": "http://arxiv.org/abs/cs/0506089v1",
        "categories": [
            "cs.CV",
            "astro-ph",
            "cs.AI",
            "cs.CE",
            "cs.HC",
            "cs.RO"
        ]
    },
    {
        "title": "Making Space for Stories: Ambiguity in the Design of Personal   Communication Systems",
        "authors": [
            "Paul M. Aoki",
            "Allison Woodruff"
        ],
        "summary": "Pervasive personal communication technologies offer the potential for important social benefits for individual users, but also the potential for significant social difficulties and costs. In research on face-to-face social interaction, ambiguity is often identified as an important resource for resolving social difficulties. In this paper, we discuss two design cases of personal communication systems, one based on fieldwork of a commercial system and another based on an unrealized design concept. The cases illustrate how user behavior concerning a particular social difficulty, unexplained unresponsiveness, can be influenced by technological issues that result in interactional ambiguity. The cases also highlight the need to balance the utility of ambiguity against the utility of usability and communicative clarity.",
        "published": "2005-07-06T21:29:09Z",
        "link": "http://arxiv.org/abs/cs/0507019v1",
        "categories": [
            "cs.HC",
            "H.4.3"
        ]
    },
    {
        "title": "Adapting CBPP platforms for instructional use",
        "authors": [
            "Robert Milson",
            "Aaron Krowne"
        ],
        "summary": "Commons based peer-production (CBPP) is the de-centralized, net-based approach to the creation and dissemination of information resources. Underlying every CBPP system is a virtual community brought together by an internet tool (such as a web site) and structured by a specific collaboration protocol. In this talk we will argue that the value of such platforms can be leveraged by adapting them for pedagogical purposes.   We report on one such recent adaptation. The Noosphere system is a web-based collaboration environment that underlies the popular Planetmath website, a collaboratively written encyclopedia of mathematics licensed under the GNU Free Documentation License (FDL). Recently, the system was used to host a graduate-level mathematics course at Dalhousie University, in Halifax, Canada. The course consisted of regular lectures and assignment problems. The students in the course collaborated on a set of course notes, encapsulating the lecture content and giving solutions of assigned problems. The successful outcome of this experiment demonstrated that a dedicated Noosphere system is well suited for classroom applications. We argue that this ``proof of concept'' experience also strongly suggests that every successful CBPP platform possesses latent pedagogical value.",
        "published": "2005-07-10T19:05:26Z",
        "link": "http://arxiv.org/abs/cs/0507028v1",
        "categories": [
            "cs.DL",
            "cs.HC",
            "H.3.7; J.2"
        ]
    },
    {
        "title": "OpenVanilla - A Non-Intrusive Plug-In Framework of Text Services",
        "authors": [
            "Tian-Jian Jiang",
            "Deng-Liu",
            "Kang-min Liu",
            "Weizhong Yang",
            "Pek-tiong Tan",
            "Mengjuei Hsieh",
            "Tsung-hsiang Chang",
            "Wen-Lien Hsu"
        ],
        "summary": "Input method (IM) is a sine qua non for text entry of many Asian languages, but its potential applications on other languages remain under-explored. This paper proposes a philosophy of input method design by seeing it as a nonintrusive plug-in text service framework. Such design allows new functionalities of text processing to be attached onto a running application without any tweaking of code. We also introduce OpenVanilla, a cross-platform framework that is designed with the above-mentioned model in mind. Frameworks like OpenVanilla have shown that an input method can be more than just a text entry tool: it offers a convenient way for developing various text service and language tools.",
        "published": "2005-08-04T22:39:49Z",
        "link": "http://arxiv.org/abs/cs/0508041v2",
        "categories": [
            "cs.HC",
            "H.5.2"
        ]
    },
    {
        "title": "OpenVanilla - A Non-Intrusive Plug-In Framework of Text Services",
        "authors": [
            "Tien-chien Chiang",
            "Deng-Liu",
            "Kang-min Liu",
            "Weizhong Yang",
            "Pek-tiong Tan",
            "Mengjuei Hsieh",
            "Tsung-hsiang Chang",
            "Wen-Lien Hsu"
        ],
        "summary": "This paper has been withdrawn by the author, because it was merged into cs.HC/0508041",
        "published": "2005-08-04T22:49:38Z",
        "link": "http://arxiv.org/abs/cs/0508042v2",
        "categories": [
            "cs.HC",
            "H.5.2"
        ]
    },
    {
        "title": "COMODI: On the Graphical User Interface",
        "authors": [
            "Zsolt I. Lázár",
            "Andreea Fanea",
            "Dragoş Petraşcu",
            "Vladiela Ciobotariu-Boer",
            "Bazil Pârv"
        ],
        "summary": "We propose a series of features for the graphical user interface (GUI) of the COmputational MOdule Integrator (COMODI) \\cite{Synasc05a}\\cite{COMODI}. In view of the special requirements that a COMODI type of framework for scientific computing imposes and inspiring from existing solutions that provide advanced graphical visual programming environments, we identify those elements and associated behaviors that will have to find their way into the first release of COMODI.",
        "published": "2005-10-14T01:03:55Z",
        "link": "http://arxiv.org/abs/cs/0510034v1",
        "categories": [
            "cs.HC",
            "cs.CE",
            "cs.MS",
            "D.2.13; D.2.12; D.2.11; D.2.9; D.2.6"
        ]
    }
]