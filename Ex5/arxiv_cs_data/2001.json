[
    {
        "title": "On the problem of computing the well-founded semantics",
        "authors": [
            "Zbigniew Lonc",
            "Miroslaw Truszczynski"
        ],
        "summary": "The well-founded semantics is one of the most widely studied and used semantics of logic programs with negation. In the case of finite propositional programs, it can be computed in polynomial time, more specifically, in O(|At(P)|size(P)) steps, where size(P) denotes the total number of occurrences of atoms in a logic program P. This bound is achieved by an algorithm introduced by Van Gelder and known as the alternating-fixpoint algorithm. Improving on the alternating-fixpoint algorithm turned out to be difficult. In this paper we study extensions and modifications of the alternating-fixpoint approach. We then restrict our attention to the class of programs whose rules have no more than one positive occurrence of an atom in their bodies. For programs in that class we propose a new implementation of the alternating-fixpoint method in which false atoms are computed in a top-down fashion. We show that our algorithm is faster than other known algorithms and that for a wide class of programs it is linear and so, asymptotically optimal.",
        "published": "2001-01-17T13:33:12Z",
        "link": "http://arxiv.org/abs/cs/0101014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.DS",
            "I.2.3; F.2.2"
        ]
    },
    {
        "title": "General Loss Bounds for Universal Sequence Prediction",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The Bayesian framework is ideally suited for induction problems. The probability of observing $x_t$ at time $t$, given past observations $x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\\mu$ of the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many cases one does not even have a reasonable estimate of the true distribution. In order to overcome this problem a universal distribution $\\xi$ is defined as a weighted sum of distributions $\\mu_i\\inM$, where $M$ is any countable set of distributions including $\\mu$. This is a generalization of Solomonoff induction, in which $M$ is the set of all enumerable semi-measures. Systems which predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$ if $x_t$ is the true next symbol of the sequence are considered. It is proven that using the universal $\\xi$ as a prior is nearly as good as using the unknown true distribution $\\mu$. Furthermore, games of chance, defined as a sequence of bets, observations, and rewards are studied. The time needed to reach the winning zone is bounded in terms of the relative entropy of $\\mu$ and $\\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and more active systems are discussed.",
        "published": "2001-01-21T17:19:37Z",
        "link": "http://arxiv.org/abs/cs/0101019v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "math.ST",
            "stat.TH",
            "I.2; I.2.6; I.2.8; F.1.3"
        ]
    },
    {
        "title": "The Generalized Universal Law of Generalization",
        "authors": [
            "Nick Chater",
            "Paul Vitanyi"
        ],
        "summary": "It has been argued by Shepard that there is a robust psychological law that relates the distance between a pair of items in psychological space and the probability that they will be confused with each other. Specifically, the probability of confusion is a negative exponential function of the distance between the pair of items. In experimental contexts, distance is typically defined in terms of a multidimensional Euclidean space-but this assumption seems unlikely to hold for complex stimuli. We show that, nonetheless, the Universal Law of Generalization can be derived in the more complex setting of arbitrary stimuli, using a much more universal measure of distance. This universal distance is defined as the length of the shortest program that transforms the representations of the two items of interest into one another: the algorithmic information distance. It is universal in the sense that it minorizes every computable distance: it is the smallest computable distance. We show that the universal law of generalization holds with probability going to one-provided the confusion probabilities are computable. We also give a mathematically more appealing form",
        "published": "2001-01-29T17:54:50Z",
        "link": "http://arxiv.org/abs/cs/0101036v1",
        "categories": [
            "cs.CV",
            "cs.AI",
            "math.PR",
            "physics.soc-ph",
            "J.4"
        ]
    },
    {
        "title": "On the predictability of Rainfall in Kerala- An application of ABF   Neural Network",
        "authors": [
            "Ninan Sajeeth Philip",
            "K. Babu Joseph"
        ],
        "summary": "Rainfall in Kerala State, the southern part of Indian Peninsula in particular is caused by the two monsoons and the two cyclones every year. In general, climate and rainfall are highly nonlinear phenomena in nature giving rise to what is known as the `butterfly effect'. We however attempt to train an ABF neural network on the time series rainfall data and show for the first time that in spite of the fluctuations resulting from the nonlinearity in the system, the trends in the rainfall pattern in this corner of the globe have remained unaffected over the past 87 years from 1893 to 1980. We also successfully filter out the chaotic part of the system and illustrate that its effects are marginal over long term predictions.",
        "published": "2001-02-18T19:17:18Z",
        "link": "http://arxiv.org/abs/cs/0102014v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "A0"
        ]
    },
    {
        "title": "An effective Procedure for Speeding up Algorithms",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The provably asymptotically fastest algorithm within a factor of 5 for formally described problems will be constructed. The main idea is to enumerate all programs provably equivalent to the original problem by enumerating all proofs. The algorithm could be interpreted as a generalization and improvement of Levin search, which is, within a multiplicative constant, the fastest algorithm for inverting functions. Blum's speed-up theorem is avoided by taking into account only programs for which a correctness proof exists. Furthermore, it is shown that the fastest program that computes a certain function is also one of the shortest programs provably computing this function. To quantify this statement, the definition of Kolmogorov complexity is extended, and two new natural measures for the complexity of a function are defined.",
        "published": "2001-02-21T20:52:28Z",
        "link": "http://arxiv.org/abs/cs/0102018v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.LG",
            "F.2.3"
        ]
    },
    {
        "title": "Gene Expression Programming: a New Adaptive Algorithm for Solving   Problems",
        "authors": [
            "Candida Ferreira"
        ],
        "summary": "Gene expression programming, a genotype/phenotype genetic algorithm (linear and ramified), is presented here for the first time as a new technique for the creation of computer programs. Gene expression programming uses character linear chromosomes composed of genes structurally organized in a head and a tail. The chromosomes function as a genome and are subjected to modification by means of mutation, transposition, root transposition, gene transposition, gene recombination, and one- and two-point recombination. The chromosomes encode expression trees which are the object of selection. The creation of these separate entities (genome and expression tree) with distinct functions allows the algorithm to perform with high efficiency that greatly surpasses existing adaptive techniques. The suite of problems chosen to illustrate the power and versatility of gene expression programming includes symbolic regression, sequence induction with and without constant creation, block stacking, cellular automata rules for the density-classification problem, and two problems of boolean concept learning: the 11-multiplexer and the GP rule problem.",
        "published": "2001-02-25T19:29:55Z",
        "link": "http://arxiv.org/abs/cs/0102027v3",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.2"
        ]
    },
    {
        "title": "Quantitative Neural Network Model of the Tip-of-the-Tongue Phenomenon   Based on Synthesized Memory-Psycholinguistic-Metacognitive Approach",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "A new three-stage computer artificial neural network model of the tip-of-the-tongue phenomenon is proposed. Each word's node is build from some interconnected learned auto-associative two-layer neural networks each of which represents separate word's semantic, lexical, or phonological components. The model synthesizes memory, psycholinguistic, and metamemory approaches, bridges speech errors and naming chronometry research traditions, and can explain quantitatively many tip-of-the-tongue effects.",
        "published": "2001-03-02T00:20:01Z",
        "link": "http://arxiv.org/abs/cs/0103002v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC",
            "q-bio.QM",
            "I.2.7"
        ]
    },
    {
        "title": "Fitness Uniform Selection to Preserve Genetic Diversity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "In evolutionary algorithms, the fitness of a population increases with time by mutating and recombining individuals and by a biased selection of more fit individuals. The right selection pressure is critical in ensuring sufficient optimization progress on the one hand and in preserving genetic diversity to be able to escape from local optima on the other. We propose a new selection scheme, which is uniform in the fitness values. It generates selection pressure towards sparsely populated fitness regions, not necessarily towards higher fitness, as is the case for all other selection schemes. We show that the new selection scheme can be much more effective than standard selection schemes.",
        "published": "2001-03-14T18:40:32Z",
        "link": "http://arxiv.org/abs/cs/0103015v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "q-bio",
            "I.2; I.2.6; I.2.8; F.2"
        ]
    },
    {
        "title": "Belief Revision: A Critique",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "We examine carefully the rationale underlying the approaches to belief change taken in the literature, and highlight what we view as methodological problems. We argue that to study belief change carefully, we must be quite explicit about the ``ontology'' or scenario underlying the belief change process. This is something that has been missing in previous work, with its focus on postulates. Our analysis shows that we must pay particular attention to two issues that have often been taken for granted: The first is how we model the agent's epistemic state. (Do we use a set of beliefs, or a richer structure, such as an ordering on worlds? And if we use a set of beliefs, in what language are these beliefs are expressed?) We show that even postulates that have been called ``beyond controversy'' are unreasonable when the agent's beliefs include beliefs about her own epistemic state as well as the external world. The second is the status of observations. (Are observations known to be true, or just believed? In the latter case, how firm is the belief?) Issues regarding the status of observations arise particularly when we consider iterated belief revision, and we must confront the possibility of revising by p and then by not-p.",
        "published": "2001-03-27T20:33:51Z",
        "link": "http://arxiv.org/abs/cs/0103020v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "Local Search Techniques for Constrained Portfolio Selection Problems",
        "authors": [
            "Andrea Schaerf"
        ],
        "summary": "We consider the problem of selecting a portfolio of assets that provides the investor a suitable balance of expected return and risk. With respect to the seminal mean-variance model of Markowitz, we consider additional constraints on the cardinality of the portfolio and on the quantity of individual shares. Such constraints better capture the real-world trading system, but make the problem more difficult to be solved with exact methods. We explore the use of local search techniques, mainly tabu search, for the portfolio selection problem. We compare and combine previous work on portfolio selection that makes use of the local search approach and we propose new algorithms that combine different neighborhood relations. In addition, we show how the use of randomization and of a simple form of adaptiveness simplifies the setting of a large number of critical parameters. Finally, we show how our techniques perform on public benchmarks.",
        "published": "2001-04-18T13:42:49Z",
        "link": "http://arxiv.org/abs/cs/0104017v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "J.1; I.2.8"
        ]
    },
    {
        "title": "Coaxing Confidences from an Old Friend: Probabilistic Classifications   from Transformation Rule Lists",
        "authors": [
            "Radu Florian",
            "John C. Henderson",
            "Grace Ngai"
        ],
        "summary": "Transformation-based learning has been successfully employed to solve many natural language processing problems. It has many positive features, but one drawback is that it does not provide estimates of class membership probabilities.   In this paper, we present a novel method for obtaining class membership probabilities from a transformation-based rule list classifier. Three experiments are presented which measure the modeling accuracy and cross-entropy of the probabilistic classifier on unseen data and the degree to which the output probabilities from the classifier can be used to estimate confidences in its classification decisions.   The results of these experiments show that, for the task of text chunking, the estimates produced by this technique are more informative than those generated by a state-of-the-art decision tree.",
        "published": "2001-04-27T23:16:21Z",
        "link": "http://arxiv.org/abs/cs/0104020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun   Phrase Chunking",
        "authors": [
            "Grace Ngai",
            "David Yarowsky"
        ],
        "summary": "This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive real-time human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.",
        "published": "2001-05-02T08:39:32Z",
        "link": "http://arxiv.org/abs/cs/0105003v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "The alldifferent Constraint: A Survey",
        "authors": [
            "W. J. van Hoeve"
        ],
        "summary": "The constraint of difference is known to the constraint programming community since Lauriere introduced Alice in 1978. Since then, several solving strategies have been designed for this constraint. In this paper we give both a practical overview and an abstract comparison of these different strategies.",
        "published": "2001-05-08T13:13:04Z",
        "link": "http://arxiv.org/abs/cs/0105015v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.3"
        ]
    },
    {
        "title": "Optimization Over Zonotopes and Training Support Vector Machines",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We make a connection between classical polytopes called zonotopes and Support Vector Machine (SVM) classifiers. We combine this connection with the ellipsoid method to give some new theoretical results on training SVMs. We also describe some special properties of soft margin C-SVMs as parameter C goes to infinity.",
        "published": "2001-05-08T21:07:43Z",
        "link": "http://arxiv.org/abs/cs/0105017v1",
        "categories": [
            "cs.CG",
            "cs.AI",
            "F.2.2; G.1.6; I.2.6; I.5.1"
        ]
    },
    {
        "title": "Solving Composed First-Order Constraints from Discrete-Time Robust   Control",
        "authors": [
            "Stefan Ratschan",
            "Luc Jaulin"
        ],
        "summary": "This paper deals with a problem from discrete-time robust control which requires the solution of constraints over the reals that contain both universal and existential quantifiers. For solving this problem we formulate it as a program in a (fictitious) constraint logic programming language with explicit quantifier notation. This allows us to clarify the special structure of the problem, and to extend an algorithm for computing approximate solution sets of first-order constraints over the reals to exploit this structure. As a result we can deal with inputs that are clearly out of reach for current symbolic solvers.",
        "published": "2001-05-11T08:28:33Z",
        "link": "http://arxiv.org/abs/cs/0105021v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CE",
            "F.4.1;I.2.8"
        ]
    },
    {
        "title": "Multi-Channel Parallel Adaptation Theory for Rule Discovery",
        "authors": [
            "Li Min Fu"
        ],
        "summary": "In this paper, we introduce a new machine learning theory based on multi-channel parallel adaptation for rule discovery. This theory is distinguished from the familiar parallel-distributed adaptation theory of neural networks in terms of channel-based convergence to the target rules. We show how to realize this theory in a learning system named CFRule. CFRule is a parallel weight-based model, but it departs from traditional neural computing in that its internal knowledge is comprehensible. Furthermore, when the model converges upon training, each channel converges to a target rule. The model adaptation rule is derived by multi-level parallel weight optimization based on gradient descent. Since, however, gradient descent only guarantees local optimization, a multi-channel regression-based optimization strategy is developed to effectively deal with this problem. Formally, we prove that the CFRule model can explicitly and precisely encode any given rule set. Also, we prove a property related to asynchronous parallel convergence, which is a critical element of the multi-channel parallel adaptation theory for rule learning. Thanks to the quantizability nature of the CFRule model, rules can be extracted completely and soundly via a threshold-based mechanism. Finally, the practical application of the theory is demonstrated in DNA promoter recognition and hepatitis prognosis prediction.",
        "published": "2001-05-11T14:17:42Z",
        "link": "http://arxiv.org/abs/cs/0105022v1",
        "categories": [
            "cs.AI",
            "I2.6"
        ]
    },
    {
        "title": "Market-Based Reinforcement Learning in Partially Observable Worlds",
        "authors": [
            "Ivo Kwee",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ],
        "summary": "Unlike traditional reinforcement learning (RL), market-based RL is in principle applicable to worlds described by partially observable Markov Decision Processes (POMDPs), where an agent needs to learn short-term memories of relevant previous events in order to execute optimal actions. Most previous work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here we reimplement a recent approach to market-based RL and for the first time evaluate it in a toy POMDP setting.",
        "published": "2001-05-15T19:07:28Z",
        "link": "http://arxiv.org/abs/cs/0105025v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MA",
            "cs.NE",
            "I.2"
        ]
    },
    {
        "title": "Bounds on sample size for policy evaluation in Markov environments",
        "authors": [
            "Leonid Peshkin",
            "Sayan Mukherjee"
        ],
        "summary": "Reinforcement learning means finding the optimal course of action in Markovian environments without knowledge of the environment's dynamics. Stochastic optimization algorithms used in the field rely on estimates of the value of a policy. Typically, the value of a policy is estimated from results of simulating that very policy in the environment. This approach requires a large amount of simulation as different points in the policy space are considered. In this paper, we develop value estimators that utilize data gathered when using one policy to estimate the value of using another policy, resulting in much more data-efficient algorithms. We consider the question of accumulating a sufficient experience and give PAC-style bounds.",
        "published": "2001-05-17T18:33:56Z",
        "link": "http://arxiv.org/abs/cs/0105027v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "G.2;G.1.6;I.2;I.2.6"
        ]
    },
    {
        "title": "Disjunctive Logic Programs with Inheritance",
        "authors": [
            "Francesco Buccafurri",
            "Wolfgang Faber",
            "Nicola Leone"
        ],
        "summary": "The paper proposes a new knowledge representation language, called DLP<, which extends disjunctive logic programming (with strong negation) by inheritance. The addition of inheritance enhances the knowledge modeling features of the language providing a natural representation of default reasoning with exceptions.   A declarative model-theoretic semantics of DLP< is provided, which is shown to generalize the Answer Set Semantics of disjunctive logic programs.   The knowledge modeling features of the language are illustrated by encoding classical nonmonotonic problems in DLP<.   The complexity of DLP< is analyzed, proving that inheritance does not cause any computational overhead, as reasoning in DLP< has exactly the same complexity as reasoning in disjunctive logic programming. This is confirmed by the existence of an efficient translation from DLP< to plain disjunctive logic programming. Using this translation, an advanced KR system supporting the DLP< language has been implemented on top of the DLV system and has subsequently been integrated into DLV.",
        "published": "2001-05-30T20:32:49Z",
        "link": "http://arxiv.org/abs/cs/0105036v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Soft Scheduling",
        "authors": [
            "Hana Rudova"
        ],
        "summary": "Classical notions of disjunctive and cumulative scheduling are studied from the point of view of soft constraint satisfaction. Soft disjunctive scheduling is introduced as an instance of soft CSP and preferences included in this problem are applied to generate a lower bound based on existing discrete capacity resource. Timetabling problems at Purdue University and Faculty of Informatics at Masaryk University considering individual course requirements of students demonstrate practical problems which are solved via proposed methods. Implementation of general preference constraint solver is discussed and first computational results for timetabling problem are presented.",
        "published": "2001-06-02T22:36:02Z",
        "link": "http://arxiv.org/abs/cs/0106004v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "I.2.8; F.4.1; I.6.5"
        ]
    },
    {
        "title": "The Representation of Legal Contracts",
        "authors": [
            "Aspassia Daskalopulu",
            "Marek Sergot"
        ],
        "summary": "The paper outlines ongoing research on logic-based tools for the analysis and representation of legal contracts of the kind frequently encountered in large-scale engineering projects and complex, long-term trading agreements. We consider both contract formation and contract performance, in each case identifying the representational issues and the prospects for providing automated support tools.",
        "published": "2001-06-07T11:00:12Z",
        "link": "http://arxiv.org/abs/cs/0106005v1",
        "categories": [
            "cs.AI",
            "cs.CY",
            "I.2"
        ]
    },
    {
        "title": "A Constraint-Driven System for Contract Assembly",
        "authors": [
            "Aspassia Daskalopulu",
            "Marek Sergot"
        ],
        "summary": "We present an approach for modelling the structure and coarse content of legal documents with a view to providing automated support for the drafting of contracts and contract database retrieval. The approach is designed to be applicable where contract drafting is based on model-form contracts or on existing examples of a similar type. The main features of the approach are: (1) the representation addresses the structure and the interrelationships between the constituent parts of contracts, but not the text of the document itself; (2) the representation of documents is separated from the mechanisms that manipulate it; and (3) the drafting process is subject to a collection of explicitly stated constraints that govern the structure of the documents. We describe the representation of document instances and of 'generic documents', which are data structures used to drive the creation of new document instances, and we show extracts from a sample session to illustrate the features of a prototype system implemented in MacProlog.",
        "published": "2001-06-07T14:27:30Z",
        "link": "http://arxiv.org/abs/cs/0106006v1",
        "categories": [
            "cs.AI",
            "I.2; H.4.1"
        ]
    },
    {
        "title": "Modelling Contractual Arguments",
        "authors": [
            "Chris Reed",
            "Aspassia Daskalopulu"
        ],
        "summary": "One influential approach to assessing the \"goodness\" of arguments is offered by the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can be compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988), an approach that originates in discourse analysis. In p-d terms an argument is good if it avoids committing a fallacy, whereas in RST terms an argument is good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for providing only a partially functional account of argument, and similar criticisms have been raised in the Natural Language Generation (NLG) community-particularly by Moore & Pollack (1992)- with regards to its account of intentionality in text in general. Mann and Thompson themselves note that although RST can be successfully applied to a wide range of texts from diverse domains, it fails to characterise some types of text, most notably legal contracts. There is ongoing research in the Artificial Intelligence and Law community exploring the potential for providing electronic support to contract negotiators, focusing on long-term, complex engineering agreements (see for example Daskalopulu & Sergot 1997). This paper provides a brief introduction to RST and illustrates its shortcomings with respect to contractual text. An alternative approach for modelling argument structure is presented which not only caters for contractual text, but also overcomes the aforementioned limitations of RST.",
        "published": "2001-06-07T14:40:04Z",
        "link": "http://arxiv.org/abs/cs/0106007v1",
        "categories": [
            "cs.AI",
            "I.2"
        ]
    },
    {
        "title": "Computing Functional and Relational Box Consistency by Structured   Propagation in Atomic Constraint Systems",
        "authors": [
            "M. H. van Emden"
        ],
        "summary": "Box consistency has been observed to yield exponentially better performance than chaotic constraint propagation in the interval constraint system obtained by decomposing the original expression into primitive constraints. The claim was made that the improvement is due to avoiding decomposition. In this paper we argue that the improvement is due to replacing chaotic iteration by a more structured alternative.   To this end we distinguish the existing notion of box consistency from relational box consistency. We show that from a computational point of view it is important to maintain the functional structure in constraint systems that are associated with a system of equations. So far, it has only been considered computationally important that constraint propagation be fair. With the additional structure of functional constraint systems, one can define and implement computationally effective, structured, truncated constraint propagations. The existing algorithm for box consistency is one such. Our results suggest that there are others worth investigating.",
        "published": "2001-06-07T14:50:40Z",
        "link": "http://arxiv.org/abs/cs/0106008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3; F.4.1"
        ]
    },
    {
        "title": "Modelling Legal Contracts as Processes",
        "authors": [
            "Aspassia Daskalopulu"
        ],
        "summary": "This paper concentrates on the representation of the legal relations that obtain between parties once they have entered a contractual agreement and their evolution as the agreement progresses through time. Contracts are regarded as process and they are analysed in terms of the obligations that are active at various points during their life span. An informal notation is introduced that summarizes conveniently the states of an agreement as it evolves over time. Such a representation enables us to determine what the status of an agreement is, given an event or a sequence of events that concern the performance of actions by the agents involved. This is useful both in the context of contract drafting (where parties might wish to preview how their agreement might evolve) and in the context of contract performance monitoring (where parties might with to establish what their legal positions are once their agreement is in force). The discussion is based on an example that illustrates some typical patterns of contractual obligations.",
        "published": "2001-06-07T14:59:03Z",
        "link": "http://arxiv.org/abs/cs/0106010v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4"
        ]
    },
    {
        "title": "L.T.Kuzin: Research Program",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "Lev T. Kuzin (1928--1997) is one of the founders of modern cybernetics and information science in Russia. He was awarded and honored the USSR State Prize for inspiring vision into the future of technical cybernetics and his invention and innovation of key technologies.   The last years he interested in the computational models of geometrical and algebraic nature and their applications in various branches of computer science and information technologies. In the recent years the interest in computation models based on object notion has grown tremendously stimulating an interest to Kuzin's ideas. This year of 50th Anniversary of Cybernetics and on the occasion of his 70th birthday on September 12, 1998 seems especially appropriate for discussing Kuzin's Research Program.",
        "published": "2001-06-08T17:42:12Z",
        "link": "http://arxiv.org/abs/cs/0106014v1",
        "categories": [
            "cs.DM",
            "cs.AI",
            "cs.SE",
            "A.0; F.0; I.2"
        ]
    },
    {
        "title": "File mapping Rule-based DBMS and Natural Language Processing",
        "authors": [
            "Vjacheslav M. Novikov"
        ],
        "summary": "This paper describes the system of storage, extract and processing of information structured similarly to the natural language. For recursive inference the system uses the rules having the same representation, as the data. The environment of storage of information is provided with the File Mapping (SHM) mechanism of operating system. In the paper the main principles of construction of dynamic data structure and language for record of the inference rules are stated; the features of available implementation are considered and the description of the application realizing semantic information retrieval on the natural language is given.",
        "published": "2001-06-10T14:56:51Z",
        "link": "http://arxiv.org/abs/cs/0106016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "cs.PL",
            "D.3.2; H.2.4"
        ]
    },
    {
        "title": "Information Integration and Computational Logic",
        "authors": [
            "Yannis Dimopoulos",
            "Antonis Kakas"
        ],
        "summary": "Information Integration is a young and exciting field with enormous research and commercial significance in the new world of the Information Society. It stands at the crossroad of Databases and Artificial Intelligence requiring novel techniques that bring together different methods from these fields. Information from disparate heterogeneous sources often with no a-priori common schema needs to be synthesized in a flexible, transparent and intelligent way in order to respond to the demands of a query thus enabling a more informed decision by the user or application program. The field although relatively young has already found many practical applications particularly for integrating information over the World Wide Web. This paper gives a brief introduction of the field highlighting some of the main current and future research issues and application areas. It attempts to evaluate the current and potential role of Computational Logic in this and suggests some of the problems where logic-based techniques could be used.",
        "published": "2001-06-11T20:00:04Z",
        "link": "http://arxiv.org/abs/cs/0106025v1",
        "categories": [
            "cs.AI",
            "H.2.m;I.2.m"
        ]
    },
    {
        "title": "Complexity Results and Practical Algorithms for Logics in Knowledge   Representation",
        "authors": [
            "Stephan Tobies"
        ],
        "summary": "Description Logics (DLs) are used in knowledge-based systems to represent and reason about terminological knowledge of the application domain in a semantically well-defined manner. In this thesis, we establish a number of novel complexity results and give practical algorithms for expressive DLs that provide different forms of counting quantifiers.   We show that, in many cases, adding local counting in the form of qualifying number restrictions to DLs does not increase the complexity of the inference problems, even if binary coding of numbers in the input is assumed. On the other hand, we show that adding different forms of global counting restrictions to a logic may increase the complexity of the inference problems dramatically.   We provide exact complexity results and a practical, tableau based algorithm for the DL SHIQ, which forms the basis of the highly optimized DL system iFaCT.   Finally, we describe a tableau algorithm for the clique guarded fragment (CGF), which we hope will serve as the basis for an efficient implementation of a CGF reasoner.",
        "published": "2001-06-13T11:20:30Z",
        "link": "http://arxiv.org/abs/cs/0106031v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3; I.2.4; F.2.2"
        ]
    },
    {
        "title": "Convergence and Error Bounds for Universal Prediction of Nonbinary   Sequences",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff's uncomputable universal prediction scheme $\\xi$ allows to predict the next symbol $x_k$ of a sequence $x_1...x_{k-1}$ for any Turing computable, but otherwise unknown, probabilistic environment $\\mu$. This scheme will be generalized to arbitrary environmental classes, which, among others, allows the construction of computable universal prediction schemes $\\xi$. Convergence of $\\xi$ to $\\mu$ in a conditional mean squared sense and with $\\mu$ probability 1 is proven. It is shown that the average number of prediction errors made by the universal $\\xi$ scheme rapidly converges to those made by the best possible informed $\\mu$ scheme. The schemes, theorems and proofs are given for general finite alphabet, which results in additional complications as compared to the binary case. Several extensions of the presented theory and results are outlined. They include general loss functions and bounds, games of chance, infinite alphabet, partial and delayed prediction, classification, and more active systems.",
        "published": "2001-06-15T09:12:51Z",
        "link": "http://arxiv.org/abs/cs/0106036v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "math.PR",
            "F.2.3"
        ]
    },
    {
        "title": "Stacking classifiers for anti-spam filtering of e-mail",
        "authors": [
            "G. Sakkis",
            "I. Androutsopoulos",
            "G. Paliouras",
            "V. Karkaletsis",
            "C. D. Spyropoulos",
            "P. Stamatopoulos"
        ],
        "summary": "We evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization. Unsolicited commercial e-mail, or \"spam\", floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content. Using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in real-life applications.",
        "published": "2001-06-19T14:56:02Z",
        "link": "http://arxiv.org/abs/cs/0106040v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "A Sequential Model for Multi-Class Classification",
        "authors": [
            "Yair Even-Zohar",
            "Dan Roth"
        ],
        "summary": "Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in part-of-speech tagging.",
        "published": "2001-06-20T19:01:41Z",
        "link": "http://arxiv.org/abs/cs/0106044v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "I.2.6;I.2.67"
        ]
    },
    {
        "title": "Software Toolkit for Building Embedded and Distributed Knowledge-based   Systems",
        "authors": [
            "Dmitri Soshnikov"
        ],
        "summary": "The paper discusses the basic principles and the architecture of the software toolkit for constructing knowledge-based systems which can be used cooperatively over computer networks and also embedded into larger software systems in different ways. Presented architecture is based on frame knowledge representation and production rules, which also allows to interface high-level programming languages and relational databases by exposing corresponding classes or database tables as frames. Frames located on the remote computers can also be transparently accessed and used in inference, and the dynamic knowledge for specific frames can also be transferred over the network. The issues of implementation of such a system are addressed, which use Java programming language, CORBA and XML for external knowledge representation. Finally, some applications of the toolkit are considered, including e-business approach to knowledge sharing, intelligent web behaviours, etc.",
        "published": "2001-06-26T19:50:54Z",
        "link": "http://arxiv.org/abs/cs/0106054v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "I.2; D.2.12"
        ]
    },
    {
        "title": "Enhancing Constraint Propagation with Composition Operators",
        "authors": [
            "Laurent Granvilliers",
            "Eric Monfroy"
        ],
        "summary": "Constraint propagation is a general algorithmic approach for pruning the search space of a CSP. In a uniform way, K. R. Apt has defined a computation as an iteration of reduction functions over a domain. He has also demonstrated the need for integrating static properties of reduction functions (commutativity and semi-commutativity) to design specialized algorithms such as AC3 and DAC. We introduce here a set of operators for modeling compositions of reduction functions. Two of the major goals are to tackle parallel computations, and dynamic behaviours (such as slow convergence).",
        "published": "2001-07-02T08:08:39Z",
        "link": "http://arxiv.org/abs/cs/0107002v1",
        "categories": [
            "cs.AI",
            "F.4.1; D.3.3"
        ]
    },
    {
        "title": "Three-Stage Quantitative Neural Network Model of the Tip-of-the-Tongue   Phenomenon",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "A new three-stage computer artificial neural network model of the tip-of-the-tongue phenomenon is shortly described, and its stochastic nature was demonstrated. A way to calculate strength and appearance probability of tip-of-the-tongue states, neural network mechanism of feeling-of-knowing phenomenon are proposed. The model synthesizes memory, psycholinguistic, and metamemory approaches, bridges speech errors and naming chronometry research traditions. A model analysis of a tip-of-the-tongue case from Anton Chekhov's short story 'A Horsey Name' is performed. A new 'throw-up-one's-arms effect' is defined.",
        "published": "2001-07-09T21:00:42Z",
        "link": "http://arxiv.org/abs/cs/0107012v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC",
            "q-bio.QM",
            "I.2.7"
        ]
    },
    {
        "title": "The Logic Programming Paradigm and Prolog",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "This is a tutorial on logic programming and Prolog appropriate for a course on programming languages for students familiar with imperative programming.",
        "published": "2001-07-10T10:24:19Z",
        "link": "http://arxiv.org/abs/cs/0107013v2",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Transformations of CCP programs",
        "authors": [
            "Sandro Etalle",
            "Maurizio Gabbrielli",
            "Maria Chiara Meo"
        ],
        "summary": "We introduce a transformation system for concurrent constraint programming (CCP). We define suitable applicability conditions for the transformations which guarantee that the input/output CCP semantics is preserved also when distinguishing deadlocked computations from successful ones and when considering intermediate results of (possibly) non-terminating computations.   The system allows us to optimize CCP programs while preserving their intended meaning: In addition to the usual benefits that one has for sequential declarative languages, the transformation of concurrent programs can also lead to the elimination of communication channels and of synchronization points, to the transformation of non-deterministic computations into deterministic ones, and to the crucial saving of computational space. Furthermore, since the transformation system preserves the deadlock behavior of programs, it can be used for proving deadlock freeness of a given program wrt a class of queries. To this aim it is sometimes sufficient to apply our transformations and to specialize the resulting program wrt the given queries in such a way that the obtained program is trivially deadlock free.",
        "published": "2001-07-10T13:32:17Z",
        "link": "http://arxiv.org/abs/cs/0107014v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO",
            "I.2.2; D.1.3; D.3.2"
        ]
    },
    {
        "title": "Annotated revision programs",
        "authors": [
            "Victor Marek",
            "Inna Pivkina",
            "Miroslaw Truszczynski"
        ],
        "summary": "Revision programming is a formalism to describe and enforce updates of belief sets and databases. That formalism was extended by Fitting who assigned annotations to revision atoms. Annotations provide a way to quantify the confidence (probability) that a revision atom holds. The main goal of our paper is to reexamine the work of Fitting, argue that his semantics does not always provide results consistent with intuition, and to propose an alternative treatment of annotated revision programs. Our approach differs from that proposed by Fitting in two key aspects: we change the notion of a model of a program and we change the notion of a justified revision. We show that under this new approach fundamental properties of justified revisions of standard revision programs extend to the annotated case.",
        "published": "2001-07-19T15:41:36Z",
        "link": "http://arxiv.org/abs/cs/0107026v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4;I.2.3"
        ]
    },
    {
        "title": "Fixed-parameter complexity of semantics for logic programs",
        "authors": [
            "Zbigniew Lonc",
            "Miroslaw Truszczynski"
        ],
        "summary": "A decision problem is called parameterized if its input is a pair of strings. One of these strings is referred to as a parameter. The problem: given a propositional logic program P and a non-negative integer k, decide whether P has a stable model of size no more than k, is an example of a parameterized decision problem with k serving as a parameter. Parameterized problems that are NP-complete often become solvable in polynomial time if the parameter is fixed. The problem to decide whether a program P has a stable model of size no more than k, where k is fixed and not a part of input, can be solved in time O(mn^k), where m is the size of P and n is the number of atoms in P. Thus, this problem is in the class P. However, algorithms with the running time given by a polynomial of order k are not satisfactory even for relatively small values of k.   The key question then is whether significantly better algorithms (with the degree of the polynomial not dependent on k) exist. To tackle it, we use the framework of fixed-parameter complexity. We establish the fixed-parameter complexity for several parameterized decision problems involving models, supported models and stable models of logic programs. We also establish the fixed-parameter complexity for variants of these problems resulting from restricting attention to Horn programs and to purely negative programs. Most of the problems considered in the paper have high fixed-parameter complexity. Thus, it is unlikely that fixing bounds on models (supported models, stable models) will lead to fast algorithms to decide the existence of such models.",
        "published": "2001-07-19T15:52:54Z",
        "link": "http://arxiv.org/abs/cs/0107027v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.1.3;I.2.4;D.1.6"
        ]
    },
    {
        "title": "Propositional satisfiability in answer-set programming",
        "authors": [
            "Deborah East",
            "Miroslaw Truszczynski"
        ],
        "summary": "We show that propositional logic and its extensions can support answer-set programming in the same way stable logic programming and disjunctive logic programming do. To this end, we introduce a logic based on the logic of propositional schemata and on a version of the Closed World Assumption. We call it the extended logic of propositional schemata with CWA (PS+, in symbols). An important feature of this logic is that it supports explicit modeling of constraints on cardinalities of sets. In the paper, we characterize the class of problems that can be solved by finite PS+ theories. We implement a programming system based on the logic PS+ and design and implement a solver for processing theories in PS+. We present encouraging performance results for our approach --- we show it to be competitive with smodels, a state-of-the-art answer-set programming system based on stable logic programming.",
        "published": "2001-07-19T17:53:11Z",
        "link": "http://arxiv.org/abs/cs/0107028v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1;D.1.6;I.2.4"
        ]
    },
    {
        "title": "aspps --- an implementation of answer-set programming with propositional   schemata",
        "authors": [
            "Deborah East. Miroslaw Truszczynski"
        ],
        "summary": "We present an implementation of an answer-set programming paradigm, called aspps (short for answer-set programming with propositional schemata). The system aspps is designed to process PS+ theories. It consists of two basic modules. The first module, psgrnd, grounds an PS+ theory. The second module, referred to as aspps, is a solver. It computes models of ground PS+ theories.",
        "published": "2001-07-19T18:02:50Z",
        "link": "http://arxiv.org/abs/cs/0107029v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1;I.2.4;D.I.6"
        ]
    },
    {
        "title": "Using Methods of Declarative Logic Programming for Intelligent   Information Agents",
        "authors": [
            "T. Eiter",
            "M. Fink",
            "G. Sabbatini",
            "H. Tompits"
        ],
        "summary": "The search for information on the web is faced with several problems, which arise on the one hand from the vast number of available sources, and on the other hand from their heterogeneity. A promising approach is the use of multi-agent systems of information agents, which cooperatively solve advanced information-retrieval problems. This requires capabilities to address complex tasks, such as search and assessment of sources, query planning, information merging and fusion, dealing with incomplete information, and handling of inconsistency. In this paper, our interest is in the role which some methods from the field of declarative logic programming can play in the realization of reasoning capabilities for information agents. In particular, we are interested in how they can be used and further developed for the specific needs of this application domain. We review some existing systems and current projects, which address information-integration problems. We then focus on declarative knowledge-representation methods, and review and evaluate approaches from logic programming and nonmonotonic reasoning for information agents. We discuss advantages and drawbacks, and point out possible extensions and open issues.",
        "published": "2001-08-14T14:51:23Z",
        "link": "http://arxiv.org/abs/cs/0108008v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2"
        ]
    },
    {
        "title": "Convergent Approximate Solving of First-Order Constraints by Approximate   Quantifiers",
        "authors": [
            "Stefan Ratschan"
        ],
        "summary": "Exactly solving first-order constraints (i.e., first-order formulas over a certain predefined structure) can be a very hard, or even undecidable problem. In continuous structures like the real numbers it is promising to compute approximate solutions instead of exact ones. However, the quantifiers of the first-order predicate language are an obstacle to allowing approximations to arbitrary small error bounds. In this paper we solve the problem by modifying the first-order language and replacing the classical quantifiers with approximate quantifiers. These also have two additional advantages: First, they are tunable, in the sense that they allow the user to decide on the trade-off between precision and efficiency. Second, they introduce additional expressivity into the first-order language by allowing reasoning over the size of solution sets.",
        "published": "2001-08-22T20:18:51Z",
        "link": "http://arxiv.org/abs/cs/0108013v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "On Properties of Update Sequences Based on Causal Rejection",
        "authors": [
            "T. Eiter",
            "M. Fink",
            "G. Sabbatini",
            "H. Tompits"
        ],
        "summary": "We consider an approach to update nonmonotonic knowledge bases represented as extended logic programs under answer set semantics. New information is incorporated into the current knowledge base subject to a causal rejection principle enforcing that, in case of conflicts, more recent rules are preferred and older rules are overridden. Such a rejection principle is also exploited in other approaches to update logic programs, e.g., in dynamic logic programming by Alferes et al. We give a thorough analysis of properties of our approach, to get a better understanding of the causal rejection principle. We review postulates for update and revision operators from the area of theory change and nonmonotonic reasoning, and some new properties are considered as well. We then consider refinements of our semantics which incorporate a notion of minimality of change. As well, we investigate the relationship to other approaches, showing that our approach is semantically equivalent to inheritance programs by Buccafurri et al. and that it coincides with certain classes of dynamic logic programs, for which we provide characterizations in terms of graph conditions. Therefore, most of our results about properties of causal rejection principle apply to these approaches as well. Finally, we deal with computational complexity of our approach, and outline how the update semantics and its refinements can be implemented on top of existing logic programming engines.",
        "published": "2001-09-05T09:19:34Z",
        "link": "http://arxiv.org/abs/cs/0109006v1",
        "categories": [
            "cs.AI",
            "I.2"
        ]
    },
    {
        "title": "Assigning Satisfaction Values to Constraints: An Algorithm to Solve   Dynamic Meta-Constraints",
        "authors": [
            "Janet van der Linden"
        ],
        "summary": "The model of Dynamic Meta-Constraints has special activity constraints which can activate other constraints. It also has meta-constraints which range over other constraints. An algorithm is presented in which constraints can be assigned one of five different satisfaction values, which leads to the assignment of domain values to the variables in the CSP. An outline of the model and the algorithm is presented, followed by some initial results for two problems: a simple classic CSP and the Car Configuration Problem. The algorithm is shown to perform few backtracks per solution, but to have overheads in the form of historical records required for the implementation of state.",
        "published": "2001-09-13T11:03:18Z",
        "link": "http://arxiv.org/abs/cs/0109014v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D3.3"
        ]
    },
    {
        "title": "Interactive Timetabling",
        "authors": [
            "Tomas Muller",
            "Roman Bartak"
        ],
        "summary": "Timetabling is a typical application of constraint programming whose task is to allocate activities to slots in available resources respecting various constraints like precedence and capacity. In this paper we present a basic concept, a constraint model, and the solving algorithms for interactive timetabling. Interactive timetabling combines automated timetabling (the machine allocates the activities) with user interaction (the user can interfere with the process of timetabling). Because the user can see how the timetabling proceeds and can intervene this process, we believe that such approach is more convenient than full automated timetabling which behaves like a black-box. The contribution of this paper is twofold: we present a generic model to describe timetabling (and scheduling in general) problems and we propose an interactive algorithm for solving such problems.",
        "published": "2001-09-17T09:33:49Z",
        "link": "http://arxiv.org/abs/cs/0109022v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3; F2.2"
        ]
    },
    {
        "title": "Integrating Multiple Knowledge Sources for Robust Semantic Parsing",
        "authors": [
            "Jordi Atserias",
            "Lluis Padro",
            "German Rigau"
        ],
        "summary": "This work explores a new robust approach for Semantic Parsing of unrestricted texts. Our approach considers Semantic Parsing as a Consistent Labelling Problem (CLP), allowing the integration of several knowledge types (syntactic and semantic) obtained from different sources (linguistic and statistic). The current implementation obtains 95% accuracy in model identification and 72% in case-role filling.",
        "published": "2001-09-17T14:41:14Z",
        "link": "http://arxiv.org/abs/cs/0109023v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Dynamic Global Constraints: A First View",
        "authors": [
            "Roman Bartak"
        ],
        "summary": "Global constraints proved themselves to be an efficient tool for modelling and solving large-scale real-life combinatorial problems. They encapsulate a set of binary constraints and using global reasoning about this set they filter the domains of involved variables better than arc consistency among the set of binary constraints. Moreover, global constraints exploit semantic information to achieve more efficient filtering than generalised consistency algorithms for n-ary constraints. Continued expansion of constraint programming (CP) to various application areas brings new challenges for design of global constraints. In particular, application of CP to advanced planning and scheduling (APS) requires dynamic additions of new variables and constraints during the process of constraint satisfaction and, thus, it would be helpful if the global constraints could adopt new variables. In the paper, we give a motivation for such dynamic global constraints and we describe a dynamic version of the well-known alldifferent constraint.",
        "published": "2001-09-18T05:18:35Z",
        "link": "http://arxiv.org/abs/cs/0109025v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3; D.1.6"
        ]
    },
    {
        "title": "Relevant Knowledge First - Reinforcement Learning and Forgetting in   Knowledge Based Configuration",
        "authors": [
            "Ingo Kreuz",
            "Dieter Roller"
        ],
        "summary": "In order to solve complex configuration tasks in technical domains, various knowledge based methods have been developed. However their applicability is often unsuccessful due to their low efficiency. One of the reasons for this is that (parts of the) problems have to be solved again and again, instead of being \"learnt\" from preceding processes. However, learning processes bring with them the problem of conservatism, for in technical domains innovation is a deciding factor in competition. On the other hand a certain amount of conservatism is often desired since uncontrolled innovation as a rule is also detrimental. This paper proposes the heuristic RKF (Relevant Knowledge First) for making decisions in configuration processes based on the so-called relevance of objects in a knowledge base. The underlying relevance-function has two components, one based on reinforcement learning and the other based on forgetting (fading). Relevance of an object increases with its successful use and decreases with age when it is not used. RKF has been developed to speed up the configuration process and to improve the quality of the solutions relative to the reward value that is given by users.",
        "published": "2001-09-19T08:07:38Z",
        "link": "http://arxiv.org/abs/cs/0109034v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "H.4.2; I.2.0; I.2.1; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Intelligent Search of Correlated Alarms from Database containing Noise   Data",
        "authors": [
            "Qingguo Zheng",
            "Ke Xu",
            "Weifeng Lv",
            "Shilong Ma"
        ],
        "summary": "Alarm correlation plays an important role in improving the service and reliability in modern telecommunications networks. Most previous research of alarm correlation didn't consider the effect of noise data in Database. This paper focuses on the method of discovering alarm correlation rules from database containing noise data. We firstly define two parameters Win_freq and Win_add as the measure of noise data and then present the Robust_search algorithm to solve the problem. At different size of Win_freq and Win_add, experiments with alarm data containing noise data show that the Robust_search Algorithm can discover the more rules with the bigger size of Win_add. We also experimentally compare two different interestingness measures of confidence and correlation.",
        "published": "2001-09-21T12:30:35Z",
        "link": "http://arxiv.org/abs/cs/0109042v2",
        "categories": [
            "cs.NI",
            "cs.AI",
            "C.2.3"
        ]
    },
    {
        "title": "The temporal calculus of conditional objects and conditional events",
        "authors": [
            "Jerzy Tyszkiewicz",
            "Arthur Ramer",
            "Achim Hoffmann"
        ],
        "summary": "We consider the problem of defining conditional objects (a|b), which would allow one to regard the conditional probability Pr(a|b) as a probability of a well-defined event rather than as a shorthand for Pr(ab)/Pr(b). The next issue is to define boolean combinations of conditional objects, and possibly also the operator of further conditioning. These questions have been investigated at least since the times of George Boole, leading to a number of formalisms proposed for conditional objects, mostly of syntactical, proof-theoretic vein.   We propose a unifying, semantical approach, in which conditional events are (projections of) Markov chains, definable in the three-valued extension of the past tense fragment of propositional linear time logic, or, equivalently, by three-valued counter-free Moore machines. Thus our conditional objects are indeed stochastic processes, one of the central notions of modern probability theory.   Our model fulfills early ideas of Bruno de Finetti and, moreover, as we show in a separate paper, all the previously proposed algebras of conditional events can be isomorphically embedded in our model.",
        "published": "2001-10-01T08:58:34Z",
        "link": "http://arxiv.org/abs/cs/0110003v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Embedding conditional event algebras into temporal calculus of   conditionals",
        "authors": [
            "Jerzy Tyszkiewicz",
            "Achim Hoffmann",
            "Arthur Ramer"
        ],
        "summary": "In this paper we prove that all the existing conditional event algebras embed into a three-valued extension of temporal logic of discrete past time, which the authors of this paper have proposed in anothe paper as a general model of conditional events.   First of all, we discuss the descriptive incompleteness of the cea's. In this direction, we show that some important notions, like independence of conditional events, cannot be properly addressed in the framework of conditional event algebras, while they can be precisely formulated and analyzed in the temporal setting.   We also demonstrate that the embeddings allow one to use Markov chain algorithms (suitable for the temporal calculus) for computing probabilities of complex conditional expressions of the embedded conditional event algebras, and that these algorithms can outperform those previously known.",
        "published": "2001-10-01T09:21:26Z",
        "link": "http://arxiv.org/abs/cs/0110004v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Set Unification",
        "authors": [
            "Agostino Dovier",
            "Enrico Pontelli",
            "Gianfranco Rossi"
        ],
        "summary": "The unification problem in algebras capable of describing sets has been tackled, directly or indirectly, by many researchers and it finds important applications in various research areas--e.g., deductive databases, theorem proving, static analysis, rapid software prototyping. The various solutions proposed are spread across a large literature. In this paper we provide a uniform presentation of unification of sets, formalizing it at the level of set theory. We address the problem of deciding existence of solutions at an abstract level. This provides also the ability to classify different types of set unification problems. Unification algorithms are uniformly proposed to solve the unification problem in each of such classes.   The algorithms presented are partly drawn from the literature--and properly revisited and analyzed--and partly novel proposals. In particular, we present a new goal-driven algorithm for general ACI1 unification and a new simpler algorithm for general (Ab)(Cl) unification.",
        "published": "2001-10-09T10:57:56Z",
        "link": "http://arxiv.org/abs/cs/0110023v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.SC",
            "F.2.2; F.4.1; I.1.2; I.2.3"
        ]
    },
    {
        "title": "A logic-based approach to data integration",
        "authors": [
            "J. Grant",
            "J. Minker"
        ],
        "summary": "An important aspect of data integration involves answering queries using various resources rather than by accessing database relations. The process of transforming a query from the database relations to the resources is often referred to as query folding or answering queries using views, where the views are the resources. We present a uniform approach that includes as special cases much of the previous work on this subject. Our approach is logic-based using resolution. We deal with integrity constraints, negation, and recursion also within this framework.",
        "published": "2001-10-16T19:05:24Z",
        "link": "http://arxiv.org/abs/cs/0110032v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2, I.2.4"
        ]
    },
    {
        "title": "Generating Multilingual Personalized Descriptions of Museum Exhibits -   The M-PIRO Project",
        "authors": [
            "Ion Androutsopoulos",
            "Vassiliki Kokkinaki",
            "Aggeliki Dimitromanolaki",
            "Jo Calder",
            "Jon Oberlander",
            "Elena Not"
        ],
        "summary": "This paper provides an overall presentation of the M-PIRO project. M-PIRO is developing technology that will allow museums to generate automatically textual or spoken descriptions of exhibits for collections available over the Web or in virtual reality environments. The descriptions are generated in several languages from information in a language-independent database and small fragments of text, and they can be tailored according to the backgrounds of the users, their ages, and their previous interaction with the system. An authoring tool allows museum curators to update the system's database and to control the language and content of the resulting descriptions. Although the project is still in progress, a Web-based demonstrator that supports English, Greek and Italian is already available, and it is used throughout the paper to highlight the capabilities of the emerging technology.",
        "published": "2001-10-29T16:55:32Z",
        "link": "http://arxiv.org/abs/cs/0110057v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7; H.5.2; H.5.4; I.7.4"
        ]
    },
    {
        "title": "Intelligent Anticipated Exploration of Web Sites",
        "authors": [
            "Giovambattista Ianni"
        ],
        "summary": "In this paper we describe a web search agent, called Global Search Agent (hereafter GSA for short). GSA integrates and enhances several search techniques in order to achieve significant improvements in the user-perceived quality of delivered information as compared to usual web search engines. GSA features intelligent merging of relevant documents from different search engines, anticipated selective exploration and evaluation of links from the current result set, automated derivation of refined queries based on user relevance feedback. System architecture as well as experimental accounts are also illustrated.",
        "published": "2001-11-06T18:00:49Z",
        "link": "http://arxiv.org/abs/cs/0111012v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "I.2.11;H.3.3"
        ]
    },
    {
        "title": "Data Acquisition and Database Management System for Samsung   Superconductor Test Facility",
        "authors": [
            "Y. Chu",
            "S. Baek",
            "H. Yonekawa",
            "A. Chertovskikh",
            "M. Kim",
            "J. S. Kim",
            "K. Park",
            "S. Baang",
            "Y. Chang",
            "J. H. Kim",
            "S. Lee",
            "B. Lim",
            "W. Chung",
            "H. Park",
            "K. Kim"
        ],
        "summary": "In order to fulfill the test requirement of KSTAR (Korea Superconducting Tokamak Advanced Research) superconducting magnet system, a large scale superconducting magnet and conductor test facility, SSTF (Samsung Superconductor Test Facility), has been constructed at Samsung Advanced Institute of Technology. The computer system for SSTF DAC (Data Acquisition and Control) is based on UNIX system and VxWorks is used for the real-time OS of the VME system. EPICS (Experimental Physics and Industrial Control System) is used for the communication between IOC server and client. A database program has been developed for the efficient management of measured data and a Linux workstation with PENTIUM-4 CPU is used for the database server. In this paper, the current status of SSTF DAC system, the database management system and recent test results are presented.",
        "published": "2001-11-08T18:28:46Z",
        "link": "http://arxiv.org/abs/cs/0111018v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "B.1.1"
        ]
    },
    {
        "title": "Arc consistency for soft constraints",
        "authors": [
            "Martin Cooper",
            "Thomas Schiex"
        ],
        "summary": "The notion of arc consistency plays a central role in constraint satisfaction. It is known that the notion of local consistency can be extended to constraint optimisation problems defined by soft constraint frameworks based on an idempotent cost combination operator. This excludes non idempotent operators such as + which define problems which are very important in practical applications such as Max-CSP, where the aim is to minimize the number of violated constraints. In this paper, we show that using a weak additional axiom satisfied by most existing soft constraints proposals, it is possible to define a notion of soft arc consistency that extends the classical notion of arc consistency and this even in the case of non idempotent cost combination operators. A polynomial time algorithm for enforcing this soft arc consistency exists and its space and time complexities are identical to that of enforcing arc consistency in CSPs when the cost combination operator is strictly monotonic (for example Max-CSP). A directional version of arc consistency is potentially even stronger than the non-directional version, since it allows non local propagation of penalties. We demonstrate the utility of directional arc consistency by showing that it not only solves soft constraint problems on trees, but that it also implies a form of local optimality, which we call arc irreducibility.",
        "published": "2001-11-14T12:06:42Z",
        "link": "http://arxiv.org/abs/cs/0111038v3",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "I.2.8;F.4.1;D.3.3"
        ]
    },
    {
        "title": "Bayesian Logic Programs",
        "authors": [
            "Kristian Kersting",
            "Luc De Raedt"
        ],
        "summary": "Bayesian networks provide an elegant formalism for representing and reasoning about uncertainty using probability theory. Theyare a probabilistic extension of propositional logic and, hence, inherit some of the limitations of propositional logic, such as the difficulties to represent objects and relations. We introduce a generalization of Bayesian networks, called Bayesian logic programs, to overcome these limitations. In order to represent objects and relations it combines Bayesian networks with definite clause logic by establishing a one-to-one mapping between ground atoms and random variables. We show that Bayesian logic programs combine the advantages of both definite clause logic and Bayesian networks. This includes the separation of quantitative and qualitative aspects of the model. Furthermore, Bayesian logic programs generalize both Bayesian networks as well as logic programs. So, many ideas developed",
        "published": "2001-11-23T20:59:09Z",
        "link": "http://arxiv.org/abs/cs/0111058v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2; I.2.3; I.2.4; G.3; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Gradient-based Reinforcement Planning in Policy-Search Methods",
        "authors": [
            "Ivo Kwee",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ],
        "summary": "We introduce a learning method called ``gradient-based reinforcement planning'' (GREP). Unlike traditional DP methods that improve their policy backwards in time, GREP is a gradient-based method that plans ahead and improves its policy before it actually acts in the environment. We derive formulas for the exact policy gradient that maximizes the expected future reward and confirm our ideas with numerical experiments.",
        "published": "2001-11-28T13:43:13Z",
        "link": "http://arxiv.org/abs/cs/0111060v1",
        "categories": [
            "cs.AI",
            "I.2; I.2.6; I.2.8"
        ]
    },
    {
        "title": "A Logic Programming Approach to Knowledge-State Planning: Semantics and   Complexity",
        "authors": [
            "Thomas Eiter",
            "Wolfgang Faber",
            "Nicola Leone",
            "Gerald Pfeifer",
            "Axel Polleres"
        ],
        "summary": "We propose a new declarative planning language, called K, which is based on principles and methods of logic programming. In this language, transitions between states of knowledge can be described, rather than transitions between completely described states of the world, which makes the language well-suited for planning under incomplete knowledge. Furthermore, it enables the use of default principles in the planning process by supporting negation as failure. Nonetheless, K also supports the representation of transitions between states of the world (i.e., states of complete knowledge) as a special case, which shows that the language is very flexible. As we demonstrate on particular examples, the use of knowledge states may allow for a natural and compact problem representation. We then provide a thorough analysis of the computational complexity of K, and consider different planning problems, including standard planning and secure planning (also known as conformant planning) problems. We show that these problems have different complexities under various restrictions, ranging from NP to NEXPTIME in the propositional case. Our results form the theoretical basis for the DLV^K system, which implements the language K on top of the DLV logic programming system.",
        "published": "2001-12-05T12:41:48Z",
        "link": "http://arxiv.org/abs/cs/0112006v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.2.8; I.2.3"
        ]
    },
    {
        "title": "A Tight Upper Bound on the Number of Candidate Patterns",
        "authors": [
            "Floris Geerts",
            "Bart Goethals",
            "Jan Van den Bussche"
        ],
        "summary": "In the context of mining for frequent patterns using the standard levelwise algorithm, the following question arises: given the current level and the current set of frequent patterns, what is the maximal number of candidate patterns that can be generated on the next level? We answer this question by providing a tight upper bound, derived from a combinatorial result from the sixties by Kruskal and Katona. Our result is useful to reduce the number of database scans.",
        "published": "2001-12-07T15:40:11Z",
        "link": "http://arxiv.org/abs/cs/0112007v2",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Representation of Uncertainty for Limit Processes",
        "authors": [
            "Mark Burgin"
        ],
        "summary": "Many mathematical models utilize limit processes. Continuous functions and the calculus, differential equations and topology, all are based on limits and continuity. However, when we perform measurements and computations, we can achieve only approximate results. In some cases, this discrepancy between theoretical schemes and practical actions changes drastically outcomes of a research and decision-making resulting in uncertainty of knowledge. In the paper, a mathematical approach to such kind of uncertainty, which emerges in computation and measurement, is suggested on the base of the concept of a fuzzy limit. A mathematical technique is developed for differential models with uncertainty. To take into account the intrinsic uncertainty of a model, it is suggested to use fuzzy derivatives instead of conventional derivatives of functions in this model.",
        "published": "2001-12-07T23:56:51Z",
        "link": "http://arxiv.org/abs/cs/0112008v1",
        "categories": [
            "cs.AI",
            "cs.NA",
            "G.1; E.1"
        ]
    },
    {
        "title": "Interactive Constrained Association Rule Mining",
        "authors": [
            "Bart Goethals",
            "Jan Van den Bussche"
        ],
        "summary": "We investigate ways to support interactive mining sessions, in the setting of association rule mining. In such sessions, users specify conditions (queries) on the associations to be generated. Our approach is a combination of the integration of querying conditions inside the mining phase, and the incremental querying of already generated associations. We present several concrete algorithms and compare their performance.",
        "published": "2001-12-10T15:50:47Z",
        "link": "http://arxiv.org/abs/cs/0112011v2",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "A Data Mining Framework for Optimal Product Selection in Retail   Supermarket Data: The Generalized PROFSET Model",
        "authors": [
            "Tom Brijs",
            "Bart Goethals",
            "Gilbert Swinnen",
            "Koen Vanhoof",
            "Geert Wets"
        ],
        "summary": "In recent years, data mining researchers have developed efficient association rule algorithms for retail market basket analysis. Still, retailers often complain about how to adopt association rules to optimize concrete retail marketing-mix decisions. It is in this context that, in a previous paper, the authors have introduced a product selection model called PROFSET. This model selects the most interesting products from a product assortment based on their cross-selling potential given some retailer defined constraints. However this model suffered from an important deficiency: it could not deal effectively with supermarket data, and no provisions were taken to include retail category management principles. Therefore, in this paper, the authors present an important generalization of the existing model in order to make it suitable for supermarket data as well, and to enable retailers to add category restrictions to the model. Experiments on real world data obtained from a Belgian supermarket chain produce very promising results and demonstrate the effectiveness of the generalized PROFSET model.",
        "published": "2001-12-11T19:23:11Z",
        "link": "http://arxiv.org/abs/cs/0112013v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Rational Competitive Analysis",
        "authors": [
            "Moshe Tennenholtz"
        ],
        "summary": "Much work in computer science has adopted competitive analysis as a tool for decision making under uncertainty. In this work we extend competitive analysis to the context of multi-agent systems. Unlike classical competitive analysis where the behavior of an agent's environment is taken to be arbitrary, we consider the case where an agent's environment consists of other agents. These agents will usually obey some (minimal) rationality constraints. This leads to the definition of rational competitive analysis. We introduce the concept of rational competitive analysis, and initiate the study of competitive analysis for multi-agent systems. We also discuss the application of rational competitive analysis to the context of bidding games, as well as to the classical one-way trading problem.",
        "published": "2001-12-13T00:46:10Z",
        "link": "http://arxiv.org/abs/cs/0112015v1",
        "categories": [
            "cs.AI",
            "I.2.11"
        ]
    },
    {
        "title": "Distribution of Mutual Information",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The mutual information of two random variables i and j with joint probabilities t_ij is commonly used in learning Bayesian nets as well as in many other fields. The chances t_ij are usually estimated by the empirical sampling frequency n_ij/n leading to a point estimate I(n_ij/n) for the mutual information. To answer questions like \"is I(n_ij/n) consistent with zero?\" or \"what is the probability that the true mutual information is much larger than the point estimate?\" one has to go beyond the point estimate. In the Bayesian framework one can answer these questions by utilizing a (second order) prior distribution p(t) comprising prior information about t. From the prior p(t) one can compute the posterior p(t|n), from which the distribution p(I|n) of the mutual information can be calculated. We derive reliable and quickly computable approximations for p(I|n). We concentrate on the mean, variance, skewness, and kurtosis, and non-informative priors. For the mean we also give an exact expression. Numerical issues and the range of validity are discussed.",
        "published": "2001-12-15T12:58:39Z",
        "link": "http://arxiv.org/abs/cs/0112019v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH",
            "G.3; G.1.2"
        ]
    },
    {
        "title": "General Loss Bounds for Universal Sequence Prediction",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The Bayesian framework is ideally suited for induction problems. The probability of observing $x_t$ at time $t$, given past observations $x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\\mu$ of the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many cases one does not even have a reasonable estimate of the true distribution. In order to overcome this problem a universal distribution $\\xi$ is defined as a weighted sum of distributions $\\mu_i\\inM$, where $M$ is any countable set of distributions including $\\mu$. This is a generalization of Solomonoff induction, in which $M$ is the set of all enumerable semi-measures. Systems which predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$ if $x_t$ is the true next symbol of the sequence are considered. It is proven that using the universal $\\xi$ as a prior is nearly as good as using the unknown true distribution $\\mu$. Furthermore, games of chance, defined as a sequence of bets, observations, and rewards are studied. The time needed to reach the winning zone is bounded in terms of the relative entropy of $\\mu$ and $\\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and more active systems are discussed.",
        "published": "2001-01-21T17:19:37Z",
        "link": "http://arxiv.org/abs/cs/0101019v2",
        "categories": [
            "cs.AI",
            "cs.LG",
            "math.ST",
            "stat.TH",
            "I.2; I.2.6; I.2.8; F.1.3"
        ]
    },
    {
        "title": "Non-convex cost functionals in boosting algorithms and methods for panel   selection",
        "authors": [
            "Marco Visentin"
        ],
        "summary": "In this document we propose a new improvement for boosting techniques as proposed in Friedman '99 by the use of non-convex cost functional. The idea is to introduce a correlation term to better deal with forecasting of additive time series. The problem is discussed in a theoretical way to prove the existence of minimizing sequence, and in a numerical way to propose a new \"ArgMin\" algorithm. The model has been used to perform the touristic presence forecast for the winter season 1999/2000 in Trentino (italian Alps).",
        "published": "2001-02-20T13:08:15Z",
        "link": "http://arxiv.org/abs/cs/0102015v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "cs.NA",
            "math.NA",
            "I.2.6;G.1.2;G.3;I.6.5"
        ]
    },
    {
        "title": "An effective Procedure for Speeding up Algorithms",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The provably asymptotically fastest algorithm within a factor of 5 for formally described problems will be constructed. The main idea is to enumerate all programs provably equivalent to the original problem by enumerating all proofs. The algorithm could be interpreted as a generalization and improvement of Levin search, which is, within a multiplicative constant, the fastest algorithm for inverting functions. Blum's speed-up theorem is avoided by taking into account only programs for which a correctness proof exists. Furthermore, it is shown that the fastest program that computes a certain function is also one of the shortest programs provably computing this function. To quantify this statement, the definition of Kolmogorov complexity is extended, and two new natural measures for the complexity of a function are defined.",
        "published": "2001-02-21T20:52:28Z",
        "link": "http://arxiv.org/abs/cs/0102018v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.LG",
            "F.2.3"
        ]
    },
    {
        "title": "Learning Policies with External Memory",
        "authors": [
            "Leonid Peshkin",
            "Nicolas Meuleau",
            "Leslie Kaelbling"
        ],
        "summary": "In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations. In this paper, we explore a {\\it stigmergic} approach, in which the agent's actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent. In this case, we need to learn a reactive policy in a highly non-Markovian domain. We explore two algorithms: SARSA(\\lambda), which has had empirical success in partially observable domains, and VAPS, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains. We compare the performance of these two algorithms on benchmark problems.",
        "published": "2001-03-02T01:55:46Z",
        "link": "http://arxiv.org/abs/cs/0103003v1",
        "categories": [
            "cs.LG",
            "I.2.8;I.2.6;I.2.11;I.2;I.2.3"
        ]
    },
    {
        "title": "Fitness Uniform Selection to Preserve Genetic Diversity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "In evolutionary algorithms, the fitness of a population increases with time by mutating and recombining individuals and by a biased selection of more fit individuals. The right selection pressure is critical in ensuring sufficient optimization progress on the one hand and in preserving genetic diversity to be able to escape from local optima on the other. We propose a new selection scheme, which is uniform in the fitness values. It generates selection pressure towards sparsely populated fitness regions, not necessarily towards higher fitness, as is the case for all other selection schemes. We show that the new selection scheme can be much more effective than standard selection schemes.",
        "published": "2001-03-14T18:40:32Z",
        "link": "http://arxiv.org/abs/cs/0103015v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "q-bio",
            "I.2; I.2.6; I.2.8; F.2"
        ]
    },
    {
        "title": "Bootstrapping Structure using Similarity",
        "authors": [
            "Menno van Zaanen"
        ],
        "summary": "In this paper a new similarity-based learning algorithm, inspired by string edit-distance (Wagner and Fischer, 1974), is applied to the problem of bootstrapping structure from scratch. The algorithm takes a corpus of unannotated sentences as input and returns a corpus of bracketed sentences. The method works on pairs of unstructured sentences or sentences partially bracketed by the algorithm that have one or more words in common. It finds parts of sentences that are interchangeable (i.e. the parts of the sentences that are different in both sentences). These parts are taken as possible constituents of the same type. While this corresponds to the basic bootstrapping step of the algorithm, further structure may be learned from comparison with other (similar) sentences.   We used this method for bootstrapping structure from the flat sentences of the Penn Treebank ATIS corpus, and compared the resulting structured sentences to the structured sentences in the ATIS corpus. Similarly, the algorithm was tested on the OVIS corpus. We obtained 86.04 % non-crossing brackets precision on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS corpus.",
        "published": "2001-04-03T14:09:12Z",
        "link": "http://arxiv.org/abs/cs/0104005v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2, I.2.6, I.2.7"
        ]
    },
    {
        "title": "ABL: Alignment-Based Learning",
        "authors": [
            "Menno van Zaanen"
        ],
        "summary": "This paper introduces a new type of grammar learning algorithm, inspired by string edit distance (Wagner and Fischer, 1974). The algorithm takes a corpus of flat sentences as input and returns a corpus of labelled, bracketed sentences. The method works on pairs of unstructured sentences that have one or more words in common. When two sentences are divided into parts that are the same in both sentences and parts that are different, this information is used to find parts that are interchangeable. These parts are taken as possible constituents of the same type. After this alignment learning step, the selection learning step selects the most probable constituents from all possible constituents.   This method was used to bootstrap structure on the ATIS corpus (Marcus et al., 1993) and on the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.) corpus (Bonnema et al., 1997). While the results are encouraging (we obtained up to 89.25 % non-crossing brackets precision), this paper will point out some of the shortcomings of our approach and will suggest possible solutions.",
        "published": "2001-04-03T14:20:26Z",
        "link": "http://arxiv.org/abs/cs/0104006v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Bootstrapping Syntax and Recursion using Alignment-Based Learning",
        "authors": [
            "Menno van Zaanen"
        ],
        "summary": "This paper introduces a new type of unsupervised learning algorithm, based on the alignment of sentences and Harris's (1951) notion of interchangeability. The algorithm is applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of the corpus. Firstly, the algorithm aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are similar in both sentences and parts that are dissimilar. This information is used to find (possibly overlapping) constituents. Next, the algorithm selects (non-overlapping) constituents. Several instances of the algorithm are applied to the ATIS corpus (Marcus et al., 1993) and the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.) corpus (Bonnema et al., 1997). Apart from the promising numerical results, the most striking result is that even the simplest algorithm based on alignment learns recursion.",
        "published": "2001-04-03T15:03:16Z",
        "link": "http://arxiv.org/abs/cs/0104007v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Market-Based Reinforcement Learning in Partially Observable Worlds",
        "authors": [
            "Ivo Kwee",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ],
        "summary": "Unlike traditional reinforcement learning (RL), market-based RL is in principle applicable to worlds described by partially observable Markov Decision Processes (POMDPs), where an agent needs to learn short-term memories of relevant previous events in order to execute optimal actions. Most previous work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here we reimplement a recent approach to market-based RL and for the first time evaluate it in a toy POMDP setting.",
        "published": "2001-05-15T19:07:28Z",
        "link": "http://arxiv.org/abs/cs/0105025v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MA",
            "cs.NE",
            "I.2"
        ]
    },
    {
        "title": "Bounds on sample size for policy evaluation in Markov environments",
        "authors": [
            "Leonid Peshkin",
            "Sayan Mukherjee"
        ],
        "summary": "Reinforcement learning means finding the optimal course of action in Markovian environments without knowledge of the environment's dynamics. Stochastic optimization algorithms used in the field rely on estimates of the value of a policy. Typically, the value of a policy is estimated from results of simulating that very policy in the environment. This approach requires a large amount of simulation as different points in the policy space are considered. In this paper, we develop value estimators that utilize data gathered when using one policy to estimate the value of using another policy, resulting in much more data-efficient algorithms. We consider the question of accumulating a sufficient experience and give PAC-style bounds.",
        "published": "2001-05-17T18:33:56Z",
        "link": "http://arxiv.org/abs/cs/0105027v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "G.2;G.1.6;I.2;I.2.6"
        ]
    },
    {
        "title": "Learning to Cooperate via Policy Search",
        "authors": [
            "Leonid Peshkin",
            "Kee-Eung Kim",
            "Nicolas Meuleau",
            "Leslie Pack Kaelbling"
        ],
        "summary": "Cooperative games are those in which both agents share the same payoff structure. Value-based reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain.",
        "published": "2001-05-25T02:52:07Z",
        "link": "http://arxiv.org/abs/cs/0105032v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "I.2;I.2.9;I.2.11"
        ]
    },
    {
        "title": "Mathematics of learning",
        "authors": [
            "Natalia Komarova",
            "Igor Rivin"
        ],
        "summary": "We study the convergence properties of a pair of learning algorithms (learning with and without memory). This leads us to study the dominant eigenvalue of a class of random matrices. This turns out to be related to the roots of the derivative of random polynomials (generated by picking their roots uniformly at random in the interval [0, 1], although our results extend to other distributions). This, in turn, requires the study of the statistical behavior of the harmonic mean of random variables as above, which leads us to delicate question of the rate of convergence to stable laws and tail estimates for stable laws. The reader can find the proofs of most of the results announced here in the paper entitled \"Harmonic mean, random polynomials, and random matrices\", by the same authors.",
        "published": "2001-05-29T02:20:17Z",
        "link": "http://arxiv.org/abs/math/0105235v3",
        "categories": [
            "math.PR",
            "cs.LG",
            "math.CO",
            "math.DS",
            "60E07, 60F15, 60J20, 91E40, 26C10"
        ]
    },
    {
        "title": "Harmonic mean, random polynomials and stochastic matrices",
        "authors": [
            "Natalia Komarova",
            "Igor Rivin"
        ],
        "summary": "Motivated by a problem in learning theory, we are led to study the dominant eigenvalue of a class of random matrices. This turns out to be related to the roots of the derivative of random polynomials (generated by picking their roots uniformly at random in the interval [0, 1], although our results extend to other distributions). This, in turn, requires the study of the statistical behavior of the harmonic mean of random variables as above, and that, in turn, leads us to delicate question of the rate of convergence to stable laws and tail estimates for stable laws.",
        "published": "2001-05-29T02:25:23Z",
        "link": "http://arxiv.org/abs/math/0105236v2",
        "categories": [
            "math.PR",
            "cs.LG",
            "math.CA",
            "math.CO",
            "math.DS",
            "60E07, 60F15, 60J20, 91E40, 26C10"
        ]
    },
    {
        "title": "File mapping Rule-based DBMS and Natural Language Processing",
        "authors": [
            "Vjacheslav M. Novikov"
        ],
        "summary": "This paper describes the system of storage, extract and processing of information structured similarly to the natural language. For recursive inference the system uses the rules having the same representation, as the data. The environment of storage of information is provided with the File Mapping (SHM) mechanism of operating system. In the paper the main principles of construction of dynamic data structure and language for record of the inference rules are stated; the features of available implementation are considered and the description of the application realizing semantic information retrieval on the natural language is given.",
        "published": "2001-06-10T14:56:51Z",
        "link": "http://arxiv.org/abs/cs/0106016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "cs.PL",
            "D.3.2; H.2.4"
        ]
    },
    {
        "title": "Convergence and Error Bounds for Universal Prediction of Nonbinary   Sequences",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff's uncomputable universal prediction scheme $\\xi$ allows to predict the next symbol $x_k$ of a sequence $x_1...x_{k-1}$ for any Turing computable, but otherwise unknown, probabilistic environment $\\mu$. This scheme will be generalized to arbitrary environmental classes, which, among others, allows the construction of computable universal prediction schemes $\\xi$. Convergence of $\\xi$ to $\\mu$ in a conditional mean squared sense and with $\\mu$ probability 1 is proven. It is shown that the average number of prediction errors made by the universal $\\xi$ scheme rapidly converges to those made by the best possible informed $\\mu$ scheme. The schemes, theorems and proofs are given for general finite alphabet, which results in additional complications as compared to the binary case. Several extensions of the presented theory and results are outlined. They include general loss functions and bounds, games of chance, infinite alphabet, partial and delayed prediction, classification, and more active systems.",
        "published": "2001-06-15T09:12:51Z",
        "link": "http://arxiv.org/abs/cs/0106036v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "math.PR",
            "F.2.3"
        ]
    },
    {
        "title": "A Sequential Model for Multi-Class Classification",
        "authors": [
            "Yair Even-Zohar",
            "Dan Roth"
        ],
        "summary": "Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in part-of-speech tagging.",
        "published": "2001-06-20T19:01:41Z",
        "link": "http://arxiv.org/abs/cs/0106044v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "I.2.6;I.2.67"
        ]
    },
    {
        "title": "Coupled Clustering: a Method for Detecting Structural Correspondence",
        "authors": [
            "Zvika Marx",
            "Ido Dagan",
            "Joachim Buhmann"
        ],
        "summary": "This paper proposes a new paradigm and computational framework for identification of correspondences between sub-structures of distinct composite systems. For this, we define and investigate a variant of traditional data clustering, termed coupled clustering, which simultaneously identifies corresponding clusters within two data sets. The presented method is demonstrated and evaluated for detecting topical correspondences in textual corpora.",
        "published": "2001-07-23T11:06:45Z",
        "link": "http://arxiv.org/abs/cs/0107032v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.6; I.2.7; I.5.3; I.5.4"
        ]
    },
    {
        "title": "Yet another zeta function and learning",
        "authors": [
            "Igor Rivin"
        ],
        "summary": "We study the convergence speed of the batch learning algorithm, and compare its speed to that of the memoryless learning algorithm and of learning with memory (as analyzed in joint work with N. Komarova). We obtain precise results and show in particular that the batch learning algorithm is never worse than the memoryless learning algorithm (at least asymptotically). Its performance vis-a-vis learning with full memory is less clearcut, and depends on certainprobabilistic assumptions. These results necessitate theintroduction of the moment zeta function of a probability distribution and the study of some of its properties.",
        "published": "2001-07-25T15:50:43Z",
        "link": "http://arxiv.org/abs/cs/0107033v1",
        "categories": [
            "cs.LG",
            "cs.DM",
            "math.PR",
            "I.2.6; G.3"
        ]
    },
    {
        "title": "Bipartite graph partitioning and data clustering",
        "authors": [
            "H. Zha",
            "X. He",
            "C. Ding",
            "M. Gu",
            "H. Simon"
        ],
        "summary": "Many data types arising from data mining applications can be modeled as bipartite graphs, examples include terms and documents in a text corpus, customers and purchasing items in market basket analysis and reviewers and movies in a movie recommender system. In this paper, we propose a new data clustering method based on partitioning the underlying bipartite graph. The partition is constructed by minimizing a normalized sum of edge weights between unmatched pairs of vertices of the bipartite graph. We show that an approximate solution to the minimization problem can be obtained by computing a partial singular value decomposition (SVD) of the associated edge weight matrix of the bipartite graph. We point out the connection of our clustering algorithm to correspondence analysis used in multivariate analysis. We also briefly discuss the issue of assigning data objects to multiple clusters. In the experimental results, we apply our clustering algorithm to the problem of document clustering to illustrate its effectiveness and efficiency.",
        "published": "2001-08-27T13:07:44Z",
        "link": "http://arxiv.org/abs/cs/0108018v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.3; G.1.3; G.2.2"
        ]
    },
    {
        "title": "Relevant Knowledge First - Reinforcement Learning and Forgetting in   Knowledge Based Configuration",
        "authors": [
            "Ingo Kreuz",
            "Dieter Roller"
        ],
        "summary": "In order to solve complex configuration tasks in technical domains, various knowledge based methods have been developed. However their applicability is often unsuccessful due to their low efficiency. One of the reasons for this is that (parts of the) problems have to be solved again and again, instead of being \"learnt\" from preceding processes. However, learning processes bring with them the problem of conservatism, for in technical domains innovation is a deciding factor in competition. On the other hand a certain amount of conservatism is often desired since uncontrolled innovation as a rule is also detrimental. This paper proposes the heuristic RKF (Relevant Knowledge First) for making decisions in configuration processes based on the so-called relevance of objects in a knowledge base. The underlying relevance-function has two components, one based on reinforcement learning and the other based on forgetting (fading). Relevance of an object increases with its successful use and decreases with age when it is not used. RKF has been developed to speed up the configuration process and to improve the quality of the solutions relative to the reward value that is given by users.",
        "published": "2001-09-19T08:07:38Z",
        "link": "http://arxiv.org/abs/cs/0109034v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "H.4.2; I.2.0; I.2.1; I.2.6; I.2.8"
        ]
    },
    {
        "title": "Efficient algorithms for decision tree cross-validation",
        "authors": [
            "Hendrik Blockeel",
            "Jan Struyf"
        ],
        "summary": "Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. The analysis is supported by experimental results.",
        "published": "2001-10-17T15:45:23Z",
        "link": "http://arxiv.org/abs/cs/0110036v1",
        "categories": [
            "cs.LG",
            "I.2.6"
        ]
    },
    {
        "title": "Machine Learning in Automated Text Categorization",
        "authors": [
            "Fabrizio Sebastiani"
        ],
        "summary": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.",
        "published": "2001-10-26T09:27:48Z",
        "link": "http://arxiv.org/abs/cs/0110053v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.1;H.3.3;I.2.3"
        ]
    },
    {
        "title": "The Use of Classifiers in Sequential Inference",
        "authors": [
            "Vasin Punyakanok",
            "Dan Roth"
        ],
        "summary": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem-identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.",
        "published": "2001-11-01T03:02:19Z",
        "link": "http://arxiv.org/abs/cs/0111003v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6, I.2.7"
        ]
    },
    {
        "title": "Combinatorial Toolbox for Protein Sequence Design and Landscape Analysis   in the Grand Canonical Model",
        "authors": [
            "James Aspnes",
            "Julia Hartling",
            "Ming-Yang Kao",
            "Junhyong Kim",
            "Gauri Shah"
        ],
        "summary": "In modern biology, one of the most important research problems is to understand how protein sequences fold into their native 3D structures. To investigate this problem at a high level, one wishes to analyze the protein landscapes, i.e., the structures of the space of all protein sequences and their native 3D structures. Perhaps the most basic computational problem at this level is to take a target 3D structure as input and design a fittest protein sequence with respect to one or more fitness functions of the target 3D structure. We develop a toolbox of combinatorial techniques for protein landscape analysis in the Grand Canonical model of Sun, Brem, Chan, and Dill. The toolbox is based on linear programming, network flow, and a linear-size representation of all minimum cuts of a network. It not only substantially expands the network flow technique for protein sequence design in Kleinberg's seminal work but also is applicable to a considerably broader collection of computational problems than those considered by Kleinberg. We have used this toolbox to obtain a number of efficient algorithms and hardness results. We have further used the algorithms to analyze 3D structures drawn from the Protein Data Bank and have discovered some novel relationships between such native 3D structures and the Grand Canonical model.",
        "published": "2001-01-17T23:52:00Z",
        "link": "http://arxiv.org/abs/cs/0101015v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "q-bio.BM",
            "F.2; J.3"
        ]
    },
    {
        "title": "Deterministic computations whose history is independent of the order of   asynchronous updating",
        "authors": [
            "Peter Gacs"
        ],
        "summary": "Consider a network of processors (sites) in which each site x has a finite set N(x) of neighbors. There is a transition function f that for each site x computes the next state \\xi(x) from the states in N(x). But these transitions (updates) are applied in arbitrary order, one or many at a time. If the state of site x at time t is \\eta(x,t) then let us define the sequence \\zeta(x,0), \\zeta(x,1), ... by taking the sequence \\eta(x,0), \\eta(x,1), ..., and deleting repetitions. The function f is said to have invariant histories if the sequence \\zeta(x,i), (while it lasts, in case it is finite) depends only on the initial configuration, not on the order of updates.   This paper shows that though the invariant history property is typically undecidable, there is a useful simple sufficient condition, called commutativity: For any configuration, for any pair x,y of neighbors, if the updating would change both \\xi(x) and \\xi(y) then the result of updating first x and then y is the same as the result of doing this in the reverse order.",
        "published": "2001-01-24T20:33:21Z",
        "link": "http://arxiv.org/abs/cs/0101026v1",
        "categories": [
            "cs.DC",
            "cs.CC",
            "F.1.2"
        ]
    },
    {
        "title": "Time and Space Bounds for Reversible Simulation",
        "authors": [
            "Harry Buhrman",
            "J. Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We prove a general upper bound on the tradeoff between time and space that suffices for the reversible simulation of irreversible computation. Previously, only simulations using exponential time or quadratic space were known.   The tradeoff shows for the first time that we can simultaneously achieve subexponential time and subquadratic space.   The boundary values are the exponential time with hardly any extra space required by the Lange-McKenzie-Tapp method and the ($\\log 3$)th power time with square space required by the Bennett method. We also give the first general lower bound on the extra storage space required by general reversible simulation. This lower bound is optimal in that it is achieved by some reversible simulations.",
        "published": "2001-01-29T17:32:07Z",
        "link": "http://arxiv.org/abs/quant-ph/0101133v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Quantum Pushdown Automata",
        "authors": [
            "Marats Golovkins"
        ],
        "summary": "Quantum finite automata, as well as quantum pushdown automata (QPA) were first introduced by C. Moore and J. P. Crutchfield. In this paper we introduce the notion of QPA in a non-equivalent way, including unitarity criteria, by using the definition of quantum finite automata of Kondacs and Watrous. It is established that the unitarity criteria of QPA are not equivalent to the corresponding unitarity criteria of quantum Turing machines. We show that QPA can recognize every regular language. Finally we present some simple languages recognized by QPA, not recognizable by deterministic pushdown automata.",
        "published": "2001-02-09T19:53:52Z",
        "link": "http://arxiv.org/abs/quant-ph/0102054v1",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.FL"
        ]
    },
    {
        "title": "Quantum Multi-Prover Interactive Proof Systems with Limited Prior   Entanglement",
        "authors": [
            "Hirotada Kobayashi",
            "Keiji Matsumoto"
        ],
        "summary": "This paper gives the first formal treatment of a quantum analogue of multi-prover interactive proof systems. It is proved that the class of languages having quantum multi-prover interactive proof systems is necessarily contained in NEXP, under the assumption that provers are allowed to share at most polynomially many prior-entangled qubits. This implies that, in particular, if provers do not share any prior entanglement with each other, the class of languages having quantum multi-prover interactive proof systems is equal to NEXP. Related to these, it is shown that, in the case a prover does not have his private qubits, the class of languages having quantum single-prover interactive proof systems is also equal to NEXP.",
        "published": "2001-02-19T19:46:12Z",
        "link": "http://arxiv.org/abs/cs/0102013v5",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.2;F.1.3"
        ]
    },
    {
        "title": "Parity Problem With A Cellular Automaton Solution",
        "authors": [
            "K. M. Lee",
            "Hao Xu",
            "H. F. Chau"
        ],
        "summary": "The parity of a bit string of length $N$ is a global quantity that can be efficiently compute using a global counter in ${O} (N)$ time. But is it possible to find the parity using cellular automata with a set of local rule tables without using any global counter? Here, we report a way to solve this problem using a number of $r=1$ binary, uniform, parallel and deterministic cellular automata applied in succession for a total of ${O} (N^2)$ time.",
        "published": "2001-02-21T09:16:50Z",
        "link": "http://arxiv.org/abs/nlin/0102026v2",
        "categories": [
            "nlin.CG",
            "cond-mat.stat-mech",
            "cs.CC",
            "nlin.AO"
        ]
    },
    {
        "title": "Quantum Kolmogorov Complexity Based on Classical Descriptions",
        "authors": [
            "Paul M. B. Vitanyi"
        ],
        "summary": "We develop a theory of the algorithmic information in bits contained in an individual pure quantum state. This extends classical Kolmogorov complexity to the quantum domain retaining classical descriptions. Quantum Kolmogorov complexity coincides with the classical Kolmogorov complexity on the classical domain. Quantum Kolmogorov complexity is upper bounded and can be effectively approximated from above under certain conditions. With high probability a quantum object is incompressible. Upper- and lower bounds of the quantum complexity of multiple copies of individual pure quantum states are derived and may shed some light on the no-cloning properties of quantum states. In the quantum situation complexity is not sub-additive. We discuss some relations with ``no-cloning'' and ``approximate cloning'' properties.",
        "published": "2001-02-21T16:58:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0102108v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.IT",
            "math.IT",
            "math.LO"
        ]
    },
    {
        "title": "An effective Procedure for Speeding up Algorithms",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The provably asymptotically fastest algorithm within a factor of 5 for formally described problems will be constructed. The main idea is to enumerate all programs provably equivalent to the original problem by enumerating all proofs. The algorithm could be interpreted as a generalization and improvement of Levin search, which is, within a multiplicative constant, the fastest algorithm for inverting functions. Blum's speed-up theorem is avoided by taking into account only programs for which a correctness proof exists. Furthermore, it is shown that the fastest program that computes a certain function is also one of the shortest programs provably computing this function. To quantify this statement, the definition of Kolmogorov complexity is extended, and two new natural measures for the complexity of a function are defined.",
        "published": "2001-02-21T20:52:28Z",
        "link": "http://arxiv.org/abs/cs/0102018v1",
        "categories": [
            "cs.CC",
            "cs.AI",
            "cs.LG",
            "F.2.3"
        ]
    },
    {
        "title": "Easy and Hard Constraint Ranking in OT: Algorithms and Complexity",
        "authors": [
            "Jason Eisner"
        ],
        "summary": "We consider the problem of ranking a set of OT constraints in a manner consistent with data.   We speed up Tesar and Smolensky's RCD algorithm to be linear on the number of constraints. This finds a ranking so each attested form x_i beats or ties a particular competitor y_i. We also generalize RCD so each x_i beats or ties all possible competitors.   Alas, this more realistic version of learning has no polynomial algorithm unless P=NP! Indeed, not even generation does. So one cannot improve qualitatively upon brute force:   Merely checking that a single (given) ranking is consistent with given forms is coNP-complete if the surface forms are fully observed and Delta_2^p-complete if not. Indeed, OT generation is OptP-complete. As for ranking, determining whether any consistent ranking exists is coNP-hard (but in Delta_2^p) if the forms are fully observed, and Sigma_2^p-complete if not.   Finally, we show that generation and ranking are easier in derivational theories: in P, and NP-complete.",
        "published": "2001-02-22T03:50:56Z",
        "link": "http://arxiv.org/abs/cs/0102019v1",
        "categories": [
            "cs.CL",
            "cs.CC",
            "I.2.7; F.2.2"
        ]
    },
    {
        "title": "P-Immune Sets with Holes Lack Self-Reducibility Properties",
        "authors": [
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "No P-immune set having exponential gaps is positive-Turing self-reducible.",
        "published": "2001-02-23T17:10:50Z",
        "link": "http://arxiv.org/abs/cs/0102024v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2; F.1.1"
        ]
    },
    {
        "title": "Exact solutions for diluted spin glasses and optimization problems",
        "authors": [
            "S. Franz",
            "M. Leone",
            "F. Ricci-Tersenghi",
            "R. Zecchina"
        ],
        "summary": "We study the low temperature properties of p-spin glass models with finite connectivity and of some optimization problems. Using a one-step functional replica symmetry breaking Ansatz we can solve exactly the saddle-point equations for graphs with uniform connectivity. The resulting ground state energy is in perfect agreement with numerical simulations. For fluctuating connectivity graphs, the same Ansatz can be used in a variational way: For p-spin models (known as p-XOR-SAT in computer science) it provides the exact configurational entropy together with the dynamical and static critical connectivities (for p=3, \\gamma_d=0.818 and \\gamma_s=0.918 resp.), whereas for hard optimization problems like 3-SAT or Bicoloring it provides new upper bounds for their critical thresholds (\\gamma_c^{var}=4.396 and \\gamma_c^{var}=2.149 resp.).",
        "published": "2001-03-15T18:36:06Z",
        "link": "http://arxiv.org/abs/cond-mat/0103328v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "On the NP-completeness of Finding an Optimal Strategy in Games with   Common Payoffs",
        "authors": [
            "Francis Chu",
            "Joseph Y. Halpern"
        ],
        "summary": "Consider a very simple class of (finite) games: after an initial move by nature, each player makes one move. Moreover, the players have common interests: at each node, all the players get the same payoff. We show that the problem of determining whether there exists a joint strategy where each player has an expected payoff of at least r is NP-complete as a function of the number of nodes in the extensive-form representation of the game.",
        "published": "2001-03-27T19:51:09Z",
        "link": "http://arxiv.org/abs/cs/0103019v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "Quantum Clock Synchronization with one qubit",
        "authors": [
            "Chris Harrelson",
            "Iordanis Kerenidis"
        ],
        "summary": "The clock synchronization problem is to determine the time difference T between two spatially separated parties. We improve on I. Chuang's quantum clock synchronization algorithm and show that it is possible to obtain T to n bits of accuracy while communicating only one qubit in one direction and using an O(2^n) frequency range. We also prove a quantum lower bound of \\Omega(2^n) for the product of the transmitted qubits and the range of frequencies, thus showing that our algorithm is optimal.",
        "published": "2001-03-28T02:26:09Z",
        "link": "http://arxiv.org/abs/cs/0103021v3",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.2"
        ]
    },
    {
        "title": "Finding The Sign Of A Function Value By Binary Cellular Automaton",
        "authors": [
            "H. F. Chau",
            "H. Xu",
            "K. M. Lee",
            "L. W. Siu",
            "K. K. Yan"
        ],
        "summary": "Given a continuous function $f(x)$, suppose that the sign of $f$ only has finitely many discontinuous points in the interval $[0,1]$. We show how to use a sequence of one dimensional deterministic binary cellular automata to determine the sign of $f(\\rho)$ where $\\rho$ is the (number) density of 1s in an arbitrarily given bit string of finite length provided that $f$ satisfies certain technical conditions.",
        "published": "2001-03-30T10:33:26Z",
        "link": "http://arxiv.org/abs/nlin/0103057v1",
        "categories": [
            "nlin.CG",
            "cond-mat",
            "cs.CC"
        ]
    },
    {
        "title": "Quantum Formulas: a Lower Bound and Simulation",
        "authors": [
            "Vwani P. Roychowdhury",
            "Farrokh Vatan"
        ],
        "summary": "We show that Nechiporuk's method for proving lower bounds for Boolean formulas can be extended to the quantum case. This leads to an $\\Omega(n^2 / \\log^2 n)$ lower bound for quantum formulas computing an explicit function. The only known previous explicit lower bound for quantum formulas states that the majority function does not have a linear-size quantum formula. We also show that quantum formulas can be simulated by Boolean circuits of almost the same size.",
        "published": "2001-04-10T19:20:41Z",
        "link": "http://arxiv.org/abs/quant-ph/0104053v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Application of Support Vector Machine to detect an association between a   disease or trait and multiple SNP variations",
        "authors": [
            "Gene Kim",
            "MyungHo Kim"
        ],
        "summary": "After the completion of human genome sequence was anounced, it is evident that interpretation of DNA sequences is an immediate task to work on. For understanding their signals, improvement of present sequence analysis tools and developing new ones become necessary. Along this current trend, we attack one of the fundamental questions, which set of SNP(single nucleotide polymorphism) variations is related to a specific disease or trait is. For, in the whole DNA sequence, it is known that people have different DNAs only at SNP locations, and moreover, the total SNPs are less than 5 millions, finding an association between SNP variations and certain disease or trait is believed to be one of the essential steps not only for genetic researches but for drug design and discovery. In this paper, we are going to present a method of detecting whether there is an association between multiple SNP variations and a trait or disease. The method exploits the Support Vector Machine which has been attracting lots of attentions recently.",
        "published": "2001-04-17T13:43:09Z",
        "link": "http://arxiv.org/abs/cs/0104015v3",
        "categories": [
            "cs.CC",
            "q-bio",
            "J.3"
        ]
    },
    {
        "title": "Lower bounds in the quantum cell probe model",
        "authors": [
            "Pranab Sen",
            "S. Venkatesh"
        ],
        "summary": "We introduce a new model for studying quantum data structure problems -- the \"quantum cell probe model\". We prove a lower bound for the static predecessor problem in the address-only version of this model where we allow quantum parallelism only over the `address lines' of the queries. The address-only quantum cell probe model subsumes the classical cell probe model, and many quantum query algorithms like Grover's algorithm fall into this framework. Our lower bound improves the previous known lower bound for the predecessor problem in the classical cell probe model with randomised query schemes, and matches the classical deterministic upper bound of Beame and Fich. Beame and Fich have also proved a matching lower bound for the predecessor problem, but only in the classical deterministic setting. Our lower bound has the advantage that it holds for the more general quantum model, and also, its proof is substantially simpler than that of Beame and Fich. We prove our lower bound by obtaining a round elimination lemma for quantum communication complexity. A similar lemma was proved by Miltersen, Nisan, Safra and Wigderson for classical communication complexity, but it was not strong enough to prove a lower bound matching the upper bound of Beame and Fich. Our quantum round elimination lemma also allows us to prove rounds versus communication tradeoffs for some quantum communication complexity problems like the \"greater-than\" problem. We also study the \"static membership\" problem in the quantum cell probe model. Generalising a result of Yao, we show that if the storage scheme is implicit, that is it can only store members of the subset and `pointers', then any quantum query scheme must make $\\Omega(\\log n)$ probes.",
        "published": "2001-04-19T22:33:01Z",
        "link": "http://arxiv.org/abs/quant-ph/0104100v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Bounds on sample size for policy evaluation in Markov environments",
        "authors": [
            "Leonid Peshkin",
            "Sayan Mukherjee"
        ],
        "summary": "Reinforcement learning means finding the optimal course of action in Markovian environments without knowledge of the environment's dynamics. Stochastic optimization algorithms used in the field rely on estimates of the value of a policy. Typically, the value of a policy is estimated from results of simulating that very policy in the environment. This approach requires a large amount of simulation as different points in the policy space are considered. In this paper, we develop value estimators that utilize data gathered when using one policy to estimate the value of using another policy, resulting in much more data-efficient algorithms. We consider the question of accumulating a sufficient experience and give PAC-style bounds.",
        "published": "2001-05-17T18:33:56Z",
        "link": "http://arxiv.org/abs/cs/0105027v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "G.2;G.1.6;I.2;I.2.6"
        ]
    },
    {
        "title": "Coloring k-colorable graphs using relatively small palettes",
        "authors": [
            "Eran Halperin",
            "Ram Nathaniel",
            "Uri Zwick"
        ],
        "summary": "We obtain the following new coloring results:   * A 3-colorable graph on $n$ vertices with maximum degree~$\\Delta$ can be colored, in polynomial time, using $O((\\Delta \\log\\Delta)^{1/3} \\cdot\\log{n})$ colors. This slightly improves an $O((\\Delta^{{1}/{3}} \\log^{1/2}\\Delta)\\cdot\\log{n})$ bound given by Karger, Motwani and Sudan. More generally, $k$-colorable graphs with maximum degree $\\Delta$ can be colored, in polynomial time, using $O((\\Delta^{1-{2}/{k}}\\log^{1/k}\\Delta) \\cdot\\log{n})$ colors.   * A 4-colorable graph on $n$ vertices can be colored, in polynomial time, using $\\Ot(n^{7/19})$ colors. This improves an $\\Ot(n^{2/5})$ bound given again by Karger, Motwani and Sudan. More generally, $k$-colorable graphs on $n$ vertices can be colored, in polynomial time, using $\\Ot(n^{\\alpha_k})$ colors, where $\\alpha_5=97/207$, $\\alpha_6=43/79$, $\\alpha_7=1391/2315$, $\\alpha_8=175/271$, ...   The first result is obtained by a slightly more refined probabilistic analysis of the semidefinite programming based coloring algorithm of Karger, Motwani and Sudan. The second result is obtained by combining the coloring algorithm of Karger, Motwani and Sudan, the combinatorial coloring algorithms of Blum and an extension of a technique of Alon and Kahale (which is based on the Karger, Motwani and Sudan algorithm) for finding relatively large independent sets in graphs that are guaranteed to have very large independent sets. The extension of the Alon and Kahale result may be of independent interest.",
        "published": "2001-05-21T11:56:18Z",
        "link": "http://arxiv.org/abs/cs/0105029v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2;G.2.2;G.3;G.1.6"
        ]
    },
    {
        "title": "Computational Properties of Metaquerying Problems",
        "authors": [
            "F. Angiulli",
            "R. Ben-Eliyahu-Zohary",
            "G. Ianni",
            "L. Palopoli"
        ],
        "summary": "Metaquerying is a datamining technology by which hidden dependencies among several database relations can be discovered. This tool has already been successfully applied to several real-world applications. Recent papers provide only preliminary results about the complexity of metaquerying. In this paper we define several variants of metaquerying that encompass, as far as we know, all variants defined in the literature. We study both the combined complexity and the data complexity of these variants. We show that, under the combined complexity measure, metaquerying is generally intractable (unless P=NP), lying sometimes quite high in the complexity hierarchies (as high as NP^PP), depending on the characteristics of the plausibility index. However, we are able to single out some tractable and interesting metaquerying cases (whose combined complexity is LOGCFL-complete). As for the data complexity of metaquerying, we prove that, in general, this is in TC0, but lies within AC0 in some simpler cases. Finally, we discuss implementation of metaqueries, by providing algorithms to answer them.",
        "published": "2001-06-07T17:29:03Z",
        "link": "http://arxiv.org/abs/cs/0106012v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "H.2.3;F.2.0;H.2.8"
        ]
    },
    {
        "title": "Playing Games with Algorithms: Algorithmic Combinatorial Game Theory",
        "authors": [
            "Erik D. Demaine",
            "Robert A. Hearn"
        ],
        "summary": "Combinatorial games lead to several interesting, clean problems in algorithms and complexity theory, many of which remain open. The purpose of this paper is to provide an overview of the area to encourage further research. In particular, we begin with general background in Combinatorial Game Theory, which analyzes ideal play in perfect-information games, and Constraint Logic, which provides a framework for showing hardness. Then we survey results about the complexity of determining ideal play in these games, and the related problems of solving puzzles, in terms of both polynomial-time algorithms and computational intractability results. Our review of background and survey of algorithmic results are by no means complete, but should serve as a useful primer.",
        "published": "2001-06-11T03:49:59Z",
        "link": "http://arxiv.org/abs/cs/0106019v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "math.CO",
            "F.2.2; G.2.1; F.1.3"
        ]
    },
    {
        "title": "Convergence and Error Bounds for Universal Prediction of Nonbinary   Sequences",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "Solomonoff's uncomputable universal prediction scheme $\\xi$ allows to predict the next symbol $x_k$ of a sequence $x_1...x_{k-1}$ for any Turing computable, but otherwise unknown, probabilistic environment $\\mu$. This scheme will be generalized to arbitrary environmental classes, which, among others, allows the construction of computable universal prediction schemes $\\xi$. Convergence of $\\xi$ to $\\mu$ in a conditional mean squared sense and with $\\mu$ probability 1 is proven. It is shown that the average number of prediction errors made by the universal $\\xi$ scheme rapidly converges to those made by the best possible informed $\\mu$ scheme. The schemes, theorems and proofs are given for general finite alphabet, which results in additional complications as compared to the binary case. Several extensions of the presented theory and results are outlined. They include general loss functions and bounds, games of chance, infinite alphabet, partial and delayed prediction, classification, and more active systems.",
        "published": "2001-06-15T09:12:51Z",
        "link": "http://arxiv.org/abs/cs/0106036v1",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "math.PR",
            "F.2.3"
        ]
    },
    {
        "title": "Using the No-Search Easy-Hard Technique for Downward Collapse",
        "authors": [
            "Edith Hemaspaandra",
            "Lane A. Hemaspaandra",
            "Harald Hempel"
        ],
        "summary": "The top part of the preceding figure [figure appears in actual paper] shows some classes from the (truth-table) bounded-query and boolean hierarchies. It is well-known that if either of these hierarchies collapses at a given level, then all higher levels of that hierarchy collapse to that same level. This is a standard ``upward translation of equality'' that has been known for over a decade. The issue of whether these hierarchies can translate equality {\\em downwards\\/} has proven vastly more challenging. In particular, with regard to the figure above, consider the following claim:   $$P_{m-tt}^{\\Sigma_k^p} = P_{m+1-tt}^{\\Sigma_k^p} \\implies   DIFF_m(\\Sigma_k^p) coDIFF_m(\\Sigma_k^p) = BH(\\Sigma_k^p). (*) $$   This claim, if true, says that equality translates downwards between levels of the bounded-query hierarchy and the boolean hierarchy levels that (before the fact) are immediately below them.   Until recently, it was not known whether (*) {\\em ever\\/} held, except for the degenerate cases $m=0$ and $k=0$. Then Hemaspaandra, Hemaspaandra, and Hempel \\cite{hem-hem-hem:j:downward-translation} proved that (*) holds for all $m$, for $k > 2$. Buhrman and Fortnow~\\cite{buh-for:j:two-queries} then showed that, when $k=2$, (*) holds for the case $m = 1$. In this paper, we prove that for the case $k=2$, (*) holds for all values of $m$. Since there is an oracle relative to which ``for $k=1$, (*) holds for all $m$'' fails \\cite{buh-for:j:two-queries}, our achievement of the $k=2$ case cannot to be strengthened to $k=1$ by any relativizable proof technique. The new downward translation we obtain also tightens the collapse in the polynomial hierarchy implied by a collapse in the bounded-query hierarchy of the second level of the polynomial hierarchy.",
        "published": "2001-06-15T11:07:02Z",
        "link": "http://arxiv.org/abs/cs/0106037v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.1.2"
        ]
    },
    {
        "title": "Computing Complete Graph Isomorphisms and Hamiltonian Cycles from   Partial Ones",
        "authors": [
            "André Grosse",
            "Joerg Rothe",
            "Gerd Wechsung"
        ],
        "summary": "We prove that computing a single pair of vertices that are mapped onto each other by an isomorphism $\\phi$ between two isomorphic graphs is as hard as computing $\\phi$ itself. This result optimally improves upon a result of G\\'{a}l et al. We establish a similar, albeit slightly weaker, result about computing complete Hamiltonian cycles of a graph from partial Hamiltonian cycles.",
        "published": "2001-06-19T16:10:55Z",
        "link": "http://arxiv.org/abs/cs/0106041v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "A Note on the Complexity of Computing the Smallest Four-Coloring of   Planar Graphs",
        "authors": [
            "Andre Grosse",
            "Joerg Rothe",
            "Gerd Wechsung"
        ],
        "summary": "We show that computing the lexicographically first four-coloring for planar graphs is P^{NP}-hard. This result optimally improves upon a result of Khuller and Vazirani who prove this problem to be NP-hard, and conclude that it is not self-reducible in the sense of Schnorr, assuming P \\neq NP. We discuss this application to non-self-reducibility and provide a general related result.",
        "published": "2001-06-21T07:04:31Z",
        "link": "http://arxiv.org/abs/cs/0106045v2",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "On some optimization problems for star-free graphs",
        "authors": [
            "V. G. Naidenko",
            "Yu. L. Orlovich"
        ],
        "summary": "It is shown that in star-free graphs the maximum independent set problem, the minimum dominating set problem and the minimum independent dominating set problem are approximable up to constant factor by any maximal independent set.",
        "published": "2001-06-25T10:12:33Z",
        "link": "http://arxiv.org/abs/cs/0106048v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "G.1.2; G.2.2"
        ]
    },
    {
        "title": "Recursively Undecidable Properties of NP",
        "authors": [
            "V. G. Naidenko"
        ],
        "summary": "We show that there cannot be any algorithm that for a given nondeterministic polynomial-time Turing machine determinates whether or not the language recognized by this machine belongs to P",
        "published": "2001-06-25T13:37:59Z",
        "link": "http://arxiv.org/abs/cs/0106049v2",
        "categories": [
            "cs.CC",
            "F.0; F.1.3"
        ]
    },
    {
        "title": "Complex Tilings",
        "authors": [
            "Bruno Durand",
            "Leonid A. Levin",
            "Alexander Shen"
        ],
        "summary": "We study the minimal complexity of tilings of a plane with a given tile set. We note that every tile set admits either no tiling or some tiling with O(n) Kolmogorov complexity of its n-by-n squares. We construct tile sets for which this bound is tight: all n-by-n squares in all tilings have complexity at least n. This adds a quantitative angle to classical results on non-recursivity of tilings -- that we also develop in terms of Turing degrees of unsolvability.   Keywords: Tilings, Kolmogorov complexity, recursion theory",
        "published": "2001-07-04T21:09:25Z",
        "link": "http://arxiv.org/abs/cs/0107008v3",
        "categories": [
            "cs.CC",
            "cs.DM",
            "F.1.1; G.2.1"
        ]
    },
    {
        "title": "Algorithms for Boolean Function Query Properties",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "We present new algorithms to compute fundamental properties of a Boolean function given in truth-table form. Specifically, we give an O(N^2.322 log N) algorithm for block sensitivity, an O(N^1.585 log N) algorithm for `tree decomposition,' and an O(N) algorithm for `quasisymmetry.' These algorithms are based on new insights into the structure of Boolean functions that may be of independent interest. We also give a subexponential-time algorithm for the space-bounded quantum query complexity of a Boolean function. To prove this algorithm correct, we develop a theory of limited-precision representation of unitary operators, building on work of Bernstein and Vazirani.",
        "published": "2001-07-05T08:00:57Z",
        "link": "http://arxiv.org/abs/cs/0107010v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.2; F.1.3; F.2.2"
        ]
    },
    {
        "title": "From Neel to NPC: Colouring Small Worlds",
        "authors": [
            "Pontus Svenson"
        ],
        "summary": "In this note, we present results for the colouring problem on small world graphs created by rewiring square, triangular, and two kinds of cubic (with coordination numbers 5 and 6) lattices. As the rewiring parameter p tends to 1, we find the expected crossover to the behaviour of random graphs with corresponding connectivity. However, for the cubic lattices there is a region near p=0 for which the graphs are colourable. This could in principle be used as an additional heuristic for solving real world colouring or scheduling problems. Small worlds with connectivity 5 and p ~ 0.1 provide an interesting ensemble of graphs whose colourability is hard to determine. For square lattices, we get good data collapse plotting the fraction of colourable graphs against the rescaled parameter parameter $p N^{-\\nu}$ with $\\nu = 1.35$. No such collapse can be obtained for the data from lattices with coordination number 5 or 6.",
        "published": "2001-07-11T12:13:18Z",
        "link": "http://arxiv.org/abs/cs/0107015v1",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "F.2.m"
        ]
    },
    {
        "title": "The Complexity of Clickomania",
        "authors": [
            "Therese C. Biedl",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Rudolf Fleischer",
            "Lars Jacobsen",
            "J. Ian Munro"
        ],
        "summary": "We study a popular puzzle game known variously as Clickomania and Same Game. Basically, a rectangular grid of blocks is initially colored with some number of colors, and the player repeatedly removes a chosen connected monochromatic group of at least two square blocks, and any blocks above it fall down. We show that one-column puzzles can be solved, i.e., the maximum possible number of blocks can be removed, in linear time for two colors, and in polynomial time for an arbitrary number of colors. On the other hand, deciding whether a puzzle is solvable (all blocks can be removed) is NP-complete for two columns and five colors, or five columns and three colors.",
        "published": "2001-07-21T14:55:20Z",
        "link": "http://arxiv.org/abs/cs/0107031v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F.2.2; F.1.3; F.1.1; G.2.1"
        ]
    },
    {
        "title": "Classical simulation of noninteracting-fermion quantum circuits",
        "authors": [
            "Barbara M. Terhal",
            "David P. DiVincenzo"
        ],
        "summary": "We show that a class of quantum computations that was recently shown to be efficiently simulatable on a classical computer by Valiant corresponds to a physical model of noninteracting fermions in one dimension. We give an alternative proof of his result using the language of fermions and extend the result to noninteracting fermions with arbitrary pairwise interactions, where gates can be conditioned on outcomes of complete von Neumann measurements in the computational basis on other fermionic modes in the circuit. This last result is in remarkable contrast with the case of noninteracting bosons where universal quantum computation can be achieved by allowing gates to be conditioned on classical bits (quant-ph/0006088).",
        "published": "2001-08-01T21:21:12Z",
        "link": "http://arxiv.org/abs/quant-ph/0108010v2",
        "categories": [
            "quant-ph",
            "cond-mat",
            "cs.CC"
        ]
    },
    {
        "title": "Fermionic Linear Optics and Matchgates",
        "authors": [
            "E. Knill"
        ],
        "summary": "Fermionic linear optics is efficiently classically simulatable. Here it is shown that the set of states achievable with fermionic linear optics and particle measurements is the closure of a low dimensional Lie group. The weakness of fermionic linear optics and measurements can therefore be explained and contrasted with the strength of bosonic linear optics with particle measurements. An analysis of fermionic linear optics is used to show that the two-qubit matchgates and the simulatable matchcircuits introduced by Valiant generate a monoid of extended fermionic linear optics operators. A useful interpretation of efficient classical simulations such as this one is as a simulation of a model of non-deterministic quantum computation. Problem areas for future investigations are suggested.",
        "published": "2001-08-07T22:07:37Z",
        "link": "http://arxiv.org/abs/quant-ph/0108033v2",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A Note on Tiling under Tomographic Constraints",
        "authors": [
            "Marek Chrobak",
            "Peter Couperus",
            "Christoph Durr",
            "Gerhard Woeginger"
        ],
        "summary": "Given a tiling of a 2D grid with several types of tiles, we can count for every row and column how many tiles of each type it intersects. These numbers are called the_projections_. We are interested in the problem of reconstructing a tiling which has given projections. Some simple variants of this problem, involving tiles that are 1x1 or 1x2 rectangles, have been studied in the past, and were proved to be either solvable in polynomial time or NP-complete. In this note we make progress toward a comprehensive classification of various tiling reconstruction problems, by proving NP-completeness results for several sets of tiles.",
        "published": "2001-08-21T10:29:32Z",
        "link": "http://arxiv.org/abs/cs/0108010v3",
        "categories": [
            "cs.CC",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Complex Philosophy",
        "authors": [
            "Carlos Gershenson"
        ],
        "summary": "We present several philosophical ideas emerging from the studies of complex systems. We make a brief introduction to the basic concepts of complex systems, for then defining \"abstraction levels\". These are useful for representing regularities in nature. We define absolute being (observer independent, infinite) and relative being (observer dependent, finite), and notice the differences between them. We draw issues on relative causality and absolute causality among abstraction levels. We also make reflections on determinism. We reject the search for any absolute truth (because of their infinity), and promote the idea that all comprehensible truths are relative, since they were created in finite contexts. This leads us to suggest to search the less-incompleteness of ideas and contexts instead of their truths.",
        "published": "2001-09-01T10:50:22Z",
        "link": "http://arxiv.org/abs/nlin/0109001v3",
        "categories": [
            "nlin.AO",
            "cs.CC"
        ]
    },
    {
        "title": "Communication Complexity and Secure Function Evaluation",
        "authors": [
            "Moni Naor",
            "Kobbi Nissim"
        ],
        "summary": "We suggest two new methodologies for the design of efficient secure protocols, that differ with respect to their underlying computational models. In one methodology we utilize the communication complexity tree (or branching for f and transform it into a secure protocol. In other words, \"any function f that can be computed using communication complexity c can be can be computed securely using communication complexity that is polynomial in c and a security parameter\". The second methodology uses the circuit computing f, enhanced with look-up tables as its underlying computational model. It is possible to simulate any RAM machine in this model with polylogarithmic blowup. Hence it is possible to start with a computation of f on a RAM machine and transform it into a secure protocol.   We show many applications of these new methodologies resulting in protocols efficient either in communication or in computation. In particular, we exemplify a protocol for the \"millionaires problem\", where two participants want to compare their values but reveal no other information. Our protocol is more efficient than previously known ones in either communication or computation.",
        "published": "2001-09-09T20:49:32Z",
        "link": "http://arxiv.org/abs/cs/0109011v1",
        "categories": [
            "cs.CR",
            "cs.CC",
            "D.4.6"
        ]
    },
    {
        "title": "Improved Quantum Communication Complexity Bounds for Disjointness and   Equality",
        "authors": [
            "Peter Hoyer",
            "Ronald de Wolf"
        ],
        "summary": "We prove new bounds on the quantum communication complexity of the disjointness and equality problems. For the case of exact and non-deterministic protocols we show that these complexities are all equal to n+1, the previous best lower bound being n/2. We show this by improving a general bound for non-deterministic protocols of de Wolf. We also give an O(sqrt{n}c^{log^* n})-qubit bounded-error protocol for disjointness, modifying and improving the earlier O(sqrt{n}log n) protocol of Buhrman, Cleve, and Wigderson, and prove an Omega(sqrt{n}) lower bound for a large class of protocols that includes the BCW-protocol as well as our new protocol.",
        "published": "2001-09-14T09:31:31Z",
        "link": "http://arxiv.org/abs/quant-ph/0109068v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Exact Complexity of Exact-Four-Colorability",
        "authors": [
            "Jörg Rothe"
        ],
        "summary": "Let $M_k \\seq \\nats$ be a given set that consists of $k$ noncontiguous integers. Define $\\exactcolor{M_k}$ to be the problem of determining whether $\\chi(G)$, the chromatic number of a given graph $G$, equals one of the $k$ elements of the set $M_k$ exactly. In 1987, Wagner \\cite{wag:j:min-max} proved that $\\exactcolor{M_k}$ is $\\bhlevel{2k}$-complete, where $M_k = \\{6k+1, 6k+3, >..., 8k-1 \\}$ and $\\bhlevel{2k}$ is the $2k$th level of the boolean hierarchy over $\\np$. In particular, for $k = 1$, it is DP-complete to determine whether $\\chi(G) = 7$, where $\\DP = \\bhlevel{2}$. Wagner raised the question of how small the numbers in a $k$-element set $M_k$ can be chosen such that $\\exactcolor{M_k}$ still is $\\bhlevel{2k}$-complete. In particular, for $k = 1$, he asked if it is DP-complete to determine whether $\\chi(G) = 4$. In this note, we solve this question of Wagner and determine the precise threshold $t \\in \\{4, 5, 6, 7\\}$ for which the problem $\\exactcolor{\\{t\\}}$ jumps from NP to DP-completeness: It is DP-complete to determine whether $\\chi(G) = 4$, yet $\\exactcolor{\\{3\\}}$ is in $\\np$. More generally, for each $k \\geq 1$, we show that $\\exactcolor{M_k}$ is $\\bhlevel{2k}$-complete for $M_k = \\{3k+1, 3k+3,..., 5k-1\\}$.",
        "published": "2001-09-14T15:07:08Z",
        "link": "http://arxiv.org/abs/cs/0109018v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "On Quantum Versions of the Yao Principle",
        "authors": [
            "Mart de Graaf",
            "Ronald de Wolf"
        ],
        "summary": "The classical Yao principle states that the complexity R_epsilon(f) of an optimal randomized algorithm for a function f with success probability 1-epsilon equals the complexity max_mu D_epsilon^mu(f) of an optimal deterministic algorithm for f that is correct on a fraction 1-epsilon of the inputs, weighed according to the hardest distribution mu over the inputs. In this paper we investigate to what extent such a principle holds for quantum algorithms. We propose two natural candidate quantum Yao principles, a ``weak'' and a ``strong'' one. For both principles, we prove that the quantum bounded-error complexity is a lower bound on the quantum analogues of max mu D_epsilon^mu(f). We then prove that equality cannot be obtained for the ``strong'' version, by exhibiting an exponential gap. On the other hand, as a positive result we prove that the ``weak'' version holds up to a constant factor for the query complexity of all symmetric Boolean functions",
        "published": "2001-09-14T15:45:33Z",
        "link": "http://arxiv.org/abs/quant-ph/0109070v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Random Walks in Routing Landscapes",
        "authors": [
            "T. Michalareas",
            "L. Sacks"
        ],
        "summary": "In this paper we present a combinatorial optimisation view on the routing problem for connectionless packet networks by using the metaphor of a landscape. We examine the main properties of the routing landscapes as we define them and how they can help us on the evaluation of the problem difficulty and the generation of effective algorithms. We also present the random walk statistical technique to evaluate the main properties of those landscapes and a number of examples to demonstrate the use of the method.",
        "published": "2001-09-18T12:17:39Z",
        "link": "http://arxiv.org/abs/cs/0109028v1",
        "categories": [
            "cs.NI",
            "cs.CC",
            "C.2.2"
        ]
    },
    {
        "title": "Quantum Certificate Verification: Single versus Multiple Quantum   Certificates",
        "authors": [
            "Hirotada Kobayashi",
            "Keiji Matsumoto",
            "Tomoyuki Yamakami"
        ],
        "summary": "The class MA consists of languages that can be efficiently verified by classical probabilistic verifiers using a single classical certificate, and the class QMA consists of languages that can be efficiently verified by quantum verifiers using a single quantum certificate. Suppose that a verifier receives not only one but multiple certificates. In the classical setting, it is obvious that a classical verifier with multiple classical certificates is essentially the same with the one with a single classical certificate. However, in the quantum setting where a quantum verifier is given a set of quantum certificates in tensor product form (i.e. each quantum certificate is not entangled with others), the situation is different, because the quantum verifier might utilize the structure of the tensor product form. This suggests a possibility of another hierarchy of complexity classes, namely the QMA hierarchy. From this point of view, we extend the definition of QMA to QMA(k) for the case quantum verifiers use k quantum certificates, and analyze the properties of QMA(k).   To compare the power of QMA(2) with that of QMA(1) = QMA, we show one interesting property of ``quantum indistinguishability''. This gives a strong evidence that QMA(2) is more powerful than QMA(1). Furthermore, we show that, for any fixed positive integer $k \\geq 2$, if a language L has a one-sided bounded error QMA(k) protocol with a quantum verifier using k quantum certificates, L necessarily has a one-sided bounded error QMA(2) protocol with a quantum verifier using only two quantum certificates.",
        "published": "2001-10-01T10:05:26Z",
        "link": "http://arxiv.org/abs/quant-ph/0110006v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "Two-way Quantum One-counter Automata",
        "authors": [
            "Tomohiro Yamasaki",
            "Hirotada Kobayashi",
            "Hiroshi Imai"
        ],
        "summary": "After the first treatments of quantum finite state automata by Moore and Crutchfield and by Kondacs and Watrous, a number of papers study the power of quantum finite state automata and their variants. This paper introduces a model of two-way quantum one-counter automata (2Q1CAs), combining the model of two-way quantum finite state automata (2QFAs) by Kondacs and Watrous and the model of one-way quantum one-counter automata (1Q1CAs) by Kravtsev. We give the definition of 2Q1CAs with well-formedness conditions. It is proved that 2Q1CAs are at least as powerful as classical two-way deterministic one-counter automata (2D1CAs), that is, every language L recognizable by 2D1CAs is recognized by 2Q1CAs with no error. It is also shown that several non-context-free languages including {a^n b^{n^2}} and {a^n b^{2^n}} are recognizable by 2Q1CAs with bounded error.",
        "published": "2001-10-02T11:51:19Z",
        "link": "http://arxiv.org/abs/cs/0110005v1",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.1"
        ]
    },
    {
        "title": "Algorithmic Information Theoretic Issues in Quantum Mechanics",
        "authors": [
            "Gavriel Segre"
        ],
        "summary": "taking aside the review part, a finite-cardinality's set of new ideas concerning algorithmic information issues in Quantum Mechanics is introduced and analyzed",
        "published": "2001-10-02T18:35:51Z",
        "link": "http://arxiv.org/abs/quant-ph/0110018v7",
        "categories": [
            "quant-ph",
            "cs.CC",
            "math-ph",
            "math.MP"
        ]
    },
    {
        "title": "Frontier between separability and quantum entanglement in a many spin   system",
        "authors": [
            "F. C. Alcaraz",
            "C. Tsallis"
        ],
        "summary": "We discuss the critical point $x_c$ separating the quantum entangled and separable states in two series of N spins S in the simple mixed state characterized by the matrix operator $\\rho=x|\\tilde{\\phi}><\\tilde{\\phi}| + \\frac{1-x}{D^N} I_{D^N}$ where $x \\in [0,1]$, $D =2S+1$, ${\\bf I}_{D^N}$ is the $D^N \\times D^N$ unity matrix and $|\\tilde {\\phi}>$ is a special entangled state. The cases x=0 and x=1 correspond respectively to fully random spins and to a fully entangled state. In the first of these series we consider special states $|\\tilde{\\phi}>$ invariant under charge conjugation, that generalizes the N=2 spin S=1/2 Einstein-Podolsky-Rosen state, and in the second one we consider generalizations of the Weber density matrices. The evaluation of the critical point $x_c$ was done through bounds coming from the partial transposition method of Peres and the conditional nonextensive entropy criterion. Our results suggest the conjecture that whenever the bounds coming from both methods coincide the result of $x_c$ is the exact one. The results we present are relevant for the discussion of quantum computing, teleportation and cryptography.",
        "published": "2001-10-10T14:31:40Z",
        "link": "http://arxiv.org/abs/quant-ph/0110067v1",
        "categories": [
            "quant-ph",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Recognizing When Heuristics Can Approximate Minimum Vertex Covers Is   Complete for Parallel Access to NP",
        "authors": [
            "Edith Hemaspaandra",
            "Jörg Rothe",
            "Holger Spakowski"
        ],
        "summary": "For both the edge deletion heuristic and the maximum-degree greedy heuristic, we study the problem of recognizing those graphs for which that heuristic can approximate the size of a minimum vertex cover within a constant factor of r, where r is a fixed rational number. Our main results are that these problems are complete for the class of problems solvable via parallel access to NP. To achieve these main results, we also show that the restriction of the vertex cover problem to those graphs for which either of these heuristics can find an optimal solution remains NP-hard.",
        "published": "2001-10-10T15:02:35Z",
        "link": "http://arxiv.org/abs/cs/0110025v2",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2"
        ]
    },
    {
        "title": "Counting is Easy",
        "authors": [
            "Joel Seiferas",
            "Paul Vitanyi"
        ],
        "summary": "For any fixed $k$, a remarkably simple single-tape Turing machine can simulate $k$ independent counters in real time. Informally, a counter is a storage unit that maintains a single integer (initially 0), incrementing it, decrementing it, or reporting its sign (positive, negative, or zero) on command. Any automaton that responds to each successive command as a counter would is said to simulate a counter. (Only for a sign inquiry is the response of interest, of course. And zeroness is the only real issue, since a simulator can readily use zero detection to keep track of positivity and negativity in finite-state control. In this paper we describe a remarkably simple real-time simulation, based on just five simple rewriting rules, of any fixed number $k$ of independent counters. On a Turing machine with a single, binary work tape, the simulation runs in real time, handling an arbitrary counter command at each step. The space used by the simulation can be held to $(k+\\epsilon) \\log_2 n$ bits for the first $n$ commands, for any specified $\\epsilon > 0$.",
        "published": "2001-10-18T13:21:01Z",
        "link": "http://arxiv.org/abs/cs/0110038v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "E.1, F.1.1., F.2.2, G.2.1, E.2, E.4"
        ]
    },
    {
        "title": "Two heads are better than two tapes",
        "authors": [
            "Tao Jiang",
            "Joel Seiferas",
            "Paul Vitanyi"
        ],
        "summary": "We show that a Turing machine with two single-head one-dimensional tapes cannot recognize the set {x2x'| x \\in {0,1}^* and x' is a prefix of x} in real time, although it can do so with three tapes, two two-dimensional tapes, or one two-head tape, or in linear time with just one tape. In particular, this settles the longstanding conjecture that a two-head Turing machine can recognize more languages in real time if its heads are on the same one-dimensional tape than if they are on separate one-dimensional tapes.",
        "published": "2001-10-18T14:58:26Z",
        "link": "http://arxiv.org/abs/cs/0110039v1",
        "categories": [
            "cs.CC",
            "F.2.2, F.1.1"
        ]
    },
    {
        "title": "A New Approach to Formal Language Theory by Kolmogorov Complexity",
        "authors": [
            "Ming Li",
            "Paul Vitanyi"
        ],
        "summary": "We present a new approach to formal language theory using Kolmogorov complexity. The main results presented here are an alternative for pumping lemma(s), a new characterization for regular languages, and a new method to separate deterministic context-free languages and nondeterministic context-free languages. The use of the new `incompressibility arguments' is illustrated by many examples. The approach is also successful at the high end of the Chomsky hierarchy since one can quantify nonrecursiveness in terms of Kolmogorov complexity. (This is a preliminary uncorrected version. The final version is the one published in SIAM J. Comput., 24:2(1995), 398-410.)",
        "published": "2001-10-18T16:43:09Z",
        "link": "http://arxiv.org/abs/cs/0110040v1",
        "categories": [
            "cs.CC",
            "F.4.2, F.2, I.2.7"
        ]
    },
    {
        "title": "A Symmetric Strategy in Graph Avoidance Games",
        "authors": [
            "Frank Harary",
            "Wolfgang Slany",
            "Oleg Verbitsky"
        ],
        "summary": "In the graph avoidance game two players alternatingly color edges of a graph G in red and in blue respectively. The player who first creates a monochromatic subgraph isomorphic to a forbidden graph F loses. A symmetric strategy of the second player ensures that, independently of the first player's strategy, the blue and the red subgraph are isomorphic after every round of the game. We address the class of those graphs G that admit a symmetric strategy for all F and discuss relevant graph-theoretic and complexity issues. We also show examples when, though a symmetric strategy on G generally does not exist, it is still available for a particular F.",
        "published": "2001-10-24T10:22:57Z",
        "link": "http://arxiv.org/abs/cs/0110049v1",
        "categories": [
            "cs.DM",
            "cs.CC",
            "F.1.3; G.2.1; G.2.2"
        ]
    },
    {
        "title": "Probabilistic analysis of a differential equation for linear programming",
        "authors": [
            "Asa Ben-Hur",
            "Joshua Feinberg",
            "Shmuel Fishman",
            "Hava T. Siegelmann"
        ],
        "summary": "In this paper we address the complexity of solving linear programming problems with a set of differential equations that converge to a fixed point that represents the optimal solution. Assuming a probabilistic model, where the inputs are i.i.d. Gaussian variables, we compute the distribution of the convergence rate to the attracting fixed point. Using the framework of Random Matrix Theory, we derive a simple expression for this distribution in the asymptotic limit of large problem size. In this limit, we find that the distribution of the convergence rate is a scaling function, namely it is a function of one variable that is a combination of three parameters: the number of variables, the number of constraints and the convergence rate, rather than a function of these parameters separately. We also estimate numerically the distribution of computation times, namely the time required to reach a vicinity of the attracting fixed point, and find that it is also a scaling function. Using the problem size dependence of the distribution functions, we derive high probability bounds on the convergence rates and on the computation times.",
        "published": "2001-10-29T14:40:07Z",
        "link": "http://arxiv.org/abs/cs/0110056v2",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "math-ph",
            "math.MP",
            "math.OC",
            "F.1.3, F.2"
        ]
    },
    {
        "title": "On the complexity of inducing categorical and quantitative association   rules",
        "authors": [
            "Fabrizio Angiulli",
            "Giovambattista Ianni",
            "Luigi Palopoli"
        ],
        "summary": "Inducing association rules is one of the central tasks in data mining applications. Quantitative association rules induced from databases describe rich and hidden relationships holding within data that can prove useful for various application purposes (e.g., market basket analysis, customer profiling, and others). Even though such association rules are quite widely used in practice, a thorough analysis of the computational complexity of inducing them is missing. This paper intends to provide a contribution in this setting. To this end, we first formally define quantitative association rule mining problems, which entail boolean association rules as a special case, and then analyze their computational complexities, by considering both the standard cases, and a some special interesting case, that is, association rule induction over databases with null values, fixed-size attribute set databases, sparse databases, fixed threshold problems.",
        "published": "2001-11-06T15:42:06Z",
        "link": "http://arxiv.org/abs/cs/0111009v1",
        "categories": [
            "cs.CC",
            "F.2.0"
        ]
    },
    {
        "title": "Hiding solutions in random satisfiability problems: A statistical   mechanics approach",
        "authors": [
            "W. Barthel",
            "A. K. Hartmann",
            "M. Leone",
            "F. Ricci-Tersenghi",
            "M. Weigt",
            "R. Zecchina"
        ],
        "summary": "A major problem in evaluating stochastic local search algorithms for NP-complete problems is the need for a systematic generation of hard test instances having previously known properties of the optimal solutions. On the basis of statistical mechanics results, we propose random generators of hard and satisfiable instances for the 3-satisfiability problem (3SAT). The design of the hardest problem instances is based on the existence of a first order ferromagnetic phase transition and the glassy nature of excited states. The analytical predictions are corroborated by numerical results obtained from complete as well as stochastic local algorithms.",
        "published": "2001-11-09T09:10:57Z",
        "link": "http://arxiv.org/abs/cond-mat/0111153v2",
        "categories": [
            "cond-mat.dis-nn",
            "cond-mat.stat-mech",
            "cs.CC"
        ]
    },
    {
        "title": "Arc consistency for soft constraints",
        "authors": [
            "Martin Cooper",
            "Thomas Schiex"
        ],
        "summary": "The notion of arc consistency plays a central role in constraint satisfaction. It is known that the notion of local consistency can be extended to constraint optimisation problems defined by soft constraint frameworks based on an idempotent cost combination operator. This excludes non idempotent operators such as + which define problems which are very important in practical applications such as Max-CSP, where the aim is to minimize the number of violated constraints. In this paper, we show that using a weak additional axiom satisfied by most existing soft constraints proposals, it is possible to define a notion of soft arc consistency that extends the classical notion of arc consistency and this even in the case of non idempotent cost combination operators. A polynomial time algorithm for enforcing this soft arc consistency exists and its space and time complexities are identical to that of enforcing arc consistency in CSPs when the cost combination operator is strictly monotonic (for example Max-CSP). A directional version of arc consistency is potentially even stronger than the non-directional version, since it allows non local propagation of penalties. We demonstrate the utility of directional arc consistency by showing that it not only solves soft constraint problems on trees, but that it also implies a form of local optimality, which we call arc irreducibility.",
        "published": "2001-11-14T12:06:42Z",
        "link": "http://arxiv.org/abs/cs/0111038v3",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "I.2.8;F.4.1;D.3.3"
        ]
    },
    {
        "title": "Quantum Lower Bound for the Collision Problem",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "The collision problem is to decide whether a function X:{1,..,n}->{1,..,n} is one-to-one or two-to-one, given that one of these is the case. We show a lower bound of Theta(n^{1/5}) on the number of queries needed by a quantum computer to solve this problem with bounded error probability. The best known upper bound is O(n^{1/3}), but obtaining any lower bound better than Theta(1) was an open problem since 1997. Our proof uses the polynomial method augmented by some new ideas. We also give a lower bound of Theta(n^{1/7}) for the problem of deciding whether two sets are equal or disjoint on a constant fraction of elements. Finally we give implications of these results for quantum complexity theory.",
        "published": "2001-11-20T00:38:26Z",
        "link": "http://arxiv.org/abs/quant-ph/0111102v1",
        "categories": [
            "quant-ph",
            "cs.CC"
        ]
    },
    {
        "title": "A comparison of Zeroes and Ones of a Boolean Polynomial",
        "authors": [
            "M. N. Vyalyi"
        ],
        "summary": "In this paper we consider the computational complexity of the following problem. Let $f$ be a Boolean polynomial. What value of $f$, 0 or 1, is taken more frequently? The problem is solved in polynomial time for polynomials of degrees 1,2. The next case of degree 3 appears to be PP-complete under polynomial reductions in the class of promise problems. The proof is based on techniques of quantum computation.",
        "published": "2001-11-20T15:03:19Z",
        "link": "http://arxiv.org/abs/cs/0111052v1",
        "categories": [
            "cs.CC",
            "F.1.3"
        ]
    },
    {
        "title": "The similarity metric",
        "authors": [
            "Ming Li",
            "Xin Chen",
            "Xin Li",
            "Bin Ma",
            "Paul Vitanyi"
        ],
        "summary": "A new class of distances appropriate for measuring similarity relations between sequences, say one type of similarity per distance, is studied. We propose a new ``normalized information distance'', based on the noncomputable notion of Kolmogorov complexity, and show that it is in this class and it minorizes every computable distance in the class (that is, it is universal in that it discovers all computable similarities). We demonstrate that it is a metric and call it the {\\em similarity metric}. This theory forms the foundation for a new practical tool. To evidence generality and robustness we give two distinctive applications in widely divergent areas using standard compression programs like gzip and GenCompress. First, we compare whole mitochondrial genomes and infer their evolutionary history. This results in a first completely automatic computed whole mitochondrial phylogeny tree. Secondly, we fully automatically compute the language tree of 52 different languages.",
        "published": "2001-11-20T15:25:47Z",
        "link": "http://arxiv.org/abs/cs/0111054v3",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "cs.CE",
            "cs.CV",
            "math.CO",
            "math.MG",
            "math.ST",
            "physics.data-an",
            "q-bio.GN",
            "stat.TH",
            "J.3, E.4"
        ]
    },
    {
        "title": "Meaningful Information",
        "authors": [
            "Paul Vitanyi"
        ],
        "summary": "The information in an individual finite object (like a binary string) is commonly measured by its Kolmogorov complexity. One can divide that information into two parts: the information accounting for the useful regularity present in the object and the information accounting for the remaining accidental information. There can be several ways (model classes) in which the regularity is expressed. Kolmogorov has proposed the model class of finite sets, generalized later to computable probability mass functions. The resulting theory, known as Algorithmic Statistics, analyzes the algorithmic sufficient statistic when the statistic is restricted to the given model class. However, the most general way to proceed is perhaps to express the useful information as a recursive function. The resulting measure has been called the ``sophistication'' of the object. We develop the theory of recursive functions statistic, the maximum and minimum value, the existence of absolutely nonstochastic objects (that have maximal sophistication--all the information in them is meaningful and there is no residual randomness), determine its relation with the more restricted model classes of finite sets, and computable probability distributions, in particular with respect to the algorithmic (Kolmogorov) minimal sufficient statistic, the relation to the halting problem and further algorithmic properties.",
        "published": "2001-11-20T15:38:30Z",
        "link": "http://arxiv.org/abs/cs/0111053v3",
        "categories": [
            "cs.CC",
            "math-ph",
            "math.MP",
            "math.PR",
            "physics.data-an",
            "E.5; E.4; E.2; H.1.1; F.1.1; F.1.3"
        ]
    },
    {
        "title": "Some Facets of Complexity Theory and Cryptography: A Five-Lectures   Tutorial",
        "authors": [
            "Jörg Rothe"
        ],
        "summary": "In this tutorial, selected topics of cryptology and of computational complexity theory are presented. We give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. Particular attention is paid to cryptographic protocols and the problem of constructing the key components of such protocols such as one-way functions. A function is one-way if it is easy to compute, but hard to invert. We discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. We also consider interactive proof systems and present some interesting zero-knowledge protocols. In a zero-knowledge protocol one party can convince the other party of knowing some secret information without disclosing any bit of this information. Motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.",
        "published": "2001-11-21T11:24:27Z",
        "link": "http://arxiv.org/abs/cs/0111056v2",
        "categories": [
            "cs.CC",
            "cs.CR",
            "E.3; F.1.3; F.2.2"
        ]
    },
    {
        "title": "Towards a characterization of the star-free sets of integers",
        "authors": [
            "Michel Rigo"
        ],
        "summary": "Let U be a numeration system, a set X of integers is U-star-free if the set made up of the U-representations of the elements in X is a star-free regular language. Answering a question of A. de Luca and A. Restivo, we obtain a complete logical characterization of the U-star-free sets of integers for suitable numeration systems related to a Pisot number and in particular for integer base systems. For these latter systems, we study as well the problem of the base dependence. Finally, the case of k-adic systems is also investigated.",
        "published": "2001-11-23T10:18:32Z",
        "link": "http://arxiv.org/abs/cs/0111057v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.1;F.4.1;F.4.3"
        ]
    },
    {
        "title": "One-way communication complexity and the Neciporuk lower bound on   formula size",
        "authors": [
            "Hartmut Klauck"
        ],
        "summary": "In this paper the Neciporuk method for proving lower bounds on the size of Boolean formulae is reformulated in terms of one-way communication complexity. We investigate the scenarios of probabilistic formulae, nondeterministic formulae, and quantum formulae. In all cases we can use results about one-way communication complexity to prove lower bounds on formula size. In the latter two cases we newly develop the employed communication complexity bounds. The main results regarding formula size are as follows: A polynomial size gap between probabilistic/quantum and deterministic formulae. A near-quadratic size gap for nondeterministic formulae with limited access to nondeterministic bits. A near quadratic lower bound on quantum formula size, as well as a polynomial separation between the sizes of quantum formulae with and without multiple read random inputs. The methods for quantum and probabilistic formulae employ a variant of the Neciporuk bound in terms of the VC-dimension. Regarding communication complexity we give optimal separations between one-way and two-way protocols in the cases of limited nondeterministic and quantum communication, and we show that zero-error quantum one-way communication complexity asymptotically equals deterministic one-way communication complexity for total functions.",
        "published": "2001-11-29T19:23:33Z",
        "link": "http://arxiv.org/abs/cs/0111062v2",
        "categories": [
            "cs.CC",
            "quant-ph",
            "F.1.3;F.1.2"
        ]
    },
    {
        "title": "An Average Case NP-Complete Graph Coloring Problem",
        "authors": [
            "Leonid A. Levin",
            "Ramarathnam Venkatesan"
        ],
        "summary": "NP-complete problems should be hard on some instances but those may be extremely rare. On generic instances many such problems, especially related to random graphs, have been proven easy. We show the intractability of random instances of a graph coloring problem: this graph problem is hard on average unless all NP problem under all samplable (i.e., generatable in polynomial time) distributions are easy. Worst case reductions use special gadgets and typically map instances into a negligible fraction of possible outputs. Ours must output nearly random graphs and avoid any super-polynomial distortion of probabilities.",
        "published": "2001-12-02T21:44:23Z",
        "link": "http://arxiv.org/abs/cs/0112001v10",
        "categories": [
            "cs.CC",
            "60C-05, 68Q-17, 25, 87, 05C-15, 20, 80",
            "F.2.2"
        ]
    },
    {
        "title": "Program schemes with binary write-once arrays and the complexity classes   they capture",
        "authors": [
            "Iain A. Stewart"
        ],
        "summary": "We study a class of program schemes, NPSB, in which, aside from basic assignments, non-deterministic guessing and while loops, we have access to arrays; but where these arrays are binary write-once in that they are initialized to `zero' and can only ever be set to `one'. We show, amongst other results, that: NPSB can be realized as a vectorized Lindstrom logic; there are problems accepted by program schemes of NPSB that are not definable in the bounded-variable infinitary logic ${\\cal L}^\\omega_{\\infty\\omega}$; all problems accepted by the program schemes of NPSB have a zero-one law; and on ordered structures, NPSB captures the complexity class $[ L]^[{\\scriptsize NP\\normalsize}]$. The class of program schemes NPSB is actually the union of an infinite hierarchy of classes of program schemes. When we amend the semantics of our program schemes slightly, we find that the classes of the resulting hierarchy capture the complexity classes $\\Sigma^p_i$ (where $i\\geq 1$) of the Polynomial Hierarchy PH. Finally, we give logical equivalences of the complexity-theoretic question `Does NP equal PSPACE?' where the logics (and classes of program schemes) involved define only problems with zero-one laws (and so do not define some computationally trivial problems).",
        "published": "2001-12-03T15:15:17Z",
        "link": "http://arxiv.org/abs/cs/0112002v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.1.3; F.1.1"
        ]
    },
    {
        "title": "Boosting search by rare events",
        "authors": [
            "Andrea Montanari",
            "Riccardo Zecchina"
        ],
        "summary": "Randomized search algorithms for hard combinatorial problems exhibit a large variability of performances. We study the different types of rare events which occur in such out-of-equilibrium stochastic processes and we show how they cooperate in determining the final distribution of running times. As a byproduct of our analysis we show how search algorithms are optimized by random restarts.",
        "published": "2001-12-08T18:14:52Z",
        "link": "http://arxiv.org/abs/cond-mat/0112142v2",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.CC"
        ]
    },
    {
        "title": "DNA Self-Assembly For Constructing 3D Boxes",
        "authors": [
            "Ming-Yang Kao",
            "Vijay Ramachandran"
        ],
        "summary": "We propose a mathematical model of DNA self-assembly using 2D tiles to form 3D nanostructures. This is the first work to combine studies in self-assembly and nanotechnology in 3D, just as Rothemund and Winfree did in the 2D case. Our model is a more precise superset of their Tile Assembly Model that facilitates building scalable 3D molecules. Under our model, we present algorithms to build a hollow cube, which is intuitively one of the simplest 3D structures to construct. We also introduce five basic measures of complexity to analyze these algorithms. Our model and algorithmic techniques are applicable to more complex 2D and 3D nanostructures.",
        "published": "2001-12-08T21:36:28Z",
        "link": "http://arxiv.org/abs/cs/0112009v1",
        "categories": [
            "cs.CC",
            "cs.CE",
            "F.2.2; F.1.1; J.3"
        ]
    },
    {
        "title": "Guaranteeing the diversity of number generators",
        "authors": [
            "Adi Shamir",
            "Boaz Tsaban"
        ],
        "summary": "A major problem in using iterative number generators of the form x_i=f(x_{i-1}) is that they can enter unexpectedly short cycles. This is hard to analyze when the generator is designed, hard to detect in real time when the generator is used, and can have devastating cryptanalytic implications. In this paper we define a measure of security, called_sequence_diversity_, which generalizes the notion of cycle-length for non-iterative generators. We then introduce the class of counter assisted generators, and show how to turn any iterative generator (even a bad one designed or seeded by an adversary) into a counter assisted generator with a provably high diversity, without reducing the quality of generators which are already cryptographically strong.",
        "published": "2001-12-12T18:35:29Z",
        "link": "http://arxiv.org/abs/cs/0112014v5",
        "categories": [
            "cs.CR",
            "cs.CC",
            "math.CO",
            "G.3; G.2.1; D.4.6"
        ]
    },
    {
        "title": "Pseudorandom permutations with the fast forward property",
        "authors": [
            "Boaz Tsaban"
        ],
        "summary": "This paper has been withdrawn by the author(s), due to the existence of a much better paper in http://arxiv.org/abs/cs.CR/0207027",
        "published": "2001-12-13T18:31:35Z",
        "link": "http://arxiv.org/abs/cs/0112016v2",
        "categories": [
            "cs.CR",
            "cs.CC",
            "G.3; G.2.1; D.4.6"
        ]
    },
    {
        "title": "Exact Complexity of the Winner Problem for Young Elections",
        "authors": [
            "Jörg Rothe",
            "Holger Spakowski",
            "Jörg Vogel"
        ],
        "summary": "In 1977, Young proposed a voting scheme that extends the Condorcet Principle based on the fewest possible number of voters whose removal yields a Condorcet winner. We prove that both the winner and the ranking problem for Young elections is complete for the class of problems solvable in polynomial time by parallel access to NP. Analogous results for Lewis Carroll's 1876 voting scheme were recently established by Hemaspaandra et al. In contrast, we prove that the winner and ranking problems in Fishburn's homogeneous variant of Carroll's voting scheme can be solved efficiently by linear programming.",
        "published": "2001-12-20T11:37:33Z",
        "link": "http://arxiv.org/abs/cs/0112021v1",
        "categories": [
            "cs.CC",
            "F.1.3; F.2.2; J.4"
        ]
    },
    {
        "title": "The computational complexity of the local postage stamp problem",
        "authors": [
            "Jeffrey Shallit"
        ],
        "summary": "The well-studied local postage stamp problem (LPSP) is the following: given a positive integer k, a set of postive integers 1 = a1 < a2 < ... < ak and an integer h >= 1, what is the smallest positive integer which cannot be represented as a linear combination x1 a1 + ... + xk ak where x1 + ... + xk <= h and each xi is a non-negative integer? In this note we prove that LPSP is NP-hard under Turing reductions, but can be solved in polynomial time if k is fixed.",
        "published": "2001-12-22T21:20:57Z",
        "link": "http://arxiv.org/abs/math/0112257v1",
        "categories": [
            "math.NT",
            "cs.CC",
            "math.CO",
            "11B13 (primary); 11D85; 68Q25; 11Y16 (secondary)"
        ]
    },
    {
        "title": "A Classification of Symbolic Transition Systems",
        "authors": [
            "Thomas A. Henzinger",
            "Rupak Majumdar",
            "Jean-Francois Raskin"
        ],
        "summary": "We define five increasingly comprehensive classes of infinite-state systems, called STS1--5, whose state spaces have finitary structure. For four of these classes, we provide examples from hybrid systems.",
        "published": "2001-01-16T20:17:30Z",
        "link": "http://arxiv.org/abs/cs/0101013v1",
        "categories": [
            "cs.LO",
            "F.3.1"
        ]
    },
    {
        "title": "On the problem of computing the well-founded semantics",
        "authors": [
            "Zbigniew Lonc",
            "Miroslaw Truszczynski"
        ],
        "summary": "The well-founded semantics is one of the most widely studied and used semantics of logic programs with negation. In the case of finite propositional programs, it can be computed in polynomial time, more specifically, in O(|At(P)|size(P)) steps, where size(P) denotes the total number of occurrences of atoms in a logic program P. This bound is achieved by an algorithm introduced by Van Gelder and known as the alternating-fixpoint algorithm. Improving on the alternating-fixpoint algorithm turned out to be difficult. In this paper we study extensions and modifications of the alternating-fixpoint approach. We then restrict our attention to the class of programs whose rules have no more than one positive occurrence of an atom in their bodies. For programs in that class we propose a new implementation of the alternating-fixpoint method in which false atoms are computed in a top-down fashion. We show that our algorithm is faster than other known algorithms and that for a wide class of programs it is linear and so, asymptotically optimal.",
        "published": "2001-01-17T13:33:12Z",
        "link": "http://arxiv.org/abs/cs/0101014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.DS",
            "I.2.3; F.2.2"
        ]
    },
    {
        "title": "Checking Properties within Fairness and Behavior Abstractions",
        "authors": [
            "Ulrich Ultes-Nitsche",
            "Pierre Wolper"
        ],
        "summary": "This paper is motivated by the fact that verifying liveness properties under a fairness condition is often problematic, especially when abstraction is used. It shows that using a more abstract notion than truth under fairness, specifically the concept of a property being satisfied within fairness can lead to interesting possibilities. Technically, it is first established that deciding satisfaction within fairness is a PSPACE-complete problem and it is shown that properties satisfied within fairness can always be satisfied by some fair implementation. Thereafter, the interaction between behavior abstraction and satisfaction within fairness is studied and it is proved that satisfaction of properties within fairness can be verified on behavior abstractions, if the abstraction homomorphism is weakly continuation-closed.",
        "published": "2001-01-19T09:35:10Z",
        "link": "http://arxiv.org/abs/cs/0101017v1",
        "categories": [
            "cs.LO",
            "D.2.4; F.3.1"
        ]
    },
    {
        "title": "Semantics and Termination of Simply-Moded Logic Programs with Dynamic   Scheduling",
        "authors": [
            "Annalisa Bossi",
            "Sandro Etalle",
            "Sabina Rossi",
            "Jan-Georg Smaus"
        ],
        "summary": "In logic programming, dynamic scheduling refers to a situation where the selection of the atom in each resolution (computation) step is determined at runtime, as opposed to a fixed selection rule such as the left-to-right one of Prolog. This has applications e.g. in parallel programming. A mechanism to control dynamic scheduling is provided in existing languages in the form of delay declarations.   Input-consuming derivations were introduced to describe dynamic scheduling while abstracting from the technical details. In this paper, we first formalise the relationship between delay declarations and input-consuming derivations, showing in many cases a one-to-one correspondence. Then, we define a model-theoretic semantics for input-consuming derivations of simply-moded programs. Finally, for this class of programs, we provide a necessary and sufficient criterion for termination.",
        "published": "2001-01-23T12:50:12Z",
        "link": "http://arxiv.org/abs/cs/0101022v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.3; D.1.6; F.3.2"
        ]
    },
    {
        "title": "Properties of Input-Consuming Derivations",
        "authors": [
            "Annalisa Bossi",
            "Sandro Etalle",
            "Sabina Rossi"
        ],
        "summary": "We study the properties of input-consuming derivations of moded logic programs. Input-consuming derivations can be used to model the behavior of logic programs using dynamic scheduling and employing constructs such as delay declarations.   We consider the class of nicely-moded programs and queries. We show that for these programs a weak version of the well-known switching lemma holds also for input-consuming derivations. Furthermore, we show that, under suitable conditions, there exists an algebraic characterization of termination of input-consuming derivations.",
        "published": "2001-01-23T13:35:27Z",
        "link": "http://arxiv.org/abs/cs/0101023v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6;D.3.1;F.3.2"
        ]
    },
    {
        "title": "The Limits of Horn Logic Programs",
        "authors": [
            "Shilong Ma",
            "Yuefei Sui",
            "Ke Xu"
        ],
        "summary": "Given a sequence $\\{\\Pi_n\\}$ of Horn logic programs, the limit $\\Pi$ of $\\{\\Pi_n\\}$ is the set of the clauses such that every clause in $\\Pi$ belongs to almost every $\\Pi_n$ and every clause in infinitely many $\\Pi_n$'s belongs to $\\Pi$ also. The limit program $\\Pi$ is still Horn but may be infinite. In this paper, we consider if the least Herbrand model of the limit of a given Horn logic program sequence $\\{\\Pi_n\\}$ equals the limit of the least Herbrand models of each logic program $\\Pi_n$. It is proved that this property is not true in general but holds if Horn logic programs satisfy an assumption which can be syntactically checked and be satisfied by a class of Horn logic programs. Thus, under this assumption we can approach the least Herbrand model of the limit $\\Pi$ by the sequence of the least Herbrand models of each finite program $\\Pi_n$. We also prove that if a finite Horn logic program satisfies this assumption, then the least Herbrand model of this program is recursive. Finally, by use of the concept of stability from dynamical systems, we prove that this assumption is exactly a sufficient condition to guarantee the stability of fixed points for Horn logic programs.",
        "published": "2001-03-08T07:42:48Z",
        "link": "http://arxiv.org/abs/cs/0103008v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "The Existential Theory of Equations with Rational Constraints in Free   Groups is PSPACE-Complete",
        "authors": [
            "Volker Diekert",
            "Claudio Gutierrez",
            "Christian Hagenah"
        ],
        "summary": "It is known that the existential theory of equations in free groups is decidable. This is a famous result of Makanin. On the other hand it has been shown that the scheme of his algorithm is not primitive recursive. In this paper we present an algorithm that works in polynomial space, even in the more general setting where each variable has a rational constraint, that is, the solution has to respect a specification given by a regular word language. Our main result states that the existential theory of equations in free groups with rational constraints is PSPACE-complete. We obtain this result as a corollary of the corresponding statement about free monoids with involution.",
        "published": "2001-03-26T15:58:40Z",
        "link": "http://arxiv.org/abs/cs/0103018v1",
        "categories": [
            "cs.DS",
            "cs.LO",
            "F.2.2; F.4"
        ]
    },
    {
        "title": "Belief Revision: A Critique",
        "authors": [
            "Nir Friedman",
            "Joseph Y. Halpern"
        ],
        "summary": "We examine carefully the rationale underlying the approaches to belief change taken in the literature, and highlight what we view as methodological problems. We argue that to study belief change carefully, we must be quite explicit about the ``ontology'' or scenario underlying the belief change process. This is something that has been missing in previous work, with its focus on postulates. Our analysis shows that we must pay particular attention to two issues that have often been taken for granted: The first is how we model the agent's epistemic state. (Do we use a set of beliefs, or a richer structure, such as an ordering on worlds? And if we use a set of beliefs, in what language are these beliefs are expressed?) We show that even postulates that have been called ``beyond controversy'' are unreasonable when the agent's beliefs include beliefs about her own epistemic state as well as the external world. The second is the status of observations. (Are observations known to be true, or just believed? In the latter case, how firm is the belief?) Issues regarding the status of observations arise particularly when we consider iterated belief revision, and we must confront the possibility of revising by p and then by not-p.",
        "published": "2001-03-27T20:33:51Z",
        "link": "http://arxiv.org/abs/cs/0103020v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4, F.4.1"
        ]
    },
    {
        "title": "A Higher-Order Implementation of Rewriting",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "Many automatic theorem-provers rely on rewriting. Using theorems as rewrite rules helps to simplify the subgoals that arise during a proof.   LCF is an interactive theorem-prover intended for reasoning about computation. Its implementation of rewriting is presented in detail. LCF provides a family of rewriting functions, and operators to combine them. A succession of functions is described, from pattern matching primitives to the rewriting tool that performs most inferences in LCF proofs.   The design is highly modular. Each function performs a basic, specific task, such as recognizing a certain form of tautology. Each operator implements one method of building a rewriting function from simpler ones. These pieces can be put together in numerous ways, yielding a variety of rewrit- ing strategies.   The approach involves programming with higher-order functions. Rewriting functions are data values, produced by computation on other rewriting functions. The code is in daily use at Cambridge, demonstrating the practical use of functional programming.",
        "published": "2001-03-29T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301108v1",
        "categories": [
            "cs.LO",
            "D.1.1, D.2.4, F.4.1"
        ]
    },
    {
        "title": "Logic Programming, Functional Programming, and Inductive Definitions",
        "authors": [
            "Lawrence C. Paulson",
            "Andrew W. Smith"
        ],
        "summary": "An attempt at unifying logic and functional programming is reported. As a starting point, we take the view that \"logic programs\" are not about logic but constitute inductive definitions of sets and relations. A skeletal language design based on these considerations is sketched and a prototype implementation discussed.",
        "published": "2001-03-29T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301109v1",
        "categories": [
            "cs.LO",
            "D.1.1, D.1.6"
        ]
    },
    {
        "title": "Designing a Theorem Prover",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "A step-by-step presentation of the code for a small theorem prover introduces theorem-proving techniques. The programming language used is Standard ML. The prover operates on a sequent calculus formulation of first-order logic, which is briefly explained. The implementation of unification and logical inference is shown. The prover is demonstrated on several small examples, including one that shows its limitations. The final part of the paper is a survey of contemporary research on interactive theorem proving.",
        "published": "2001-03-29T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9301110v1",
        "categories": [
            "cs.LO",
            "F.4.1, I.2.3, D.1.1"
        ]
    },
    {
        "title": "A Concrete Final Coalgebra Theorem for ZF Set Theory",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "A special final coalgebra theorem, in the style of Aczel's, is proved within standard Zermelo-Fraenkel set theory. Aczel's Anti-Foundation Axiom is replaced by a variant definition of function that admits non-well-founded constructions. Variant ordered pairs and tuples, of possibly infinite length, are special cases of variant functions. Analogues of Aczel's Solution and Substitution Lemmas are proved in the style of Rutten and Turi. The approach is less general than Aczel's, but the treatment of non-well-founded objects is simple and concrete. The final coalgebra of a functor is its greatest fixedpoint. The theory is intended for machine implementation and a simple case of it is already implemented using the theorem prover Isabelle.",
        "published": "2001-03-29T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9511103v1",
        "categories": [
            "cs.LO",
            "F.3.2"
        ]
    },
    {
        "title": "Mechanizing Set Theory: Cardinal Arithmetic and the Axiom of Choice.",
        "authors": [
            "Lawrence C. Paulson",
            "Krzysztof Grabczewski"
        ],
        "summary": "Fairly deep results of Zermelo-Frenkel (ZF) set theory have been mechanized using the proof assistant Isabelle. The results concern cardinal arithmetic and the Axiom of Choice (AC). A key result about cardinal multiplication is K*K = K, where K is any infinite cardinal. Proving this result required developing theories of orders, order-isomorphisms, order types, ordinal arithmetic, cardinals, etc.; this covers most of Kunen, Set Theory, Chapter I. Furthermore, we have proved the equivalence of 7 formulations of the Well-ordering Theorem and 20 formulations of AC; this covers the first two chapters of Rubin and Rubin, Equivalents of the Axiom of Choice, and involves highly technical material. The definitions used in the proofs are largely faithful in style to the original mathematics.",
        "published": "2001-03-29T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9612104v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Generic Automatic Proof Tools",
        "authors": [
            "Lawrence C. Paulson"
        ],
        "summary": "This book chapter establishes connections between the interactive proof tool Isabelle and classical tableau and resolution technology. Isabelle's classical reasoner is described and demonstrated by an extended case study: the Church-Rosser theorem for combinators. Compared with other interactive theorem provers, Isabelle's classical reasoner achieves a high degree of automation.",
        "published": "2001-03-29T00:00:00Z",
        "link": "http://arxiv.org/abs/cs/9711106v1",
        "categories": [
            "cs.LO",
            "F.4.1, I.2.3"
        ]
    },
    {
        "title": "Chain Programs for Writing Deterministic Metainterpreters",
        "authors": [
            "David A. Rosenblueth"
        ],
        "summary": "Many metainterpreters found in the logic programming literature are nondeterministic in the sense that the selection of program clauses is not determined. Examples are the familiar \"demo\" and \"vanilla\" metainterpreters. For some applications this nondeterminism is convenient. In some cases, however, a deterministic metainterpreter, having an explicit selection of clauses, is needed. Such cases include (1) conversion of OR parallelism into AND parallelism for \"committed-choice\" processors, (2) logic-based, imperative-language implementation of search strategies, and (3) simulation of bounded-resource reasoning.   Deterministic metainterpreters are difficult to write because the programmer must be concerned about the set of unifiers of the children of a node in the derivation tree. We argue that it is both possible and advantageous to write these metainterpreters by reasoning in terms of object programs converted into a syntactically restricted form that we call \"chain\" form, where we can forget about unification, except for unit clauses. We give two transformations converting logic programs into chain form, one for \"moded\" programs (implicit in two existing exhaustive-traversal methods for committed-choice execution), and one for arbitrary definite programs. As illustrations of our approach we show examples of the three applications mentioned above.",
        "published": "2001-04-02T23:34:01Z",
        "link": "http://arxiv.org/abs/cs/0104003v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; F.3.2; D.3.2; D.3.4"
        ]
    },
    {
        "title": "Disjunction and modular goal-directed proof search",
        "authors": [
            "Matthew Stone"
        ],
        "summary": "This paper explores goal-directed proof search in first-order multi-modal logic. The key issue is to design a proof system that respects the modularity and locality of assumptions of many modal logics. By forcing ambiguities to be considered independently, modular disjunctions in particular can be used to construct efficiently executable specifications in reasoning tasks involving partial information that otherwise might require prohibitive search. To achieve this behavior requires prior proof-theoretic justifications of logic programming to be extended, strengthened, and combined with proof-theoretic analyses of modal deduction in a novel way.",
        "published": "2001-04-30T14:23:50Z",
        "link": "http://arxiv.org/abs/cs/0104021v2",
        "categories": [
            "cs.LO",
            "F.4.1; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Analysis of Polymorphically Typed Logic Programs Using ACI-Unification",
        "authors": [
            "Jan-Georg Smaus"
        ],
        "summary": "Analysis of (partial) groundness is an important application of abstract interpretation. There are several proposals for improving the precision of such an analysis by exploiting type information, icluding our own work with Hill and King, where we had shown how the information present in the type declarations of a program can be used to characterise the degree of instantiation of a term in a precise and yet inherently finite way. This approach worked for polymorphically typed programs as in Goedel or HAL. Here, we recast this approach following works by Codish, Lagoon and Stuckey. To formalise which properties of terms we want to characterise, we use labelling functions, which are functions that extract subterms from a term along certain paths. An abstract term collects the results of all labelling functions of a term. For the analysis, programs are executed on abstract terms instead of the concrete ones, and usual unification is replaced by unification modulo an equality theory which includes the well-known ACI-theory. Thus we generalise the works by Codish, Lagoon and Stuckey w.r.t. the type systems considered and relate the works among each other.",
        "published": "2001-05-04T10:33:39Z",
        "link": "http://arxiv.org/abs/cs/0105007v1",
        "categories": [
            "cs.LO",
            "D.1.6; F.3.2; F.3.3"
        ]
    },
    {
        "title": "A Logical Framework for Convergent Infinite Computations",
        "authors": [
            "Wei Li",
            "Shilong Ma",
            "Yuefei Sui",
            "Ke Xu"
        ],
        "summary": "Classical computations can not capture the essence of infinite computations very well. This paper will focus on a class of infinite computations called convergent infinite computations}. A logic for convergent infinite computations is proposed by extending first order theories using Cauchy sequences, which has stronger expressive power than the first order logic. A class of fixed points characterizing the logical properties of the limits can be represented by means of infinite-length terms defined by Cauchy sequences. We will show that the limit of sequence of first order theories can be defined in terms of distance, similar to the $\\epsilon-N$ style definition of limits in real analysis. On the basis of infinitary terms, a computation model for convergent infinite computations is proposed. Finally, the interpretations of logic programs are extended by introducing real Herbrand models of logic programs and a sufficient condition for computing a real Herbrand model of Horn logic programs using convergent infinite computation is given.",
        "published": "2001-05-10T03:58:46Z",
        "link": "http://arxiv.org/abs/cs/0105020v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1, D.1.6"
        ]
    },
    {
        "title": "Solving Composed First-Order Constraints from Discrete-Time Robust   Control",
        "authors": [
            "Stefan Ratschan",
            "Luc Jaulin"
        ],
        "summary": "This paper deals with a problem from discrete-time robust control which requires the solution of constraints over the reals that contain both universal and existential quantifiers. For solving this problem we formulate it as a program in a (fictitious) constraint logic programming language with explicit quantifier notation. This allows us to clarify the special structure of the problem, and to extend an algorithm for computing approximate solution sets of first-order constraints over the reals to exploit this structure. As a result we can deal with inputs that are clearly out of reach for current symbolic solvers.",
        "published": "2001-05-11T08:28:33Z",
        "link": "http://arxiv.org/abs/cs/0105021v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CE",
            "F.4.1;I.2.8"
        ]
    },
    {
        "title": "Disjunctive Logic Programs with Inheritance",
        "authors": [
            "Francesco Buccafurri",
            "Wolfgang Faber",
            "Nicola Leone"
        ],
        "summary": "The paper proposes a new knowledge representation language, called DLP<, which extends disjunctive logic programming (with strong negation) by inheritance. The addition of inheritance enhances the knowledge modeling features of the language providing a natural representation of default reasoning with exceptions.   A declarative model-theoretic semantics of DLP< is provided, which is shown to generalize the Answer Set Semantics of disjunctive logic programs.   The knowledge modeling features of the language are illustrated by encoding classical nonmonotonic problems in DLP<.   The complexity of DLP< is analyzed, proving that inheritance does not cause any computational overhead, as reasoning in DLP< has exactly the same complexity as reasoning in disjunctive logic programming. This is confirmed by the existence of an efficient translation from DLP< to plain disjunctive logic programming. Using this translation, an advanced KR system supporting the DLP< language has been implemented on top of the DLV system and has subsequently been integrated into DLV.",
        "published": "2001-05-30T20:32:49Z",
        "link": "http://arxiv.org/abs/cs/0105036v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "D.1.6; I.2.3; I.2.4"
        ]
    },
    {
        "title": "Model Checking Contractual Protocols",
        "authors": [
            "Aspassia Daskalopulu"
        ],
        "summary": "This paper discusses how model checking, a technique used for the verification of behavioural requirements of dynamic systems, can be usefully deployed for the verification of contracts. A process view of agreements between parties is taken, whereby a contract is modelled as it evolves over time in terms of actions or more generally events that effect changes in its state. Modelling is done with Petri Nets in the spirit of other research work on the representation of trade procedures. The paper illustrates all the phases of the verification technique through an example and argues that the approach is useful particularly in the context of pre-contractual negotiation and contract drafting. The work reported here is part of a broader project on the development of logic-based tools for the analysis and representation of legal contracts.",
        "published": "2001-06-07T14:53:00Z",
        "link": "http://arxiv.org/abs/cs/0106009v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.2.4;I.2.4"
        ]
    },
    {
        "title": "Modelling Legal Contracts as Processes",
        "authors": [
            "Aspassia Daskalopulu"
        ],
        "summary": "This paper concentrates on the representation of the legal relations that obtain between parties once they have entered a contractual agreement and their evolution as the agreement progresses through time. Contracts are regarded as process and they are analysed in terms of the obligations that are active at various points during their life span. An informal notation is introduced that summarizes conveniently the states of an agreement as it evolves over time. Such a representation enables us to determine what the status of an agreement is, given an event or a sequence of events that concern the performance of actions by the agents involved. This is useful both in the context of contract drafting (where parties might wish to preview how their agreement might evolve) and in the context of contract performance monitoring (where parties might with to establish what their legal positions are once their agreement is in force). The discussion is based on an example that illustrates some typical patterns of contractual obligations.",
        "published": "2001-06-07T14:59:03Z",
        "link": "http://arxiv.org/abs/cs/0106010v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4"
        ]
    },
    {
        "title": "The Set of Equations to Evaluate Objects",
        "authors": [
            "Larissa Ismailova"
        ],
        "summary": "The notion of an equational shell is studied to involve the objects and their environment. Appropriate methods are studied as valid embeddings of refined objects. The refinement process determines the linkages between the variety of possible representations giving rise to variants of computations. The case study is equipped with the adjusted equational systems that validate the initial applicative framework.",
        "published": "2001-06-08T09:28:46Z",
        "link": "http://arxiv.org/abs/cs/0106013v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.3.1; D.3.2"
        ]
    },
    {
        "title": "An object evaluator to generate flexible applications",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko"
        ],
        "summary": "This paper contains a brief discussion of an object evaluator which is based on principles of evaluations in a category. The main tool system referred as the Application Development Environment (ADE) is used to build database applications involving the graphical user interface (GUI). The separation of a database access and the user interface is reached by distinguishing the potential and actual objects. The variety of applications may be generated that communicate with different and distinct desktop databases. The commutative diagrams' technique allows to involve retrieval and call of the delayed procedures.",
        "published": "2001-06-10T16:52:29Z",
        "link": "http://arxiv.org/abs/cs/0106017v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "H.1; H.2"
        ]
    },
    {
        "title": "Building the access pointers to a computation environment",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "A common object technique equipped with the categorical and computational styles is briefly outlined. An object is evaluated by embedding in a host computational environment which is the domain-ranged structure. An embedded object is accessed by the pointers generated within the host system. To assist with an easy extract the result of the evaluation a pre-embedded object is generated. It is observed as the decomposition into substitutional part and access function part which are generated during the object evaluation.",
        "published": "2001-06-10T17:31:13Z",
        "link": "http://arxiv.org/abs/cs/0106018v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Object-oriented solutions",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "In this paper are briefly outlined the motivations, mathematical ideas in use, pre-formalization and assumptions, object-as-functor construction, `soft' types and concept constructions, case study for concepts based on variable domains, extracting a computational background, and examples of evaluations.",
        "published": "2001-06-11T11:46:14Z",
        "link": "http://arxiv.org/abs/cs/0106021v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Object-oriented tools for advanced applications",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko"
        ],
        "summary": "This paper contains a brief discussion of the Application Development Environment (ADE) that is used to build database applications involving the graphical user interface (GUI). ADE computing separates the database access and the user interface. The variety of applications may be generated that communicate with different and distinct desktop databases. The advanced techniques allows to involve remote or stored procedures retrieval and call.",
        "published": "2001-06-11T17:25:25Z",
        "link": "http://arxiv.org/abs/cs/0106023v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Objects and their computational framework",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "Most of the object notions are embedded into a logical domain, especially when dealing with a database theory. Thus, their properties within a computational domain are not yet studied properly. The main topic of this paper is to analyze different concepts of the distinct computational primitive frames to extract the useful object properties and their possible advantages. Some important metaoperators are used to unify the approaches and to establish their possible correspondences.",
        "published": "2001-06-11T17:35:18Z",
        "link": "http://arxiv.org/abs/cs/0106024v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Event Driven Computations for Relational Query Language",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko",
            "Lioubouv Bourmistrova"
        ],
        "summary": "This paper deals with an extended model of computations which uses the parameterized families of entities for data objects and reflects a preliminary outline of this problem. Some topics are selected out, briefly analyzed and arranged to cover a general problem. The authors intended more to discuss the particular topics, their interconnection and computational meaning as a panel proposal, so that this paper is not yet to be evaluated as a closed journal paper. To save space all the technical and implementation features are left for the future paper.   Data object is a schematic entity and modelled by the partial function. A notion of type is extended by the variable domains which depend on events and types. A variable domain is built from the potential and schematic individuals and generates the valid families of types depending on a sequence of events. Each valid type consists of the actual individuals which are actual relatively the event or script. In case when a type depends on the script then corresponding view for data objects is attached, otherwise a snapshot is generated. The type thus determined gives an upper range for typed variables so that the local ranges are event driven resulting is the families of actual individuals. An expressive power of the query language is extended using the extensional and intentional relations.",
        "published": "2001-06-12T10:18:36Z",
        "link": "http://arxiv.org/abs/cs/0106026v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Event Driven Objects",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "A formal consideration in this paper is given for the essential notations to characterize the object that is distinguished in a problem domain. The distinct object is represented by another idealized object, which is a schematic element. When the existence of an element is significant, then a class of these partial elements is dropped down into actual, potential and virtual objects. The potential objects are gathered into the variable domains which are the extended ranges for unbound variables. The families of actual objects are shown to be parameterized with the types and events. The transitions between events are shown to be driven by the scripts. A computational framework arises which is described by the commutative diagrams.",
        "published": "2001-06-12T10:48:30Z",
        "link": "http://arxiv.org/abs/cs/0106027v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.SE",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Building Views with Description Logics in ADE: Application Development   Environment",
        "authors": [
            "Larissa Ismailova",
            "Sergey Kosikov",
            "Konstantin Zinchenko",
            "Alexey Mikhaylov",
            "Lioubouv Bourmistrova",
            "Anastassiya Berezovskaya"
        ],
        "summary": "Any of views is formally defined within description logics that were established as a family of logics for modeling complex hereditary structures and have a suitable expressive power. This paper considers the Application Development Environment (ADE) over generalized variable concepts that are used to build database applications involving the supporting views. The front-end user interacts the database via separate ADE access mechanism intermediated by view support. The variety of applications may be generated that communicate with different and distinct desktop databases in a data warehouse. The advanced techniques allows to involve remote or stored procedures retrieval and call.",
        "published": "2001-06-12T12:23:20Z",
        "link": "http://arxiv.org/abs/cs/0106029v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.DS",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Logic, Individuals and Concepts",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "This extended abstract gives a brief outline of the connections between the descriptions and variable concepts. Thus, the notion of a concept is extended to include both the syntax and semantics features. The evaluation map in use is parameterized by a kind of computational environment, the index, giving rise to indexed concepts. The concepts are inhabited into language by the descriptions from the higher order logic. In general the idea of object-as-functor should assist the designer to outline a programming tool in conceptual shell style.",
        "published": "2001-06-12T12:58:27Z",
        "link": "http://arxiv.org/abs/cs/0106030v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.DM",
            "cs.SE",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Complexity Results and Practical Algorithms for Logics in Knowledge   Representation",
        "authors": [
            "Stephan Tobies"
        ],
        "summary": "Description Logics (DLs) are used in knowledge-based systems to represent and reason about terminological knowledge of the application domain in a semantically well-defined manner. In this thesis, we establish a number of novel complexity results and give practical algorithms for expressive DLs that provide different forms of counting quantifiers.   We show that, in many cases, adding local counting in the form of qualifying number restrictions to DLs does not increase the complexity of the inference problems, even if binary coding of numbers in the input is assumed. On the other hand, we show that adding different forms of global counting restrictions to a logic may increase the complexity of the inference problems dramatically.   We provide exact complexity results and a practical, tableau based algorithm for the DL SHIQ, which forms the basis of the highly optimized DL system iFaCT.   Finally, we describe a tableau algorithm for the clique guarded fragment (CGF), which we hope will serve as the basis for an efficient implementation of a CGF reasoner.",
        "published": "2001-06-13T11:20:30Z",
        "link": "http://arxiv.org/abs/cs/0106031v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1;I.2.3; I.2.4; F.2.2"
        ]
    },
    {
        "title": "Solving equations in the relational algebra",
        "authors": [
            "Joachim Biskup",
            "Jan Paredaens",
            "Thomas Schwentick",
            "Jan Van den Bussche"
        ],
        "summary": "Enumerating all solutions of a relational algebra equation is a natural and powerful operation which, when added as a query language primitive to the nested relational algebra, yields a query language for nested relational databases, equivalent to the well-known powerset algebra. We study \\emph{sparse} equations, which are equations with at most polynomially many solutions. We look at their complexity, and compare their expressive power with that of similar notions in the powerset algebra.",
        "published": "2001-06-14T15:39:14Z",
        "link": "http://arxiv.org/abs/cs/0106034v2",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.2; H.2.3"
        ]
    },
    {
        "title": "Polymorphic type inference for the relational algebra",
        "authors": [
            "Jan Van den Bussche",
            "Emmanuel Waller"
        ],
        "summary": "We give a polymorphic account of the relational algebra. We introduce a formalism of ``type formulas'' specifically tuned for relational algebra expressions, and present an algorithm that computes the ``principal'' type for a given expression. The principal type of an expression is a formula that specifies, in a clear and concise manner, all assignments of types (sets of attributes) to relation names, under which a given relational algebra expression is well-typed, as well as the output type that expression will have under each of these assignments. Topics discussed include complexity and polymorphic expressive power.",
        "published": "2001-06-14T16:19:30Z",
        "link": "http://arxiv.org/abs/cs/0106035v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "D.3.3, H.2.3"
        ]
    },
    {
        "title": "MACE 2.0 Reference Manual and Guide",
        "authors": [
            "William McCune"
        ],
        "summary": "MACE is a program that searches for finite models of first-order statements. The statement to be modeled is first translated to clauses, then to relational clauses; finally for the given domain size, the ground instances are constructed. A Davis-Putnam-Loveland-Logeman procedure decides the propositional problem, and any models found are translated to first-order models. MACE is a useful complement to the theorem prover Otter, with Otter searching for proofs and MACE looking for countermodels.",
        "published": "2001-06-19T16:11:19Z",
        "link": "http://arxiv.org/abs/cs/0106042v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "I.2.3; I.2.8"
        ]
    },
    {
        "title": "Expressing the cone radius in the relational calculus with real   polynomial constraints",
        "authors": [
            "Floris Geerts"
        ],
        "summary": "We show that there is a query expressible in first-order logic over the reals that returns, on any given semi-algebraic set A, for every point a radius around which A is conical. We obtain this result by combining famous results from calculus and real algebraic geometry, notably Sard's theorem and Thom's first isotopy lemma, with recent algorithmic results by Rannou.",
        "published": "2001-06-21T16:33:18Z",
        "link": "http://arxiv.org/abs/cs/0106046v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3; H.2.8"
        ]
    },
    {
        "title": "Classes of Terminating Logic Programs",
        "authors": [
            "Dino Pedreschi",
            "Salvatore Ruggieri",
            "Jan-Georg Smaus"
        ],
        "summary": "Termination of logic programs depends critically on the selection rule, i.e. the rule that determines which atom is selected in each resolution step. In this article, we classify programs (and queries) according to the selection rules for which they terminate. This is a survey and unified view on different approaches in the literature. For each class, we present a sufficient, for most classes even necessary, criterion for determining that a program is in that class. We study six classes: a program strongly terminates if it terminates for all selection rules; a program input terminates if it terminates for selection rules which only select atoms that are sufficiently instantiated in their input positions, so that these arguments do not get instantiated any further by the unification; a program local delay terminates if it terminates for local selection rules which only select atoms that are bounded w.r.t. an appropriate level mapping; a program left-terminates if it terminates for the usual left-to-right selection rule; a program exists-terminates if there exists a selection rule for which it terminates; finally, a program has bounded nondeterminism if it only has finitely many refutations. We propose a semantics-preserving transformation from programs with bounded nondeterminism into strongly terminating programs. Moreover, by unifying different formalisms and making appropriate assumptions, we are able to establish a formal hierarchy between the different classes.",
        "published": "2001-06-25T16:32:52Z",
        "link": "http://arxiv.org/abs/cs/0106050v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; D.2.4; F.3.1"
        ]
    },
    {
        "title": "Acceptability with general orderings",
        "authors": [
            "Danny De Schreye",
            "Alexander Serebrenik"
        ],
        "summary": "We present a new approach to termination analysis of logic programs. The essence of the approach is that we make use of general orderings (instead of level mappings), like it is done in transformational approaches to logic program termination analysis, but we apply these orderings directly to the logic program and not to the term-rewrite system obtained through some transformation. We define some variants of acceptability, based on general orderings, and show how they are equivalent to LD-termination. We develop a demand driven, constraint-based approach to verify these acceptability-variants.   The advantage of the approach over standard acceptability is that in some cases, where complex level mappings are needed, fairly simple orderings may be easily generated. The advantage over transformational approaches is that it avoids the transformation step all together.   {\\bf Keywords:} termination analysis, acceptability, orderings.",
        "published": "2001-06-26T12:34:24Z",
        "link": "http://arxiv.org/abs/cs/0106052v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "Inference of termination conditions for numerical loops",
        "authors": [
            "Alexander Serebrenik",
            "Danny De Schreye"
        ],
        "summary": "We present a new approach to termination analysis of numerical computations in logic programs. Traditional approaches fail to analyse them due to non well-foundedness of the integers. We present a technique that allows to overcome these difficulties. Our approach is based on transforming a program in way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query-mapping pairs with the well-known framework of acceptability. Such an integration not only contributes to the understanding of termination behaviour of numerical computations, but also allows to perform a correct analysis of such computations automatically, thus, extending previous work on a constraints-based approach to termination. In the last section of the paper we discuss possible extensions of the technique, including incorporating general term orderings.",
        "published": "2001-06-26T12:42:55Z",
        "link": "http://arxiv.org/abs/cs/0106053v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "Transformations of CCP programs",
        "authors": [
            "Sandro Etalle",
            "Maurizio Gabbrielli",
            "Maria Chiara Meo"
        ],
        "summary": "We introduce a transformation system for concurrent constraint programming (CCP). We define suitable applicability conditions for the transformations which guarantee that the input/output CCP semantics is preserved also when distinguishing deadlocked computations from successful ones and when considering intermediate results of (possibly) non-terminating computations.   The system allows us to optimize CCP programs while preserving their intended meaning: In addition to the usual benefits that one has for sequential declarative languages, the transformation of concurrent programs can also lead to the elimination of communication channels and of synchronization points, to the transformation of non-deterministic computations into deterministic ones, and to the crucial saving of computational space. Furthermore, since the transformation system preserves the deadlock behavior of programs, it can be used for proving deadlock freeness of a given program wrt a class of queries. To this aim it is sometimes sufficient to apply our transformations and to specialize the resulting program wrt the given queries in such a way that the obtained program is trivially deadlock free.",
        "published": "2001-07-10T13:32:17Z",
        "link": "http://arxiv.org/abs/cs/0107014v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO",
            "I.2.2; D.1.3; D.3.2"
        ]
    },
    {
        "title": "An interactive semantics of logic programming",
        "authors": [
            "Roberto Bruni",
            "Ugo Montanari",
            "Francesca Rossi"
        ],
        "summary": "We apply to logic programming some recently emerging ideas from the field of reduction-based communicating systems, with the aim of giving evidence of the hidden interactions and the coordination mechanisms that rule the operational machinery of such a programming paradigm. The semantic framework we have chosen for presenting our results is tile logic, which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results. As main contributions, we mention the finitary presentation of abstract unification, and a concurrent and coordinated abstract semantics consistent with the most common semantics of logic programming. Moreover, the compositionality of the tile semantics is guaranteed by standard results, as it reduces to check that the tile systems associated to logic programs enjoy the tile decomposition property. An extension of the approach for handling constraint systems is also discussed.",
        "published": "2001-07-17T16:43:24Z",
        "link": "http://arxiv.org/abs/cs/0107022v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; D.3.2; D.3.3; F.3.2"
        ]
    },
    {
        "title": "Annotated revision programs",
        "authors": [
            "Victor Marek",
            "Inna Pivkina",
            "Miroslaw Truszczynski"
        ],
        "summary": "Revision programming is a formalism to describe and enforce updates of belief sets and databases. That formalism was extended by Fitting who assigned annotations to revision atoms. Annotations provide a way to quantify the confidence (probability) that a revision atom holds. The main goal of our paper is to reexamine the work of Fitting, argue that his semantics does not always provide results consistent with intuition, and to propose an alternative treatment of annotated revision programs. Our approach differs from that proposed by Fitting in two key aspects: we change the notion of a model of a program and we change the notion of a justified revision. We show that under this new approach fundamental properties of justified revisions of standard revision programs extend to the annotated case.",
        "published": "2001-07-19T15:41:36Z",
        "link": "http://arxiv.org/abs/cs/0107026v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4;I.2.3"
        ]
    },
    {
        "title": "Fixed-parameter complexity of semantics for logic programs",
        "authors": [
            "Zbigniew Lonc",
            "Miroslaw Truszczynski"
        ],
        "summary": "A decision problem is called parameterized if its input is a pair of strings. One of these strings is referred to as a parameter. The problem: given a propositional logic program P and a non-negative integer k, decide whether P has a stable model of size no more than k, is an example of a parameterized decision problem with k serving as a parameter. Parameterized problems that are NP-complete often become solvable in polynomial time if the parameter is fixed. The problem to decide whether a program P has a stable model of size no more than k, where k is fixed and not a part of input, can be solved in time O(mn^k), where m is the size of P and n is the number of atoms in P. Thus, this problem is in the class P. However, algorithms with the running time given by a polynomial of order k are not satisfactory even for relatively small values of k.   The key question then is whether significantly better algorithms (with the degree of the polynomial not dependent on k) exist. To tackle it, we use the framework of fixed-parameter complexity. We establish the fixed-parameter complexity for several parameterized decision problems involving models, supported models and stable models of logic programs. We also establish the fixed-parameter complexity for variants of these problems resulting from restricting attention to Horn programs and to purely negative programs. Most of the problems considered in the paper have high fixed-parameter complexity. Thus, it is unlikely that fixing bounds on models (supported models, stable models) will lead to fast algorithms to decide the existence of such models.",
        "published": "2001-07-19T15:52:54Z",
        "link": "http://arxiv.org/abs/cs/0107027v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.1.3;I.2.4;D.1.6"
        ]
    },
    {
        "title": "Propositional satisfiability in answer-set programming",
        "authors": [
            "Deborah East",
            "Miroslaw Truszczynski"
        ],
        "summary": "We show that propositional logic and its extensions can support answer-set programming in the same way stable logic programming and disjunctive logic programming do. To this end, we introduce a logic based on the logic of propositional schemata and on a version of the Closed World Assumption. We call it the extended logic of propositional schemata with CWA (PS+, in symbols). An important feature of this logic is that it supports explicit modeling of constraints on cardinalities of sets. In the paper, we characterize the class of problems that can be solved by finite PS+ theories. We implement a programming system based on the logic PS+ and design and implement a solver for processing theories in PS+. We present encouraging performance results for our approach --- we show it to be competitive with smodels, a state-of-the-art answer-set programming system based on stable logic programming.",
        "published": "2001-07-19T17:53:11Z",
        "link": "http://arxiv.org/abs/cs/0107028v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1;D.1.6;I.2.4"
        ]
    },
    {
        "title": "aspps --- an implementation of answer-set programming with propositional   schemata",
        "authors": [
            "Deborah East. Miroslaw Truszczynski"
        ],
        "summary": "We present an implementation of an answer-set programming paradigm, called aspps (short for answer-set programming with propositional schemata). The system aspps is designed to process PS+ theories. It consists of two basic modules. The first module, psgrnd, grounds an PS+ theory. The second module, referred to as aspps, is a solver. It computes models of ground PS+ theories.",
        "published": "2001-07-19T18:02:50Z",
        "link": "http://arxiv.org/abs/cs/0107029v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "F.4.1;I.2.4;D.I.6"
        ]
    },
    {
        "title": "Abstract versus Concrete Computation on Metric Partial Algebras",
        "authors": [
            "J. V. Tucker",
            "J. I. Zucker"
        ],
        "summary": "A model of computation is abstract if, when applied to any algebra, the resulting programs for computable functions and sets on that algebra are invariant under isomorphisms, and hence do not depend on a representation for the algebra. Otherwise it is concrete. Intuitively, concrete models depend on the implementation of the algebra.   The difference is particularly striking in the case of topological partial algebras, and notably in algebras over the reals. We investigate the relationship between abstract and concrete models of partial metric algebras. In the course of this investigation, interesting aspects of continuity, extensionality and non-determinism are uncovered.",
        "published": "2001-08-12T21:25:13Z",
        "link": "http://arxiv.org/abs/cs/0108007v1",
        "categories": [
            "cs.LO",
            "F.1.1;F.4.1"
        ]
    },
    {
        "title": "Convergent Approximate Solving of First-Order Constraints by Approximate   Quantifiers",
        "authors": [
            "Stefan Ratschan"
        ],
        "summary": "Exactly solving first-order constraints (i.e., first-order formulas over a certain predefined structure) can be a very hard, or even undecidable problem. In continuous structures like the real numbers it is promising to compute approximate solutions instead of exact ones. However, the quantifiers of the first-order predicate language are an obstacle to allowing approximations to arbitrary small error bounds. In this paper we solve the problem by modifying the first-order language and replacing the classical quantifiers with approximate quantifiers. These also have two additional advantages: First, they are tunable, in the sense that they allow the user to decide on the trade-off between precision and efficiency. Second, they introduce additional expressivity into the first-order language by allowing reasoning over the size of solution sets.",
        "published": "2001-08-22T20:18:51Z",
        "link": "http://arxiv.org/abs/cs/0108013v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "F.4.1; I.2.3"
        ]
    },
    {
        "title": "Abstract Computability, Algebraic Specification and Initiality",
        "authors": [
            "J. V. Tucker",
            "J. I. Zucker"
        ],
        "summary": "computable functions are defined by abstract finite deterministic algorithms on many-sorted algebras. We show that there exist finite universal algebraic specifications that specify uniquely (up to isomorphism) (i) all abstract computable functions on any many-sorted algebra; and (ii) all functions effectively approximable by abstract computable functions on any metric algebra.   We show that there exist universal algebraic specifications for all the classically computable functions on the set R of real numbers. The algebraic specifications used are mainly bounded universal equations and conditional equations. We investigate the initial algebra semantics of these specifications, and derive situations where algebraic specifications define precisely the computable functions.",
        "published": "2001-09-02T18:52:51Z",
        "link": "http://arxiv.org/abs/cs/0109001v1",
        "categories": [
            "cs.LO",
            "F.1.1;F.3.1;F.4.1"
        ]
    },
    {
        "title": "Higher-Order Pattern Complement and the Strict Lambda-Calculus",
        "authors": [
            "Alberto Momigliano",
            "Frank Pfenning"
        ],
        "summary": "We address the problem of complementing higher-order patterns without repetitions of existential variables. Differently from the first-order case, the complement of a pattern cannot, in general, be described by a pattern, or even by a finite set of patterns. We therefore generalize the simply-typed lambda-calculus to include an internal notion of strict function so that we can directly express that a term must depend on a given variable. We show that, in this more expressive calculus, finite sets of patterns without repeated variables are closed under complement and intersection. Our principal application is the transformational approach to negation in higher-order logic programs.",
        "published": "2001-09-24T17:29:50Z",
        "link": "http://arxiv.org/abs/cs/0109072v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.3;D.1.6;F.4.1"
        ]
    },
    {
        "title": "The temporal calculus of conditional objects and conditional events",
        "authors": [
            "Jerzy Tyszkiewicz",
            "Arthur Ramer",
            "Achim Hoffmann"
        ],
        "summary": "We consider the problem of defining conditional objects (a|b), which would allow one to regard the conditional probability Pr(a|b) as a probability of a well-defined event rather than as a shorthand for Pr(ab)/Pr(b). The next issue is to define boolean combinations of conditional objects, and possibly also the operator of further conditioning. These questions have been investigated at least since the times of George Boole, leading to a number of formalisms proposed for conditional objects, mostly of syntactical, proof-theoretic vein.   We propose a unifying, semantical approach, in which conditional events are (projections of) Markov chains, definable in the three-valued extension of the past tense fragment of propositional linear time logic, or, equivalently, by three-valued counter-free Moore machines. Thus our conditional objects are indeed stochastic processes, one of the central notions of modern probability theory.   Our model fulfills early ideas of Bruno de Finetti and, moreover, as we show in a separate paper, all the previously proposed algebras of conditional events can be isomorphically embedded in our model.",
        "published": "2001-10-01T08:58:34Z",
        "link": "http://arxiv.org/abs/cs/0110003v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Embedding conditional event algebras into temporal calculus of   conditionals",
        "authors": [
            "Jerzy Tyszkiewicz",
            "Achim Hoffmann",
            "Arthur Ramer"
        ],
        "summary": "In this paper we prove that all the existing conditional event algebras embed into a three-valued extension of temporal logic of discrete past time, which the authors of this paper have proposed in anothe paper as a general model of conditional events.   First of all, we discuss the descriptive incompleteness of the cea's. In this direction, we show that some important notions, like independence of conditional events, cannot be properly addressed in the framework of conditional event algebras, while they can be precisely formulated and analyzed in the temporal setting.   We also demonstrate that the embeddings allow one to use Markov chain algorithms (suitable for the temporal calculus) for computing probabilities of complex conditional expressions of the embedded conditional event algebras, and that these algorithms can outperform those previously known.",
        "published": "2001-10-01T09:21:26Z",
        "link": "http://arxiv.org/abs/cs/0110004v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.3;I.2.4;F.4.1"
        ]
    },
    {
        "title": "Pushdown Timed Automata: a Binary Reachability Characterization and   Safety Verification",
        "authors": [
            "Zhe Dang"
        ],
        "summary": "We consider pushdown timed automata (PTAs) that are timed automata (with dense clocks) augmented with a pushdown stack. A configuration of a PTA includes a control state, dense clock values and a stack word. By using the pattern technique, we give a decidable characterization of the binary reachability (i.e., the set of all pairs of configurations such that one can reach the other) of a PTA. Since a timed automaton can be treated as a PTA without the pushdown stack, we can show that the binary reachability of a timed automaton is definable in the additive theory of reals and integers. The results can be used to verify a class of properties containing linear relations over both dense variables and unbounded discrete variables. The properties previously could not be verified using the classic region technique nor expressed by timed temporal logics for timed automata and CTL$^*$ for pushdown systems. The results are also extended to other generalizations of timed automata.",
        "published": "2001-10-02T22:47:41Z",
        "link": "http://arxiv.org/abs/cs/0110010v1",
        "categories": [
            "cs.LO",
            "D.2.4;F.1.1"
        ]
    },
    {
        "title": "A Proposal for Dynamic Access Lists for TCP/IP Packet Filering",
        "authors": [
            "Scott Hazelhurst"
        ],
        "summary": "The use of IP filtering to improve system security is well established, and although limited in what it can achieve has proved to be efficient and effective.   In the design of a security policy there is always a trade-off between usability and security. Restricting access means that legitimate use of the network is prevented; allowing access means illegitimate use may be allowed. Static access list make finding a balance particularly stark -- we pay the price of decreased security 100% of the time even if the benefit of increased usability is only gained 1% of the time.   Dynamic access lists would allow the rules to change for short periods of time, and to allow local changes by non-experts. The network administrator can set basic security guide-lines which allow certain basic services only. All other services are restricted, but users are able to request temporary exceptions in order to allow additional access to the network. These exceptions are granted depending on the privileges of the user.   This paper covers the following topics: (1) basic introduction to TCP/IP filtering; (2) semantics for dynamic access lists and; (3) a proposed protocol for allowing dynamic access; and (4) a method for representing access lists so that dynamic update and look-up can be done efficiently performed.",
        "published": "2001-10-03T09:24:17Z",
        "link": "http://arxiv.org/abs/cs/0110013v1",
        "categories": [
            "cs.NI",
            "cs.LO",
            "F2.2; F4.1; F4.2; I2.4"
        ]
    },
    {
        "title": "Set Unification",
        "authors": [
            "Agostino Dovier",
            "Enrico Pontelli",
            "Gianfranco Rossi"
        ],
        "summary": "The unification problem in algebras capable of describing sets has been tackled, directly or indirectly, by many researchers and it finds important applications in various research areas--e.g., deductive databases, theorem proving, static analysis, rapid software prototyping. The various solutions proposed are spread across a large literature. In this paper we provide a uniform presentation of unification of sets, formalizing it at the level of set theory. We address the problem of deciding existence of solutions at an abstract level. This provides also the ability to classify different types of set unification problems. Unification algorithms are uniformly proposed to solve the unification problem in each of such classes.   The algorithms presented are partly drawn from the literature--and properly revisited and analyzed--and partly novel proposals. In particular, we present a new goal-driven algorithm for general ACI1 unification and a new simpler algorithm for general (Ab)(Cl) unification.",
        "published": "2001-10-09T10:57:56Z",
        "link": "http://arxiv.org/abs/cs/0110023v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.SC",
            "F.2.2; F.4.1; I.1.2; I.2.3"
        ]
    },
    {
        "title": "On Equivalence and Canonical Forms in the LF Type Theory",
        "authors": [
            "Robert Harper",
            "Frank Pfenning"
        ],
        "summary": "Decidability of definitional equality and conversion of terms into canonical form play a central role in the meta-theory of a type-theoretic logical framework. Most studies of definitional equality are based on a confluent, strongly-normalizing notion of reduction. Coquand has considered a different approach, directly proving the correctness of a practical equivalance algorithm based on the shape of terms. Neither approach appears to scale well to richer languages with unit types or subtyping, and neither directly addresses the problem of conversion to canonical.   In this paper we present a new, type-directed equivalence algorithm for the LF type theory that overcomes the weaknesses of previous approaches. The algorithm is practical, scales to richer languages, and yields a new notion of canonical form sufficient for adequate encodings of logical systems. The algorithm is proved complete by a Kripke-style logical relations argument similar to that suggested by Coquand. Crucially, both the algorithm itself and the logical relations rely only on the shapes of types, ignoring dependencies on terms.",
        "published": "2001-10-11T16:36:18Z",
        "link": "http://arxiv.org/abs/cs/0110028v1",
        "categories": [
            "cs.LO",
            "F.4.1"
        ]
    },
    {
        "title": "Inference of termination conditions for numerical loops in Prolog",
        "authors": [
            "Alexander Serebrenik",
            "Danny De Schreye"
        ],
        "summary": "We present a new approach to termination analysis of numerical computations in logic programs. Traditional approaches fail to analyse them due to non well-foundedness of the integers. We present a technique that allows overcoming these difficulties. Our approach is based on transforming a program in a way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query-mapping pairs with the well-known framework of acceptability. Such an integration not only contributes to the understanding of termination behaviour of numerical computations, but also allows us to perform a correct analysis of such computations automatically, by extending previous work on a constraint-based approach to termination. Finally, we discuss possible extensions of the technique, including incorporating general term orderings.",
        "published": "2001-10-17T11:31:18Z",
        "link": "http://arxiv.org/abs/cs/0110034v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "On termination of meta-programs",
        "authors": [
            "Alexander Serebrenik",
            "Danny De Schreye"
        ],
        "summary": "The term {\\em meta-programming} refers to the ability of writing programs that have other programs as data and exploit their semantics.   The aim of this paper is presenting a methodology allowing us to perform a correct termination analysis for a broad class of practical meta-interpreters, including negation and performing different tasks during the execution. It is based on combining the power of general orderings, used in proving termination of term-rewrite systems and programs, and on the well-known acceptability condition, used in proving termination of logic programs.   The methodology establishes a relationship between the ordering needed to prove termination of the interpreted program and the ordering needed to prove termination of the meta-interpreter together with this interpreted program. If such a relationship is established, termination of one of those implies termination of the other one, i.e., the meta-interpreter preserves termination.   Among the meta-interpreters that are analysed correctly are a proof trees constructing meta-interpreter, different kinds of tracers and reasoners.   To appear without appendix in Theory and Practice of Logic Programming.",
        "published": "2001-10-17T11:37:46Z",
        "link": "http://arxiv.org/abs/cs/0110035v3",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "Quantum Algorithm for Hilbert's Tenth Problem",
        "authors": [
            "Tien D Kieu"
        ],
        "summary": "We explore in the framework of Quantum Computation the notion of {\\em Computability}, which holds a central position in Mathematics and Theoretical Computer Science. A quantum algorithm for Hilbert's tenth problem, which is equivalent to the Turing halting problem and is known to be mathematically noncomputable, is proposed where quantum continuous variables and quantum adiabatic evolution are employed. If this algorithm could be physically implemented, as much as it is valid in principle--that is, if certain hamiltonian and its ground state can be physically constructed according to the proposal--quantum computability would surpass classical computability as delimited by the Church-Turing thesis. It is thus argued that computability, and with it the limits of Mathematics, ought to be determined not solely by Mathematics itself but also by Physical Principles.",
        "published": "2001-10-24T06:33:33Z",
        "link": "http://arxiv.org/abs/quant-ph/0110136v3",
        "categories": [
            "quant-ph",
            "cs.LO",
            "hep-th",
            "math.LO",
            "math.NT"
        ]
    },
    {
        "title": "Selected Topics in Asynchronous Automata",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "The paper is concerned with defining the electrical signals and their models. The delays are discussed, the asynchronous automata - which are the models of the asynchronous circuits - and the examples of the clock generator and of the R-S latch are given. We write the equations of the asynchronous automata, which combine the pure delay model and the inertial delay model; the simple gate model and the complex gate model; the fixed, bounded and unbounded delay model. We give the solutions of these equations, which are written on R->{0,1} functions, where R is the time set. The connection between the real time and the discrete time is discussed. The stability, the fundamental mode of operation, the combinational automata, the semi-modularity are defined and characterized. Some connections are suggested with the linear time and the branching time temporal logic of the propositions.",
        "published": "2001-10-31T10:35:32Z",
        "link": "http://arxiv.org/abs/cs/0110060v1",
        "categories": [
            "cs.LO",
            "94C05; 94C10"
        ]
    },
    {
        "title": "An Asynchronous Automata Approach to the Semantics of Temporal Logic",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "The paper presents the differential equations that characterize an asynchronous automaton and gives their solution x:R->{0,1}x...x{0,1}. Remarks are made on the connection between the continuous time and the discrete time of the approach. The continuous and the discrete time, the linear and the branching temporal logics have the semantics depending on x and their formulas give the properties of the automaton.",
        "published": "2001-10-31T10:44:55Z",
        "link": "http://arxiv.org/abs/cs/0110061v1",
        "categories": [
            "cs.LO",
            "94C05; 94C10"
        ]
    },
    {
        "title": "The Delay-Insensitivity, the Hazard-Freedom, the Semi-Modularity and the   Technical Condition of Good Running of the Discrete Time Asynchronous   Automata",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "The paper studies some important properties of the asynchronous (=timed) automata: the delay-insensitivity, the hazard-freedom, the semi-modularity and the technical condition of good running. Time is discrete.",
        "published": "2001-10-31T10:53:34Z",
        "link": "http://arxiv.org/abs/cs/0110062v1",
        "categories": [
            "cs.LO",
            "94C05; 94C10"
        ]
    },
    {
        "title": "The Existence of $ω$-Chains for Transitive Mixed Linear Relations   and Its Applications",
        "authors": [
            "Zhe Dang",
            "Oscar Ibarra"
        ],
        "summary": "We show that it is decidable whether a transitive mixed linear relation has an $\\omega$-chain. Using this result, we study a number of liveness verification problems for generalized timed automata within a unified framework. More precisely, we prove that (1) the mixed linear liveness problem for a timed automaton with dense clocks, reversal-bounded counters, and a free counter is decidable, and (2) the Presburger liveness problem for a timed automaton with discrete clocks, reversal-bounded counters, and a pushdown stack is decidable.",
        "published": "2001-10-31T10:56:32Z",
        "link": "http://arxiv.org/abs/cs/0110063v2",
        "categories": [
            "cs.LO",
            "D.2.4; F.1.1"
        ]
    },
    {
        "title": "Applications of the Differential Calculus in the Study of the Timed   Automata: the Inertial Delay Buffer",
        "authors": [
            "Serban E. Vlad"
        ],
        "summary": "We write the relations that characterize the simpliest timed automaton, the inertial delay buffer, in two versions: the non-deterministic and the deterministic one, by making use of the derivatives of the R->{0,1} functions.",
        "published": "2001-10-31T10:59:16Z",
        "link": "http://arxiv.org/abs/cs/0110064v1",
        "categories": [
            "cs.LO",
            "94C05; 94C10"
        ]
    },
    {
        "title": "Abduction with Penalization in Logic Programming",
        "authors": [
            "Giovambattista Ianni",
            "Nicola Leone",
            "Simona Perri",
            "Francesco Scarcello"
        ],
        "summary": "Abduction, first proposed in the setting of classical logics, has been studied with growing interest in the logic programming area during the last years.   In this paper we study {\\em abduction with penalization} in logic programming. This form of abductive reasoning, which has not been previously analyzed in logic programming, turns out to represent several relevant problems, including optimization problems, very naturally. We define a formal model for abduction with penalization from logic programs, which extends the abductive framework proposed by Kakas and Mancarella. We show the high expressiveness of this formalism, by encoding a couple of relevant problems, including the well-know Traveling Salesman Problem from optimization theory, in this abductive framework. The resulting encodings are very simple and elegant. We analyze the complexity of the main decisional problems arising in this framework. An interesting result in this course is that ``negation comes for free.'' Indeed, the addition of (even unstratified) negation does not cause any further increase to the complexity of the abductive reasoning tasks (which remains the same as for not-free programs).",
        "published": "2001-11-06T16:33:29Z",
        "link": "http://arxiv.org/abs/cs/0111010v1",
        "categories": [
            "cs.LO",
            "D.1.6"
        ]
    },
    {
        "title": "Sintesi di algoritmi con SKY",
        "authors": [
            "Giovambattista Ianni"
        ],
        "summary": "This paper describes the semantics and ideas about SKY, a logic programming language intended in order to specify algorithmic strategies for the evaluation of problems.",
        "published": "2001-11-06T17:02:25Z",
        "link": "http://arxiv.org/abs/cs/0111011v1",
        "categories": [
            "cs.LO",
            "D.1.6"
        ]
    },
    {
        "title": "An Environment for the Exploration of Non Monotonic Logic Programs",
        "authors": [
            "Luis F. Castro",
            "David S. Warren"
        ],
        "summary": "Stable Model Semantics and Well Founded Semantics have been shown to be very useful in several applications of non-monotonic reasoning. However, Stable Models presents a high computational complexity, whereas Well Founded Semantics is easy to compute and provides an approximation of Stable Models. Efficient engines exist for both semantics of logic programs. This work presents a computational integration of two of such systems, namely XSB and SMODELS. The resulting system is called XNMR, and provides an interactive system for the exploration of both semantics. Aspects such as modularity can be exploited in order to ease debugging of large knowledge bases with the usual Prolog debugging techniques and an interactive environment. Besides, the use of a full Prolog system as a front-end to a Stable Models engine augments the language usually accepted by such systems.",
        "published": "2001-11-19T16:29:35Z",
        "link": "http://arxiv.org/abs/cs/0111049v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.6"
        ]
    },
    {
        "title": "Towards a characterization of the star-free sets of integers",
        "authors": [
            "Michel Rigo"
        ],
        "summary": "Let U be a numeration system, a set X of integers is U-star-free if the set made up of the U-representations of the elements in X is a star-free regular language. Answering a question of A. de Luca and A. Restivo, we obtain a complete logical characterization of the U-star-free sets of integers for suitable numeration systems related to a Pisot number and in particular for integer base systems. For these latter systems, we study as well the problem of the base dependence. Finally, the case of k-adic systems is also investigated.",
        "published": "2001-11-23T10:18:32Z",
        "link": "http://arxiv.org/abs/cs/0111057v1",
        "categories": [
            "cs.CC",
            "cs.LO",
            "F.1.1;F.4.1;F.4.3"
        ]
    },
    {
        "title": "Bayesian Logic Programs",
        "authors": [
            "Kristian Kersting",
            "Luc De Raedt"
        ],
        "summary": "Bayesian networks provide an elegant formalism for representing and reasoning about uncertainty using probability theory. Theyare a probabilistic extension of propositional logic and, hence, inherit some of the limitations of propositional logic, such as the difficulties to represent objects and relations. We introduce a generalization of Bayesian networks, called Bayesian logic programs, to overcome these limitations. In order to represent objects and relations it combines Bayesian networks with definite clause logic by establishing a one-to-one mapping between ground atoms and random variables. We show that Bayesian logic programs combine the advantages of both definite clause logic and Bayesian networks. This includes the separation of quantitative and qualitative aspects of the model. Furthermore, Bayesian logic programs generalize both Bayesian networks as well as logic programs. So, many ideas developed",
        "published": "2001-11-23T20:59:09Z",
        "link": "http://arxiv.org/abs/cs/0111058v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2; I.2.3; I.2.4; G.3; F.3.2; F.4.1"
        ]
    },
    {
        "title": "Hypotheses Founded Semantics of Logic Programs for Information   Integration in Multi-Valued Logics",
        "authors": [
            "Yann Loyer",
            "Nicolas Spyratos",
            "Daniel Stamate"
        ],
        "summary": "We address the problem of integrating information coming from different sources. The information consists of facts that a central server collects and tries to combine using (a) a set of logical rules, i.e. a logic program, and (b) a hypothesis representing the server's own estimates. In such a setting incomplete information from a source or contradictory information from different sources necessitate the use of many-valued logics in which programs can be evaluated and hypotheses can be tested. To carry out such activities we propose a formal framework based on bilattices such as Belnap's four-valued logics. In this framework we work with the class of programs defined by Fitting and we develop a theory for information integration.   We also establish an intuitively appealing connection between our hypothesis testing mechanism on the one hand, and the well-founded semantics and Kripke-Kleene semantics of Datalog programs with negation, on the other hand.",
        "published": "2001-11-27T17:43:20Z",
        "link": "http://arxiv.org/abs/cs/0111059v1",
        "categories": [
            "cs.LO",
            "I.2.3; I.2.11; H.1.1"
        ]
    },
    {
        "title": "Program schemes with binary write-once arrays and the complexity classes   they capture",
        "authors": [
            "Iain A. Stewart"
        ],
        "summary": "We study a class of program schemes, NPSB, in which, aside from basic assignments, non-deterministic guessing and while loops, we have access to arrays; but where these arrays are binary write-once in that they are initialized to `zero' and can only ever be set to `one'. We show, amongst other results, that: NPSB can be realized as a vectorized Lindstrom logic; there are problems accepted by program schemes of NPSB that are not definable in the bounded-variable infinitary logic ${\\cal L}^\\omega_{\\infty\\omega}$; all problems accepted by the program schemes of NPSB have a zero-one law; and on ordered structures, NPSB captures the complexity class $[ L]^[{\\scriptsize NP\\normalsize}]$. The class of program schemes NPSB is actually the union of an infinite hierarchy of classes of program schemes. When we amend the semantics of our program schemes slightly, we find that the classes of the resulting hierarchy capture the complexity classes $\\Sigma^p_i$ (where $i\\geq 1$) of the Polynomial Hierarchy PH. Finally, we give logical equivalences of the complexity-theoretic question `Does NP equal PSPACE?' where the logics (and classes of program schemes) involved define only problems with zero-one laws (and so do not define some computationally trivial problems).",
        "published": "2001-12-03T15:15:17Z",
        "link": "http://arxiv.org/abs/cs/0112002v1",
        "categories": [
            "cs.LO",
            "cs.CC",
            "F.4.1; F.1.3; F.1.1"
        ]
    },
    {
        "title": "A Logic Programming Approach to Knowledge-State Planning: Semantics and   Complexity",
        "authors": [
            "Thomas Eiter",
            "Wolfgang Faber",
            "Nicola Leone",
            "Gerald Pfeifer",
            "Axel Polleres"
        ],
        "summary": "We propose a new declarative planning language, called K, which is based on principles and methods of logic programming. In this language, transitions between states of knowledge can be described, rather than transitions between completely described states of the world, which makes the language well-suited for planning under incomplete knowledge. Furthermore, it enables the use of default principles in the planning process by supporting negation as failure. Nonetheless, K also supports the representation of transitions between states of the world (i.e., states of complete knowledge) as a special case, which shows that the language is very flexible. As we demonstrate on particular examples, the use of knowledge states may allow for a natural and compact problem representation. We then provide a thorough analysis of the computational complexity of K, and consider different planning problems, including standard planning and secure planning (also known as conformant planning) problems. We show that these problems have different complexities under various restrictions, ranging from NP to NEXPTIME in the propositional case. Our results form the theoretical basis for the DLV^K system, which implements the language K on top of the DLV logic programming system.",
        "published": "2001-12-05T12:41:48Z",
        "link": "http://arxiv.org/abs/cs/0112006v1",
        "categories": [
            "cs.AI",
            "cs.LO",
            "I.2.4; I.2.8; I.2.3"
        ]
    },
    {
        "title": "Concurrent computing machines and physical space-time",
        "authors": [
            "Philippe Matherat",
            "Marc-Thierry Jaekel"
        ],
        "summary": "Concrete computing machines, either sequential or concurrent, rely on an intimate relation between computation and time. We recall the general characteristic properties of physical time and of present realizations of computing systems. We emphasize the role of computing interferences, i.e. the necessity to avoid them in order to give a causal implementation to logical operations. We compare synchronous and asynchronous systems, and make a brief survey of some methods used to deal with computing interferences. Using a graphic representation, we show that synchronous and asynchronous circuits reflect the same opposition as the Newtonian and relativistic causal structures for physical space-time.",
        "published": "2001-12-17T14:10:32Z",
        "link": "http://arxiv.org/abs/cs/0112020v1",
        "categories": [
            "cs.DC",
            "cs.LO",
            "gr-qc",
            "F.1.2;B.6.1;D.3.3"
        ]
    },
    {
        "title": "A Price Dynamics in Bandwidth Markets for Point-to-point Connections",
        "authors": [
            "Lars Rasmusson",
            "Erik Aurell"
        ],
        "summary": "We simulate a network of N routers and M network users making concurrent point-to-point connections by buying and selling router capacity from each other. The resources need to be acquired in complete sets, but there is only one spot market for each router. In order to describe the internal dynamics of the market, we model the observed prices by N-dimensional Ito-processes. Modeling using stochastic processes is novel in this context of describing interactions between end-users in a system with shared resources, and allows a standard set of mathematical tools to be applied. The derived models can also be used to price contingent claims on network capacity and thus to price complex network services such as quality of service levels, multicast, etc.",
        "published": "2001-02-15T10:44:06Z",
        "link": "http://arxiv.org/abs/cs/0102011v1",
        "categories": [
            "cs.NI",
            "cond-mat.soft",
            "cs.MA",
            "C.2.3; C.4"
        ]
    },
    {
        "title": "Market-Based Reinforcement Learning in Partially Observable Worlds",
        "authors": [
            "Ivo Kwee",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ],
        "summary": "Unlike traditional reinforcement learning (RL), market-based RL is in principle applicable to worlds described by partially observable Markov Decision Processes (POMDPs), where an agent needs to learn short-term memories of relevant previous events in order to execute optimal actions. Most previous work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here we reimplement a recent approach to market-based RL and for the first time evaluate it in a toy POMDP setting.",
        "published": "2001-05-15T19:07:28Z",
        "link": "http://arxiv.org/abs/cs/0105025v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MA",
            "cs.NE",
            "I.2"
        ]
    },
    {
        "title": "Learning to Cooperate via Policy Search",
        "authors": [
            "Leonid Peshkin",
            "Kee-Eung Kim",
            "Nicolas Meuleau",
            "Leslie Pack Kaelbling"
        ],
        "summary": "Cooperative games are those in which both agents share the same payoff structure. Value-based reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain.",
        "published": "2001-05-25T02:52:07Z",
        "link": "http://arxiv.org/abs/cs/0105032v1",
        "categories": [
            "cs.LG",
            "cs.MA",
            "I.2;I.2.9;I.2.11"
        ]
    },
    {
        "title": "Software Toolkit for Building Embedded and Distributed Knowledge-based   Systems",
        "authors": [
            "Dmitri Soshnikov"
        ],
        "summary": "The paper discusses the basic principles and the architecture of the software toolkit for constructing knowledge-based systems which can be used cooperatively over computer networks and also embedded into larger software systems in different ways. Presented architecture is based on frame knowledge representation and production rules, which also allows to interface high-level programming languages and relational databases by exposing corresponding classes or database tables as frames. Frames located on the remote computers can also be transparently accessed and used in inference, and the dynamic knowledge for specific frames can also be transferred over the network. The issues of implementation of such a system are addressed, which use Java programming language, CORBA and XML for external knowledge representation. Finally, some applications of the toolkit are considered, including e-business approach to knowledge sharing, intelligent web behaviours, etc.",
        "published": "2001-06-26T19:50:54Z",
        "link": "http://arxiv.org/abs/cs/0106054v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "I.2; D.2.12"
        ]
    },
    {
        "title": "Using Methods of Declarative Logic Programming for Intelligent   Information Agents",
        "authors": [
            "T. Eiter",
            "M. Fink",
            "G. Sabbatini",
            "H. Tompits"
        ],
        "summary": "The search for information on the web is faced with several problems, which arise on the one hand from the vast number of available sources, and on the other hand from their heterogeneity. A promising approach is the use of multi-agent systems of information agents, which cooperatively solve advanced information-retrieval problems. This requires capabilities to address complex tasks, such as search and assessment of sources, query planning, information merging and fusion, dealing with incomplete information, and handling of inconsistency. In this paper, our interest is in the role which some methods from the field of declarative logic programming can play in the realization of reasoning capabilities for information agents. In particular, we are interested in how they can be used and further developed for the specific needs of this application domain. We review some existing systems and current projects, which address information-integration problems. We then focus on declarative knowledge-representation methods, and review and evaluate approaches from logic programming and nonmonotonic reasoning for information agents. We discuss advantages and drawbacks, and point out possible extensions and open issues.",
        "published": "2001-08-14T14:51:23Z",
        "link": "http://arxiv.org/abs/cs/0108008v1",
        "categories": [
            "cs.MA",
            "cs.AI",
            "I.2"
        ]
    },
    {
        "title": "Coordination of Decisions in a Spatial Agent Model",
        "authors": [
            "Frank Schweitzer",
            "Joerg Zimmermann",
            "Heinz Muehlenbein"
        ],
        "summary": "For a binary choice problem, the spatial coordination of decisions in an agent community is investigated both analytically and by means of stochastic computer simulations. The individual decisions are based on different local information generated by the agents with a finite lifetime and disseminated in the system with a finite velocity. We derive critical parameters for the emergence of minorities and majorities of agents making opposite decisions and investigate their spatial organization. We find that dependent on two essential parameters describing the local impact and the spatial dissemination of information, either a definite stable minority/majority relation (single-attractor regime) or a broad range of possible values (multi-attractor regime) occurs. In the latter case, the outcome of the decision process becomes rather diverse and hard to predict, both with respect to the share of the majority and their spatial distribution. We further investigate how a dissemination of information on different time scales affects the outcome of the decision process. We find that a more ``efficient'' information exchange within a subpopulation provides a suitable way to stabilize their majority status and to reduce ``diversity'' and uncertainty in the decision process.",
        "published": "2001-09-06T17:44:36Z",
        "link": "http://arxiv.org/abs/cond-mat/0109121v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.MA"
        ]
    },
    {
        "title": "Classification of Finite Dynamical Systems",
        "authors": [
            "Luis Garcia",
            "Abdul Salam Jarrah",
            "Reinhard Laubenbacher"
        ],
        "summary": "This paper is motivated by the theory of sequential dynamical systems, developed as a basis for a mathematical theory of computer simulation. It contains a classification of finite dynamical systems on binary strings, which are obtained by composing functions defined on the coordinates. The classification is in terms of the dependency relations among the coordinate functions. It suggests a natural notion of the linearization of a system. Furthermore, it contains a sharp upper bound on the number of systems in terms of the dependencies among the coordinate functions. This upper bound generalizes an upper bound for sequential dynamical systems.",
        "published": "2001-12-20T06:01:13Z",
        "link": "http://arxiv.org/abs/math/0112216v1",
        "categories": [
            "math.DS",
            "cs.MA",
            "math.CO"
        ]
    },
    {
        "title": "Easy and Hard Constraint Ranking in OT: Algorithms and Complexity",
        "authors": [
            "Jason Eisner"
        ],
        "summary": "We consider the problem of ranking a set of OT constraints in a manner consistent with data.   We speed up Tesar and Smolensky's RCD algorithm to be linear on the number of constraints. This finds a ranking so each attested form x_i beats or ties a particular competitor y_i. We also generalize RCD so each x_i beats or ties all possible competitors.   Alas, this more realistic version of learning has no polynomial algorithm unless P=NP! Indeed, not even generation does. So one cannot improve qualitatively upon brute force:   Merely checking that a single (given) ranking is consistent with given forms is coNP-complete if the surface forms are fully observed and Delta_2^p-complete if not. Indeed, OT generation is OptP-complete. As for ranking, determining whether any consistent ranking exists is coNP-hard (but in Delta_2^p) if the forms are fully observed, and Sigma_2^p-complete if not.   Finally, we show that generation and ranking are easier in derivational theories: in P, and NP-complete.",
        "published": "2001-02-22T03:50:56Z",
        "link": "http://arxiv.org/abs/cs/0102019v1",
        "categories": [
            "cs.CL",
            "cs.CC",
            "I.2.7; F.2.2"
        ]
    },
    {
        "title": "Taking Primitive Optimality Theory Beyond the Finite State",
        "authors": [
            "Daniel Albro"
        ],
        "summary": "Primitive Optimality Theory (OTP) (Eisner, 1997a; Albro, 1998), a computational model of Optimality Theory (Prince and Smolensky, 1993), employs a finite state machine to represent the set of active candidates at each stage of an Optimality Theoretic derivation, as well as weighted finite state machines to represent the constraints themselves. For some purposes, however, it would be convenient if the set of candidates were limited by some set of criteria capable of being described only in a higher-level grammar formalism, such as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple Context Free Grammar (Seki et al., 1991). Examples include reduplication and phrasal stress models. Here we introduce a mechanism for OTP-like Optimality Theory in which the constraints remain weighted finite state machines, but sets of candidates are represented by higher-level grammars. In particular, we use multiple context-free grammars to model reduplication in the manner of Correspondence Theory (McCarthy and Prince, 1995), and develop an extended version of the Earley Algorithm (Earley, 1970) to apply the constraints to a reduplicating candidate set.",
        "published": "2001-02-22T13:09:25Z",
        "link": "http://arxiv.org/abs/cs/0102021v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Finite-State Phonology: Proceedings of the 5th Workshop of the ACL   Special Interest Group in Computational Phonology (SIGPHON)",
        "authors": [
            "Jason Eisner",
            "Lauri Karttunen",
            "Alain Theriault"
        ],
        "summary": "Home page of the workshop proceedings, with pointers to the individually archived papers. Includes front matter from the printed version of the proceedings.",
        "published": "2001-02-22T14:10:20Z",
        "link": "http://arxiv.org/abs/cs/0102022v2",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Multi-Syllable Phonotactic Modelling",
        "authors": [
            "Anja Belz"
        ],
        "summary": "This paper describes a novel approach to constructing phonotactic models. The underlying theoretical approach to phonological description is the multisyllable approach in which multiple syllable classes are defined that reflect phonotactically idiosyncratic syllable subcategories. A new finite-state formalism, OFS Modelling, is used as a tool for encoding, automatically constructing and generalising phonotactic descriptions. Language-independent prototype models are constructed which are instantiated on the basis of data sets of phonological strings, and generalised with a clustering algorithm. The resulting approach enables the automatic construction of phonotactic models that encode arbitrarily close approximations of a language's set of attested phonological forms. The approach is applied to the construction of multi-syllable word-level phonotactic models for German, English and Dutch.",
        "published": "2001-02-22T21:05:01Z",
        "link": "http://arxiv.org/abs/cs/0102020v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Mathematical Model of Word Length on the Basis of the Cebanov-Fucks   Distribution with Uniform Parameter Distribution",
        "authors": [
            "Victor Kromer"
        ],
        "summary": "The data on 13 typologically different languages have been processed using a two-parameter word length model, based on 1-displaced uniform Poisson distribution. Statistical dependencies of the 2nd parameter on the 1st one are revealed for the German texts and genre of letters.",
        "published": "2001-02-24T06:56:43Z",
        "link": "http://arxiv.org/abs/cs/0102026v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Quantitative Neural Network Model of the Tip-of-the-Tongue Phenomenon   Based on Synthesized Memory-Psycholinguistic-Metacognitive Approach",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "A new three-stage computer artificial neural network model of the tip-of-the-tongue phenomenon is proposed. Each word's node is build from some interconnected learned auto-associative two-layer neural networks each of which represents separate word's semantic, lexical, or phonological components. The model synthesizes memory, psycholinguistic, and metamemory approaches, bridges speech errors and naming chronometry research traditions, and can explain quantitatively many tip-of-the-tongue effects.",
        "published": "2001-03-02T00:20:01Z",
        "link": "http://arxiv.org/abs/cs/0103002v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC",
            "q-bio.QM",
            "I.2.7"
        ]
    },
    {
        "title": "Two-parameter Model of Word Length \"Language - Genre\"",
        "authors": [
            "Victor Kromer"
        ],
        "summary": "A two-parameter model of word length measured by the number of syllables comprising it is proposed. The first parameter is dependent on language type, the second one - on text genre and reflects the degree of completion of synergetic processes of language optimization.",
        "published": "2001-03-08T06:30:27Z",
        "link": "http://arxiv.org/abs/cs/0103007v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition   in Japanese and English Sentences",
        "authors": [
            "Masaki Murata",
            "Kiyotaka Uchimoto",
            "Qing Ma",
            "Hitoshi Isahara"
        ],
        "summary": "George A. Miller said that human beings have only seven chunks in short-term memory, plus or minus two. We counted the number of bunsetsus (phrases) whose modifiees are undetermined in each step of an analysis of the dependency structure of Japanese sentences, and which therefore must be stored in short-term memory. The number was roughly less than nine, the upper bound of seven plus or minus two. We also obtained similar results with English sentences under the assumption that human beings recognize a series of words, such as a noun phrase (NP), as a unit. This indicates that if we assume that the human cognitive units in Japanese and English are bunsetsu and NP respectively, analysis will support Miller's $7 \\pm 2$ theory.",
        "published": "2001-03-12T08:23:19Z",
        "link": "http://arxiv.org/abs/cs/0103010v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "A Machine-Learning Approach to Estimating the Referential Properties of   Japanese Noun Phrases",
        "authors": [
            "Masaki Murata",
            "Kiyotaka Uchimoto",
            "Qing Ma",
            "Hitoshi Isahara"
        ],
        "summary": "The referential properties of noun phrases in the Japanese language, which has no articles, are useful for article generation in Japanese-English machine translation and for anaphora resolution in Japanese noun phrases. They are generally classified as generic noun phrases, definite noun phrases, and indefinite noun phrases. In the previous work, referential properties were estimated by developing rules that used clue words. If two or more rules were in conflict with each other, the category having the maximum total score given by the rules was selected as the desired category. The score given by each rule was established by hand, so the manpower cost was high. In this work, we automatically adjusted these scores by using a machine-learning method and succeeded in reducing the amount of manpower needed to adjust these scores.",
        "published": "2001-03-12T08:29:59Z",
        "link": "http://arxiv.org/abs/cs/0103011v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Meaning Sort - Three examples: dictionary construction, tagged corpus   construction, and information presentation system",
        "authors": [
            "Masaki Murata",
            "Kyoko Kanzaki",
            "Kiyotaka Uchimoto",
            "Qing Ma",
            "Hitoshi Isahara"
        ],
        "summary": "It is often useful to sort words into an order that reflects relations among their meanings as obtained by using a thesaurus. In this paper, we introduce a method of arranging words semantically by using several types of `{\\sf is-a}' thesauri and a multi-dimensional thesaurus. We also describe three major applications where a meaning sort is useful and show the effectiveness of a meaning sort. Since there is no doubt that a word list in meaning-order is easier to use than a word list in some random order, a meaning sort, which can easily produce a word list in meaning-order, must be useful and effective.",
        "published": "2001-03-12T08:43:11Z",
        "link": "http://arxiv.org/abs/cs/0103012v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "CRL at Ntcir2",
        "authors": [
            "Masaki Murata",
            "Masao Utiyama",
            "Qing Ma",
            "Hiromi Ozaku",
            "Hitoshi Isahara"
        ],
        "summary": "We have developed systems of two types for NTCIR2. One is an enhenced version of the system we developed for NTCIR1 and IREX. It submitted retrieval results for JJ and CC tasks. A variety of parameters were tried with the system. It used such characteristics of newspapers as locational information in the CC tasks. The system got good results for both of the tasks. The other system is a portable system which avoids free parameters as much as possible. The system submitted retrieval results for JJ, JE, EE, EJ, and CC tasks. The system automatically determined the number of top documents and the weight of the original query used in automatic-feedback retrieval. It also determined relevant terms quite robustly. For EJ and JE tasks, it used document expansion to augment the initial queries. It achieved good results, except on the CC tasks.",
        "published": "2001-03-12T09:36:50Z",
        "link": "http://arxiv.org/abs/cs/0103013v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "A Decision Tree of Bigrams is an Accurate Predictor of Word Sense",
        "authors": [
            "Ted Pedersen"
        ],
        "summary": "This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.",
        "published": "2001-03-29T23:08:33Z",
        "link": "http://arxiv.org/abs/cs/0103026v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Bootstrapping Structure using Similarity",
        "authors": [
            "Menno van Zaanen"
        ],
        "summary": "In this paper a new similarity-based learning algorithm, inspired by string edit-distance (Wagner and Fischer, 1974), is applied to the problem of bootstrapping structure from scratch. The algorithm takes a corpus of unannotated sentences as input and returns a corpus of bracketed sentences. The method works on pairs of unstructured sentences or sentences partially bracketed by the algorithm that have one or more words in common. It finds parts of sentences that are interchangeable (i.e. the parts of the sentences that are different in both sentences). These parts are taken as possible constituents of the same type. While this corresponds to the basic bootstrapping step of the algorithm, further structure may be learned from comparison with other (similar) sentences.   We used this method for bootstrapping structure from the flat sentences of the Penn Treebank ATIS corpus, and compared the resulting structured sentences to the structured sentences in the ATIS corpus. Similarly, the algorithm was tested on the OVIS corpus. We obtained 86.04 % non-crossing brackets precision on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS corpus.",
        "published": "2001-04-03T14:09:12Z",
        "link": "http://arxiv.org/abs/cs/0104005v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2, I.2.6, I.2.7"
        ]
    },
    {
        "title": "ABL: Alignment-Based Learning",
        "authors": [
            "Menno van Zaanen"
        ],
        "summary": "This paper introduces a new type of grammar learning algorithm, inspired by string edit distance (Wagner and Fischer, 1974). The algorithm takes a corpus of flat sentences as input and returns a corpus of labelled, bracketed sentences. The method works on pairs of unstructured sentences that have one or more words in common. When two sentences are divided into parts that are the same in both sentences and parts that are different, this information is used to find parts that are interchangeable. These parts are taken as possible constituents of the same type. After this alignment learning step, the selection learning step selects the most probable constituents from all possible constituents.   This method was used to bootstrap structure on the ATIS corpus (Marcus et al., 1993) and on the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.) corpus (Bonnema et al., 1997). While the results are encouraging (we obtained up to 89.25 % non-crossing brackets precision), this paper will point out some of the shortcomings of our approach and will suggest possible solutions.",
        "published": "2001-04-03T14:20:26Z",
        "link": "http://arxiv.org/abs/cs/0104006v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Bootstrapping Syntax and Recursion using Alignment-Based Learning",
        "authors": [
            "Menno van Zaanen"
        ],
        "summary": "This paper introduces a new type of unsupervised learning algorithm, based on the alignment of sentences and Harris's (1951) notion of interchangeability. The algorithm is applied to an untagged, unstructured corpus of natural language sentences, resulting in a labelled, bracketed version of the corpus. Firstly, the algorithm aligns all sentences in the corpus in pairs, resulting in a partition of the sentences consisting of parts of the sentences that are similar in both sentences and parts that are dissimilar. This information is used to find (possibly overlapping) constituents. Next, the algorithm selects (non-overlapping) constituents. Several instances of the algorithm are applied to the ATIS corpus (Marcus et al., 1993) and the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.) corpus (Bonnema et al., 1997). Apart from the promising numerical results, the most striking result is that even the simplest algorithm based on alignment learns recursion.",
        "published": "2001-04-03T15:03:16Z",
        "link": "http://arxiv.org/abs/cs/0104007v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2; I.2.6; I.2.7"
        ]
    },
    {
        "title": "Type Arithmetics: Computation based on the theory of types",
        "authors": [
            "Oleg Kiselyov"
        ],
        "summary": "The present paper shows meta-programming turn programming, which is rich enough to express arbitrary arithmetic computations. We demonstrate a type system that implements Peano arithmetics, slightly generalized to negative numbers. Certain types in this system denote numerals. Arithmetic operations on such types-numerals - addition, subtraction, and even division - are expressed as type reduction rules executed by a compiler. A remarkable trait is that division by zero becomes a type error - and reported as such by a compiler.",
        "published": "2001-04-03T23:22:17Z",
        "link": "http://arxiv.org/abs/cs/0104010v1",
        "categories": [
            "cs.CL",
            "F.3.3; F.4.2; D.3.3"
        ]
    },
    {
        "title": "Beyond the Zipf-Mandelbrot law in quantitative linguistics",
        "authors": [
            "Marcelo A. Montemurro"
        ],
        "summary": "In this paper the Zipf-Mandelbrot law is revisited in the context of linguistics. Despite its widespread popularity the Zipf--Mandelbrot law can only describe the statistical behaviour of a rather restricted fraction of the total number of words contained in some given corpus. In particular, we focus our attention on the important deviations that become statistically relevant as larger corpora are considered and that ultimately could be understood as salient features of the underlying complex process of language generation. Finally, it is shown that all the different observed regimes can be accurately encompassed within a single mathematical framework recently introduced by C. Tsallis.",
        "published": "2001-04-04T02:01:01Z",
        "link": "http://arxiv.org/abs/cond-mat/0104066v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL",
            "nlin.AO"
        ]
    },
    {
        "title": "Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based   Adaptation",
        "authors": [
            "Radu Florian",
            "David Yarowsky"
        ],
        "summary": "This paper presents a novel method of generating and applying hierarchical, dynamic topic-based language models. It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques. These combined models help capture long-distance lexical dependencies. Experiments on the Broadcast News corpus show significant improvement in perplexity (10.5% overall and 33.5% on target vocabulary).",
        "published": "2001-04-27T22:50:31Z",
        "link": "http://arxiv.org/abs/cs/0104019v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Coaxing Confidences from an Old Friend: Probabilistic Classifications   from Transformation Rule Lists",
        "authors": [
            "Radu Florian",
            "John C. Henderson",
            "Grace Ngai"
        ],
        "summary": "Transformation-based learning has been successfully employed to solve many natural language processing problems. It has many positive features, but one drawback is that it does not provide estimates of class membership probabilities.   In this paper, we present a novel method for obtaining class membership probabilities from a transformation-based rule list classifier. Three experiments are presented which measure the modeling accuracy and cross-entropy of the probabilistic classifier on unseen data and the degree to which the output probabilities from the classifier can be used to estimate confidences in its classification decisions.   The results of these experiments show that, for the task of text chunking, the estimates produced by this technique are more informative than those generated by a state-of-the-art decision tree.",
        "published": "2001-04-27T23:16:21Z",
        "link": "http://arxiv.org/abs/cs/0104020v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Microplanning with Communicative Intentions: The SPUD System",
        "authors": [
            "Matthew Stone",
            "Christine Doran",
            "Bonnie Webber",
            "Tonia Bleam",
            "Martha Palmer"
        ],
        "summary": "The process of microplanning encompasses a range of problems in Natural Language Generation (NLG), such as referring expression generation, lexical choice, and aggregation, problems in which a generator must bridge underlying domain-specific representations and general linguistic representations. In this paper, we describe a uniform approach to microplanning based on declarative representations of a generator's communicative intent. These representations describe the results of NLG: communicative intent associates the concrete linguistic structure planned by the generator with inferences that show how the meaning of that structure communicates needed information about some application domain in the current discourse context. Our approach, implemented in the SPUD (sentence planning using description) microplanner, uses the lexicalized tree-adjoining grammar formalism (LTAG) to connect structure to meaning and uses modal logic programming to connect meaning to context. At the same time, communicative intent representations provide a resource for the process of NLG. Using representations of communicative intent, a generator can augment the syntax, semantics and pragmatics of an incomplete sentence simultaneously, and can assess its progress on the various problems of microplanning incrementally. The declarative formulation of communicative intent translates into a well-defined methodology for designing grammatical and conceptual resources which the generator can use to achieve desired microplanning behavior in a specified domain.",
        "published": "2001-04-30T15:12:52Z",
        "link": "http://arxiv.org/abs/cs/0104022v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Correction of Errors in a Modality Corpus Used for Machine Translation   by Using Machine-learning Method",
        "authors": [
            "Masaki Murata",
            "Masao Utiyama",
            "Kiyotaka Uchimoto",
            "Qing Ma",
            "Hitoshi Isahara"
        ],
        "summary": "We performed corpus correction on a modality corpus for machine translation by using such machine-learning methods as the maximum-entropy method. We thus constructed a high-quality modality corpus based on corpus correction. We compared several kinds of methods for corpus correction in our experiments and developed a good method for corpus correction.",
        "published": "2001-05-02T05:29:27Z",
        "link": "http://arxiv.org/abs/cs/0105001v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Man [and Woman] vs. Machine: A Case Study in Base Noun Phrase Learning",
        "authors": [
            "Eric Brill",
            "Grace Ngai"
        ],
        "summary": "A great deal of work has been done demonstrating the ability of machine learning algorithms to automatically extract linguistic knowledge from annotated corpora. Very little work has gone into quantifying the difference in ability at this task between a person and a machine. This paper is a first step in that direction.",
        "published": "2001-05-02T07:49:14Z",
        "link": "http://arxiv.org/abs/cs/0105002v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun   Phrase Chunking",
        "authors": [
            "Grace Ngai",
            "David Yarowsky"
        ],
        "summary": "This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive real-time human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.",
        "published": "2001-05-02T08:39:32Z",
        "link": "http://arxiv.org/abs/cs/0105003v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "A Complete WordNet1.5 to WordNet1.6 Mapping",
        "authors": [
            "J. Daudé",
            "L. Padró",
            "G. Rigau"
        ],
        "summary": "We describe a robust approach for linking already existing lexical/semantic hierarchies. We use a constraint satisfaction algorithm (relaxation labelling) to select --among a set of candidates-- the node in a target taxonomy that bests matches each node in a source taxonomy. In this paper we present the complete mapping of the nominal, verbal, adjectival and adverbial parts of WordNet 1.5 onto WordNet 1.6.",
        "published": "2001-05-04T08:55:02Z",
        "link": "http://arxiv.org/abs/cs/0105005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Joint and conditional estimation of tagging and parsing models",
        "authors": [
            "Mark Johnson"
        ],
        "summary": "This paper compares two different ways of estimating statistical language models. Many statistical NLP tagging and parsing models are estimated by maximizing the (joint) likelihood of the fully-observed training data. However, since these applications only require the conditional probability distributions, these distributions can in principle be learnt by maximizing the conditional likelihood of the training data. Perhaps somewhat surprisingly, models estimated by maximizing the joint were superior to models estimated by maximizing the conditional, even though some of the latter models intuitively had access to ``more information''.",
        "published": "2001-05-07T14:25:22Z",
        "link": "http://arxiv.org/abs/cs/0105012v1",
        "categories": [
            "cs.CL",
            "H.5.2"
        ]
    },
    {
        "title": "Probabilistic top-down parsing and language modeling",
        "authors": [
            "Brian Roark"
        ],
        "summary": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model which utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model.",
        "published": "2001-05-08T15:35:07Z",
        "link": "http://arxiv.org/abs/cs/0105016v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Robust Probabilistic Predictive Syntactic Processing",
        "authors": [
            "Brian Roark"
        ],
        "summary": "This thesis presents a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The parser builds fully connected derivations incrementally, in a single pass from left-to-right across the string. We argue that the parsing approach that we have adopted is well-motivated from a psycholinguistic perspective, as a model that captures probabilistic dependencies between lexical items, as part of the process of building connected syntactic structures. The basic parser and conditional probability models are presented, and empirical results are provided for its parsing accuracy on both newspaper text and spontaneous telephone conversations. Modifications to the probability model are presented that lead to improved performance. A new language model which uses the output of the parser is then defined. Perplexity and word error rate reduction are demonstrated over trigram models, even when the trigram is trained on significantly more data. Interpolation on a word-by-word basis with a trigram model yields additional improvements.",
        "published": "2001-05-09T17:01:10Z",
        "link": "http://arxiv.org/abs/cs/0105019v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Generating a 3D Simulation of a Car Accident from a Written Description   in Natural Language: the CarSim System",
        "authors": [
            "Sylvain Dupuy",
            "Arjan Egges",
            "Vincent Legendre",
            "Pierre Nugues"
        ],
        "summary": "This paper describes a prototype system to visualize and animate 3D scenes from car accident reports, written in French. The problem of generating such a 3D simulation can be divided into two subtasks: the linguistic analysis and the virtual scene generation. As a means of communication between these two modules, we first designed a template formalism to represent a written accident report. The CarSim system first processes written reports, gathers relevant information, and converts it into a formal description. Then, it creates the corresponding 3D scene and animates the vehicles.",
        "published": "2001-05-14T09:05:45Z",
        "link": "http://arxiv.org/abs/cs/0105023v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.5.1"
        ]
    },
    {
        "title": "The OLAC Metadata Set and Controlled Vocabularies",
        "authors": [
            "Steven Bird",
            "Gary Simons"
        ],
        "summary": "As language data and associated technologies proliferate and as the language resources community rapidly expands, it has become difficult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool can work with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to find language resources. This paper describes a new digital infrastructure for language resource discovery, based on the Open Archives Initiative, and called OLAC -- the Open Language Archives Community. The OLAC Metadata Set and the associated controlled vocabularies facilitate consistent description and focussed searching. We report progress on the metadata set and controlled vocabularies, describing current issues and soliciting input from the language resources community.",
        "published": "2001-05-21T16:53:55Z",
        "link": "http://arxiv.org/abs/cs/0105030v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Historical Dynamics of Lexical System as Random Walk Process",
        "authors": [
            "Victor Kromer"
        ],
        "summary": "It is offered to consider word meanings changes in diachrony as semicontinuous random walk with reflecting and swallowing screens. The basic characteristics of word life cycle are defined. Verification of the model has been realized on the data of Russian words distribution on various age periods.",
        "published": "2001-05-30T03:55:23Z",
        "link": "http://arxiv.org/abs/cs/0105035v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Integrating Prosodic and Lexical Cues for Automatic Topic Segmentation",
        "authors": [
            "G. Tur",
            "D. Hakkani-Tur",
            "A. Stolcke",
            "E. Shriberg"
        ],
        "summary": "We present a probabilistic model that uses both prosodic and lexical cues for the automatic segmentation of speech into topically coherent units. We propose two methods for combining lexical and prosodic information using hidden Markov models and decision trees. Lexical information is obtained from a speech recognizer, and prosodic features are extracted automatically from speech waveforms. We evaluate our approach on the Broadcast News corpus, using the DARPA-TDT evaluation metrics. Results show that the prosodic model alone is competitive with word-based segmentation methods. Furthermore, we achieve a significant reduction in error by combining the prosodic and word-based knowledge sources.",
        "published": "2001-05-31T18:08:57Z",
        "link": "http://arxiv.org/abs/cs/0105037v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Computational properties of environment-based disambiguation",
        "authors": [
            "William Schuler"
        ],
        "summary": "The standard pipeline approach to semantic processing, in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted, is a poor fit for applications such as natural language interfaces. This is because the environment information, in the form of the objects and events in the application's run-time environment, cannot be used to inform parsing decisions unless the input sentence is semantically analyzed, but this does not occur until after parsing in the single-tree semantic architecture. This paper describes the computational properties of an alternative architecture, in which semantic analysis is performed on all possible interpretations during parsing, in polynomial time.",
        "published": "2001-06-07T15:47:31Z",
        "link": "http://arxiv.org/abs/cs/0106011v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "I.2.7; H.5.2"
        ]
    },
    {
        "title": "Organizing Encyclopedic Knowledge based on the Web and its Application   to Question Answering",
        "authors": [
            "Atsushi Fujii",
            "Tetsuya Ishikawa"
        ],
        "summary": "We propose a method to generate large-scale encyclopedic knowledge, which is valuable for much NLP research, based on the Web. We first search the Web for pages containing a term in question. Then we use linguistic patterns and HTML structures to extract text fragments describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese Information-Technology Engineers Examination.",
        "published": "2001-06-10T06:14:09Z",
        "link": "http://arxiv.org/abs/cs/0106015v1",
        "categories": [
            "cs.CL",
            "I.2.7; H.3.3; H.3.4; H.3.5"
        ]
    },
    {
        "title": "File mapping Rule-based DBMS and Natural Language Processing",
        "authors": [
            "Vjacheslav M. Novikov"
        ],
        "summary": "This paper describes the system of storage, extract and processing of information structured similarly to the natural language. For recursive inference the system uses the rules having the same representation, as the data. The environment of storage of information is provided with the File Mapping (SHM) mechanism of operating system. In the paper the main principles of construction of dynamic data structure and language for record of the inference rules are stated; the features of available implementation are considered and the description of the application realizing semantic information retrieval on the natural language is given.",
        "published": "2001-06-10T14:56:51Z",
        "link": "http://arxiv.org/abs/cs/0106016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "cs.PL",
            "D.3.2; H.2.4"
        ]
    },
    {
        "title": "Iterative Residual Rescaling: An Analysis and Generalization of LSI",
        "authors": [
            "Rie Kubota Ando",
            "Lillian Lee"
        ],
        "summary": "We consider the problem of creating document representations in which inter-document similarity measurements correspond to semantic similarity. We first present a novel subspace-based framework for formalizing this task. Using this framework, we derive a new analysis of Latent Semantic Indexing (LSI), showing a precise relationship between its performance and the uniformity of the underlying distribution of documents over topics. This analysis helps explain the improvements gained by Ando's (2000) Iterative Residual Rescaling (IRR) algorithm: IRR can compensate for distributional non-uniformity. A further benefit of our framework is that it provides a well-motivated, effective method for automatically determining the rescaling factor IRR depends on, leading to further improvements. A series of experiments over various settings and with several evaluation metrics validates our claims.",
        "published": "2001-06-17T20:45:37Z",
        "link": "http://arxiv.org/abs/cs/0106039v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Stacking classifiers for anti-spam filtering of e-mail",
        "authors": [
            "G. Sakkis",
            "I. Androutsopoulos",
            "G. Paliouras",
            "V. Karkaletsis",
            "C. D. Spyropoulos",
            "P. Stamatopoulos"
        ],
        "summary": "We evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization. Unsolicited commercial e-mail, or \"spam\", floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content. Using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in real-life applications.",
        "published": "2001-06-19T14:56:02Z",
        "link": "http://arxiv.org/abs/cs/0106040v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
        ]
    },
    {
        "title": "Using the Distribution of Performance for Studying Statistical NLP   Systems and Corpora",
        "authors": [
            "Yuval Krymolowski"
        ],
        "summary": "Statistical NLP systems are frequently evaluated and compared on the basis of their performances on a single split of training and test data. Results obtained using a single split are, however, subject to sampling noise. In this paper we argue in favour of reporting a distribution of performance figures, obtained by resampling the training data, rather than a single number. The additional information from distributions can be used to make statistically quantified statements about differences across parameter settings, systems, and corpora.",
        "published": "2001-06-20T14:16:17Z",
        "link": "http://arxiv.org/abs/cs/0106043v1",
        "categories": [
            "cs.CL",
            "G.3; I.2.6; I.2.7"
        ]
    },
    {
        "title": "A Sequential Model for Multi-Class Classification",
        "authors": [
            "Yair Even-Zohar",
            "Dan Roth"
        ],
        "summary": "Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach -- a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in part-of-speech tagging.",
        "published": "2001-06-20T19:01:41Z",
        "link": "http://arxiv.org/abs/cs/0106044v1",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "I.2.6;I.2.67"
        ]
    },
    {
        "title": "Modeling informational novelty in a conversational system with a hybrid   statistical and grammar-based approach to natural language generation",
        "authors": [
            "Adwait Ratnaparkhi"
        ],
        "summary": "We present a hybrid statistical and grammar-based system for surface natural language generation (NLG) that uses grammar rules, conditions on using those grammar rules, and corpus statistics to determine the word order. We also describe how this surface NLG module is implemented in a prototype conversational system, and how it attempts to model informational novelty by varying the word order. Using a combination of rules and statistical information, the conversational system expresses the novel information differently than the given information, based on the run-time dialog state. We also discuss our plans for evaluating the generation strategy.",
        "published": "2001-06-21T20:37:43Z",
        "link": "http://arxiv.org/abs/cs/0106047v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "CHR as grammar formalism. A first report",
        "authors": [
            "Henning Christiansen"
        ],
        "summary": "Grammars written as Constraint Handling Rules (CHR) can be executed as efficient and robust bottom-up parsers that provide a straightforward, non-backtracking treatment of ambiguity. Abduction with integrity constraints as well as other dynamic hypothesis generation techniques fit naturally into such grammars and are exemplified for anaphora resolution, coordination and text interpretation.",
        "published": "2001-06-29T08:41:02Z",
        "link": "http://arxiv.org/abs/cs/0106059v1",
        "categories": [
            "cs.PL",
            "cs.CL",
            "I.2.7;D.3.2;F.4.1;F.4.2"
        ]
    },
    {
        "title": "The Role of Conceptual Relations in Word Sense Disambiguation",
        "authors": [
            "David Fernandez-Amoros",
            "Julio Gonzalo",
            "Felisa Verdejo"
        ],
        "summary": "We explore many ways of using conceptual distance measures in Word Sense Disambiguation, starting with the Agirre-Rigau conceptual density measure. We use a generalized form of this measure, introducing many (parameterized) refinements and performing an exhaustive evaluation of all meaningful combinations. We finally obtain a 42% improvement over the original algorithm, and show that measures of conceptual distance are not worse indicators for sense disambiguation than measures based on word-coocurrence (exemplified by the Lesk algorithm). Our results, however, reinforce the idea that only a combination of different sources of knowledge might eventually lead to accurate word sense disambiguation.",
        "published": "2001-07-03T10:27:44Z",
        "link": "http://arxiv.org/abs/cs/0107005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Looking Under the Hood : Tools for Diagnosing your Question Answering   Engine",
        "authors": [
            "Eric Breck",
            "Marc Light",
            "Gideon S. Mann",
            "Ellen Riloff",
            "Brianne Brown Pranav Anand",
            "Mats Rooth",
            "Michael Thelen"
        ],
        "summary": "In this paper we analyze two question answering tasks : the TREC-8 question answering task and a set of reading comprehension exams. First, we show that Q/A systems perform better when there are multiple answer opportunities per question. Next, we analyze common approaches to two subproblems: term overlap for answer sentence identification, and answer typing for short answer extraction. We present general tools for analyzing the strengths and limitations of techniques for these subproblems. Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates.",
        "published": "2001-07-03T18:06:05Z",
        "link": "http://arxiv.org/abs/cs/0107006v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Three-Stage Quantitative Neural Network Model of the Tip-of-the-Tongue   Phenomenon",
        "authors": [
            "Petro M. Gopych"
        ],
        "summary": "A new three-stage computer artificial neural network model of the tip-of-the-tongue phenomenon is shortly described, and its stochastic nature was demonstrated. A way to calculate strength and appearance probability of tip-of-the-tongue states, neural network mechanism of feeling-of-knowing phenomenon are proposed. The model synthesizes memory, psycholinguistic, and metamemory approaches, bridges speech errors and naming chronometry research traditions. A model analysis of a tip-of-the-tongue case from Anton Chekhov's short story 'A Horsey Name' is performed. A new 'throw-up-one's-arms effect' is defined.",
        "published": "2001-07-09T21:00:42Z",
        "link": "http://arxiv.org/abs/cs/0107012v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "q-bio.NC",
            "q-bio.QM",
            "I.2.7"
        ]
    },
    {
        "title": "Introduction to the CoNLL-2001 Shared Task: Clause Identification",
        "authors": [
            "Erik F. Tjong Kim Sang",
            "Herve Dejean"
        ],
        "summary": "We describe the CoNLL-2001 shared task: dividing text into clauses. We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.",
        "published": "2001-07-15T12:51:01Z",
        "link": "http://arxiv.org/abs/cs/0107016v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Learning Computational Grammars",
        "authors": [
            "John Nerbonne",
            "Anja Belz",
            "Nicola Cancedda",
            "Herve Dejean",
            "James Hammerton",
            "Rob Koeling",
            "Stasinos Konstantopoulos",
            "Miles Osborne",
            "Franck Thollard",
            "Erik F. Tjong Kim Sang"
        ],
        "summary": "This paper reports on the \"Learning Computational Grammars\" (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use. We were interested in a more systematic survey to understand the relevance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars). We focused on syntax, esp. noun phrase (NP) syntax.",
        "published": "2001-07-15T13:21:48Z",
        "link": "http://arxiv.org/abs/cs/0107017v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Combining a self-organising map with memory-based learning",
        "authors": [
            "James Hammerton",
            "Erik F. Tjong Kim Sang"
        ],
        "summary": "Memory-based learning (MBL) has enjoyed considerable success in corpus-based natural language processing (NLP) tasks and is thus a reliable method of getting a high-level of performance when building corpus-based NLP systems. However there is a bottleneck in MBL whereby any novel testing item has to be compared against all the training items in memory base. For this reason there has been some interest in various forms of memory editing whereby some method of selecting a subset of the memory base is employed to reduce the number of comparisons. This paper investigates the use of a modified self-organising map (SOM) to select a subset of the memory items for comparison. This method involves reducing the number of comparisons to a value proportional to the square root of the number of training items. The method is tested on the identification of base noun-phrases in the Wall Street Journal corpus, using sections 15 to 18 for training and section 20 for testing.",
        "published": "2001-07-15T13:32:36Z",
        "link": "http://arxiv.org/abs/cs/0107018v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Applying Natural Language Generation to Indicative Summarization",
        "authors": [
            "Min-Yen Kan",
            "Kathleen R. McKeown",
            "Judith L. Klavans"
        ],
        "summary": "The task of creating indicative summaries that help a searcher decide whether to read a particular document is a difficult task. This paper examines the indicative summarization task from a generation perspective, by first analyzing its required content via published guidelines and corpus analysis. We show how these summaries can be factored into a set of document features, and how an implemented content planner uses the topicality document feature to create indicative multidocument query-based summaries.",
        "published": "2001-07-16T22:59:15Z",
        "link": "http://arxiv.org/abs/cs/0107019v2",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Transformation-Based Learning in the Fast Lane",
        "authors": [
            "Grace Ngai",
            "Radu Florian"
        ],
        "summary": "Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance. The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system \\cite{hepple00:tbl}. The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.",
        "published": "2001-07-17T15:26:13Z",
        "link": "http://arxiv.org/abs/cs/0107020v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Multidimensional Transformation-Based Learning",
        "authors": [
            "Radu Florian",
            "Grace Ngai"
        ],
        "summary": "This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm \\cite{brill95:tagging} to be applied to multiple classification tasks by training jointly and simultaneously on all fields. The motivation for constructing such a system stems from the observation that many tasks in natural language processing are naturally composed of multiple subtasks which need to be resolved simultaneously; also tasks usually learned in isolation can possibly benefit from being learned in a joint framework, as the signals for the extra tasks usually constitute inductive bias.   The proposed algorithm is evaluated in two experiments: in one, the system is used to jointly predict the part-of-speech and text chunks/baseNP chunks of an English corpus; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese. The results show that the simultaneous learning of multiple tasks does achieve an improvement in each task upon training the same tasks sequentially. The part-of-speech tagging result of 96.63% is state-of-the-art for individual systems on the particular train/test split.",
        "published": "2001-07-17T15:43:03Z",
        "link": "http://arxiv.org/abs/cs/0107021v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Coupled Clustering: a Method for Detecting Structural Correspondence",
        "authors": [
            "Zvika Marx",
            "Ido Dagan",
            "Joachim Buhmann"
        ],
        "summary": "This paper proposes a new paradigm and computational framework for identification of correspondences between sub-structures of distinct composite systems. For this, we define and investigate a variant of traditional data clustering, termed coupled clustering, which simultaneously identifies corresponding clusters within two data sets. The presented method is demonstrated and evaluated for detecting topical correspondences in textual corpora.",
        "published": "2001-07-23T11:06:45Z",
        "link": "http://arxiv.org/abs/cs/0107032v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.6; I.2.7; I.5.3; I.5.4"
        ]
    },
    {
        "title": "Classes for Fast Maximum Entropy Training",
        "authors": [
            "Joshua Goodman"
        ],
        "summary": "Maximum entropy models are considered by many to be one of the most promising avenues of language modeling research. Unfortunately, long training times make maximum entropy research difficult. We present a novel speedup technique: we change the form of the model to use classes. Our speedup works by creating two maximum entropy models, the first of which predicts the class of each word, and the second of which predicts the word itself. This factoring of the model leads to fewer non-zero indicator functions, and faster normalization, achieving speedups of up to a factor of 35 over one of the best previous techniques. It also results in typically slightly lower perplexities. The same trick can be used to speed training of other machine learning techniques, e.g. neural networks, applied to any problem with a large number of outputs, such as language modeling.",
        "published": "2001-08-09T19:17:58Z",
        "link": "http://arxiv.org/abs/cs/0108006v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "A Bit of Progress in Language Modeling",
        "authors": [
            "Joshua Goodman"
        ],
        "summary": "In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We find some significant interactions, especially with smoothing and clustering techniques. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38% and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline. This is the extended version of the paper; it contains additional details and proofs, and is designed to be a good introduction to the state of the art in language modeling.",
        "published": "2001-08-09T19:24:28Z",
        "link": "http://arxiv.org/abs/cs/0108005v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Portability of Syntactic Structure for Language Modeling",
        "authors": [
            "Ciprian Chelba"
        ],
        "summary": "The paper presents a study on the portability of statistical syntactic knowledge in the framework of the structured language model (SLM). We investigate the impact of porting SLM statistics from the Wall Street Journal (WSJ) to the Air Travel Information System (ATIS) domain. We compare this approach to applying the Microsoft rule-based parser (NLPwin) for the ATIS data and to using a small amount of data manually parsed at UPenn for gathering the intial SLM statistics. Surprisingly, despite the fact that it performs modestly in perplexity (PPL), the model initialized on WSJ parses outperforms the other initialization methods based on in-domain annotated data, achieving a significant 0.4% absolute and 7% relative reduction in word error rate (WER) over a baseline system whose word error rate is 5.8%; the improvement measured relative to the minimum WER achievable on the N-best lists we worked with is 12%.",
        "published": "2001-08-29T03:59:31Z",
        "link": "http://arxiv.org/abs/cs/0108022v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Information Extraction Using the Structured Language Model",
        "authors": [
            "Ciprian Chelba",
            "Milind Mahajan"
        ],
        "summary": "The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser. The task of template filling is cast as constrained parsing using the SLM. The model is automatically trained from a set of sentences annotated with frame/slot labels and spans. Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage. Despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad --- personal information management --- task.",
        "published": "2001-08-29T04:00:24Z",
        "link": "http://arxiv.org/abs/cs/0108023v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "I.2.7"
        ]
    },
    {
        "title": "Anaphora and Discourse Structure",
        "authors": [
            "Bonnie Webber",
            "Matthew Stone",
            "Aravind Joshi",
            "Alistair Knott"
        ],
        "summary": "We argue in this paper that many common adverbial phrases generally taken to signal a discourse relation between syntactically connected units within discourse structure, instead work anaphorically to contribute relational meaning, with only indirect dependence on discourse structure. This allows a simpler discourse structure to provide scaffolding for compositional semantics, and reveals multiple ways in which the relational meaning conveyed by adverbial connectives can interact with that associated with discourse structure. We conclude by sketching out a lexicalised grammar for discourse that facilitates discourse interpretation as a product of compositional rules, anaphor resolution and inference.",
        "published": "2001-09-09T16:41:59Z",
        "link": "http://arxiv.org/abs/cs/0109010v2",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Conceptual Analysis of Lexical Taxonomies: The Case of WordNet Top-Level",
        "authors": [
            "Aldo Gangemi",
            "Nicola Guarino",
            "Alessandro Oltramari"
        ],
        "summary": "In this paper we propose an analysis and an upgrade of WordNet's top-level synset taxonomy. We briefly review WordNet and identify its main semantic limitations. Some principles from a forthcoming OntoClean methodology are applied to the ontological analysis of WordNet. A revised top-level taxonomy is proposed, which is meant to be more conceptually rigorous, cognitively transparent, and efficiently exploitable in several applications.",
        "published": "2001-09-11T12:17:04Z",
        "link": "http://arxiv.org/abs/cs/0109013v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H3.1"
        ]
    },
    {
        "title": "Entropic analysis of the role of words in literary texts",
        "authors": [
            "Marcelo A. Montemurro",
            "Damian H. Zanette"
        ],
        "summary": "Beyond the local constraints imposed by grammar, words concatenated in long sequences carrying a complex message show statistical regularities that may reflect their linguistic role in the message. In this paper, we perform a systematic statistical analysis of the use of words in literary English corpora. We show that there is a quantitative relation between the role of content words in literary English and the Shannon information entropy defined over an appropriate probability distribution. Without assuming any previous knowledge about the syntactic structure of language, we are able to cluster certain groups of words according to their specific role in the text.",
        "published": "2001-09-12T18:08:07Z",
        "link": "http://arxiv.org/abs/cond-mat/0109218v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.CL"
        ]
    },
    {
        "title": "Boosting Trees for Anti-Spam Email Filtering",
        "authors": [
            "Xavier Carreras",
            "Lluis Marquez"
        ],
        "summary": "This paper describes a set of comparative experiments for the problem of automatically filtering unwanted electronic mail messages. Several variants of the AdaBoost algorithm with confidence-rated predictions [Schapire & Singer, 99] have been applied, which differ in the complexity of the base learners considered. Two main conclusions can be drawn from our experiments: a) The boosting-based methods clearly outperform the baseline learning algorithms (Naive Bayes and Induction of Decision Trees) on the PU1 corpus, achieving very high levels of the F1 measure; b) Increasing the complexity of the base learners allows to obtain better ``high-precision'' classifiers, which is a very important issue when misclassification costs are considered.",
        "published": "2001-09-13T14:52:41Z",
        "link": "http://arxiv.org/abs/cs/0109015v1",
        "categories": [
            "cs.CL",
            "I.2.7;I.5.4"
        ]
    },
    {
        "title": "Modelling Semantic Association and Conceptual Inheritance for Semantic   Analysis",
        "authors": [
            "Pascal Vaillant"
        ],
        "summary": "Allowing users to interact through language borders is an interesting challenge for information technology. For the purpose of a computer assisted language learning system, we have chosen icons for representing meaning on the input interface, since icons do not depend on a particular language. However, a key limitation of this type of communication is the expression of articulated ideas instead of isolated concepts. We propose a method to interpret sequences of icons as complex messages by reconstructing the relations between concepts, so as to build conceptual graphs able to represent meaning and to be used for natural language sentence generation. This method is based on an electronic dictionary containing semantic information.",
        "published": "2001-09-15T22:44:55Z",
        "link": "http://arxiv.org/abs/cs/0109020v1",
        "categories": [
            "cs.CL",
            "H.3.1; I.2.7"
        ]
    },
    {
        "title": "Integrating Multiple Knowledge Sources for Robust Semantic Parsing",
        "authors": [
            "Jordi Atserias",
            "Lluis Padro",
            "German Rigau"
        ],
        "summary": "This work explores a new robust approach for Semantic Parsing of unrestricted texts. Our approach considers Semantic Parsing as a Consistent Labelling Problem (CLP), allowing the integration of several knowledge types (syntactic and semantic) obtained from different sources (linguistic and statistic). The current implementation obtains 95% accuracy in model identification and 72% in case-role filling.",
        "published": "2001-09-17T14:41:14Z",
        "link": "http://arxiv.org/abs/cs/0109023v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7"
        ]
    },
    {
        "title": "Learning class-to-class selectional preferences",
        "authors": [
            "E. Agirre",
            "D. Martinez"
        ],
        "summary": "Selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This papers extends previous statistical models to class-to-class preferences, and presents a model that learns selectional preferences for classes of verbs. The motivation is twofold: different senses of a verb may have different preferences, and some classes of verbs can share preferences. The model is tested on a word sense disambiguation task which uses subject-verb and object-verb relationships extracted from a small sense-disambiguated corpus.",
        "published": "2001-09-18T14:00:27Z",
        "link": "http://arxiv.org/abs/cs/0109029v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Knowledge Sources for Word Sense Disambiguation",
        "authors": [
            "Eneko Agirre",
            "David Martinez"
        ],
        "summary": "Two kinds of systems have been defined during the long history of WSD: principled systems that define which knowledge types are useful for WSD, and robust systems that use the information sources at hand, such as, dictionaries, light-weight ontologies or hand-tagged corpora. This paper tries to systematize the relation between desired knowledge types and actual information sources. We also compare the results for a wide range of algorithms that have been evaluated on a common test setting in our research group. We hope that this analysis will help change the shift from systems based on information sources to systems based on knowledge sources. This study might also shed some light on semi-automatic acquisition of desired knowledge types from existing resources.",
        "published": "2001-09-18T14:14:52Z",
        "link": "http://arxiv.org/abs/cs/0109030v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Enriching WordNet concepts with topic signatures",
        "authors": [
            "Eneko Agirre",
            "Olatz Ansa",
            "Eduard Hovy",
            "David Martinez"
        ],
        "summary": "This paper explores the possibility of enriching the content of existing ontologies. The overall goal is to overcome the lack of topical links among concepts in WordNet. Each concept is to be associated to a topic signature, i.e., a set of related words with associated weights. The signatures can be automatically constructed from the WWW or from sense-tagged corpora. Both approaches are compared and evaluated on a word sense disambiguation task. The results show that it is possible to construct clean signatures from the WWW using some filtering techniques.",
        "published": "2001-09-18T14:18:58Z",
        "link": "http://arxiv.org/abs/cs/0109031v2",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Testing for Mathematical Lineation in Jim Crace's \"Quarantine\" and T. S.   Eliot's \"Four Quartets\"",
        "authors": [
            "John Constable",
            "Hideaki Aoyama"
        ],
        "summary": "The mathematical distinction between prose and verse may be detected in writings that are not apparently lineated, for example in T. S. Eliot's \"Burnt Norton\", and Jim Crace's \"Quarantine\". In this paper we offer comments on appropriate statistical methods for such work, and also on the nature of formal innovation in these two texts. Additional remarks are made on the roots of lineation as a metrical form, and on the prose-verse continuum.",
        "published": "2001-09-20T06:42:11Z",
        "link": "http://arxiv.org/abs/cs/0109039v1",
        "categories": [
            "cs.CL",
            "H.3.1;I.2.7;I.5.4"
        ]
    },
    {
        "title": "The Open Language Archives Community and Asian Language Resources",
        "authors": [
            "Steven Bird",
            "Gary Simons",
            "Chu-Ren Huang"
        ],
        "summary": "The Open Language Archives Community (OLAC) is a new project to build a worldwide system of federated language archives based on the Open Archives Initiative and the Dublin Core Metadata Initiative. This paper aims to disseminate the OLAC vision to the language resources community in Asia, and to show language technologists and linguists how they can document their tools and data in such a way that others can easily discover them. We describe OLAC and the OLAC Metadata Set, then discuss two key issues in the Asian context: language classification and multilingual resource classification.",
        "published": "2001-10-03T12:45:41Z",
        "link": "http://arxiv.org/abs/cs/0110014v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Richer Syntactic Dependencies for Structured Language Modeling",
        "authors": [
            "Ciprian Chelba",
            "Peng Xu"
        ],
        "summary": "The paper investigates the use of richer syntactic dependencies in the structured language model (SLM). We present two simple methods of enriching the dependencies in the syntactic parse trees used for intializing the SLM. We evaluate the impact of both methods on the perplexity (PPL) and word-error-rate(WER, N-best rescoring) performance of the SLM. We show that the new model achieves an improvement in PPL and WER over the baseline results reported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ) corpora, respectively.",
        "published": "2001-10-03T18:34:36Z",
        "link": "http://arxiv.org/abs/cs/0110015v1",
        "categories": [
            "cs.CL",
            "I.2.7; G.3"
        ]
    },
    {
        "title": "Part-of-Speech Tagging with Two Sequential Transducers",
        "authors": [
            "Andre Kempe"
        ],
        "summary": "We present a method of constructing and using a cascade consisting of a left- and a right-sequential finite-state transducer (FST), T1 and T2, for part-of-speech (POS) disambiguation. Compared to an HMM, this FST cascade has the advantage of significantly higher processing speed, but at the cost of slightly lower accuracy. Applications such as Information Retrieval, where the speed can be more important than accuracy, could benefit from this approach.   In the process of tagging, we first assign every word a unique ambiguity class c_i that can be looked up in a lexicon encoded by a sequential FST. Every c_i is denoted by a single symbol, e.g. [ADJ_NOUN], although it represents a set of alternative tags that a given word can occur with. The sequence of the c_i of all words of one sentence is the input to our FST cascade. It is mapped by T1, from left to right, to a sequence of reduced ambiguity classes r_i. Every r_i is denoted by a single symbol, although it represents a set of alternative tags. Intuitively, T1 eliminates the less likely tags from c_i, thus creating r_i. Finally, T2 maps the sequence of r_i, from right to left, to a sequence of single POS tags t_i. Intuitively, T2 selects the most likely t_i from every r_i.   The probabilities of all t_i, r_i, and c_i are used only at compile time, not at run time. They do not (directly) occur in the FSTs, but are \"implicitly contained\" in their structure.",
        "published": "2001-10-11T11:29:49Z",
        "link": "http://arxiv.org/abs/cs/0110027v1",
        "categories": [
            "cs.CL",
            "F.1.1; I.2.7"
        ]
    },
    {
        "title": "Towards Solving the Interdisciplinary Language Barrier Problem",
        "authors": [
            "Sebastien Paquet"
        ],
        "summary": "This work aims to make it easier for a specialist in one field to find and explore ideas from another field which may be useful in solving a new problem arising in his practice. It presents a methodology which serves to represent the relationships that exist between concepts, problems, and solution patterns from different fields of human activity in the form of a graph. Our approach is based upon generalization and specialization relationships and problem solving. It is simple enough to be understood quite easily, and general enough to enable coherent integration of concepts and problems from virtually any field. We have built an implementation which uses the World Wide Web as a support to allow navigation between graph nodes and collaborative development of the graph.",
        "published": "2001-10-18T19:39:22Z",
        "link": "http://arxiv.org/abs/cs/0110041v1",
        "categories": [
            "cs.CY",
            "cs.CL",
            "cs.IR",
            "H3.1; I2.4; K3.1"
        ]
    },
    {
        "title": "What is the minimal set of fragments that achieves maximal parse   accuracy?",
        "authors": [
            "Rens Bod"
        ],
        "summary": "We aim at finding the minimal set of fragments which achieves maximal parse accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street Journal treebank show that counts of almost arbitrary fragments within parse trees are important, leading to improved parse accuracy over previous models tested on this treebank (a precision of 90.8% and a recall of 90.6%). We isolate some dependency relations which previous models neglect but which contribute to higher parse accuracy.",
        "published": "2001-10-24T11:01:08Z",
        "link": "http://arxiv.org/abs/cs/0110050v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Combining semantic and syntactic structure for language modeling",
        "authors": [
            "Rens Bod"
        ],
        "summary": "Structured language models for speech recognition have been shown to remedy the weaknesses of n-gram models. All current structured language models are, however, limited in that they do not take into account dependencies between non-headwords. We show that non-headword dependencies contribute to significantly improved word error rate, and that a data-oriented parsing model trained on semantically and syntactically annotated data can exploit these dependencies. This paper also contains the first DOP model trained by means of a maximum likelihood reestimation procedure, which solves some of the theoretical shortcomings of previous DOP models.",
        "published": "2001-10-24T11:30:50Z",
        "link": "http://arxiv.org/abs/cs/0110051v1",
        "categories": [
            "cs.CL",
            "I.2.7"
        ]
    },
    {
        "title": "Generating Multilingual Personalized Descriptions of Museum Exhibits -   The M-PIRO Project",
        "authors": [
            "Ion Androutsopoulos",
            "Vassiliki Kokkinaki",
            "Aggeliki Dimitromanolaki",
            "Jo Calder",
            "Jon Oberlander",
            "Elena Not"
        ],
        "summary": "This paper provides an overall presentation of the M-PIRO project. M-PIRO is developing technology that will allow museums to generate automatically textual or spoken descriptions of exhibits for collections available over the Web or in virtual reality environments. The descriptions are generated in several languages from information in a language-independent database and small fragments of text, and they can be tailored according to the backgrounds of the users, their ages, and their previous interaction with the system. An authoring tool allows museum curators to update the system's database and to control the language and content of the resulting descriptions. Although the project is still in progress, a Web-based demonstrator that supports English, Greek and Italian is already available, and it is used throughout the paper to highlight the capabilities of the emerging technology.",
        "published": "2001-10-29T16:55:32Z",
        "link": "http://arxiv.org/abs/cs/0110057v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "I.2.7; H.5.2; H.5.4; I.7.4"
        ]
    },
    {
        "title": "The Use of Classifiers in Sequential Inference",
        "authors": [
            "Vasin Punyakanok",
            "Dan Roth"
        ],
        "summary": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem-identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.",
        "published": "2001-11-01T03:02:19Z",
        "link": "http://arxiv.org/abs/cs/0111003v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "I.2.6, I.2.7"
        ]
    },
    {
        "title": "A procedure for unsupervised lexicon learning",
        "authors": [
            "Anand Venkataraman"
        ],
        "summary": "We describe an incremental unsupervised procedure to learn words from transcribed continuous speech. The algorithm is based on a conservative and traditional statistical model, and results of empirical tests show that it is competitive with other algorithms that have been proposed recently for this task.",
        "published": "2001-11-30T20:30:52Z",
        "link": "http://arxiv.org/abs/cs/0111064v1",
        "categories": [
            "cs.CL",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "A Statistical Model for Word Discovery in Transcribed Speech",
        "authors": [
            "Anand Venkataraman"
        ],
        "summary": "A statistical model for segmentation and word discovery in continuous speech is presented. An incremental unsupervised learning algorithm to infer word boundaries based on this model is described. Results of empirical tests showing that the algorithm is competitive with other models that have been used for similar tasks are also presented.",
        "published": "2001-11-30T20:40:50Z",
        "link": "http://arxiv.org/abs/cs/0111065v1",
        "categories": [
            "cs.CL",
            "I.2.6;I.2.7"
        ]
    },
    {
        "title": "Using a Support-Vector Machine for Japanese-to-English Translation of   Tense, Aspect, and Modality",
        "authors": [
            "Masaki Murata",
            "Kiyotaka Uchimoto",
            "Qing Ma",
            "Hitoshi Isahara"
        ],
        "summary": "This paper describes experiments carried out using a variety of machine-learning methods, including the k-nearest neighborhood method that was used in a previous study, for the translation of tense, aspect, and modality. It was found that the support-vector machine method was the most precise of all the methods tested.",
        "published": "2001-12-05T05:35:07Z",
        "link": "http://arxiv.org/abs/cs/0112003v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Part of Speech Tagging in Thai Language Using Support Vector Machine",
        "authors": [
            "Masaki Murata",
            "Qing Ma",
            "Hitoshi Isahara"
        ],
        "summary": "The elastic-input neuro tagger and hybrid tagger, combined with a neural network and Brill's error-driven learning, have already been proposed for the purpose of constructing a practical tagger using as little training data as possible. When a small Thai corpus is used for training, these taggers have tagging accuracies of 94.4% and 95.5% (accounting only for the ambiguous words in terms of the part of speech), respectively. In this study, in order to construct more accurate taggers we developed new tagging methods using three machine learning methods: the decision-list, maximum entropy, and support vector machine methods. We then performed tagging experiments by using these methods. Our results showed that the support vector machine method has the best precision (96.1%), and that it is capable of improving the accuracy of tagging in the Thai language. Finally, we theoretically examined all these methods and discussed how the improvements were achived.",
        "published": "2001-12-05T05:48:21Z",
        "link": "http://arxiv.org/abs/cs/0112004v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Universal Model for Paraphrasing -- Using Transformation Based on a   Defined Criteria --",
        "authors": [
            "Masaki Murata",
            "Hitoshi Isahara"
        ],
        "summary": "This paper describes a universal model for paraphrasing that transforms according to defined criteria. We showed that by using different criteria we could construct different kinds of paraphrasing systems including one for answering questions, one for compressing sentences, one for polishing up, and one for transforming written language to spoken language.",
        "published": "2001-12-05T05:56:13Z",
        "link": "http://arxiv.org/abs/cs/0112005v1",
        "categories": [
            "cs.CL",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "A Straightforward Approach to Morphological Analysis and Synthesis",
        "authors": [
            "Kyriakos N. Sgarbas",
            "Nikos D. Fakotakis",
            "George K. Kokkinakis"
        ],
        "summary": "In this paper we present a lexicon-based approach to the problem of morphological processing. Full-form words, lemmas and grammatical tags are interconnected in a DAWG. Thus, the process of analysis/synthesis is reduced to a search in the graph, which is very fast and can be performed even if several pieces of information are missing from the input. The contents of the DAWG are updated using an on-line incremental process. The proposed approach is language independent and it does not utilize any morphophonetic rules or any other special linguistic information.",
        "published": "2001-12-10T13:01:21Z",
        "link": "http://arxiv.org/abs/cs/0112010v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "I.2.7; F.2.2; E.1"
        ]
    },
    {
        "title": "Fast Context-Free Grammar Parsing Requires Fast Boolean Matrix   Multiplication",
        "authors": [
            "Lillian Lee"
        ],
        "summary": "In 1975, Valiant showed that Boolean matrix multiplication can be used for parsing context-free grammars (CFGs), yielding the asympotically fastest (although not practical) CFG parsing algorithm known. We prove a dual result: any CFG parser with time complexity $O(g n^{3 - \\epsilson})$, where $g$ is the size of the grammar and $n$ is the length of the input string, can be efficiently converted into an algorithm to multiply $m \\times m$ Boolean matrices in time $O(m^{3 - \\epsilon/3})$.   Given that practical, substantially sub-cubic Boolean matrix multiplication algorithms have been quite difficult to find, we thus explain why there has been little progress in developing practical, substantially sub-cubic general CFG parsers. In proving this result, we also develop a formalization of the notion of parsing.",
        "published": "2001-12-15T19:43:17Z",
        "link": "http://arxiv.org/abs/cs/0112018v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "I.2.7; F.2.2"
        ]
    },
    {
        "title": "Communities of Practice in the Distributed International Environment",
        "authors": [
            "Paul Hildreth",
            "Chris Kimble",
            "Peter Wright"
        ],
        "summary": "Modern commercial organisations are facing pressures which have caused them to lose personnel. When they lose people, they also lose their knowledge. Organisations also have to cope with the internationalisation of business forcing collaboration and knowledge sharing across time and distance. Knowledge Management (KM) claims to tackle these issues. This paper looks at an area where KM does not offer sufficient support, that is, the sharing of knowledge that is not easy to articulate.   The focus in this paper is on Communities of Practice in commercial organisations. We do this by exploring knowledge sharing in Lave and Wenger's [1] theory of Communities of Practice and investigating how Communities of Practice may translate to a distributed international environment. The paper reports on two case studies that explore the functioning of Communities of Practice across international boundaries.",
        "published": "2001-01-16T12:18:12Z",
        "link": "http://arxiv.org/abs/cs/0101012v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.5.3"
        ]
    },
    {
        "title": "On the Automated Classification of Web Sites",
        "authors": [
            "John M. Pierre"
        ],
        "summary": "In this paper we discuss several issues related to automated text classification of web sites. We analyze the nature of web content and metadata in relation to requirements for text features. We find that HTML metatags are a good source of text features, but are not in wide use despite their role in search engine rankings. We present an approach for targeted spidering including metadata extraction and opportunistic crawling of specific semantic hyperlinks. We describe a system for automatically classifying web sites into industry categories and present performance results based on different combinations of text features and training data. This system can serve as the basis for a generalized framework for automated metadata creation.",
        "published": "2001-02-01T23:03:49Z",
        "link": "http://arxiv.org/abs/cs/0102002v1",
        "categories": [
            "cs.IR",
            "H.3.3; I.5.2; H.5.4"
        ]
    },
    {
        "title": "Event Indexing Systems for Efficient Selection and Analysis of HERA Data",
        "authors": [
            "L. A. T. Bauerdick",
            "Adrian Fox-Murphy",
            "Tobias Haas",
            "Stefan Stonjek",
            "Enrico Tassi"
        ],
        "summary": "The design and implementation of two software systems introduced to improve the efficiency of offline analysis of event data taken with the ZEUS Detector at the HERA electron-proton collider at DESY are presented. Two different approaches were made, one using a set of event directories and the other using a tag database based on a commercial object-oriented database management system. These are described and compared. Both systems provide quick direct access to individual collision events in a sequential data store of several terabytes, and they both considerably improve the event analysis efficiency. In particular the tag database provides a very flexible selection mechanism and can dramatically reduce the computing time needed to extract small subsamples from the total event sample. Gains as large as a factor 20 have been obtained.",
        "published": "2001-04-03T16:27:48Z",
        "link": "http://arxiv.org/abs/cs/0104008v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.2.4; H.3.1; H.3.3; H.3.4; J.2; H.2.8"
        ]
    },
    {
        "title": "Evaluating Recommendation Algorithms by Graph Analysis",
        "authors": [
            "Batul J. Mirza",
            "Benjamin J. Keller",
            "Naren Ramakrishnan"
        ],
        "summary": "We present a novel framework for evaluating recommendation algorithms in terms of the `jumps' that they make to connect people to artifacts. This approach emphasizes reachability via an algorithm within the implicit graph structure underlying a recommender dataset, and serves as a complement to evaluation in terms of predictive accuracy. The framework allows us to consider questions relating algorithmic parameters to properties of the datasets. For instance, given a particular algorithm `jump,' what is the average path length from a person to an artifact? Or, what choices of minimum ratings and jumps maintain a connected graph? We illustrate the approach with a common jump called the `hammock' using movie recommender datasets.",
        "published": "2001-04-03T22:07:28Z",
        "link": "http://arxiv.org/abs/cs/0104009v1",
        "categories": [
            "cs.IR",
            "cs.DM",
            "cs.DS",
            "H.4.2"
        ]
    },
    {
        "title": "File mapping Rule-based DBMS and Natural Language Processing",
        "authors": [
            "Vjacheslav M. Novikov"
        ],
        "summary": "This paper describes the system of storage, extract and processing of information structured similarly to the natural language. For recursive inference the system uses the rules having the same representation, as the data. The environment of storage of information is provided with the File Mapping (SHM) mechanism of operating system. In the paper the main principles of construction of dynamic data structure and language for record of the inference rules are stated; the features of available implementation are considered and the description of the application realizing semantic information retrieval on the natural language is given.",
        "published": "2001-06-10T14:56:51Z",
        "link": "http://arxiv.org/abs/cs/0106016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "cs.PL",
            "D.3.2; H.2.4"
        ]
    },
    {
        "title": "location.location.location: Internet Addresses as Evolving Property",
        "authors": [
            "Kenton K. Yee"
        ],
        "summary": "I describe recent developments in the rules governing registration and ownership of Internet and World Wide Web addresses or \"domain names.\" I consider the idea that \"virtual\" properties like domain names are more similar to real estate than to trademarks. Therefore, it would be economically efficient to grant domain name owners stronger rights than those of trademarks and copyright holders.",
        "published": "2001-06-14T02:02:11Z",
        "link": "http://arxiv.org/abs/cs/0106033v2",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.IR",
            "C.0; J.4; K.0; K.1; K.4; K.4.4; K.5; K.5.1; K.7.4"
        ]
    },
    {
        "title": "Iterative Residual Rescaling: An Analysis and Generalization of LSI",
        "authors": [
            "Rie Kubota Ando",
            "Lillian Lee"
        ],
        "summary": "We consider the problem of creating document representations in which inter-document similarity measurements correspond to semantic similarity. We first present a novel subspace-based framework for formalizing this task. Using this framework, we derive a new analysis of Latent Semantic Indexing (LSI), showing a precise relationship between its performance and the uniformity of the underlying distribution of documents over topics. This analysis helps explain the improvements gained by Ando's (2000) Iterative Residual Rescaling (IRR) algorithm: IRR can compensate for distributional non-uniformity. A further benefit of our framework is that it provides a well-motivated, effective method for automatically determining the rescaling factor IRR depends on, leading to further improvements. A series of experiments over various settings and with several evaluation metrics validates our claims.",
        "published": "2001-06-17T20:45:37Z",
        "link": "http://arxiv.org/abs/cs/0106039v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.7"
        ]
    },
    {
        "title": "Coupled Clustering: a Method for Detecting Structural Correspondence",
        "authors": [
            "Zvika Marx",
            "Ido Dagan",
            "Joachim Buhmann"
        ],
        "summary": "This paper proposes a new paradigm and computational framework for identification of correspondences between sub-structures of distinct composite systems. For this, we define and investigate a variant of traditional data clustering, termed coupled clustering, which simultaneously identifies corresponding clusters within two data sets. The presented method is demonstrated and evaluated for detecting topical correspondences in textual corpora.",
        "published": "2001-07-23T11:06:45Z",
        "link": "http://arxiv.org/abs/cs/0107032v1",
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.6; I.2.7; I.5.3; I.5.4"
        ]
    },
    {
        "title": "The Partial Evaluation Approach to Information Personalization",
        "authors": [
            "Naren Ramakrishnan",
            "Saverio Perugini"
        ],
        "summary": "Information personalization refers to the automatic adjustment of information content, structure, and presentation tailored to an individual user. By reducing information overload and customizing information access, personalization systems have emerged as an important segment of the Internet economy. This paper presents a systematic modeling methodology - PIPE (`Personalization is Partial Evaluation') - for personalization. Personalization systems are designed and implemented in PIPE by modeling an information-seeking interaction in a programmatic representation. The representation supports the description of information-seeking activities as partial information and their subsequent realization by partial evaluation, a technique for specializing programs. We describe the modeling methodology at a conceptual level and outline representational choices. We present two application case studies that use PIPE for personalizing web sites and describe how PIPE suggests a novel evaluation criterion for information system designs. Finally, we mention several fundamental implications of adopting the PIPE model for personalization and when it is (and is not) applicable.",
        "published": "2001-08-07T20:27:39Z",
        "link": "http://arxiv.org/abs/cs/0108003v1",
        "categories": [
            "cs.IR",
            "cs.PL",
            "D.3.4; H.4.2; H5.2; H5.4"
        ]
    },
    {
        "title": "Links tell us about lexical and semantic Web content",
        "authors": [
            "Filippo Menczer"
        ],
        "summary": "The latest generation of Web search tools is beginning to exploit hypertext link information to improve ranking\\cite{Brin98,Kleinberg98} and crawling\\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden assumption behind such approaches, a correlation between the graph structure of the Web and its content, has not been tested explicitly despite increasing research on Web topology\\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I formalize and quantitatively validate two conjectures drawing connections from link information to lexical and semantic Web content. The clink-content conjecture states that a page is similar to the pages that link to it, i.e., one can infer the lexical content of a page by looking at the pages that link to it. I also show that lexical inferences based on link cues are quite heterogeneous across Web communities. The link-cluster conjecture states that pages about the same topic are clustered together, i.e., one can infer the meaning of a page by looking at its neighbours. These results explain the success of the newest search technologies and open the way for more dynamic and scalable methods to locate information in a topic or user driven way.",
        "published": "2001-08-08T02:16:15Z",
        "link": "http://arxiv.org/abs/cs/0108004v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "Bipartite graph partitioning and data clustering",
        "authors": [
            "H. Zha",
            "X. He",
            "C. Ding",
            "M. Gu",
            "H. Simon"
        ],
        "summary": "Many data types arising from data mining applications can be modeled as bipartite graphs, examples include terms and documents in a text corpus, customers and purchasing items in market basket analysis and reviewers and movies in a movie recommender system. In this paper, we propose a new data clustering method based on partitioning the underlying bipartite graph. The partition is constructed by minimizing a normalized sum of edge weights between unmatched pairs of vertices of the bipartite graph. We show that an approximate solution to the minimization problem can be obtained by computing a partial singular value decomposition (SVD) of the associated edge weight matrix of the bipartite graph. We point out the connection of our clustering algorithm to correspondence analysis used in multivariate analysis. We also briefly discuss the issue of assigning data objects to multiple clusters. In the experimental results, we apply our clustering algorithm to the problem of document clustering to illustrate its effectiveness and efficiency.",
        "published": "2001-08-27T13:07:44Z",
        "link": "http://arxiv.org/abs/cs/0108018v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.3; G.1.3; G.2.2"
        ]
    },
    {
        "title": "Information Extraction Using the Structured Language Model",
        "authors": [
            "Ciprian Chelba",
            "Milind Mahajan"
        ],
        "summary": "The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser. The task of template filling is cast as constrained parsing using the SLM. The model is automatically trained from a set of sentences annotated with frame/slot labels and spans. Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage. Despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad --- personal information management --- task.",
        "published": "2001-08-29T04:00:24Z",
        "link": "http://arxiv.org/abs/cs/0108023v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "I.2.7"
        ]
    },
    {
        "title": "Conceptual Analysis of Lexical Taxonomies: The Case of WordNet Top-Level",
        "authors": [
            "Aldo Gangemi",
            "Nicola Guarino",
            "Alessandro Oltramari"
        ],
        "summary": "In this paper we propose an analysis and an upgrade of WordNet's top-level synset taxonomy. We briefly review WordNet and identify its main semantic limitations. Some principles from a forthcoming OntoClean methodology are applied to the ontological analysis of WordNet. A revised top-level taxonomy is proposed, which is meant to be more conceptually rigorous, cognitively transparent, and efficiently exploitable in several applications.",
        "published": "2001-09-11T12:17:04Z",
        "link": "http://arxiv.org/abs/cs/0109013v1",
        "categories": [
            "cs.CL",
            "cs.IR",
            "H3.1"
        ]
    },
    {
        "title": "Information retrieval in Current Research Information Systems",
        "authors": [
            "Andrei Lopatenko"
        ],
        "summary": "In this paper we describe the requirements for research information systems and problems which arise in the development of such system. Here is shown which problems could be solved by using of knowledge markup technologies. Ontology for Research Information System offered. Architecture for collecting research data and providing access to it is described.",
        "published": "2001-10-10T15:28:00Z",
        "link": "http://arxiv.org/abs/cs/0110026v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.3; H.3.4; H.3.7"
        ]
    },
    {
        "title": "Towards Solving the Interdisciplinary Language Barrier Problem",
        "authors": [
            "Sebastien Paquet"
        ],
        "summary": "This work aims to make it easier for a specialist in one field to find and explore ideas from another field which may be useful in solving a new problem arising in his practice. It presents a methodology which serves to represent the relationships that exist between concepts, problems, and solution patterns from different fields of human activity in the form of a graph. Our approach is based upon generalization and specialization relationships and problem solving. It is simple enough to be understood quite easily, and general enough to enable coherent integration of concepts and problems from virtually any field. We have built an implementation which uses the World Wide Web as a support to allow navigation between graph nodes and collaborative development of the graph.",
        "published": "2001-10-18T19:39:22Z",
        "link": "http://arxiv.org/abs/cs/0110041v1",
        "categories": [
            "cs.CY",
            "cs.CL",
            "cs.IR",
            "H3.1; I2.4; K3.1"
        ]
    },
    {
        "title": "Machine Learning in Automated Text Categorization",
        "authors": [
            "Fabrizio Sebastiani"
        ],
        "summary": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last ten years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert manpower, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely document representation, classifier construction, and classifier evaluation.",
        "published": "2001-10-26T09:27:48Z",
        "link": "http://arxiv.org/abs/cs/0110053v1",
        "categories": [
            "cs.IR",
            "cs.LG",
            "H.3.1;H.3.3;I.2.3"
        ]
    },
    {
        "title": "Explaining Scenarios for Information Personalization",
        "authors": [
            "Naren Ramakrishnan",
            "Mary Beth Rosson",
            "John M. Carroll"
        ],
        "summary": "Personalization customizes information access. The PIPE (\"Personalization is Partial Evaluation\") modeling methodology represents interaction with an information space as a program. The program is then specialized to a user's known interests or information seeking activity by the technique of partial evaluation. In this paper, we elaborate PIPE by considering requirements analysis in the personalization lifecycle. We investigate the use of scenarios as a means of identifying and analyzing personalization requirements. As our first result, we show how designing a PIPE representation can be cast as a search within a space of PIPE models, organized along a partial order. This allows us to view the design of a personalization system, itself, as specialized interpretation of an information space. We then exploit the underlying equivalence of explanation-based generalization (EBG) and partial evaluation to realize high-level goals and needs identified in scenarios; in particular, we specialize (personalize) an information space based on the explanation of a user scenario in that information space, just as EBG specializes a theory based on the explanation of an example in that theory. In this approach, personalization becomes the transformation of information spaces to support the explanation of usage scenarios. An example application is described.",
        "published": "2001-11-05T19:02:47Z",
        "link": "http://arxiv.org/abs/cs/0111007v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.3.5; H.4.2; H.5.4; I.2.6; K.8"
        ]
    },
    {
        "title": "Intelligent Anticipated Exploration of Web Sites",
        "authors": [
            "Giovambattista Ianni"
        ],
        "summary": "In this paper we describe a web search agent, called Global Search Agent (hereafter GSA for short). GSA integrates and enhances several search techniques in order to achieve significant improvements in the user-perceived quality of delivered information as compared to usual web search engines. GSA features intelligent merging of relevant documents from different search engines, anticipated selective exploration and evaluation of links from the current result set, automated derivation of refined queries based on user relevance feedback. System architecture as well as experimental accounts are also illustrated.",
        "published": "2001-11-06T18:00:49Z",
        "link": "http://arxiv.org/abs/cs/0111012v1",
        "categories": [
            "cs.AI",
            "cs.IR",
            "I.2.11;H.3.3"
        ]
    },
    {
        "title": "Automated Debugging In Java Using OCL And JDI",
        "authors": [
            "David J. Murray",
            "Dale E. Parson"
        ],
        "summary": "Correctness constraints provide a foundation for automated debugging within object-oriented systems. This paper discusses a new approach to incorporating correctness constraints into Java development environments. Our approach uses the Object Constraint Language (\"OCL\") as a specification language and the Java Debug Interface (\"JDI\") as a verification API. OCL provides a standard language for expressing object-oriented constraints that can integrate with Unified Modeling Language (\"UML\") software models. JDI provides a standard Java API capable of supporting type-safe and side effect free runtime constraint evaluation. The resulting correctness constraint mechanism: (1) entails no programming language modifications; (2) requires neither access nor changes to existing source code; and (3) works with standard off-the-shelf Java virtual machines (\"VMs\"). A prototype correctness constraint auditor is presented to demonstrate the utility of this mechanism for purposes of automated debugging.",
        "published": "2001-01-03T22:48:24Z",
        "link": "http://arxiv.org/abs/cs/0101002v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Slicing Event Traces of Large Software Systems",
        "authors": [
            "Raymond Smith",
            "Bogdan Korel"
        ],
        "summary": "Debugging of large software systems consisting of many processes accessing shared resources is a very difficult task. Many commercial systems record essential events during system execution for post-mortem analysis. However, the event traces of large and long-running systems can be quite voluminous. Analysis of such event traces to identify sources of incorrect behavior can be very tedious, error-prone, and inefficient. In this paper, we propose a novel technique of slicing event traces as a means of reducing the number of events for analysis. This technique identifies events that may have influenced observed incorrect system behavior. In order to recognize influencing events several types of dependencies between events are identified. These dependencies are determined automatically from an event trace. In order to improve the precision of slicing we propose to use additional dependencies, referred to as cause-effect dependencies, which can further reduce the size of sliced event traces. Our initial experience has shown that this slicing technique can significantly reduce the size of event traces for analysis.",
        "published": "2001-01-11T19:12:20Z",
        "link": "http://arxiv.org/abs/cs/0101005v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Assertion checker for the C programming language based on computations   over event traces",
        "authors": [
            "Mikhail Auguston"
        ],
        "summary": "This paper suggests an approach to the development of software testing and debugging automation tools based on precise program behavior models. The program behavior model is defined as a set of events (event trace) with two basic binary relations over events -- precedence and inclusion, and represents the temporal relationship between actions. A language for the computations over event traces is developed that provides a basis for assertion checking, debugging queries, execution profiles, and performance measurements. The approach is nondestructive, since assertion texts are separated from the target program source code and can be maintained independently. Assertions can capture the dynamic properties of a particular target program and can formalize the general knowledge of typical bugs and debugging strategies. An event grammar provides a sound basis for assertion language implementation via target program automatic instrumentation. An implementation architecture and preliminary experiments with a prototype assertion checker for the C programming language are discussed.",
        "published": "2001-01-12T01:13:27Z",
        "link": "http://arxiv.org/abs/cs/0101007v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "A Knowledge-based Automated Debugger in Learning System",
        "authors": [
            "Abdullah Mohd Zin",
            "Syed Ahmad Aljunid",
            "Zarina Shukur",
            "Mohd Jan Nordin"
        ],
        "summary": "Currently, programming instructors continually face the problem of helping to debug students' programs. Although there currently exist a number of debuggers and debugging tools in various platforms, most of these projects or products are crafted through the needs of software maintenance, and not through the perspective of teaching of programming. Moreover, most debuggers are too general, meant for experts as well as not user-friendly. We propose a new knowledge-based automated debugger to be used as a user-friendly tool by the students to self-debug their own programs. Stereotyped code (cliche) and bugs cliche will be stored as library of plans in the knowledge-base. Recognition of correct code or bugs is based on pattern matching and constraint satisfaction. Given a syntax error-free program and its specification, this debugger called Adil (Automated Debugger in Learning system) will be able locate, pinpoint and explain logical errors of programs. If there are no errors, it will be able to explain the meaning of the program. Adil is based on the design of the Conceiver, an automated program understanding system developed at Universiti Kebangsaan Malaysia.",
        "published": "2001-01-12T08:58:58Z",
        "link": "http://arxiv.org/abs/cs/0101008v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Generation of and Debugging with Logical Pre and Postconditions",
        "authors": [
            "Angel Herrranz-Nieva Juan Jose Moreno Navarro"
        ],
        "summary": "This paper shows the debugging facilities provided by the SLAM system. The SLAM system includes i) a specification language that integrates algebraic specifications and model-based specifications using the object oriented model. Class operations are defined by using rules each of them with logical pre and postconditions but with a functional flavour. ii) A development environment that, among other features, is able to generate readable code in a high level object oriented language. iii) The generated code includes (part of) the pre and postconditions as assertions, that can be automatically checked in the debug mode execution of programs. We focus on this last aspect.   The SLAM language is expressive enough to describe many useful properties and these properties are translated into a Prolog program that is linked (via an adequate interface) with the user program. The debugging execution of the program interacts with the Prolog engine which is responsible for checking properties.",
        "published": "2001-01-12T15:32:42Z",
        "link": "http://arxiv.org/abs/cs/0101009v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.5, F.3.1"
        ]
    },
    {
        "title": "Semantics and Termination of Simply-Moded Logic Programs with Dynamic   Scheduling",
        "authors": [
            "Annalisa Bossi",
            "Sandro Etalle",
            "Sabina Rossi",
            "Jan-Georg Smaus"
        ],
        "summary": "In logic programming, dynamic scheduling refers to a situation where the selection of the atom in each resolution (computation) step is determined at runtime, as opposed to a fixed selection rule such as the left-to-right one of Prolog. This has applications e.g. in parallel programming. A mechanism to control dynamic scheduling is provided in existing languages in the form of delay declarations.   Input-consuming derivations were introduced to describe dynamic scheduling while abstracting from the technical details. In this paper, we first formalise the relationship between delay declarations and input-consuming derivations, showing in many cases a one-to-one correspondence. Then, we define a model-theoretic semantics for input-consuming derivations of simply-moded programs. Finally, for this class of programs, we provide a necessary and sufficient criterion for termination.",
        "published": "2001-01-23T12:50:12Z",
        "link": "http://arxiv.org/abs/cs/0101022v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.3; D.1.6; F.3.2"
        ]
    },
    {
        "title": "Properties of Input-Consuming Derivations",
        "authors": [
            "Annalisa Bossi",
            "Sandro Etalle",
            "Sabina Rossi"
        ],
        "summary": "We study the properties of input-consuming derivations of moded logic programs. Input-consuming derivations can be used to model the behavior of logic programs using dynamic scheduling and employing constructs such as delay declarations.   We consider the class of nicely-moded programs and queries. We show that for these programs a weak version of the well-known switching lemma holds also for input-consuming derivations. Furthermore, we show that, under suitable conditions, there exists an algebraic characterization of termination of input-consuming derivations.",
        "published": "2001-01-23T13:35:27Z",
        "link": "http://arxiv.org/abs/cs/0101023v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6;D.3.1;F.3.2"
        ]
    },
    {
        "title": "Decomposing Non-Redundant Sharing by Complementation",
        "authors": [
            "Enea Zaffanella",
            "Patricia M. Hill",
            "Roberto Bagnara"
        ],
        "summary": "Complementation, the inverse of the reduced product operation, is a technique for systematically finding minimal decompositions of abstract domains. File' and Ranzato advanced the state of the art by introducing a simple method for computing a complement. As an application, they considered the extraction by complementation of the pair-sharing domain PS from the Jacobs and Langen's set-sharing domain SH. However, since the result of this operation was still SH, they concluded that PS was too abstract for this. Here, we show that the source of this result lies not with PS but with SH and, more precisely, with the redundant information contained in SH with respect to ground-dependencies and pair-sharing. In fact, a proper decomposition is obtained if the non-redundant version of SH, PSD, is substituted for SH. To establish the results for PSD, we define a general schema for subdomains of SH that includes PSD and Def as special cases. This sheds new light on the structure of PSD and exposes a natural though unexpected connection between Def and PSD. Moreover, we substantiate the claim that complementation alone is not sufficient to obtain truly minimal decompositions of domains. The right solution to this problem is to first remove redundancies by computing the quotient of the domain with respect to the observable behavior, and only then decompose it by complementation.",
        "published": "2001-01-23T14:20:08Z",
        "link": "http://arxiv.org/abs/cs/0101025v1",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "An Effective Fixpoint Semantics for Linear Logic Programs",
        "authors": [
            "Marco Bozzano",
            "Giorgio Delzanno",
            "Maurizio Martelli"
        ],
        "summary": "In this paper we investigate the theoretical foundation of a new bottom-up semantics for linear logic programs, and more precisely for the fragment of LinLog that consists of the language LO enriched with the constant 1. We use constraints to symbolically and finitely represent possibly infinite collections of provable goals. We define a fixpoint semantics based on a new operator in the style of Tp working over constraints. An application of the fixpoint operator can be computed algorithmically. As sufficient conditions for termination, we show that the fixpoint computation is guaranteed to converge for propositional LO. To our knowledge, this is the first attempt to define an effective fixpoint semantics for linear logic programs. As an application of our framework, we also present a formal investigation of the relations between LO and Disjunctive Logic Programming. Using an approach based on abstract interpretation, we show that DLP fixpoint semantics can be viewed as an abstraction of our semantics for LO. We prove that the resulting abstraction is correct and complete for an interesting class of LO programs encoding Petri Nets.",
        "published": "2001-02-23T17:42:36Z",
        "link": "http://arxiv.org/abs/cs/0102025v2",
        "categories": [
            "cs.PL",
            "D.3.1;F.3.1;F.3.2"
        ]
    },
    {
        "title": "Soundness, Idempotence and Commutativity of Set-Sharing",
        "authors": [
            "Patricia M. Hill",
            "Roberto Bagnara",
            "Enea Zaffanella"
        ],
        "summary": "It is important that practical data-flow analyzers are backed by reliably proven theoretical results. Abstract interpretation provides a sound mathematical framework and necessary generic properties for an abstract domain to be well-defined and sound with respect to the concrete semantics. In logic programming, the abstract domain Sharing is a standard choice for sharing analysis for both practical work and further theoretical study. In spite of this, we found that there were no satisfactory proofs for the key properties of commutativity and idempotence that are essential for Sharing to be well-defined and that published statements of the soundness of Sharing assume the occurs-check. This paper provides a generalization of the abstraction function for Sharing that can be applied to any language, with or without the occurs-check. Results for soundness, idempotence and commutativity for abstract unification using this abstraction function are proven.",
        "published": "2001-02-27T14:54:34Z",
        "link": "http://arxiv.org/abs/cs/0102030v1",
        "categories": [
            "cs.PL",
            "F.3.2"
        ]
    },
    {
        "title": "The Limits of Horn Logic Programs",
        "authors": [
            "Shilong Ma",
            "Yuefei Sui",
            "Ke Xu"
        ],
        "summary": "Given a sequence $\\{\\Pi_n\\}$ of Horn logic programs, the limit $\\Pi$ of $\\{\\Pi_n\\}$ is the set of the clauses such that every clause in $\\Pi$ belongs to almost every $\\Pi_n$ and every clause in infinitely many $\\Pi_n$'s belongs to $\\Pi$ also. The limit program $\\Pi$ is still Horn but may be infinite. In this paper, we consider if the least Herbrand model of the limit of a given Horn logic program sequence $\\{\\Pi_n\\}$ equals the limit of the least Herbrand models of each logic program $\\Pi_n$. It is proved that this property is not true in general but holds if Horn logic programs satisfy an assumption which can be syntactically checked and be satisfied by a class of Horn logic programs. Thus, under this assumption we can approach the least Herbrand model of the limit $\\Pi$ by the sequence of the least Herbrand models of each finite program $\\Pi_n$. We also prove that if a finite Horn logic program satisfies this assumption, then the least Herbrand model of this program is recursive. Finally, by use of the concept of stability from dynamical systems, we prove that this assumption is exactly a sufficient condition to guarantee the stability of fixed points for Horn logic programs.",
        "published": "2001-03-08T07:42:48Z",
        "link": "http://arxiv.org/abs/cs/0103008v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; F.3.2"
        ]
    },
    {
        "title": "Toward an architecture for quantum programming",
        "authors": [
            "S. Bettelli",
            "L. Serafini",
            "T. Calarco"
        ],
        "summary": "It is becoming increasingly clear that, if a useful device for quantum computation will ever be built, it will be embodied by a classical computing machine with control over a truly quantum subsystem, this apparatus performing a mixture of classical and quantum computation.   This paper investigates a possible approach to the problem of programming such machines: a template high level quantum language is presented which complements a generic general purpose classical language with a set of quantum primitives. The underlying scheme involves a run-time environment which calculates the byte-code for the quantum operations and pipes it to a quantum device controller or to a simulator.   This language can compactly express existing quantum algorithms and reduce them to sequences of elementary operations; it also easily lends itself to automatic, hardware independent, circuit simplification. A publicly available preliminary implementation of the proposed ideas has been realized using the C++ language.",
        "published": "2001-03-08T19:24:44Z",
        "link": "http://arxiv.org/abs/cs/0103009v3",
        "categories": [
            "cs.PL",
            "quant-ph",
            "D.3.1"
        ]
    },
    {
        "title": "Chain Programs for Writing Deterministic Metainterpreters",
        "authors": [
            "David A. Rosenblueth"
        ],
        "summary": "Many metainterpreters found in the logic programming literature are nondeterministic in the sense that the selection of program clauses is not determined. Examples are the familiar \"demo\" and \"vanilla\" metainterpreters. For some applications this nondeterminism is convenient. In some cases, however, a deterministic metainterpreter, having an explicit selection of clauses, is needed. Such cases include (1) conversion of OR parallelism into AND parallelism for \"committed-choice\" processors, (2) logic-based, imperative-language implementation of search strategies, and (3) simulation of bounded-resource reasoning.   Deterministic metainterpreters are difficult to write because the programmer must be concerned about the set of unifiers of the children of a node in the derivation tree. We argue that it is both possible and advantageous to write these metainterpreters by reasoning in terms of object programs converted into a syntactically restricted form that we call \"chain\" form, where we can forget about unification, except for unit clauses. We give two transformations converting logic programs into chain form, one for \"moded\" programs (implicit in two existing exhaustive-traversal methods for committed-choice execution), and one for arbitrary definite programs. As illustrations of our approach we show examples of the three applications mentioned above.",
        "published": "2001-04-02T23:34:01Z",
        "link": "http://arxiv.org/abs/cs/0104003v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; F.3.2; D.3.2; D.3.4"
        ]
    },
    {
        "title": "Reverse Engineering from Assembler to Formal Specifications via Program   Transformations",
        "authors": [
            "M. P. Ward"
        ],
        "summary": "The FermaT transformation system, based on research carried out over the last sixteen years at Durham University, De Montfort University and Software Migrations Ltd., is an industrial-strength formal transformation engine with many applications in program comprehension and language migration. This paper is a case study which uses automated plus manually-directed transformations and abstractions to convert an IBM 370 Assembler code program into a very high-level abstract specification.",
        "published": "2001-05-04T09:21:21Z",
        "link": "http://arxiv.org/abs/cs/0105006v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.7;D.3.2"
        ]
    },
    {
        "title": "Component Programming and Interoperability in Constraint Solver Design",
        "authors": [
            "Frederic Goualard"
        ],
        "summary": "Prolog was once the main host for implementing constraint solvers.   It seems that it is no longer so. To be useful, constraint solvers have to be integrable into industrial applications written in imperative or object-oriented languages; to be efficient, they have to interact with other solvers. To meet these requirements, many solvers are now implemented in the form of extensible object-oriented libraries. Following Pfister and Szyperski, we argue that ``objects are not enough,'' and we propose to design solvers as component-oriented libraries. We illustrate our approach by the description of the architecture of a prototype, and we assess its strong points and weaknesses.",
        "published": "2001-05-07T12:56:11Z",
        "link": "http://arxiv.org/abs/cs/0105011v1",
        "categories": [
            "cs.PL",
            "D.3.3; D.2"
        ]
    },
    {
        "title": "The alldifferent Constraint: A Survey",
        "authors": [
            "W. J. van Hoeve"
        ],
        "summary": "The constraint of difference is known to the constraint programming community since Lauriere introduced Alice in 1978. Since then, several solving strategies have been designed for this constraint. In this paper we give both a practical overview and an abstract comparison of these different strategies.",
        "published": "2001-05-08T13:13:04Z",
        "link": "http://arxiv.org/abs/cs/0105015v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.3"
        ]
    },
    {
        "title": "A Logical Framework for Convergent Infinite Computations",
        "authors": [
            "Wei Li",
            "Shilong Ma",
            "Yuefei Sui",
            "Ke Xu"
        ],
        "summary": "Classical computations can not capture the essence of infinite computations very well. This paper will focus on a class of infinite computations called convergent infinite computations}. A logic for convergent infinite computations is proposed by extending first order theories using Cauchy sequences, which has stronger expressive power than the first order logic. A class of fixed points characterizing the logical properties of the limits can be represented by means of infinite-length terms defined by Cauchy sequences. We will show that the limit of sequence of first order theories can be defined in terms of distance, similar to the $\\epsilon-N$ style definition of limits in real analysis. On the basis of infinitary terms, a computation model for convergent infinite computations is proposed. Finally, the interpretations of logic programs are extended by introducing real Herbrand models of logic programs and a sufficient condition for computing a real Herbrand model of Horn logic programs using convergent infinite computation is given.",
        "published": "2001-05-10T03:58:46Z",
        "link": "http://arxiv.org/abs/cs/0105020v3",
        "categories": [
            "cs.LO",
            "cs.PL",
            "F.4.1, D.1.6"
        ]
    },
    {
        "title": "Constraint Propagation in Presence of Arrays",
        "authors": [
            "Sebastian Brand"
        ],
        "summary": "We describe the use of array expressions as constraints, which represents a consequent generalisation of the \"element\" constraint. Constraint propagation for array constraints is studied theoretically, and for a set of domain reduction rules the local consistency they enforce, arc-consistency, is proved. An efficient algorithm is described that encapsulates the rule set and so inherits the capability to enforce arc-consistency from the rules.",
        "published": "2001-05-14T13:35:03Z",
        "link": "http://arxiv.org/abs/cs/0105024v1",
        "categories": [
            "cs.PL",
            "cs.DS",
            "D.3.3; E.1"
        ]
    },
    {
        "title": "Soft Scheduling",
        "authors": [
            "Hana Rudova"
        ],
        "summary": "Classical notions of disjunctive and cumulative scheduling are studied from the point of view of soft constraint satisfaction. Soft disjunctive scheduling is introduced as an instance of soft CSP and preferences included in this problem are applied to generate a lower bound based on existing discrete capacity resource. Timetabling problems at Purdue University and Faculty of Informatics at Masaryk University considering individual course requirements of students demonstrate practical problems which are solved via proposed methods. Implementation of general preference constraint solver is discussed and first computational results for timetabling problem are presented.",
        "published": "2001-06-02T22:36:02Z",
        "link": "http://arxiv.org/abs/cs/0106004v1",
        "categories": [
            "cs.AI",
            "cs.PL",
            "I.2.8; F.4.1; I.6.5"
        ]
    },
    {
        "title": "Computing Functional and Relational Box Consistency by Structured   Propagation in Atomic Constraint Systems",
        "authors": [
            "M. H. van Emden"
        ],
        "summary": "Box consistency has been observed to yield exponentially better performance than chaotic constraint propagation in the interval constraint system obtained by decomposing the original expression into primitive constraints. The claim was made that the improvement is due to avoiding decomposition. In this paper we argue that the improvement is due to replacing chaotic iteration by a more structured alternative.   To this end we distinguish the existing notion of box consistency from relational box consistency. We show that from a computational point of view it is important to maintain the functional structure in constraint systems that are associated with a system of equations. So far, it has only been considered computationally important that constraint propagation be fair. With the additional structure of functional constraint systems, one can define and implement computationally effective, structured, truncated constraint propagations. The existing algorithm for box consistency is one such. Our results suggest that there are others worth investigating.",
        "published": "2001-06-07T14:50:40Z",
        "link": "http://arxiv.org/abs/cs/0106008v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3; F.4.1"
        ]
    },
    {
        "title": "The Set of Equations to Evaluate Objects",
        "authors": [
            "Larissa Ismailova"
        ],
        "summary": "The notion of an equational shell is studied to involve the objects and their environment. Appropriate methods are studied as valid embeddings of refined objects. The refinement process determines the linkages between the variety of possible representations giving rise to variants of computations. The case study is equipped with the adjusted equational systems that validate the initial applicative framework.",
        "published": "2001-06-08T09:28:46Z",
        "link": "http://arxiv.org/abs/cs/0106013v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.3.1; D.3.2"
        ]
    },
    {
        "title": "File mapping Rule-based DBMS and Natural Language Processing",
        "authors": [
            "Vjacheslav M. Novikov"
        ],
        "summary": "This paper describes the system of storage, extract and processing of information structured similarly to the natural language. For recursive inference the system uses the rules having the same representation, as the data. The environment of storage of information is provided with the File Mapping (SHM) mechanism of operating system. In the paper the main principles of construction of dynamic data structure and language for record of the inference rules are stated; the features of available implementation are considered and the description of the application realizing semantic information retrieval on the natural language is given.",
        "published": "2001-06-10T14:56:51Z",
        "link": "http://arxiv.org/abs/cs/0106016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "cs.PL",
            "D.3.2; H.2.4"
        ]
    },
    {
        "title": "An object evaluator to generate flexible applications",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko"
        ],
        "summary": "This paper contains a brief discussion of an object evaluator which is based on principles of evaluations in a category. The main tool system referred as the Application Development Environment (ADE) is used to build database applications involving the graphical user interface (GUI). The separation of a database access and the user interface is reached by distinguishing the potential and actual objects. The variety of applications may be generated that communicate with different and distinct desktop databases. The commutative diagrams' technique allows to involve retrieval and call of the delayed procedures.",
        "published": "2001-06-10T16:52:29Z",
        "link": "http://arxiv.org/abs/cs/0106017v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "H.1; H.2"
        ]
    },
    {
        "title": "Building the access pointers to a computation environment",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "A common object technique equipped with the categorical and computational styles is briefly outlined. An object is evaluated by embedding in a host computational environment which is the domain-ranged structure. An embedded object is accessed by the pointers generated within the host system. To assist with an easy extract the result of the evaluation a pre-embedded object is generated. It is observed as the decomposition into substitutional part and access function part which are generated during the object evaluation.",
        "published": "2001-06-10T17:31:13Z",
        "link": "http://arxiv.org/abs/cs/0106018v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Object-oriented solutions",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "In this paper are briefly outlined the motivations, mathematical ideas in use, pre-formalization and assumptions, object-as-functor construction, `soft' types and concept constructions, case study for concepts based on variable domains, extracting a computational background, and examples of evaluations.",
        "published": "2001-06-11T11:46:14Z",
        "link": "http://arxiv.org/abs/cs/0106021v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Object-oriented tools for advanced applications",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko"
        ],
        "summary": "This paper contains a brief discussion of the Application Development Environment (ADE) that is used to build database applications involving the graphical user interface (GUI). ADE computing separates the database access and the user interface. The variety of applications may be generated that communicate with different and distinct desktop databases. The advanced techniques allows to involve remote or stored procedures retrieval and call.",
        "published": "2001-06-11T17:25:25Z",
        "link": "http://arxiv.org/abs/cs/0106023v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Objects and their computational framework",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "Most of the object notions are embedded into a logical domain, especially when dealing with a database theory. Thus, their properties within a computational domain are not yet studied properly. The main topic of this paper is to analyze different concepts of the distinct computational primitive frames to extract the useful object properties and their possible advantages. Some important metaoperators are used to unify the approaches and to establish their possible correspondences.",
        "published": "2001-06-11T17:35:18Z",
        "link": "http://arxiv.org/abs/cs/0106024v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Event Driven Computations for Relational Query Language",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko",
            "Lioubouv Bourmistrova"
        ],
        "summary": "This paper deals with an extended model of computations which uses the parameterized families of entities for data objects and reflects a preliminary outline of this problem. Some topics are selected out, briefly analyzed and arranged to cover a general problem. The authors intended more to discuss the particular topics, their interconnection and computational meaning as a panel proposal, so that this paper is not yet to be evaluated as a closed journal paper. To save space all the technical and implementation features are left for the future paper.   Data object is a schematic entity and modelled by the partial function. A notion of type is extended by the variable domains which depend on events and types. A variable domain is built from the potential and schematic individuals and generates the valid families of types depending on a sequence of events. Each valid type consists of the actual individuals which are actual relatively the event or script. In case when a type depends on the script then corresponding view for data objects is attached, otherwise a snapshot is generated. The type thus determined gives an upper range for typed variables so that the local ranges are event driven resulting is the families of actual individuals. An expressive power of the query language is extended using the extensional and intentional relations.",
        "published": "2001-06-12T10:18:36Z",
        "link": "http://arxiv.org/abs/cs/0106026v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Classes of Terminating Logic Programs",
        "authors": [
            "Dino Pedreschi",
            "Salvatore Ruggieri",
            "Jan-Georg Smaus"
        ],
        "summary": "Termination of logic programs depends critically on the selection rule, i.e. the rule that determines which atom is selected in each resolution step. In this article, we classify programs (and queries) according to the selection rules for which they terminate. This is a survey and unified view on different approaches in the literature. For each class, we present a sufficient, for most classes even necessary, criterion for determining that a program is in that class. We study six classes: a program strongly terminates if it terminates for all selection rules; a program input terminates if it terminates for selection rules which only select atoms that are sufficiently instantiated in their input positions, so that these arguments do not get instantiated any further by the unification; a program local delay terminates if it terminates for local selection rules which only select atoms that are bounded w.r.t. an appropriate level mapping; a program left-terminates if it terminates for the usual left-to-right selection rule; a program exists-terminates if there exists a selection rule for which it terminates; finally, a program has bounded nondeterminism if it only has finitely many refutations. We propose a semantics-preserving transformation from programs with bounded nondeterminism into strongly terminating programs. Moreover, by unifying different formalisms and making appropriate assumptions, we are able to establish a formal hierarchy between the different classes.",
        "published": "2001-06-25T16:32:52Z",
        "link": "http://arxiv.org/abs/cs/0106050v2",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; D.2.4; F.3.1"
        ]
    },
    {
        "title": "Acceptability with general orderings",
        "authors": [
            "Danny De Schreye",
            "Alexander Serebrenik"
        ],
        "summary": "We present a new approach to termination analysis of logic programs. The essence of the approach is that we make use of general orderings (instead of level mappings), like it is done in transformational approaches to logic program termination analysis, but we apply these orderings directly to the logic program and not to the term-rewrite system obtained through some transformation. We define some variants of acceptability, based on general orderings, and show how they are equivalent to LD-termination. We develop a demand driven, constraint-based approach to verify these acceptability-variants.   The advantage of the approach over standard acceptability is that in some cases, where complex level mappings are needed, fairly simple orderings may be easily generated. The advantage over transformational approaches is that it avoids the transformation step all together.   {\\bf Keywords:} termination analysis, acceptability, orderings.",
        "published": "2001-06-26T12:34:24Z",
        "link": "http://arxiv.org/abs/cs/0106052v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "Inference of termination conditions for numerical loops",
        "authors": [
            "Alexander Serebrenik",
            "Danny De Schreye"
        ],
        "summary": "We present a new approach to termination analysis of numerical computations in logic programs. Traditional approaches fail to analyse them due to non well-foundedness of the integers. We present a technique that allows to overcome these difficulties. Our approach is based on transforming a program in way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query-mapping pairs with the well-known framework of acceptability. Such an integration not only contributes to the understanding of termination behaviour of numerical computations, but also allows to perform a correct analysis of such computations automatically, thus, extending previous work on a constraints-based approach to termination. In the last section of the paper we discuss possible extensions of the technique, including incorporating general term orderings.",
        "published": "2001-06-26T12:42:55Z",
        "link": "http://arxiv.org/abs/cs/0106053v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "CHR as grammar formalism. A first report",
        "authors": [
            "Henning Christiansen"
        ],
        "summary": "Grammars written as Constraint Handling Rules (CHR) can be executed as efficient and robust bottom-up parsers that provide a straightforward, non-backtracking treatment of ambiguity. Abduction with integrity constraints as well as other dynamic hypothesis generation techniques fit naturally into such grammars and are exemplified for anaphora resolution, coordination and text interpretation.",
        "published": "2001-06-29T08:41:02Z",
        "link": "http://arxiv.org/abs/cs/0106059v1",
        "categories": [
            "cs.PL",
            "cs.CL",
            "I.2.7;D.3.2;F.4.1;F.4.2"
        ]
    },
    {
        "title": "The Logic Programming Paradigm and Prolog",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "summary": "This is a tutorial on logic programming and Prolog appropriate for a course on programming languages for students familiar with imperative programming.",
        "published": "2001-07-10T10:24:19Z",
        "link": "http://arxiv.org/abs/cs/0107013v2",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Transformations of CCP programs",
        "authors": [
            "Sandro Etalle",
            "Maurizio Gabbrielli",
            "Maria Chiara Meo"
        ],
        "summary": "We introduce a transformation system for concurrent constraint programming (CCP). We define suitable applicability conditions for the transformations which guarantee that the input/output CCP semantics is preserved also when distinguishing deadlocked computations from successful ones and when considering intermediate results of (possibly) non-terminating computations.   The system allows us to optimize CCP programs while preserving their intended meaning: In addition to the usual benefits that one has for sequential declarative languages, the transformation of concurrent programs can also lead to the elimination of communication channels and of synchronization points, to the transformation of non-deterministic computations into deterministic ones, and to the crucial saving of computational space. Furthermore, since the transformation system preserves the deadlock behavior of programs, it can be used for proving deadlock freeness of a given program wrt a class of queries. To this aim it is sometimes sufficient to apply our transformations and to specialize the resulting program wrt the given queries in such a way that the obtained program is trivially deadlock free.",
        "published": "2001-07-10T13:32:17Z",
        "link": "http://arxiv.org/abs/cs/0107014v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.LO",
            "I.2.2; D.1.3; D.3.2"
        ]
    },
    {
        "title": "An interactive semantics of logic programming",
        "authors": [
            "Roberto Bruni",
            "Ugo Montanari",
            "Francesca Rossi"
        ],
        "summary": "We apply to logic programming some recently emerging ideas from the field of reduction-based communicating systems, with the aim of giving evidence of the hidden interactions and the coordination mechanisms that rule the operational machinery of such a programming paradigm. The semantic framework we have chosen for presenting our results is tile logic, which has the advantage of allowing a uniform treatment of goals and observations and of applying abstract categorical tools for proving the results. As main contributions, we mention the finitary presentation of abstract unification, and a concurrent and coordinated abstract semantics consistent with the most common semantics of logic programming. Moreover, the compositionality of the tile semantics is guaranteed by standard results, as it reduces to check that the tile systems associated to logic programs enjoy the tile decomposition property. An extension of the approach for handling constraint systems is also discussed.",
        "published": "2001-07-17T16:43:24Z",
        "link": "http://arxiv.org/abs/cs/0107022v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.1.6; D.3.2; D.3.3; F.3.2"
        ]
    },
    {
        "title": "The Partial Evaluation Approach to Information Personalization",
        "authors": [
            "Naren Ramakrishnan",
            "Saverio Perugini"
        ],
        "summary": "Information personalization refers to the automatic adjustment of information content, structure, and presentation tailored to an individual user. By reducing information overload and customizing information access, personalization systems have emerged as an important segment of the Internet economy. This paper presents a systematic modeling methodology - PIPE (`Personalization is Partial Evaluation') - for personalization. Personalization systems are designed and implemented in PIPE by modeling an information-seeking interaction in a programmatic representation. The representation supports the description of information-seeking activities as partial information and their subsequent realization by partial evaluation, a technique for specializing programs. We describe the modeling methodology at a conceptual level and outline representational choices. We present two application case studies that use PIPE for personalizing web sites and describe how PIPE suggests a novel evaluation criterion for information system designs. Finally, we mention several fundamental implications of adopting the PIPE model for personalization and when it is (and is not) applicable.",
        "published": "2001-08-07T20:27:39Z",
        "link": "http://arxiv.org/abs/cs/0108003v1",
        "categories": [
            "cs.IR",
            "cs.PL",
            "D.3.4; H.4.2; H5.2; H5.4"
        ]
    },
    {
        "title": "Probabilistic asynchronous pi-calculus",
        "authors": [
            "Oltea Mihaela Herescu",
            "Catuscia Palamidessi"
        ],
        "summary": "We propose an extension of the asynchronous pi-calculus with a notion of random choice. We define an operational semantics which distinguishes between probabilistic choice, made internally by the process, and nondeterministic choice, made externally by an adversary scheduler. This distinction will allow us to reason about the probabilistic correctness of algorithms under certain schedulers. We show that in this language we can solve the electoral problem, which was proved not possible in the asynchronous $\\pi$-calculus. Finally, we show an implementation of the probabilistic asynchronous pi-calculus in a Java-like language.",
        "published": "2001-09-03T04:10:41Z",
        "link": "http://arxiv.org/abs/cs/0109002v1",
        "categories": [
            "cs.PL",
            "D.1.3;D.3.2;D.3.3"
        ]
    },
    {
        "title": "On the generalized dining philosophers problem",
        "authors": [
            "Oltea Mihaela Herescu",
            "Catuscia Palamidessi"
        ],
        "summary": "We consider a generalization of the dining philosophers problem to arbitrary connection topologies. We focus on symmetric, fully distributed systems, and we address the problem of guaranteeing progress and lockout-freedom, even in presence of adversary schedulers, by using randomized algorithms. We show that the well-known algorithms of Lehmann and Rabin do not work in the generalized case, and we propose an alternative algorithm based on the idea of letting the philosophers assign a random priority to their adjacent forks.",
        "published": "2001-09-03T05:37:48Z",
        "link": "http://arxiv.org/abs/cs/0109003v1",
        "categories": [
            "cs.PL",
            "D.4.1;C.2.4"
        ]
    },
    {
        "title": "Assigning Satisfaction Values to Constraints: An Algorithm to Solve   Dynamic Meta-Constraints",
        "authors": [
            "Janet van der Linden"
        ],
        "summary": "The model of Dynamic Meta-Constraints has special activity constraints which can activate other constraints. It also has meta-constraints which range over other constraints. An algorithm is presented in which constraints can be assigned one of five different satisfaction values, which leads to the assignment of domain values to the variables in the CSP. An outline of the model and the algorithm is presented, followed by some initial results for two problems: a simple classic CSP and the Car Configuration Problem. The algorithm is shown to perform few backtracks per solution, but to have overheads in the form of historical records required for the implementation of state.",
        "published": "2001-09-13T11:03:18Z",
        "link": "http://arxiv.org/abs/cs/0109014v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D3.3"
        ]
    },
    {
        "title": "Interactive Timetabling",
        "authors": [
            "Tomas Muller",
            "Roman Bartak"
        ],
        "summary": "Timetabling is a typical application of constraint programming whose task is to allocate activities to slots in available resources respecting various constraints like precedence and capacity. In this paper we present a basic concept, a constraint model, and the solving algorithms for interactive timetabling. Interactive timetabling combines automated timetabling (the machine allocates the activities) with user interaction (the user can interfere with the process of timetabling). Because the user can see how the timetabling proceeds and can intervene this process, we believe that such approach is more convenient than full automated timetabling which behaves like a black-box. The contribution of this paper is twofold: we present a generic model to describe timetabling (and scheduling in general) problems and we propose an interactive algorithm for solving such problems.",
        "published": "2001-09-17T09:33:49Z",
        "link": "http://arxiv.org/abs/cs/0109022v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3; F2.2"
        ]
    },
    {
        "title": "Verification of Timed Automata Using Rewrite Rules and Strategies",
        "authors": [
            "Emmanuel Beffara",
            "Olivier Bournez",
            "Hassen Kacem",
            "Claude Kirchner"
        ],
        "summary": "ELAN is a powerful language and environment for specifying and prototyping deduction systems in a language based on rewrite rules controlled by strategies. Timed automata is a class of continuous real-time models of reactive systems for which efficient model-checking algorithms have been devised. In this paper, we show that these algorithms can very easily be prototyped in the ELAN system. This paper argues through this example that rewriting based systems relying on rules and strategies are a good framework to prototype, study and test rather efficiently symbolic model-checking algorithms, i.e. algorithms which involve combination of graph exploration rules, deduction rules, constraint solving techniques and decision procedures.",
        "published": "2001-09-17T15:43:17Z",
        "link": "http://arxiv.org/abs/cs/0109024v1",
        "categories": [
            "cs.PL",
            "I.2.3"
        ]
    },
    {
        "title": "Dynamic Global Constraints: A First View",
        "authors": [
            "Roman Bartak"
        ],
        "summary": "Global constraints proved themselves to be an efficient tool for modelling and solving large-scale real-life combinatorial problems. They encapsulate a set of binary constraints and using global reasoning about this set they filter the domains of involved variables better than arc consistency among the set of binary constraints. Moreover, global constraints exploit semantic information to achieve more efficient filtering than generalised consistency algorithms for n-ary constraints. Continued expansion of constraint programming (CP) to various application areas brings new challenges for design of global constraints. In particular, application of CP to advanced planning and scheduling (APS) requires dynamic additions of new variables and constraints during the process of constraint satisfaction and, thus, it would be helpful if the global constraints could adopt new variables. In the paper, we give a motivation for such dynamic global constraints and we describe a dynamic version of the well-known alldifferent constraint.",
        "published": "2001-09-18T05:18:35Z",
        "link": "http://arxiv.org/abs/cs/0109025v1",
        "categories": [
            "cs.PL",
            "cs.AI",
            "D.3.2; D.3.3; D.1.6"
        ]
    },
    {
        "title": "CLP versus LS on Log-based Reconciliation Problems",
        "authors": [
            "Francois Fages"
        ],
        "summary": "Nomadic applications create replicas of shared objects that evolve independently while they are disconnected. When reconnecting, the system has to reconcile the divergent replicas. In the log-based approach to reconciliation, such as in the IceCube system, the input is a common initial state and logs of actions that were performed on each replica. The output is a consistent global schedule that maximises the number of accepted actions. The reconciler merges the logs according to the schedule, and replays the operations in the merged log against the initial state, yielding to a reconciled common final state.   In this paper, we show the NP-completeness of the log-based reconciliation problem and present two programs for solving it. Firstly, a constraint logic program (CLP) that uses integer constraints for expressing precedence constraints, boolean constraints for expressing dependencies between actions, and some heuristics for guiding the search. Secondly, a stochastic local search method with Tabu heuristic (LS), that computes solutions in an incremental fashion but does not prove optimality. One difficulty in the LS modeling lies in the handling of both boolean variables and integer variables, and in the handling of the objective function which differs from a max-CSP problem. Preliminary evaluation results indicate better performance for the CLP program which, on somewhat realistic benchmarks, finds nearly optimal solutions up to a thousands of actions and proves optimality up to a hundreds of actions.",
        "published": "2001-09-18T19:16:30Z",
        "link": "http://arxiv.org/abs/cs/0109033v1",
        "categories": [
            "cs.PL",
            "D.1.6; D.3.2"
        ]
    },
    {
        "title": "Branching: the Essence of Constraint Solving",
        "authors": [
            "Antonio J. Fernandez",
            "Patricia M. Hill"
        ],
        "summary": "This paper focuses on the branching process for solving any constraint satisfaction problem (CSP). A parametrised schema is proposed that (with suitable instantiations of the parameters) can solve CSP's on both finite and infinite domains. The paper presents a formal specification of the schema and a statement of a number of interesting properties that, subject to certain conditions, are satisfied by any instances of the schema.   It is also shown that the operational procedures of many constraint systems including cooperative systems) satisfy these conditions.   Moreover, the schema is also used to solve the same CSP in different ways by means of different instantiations of its parameters.",
        "published": "2001-09-24T11:33:39Z",
        "link": "http://arxiv.org/abs/cs/0109060v1",
        "categories": [
            "cs.PL",
            "D.3.3; D.3.2"
        ]
    },
    {
        "title": "CLP Approaches to 2D Angle Placements",
        "authors": [
            "Tomasz Szczygiel"
        ],
        "summary": "The paper presents two CLP approaches to 2D angle placements, implemented in CHIP v.5.3. The first is based on the classical (rectangular) cumulative global constraint, the second on the new trapezoidal cumulative global constraint. Both approaches are applied to a specific presented.",
        "published": "2001-09-24T15:39:57Z",
        "link": "http://arxiv.org/abs/cs/0109066v1",
        "categories": [
            "cs.PL",
            "D.3.3"
        ]
    },
    {
        "title": "Higher-Order Pattern Complement and the Strict Lambda-Calculus",
        "authors": [
            "Alberto Momigliano",
            "Frank Pfenning"
        ],
        "summary": "We address the problem of complementing higher-order patterns without repetitions of existential variables. Differently from the first-order case, the complement of a pattern cannot, in general, be described by a pattern, or even by a finite set of patterns. We therefore generalize the simply-typed lambda-calculus to include an internal notion of strict function so that we can directly express that a term must depend on a given variable. We show that, in this more expressive calculus, finite sets of patterns without repeated variables are closed under complement and intersection. Our principal application is the transformational approach to negation in higher-order logic programs.",
        "published": "2001-09-24T17:29:50Z",
        "link": "http://arxiv.org/abs/cs/0109072v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "D.3.3;D.1.6;F.4.1"
        ]
    },
    {
        "title": "Variable and Value Ordering When Solving Balanced Academic Curriculum   Problems",
        "authors": [
            "Carlos Castro",
            "Sebastian Manzano"
        ],
        "summary": "In this paper we present the use of Constraint Programming for solving balanced academic curriculum problems. We discuss the important role that heuristics play when solving a problem using a constraint-based approach. We also show how constraint solving techniques allow to very efficiently solve combinatorial optimization problems that are too hard for integer programming techniques.",
        "published": "2001-10-02T14:10:58Z",
        "link": "http://arxiv.org/abs/cs/0110007v1",
        "categories": [
            "cs.PL",
            "D.3"
        ]
    },
    {
        "title": "Proceedings of the 6th Annual Workshop of the ERCIM Working Group on   Constraints",
        "authors": [
            "Krzysztof R. Apt",
            "Roman Bartak",
            "Eric Monfroy",
            "Francesca Rossi",
            "Sebastian Brand"
        ],
        "summary": "Homepage of the workshop proceedings, with links to all individually archived papers",
        "published": "2001-10-03T09:08:25Z",
        "link": "http://arxiv.org/abs/cs/0110012v1",
        "categories": [
            "cs.PL",
            "D.3.3"
        ]
    },
    {
        "title": "Mixed-Initiative Interaction = Mixed Computation",
        "authors": [
            "Naren Ramakrishnan",
            "Robert Capra",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "We show that partial evaluation can be usefully viewed as a programming model for realizing mixed-initiative functionality in interactive applications. Mixed-initiative interaction between two participants is one where the parties can take turns at any time to change and steer the flow of interaction. We concentrate on the facet of mixed-initiative referred to as `unsolicited reporting' and demonstrate how out-of-turn interactions by users can be modeled by `jumping ahead' to nested dialogs (via partial evaluation). Our approach permits the view of dialog management systems in terms of their native support for staging and simplifying interactions; we characterize three different voice-based interaction technologies using this viewpoint. In particular, we show that the built-in form interpretation algorithm (FIA) in the VoiceXML dialog management architecture is actually a (well disguised) combination of an interpreter and a partial evaluator.",
        "published": "2001-10-09T14:33:55Z",
        "link": "http://arxiv.org/abs/cs/0110022v1",
        "categories": [
            "cs.PL",
            "cs.HC",
            "F3.2; H.5.2"
        ]
    },
    {
        "title": "Inference of termination conditions for numerical loops in Prolog",
        "authors": [
            "Alexander Serebrenik",
            "Danny De Schreye"
        ],
        "summary": "We present a new approach to termination analysis of numerical computations in logic programs. Traditional approaches fail to analyse them due to non well-foundedness of the integers. We present a technique that allows overcoming these difficulties. Our approach is based on transforming a program in a way that allows integrating and extending techniques originally developed for analysis of numerical computations in the framework of query-mapping pairs with the well-known framework of acceptability. Such an integration not only contributes to the understanding of termination behaviour of numerical computations, but also allows us to perform a correct analysis of such computations automatically, by extending previous work on a constraint-based approach to termination. Finally, we discuss possible extensions of the technique, including incorporating general term orderings.",
        "published": "2001-10-17T11:31:18Z",
        "link": "http://arxiv.org/abs/cs/0110034v2",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "On termination of meta-programs",
        "authors": [
            "Alexander Serebrenik",
            "Danny De Schreye"
        ],
        "summary": "The term {\\em meta-programming} refers to the ability of writing programs that have other programs as data and exploit their semantics.   The aim of this paper is presenting a methodology allowing us to perform a correct termination analysis for a broad class of practical meta-interpreters, including negation and performing different tasks during the execution. It is based on combining the power of general orderings, used in proving termination of term-rewrite systems and programs, and on the well-known acceptability condition, used in proving termination of logic programs.   The methodology establishes a relationship between the ordering needed to prove termination of the interpreted program and the ordering needed to prove termination of the meta-interpreter together with this interpreted program. If such a relationship is established, termination of one of those implies termination of the other one, i.e., the meta-interpreter preserves termination.   Among the meta-interpreters that are analysed correctly are a proof trees constructing meta-interpreter, different kinds of tracers and reasoners.   To appear without appendix in Theory and Practice of Logic Programming.",
        "published": "2001-10-17T11:37:46Z",
        "link": "http://arxiv.org/abs/cs/0110035v3",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.4"
        ]
    },
    {
        "title": "Practical Aspects for a Working Compile Time Garbage Collection System   for Mercury",
        "authors": [
            "Nancy Mazur",
            "Peter Ross",
            "Gerda Janssens",
            "Maurice Bruynooghe"
        ],
        "summary": "Compile-time garbage collection (CTGC) is still a very uncommon feature within compilers. In previous work we have developed a compile-time structure reuse system for Mercury, a logic programming language. This system indicates which datastructures can safely be reused at run-time. As preliminary experiments were promising, we have continued this work and have now a working and well performing near-to-ship CTGC-system built into the Melbourne Mercury Compiler (MMC).   In this paper we present the multiple design decisions leading to this system, we report the results of using CTGC for a set of benchmarks, including a real-world program, and finally we discuss further possible improvements. Benchmarks show substantial memory savings and a noticeable reduction in execution time.",
        "published": "2001-10-17T16:20:33Z",
        "link": "http://arxiv.org/abs/cs/0110037v1",
        "categories": [
            "cs.PL",
            "D.3.4;I.2.3"
        ]
    },
    {
        "title": "User-friendly explanations for constraint programming",
        "authors": [
            "Narendra Jussien",
            "Samir Ouis"
        ],
        "summary": "In this paper, we introduce a set of tools for providing user-friendly explanations in an explanation-based constraint programming system. The idea is to represent the constraints of a problem as an hierarchy (a tree). Users are then represented as a set of understandable nodes in that tree (a cut). Classical explanations (sets of system constraints) just need to get projected on that representation in order to be understandable by any user. We present here the main interests of this idea.",
        "published": "2001-11-14T08:43:50Z",
        "link": "http://arxiv.org/abs/cs/0111037v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6;D.3.3; F.4.1;D.2.5"
        ]
    },
    {
        "title": "An Integrated Development Environment for Declarative Multi-Paradigm   Programming",
        "authors": [
            "Michael Hanus",
            "Johannes Koj"
        ],
        "summary": "In this paper we present CIDER (Curry Integrated Development EnviRonment), an analysis and programming environment for the declarative multi-paradigm language Curry. CIDER is a graphical environment to support the development of Curry programs by providing integrated tools for the analysis and visualization of programs. CIDER is completely implemented in Curry using libraries for GUI programming (based on Tcl/Tk) and meta-programming. An important aspect of our environment is the possible adaptation of the development environment to other declarative source languages (e.g., Prolog or Haskell) and the extensibility w.r.t. new analysis methods. To support the latter feature, the lazy evaluation strategy of the underlying implementation language Curry becomes quite useful.",
        "published": "2001-11-14T12:50:17Z",
        "link": "http://arxiv.org/abs/cs/0111039v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.1; D.1.3; D.1.6; D.2.6; D.2.5; D.3.4"
        ]
    },
    {
        "title": "On the Design of a Tool for Supporting the Construction of Logic   Programs",
        "authors": [
            "Gustavo A. Ospina",
            "Baudouin Le Charlier"
        ],
        "summary": "Environments for systematic construction of logic programs are needed in the academy as well as in the industry. Such environments should support well defined construction methods and should be able to be extended and interact with other programming tools like debuggers and compilers. We present a variant of the Deville methodology for logic program development, and the design of a tool for supporting the methodology. Our aim is to facilitate the learning of logic programming and to set the basis of more sophisticated tools for program development.",
        "published": "2001-11-15T14:49:15Z",
        "link": "http://arxiv.org/abs/cs/0111041v3",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6;D.2.6"
        ]
    },
    {
        "title": "Combining Propagation Information and Search Tree Visualization using   ILOG OPL Studio",
        "authors": [
            "Christiane Bracchi",
            "Christophe Gefflot",
            "Frederic Paulin"
        ],
        "summary": "In this paper we give an overview of the current state of the graphical features provided by ILOG OPL Studio for debugging and performance tuning of OPL programs or external ILOG Solver based applications. This paper focuses on combining propagation and search information using the Search Tree view and the Propagation Spy. A new synthetic view is presented: the Christmas Tree, which combines the Search Tree view with statistics on the efficiency of the domain reduction and on the number of the propagation events triggered.",
        "published": "2001-11-15T15:03:23Z",
        "link": "http://arxiv.org/abs/cs/0111040v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.6; D.2.5; F.4.1"
        ]
    },
    {
        "title": "Proceedings of the Eleventh Workshop on Logic Programming Environments   (WLPE'01)",
        "authors": [
            "Anthony Kusalik"
        ],
        "summary": "The Eleventh Workshop on Logic Programming Environments (WLPE'01) was one in a series of international workshops in the topic area. It was held on December 1, 2001 in Paphos, Cyprus as a post-conference workshop at ICLP 2001. Eight refereed papers were presented at the conference. A majority of the papers involved, in some way, constraint logic programming and tools for software development. Other topics areas addressed include execution visualization, instructional aids (for learning users), software maintenance (including debugging), and provisions for new paradigms.",
        "published": "2001-11-16T14:40:57Z",
        "link": "http://arxiv.org/abs/cs/0111042v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Prototyping CLP(FD) Tracers: a Trace Model and an Experimental   Validation Environment",
        "authors": [
            "Ludovic Langevine",
            "Pierre Deransart",
            "Mireille Ducasse",
            "Erwan Jahier"
        ],
        "summary": "Developing and maintaining CLP programs requires visualization and explanation tools. However, existing tools are built in an ad hoc way. Therefore porting tools from one platform to another is very difficult. We have shown in previous work that, from a fine-grained execution trace, a number of interesting views about logic program executions could be generated by trace analysis.   In this article, we propose a trace model for constraint solving by narrowing. This trace model is the first one proposed for CLP(FD) and does not pretend to be the ultimate one. We also propose an instrumented meta-interpreter in order to experiment with the model. Furthermore, we show that the proposed trace model contains the necessary information to build known and useful execution views. This work sets the basis for generic execution analysis of CLP(FD) programs.",
        "published": "2001-11-16T17:02:13Z",
        "link": "http://arxiv.org/abs/cs/0111043v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.6; D.2.5; F.4.1"
        ]
    },
    {
        "title": "An Environment for the Exploration of Non Monotonic Logic Programs",
        "authors": [
            "Luis F. Castro",
            "David S. Warren"
        ],
        "summary": "Stable Model Semantics and Well Founded Semantics have been shown to be very useful in several applications of non-monotonic reasoning. However, Stable Models presents a high computational complexity, whereas Well Founded Semantics is easy to compute and provides an approximation of Stable Models. Efficient engines exist for both semantics of logic programs. This work presents a computational integration of two of such systems, namely XSB and SMODELS. The resulting system is called XNMR, and provides an interactive system for the exploration of both semantics. Aspects such as modularity can be exploited in order to ease debugging of large knowledge bases with the usual Prolog debugging techniques and an interactive environment. Besides, the use of a full Prolog system as a front-end to a Stable Models engine augments the language usually accepted by such systems.",
        "published": "2001-11-19T16:29:35Z",
        "link": "http://arxiv.org/abs/cs/0111049v1",
        "categories": [
            "cs.PL",
            "cs.LO",
            "D.1.6; D.2.6"
        ]
    },
    {
        "title": "HyperPro An integrated documentation environment for CLP",
        "authors": [
            "AbdelAli Ed-Dbali",
            "Pierre Deransart",
            "Mariza A. S. Bigonha",
            "Jose de Siqueira",
            "Roberto da S. Bigonha"
        ],
        "summary": "The purpose of this paper is to present some functionalities of the HyperPro System. HyperPro is a hypertext tool which allows to develop Constraint Logic Programming (CLP) together with their documentation. The text editing part is not new and is based on the free software Thot. A HyperPro program is a Thot document written in a report style. The tool is designed for CLP but it can be adapted to other programming paradigms as well. Thot offers navigation and editing facilities and synchronized static document views. HyperPro has new functionalities such as document exportations, dynamic views (projections), indexes and version management. Projection is a mechanism for extracting and exporting relevant pieces of code program or of document according to specific criteria. Indexes are useful to find the references and occurrences of a relation in a document, i.e., where its predicate definition is found and where a relation is used in other programs or document versions and, to translate hyper-texts links into paper references. It still lack importation facilities.",
        "published": "2001-11-19T16:50:49Z",
        "link": "http://arxiv.org/abs/cs/0111046v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.6 (possibly also D.2.5; F.4.1; I.2.3)"
        ]
    },
    {
        "title": "Optimal Moebius Transformations for Information Visualization and   Meshing",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We give linear-time quasiconvex programming algorithms for finding a Moebius transformation of a set of spheres in a unit ball or on the surface of a unit sphere that maximizes the minimum size of a transformed sphere. We can also use similar methods to maximize the minimum distance among a set of pairs of input points. We apply these results to vertex separation and symmetry display in spherical graph drawing, viewpoint selection in hyperbolic browsing, element size control in conformal structured mesh generation, and brain flat mapping.",
        "published": "2001-01-11T21:55:53Z",
        "link": "http://arxiv.org/abs/cs/0101006v2",
        "categories": [
            "cs.CG",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Computational Geometry Column 41",
        "authors": [
            "Joseph O'Rourke"
        ],
        "summary": "The recent result that n congruent balls in R^d have at most 4 distinct geometric permutations is described.",
        "published": "2001-02-06T13:23:00Z",
        "link": "http://arxiv.org/abs/cs/0102004v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Nice point sets can have nasty Delaunay triangulations",
        "authors": [
            "Jeff Erickson"
        ],
        "summary": "We consider the complexity of Delaunay triangulations of sets of points in R^3 under certain practical geometric constraints. The spread of a set of points is the ratio between the longest and shortest pairwise distances. We show that in the worst case, the Delaunay triangulation of n points in R^3 with spread D has complexity Omega(min{D^3, nD, n^2}) and O(min{D^4, n^2}). For the case D = Theta(sqrt{n}), our lower bound construction consists of a uniform sample of a smooth convex surface with bounded curvature. We also construct a family of smooth connected surfaces such that the Delaunay triangulation of any good point sample has near-quadratic complexity.",
        "published": "2001-03-23T00:23:11Z",
        "link": "http://arxiv.org/abs/cs/0103017v1",
        "categories": [
            "cs.CG",
            "F.2.2;G.2.m"
        ]
    },
    {
        "title": "Notes on computing peaks in k-levels and parametric spanning trees",
        "authors": [
            "Naoki Katoh",
            "Takeshi Tokuyama"
        ],
        "summary": "We give an algorithm to compute all the local peaks in the $k$-level of an arrangement of $n$ lines in $O(n \\log n) + \\tilde{O}((kn)^{2/3})$ time. We can also find $\\tau$ largest peaks in $O(n \\log ^2 n) + \\tilde{O}((\\tau n)^{2/3})$ time. Moreover, we consider the longest edge in a parametric minimum spanning tree (in other words, a bottleneck edge for connectivity), and give an algorithm to compute the parameter value (within a given interval) maximizing/minimizing the length of the longest edge in MST. The time complexity is $\\tilde{O}(n^{8/7}k^{1/7} + n k^{1/3})$",
        "published": "2001-03-29T04:53:47Z",
        "link": "http://arxiv.org/abs/cs/0103024v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F2.2"
        ]
    },
    {
        "title": "The Gibbs Representation of 3D Rotations",
        "authors": [
            "Ian R. Peterson"
        ],
        "summary": "This paper revisits the little-known Gibbs-Rodrigues representation of rotations in a three-dimensional space and demonstrates a set of algorithms for handling it. In this representation the rotation is itself represented as a three-dimensional vector. The vector is parallel to the axis of rotation and its three components transform covariantly on change of coordinates. The mapping from rotations to vectors is 1:1 apart from computation error. The discontinuities of the representation require special handling but are not problematic. The rotation matrix can be generated efficiently from the vector without the use of transcendental functions, and vice-versa. The representation is more efficient than Euler angles, has affinities with Hassenpflug's Argyris angles and is very closely related to the quaternion representation. While the quaternion representation avoids the discontinuities inherent in any 3-component representation, this problem is readily overcome. The present paper gives efficient algorithms for computing the set of rotations which map a given vector to another of the same length and the rotation which maps a given pair of vectors to another pair of the same length and subtended angle.",
        "published": "2001-04-18T11:58:25Z",
        "link": "http://arxiv.org/abs/cs/0104016v4",
        "categories": [
            "cs.DS",
            "cs.CG",
            "B.2.4; F.2.1; G.1.0; I.4.0"
        ]
    },
    {
        "title": "Optimization Over Zonotopes and Training Support Vector Machines",
        "authors": [
            "Marshall Bern",
            "David Eppstein"
        ],
        "summary": "We make a connection between classical polytopes called zonotopes and Support Vector Machine (SVM) classifiers. We combine this connection with the ellipsoid method to give some new theoretical results on training SVMs. We also describe some special properties of soft margin C-SVMs as parameter C goes to infinity.",
        "published": "2001-05-08T21:07:43Z",
        "link": "http://arxiv.org/abs/cs/0105017v1",
        "categories": [
            "cs.CG",
            "cs.AI",
            "F.2.2; G.1.6; I.2.6; I.5.1"
        ]
    },
    {
        "title": "A note on radial basis function computing",
        "authors": [
            "W. Chen",
            "W. He"
        ],
        "summary": "This note carries three purposes involving our latest advances on the radial basis function (RBF) approach. First, we will introduce a new scheme employing the boundary knot method (BKM) to nonlinear convection-diffusion problem. It is stressed that the new scheme directly results in a linear BKM formulation of nonlinear problems by using response point-dependent RBFs, which can be solved by any linear solver. Then we only need to solve a single nonlinear algebraic equation for desirable unknown at some inner node of interest. The numerical results demonstrate high accuracy and efficiency of this nonlinear BKM strategy. Second, we extend the concepts of distance function, which include time-space and variable transformation distance functions. Finally, we demonstrate that if the nodes are symmetrically placed, the RBF coefficient matrices have either centrosymmetric or skew centrosymmetric structures. The factorization features of such matrices lead to a considerable reduction in the RBF computing effort. A simple approach is also presented to reduce the ill-conditioning of RBF interpolation matrices in general cases.",
        "published": "2001-06-03T11:41:56Z",
        "link": "http://arxiv.org/abs/cs/0106003v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G.1.3; G.1.8"
        ]
    },
    {
        "title": "Hinged Kite Mirror Dissection",
        "authors": [
            "David Eppstein"
        ],
        "summary": "Any two polygons of equal area can be partitioned into congruent sets of polygonal pieces, and in many cases one can connect the pieces by flexible hinges while still allowing the connected set to form both polygons. However it is open whether such a hinged dissection always exists. We solve a special case of this problem, by showing that any asymmetric polygon always has a hinged dissection to its mirror image. Our dissection forms a chain of kite-shaped pieces, found by a circle-packing algorithm for quadrilateral mesh generation. A hinged mirror dissection of a polygon with n sides can be formed with O(n) kites in O(n log n) time.",
        "published": "2001-06-13T20:42:09Z",
        "link": "http://arxiv.org/abs/cs/0106032v1",
        "categories": [
            "cs.CG",
            "math.MG",
            "F.2.2"
        ]
    },
    {
        "title": "Vertex-Unfoldings of Simplicial Polyhedra",
        "authors": [
            "Erik D. Demaine",
            "David Eppstein",
            "Jeff Erickson",
            "George W. Hart",
            "Joseph O'Rourke"
        ],
        "summary": "We present two algorithms for unfolding the surface of any polyhedron, all of whose faces are triangles, to a nonoverlapping, connected planar layout. The surface is cut only along polyhedron edges. The layout is connected, but it may have a disconnected interior: the triangles are connected at vertices, but not necessarily joined along edges.",
        "published": "2001-07-18T12:35:53Z",
        "link": "http://arxiv.org/abs/cs/0107023v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Enumerating Foldings and Unfoldings between Polygons and Polytopes",
        "authors": [
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Anna Lubiw",
            "Joseph O'Rourke"
        ],
        "summary": "We pose and answer several questions concerning the number of ways to fold a polygon to a polytope, and how many polytopes can be obtained from one polygon; and the analogous questions for unfolding polytopes to polygons. Our answers are, roughly: exponentially many, or nondenumerably infinite.",
        "published": "2001-07-18T13:13:39Z",
        "link": "http://arxiv.org/abs/cs/0107024v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.1"
        ]
    },
    {
        "title": "Flipping Cubical Meshes",
        "authors": [
            "Marshall Bern",
            "David Eppstein",
            "Jeff Erickson"
        ],
        "summary": "We define and examine flip operations for quadrilateral and hexahedral meshes, similar to the flipping transformations previously used in triangular and tetrahedral mesh generation.",
        "published": "2001-08-27T20:24:51Z",
        "link": "http://arxiv.org/abs/cs/0108020v3",
        "categories": [
            "cs.CG",
            "F.2.2; G.1.8"
        ]
    },
    {
        "title": "Computational Geometry Column 42",
        "authors": [
            "Joseph S. B. Mitchell",
            "Joseph O'Rourke"
        ],
        "summary": "A compendium of thirty previously published open problems in computational geometry is presented.",
        "published": "2001-08-28T20:22:25Z",
        "link": "http://arxiv.org/abs/cs/0108021v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2"
        ]
    },
    {
        "title": "Separating Geometric Thickness from Book Thickness",
        "authors": [
            "David Eppstein"
        ],
        "summary": "We show that geometric thickness and book thickness are not asymptotically equivalent: for every t, there exists a graph with geometric thickness two and book thickness >= t.",
        "published": "2001-09-24T22:45:33Z",
        "link": "http://arxiv.org/abs/math/0109195v1",
        "categories": [
            "math.CO",
            "cs.CG",
            "cs.DM",
            "05C10"
        ]
    },
    {
        "title": "Dense point sets have sparse Delaunay triangulations",
        "authors": [
            "Jeff Erickson"
        ],
        "summary": "The spread of a finite set of points is the ratio between the longest and shortest pairwise distances. We prove that the Delaunay triangulation of any set of n points in R^3 with spread D has complexity O(D^3). This bound is tight in the worst case for all D = O(sqrt{n}). In particular, the Delaunay triangulation of any dense point set has linear complexity. We also generalize this upper bound to regular triangulations of k-ply systems of balls, unions of several dense point sets, and uniform samples of smooth surfaces. On the other hand, for any n and D=O(n), we construct a regular triangulation of complexity Omega(nD) whose n vertices have spread D.",
        "published": "2001-10-15T18:07:36Z",
        "link": "http://arxiv.org/abs/cs/0110030v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2, G.2.m"
        ]
    },
    {
        "title": "Vertex-Unfoldings of Simplicial Manifolds",
        "authors": [
            "Erik D. Demaine",
            "David Eppstein",
            "Jeff Erickson",
            "George W. Hart",
            "Joseph O'Rourke"
        ],
        "summary": "We present an algorithm to unfold any triangulated 2-manifold (in particular, any simplicial polyhedron) into a non-overlapping, connected planar layout in linear time. The manifold is cut only along its edges. The resulting layout is connected, but it may have a disconnected interior; the triangles are connected at vertices, but not necessarily joined along edges. We extend our algorithm to establish a similar result for simplicial manifolds of arbitrary dimension.",
        "published": "2001-10-27T13:34:06Z",
        "link": "http://arxiv.org/abs/cs/0110054v1",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Nonorthogonal Polyhedra Built from Rectangles",
        "authors": [
            "Melody Donoso",
            "Joseph O'Rourke"
        ],
        "summary": "We prove that any polyhedron of genus zero or genus one built out of rectangular faces must be an orthogonal polyhedron, but that there are nonorthogonal polyhedra of genus seven all of whose faces are rectangles. This leads to a resolution of a question posed by Biedl, Lubiw, and Sun [BLS99].",
        "published": "2001-10-29T19:26:05Z",
        "link": "http://arxiv.org/abs/cs/0110059v2",
        "categories": [
            "cs.CG",
            "cs.DM",
            "F.2.2;G.2.2"
        ]
    },
    {
        "title": "A Fast General Methodology for Information-Theoretically Optimal   Encodings of Graphs",
        "authors": [
            "Xin He",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "We propose a fast methodology for encoding graphs with information-theoretically minimum numbers of bits. Specifically, a graph with property pi is called a pi-graph. If pi satisfies certain properties, then an n-node m-edge pi-graph G can be encoded by a binary string X such that (1) G and X can be obtained from each other in O(n log n) time, and (2) X has at most beta(n)+o(beta(n)) bits for any continuous super-additive function beta(n) so that there are at most 2^{beta(n)+o(beta(n))} distinct n-node pi-graphs. The methodology is applicable to general classes of graphs; this paper focuses on planar graphs. Examples of such pi include all conjunctions over the following groups of properties: (1) G is a planar graph or a plane graph; (2) G is directed or undirected; (3) G is triangulated, triconnected, biconnected, merely connected, or not required to be connected; (4) the nodes of G are labeled with labels from {1, ..., ell_1} for ell_1 <= n; (5) the edges of G are labeled with labels from {1, ..., ell_2} for ell_2 <= m; and (6) each node (respectively, edge) of G has at most ell_3 = O(1) self-loops (respectively, ell_4 = O(1) multiple edges). Moreover, ell_3 and ell_4 are not required to be O(1) for the cases of pi being a plane triangulation. These examples are novel applications of small cycle separators of planar graphs and are the only nontrivial classes of graphs, other than rooted trees, with known polynomial-time information-theoretically optimal coding schemes.",
        "published": "2001-01-23T00:17:50Z",
        "link": "http://arxiv.org/abs/cs/0101021v1",
        "categories": [
            "cs.DS",
            "cs.GR",
            "E.4; F.2.2"
        ]
    },
    {
        "title": "Linear-Time Succinct Encodings of Planar Graphs via Canonical Orderings",
        "authors": [
            "Xin He",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "Let G be an embedded planar undirected graph that has n vertices, m edges, and f faces but has no self-loop or multiple edge. If G is triangulated, we can encode it using {4/3}m-1 bits, improving on the best previous bound of about 1.53m bits. In case exponential time is acceptable, roughly 1.08m bits have been known to suffice. If G is triconnected, we use at most (2.5+2\\log{3})\\min\\{n,f\\}-7 bits, which is at most 2.835m bits and smaller than the best previous bound of 3m bits. Both of our schemes take O(n) time for encoding and decoding.",
        "published": "2001-01-27T02:05:33Z",
        "link": "http://arxiv.org/abs/cs/0101033v1",
        "categories": [
            "cs.DS",
            "cs.GR",
            "E.4; F.2.2"
        ]
    },
    {
        "title": "Digital Color Imaging",
        "authors": [
            "Gaurav Sharma",
            "H. Joel Trussell"
        ],
        "summary": "This paper surveys current technology and research in the area of digital color imaging. In order to establish the background and lay down terminology, fundamental concepts of color perception and measurement are first presented us-ing vector-space notation and terminology. Present-day color recording and reproduction systems are reviewed along with the common mathematical models used for representing these devices. Algorithms for processing color images for display and communication are surveyed, and a forecast of research trends is attempted. An extensive bibliography is provided.",
        "published": "2001-09-26T22:14:40Z",
        "link": "http://arxiv.org/abs/cs/0109116v1",
        "categories": [
            "cs.CV",
            "cs.GR",
            "A.1;I.4,I.3.3,I.2.10;I.3.7;B.4.2"
        ]
    },
    {
        "title": "Olfactory search at high Reynolds number",
        "authors": [
            "Eugene Balkovsky",
            "Boris I. Shraiman"
        ],
        "summary": "Locating the source of odor in a turbulent environment - a common behavior for living organisms - is non-trivial because of the random nature of mixing. Here we analyze the statistical physics aspects of the problem and propose an efficient strategy for olfactory search which can work in turbulent plumes. The algorithm combines the maximum likelihood inference of the source position with an active search. Our approach provides the theoretical basis for the design of olfactory robots and the quantitative tools for the analysis of the observed olfactory search behavior of living creatures (e.g. odor modulated optomotor anemotaxis of moth)",
        "published": "2001-09-18T20:29:24Z",
        "link": "http://arxiv.org/abs/nlin/0109019v1",
        "categories": [
            "nlin.CD",
            "cs.RO",
            "nlin.AO",
            "physics.bio-ph"
        ]
    },
    {
        "title": "Data Security Equals Graph Connectivity",
        "authors": [
            "Ming-Yang Kao"
        ],
        "summary": "To protect sensitive information in a cross tabulated table, it is a common practice to suppress some of the cells in the table. This paper investigates four levels of data security of a two-dimensional table concerning the effectiveness of this practice. These four levels of data security protect the information contained in, respectively, individual cells, individual rows and columns, several rows or columns as a whole, and a table as a whole. The paper presents efficient algorithms and NP-completeness results for testing and achieving these four levels of data security. All these complexity results are obtained by means of fundamental equivalences between the four levels of data security of a table and four types of connectivity of a graph constructed from that table.",
        "published": "2001-01-27T02:22:54Z",
        "link": "http://arxiv.org/abs/cs/0101034v1",
        "categories": [
            "cs.CR",
            "cs.DB",
            "cs.DS",
            "F.2.2; H.2.0; H.2.8"
        ]
    },
    {
        "title": "Rapid Application Evolution and Integration Through Document   Metamorphosis",
        "authors": [
            "Paul M. Aoki",
            "Ian E. Smith",
            "James D. Thornton"
        ],
        "summary": "The Harland document management system implements a data model in which document (object) structure can be altered by mixin-style multiple inheritance at any time. This kind of structural fluidity has long been supported by knowledge-base management systems, but its use has primarily been in support of reasoning and inference. In this paper, we report our experiences building and supporting several non-trivial applications on top of this data model. Based on these experiences, we argue that structural fluidity is convenient for data-intensive applications other than knowledge-base management. Specifically, we suggest that this flexible data model is a natural fit for the decoupled programming methodology that arises naturally when using enterprise component frameworks.",
        "published": "2001-03-02T20:00:16Z",
        "link": "http://arxiv.org/abs/cs/0103004v1",
        "categories": [
            "cs.DB",
            "H.2.3; D.2.11; K.6.3"
        ]
    },
    {
        "title": "Secure, Efficient Data Transport and Replica Management for   High-Performance Data-Intensive Computing",
        "authors": [
            "Bill Allcock",
            "Joe Bester",
            "John Bresnahan",
            "Ann L. Chervenak",
            "Ian Foster",
            "Carl Kesselman",
            "Sam Meder",
            "Veronika Nefedova",
            "Darcy Quesnel",
            "Steven Tuecke"
        ],
        "summary": "An emerging class of data-intensive applications involve the geographically dispersed extraction of complex scientific information from very large collections of measured or computed data. Such applications arise, for example, in experimental physics, where the data in question is generated by accelerators, and in simulation science, where the data is generated by supercomputers. So-called Data Grids provide essential infrastructure for such applications, much as the Internet provides essential services for applications such as e-mail and the Web. We describe here two services that we believe are fundamental to any Data Grid: reliable, high-speed transporet and replica management. Our high-speed transport service, GridFTP, extends the popular FTP protocol with new features required for Data Grid applciations, such as striping and partial file access. Our replica management service integrates a replica catalog with GridFTP transfers to provide for the creation, registration, location, and management of dataset replicas. We present the design of both services and also preliminary performance results. Our implementations exploit security and other services provided by the Globus Toolkit.",
        "published": "2001-03-28T20:42:34Z",
        "link": "http://arxiv.org/abs/cs/0103022v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.1.4; E.1"
        ]
    },
    {
        "title": "Event Indexing Systems for Efficient Selection and Analysis of HERA Data",
        "authors": [
            "L. A. T. Bauerdick",
            "Adrian Fox-Murphy",
            "Tobias Haas",
            "Stefan Stonjek",
            "Enrico Tassi"
        ],
        "summary": "The design and implementation of two software systems introduced to improve the efficiency of offline analysis of event data taken with the ZEUS Detector at the HERA electron-proton collider at DESY are presented. Two different approaches were made, one using a set of event directories and the other using a tag database based on a commercial object-oriented database management system. These are described and compared. Both systems provide quick direct access to individual collision events in a sequential data store of several terabytes, and they both considerably improve the event analysis efficiency. In particular the tag database provides a very flexible selection mechanism and can dramatically reduce the computing time needed to extract small subsamples from the total event sample. Gains as large as a factor 20 have been obtained.",
        "published": "2001-04-03T16:27:48Z",
        "link": "http://arxiv.org/abs/cs/0104008v1",
        "categories": [
            "cs.DB",
            "cs.IR",
            "H.2.4; H.3.1; H.3.3; H.3.4; J.2; H.2.8"
        ]
    },
    {
        "title": "Computational Properties of Metaquerying Problems",
        "authors": [
            "F. Angiulli",
            "R. Ben-Eliyahu-Zohary",
            "G. Ianni",
            "L. Palopoli"
        ],
        "summary": "Metaquerying is a datamining technology by which hidden dependencies among several database relations can be discovered. This tool has already been successfully applied to several real-world applications. Recent papers provide only preliminary results about the complexity of metaquerying. In this paper we define several variants of metaquerying that encompass, as far as we know, all variants defined in the literature. We study both the combined complexity and the data complexity of these variants. We show that, under the combined complexity measure, metaquerying is generally intractable (unless P=NP), lying sometimes quite high in the complexity hierarchies (as high as NP^PP), depending on the characteristics of the plausibility index. However, we are able to single out some tractable and interesting metaquerying cases (whose combined complexity is LOGCFL-complete). As for the data complexity of metaquerying, we prove that, in general, this is in TC0, but lies within AC0 in some simpler cases. Finally, we discuss implementation of metaqueries, by providing algorithms to answer them.",
        "published": "2001-06-07T17:29:03Z",
        "link": "http://arxiv.org/abs/cs/0106012v1",
        "categories": [
            "cs.DB",
            "cs.CC",
            "H.2.3;F.2.0;H.2.8"
        ]
    },
    {
        "title": "File mapping Rule-based DBMS and Natural Language Processing",
        "authors": [
            "Vjacheslav M. Novikov"
        ],
        "summary": "This paper describes the system of storage, extract and processing of information structured similarly to the natural language. For recursive inference the system uses the rules having the same representation, as the data. The environment of storage of information is provided with the File Mapping (SHM) mechanism of operating system. In the paper the main principles of construction of dynamic data structure and language for record of the inference rules are stated; the features of available implementation are considered and the description of the application realizing semantic information retrieval on the natural language is given.",
        "published": "2001-06-10T14:56:51Z",
        "link": "http://arxiv.org/abs/cs/0106016v1",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.IR",
            "cs.LG",
            "cs.PL",
            "D.3.2; H.2.4"
        ]
    },
    {
        "title": "Object-oriented solutions",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "In this paper are briefly outlined the motivations, mathematical ideas in use, pre-formalization and assumptions, object-as-functor construction, `soft' types and concept constructions, case study for concepts based on variable domains, extracting a computational background, and examples of evaluations.",
        "published": "2001-06-11T11:46:14Z",
        "link": "http://arxiv.org/abs/cs/0106021v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Object-oriented tools for advanced applications",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko"
        ],
        "summary": "This paper contains a brief discussion of the Application Development Environment (ADE) that is used to build database applications involving the graphical user interface (GUI). ADE computing separates the database access and the user interface. The variety of applications may be generated that communicate with different and distinct desktop databases. The advanced techniques allows to involve remote or stored procedures retrieval and call.",
        "published": "2001-06-11T17:25:25Z",
        "link": "http://arxiv.org/abs/cs/0106023v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Objects and their computational framework",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "Most of the object notions are embedded into a logical domain, especially when dealing with a database theory. Thus, their properties within a computational domain are not yet studied properly. The main topic of this paper is to analyze different concepts of the distinct computational primitive frames to extract the useful object properties and their possible advantages. Some important metaoperators are used to unify the approaches and to establish their possible correspondences.",
        "published": "2001-06-11T17:35:18Z",
        "link": "http://arxiv.org/abs/cs/0106024v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Event Driven Computations for Relational Query Language",
        "authors": [
            "Larissa Ismailova",
            "Konstantin Zinchenko",
            "Lioubouv Bourmistrova"
        ],
        "summary": "This paper deals with an extended model of computations which uses the parameterized families of entities for data objects and reflects a preliminary outline of this problem. Some topics are selected out, briefly analyzed and arranged to cover a general problem. The authors intended more to discuss the particular topics, their interconnection and computational meaning as a panel proposal, so that this paper is not yet to be evaluated as a closed journal paper. To save space all the technical and implementation features are left for the future paper.   Data object is a schematic entity and modelled by the partial function. A notion of type is extended by the variable domains which depend on events and types. A variable domain is built from the potential and schematic individuals and generates the valid families of types depending on a sequence of events. Each valid type consists of the actual individuals which are actual relatively the event or script. In case when a type depends on the script then corresponding view for data objects is attached, otherwise a snapshot is generated. The type thus determined gives an upper range for typed variables so that the local ranges are event driven resulting is the families of actual individuals. An expressive power of the query language is extended using the extensional and intentional relations.",
        "published": "2001-06-12T10:18:36Z",
        "link": "http://arxiv.org/abs/cs/0106026v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.PL",
            "D.3.1; F.1; F.4.1"
        ]
    },
    {
        "title": "Event Driven Objects",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "A formal consideration in this paper is given for the essential notations to characterize the object that is distinguished in a problem domain. The distinct object is represented by another idealized object, which is a schematic element. When the existence of an element is significant, then a class of these partial elements is dropped down into actual, potential and virtual objects. The potential objects are gathered into the variable domains which are the extended ranges for unbound variables. The families of actual objects are shown to be parameterized with the types and events. The transitions between events are shown to be driven by the scripts. A computational framework arises which is described by the commutative diagrams.",
        "published": "2001-06-12T10:48:30Z",
        "link": "http://arxiv.org/abs/cs/0106027v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.SE",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Building Views with Description Logics in ADE: Application Development   Environment",
        "authors": [
            "Larissa Ismailova",
            "Sergey Kosikov",
            "Konstantin Zinchenko",
            "Alexey Mikhaylov",
            "Lioubouv Bourmistrova",
            "Anastassiya Berezovskaya"
        ],
        "summary": "Any of views is formally defined within description logics that were established as a family of logics for modeling complex hereditary structures and have a suitable expressive power. This paper considers the Application Development Environment (ADE) over generalized variable concepts that are used to build database applications involving the supporting views. The front-end user interacts the database via separate ADE access mechanism intermediated by view support. The variety of applications may be generated that communicate with different and distinct desktop databases in a data warehouse. The advanced techniques allows to involve remote or stored procedures retrieval and call.",
        "published": "2001-06-12T12:23:20Z",
        "link": "http://arxiv.org/abs/cs/0106029v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.DS",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Logic, Individuals and Concepts",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "This extended abstract gives a brief outline of the connections between the descriptions and variable concepts. Thus, the notion of a concept is extended to include both the syntax and semantics features. The evaluation map in use is parameterized by a kind of computational environment, the index, giving rise to indexed concepts. The concepts are inhabited into language by the descriptions from the higher order logic. In general the idea of object-as-functor should assist the designer to outline a programming tool in conceptual shell style.",
        "published": "2001-06-12T12:58:27Z",
        "link": "http://arxiv.org/abs/cs/0106030v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.DM",
            "cs.SE",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Solving equations in the relational algebra",
        "authors": [
            "Joachim Biskup",
            "Jan Paredaens",
            "Thomas Schwentick",
            "Jan Van den Bussche"
        ],
        "summary": "Enumerating all solutions of a relational algebra equation is a natural and powerful operation which, when added as a query language primitive to the nested relational algebra, yields a query language for nested relational databases, equivalent to the well-known powerset algebra. We study \\emph{sparse} equations, which are equations with at most polynomially many solutions. We look at their complexity, and compare their expressive power with that of similar notions in the powerset algebra.",
        "published": "2001-06-14T15:39:14Z",
        "link": "http://arxiv.org/abs/cs/0106034v2",
        "categories": [
            "cs.LO",
            "cs.DB",
            "F.4.2; H.2.3"
        ]
    },
    {
        "title": "Polymorphic type inference for the relational algebra",
        "authors": [
            "Jan Van den Bussche",
            "Emmanuel Waller"
        ],
        "summary": "We give a polymorphic account of the relational algebra. We introduce a formalism of ``type formulas'' specifically tuned for relational algebra expressions, and present an algorithm that computes the ``principal'' type for a given expression. The principal type of an expression is a formula that specifies, in a clear and concise manner, all assignments of types (sets of attributes) to relation names, under which a given relational algebra expression is well-typed, as well as the output type that expression will have under each of these assignments. Topics discussed include complexity and polymorphic expressive power.",
        "published": "2001-06-14T16:19:30Z",
        "link": "http://arxiv.org/abs/cs/0106035v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "D.3.3, H.2.3"
        ]
    },
    {
        "title": "Expressing the cone radius in the relational calculus with real   polynomial constraints",
        "authors": [
            "Floris Geerts"
        ],
        "summary": "We show that there is a query expressible in first-order logic over the reals that returns, on any given semi-algebraic set A, for every point a radius around which A is conical. We obtain this result by combining famous results from calculus and real algebraic geometry, notably Sard's theorem and Thom's first isotopy lemma, with recent algorithmic results by Rannou.",
        "published": "2001-06-21T16:33:18Z",
        "link": "http://arxiv.org/abs/cs/0106046v1",
        "categories": [
            "cs.DB",
            "cs.LO",
            "H.2.3; H.2.8"
        ]
    },
    {
        "title": "A Seamless Integration of Association Rule Mining with Database Systems",
        "authors": [
            "Raj P. Gopalan",
            "Tariq Nuruddin",
            "Yudho Giri Sucahyo"
        ],
        "summary": "The need for Knowledge and Data Discovery Management Systems (KDDMS) that support ad hoc data mining queries has been long recognized. A significant amount of research has gone into building tightly coupled systems that integrate association rule mining with database systems. In this paper, we describe a seamless integration scheme for database queries and association rule discovery using a common query optimizer for both. Query trees of expressions in an extended algebra are used for internal representation in the optimizer. The algebraic representation is flexible enough to deal with constrained association rule queries and other variations of association rule specifications. We propose modularization to simplify the query tree for complex tasks in data mining. It paves the way for making use of existing algorithms for constructing query plans in the optimization process. How the integration scheme we present will facilitate greater user control over the data mining process is also discussed. The work described in this paper forms part of a larger project for fully integrating data mining with database management.",
        "published": "2001-06-28T06:02:46Z",
        "link": "http://arxiv.org/abs/cs/0106055v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "The Building of BODHI, a Bio-diversity Database System",
        "authors": [
            "B. J. Srikanta",
            "Jayant Haritsa",
            "Udaysankar Sen"
        ],
        "summary": "We have recently built a database system called BODHI, intended to store plant bio-diversity information. It is based on an object-oriented modeling approach and is developed completely around public-domain software. The unique feature of BODHI is that it seamlessly integrates diverse types of data, including taxonomic characteristics, spatial distributions, and genetic sequences, thereby spanning the entire range from molecular to organism-level information. A variety of sophisticated indexing strategies are incorporated to efficiently access the various types of data, and a rule-based query processor is employed for optimizing query execution. In this paper, we report on our experiences in building BODHI and on its performance characteristics for a representative set of queries.",
        "published": "2001-09-20T12:27:45Z",
        "link": "http://arxiv.org/abs/cs/0109040v1",
        "categories": [
            "cs.DB",
            "q-bio.PE",
            "H.2.4"
        ]
    },
    {
        "title": "The Internet and Community Networks: Case Studies of Five U.S. Cities",
        "authors": [
            "John B. Horrigan"
        ],
        "summary": "This paper looks at five U.S. cities (Austin, Cleveland, Nashville, Portland, and Washington, DC) and explores strategies being employed by community activists and local governments to create and sustain community networking projects. In some cities, community networking initiatives are relatively mature, while in others they are in early or intermediate stages. The paper looks at several factors that help explain the evolution of community networks in cities:   1) Local government support; 2) Federal support 3) Degree of community activism, often reflected by public-private partnerships that help support community networks.   In addition to these (more or less) measurable elements of local support, the case studies enable description of the different objectives of community networks in different cities. Several community networking projects aim to improve the delivery of government services (e.g., Portland and Cleveland), some have a job-training focus (e.g., Austin, Washington, DC), others are oriented very explicitly toward community building (Nashville, DC), and others toward neighborhood entrepreneurship (Portland and Cleveland).   The paper ties the case studies together by asking whether community technology initiatives contribute to social capital in the cities studied.",
        "published": "2001-09-24T20:42:05Z",
        "link": "http://arxiv.org/abs/cs/0109084v1",
        "categories": [
            "cs.DB",
            "K.4.m Miscellaneous"
        ]
    },
    {
        "title": "Structuring Business Metadata in Data Warehouse Systems for Effective   Business Support",
        "authors": [
            "N. L. Sarda"
        ],
        "summary": "Large organizations today are being served by different types of data processing and informations systems, ranging from the operational (OLTP) systems, data warehouse systems, to data mining and business intelligence applications. It is important to create an integrated repository of what these systems contain and do in order to use them collectively and effectively. The repository contains metadata of source systems, data warehouse, and also the business metadata. Decision support and business analysis require extensive and in-depth understanding of business entities, tasks, rules and the environment. The purpose of business metadata is to provide this understanding. Realizing the importance of metadata, many standardization efforts has been initiated to define metadata models. In trying to define an integrated metadata and information systems for a banking application, we discover some important limitations or inadequacies of the business metadata proposals. They relate to providing an integrated and flexible inter-operability and navigation between metadata and data, and to the important issue of systematically handling temporal characteristics and evolution of the metadata itself.   In this paper, we study the issue of structuring business metadata so that it can provide a context for business management and decision support when integrated with data warehousing. We define temporal object-oriented business metadata model, and relate it both to the technical metadata and the data warehouse. We also define ways of accessing and navigating metadata in conjunction with data.",
        "published": "2001-10-08T06:29:14Z",
        "link": "http://arxiv.org/abs/cs/0110020v1",
        "categories": [
            "cs.DB",
            "H.2.1, H.2.7"
        ]
    },
    {
        "title": "A logic-based approach to data integration",
        "authors": [
            "J. Grant",
            "J. Minker"
        ],
        "summary": "An important aspect of data integration involves answering queries using various resources rather than by accessing database relations. The process of transforming a query from the database relations to the resources is often referred to as query folding or answering queries using views, where the views are the resources. We present a uniform approach that includes as special cases much of the previous work on this subject. Our approach is logic-based using resolution. We deal with integrity constraints, negation, and recursion also within this framework.",
        "published": "2001-10-16T19:05:24Z",
        "link": "http://arxiv.org/abs/cs/0110032v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2, I.2.4"
        ]
    },
    {
        "title": "EquiX--A Search and Query Language for XML",
        "authors": [
            "Sara Cohen",
            "Yaron Kanza",
            "Yakov Kogan",
            "Werner Nutt",
            "Yehoshua Sagiv",
            "Alexander Serebrenik"
        ],
        "summary": "EquiX is a search language for XML that combines the power of querying with the simplicity of searching. Requirements for such languages are discussed and it is shown that EquiX meets the necessary criteria. Both a graph-based abstract syntax and a formal concrete syntax are presented for EquiX queries. In addition, the semantics is defined and an evaluation algorithm is presented. The evaluation algorithm is polynomial under combined complexity.   EquiX combines pattern matching, quantification and logical expressions to query both the data and meta-data of XML documents. The result of a query in EquiX is a set of XML documents. A DTD describing the result documents is derived automatically from the query.",
        "published": "2001-10-22T08:03:50Z",
        "link": "http://arxiv.org/abs/cs/0110044v1",
        "categories": [
            "cs.DB",
            "H.2.5; H.2.3"
        ]
    },
    {
        "title": "Mragyati : A System for Keyword-based Searching in Databases",
        "authors": [
            "N. L. Sarda",
            "Ankur Jain"
        ],
        "summary": "The web, through many search engine sites, has popularized the keyword-based search paradigm, where a user can specify a string of keywords and expect to retrieve relevant documents, possibly ranked by their relevance to the query. Since a lot of information is stored in databases (and not as HTML documents), it is important to provide a similar search paradigm for databases, where users can query a database without knowing the database schema and database query languages such as SQL. In this paper, we propose such a database search system, which accepts a free-form query as a collection of keywords, translates it into queries on the database using the database metadata, and presents query results in a well-structured and browsable form. Th eysytem maps keywords onto the database schema and uses inter-relationships (i.e., data semantics) among the referred tables to generate meaningful query results. We also describe our prototype for database search, called Mragyati. Th eapproach proposed here is scalable, as it does not build an in-memory graph of the entire database for searching for relationships among the objects selected by the user's query.",
        "published": "2001-10-25T08:55:57Z",
        "link": "http://arxiv.org/abs/cs/0110052v1",
        "categories": [
            "cs.DB",
            "H.2.4; H.3.3; H.3.5"
        ]
    },
    {
        "title": "The Relational Database Aspects of Argonne's ATLAS Control System",
        "authors": [
            "D. E. R. Quock",
            "F. H. Munson",
            "K. J. Eder",
            "S. L. Dean"
        ],
        "summary": "The Relational Database Aspects of Argonnes ATLAS Control System Argonnes ATLAS (Argonne Tandem Linac Accelerator System) control system comprises two separate database concepts. The first is the distributed real-time database structure provided by the commercial product Vsystem [1]. The second is a more static relational database archiving system designed by ATLAS personnel using Oracle Rdb [2] and Paradox [3] software. The configuration of the ATLAS facility has presented a unique opportunity to construct a control system relational database that is capable of storing and retrieving complete archived tune-up configurations for the entire accelerator. This capability has been a major factor in allowing the facility to adhere to a rigorous operating schedule. Most recently, a Web-based operator interface to the control systems Oracle Rdb database has been installed. This paper explains the history of the ATLAS database systems, how they interact with each other, the design of the new Web-based operator interface, and future plans.",
        "published": "2001-11-01T22:07:01Z",
        "link": "http://arxiv.org/abs/cs/0111004v1",
        "categories": [
            "cs.DB",
            "C.3"
        ]
    },
    {
        "title": "Proliferation of SDDS Support for Various Platforms and Languages",
        "authors": [
            "Robert Soliday"
        ],
        "summary": "Since Self-Describing Data Sets (SDDS) were first introduced, the source code has been ported to many different operating systems and various languages. SDDS is now available in C, Tcl, Java, Fortran, and Python. All of these versions are supported on Solaris, Linux, and Windows. The C version of SDDS is also supported on VxWorks. With the recent addition of the Java port, SDDS can now be deployed on virtually any operating system. Due to this proliferation, SDDS files serve to link not only a collection of C programs, but programs and scripts in many languages on different operating systems. The platform independent binary feature of SDDS also facilitates portability among operating systems. This paper presents an overview of various benefits of SDDS platform interoperability.",
        "published": "2001-11-05T18:14:16Z",
        "link": "http://arxiv.org/abs/cs/0111006v1",
        "categories": [
            "cs.DB",
            "H.2.8"
        ]
    },
    {
        "title": "The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data",
        "authors": [
            "Alexander Szalay",
            "Jim Gray",
            "Ani Thakar",
            "Peter Z. Kunszt",
            "Tanu Malik",
            "Jordan Raddick",
            "Christopher Stoughton",
            "Jan vandenBerg"
        ],
        "summary": "The SkyServer provides Internet access to the public Sloan Digital Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and architecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.",
        "published": "2001-11-07T20:39:31Z",
        "link": "http://arxiv.org/abs/cs/0111015v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H.3.5, H.4, J.2, H.2.8"
        ]
    },
    {
        "title": "Data Acquisition and Database Management System for Samsung   Superconductor Test Facility",
        "authors": [
            "Y. Chu",
            "S. Baek",
            "H. Yonekawa",
            "A. Chertovskikh",
            "M. Kim",
            "J. S. Kim",
            "K. Park",
            "S. Baang",
            "Y. Chang",
            "J. H. Kim",
            "S. Lee",
            "B. Lim",
            "W. Chung",
            "H. Park",
            "K. Kim"
        ],
        "summary": "In order to fulfill the test requirement of KSTAR (Korea Superconducting Tokamak Advanced Research) superconducting magnet system, a large scale superconducting magnet and conductor test facility, SSTF (Samsung Superconductor Test Facility), has been constructed at Samsung Advanced Institute of Technology. The computer system for SSTF DAC (Data Acquisition and Control) is based on UNIX system and VxWorks is used for the real-time OS of the VME system. EPICS (Experimental Physics and Industrial Control System) is used for the communication between IOC server and client. A database program has been developed for the efficient management of measured data and a Linux workstation with PENTIUM-4 CPU is used for the database server. In this paper, the current status of SSTF DAC system, the database management system and recent test results are presented.",
        "published": "2001-11-08T18:28:46Z",
        "link": "http://arxiv.org/abs/cs/0111018v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "B.1.1"
        ]
    },
    {
        "title": "A Tight Upper Bound on the Number of Candidate Patterns",
        "authors": [
            "Floris Geerts",
            "Bart Goethals",
            "Jan Van den Bussche"
        ],
        "summary": "In the context of mining for frequent patterns using the standard levelwise algorithm, the following question arises: given the current level and the current set of frequent patterns, what is the maximal number of candidate patterns that can be generated on the next level? We answer this question by providing a tight upper bound, derived from a combinatorial result from the sixties by Kruskal and Katona. Our result is useful to reduce the number of database scans.",
        "published": "2001-12-07T15:40:11Z",
        "link": "http://arxiv.org/abs/cs/0112007v2",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Interactive Constrained Association Rule Mining",
        "authors": [
            "Bart Goethals",
            "Jan Van den Bussche"
        ],
        "summary": "We investigate ways to support interactive mining sessions, in the setting of association rule mining. In such sessions, users specify conditions (queries) on the associations to be generated. Our approach is a combination of the integration of querying conditions inside the mining phase, and the incremental querying of already generated associations. We present several concrete algorithms and compare their performance.",
        "published": "2001-12-10T15:50:47Z",
        "link": "http://arxiv.org/abs/cs/0112011v2",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "A Data Mining Framework for Optimal Product Selection in Retail   Supermarket Data: The Generalized PROFSET Model",
        "authors": [
            "Tom Brijs",
            "Bart Goethals",
            "Gilbert Swinnen",
            "Koen Vanhoof",
            "Geert Wets"
        ],
        "summary": "In recent years, data mining researchers have developed efficient association rule algorithms for retail market basket analysis. Still, retailers often complain about how to adopt association rules to optimize concrete retail marketing-mix decisions. It is in this context that, in a previous paper, the authors have introduced a product selection model called PROFSET. This model selects the most interesting products from a product assortment based on their cross-selling potential given some retailer defined constraints. However this model suffered from an important deficiency: it could not deal effectively with supermarket data, and no provisions were taken to include retail category management principles. Therefore, in this paper, the authors present an important generalization of the existing model in order to make it suitable for supermarket data as well, and to enable retailers to add category restrictions to the model. Experiments on real world data obtained from a Belgian supermarket chain produce very promising results and demonstrate the effectiveness of the generalized PROFSET model.",
        "published": "2001-12-11T19:23:11Z",
        "link": "http://arxiv.org/abs/cs/0112013v1",
        "categories": [
            "cs.DB",
            "cs.AI",
            "H.2.8"
        ]
    },
    {
        "title": "Structure of $Z^2$ modulo selfsimilar sublattices",
        "authors": [
            "Roberto Canogar-Mackenzie",
            "Edgar Martinez-Moro"
        ],
        "summary": "In this paper we show the combinatorial structure of $\\mathbb{Z}^2$ modulo sublattices selfsimilar to $\\mathbb{Z}^2$. The tool we use for dealing with this purpose is the notion of association scheme. We classify when the scheme defined by the lattice is imprimitive and characterize its decomposition in terms of the decomposition of the gaussian integer defining the lattice. This arise in the classification of different forms of tiling $\\mathbb{Z}^2$ by lattices of this type.",
        "published": "2001-01-10T17:12:17Z",
        "link": "http://arxiv.org/abs/math/0101092v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "05E30;52C20;11H71"
        ]
    },
    {
        "title": "Quantum Kolmogorov Complexity Based on Classical Descriptions",
        "authors": [
            "Paul M. B. Vitanyi"
        ],
        "summary": "We develop a theory of the algorithmic information in bits contained in an individual pure quantum state. This extends classical Kolmogorov complexity to the quantum domain retaining classical descriptions. Quantum Kolmogorov complexity coincides with the classical Kolmogorov complexity on the classical domain. Quantum Kolmogorov complexity is upper bounded and can be effectively approximated from above under certain conditions. With high probability a quantum object is incompressible. Upper- and lower bounds of the quantum complexity of multiple copies of individual pure quantum states are derived and may shed some light on the no-cloning properties of quantum states. In the quantum situation complexity is not sub-additive. We discuss some relations with ``no-cloning'' and ``approximate cloning'' properties.",
        "published": "2001-02-21T16:58:54Z",
        "link": "http://arxiv.org/abs/quant-ph/0102108v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.IT",
            "math.IT",
            "math.LO"
        ]
    },
    {
        "title": "Source Coding, Large Deviations, and Approximate Pattern Matching",
        "authors": [
            "A. Dembo",
            "I. Kontoyiannis"
        ],
        "summary": "We present a development of parts of rate-distortion theory and pattern- matching algorithms for lossy data compression, centered around a lossy version of the Asymptotic Equipartition Property (AEP). This treatment closely parallels the corresponding development in lossless compression, a point of view that was advanced in an important paper of Wyner and Ziv in 1989. In the lossless case we review how the AEP underlies the analysis of the Lempel-Ziv algorithm by viewing it as a random code and reducing it to the idealized Shannon code. This also provides information about the redundancy of the Lempel-Ziv algorithm and about the asymptotic behavior of several relevant quantities. In the lossy case we give various versions of the statement of the generalized AEP and we outline the general methodology of its proof via large deviations. Its relationship with Barron's generalized AEP is also discussed. The lossy AEP is applied to: (i) prove strengthened versions of Shannon's source coding theorem and universal coding theorems; (ii) characterize the performance of mismatched codebooks; (iii) analyze the performance of pattern- matching algorithms for lossy compression; (iv) determine the first order asymptotics of waiting times (with distortion) between stationary processes; (v) characterize the best achievable rate of weighted codebooks as an optimal sphere-covering exponent. We then present a refinement to the lossy AEP and use it to: (i) prove second order coding theorems; (ii) characterize which sources are easier to compress; (iii) determine the second order asymptotics of waiting times; (iv) determine the precise asymptotic behavior of longest match-lengths. Extensions to random fields are also given.",
        "published": "2001-03-02T02:19:46Z",
        "link": "http://arxiv.org/abs/math/0103007v1",
        "categories": [
            "math.PR",
            "cs.IT",
            "math.IT",
            "94A15; 60F10; 60G60"
        ]
    },
    {
        "title": "Explicit modular towers",
        "authors": [
            "Noam D. Elkies"
        ],
        "summary": "We give a general recipe for explicitly constructing asymptotically optimal towers of modular curves such as {X_0(l^n): n=1,2,3,...}. We illustrate the method by giving equations for eight towers with various geometric features. We conclude by observing that such towers are all of a specific recursive form, and speculate that perhaps every tower of this form that attains the Drinfeld-Vladut bound is modular.",
        "published": "2001-03-16T20:15:01Z",
        "link": "http://arxiv.org/abs/math/0103107v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "11G18 (Primary) 14G50 (Seconday)"
        ]
    },
    {
        "title": "In search of an evolutionary coding style",
        "authors": [
            "Torbjörn Lundh"
        ],
        "summary": "In the near future, all the human genes will be identified. But understanding the functions coded in the genes is a much harder problem. For example, by using block entropy, one has that the DNA code is closer to a random code then written text, which in turn is less ordered then an ordinary computer code; see \\cite{schmitt}.   Instead of saying that the DNA is badly written, using our programming standards, we might say that it is written in a different style -- an evolutionary style.   We will suggest a way to search for such a style in a quantified manner by using an artificial life program, and by giving a definition of general codes and a definition of style for such codes.",
        "published": "2001-03-16T23:23:24Z",
        "link": "http://arxiv.org/abs/math/0103109v1",
        "categories": [
            "math.NA",
            "cs.IT",
            "math.DS",
            "math.IT",
            "q-bio"
        ]
    },
    {
        "title": "Bounds for weight distribution of weakly self-dual codes",
        "authors": [
            "Vwani P. Roychowdhury",
            "Farrokh Vatan"
        ],
        "summary": "Upper bounds are given for the weight distribution of binary weakly self-dual codes. To get these new bounds, we introduce a novel method of utilizing unitary operations on Hilbert spaces. This method is motivated by recent progress on quantum computing. This new approach leads to much simpler proofs for such genre of bounds on the weight distributions of certain classes of codes. Moreover, in some cases, our bounds are improvements on the earlier bounds. These improvements are achieved, either by extending the range of the weights over which the bounds apply, or by extending the class of codes subjected to these bounds.",
        "published": "2001-04-02T06:29:58Z",
        "link": "http://arxiv.org/abs/math/0104016v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math.IT",
            "quant-ph",
            "94B65"
        ]
    },
    {
        "title": "Excellent nonlinear codes from modular curves",
        "authors": [
            "Noam D. Elkies"
        ],
        "summary": "We introduce a new construction of error-correcting codes from algebraic curves over finite fields. Modular curves of genus g -> infty over a field of size q0^2 yield nonlinear codes more efficient than the linear Goppa codes obtained from the same curves. These new codes now have the highest asymptotic transmission rates known for certain ranges of alphabet size and error rate. Both the theory and possible practical use of these new record codes require the development of new tools. On the theoretical side, establishing the transmission rate depends on an error estimate for a theorem of Schanuel applied to the function field of an asymptotically optimal curve. On the computational side, actual use of the codes will hinge on the solution of new problems in the computational algebraic geometry of curves.",
        "published": "2001-04-10T19:59:34Z",
        "link": "http://arxiv.org/abs/math/0104115v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "94B27;94B65"
        ]
    },
    {
        "title": "Decoding method for generalized algebraic geometry codes",
        "authors": [
            "Ryutaroh Matsumoto",
            "Masakuni Oishi"
        ],
        "summary": "We propose a decoding method for the generalized algebraic geometry codes proposed by Xing et al. To show its practical usefulness, we give an example of generalized algebraic geometry codes of length 567 over F_8 whose numbers of correctable errors by the proposed method are larger than the shortened codes of the primitive BCH codes of length 4095 in the most range of dimension.",
        "published": "2001-04-24T12:00:37Z",
        "link": "http://arxiv.org/abs/math/0104222v1",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "11T71 (Primary) 11G20, 14G50, 94B27 (Secondary)"
        ]
    },
    {
        "title": "The coset weight distributions of certain BCH codes and a family of   curves",
        "authors": [
            "Gerard van der Geer",
            "Marcel van der Vlugt"
        ],
        "summary": "We study the distribution of the number of rational points in a family of curves over a finite field of characteristic 2. This distribution determines the coset weight distribution of a certain BCH code.",
        "published": "2001-06-12T09:33:27Z",
        "link": "http://arxiv.org/abs/math/0106089v2",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.CO",
            "math.IT"
        ]
    },
    {
        "title": "Algebraic geometric construction of a quantum stabilizer code",
        "authors": [
            "Ryutaroh Matsumoto"
        ],
        "summary": "The stabilizer code is the most general algebraic construction of quantum error-correcting codes proposed so far. A stabilizer code can be constructed from a self-orthogonal subspace of a symplectic space over a finite field. We propose a construction method of such a self-orthogonal space using an algebraic curve. By using the proposed method we construct an asymptotically good sequence of binary stabilizer codes. As a byproduct we improve the Ashikhmin-Litsyn-Tsfasman bound of quantum codes. The main results in this paper can be understood without knowledge of quantum mechanics.",
        "published": "2001-07-26T06:29:08Z",
        "link": "http://arxiv.org/abs/quant-ph/0107129v3",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.AG",
            "math.IT",
            "math.SG"
        ]
    },
    {
        "title": "Geometrically Uniform Frames",
        "authors": [
            "Yonina C. Eldar",
            "H. Bolcskei"
        ],
        "summary": "We introduce a new class of frames with strong symmetry properties called geometrically uniform frames (GU), that are defined over an abelian group of unitary matrices and are generated by a single generating vector. The notion of GU frames is then extended to compound GU (CGU) frames which are generated by an abelian group of unitary matrices using multiple generating vectors.   The dual frame vectors and canonical tight frame vectors associated with GU frames are shown to be GU and therefore generated by a single generating vector, which can be computed very efficiently using a Fourier transform defined over the generating group of the frame. Similarly, the dual frame vectors and canonical tight frame vectors associated with CGU frames are shown to be CGU.   The impact of removing single or multiple elements from a GU frame is considered. A systematic method for constructing optimal GU frames from a given set of frame vectors that are not GU is also developed. Finally, the Euclidean distance properties of GU frames are discussed and conditions are derived on the abelian group of unitary matrices to yield GU frames with strictly positive distance spectrum irrespective of the generating vector.",
        "published": "2001-08-13T21:03:47Z",
        "link": "http://arxiv.org/abs/math/0108096v1",
        "categories": [
            "math.FA",
            "cs.IT",
            "math.GR",
            "math.IT"
        ]
    },
    {
        "title": "Quantum Information in Space and Time",
        "authors": [
            "Igor V. Volovich"
        ],
        "summary": "Many important results in modern quantum information theory have been obtained for an idealized situation when the spacetime dependence of quantum phenomena is neglected. However the transmission and processing of (quantum) information is a physical process in spacetime. Therefore such basic notions in quantum information theory as the notions of composite systems, entangled states and the channel should be formulated in space and time. We emphasize the importance of the investigation of quantum information in space and time. Entangled states in space and time are considered. A modification of Bell`s equation which includes the spacetime variables is suggested. A general relation between quantum theory and theory of classical stochastic processes is proposed. It expresses the condition of local realism in the form of a {\\it noncommutative spectral theorem}. Applications of this relation to the security of quantum key distribution in quantum cryptography are considered.",
        "published": "2001-08-15T20:00:14Z",
        "link": "http://arxiv.org/abs/quant-ph/0108073v1",
        "categories": [
            "quant-ph",
            "cond-mat.mes-hall",
            "cs.IT",
            "gr-qc",
            "hep-ph",
            "hep-th",
            "math-ph",
            "math.IT",
            "math.MP",
            "math.PR"
        ]
    },
    {
        "title": "On Classical and Quantum Cryptography",
        "authors": [
            "I. V. Volovich",
            "Ya. I. Volovich"
        ],
        "summary": "Lectures on classical and quantum cryptography. Contents: Private key cryptosystems. Elements of number theory. Public key cryptography and RSA cryptosystem. Shannon`s entropy and mutual information. Entropic uncertainty relations. The no cloning theorem. The BB84 quantum cryptographic protocol. Security proofs. Bell`s theorem. The EPRBE quantum cryptographic protocol.",
        "published": "2001-08-29T21:37:36Z",
        "link": "http://arxiv.org/abs/quant-ph/0108133v1",
        "categories": [
            "quant-ph",
            "cond-mat.mes-hall",
            "cs.IT",
            "hep-th",
            "math-ph",
            "math.IT",
            "math.MP"
        ]
    },
    {
        "title": "Some Applications of Algebraic Curves to Computational Vision",
        "authors": [
            "Michael Fryers",
            "Jeremy Yirmeyahu Kaminski",
            "Mina Teicher"
        ],
        "summary": "We introduce a new formalism and a number of new results in the context of geometric computational vision. The classical scope of the research in geometric computer vision is essentially limited to static configurations of points and lines in $P^3$ . By using some well known material from algebraic geometry, we open new branches to computational vision. We introduce algebraic curves embedded in $P^3$ as the building blocks from which the tensor of a couple of cameras (projections) can be computed. In the process we address dimensional issues and as a result establish the minimal number of algebraic curves required for the tensor variety to be discrete as a function of their degree and genus. We then establish new results on the reconstruction of an algebraic curves in $P^3$ from multiple projections on projective planes embedded in $P^3$ . We address three different presentations of the curve: (i) definition by a set of equations, for which we show that for a generic configuration, two projections of a curve of degree d defines a curve in $P^3$ with two irreducible components, one of degree d and the other of degree $d(d - 1)$, (ii) the dual presentation in the dual space $P^{3*}$, for which we derive a lower bound for the number of projections necessary for linear reconstruction as a function of the degree and the genus, and (iii) the presentation as an hypersurface of $P^5$, defined by the set of lines in $P^3$ meeting the curve, for which we also derive lower bounds for the number of projections necessary for linear reconstruction as a function of the degree (of the curve). Moreover we show that the latter representation yields a new and efficient algorithm for dealing with mixed configurations of static and moving points in $P^3$.",
        "published": "2001-10-15T18:37:04Z",
        "link": "http://arxiv.org/abs/math/0110157v1",
        "categories": [
            "math.AG",
            "cs.IT",
            "math.IT"
        ]
    },
    {
        "title": "Quantum entanglement and geometry of determinantal varieties",
        "authors": [
            "Hao Chen"
        ],
        "summary": "Quantum entanglement was first recognized as a feature of quantum mechanics in the famous paper of Einstein, Podolsky and Rosen [18]. Recently it has been realized that quantum entanglement is a key ingredient in quantum computation, quantum communication and quantum cryptography ([16],[17],[6]). In this paper, we introduce algebraic sets, which are determinantal varieties in the complex projective spaces or the products of complex projective spaces, for the mixed states in bipartite or multipartite quantum systems as their invariants under local unitary transformations. These invariants are naturally arised from the physical consideration of measuring mixed states by separable pure states. In this way algebraic geometry and complex differential geometry of these algebraic sets turn to be powerful tools for the understanding of quantum enatanglement. Our construction has applications in the following important topics in quantum information theory: 1) separability criterion, it is proved the algebraic sets have to be the sum of the linear subspaces if the mixed states are separable; 2) lower bound of Schmidt numbers, that is, generic low rank bipartite mixed states are entangled in many degrees of freedom; 3) simulation of Hamiltonians, it is proved the simulation of semi-positive Hamiltonians of the same rank implies the projective isomorphisms of the corresponding algebraic sets; 4) construction of bound enatanglement, examples of the entangled mixed states which are invariant under partial transpositions (thus PPT bound entanglement) are constructed systematically from our new separability criterion. On the other hand many examples of entangled mixed states with rich algebraic-geometric structure in their associated determinantal varieties are constructed and studied from this point of view.",
        "published": "2001-10-17T06:27:49Z",
        "link": "http://arxiv.org/abs/quant-ph/0110103v3",
        "categories": [
            "quant-ph",
            "cs.IT",
            "math.AG",
            "math.IT"
        ]
    },
    {
        "title": "Coding Distributive Lattices with Edge Firing Games",
        "authors": [
            "Matthieu Latapy",
            "Clemence Magnien"
        ],
        "summary": "In this note, we show that any distributive lattice is isomorphic to the set of reachable configurations of an Edge Firing Game. Together with the result of James Propp, saying that the set of reachable configurations of any Edge Firing Game is always a distributive lattice, this shows that the two concepts are equivalent.",
        "published": "2001-10-19T13:52:49Z",
        "link": "http://arxiv.org/abs/math/0110214v1",
        "categories": [
            "math.CO",
            "cs.IT",
            "math-ph",
            "math.DS",
            "math.IT",
            "math.MP"
        ]
    },
    {
        "title": "Constructing elliptic curves with a known number of points over a prime   field",
        "authors": [
            "Amod Agashe",
            "Kristin Lauter",
            "Ramarathnam Venkatesan"
        ],
        "summary": "Elliptic curves with a known number of points over a given prime field with n elements are often needed for use in cryptography. In the context of primality proving, Atkin and Morain suggested the use of the theory of complex multiplication to construct such curves. One of the steps in this method is the calculation of a root modulo n of the Hilbert class polynomial H(X) for a fundamental discriminant D. The usual way is to compute H(X) over the integers and then to find the root modulo n. We present a modified version of the Chinese remainder theorem (CRT) to compute H(X) modulo n directly from the knowledge of H(X) modulo enough small primes. Our complexity analysis suggests that asymptotically our algorithm is an improvement over previously known methods.",
        "published": "2001-11-13T20:02:14Z",
        "link": "http://arxiv.org/abs/math/0111159v2",
        "categories": [
            "math.NT",
            "cs.IT",
            "math.AG",
            "math.IT",
            "11Y16; 14H52; 11G15; 11Z05"
        ]
    },
    {
        "title": "Distribution of Mutual Information",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "The mutual information of two random variables i and j with joint probabilities t_ij is commonly used in learning Bayesian nets as well as in many other fields. The chances t_ij are usually estimated by the empirical sampling frequency n_ij/n leading to a point estimate I(n_ij/n) for the mutual information. To answer questions like \"is I(n_ij/n) consistent with zero?\" or \"what is the probability that the true mutual information is much larger than the point estimate?\" one has to go beyond the point estimate. In the Bayesian framework one can answer these questions by utilizing a (second order) prior distribution p(t) comprising prior information about t. From the prior p(t) one can compute the posterior p(t|n), from which the distribution p(I|n) of the mutual information can be calculated. We derive reliable and quickly computable approximations for p(I|n). We concentrate on the mean, variance, skewness, and kurtosis, and non-informative priors. For the mean we also give an exact expression. Numerical issues and the range of validity are discussed.",
        "published": "2001-12-15T12:58:39Z",
        "link": "http://arxiv.org/abs/cs/0112019v1",
        "categories": [
            "cs.AI",
            "cs.IT",
            "math.IT",
            "math.ST",
            "stat.TH",
            "G.3; G.1.2"
        ]
    },
    {
        "title": "Adaptive evolution on neutral networks",
        "authors": [
            "Claus O. Wilke"
        ],
        "summary": "We study the evolution of large but finite asexual populations evolving in fitness landscapes in which all mutations are either neutral or strongly deleterious. We demonstrate that despite the absence of higher fitness genotypes, adaptation takes place as regions with more advantageous distributions of neutral genotypes are discovered. Since these discoveries are typically rare events, the population dynamics can be subdivided into separate epochs, with rapid transitions between them. Within one epoch, the average fitness in the population is approximately constant. The transitions between epochs, however, are generally accompanied by a significant increase in the average fitness. We verify our theoretical considerations with two analytically tractable bitstring models.",
        "published": "2001-01-03T18:09:19Z",
        "link": "http://arxiv.org/abs/physics/0101021v1",
        "categories": [
            "physics.bio-ph",
            "cond-mat.stat-mech",
            "cs.NE",
            "nlin.AO",
            "q-bio.PE"
        ]
    },
    {
        "title": "Self-adaptive exploration in evolutionary search",
        "authors": [
            "Marc Toussaint"
        ],
        "summary": "We address a primary question of computational as well as biological research on evolution: How can an exploration strategy adapt in such a way as to exploit the information gained about the problem at hand? We first introduce an integrated formalism of evolutionary search which provides a unified view on different specific approaches. On this basis we discuss the implications of indirect modeling (via a ``genotype-phenotype mapping'') on the exploration strategy. Notions such as modularity, pleiotropy and functional phenotypic complex are discussed as implications. Then, rigorously reflecting the notion of self-adaptability, we introduce a new definition that captures self-adaptability of exploration: different genotypes that map to the same phenotype may represent (also topologically) different exploration strategies; self-adaptability requires a variation of exploration strategies along such a ``neutral space''. By this definition, the concept of neutrality becomes a central concern of this paper. Finally, we present examples of these concepts: For a specific grammar-type encoding, we observe a large variability of exploration strategies for a fixed phenotype, and a self-adaptive drift towards short representations with highly structured exploration strategy that matches the ``problem's structure''.",
        "published": "2001-02-05T13:55:16Z",
        "link": "http://arxiv.org/abs/physics/0102009v1",
        "categories": [
            "physics.bio-ph",
            "cs.NE",
            "nlin.AO",
            "q-bio"
        ]
    },
    {
        "title": "On the predictability of Rainfall in Kerala- An application of ABF   Neural Network",
        "authors": [
            "Ninan Sajeeth Philip",
            "K. Babu Joseph"
        ],
        "summary": "Rainfall in Kerala State, the southern part of Indian Peninsula in particular is caused by the two monsoons and the two cyclones every year. In general, climate and rainfall are highly nonlinear phenomena in nature giving rise to what is known as the `butterfly effect'. We however attempt to train an ABF neural network on the time series rainfall data and show for the first time that in spite of the fluctuations resulting from the nonlinearity in the system, the trends in the rainfall pattern in this corner of the globe have remained unaffected over the past 87 years from 1893 to 1980. We also successfully filter out the chaotic part of the system and illustrate that its effects are marginal over long term predictions.",
        "published": "2001-02-18T19:17:18Z",
        "link": "http://arxiv.org/abs/cs/0102014v1",
        "categories": [
            "cs.NE",
            "cs.AI",
            "A0"
        ]
    },
    {
        "title": "Non-convex cost functionals in boosting algorithms and methods for panel   selection",
        "authors": [
            "Marco Visentin"
        ],
        "summary": "In this document we propose a new improvement for boosting techniques as proposed in Friedman '99 by the use of non-convex cost functional. The idea is to introduce a correlation term to better deal with forecasting of additive time series. The problem is discussed in a theoretical way to prove the existence of minimizing sequence, and in a numerical way to propose a new \"ArgMin\" algorithm. The model has been used to perform the touristic presence forecast for the winter season 1999/2000 in Trentino (italian Alps).",
        "published": "2001-02-20T13:08:15Z",
        "link": "http://arxiv.org/abs/cs/0102015v1",
        "categories": [
            "cs.NE",
            "cs.LG",
            "cs.NA",
            "math.NA",
            "I.2.6;G.1.2;G.3;I.6.5"
        ]
    },
    {
        "title": "Gene Expression Programming: a New Adaptive Algorithm for Solving   Problems",
        "authors": [
            "Candida Ferreira"
        ],
        "summary": "Gene expression programming, a genotype/phenotype genetic algorithm (linear and ramified), is presented here for the first time as a new technique for the creation of computer programs. Gene expression programming uses character linear chromosomes composed of genes structurally organized in a head and a tail. The chromosomes function as a genome and are subjected to modification by means of mutation, transposition, root transposition, gene transposition, gene recombination, and one- and two-point recombination. The chromosomes encode expression trees which are the object of selection. The creation of these separate entities (genome and expression tree) with distinct functions allows the algorithm to perform with high efficiency that greatly surpasses existing adaptive techniques. The suite of problems chosen to illustrate the power and versatility of gene expression programming includes symbolic regression, sequence induction with and without constant creation, block stacking, cellular automata rules for the density-classification problem, and two problems of boolean concept learning: the 11-multiplexer and the GP rule problem.",
        "published": "2001-02-25T19:29:55Z",
        "link": "http://arxiv.org/abs/cs/0102027v3",
        "categories": [
            "cs.AI",
            "cs.NE",
            "I.2.2"
        ]
    },
    {
        "title": "Potholes on the Royal Road",
        "authors": [
            "Theodore C. Belding"
        ],
        "summary": "It is still unclear how an evolutionary algorithm (EA) searches a fitness landscape, and on what fitness landscapes a particular EA will do well. The validity of the building-block hypothesis, a major tenet of traditional genetic algorithm theory, remains controversial despite its continued use to justify claims about EAs. This paper outlines a research program to begin to answer some of these open questions, by extending the work done in the royal road project. The short-term goal is to find a simple class of functions which the simple genetic algorithm optimizes better than other optimization methods, such as hillclimbers. A dialectical heuristic for searching for such a class is introduced. As an example of using the heuristic, the simple genetic algorithm is compared with a set of hillclimbers on a simple subset of the hyperplane-defined functions, the pothole functions.",
        "published": "2001-04-06T21:15:31Z",
        "link": "http://arxiv.org/abs/cs/0104011v1",
        "categories": [
            "cs.NE",
            "nlin.AO",
            "I.2.m"
        ]
    },
    {
        "title": "Extremal Optimization for Graph Partitioning",
        "authors": [
            "S. Boettcher",
            "A. G. Percus"
        ],
        "summary": "Extremal optimization is a new general-purpose method for approximating solutions to hard optimization problems. We study the method in detail by way of the NP-hard graph partitioning problem. We discuss the scaling behavior of extremal optimization, focusing on the convergence of the average run as a function of runtime and system size. The method has a single free parameter, which we determine numerically and justify using a simple argument. Our numerical results demonstrate that on random graphs, extremal optimization maintains consistent accuracy for increasing system sizes, with an approximation error decreasing over runtime roughly as a power law t^(-0.4). On geometrically structured graphs, the scaling of results from the average run suggests that these are far from optimal, with large fluctuations between individual trials. But when only the best runs are considered, results consistent with theoretical arguments are recovered.",
        "published": "2001-04-12T00:15:12Z",
        "link": "http://arxiv.org/abs/cond-mat/0104214v1",
        "categories": [
            "cond-mat.stat-mech",
            "cs.NE",
            "math.OC"
        ]
    },
    {
        "title": "Market-Based Reinforcement Learning in Partially Observable Worlds",
        "authors": [
            "Ivo Kwee",
            "Marcus Hutter",
            "Juergen Schmidhuber"
        ],
        "summary": "Unlike traditional reinforcement learning (RL), market-based RL is in principle applicable to worlds described by partially observable Markov Decision Processes (POMDPs), where an agent needs to learn short-term memories of relevant previous events in order to execute optimal actions. Most previous work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here we reimplement a recent approach to market-based RL and for the first time evaluate it in a toy POMDP setting.",
        "published": "2001-05-15T19:07:28Z",
        "link": "http://arxiv.org/abs/cs/0105025v1",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.MA",
            "cs.NE",
            "I.2"
        ]
    },
    {
        "title": "Artificial Neurons with Arbitrarily Complex Internal Structures",
        "authors": [
            "G. A. Kohring"
        ],
        "summary": "Artificial neurons with arbitrarily complex internal structure are introduced. The neurons can be described in terms of a set of internal variables, a set activation functions which describe the time evolution of these variables and a set of characteristic functions which control how the neurons interact with one another. The information capacity of attractor networks composed of these generalized neurons is shown to reach the maximum allowed bound. A simple example taken from the domain of pattern recognition demonstrates the increased computational power of these neurons. Furthermore, a specific class of generalized neurons gives rise to a simple transformation relating attractor networks of generalized neurons to standard three layer feed-forward networks. Given this correspondence, we conjecture that the maximum information capacity of a three layer feed-forward network is 2 bits per weight.",
        "published": "2001-08-17T11:30:12Z",
        "link": "http://arxiv.org/abs/cs/0108009v1",
        "categories": [
            "cs.NE",
            "q-bio.NC",
            "I.5.1"
        ]
    },
    {
        "title": "On Classes of Functions for which No Free Lunch Results Hold",
        "authors": [
            "Christian Igel",
            "Marc Toussaint"
        ],
        "summary": "In a recent paper it was shown that No Free Lunch results hold for any subset F of the set of all possible functions from a finite set X to a finite set Y iff F is closed under permutation of X. In this article, we prove that the number of those subsets can be neglected compared to the overall number of possible subsets. Further, we present some arguments why problem classes relevant in practice are not likely to be closed under permutation.",
        "published": "2001-08-21T15:00:47Z",
        "link": "http://arxiv.org/abs/cs/0108011v1",
        "categories": [
            "cs.NE",
            "math.OC",
            "nlin.AO",
            "G.1.6"
        ]
    },
    {
        "title": "Alife Model of Evolutionary Emergence of Purposeful Adaptive Behavior",
        "authors": [
            "Mikhail S. Burtsev",
            "Vladimir G. Redko",
            "Roman V. Gusarev"
        ],
        "summary": "The process of evolutionary emergence of purposeful adaptive behavior is investigated by means of computer simulations. The model proposed implies that there is an evolving population of simple agents, which have two natural needs: energy and reproduction. Any need is characterized quantitatively by a corresponding motivation. Motivations determine goal-directed behavior of agents. The model demonstrates that purposeful behavior does emerge in the simulated evolutionary processes. Emergence of purposefulness is accompanied by origin of a simple hierarchy in the control system of agents.",
        "published": "2001-10-08T11:34:34Z",
        "link": "http://arxiv.org/abs/cs/0110021v1",
        "categories": [
            "cs.NE",
            "I.2.6; I.2.8; I.2.11"
        ]
    },
    {
        "title": "Jamming Model for the Extremal Optimization Heuristic",
        "authors": [
            "S. Boettcher",
            "M. Grigni"
        ],
        "summary": "Extremal Optimization, a recently introduced meta-heuristic for hard optimization problems, is analyzed on a simple model of jamming. The model is motivated first by the problem of finding lowest energy configurations for a disordered spin system on a fixed-valence graph. The numerical results for the spin system exhibit the same phenomena found in all earlier studies of extremal optimization, and our analytical results for the model reproduce many of these features.",
        "published": "2001-10-09T04:53:33Z",
        "link": "http://arxiv.org/abs/cond-mat/0110165v2",
        "categories": [
            "cond-mat.stat-mech",
            "cs.NE",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Decomposing Finite Abelian Groups",
        "authors": [
            "Kevin K. H. Cheung",
            "Michele Mosca"
        ],
        "summary": "This paper describes a quantum algorithm for efficiently decomposing finite Abelian groups. Such a decomposition is needed in order to apply the Abelian hidden subgroup algorithm. Such a decomposition (assuming the Generalized Riemann Hypothesis) also leads to an efficient algorithm for computing class numbers (known to be at least as difficult as factoring).",
        "published": "2001-01-05T15:32:16Z",
        "link": "http://arxiv.org/abs/cs/0101004v1",
        "categories": [
            "cs.DS",
            "quant-ph",
            "F.1"
        ]
    },
    {
        "title": "An Even Faster and More Unifying Algorithm for Comparing Trees via   Unbalanced Bipartite Matchings",
        "authors": [
            "Ming-Yang Kao",
            "Tak-Wah Lam",
            "Wing-Kin Sung",
            "Hing-Fung Ting"
        ],
        "summary": "A widely used method for determining the similarity of two labeled trees is to compute a maximum agreement subtree of the two trees. Previous work on this similarity measure is only concerned with the comparison of labeled trees of two special kinds, namely, uniformly labeled trees (i.e., trees with all their nodes labeled by the same symbol) and evolutionary trees (i.e., leaf-labeled trees with distinct symbols for distinct leaves). This paper presents an algorithm for comparing trees that are labeled in an arbitrary manner. In addition to this generality, this algorithm is faster than the previous algorithms.   Another contribution of this paper is on maximum weight bipartite matchings. We show how to speed up the best known matching algorithms when the input graphs are node-unbalanced or weight-unbalanced. Based on these enhancements, we obtain an efficient algorithm for a new matching problem called the hierarchical bipartite matching problem, which is at the core of our maximum agreement subtree algorithm.",
        "published": "2001-01-14T03:31:56Z",
        "link": "http://arxiv.org/abs/cs/0101010v2",
        "categories": [
            "cs.CV",
            "cs.DS",
            "F.2.2; I.5; J.3"
        ]
    },
    {
        "title": "Multiple-Size Divide-and-Conquer Recurrences",
        "authors": [
            "Ming-Yang Kao"
        ],
        "summary": "This short note reports a master theorem on tight asymptotic solutions to divide-and-conquer recurrences with more than one recursive term: for example, T(n) = 1/4 T(n/16) + 1/3 T(3n/5) + 4 T(n/100) + 10 T(n/300) + n^2.",
        "published": "2001-01-15T02:31:54Z",
        "link": "http://arxiv.org/abs/cs/0101011v1",
        "categories": [
            "cs.GL",
            "cs.DS",
            "F.2"
        ]
    },
    {
        "title": "On the problem of computing the well-founded semantics",
        "authors": [
            "Zbigniew Lonc",
            "Miroslaw Truszczynski"
        ],
        "summary": "The well-founded semantics is one of the most widely studied and used semantics of logic programs with negation. In the case of finite propositional programs, it can be computed in polynomial time, more specifically, in O(|At(P)|size(P)) steps, where size(P) denotes the total number of occurrences of atoms in a logic program P. This bound is achieved by an algorithm introduced by Van Gelder and known as the alternating-fixpoint algorithm. Improving on the alternating-fixpoint algorithm turned out to be difficult. In this paper we study extensions and modifications of the alternating-fixpoint approach. We then restrict our attention to the class of programs whose rules have no more than one positive occurrence of an atom in their bodies. For programs in that class we propose a new implementation of the alternating-fixpoint method in which false atoms are computed in a top-down fashion. We show that our algorithm is faster than other known algorithms and that for a wide class of programs it is linear and so, asymptotically optimal.",
        "published": "2001-01-17T13:33:12Z",
        "link": "http://arxiv.org/abs/cs/0101014v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.DS",
            "I.2.3; F.2.2"
        ]
    },
    {
        "title": "A Dynamic Programming Approach to De Novo Peptide Sequencing via Tandem   Mass Spectrometry",
        "authors": [
            "Ting Chen",
            "Ming-Yang Kao",
            "Matthew Tepel",
            "John Rush",
            "George M. Church"
        ],
        "summary": "The tandem mass spectrometry fragments a large number of molecules of the same peptide sequence into charged prefix and suffix subsequences, and then measures mass/charge ratios of these ions. The de novo peptide sequencing problem is to reconstruct the peptide sequence from a given tandem mass spectral data of k ions. By implicitly transforming the spectral data into an NC-spectrum graph G=(V,E) where |V|=2k+2, we can solve this problem in O(|V|+|E|) time and O(|V|) space using dynamic programming. Our approach can be further used to discover a modified amino acid in O(|V||E|) time and to analyze data with other types of noise in O(|V||E|) time. Our algorithms have been implemented and tested on actual experimental data.",
        "published": "2001-01-18T03:10:58Z",
        "link": "http://arxiv.org/abs/cs/0101016v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2; J.3"
        ]
    },
    {
        "title": "A Fast General Methodology for Information-Theoretically Optimal   Encodings of Graphs",
        "authors": [
            "Xin He",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "We propose a fast methodology for encoding graphs with information-theoretically minimum numbers of bits. Specifically, a graph with property pi is called a pi-graph. If pi satisfies certain properties, then an n-node m-edge pi-graph G can be encoded by a binary string X such that (1) G and X can be obtained from each other in O(n log n) time, and (2) X has at most beta(n)+o(beta(n)) bits for any continuous super-additive function beta(n) so that there are at most 2^{beta(n)+o(beta(n))} distinct n-node pi-graphs. The methodology is applicable to general classes of graphs; this paper focuses on planar graphs. Examples of such pi include all conjunctions over the following groups of properties: (1) G is a planar graph or a plane graph; (2) G is directed or undirected; (3) G is triangulated, triconnected, biconnected, merely connected, or not required to be connected; (4) the nodes of G are labeled with labels from {1, ..., ell_1} for ell_1 <= n; (5) the edges of G are labeled with labels from {1, ..., ell_2} for ell_2 <= m; and (6) each node (respectively, edge) of G has at most ell_3 = O(1) self-loops (respectively, ell_4 = O(1) multiple edges). Moreover, ell_3 and ell_4 are not required to be O(1) for the cases of pi being a plane triangulation. These examples are novel applications of small cycle separators of planar graphs and are the only nontrivial classes of graphs, other than rooted trees, with known polynomial-time information-theoretically optimal coding schemes.",
        "published": "2001-01-23T00:17:50Z",
        "link": "http://arxiv.org/abs/cs/0101021v1",
        "categories": [
            "cs.DS",
            "cs.GR",
            "E.4; F.2.2"
        ]
    },
    {
        "title": "On-Line Difference Maximization",
        "authors": [
            "Ming-Yang Kao",
            "Stephen R. Tate"
        ],
        "summary": "In this paper we examine problems motivated by on-line financial problems and stochastic games. In particular, we consider a sequence of entirely arbitrary distinct values arriving in random order, and must devise strategies for selecting low values followed by high values in such a way as to maximize the expected gain in rank from low values to high values.   First, we consider a scenario in which only one low value and one high value may be selected. We give an optimal on-line algorithm for this scenario, and analyze it to show that, surprisingly, the expected gain is n-O(1), and so differs from the best possible off-line gain by only a constant additive term (which is, in fact, fairly small -- at most 15).   In a second scenario, we allow multiple nonoverlapping low/high selections, where the total gain for our algorithm is the sum of the individual pair gains. We also give an optimal on-line algorithm for this problem, where the expected gain is n^2/8-\\Theta(n\\log n). An analysis shows that the optimal expected off-line gain is n^2/6+\\Theta(1), so the performance of our on-line algorithm is within a factor of 3/4 of the best off-line strategy.",
        "published": "2001-01-23T13:58:49Z",
        "link": "http://arxiv.org/abs/cs/0101024v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.1.6; G.2.1; G.2.3; G.3"
        ]
    },
    {
        "title": "Optimal Constructions of Hybrid Algorithms",
        "authors": [
            "Ming-Yang Kao",
            "Yuan Ma",
            "Michael Sipser",
            "Yiqun Yin"
        ],
        "summary": "We study on-line strategies for solving problems with hybrid algorithms. There is a problem Q and w basic algorithms for solving Q. For some lambda <= w, we have a computer with lambda disjoint memory areas, each of which can be used to run a basic algorithm and store its intermediate results. In the worst case, only one basic algorithm can solve Q in finite time, and all the other basic algorithms run forever without solving Q. To solve Q with a hybrid algorithm constructed from the basic algorithms, we run a basic algorithm for some time, then switch to another, and continue this process until Q is solved. The goal is to solve Q in the least amount of time. Using competitive ratios to measure the efficiency of a hybrid algorithm, we construct an optimal deterministic hybrid algorithm and an efficient randomized hybrid algorithm. This resolves an open question on searching with multiple robots posed by Baeza-Yates, Culberson and Rawlins. We also prove that our randomized algorithm is optimal for lambda = 1, settling a conjecture of Kao, Reif and Tate.",
        "published": "2001-01-26T01:44:19Z",
        "link": "http://arxiv.org/abs/cs/0101028v1",
        "categories": [
            "cs.DM",
            "cs.DS",
            "F.2.2"
        ]
    },
    {
        "title": "Tree Contractions and Evolutionary Trees",
        "authors": [
            "Ming-Yang Kao"
        ],
        "summary": "An evolutionary tree is a rooted tree where each internal vertex has at least two children and where the leaves are labeled with distinct symbols representing species. Evolutionary trees are useful for modeling the evolutionary history of species. An agreement subtree of two evolutionary trees is an evolutionary tree which is also a topological subtree of the two given trees. We give an algorithm to determine the largest possible number of leaves in any agreement subtree of two trees T_1 and T_2 with n leaves each. If the maximum degree d of these trees is bounded by a constant, the time complexity is O(n log^2(n)) and is within a log(n) factor of optimal. For general d, this algorithm runs in O(n d^2 log(d) log^2(n)) time or alternatively in O(n d sqrt(d) log^3(n)) time.",
        "published": "2001-01-26T21:36:30Z",
        "link": "http://arxiv.org/abs/cs/0101030v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; J.3"
        ]
    },
    {
        "title": "Cavity Matchings, Label Compressions, and Unrooted Evolutionary Trees",
        "authors": [
            "Ming-Yang Kao",
            "Tak-Wah Lam",
            "Wing-Kin Sung",
            "Hing-Fung Ting"
        ],
        "summary": "We present an algorithm for computing a maximum agreement subtree of two unrooted evolutionary trees. It takes O(n^{1.5} log n) time for trees with unbounded degrees, matching the best known time complexity for the rooted case. Our algorithm allows the input trees to be mixed trees, i.e., trees that may contain directed and undirected edges at the same time. Our algorithm adopts a recursive strategy exploiting a technique called label compression. The backbone of this technique is an algorithm that computes the maximum weight matchings over many subgraphs of a bipartite graph as fast as it takes to compute a single matching.",
        "published": "2001-01-26T23:59:55Z",
        "link": "http://arxiv.org/abs/cs/0101031v2",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; J.3"
        ]
    },
    {
        "title": "Total Protection of Analytic Invariant Information in Cross Tabulated   Tables",
        "authors": [
            "Ming-Yang Kao"
        ],
        "summary": "To protect sensitive information in a cross tabulated table, it is a common practice to suppress some of the cells in the table. An analytic invariant is a power series in terms of the suppressed cells that has a unique feasible value and a convergence radius equal to +\\infty. Intuitively, the information contained in an invariant is not protected even though the values of the suppressed cells are not disclosed. This paper gives an optimal linear-time algorithm for testing whether there exist nontrivial analytic invariants in terms of the suppressed cells in a given set of suppressed cells. This paper also presents NP-completeness results and an almost linear-time algorithm for the problem of suppressing the minimum number of cells in addition to the sensitive ones so that the resulting table does not leak analytic invariant information about a given set of suppressed cells.",
        "published": "2001-01-27T01:36:52Z",
        "link": "http://arxiv.org/abs/cs/0101032v1",
        "categories": [
            "cs.CR",
            "cs.DM",
            "cs.DS",
            "F.2.2; H.2.8; H.2.0"
        ]
    },
    {
        "title": "Linear-Time Succinct Encodings of Planar Graphs via Canonical Orderings",
        "authors": [
            "Xin He",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "Let G be an embedded planar undirected graph that has n vertices, m edges, and f faces but has no self-loop or multiple edge. If G is triangulated, we can encode it using {4/3}m-1 bits, improving on the best previous bound of about 1.53m bits. In case exponential time is acceptable, roughly 1.08m bits have been known to suffice. If G is triconnected, we use at most (2.5+2\\log{3})\\min\\{n,f\\}-7 bits, which is at most 2.835m bits and smaller than the best previous bound of 3m bits. Both of our schemes take O(n) time for encoding and decoding.",
        "published": "2001-01-27T02:05:33Z",
        "link": "http://arxiv.org/abs/cs/0101033v1",
        "categories": [
            "cs.DS",
            "cs.GR",
            "E.4; F.2.2"
        ]
    },
    {
        "title": "Data Security Equals Graph Connectivity",
        "authors": [
            "Ming-Yang Kao"
        ],
        "summary": "To protect sensitive information in a cross tabulated table, it is a common practice to suppress some of the cells in the table. This paper investigates four levels of data security of a two-dimensional table concerning the effectiveness of this practice. These four levels of data security protect the information contained in, respectively, individual cells, individual rows and columns, several rows or columns as a whole, and a table as a whole. The paper presents efficient algorithms and NP-completeness results for testing and achieving these four levels of data security. All these complexity results are obtained by means of fundamental equivalences between the four levels of data security of a table and four types of connectivity of a graph constructed from that table.",
        "published": "2001-01-27T02:22:54Z",
        "link": "http://arxiv.org/abs/cs/0101034v1",
        "categories": [
            "cs.CR",
            "cs.DB",
            "cs.DS",
            "F.2.2; H.2.0; H.2.8"
        ]
    },
    {
        "title": "Time and Space Bounds for Reversible Simulation",
        "authors": [
            "Harry Buhrman",
            "J. Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We prove a general upper bound on the tradeoff between time and space that suffices for the reversible simulation of irreversible computation. Previously, only simulations using exponential time or quadratic space were known.   The tradeoff shows for the first time that we can simultaneously achieve subexponential time and subquadratic space.   The boundary values are the exponential time with hardly any extra space required by the Lange-McKenzie-Tapp method and the ($\\log 3$)th power time with square space required by the Bennett method. We also give the first general lower bound on the extra storage space required by general reversible simulation. This lower bound is optimal in that it is achieved by some reversible simulations.",
        "published": "2001-01-29T17:32:07Z",
        "link": "http://arxiv.org/abs/quant-ph/0101133v2",
        "categories": [
            "quant-ph",
            "cs.CC",
            "cs.DS"
        ]
    },
    {
        "title": "Orderly Spanning Trees with Applications",
        "authors": [
            "Yi-Ting Chiang",
            "Ching-Chi Lin",
            "Hsueh-I Lu"
        ],
        "summary": "We introduce and study the {\\em orderly spanning trees} of plane graphs. This algorithmic tool generalizes {\\em canonical orderings}, which exist only for triconnected plane graphs. Although not every plane graph admits an orderly spanning tree, we provide an algorithm to compute an {\\em orderly pair} for any connected planar graph $G$, consisting of a plane graph $H$ of $G$, and an orderly spanning tree of $H$. We also present several applications of orderly spanning trees: (1) a new constructive proof for Schnyder's Realizer Theorem, (2) the first area-optimal 2-visibility drawing of $G$, and (3) the best known encodings of $G$ with O(1)-time query support. All algorithms in this paper run in linear time.",
        "published": "2001-02-07T15:44:49Z",
        "link": "http://arxiv.org/abs/cs/0102006v3",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2; E.4; E.1"
        ]
    },
    {
        "title": "Compact Encodings of Planar Graphs via Canonical Orderings and Multiple   Parentheses",
        "authors": [
            "Richie Chih-Nan Chuang",
            "Ashim Garg",
            "Xin He",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "Let G be a plane graph of n nodes, m edges, f faces, and no self-loop. G need not be connected or simple (i.e., free of multiple edges). We give three sets of coding schemes for G which all take O(m+n) time for encoding and decoding. Our schemes employ new properties of canonical orderings for planar graphs and new techniques of processing strings of multiple types of parentheses.   For applications that need to determine in O(1) time the adjacency of two nodes and the degree of a node, we use 2m+(5+1/k)n + o(m+n) bits for any constant k > 0 while the best previous bound by Munro and Raman is 2m+8n + o(m+n). If G is triconnected or triangulated, our bit count decreases to 2m+3n + o(m+n) or 2m+2n + o(m+n), respectively. If G is simple, our bit count is (5/3)m+(5+1/k)n + o(n) for any constant k > 0. Thus, if a simple G is also triconnected or triangulated, then 2m+2n + o(n) or 2m+n + o(n) bits suffice, respectively.   If only adjacency queries are supported, the bit counts for a general G and a simple G become 2m+(14/3)n + o(m+n) and (4/3)m+5n + o(n), respectively.   If we only need to reconstruct G from its code, a simple and triconnected G uses roughly 2.38m + O(1) bits while the best previous bound by He, Kao, and Lu is 2.84m.",
        "published": "2001-02-07T16:46:24Z",
        "link": "http://arxiv.org/abs/cs/0102005v2",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2; E.4"
        ]
    },
    {
        "title": "Common-Face Embeddings of Planar Graphs",
        "authors": [
            "Zhi-Zhong Chen",
            "Xin He",
            "Ming-Yang Kao"
        ],
        "summary": "Given a planar graph G and a sequence C_1,...,C_q, where each C_i is a family of vertex subsets of G, we wish to find a plane embedding of G, if any exists, such that for each i in {1,...,q}, there is a face F_i in the embedding whose boundary contains at least one vertex from each set in C_i. This problem has applications to the recovery of topological information from geographical data and the design of constrained layouts in VLSI. Let I be the input size, i.e., the total number of vertices and edges in G and the families C_i, counting multiplicity. We show that this problem is NP-complete in general. We also show that it is solvable in O(I log I) time for the special case where for each input family C_i, each set in C_i induces a connected subgraph of the input graph G. Note that the classical problem of simply finding a planar embedding is a further special case of this case with q=0. Therefore, the processing of the additional constraints C_1,...,C_q only incurs a logarithmic factor of overhead.",
        "published": "2001-02-10T02:58:56Z",
        "link": "http://arxiv.org/abs/cs/0102007v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "G.2.2"
        ]
    },
    {
        "title": "Optimal Bid Sequences for Multiple-Object Auctions with Unequal Budgets",
        "authors": [
            "Yuyu Chen",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "In a multiple-object auction, every bidder tries to win as many objects as possible with a bidding algorithm. This paper studies position-randomized auctions, which form a special class of multiple-object auctions where a bidding algorithm consists of an initial bid sequence and an algorithm for randomly permuting the sequence. We are especially concerned with situations where some bidders know the bidding algorithms of others. For the case of only two bidders, we give an optimal bidding algorithm for the disadvantaged bidder. Our result generalizes previous work by allowing the bidders to have unequal budgets. One might naturally anticipate that the optimal expected numbers of objects won by the bidders would be proportional to their budgets. Surprisingly, this is not true. Our new algorithm runs in optimal O(n) time in a straightforward manner. The case with more than two bidders is open.",
        "published": "2001-02-10T03:58:04Z",
        "link": "http://arxiv.org/abs/cs/0102008v1",
        "categories": [
            "cs.CE",
            "cs.DM",
            "cs.DS",
            "F.2.2; G.2.1; J.4"
        ]
    },
    {
        "title": "Optimal Augmentation for Bipartite Componentwise Biconnectivity in   Linear Time",
        "authors": [
            "Tsan-sheng Hsu",
            "Ming-Yang Kao"
        ],
        "summary": "A graph is componentwise biconnected if every connected component either is an isolated vertex or is biconnected. We present a linear-time algorithm for the problem of adding the smallest number of edges to make a bipartite graph componentwise biconnected while preserving its bipartiteness. This algorithm has immediate applications for protecting sensitive information in statistical tables.",
        "published": "2001-02-10T04:49:41Z",
        "link": "http://arxiv.org/abs/cs/0102009v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "The Enhanced Double Digest Problem for DNA Physical Mapping",
        "authors": [
            "Ming-Yang Kao",
            "Jared Samet",
            "Wing-Kin Sung"
        ],
        "summary": "The double digest problem is a common NP-hard approach to constructing physical maps of DNA sequences. This paper presents a new approach called the enhanced double digest problem. Although this new problem is also NP-hard, it can be solved in linear time in certain theoretically interesting cases.",
        "published": "2001-02-10T14:13:07Z",
        "link": "http://arxiv.org/abs/cs/0102010v1",
        "categories": [
            "cs.CE",
            "cs.DM",
            "cs.DS",
            "F.2.2; G.2.3; J.3"
        ]
    },
    {
        "title": "Construction of an algorithm in parallel for the Fast Fourier Transform",
        "authors": [
            "G. Mario A. Higuera",
            "Humberto Sarria",
            "Diana Fonseca",
            "John Idarraga"
        ],
        "summary": "It has been designed,built and executed a code for the Fast Fourier Transform (FFT),compiled and executed in a cluster of 2^n computers under the operating system MacOS and using the routines MacMPI. As practical application,the code has been used to obtain the transformed from an astronomic imagen,to execute a filter on its and with a transformed inverse to recover the image with the variates given by the filter.The computers arrangement are installed in the Observatorio Astronomico National in Colombia under the name OAN Cluster and in this has been executed several applications.",
        "published": "2001-03-01T20:40:06Z",
        "link": "http://arxiv.org/abs/cs/0103001v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.1.3"
        ]
    },
    {
        "title": "The Existential Theory of Equations with Rational Constraints in Free   Groups is PSPACE-Complete",
        "authors": [
            "Volker Diekert",
            "Claudio Gutierrez",
            "Christian Hagenah"
        ],
        "summary": "It is known that the existential theory of equations in free groups is decidable. This is a famous result of Makanin. On the other hand it has been shown that the scheme of his algorithm is not primitive recursive. In this paper we present an algorithm that works in polynomial space, even in the more general setting where each variable has a rational constraint, that is, the solution has to respect a specification given by a regular word language. Our main result states that the existential theory of equations in free groups with rational constraints is PSPACE-complete. We obtain this result as a corollary of the corresponding statement about free monoids with involution.",
        "published": "2001-03-26T15:58:40Z",
        "link": "http://arxiv.org/abs/cs/0103018v1",
        "categories": [
            "cs.DS",
            "cs.LO",
            "F.2.2; F.4"
        ]
    },
    {
        "title": "A Dualheap Selection Algorithm - A Call for Analysis",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "An algorithm is presented that efficiently solves the selection problem: finding the k-th smallest member of a set. Relevant to a divide-and-conquer strategy, the algorithm also partitions a set into small and large valued subsets. Applied recursively, this partitioning results in a sorted set. The algorithm's applicability is therefore much broader than just the selection problem. The presented algorithm is based upon R.W. Floyd's 1964 algorithm that constructs a heap from the bottom-up. Empirically, the presented algorithm's performance appears competitive with the popular quickselect algorithm, a variant of C.A.R. Hoare's 1962 quicksort algorithm. Furthermore, constructing a heap from the bottom-up is an inherently parallel process (processors can work independently and simultaneously on subheap construction), suggesting a performance advantage with parallel implementations. Given the presented algorithm's broad applicability, simplicity, serial performance, and parallel nature, further study is warranted. Specifically, worst-case analysis is an important but still unsolved problem.",
        "published": "2001-03-28T18:17:19Z",
        "link": "http://arxiv.org/abs/cs/0103023v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; E.1"
        ]
    },
    {
        "title": "Notes on computing peaks in k-levels and parametric spanning trees",
        "authors": [
            "Naoki Katoh",
            "Takeshi Tokuyama"
        ],
        "summary": "We give an algorithm to compute all the local peaks in the $k$-level of an arrangement of $n$ lines in $O(n \\log n) + \\tilde{O}((kn)^{2/3})$ time. We can also find $\\tau$ largest peaks in $O(n \\log ^2 n) + \\tilde{O}((\\tau n)^{2/3})$ time. Moreover, we consider the longest edge in a parametric minimum spanning tree (in other words, a bottleneck edge for connectivity), and give an algorithm to compute the parameter value (within a given interval) maximizing/minimizing the length of the longest edge in MST. The time complexity is $\\tilde{O}(n^{8/7}k^{1/7} + n k^{1/3})$",
        "published": "2001-03-29T04:53:47Z",
        "link": "http://arxiv.org/abs/cs/0103024v1",
        "categories": [
            "cs.CG",
            "cs.DS",
            "F2.2"
        ]
    },
    {
        "title": "Mantaining Dynamic Matrices for Fully Dynamic Transitive Closure",
        "authors": [
            "Camil Demetrescu",
            "Giuseppe F. Italiano"
        ],
        "summary": "In this paper we introduce a general framework for casting fully dynamic transitive closure into the problem of reevaluating polynomials over matrices. With this technique, we improve the best known bounds for fully dynamic transitive closure. In particular, we devise a deterministic algorithm for general directed graphs that achieves $O(n^2)$ amortized time for updates, while preserving unit worst-case cost for queries. In case of deletions only, our algorithm performs updates faster in O(n) amortized time.   Our matrix-based approach yields an algorithm for directed acyclic graphs that breaks through the $O(n^2)$ barrier on the single-operation complexity of fully dynamic transitive closure. We can answer queries in $O(n^\\epsilon)$ time and perform updates in $O(n^{\\omega(1,\\epsilon,1)-\\epsilon}+n^{1+\\epsilon})$ time, for any $\\epsilon\\in[0,1]$, where $\\omega(1,\\epsilon,1)$ is the exponent of the multiplication of an $n\\times n^{\\epsilon}$ matrix by an $n^{\\epsilon}\\times n$ matrix. The current best bounds on $\\omega(1,\\epsilon,1)$ imply an $O(n^{0.58})$ query time and an $O(n^{1.58})$ update time. Our subquadratic algorithm is randomized, and has one-side error.",
        "published": "2001-03-31T09:27:20Z",
        "link": "http://arxiv.org/abs/cs/0104001v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "F.2.2; G.2.2"
        ]
    },
    {
        "title": "Evaluating Recommendation Algorithms by Graph Analysis",
        "authors": [
            "Batul J. Mirza",
            "Benjamin J. Keller",
            "Naren Ramakrishnan"
        ],
        "summary": "We present a novel framework for evaluating recommendation algorithms in terms of the `jumps' that they make to connect people to artifacts. This approach emphasizes reachability via an algorithm within the implicit graph structure underlying a recommender dataset, and serves as a complement to evaluation in terms of predictive accuracy. The framework allows us to consider questions relating algorithmic parameters to properties of the datasets. For instance, given a particular algorithm `jump,' what is the average path length from a person to an artifact? Or, what choices of minimum ratings and jumps maintain a connected graph? We illustrate the approach with a common jump called the `hammock' using movie recommender datasets.",
        "published": "2001-04-03T22:07:28Z",
        "link": "http://arxiv.org/abs/cs/0104009v1",
        "categories": [
            "cs.IR",
            "cs.DM",
            "cs.DS",
            "H.4.2"
        ]
    },
    {
        "title": "The Gibbs Representation of 3D Rotations",
        "authors": [
            "Ian R. Peterson"
        ],
        "summary": "This paper revisits the little-known Gibbs-Rodrigues representation of rotations in a three-dimensional space and demonstrates a set of algorithms for handling it. In this representation the rotation is itself represented as a three-dimensional vector. The vector is parallel to the axis of rotation and its three components transform covariantly on change of coordinates. The mapping from rotations to vectors is 1:1 apart from computation error. The discontinuities of the representation require special handling but are not problematic. The rotation matrix can be generated efficiently from the vector without the use of transcendental functions, and vice-versa. The representation is more efficient than Euler angles, has affinities with Hassenpflug's Argyris angles and is very closely related to the quaternion representation. While the quaternion representation avoids the discontinuities inherent in any 3-component representation, this problem is readily overcome. The present paper gives efficient algorithms for computing the set of rotations which map a given vector to another of the same length and the rotation which maps a given pair of vectors to another pair of the same length and subtended angle.",
        "published": "2001-04-18T11:58:25Z",
        "link": "http://arxiv.org/abs/cs/0104016v4",
        "categories": [
            "cs.DS",
            "cs.CG",
            "B.2.4; F.2.1; G.1.0; I.4.0"
        ]
    },
    {
        "title": "Constraint Propagation in Presence of Arrays",
        "authors": [
            "Sebastian Brand"
        ],
        "summary": "We describe the use of array expressions as constraints, which represents a consequent generalisation of the \"element\" constraint. Constraint propagation for array constraints is studied theoretically, and for a set of domain reduction rules the local consistency they enforce, arc-consistency, is proved. An efficient algorithm is described that encapsulates the rule set and so inherits the capability to enforce arc-consistency from the rules.",
        "published": "2001-05-14T13:35:03Z",
        "link": "http://arxiv.org/abs/cs/0105024v1",
        "categories": [
            "cs.PL",
            "cs.DS",
            "D.3.3; E.1"
        ]
    },
    {
        "title": "When being Weak is Brave: Privacy in Recommender Systems",
        "authors": [
            "Naren Ramakrishnan",
            "Benjamin J. Keller",
            "Batul J. Mirza",
            "Ananth Y. Grama",
            "George Karypis"
        ],
        "summary": "We explore the conflict between personalization and privacy that arises from the existence of weak ties. A weak tie is an unexpected connection that provides serendipitous recommendations. However, information about weak ties could be used in conjunction with other sources of data to uncover identities and reveal other personal information. In this article, we use a graph-theoretic model to study the benefit and risk from weak ties.",
        "published": "2001-05-18T18:47:10Z",
        "link": "http://arxiv.org/abs/cs/0105028v1",
        "categories": [
            "cs.CR",
            "cs.DS",
            "H.4.2"
        ]
    },
    {
        "title": "Coloring k-colorable graphs using relatively small palettes",
        "authors": [
            "Eran Halperin",
            "Ram Nathaniel",
            "Uri Zwick"
        ],
        "summary": "We obtain the following new coloring results:   * A 3-colorable graph on $n$ vertices with maximum degree~$\\Delta$ can be colored, in polynomial time, using $O((\\Delta \\log\\Delta)^{1/3} \\cdot\\log{n})$ colors. This slightly improves an $O((\\Delta^{{1}/{3}} \\log^{1/2}\\Delta)\\cdot\\log{n})$ bound given by Karger, Motwani and Sudan. More generally, $k$-colorable graphs with maximum degree $\\Delta$ can be colored, in polynomial time, using $O((\\Delta^{1-{2}/{k}}\\log^{1/k}\\Delta) \\cdot\\log{n})$ colors.   * A 4-colorable graph on $n$ vertices can be colored, in polynomial time, using $\\Ot(n^{7/19})$ colors. This improves an $\\Ot(n^{2/5})$ bound given again by Karger, Motwani and Sudan. More generally, $k$-colorable graphs on $n$ vertices can be colored, in polynomial time, using $\\Ot(n^{\\alpha_k})$ colors, where $\\alpha_5=97/207$, $\\alpha_6=43/79$, $\\alpha_7=1391/2315$, $\\alpha_8=175/271$, ...   The first result is obtained by a slightly more refined probabilistic analysis of the semidefinite programming based coloring algorithm of Karger, Motwani and Sudan. The second result is obtained by combining the coloring algorithm of Karger, Motwani and Sudan, the combinatorial coloring algorithms of Blum and an extension of a technique of Alon and Kahale (which is based on the Karger, Motwani and Sudan algorithm) for finding relatively large independent sets in graphs that are guaranteed to have very large independent sets. The extension of the Alon and Kahale result may be of independent interest.",
        "published": "2001-05-21T11:56:18Z",
        "link": "http://arxiv.org/abs/cs/0105029v1",
        "categories": [
            "cs.DS",
            "cs.CC",
            "cs.DM",
            "F.2.2;G.2.2;G.3;G.1.6"
        ]
    },
    {
        "title": "Playing Games with Algorithms: Algorithmic Combinatorial Game Theory",
        "authors": [
            "Erik D. Demaine",
            "Robert A. Hearn"
        ],
        "summary": "Combinatorial games lead to several interesting, clean problems in algorithms and complexity theory, many of which remain open. The purpose of this paper is to provide an overview of the area to encourage further research. In particular, we begin with general background in Combinatorial Game Theory, which analyzes ideal play in perfect-information games, and Constraint Logic, which provides a framework for showing hardness. Then we survey results about the complexity of determining ideal play in these games, and the related problems of solving puzzles, in terms of both polynomial-time algorithms and computational intractability results. Our review of background and survey of algorithmic results are by no means complete, but should serve as a useful primer.",
        "published": "2001-06-11T03:49:59Z",
        "link": "http://arxiv.org/abs/cs/0106019v2",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "math.CO",
            "F.2.2; G.2.1; F.1.3"
        ]
    },
    {
        "title": "Building Views with Description Logics in ADE: Application Development   Environment",
        "authors": [
            "Larissa Ismailova",
            "Sergey Kosikov",
            "Konstantin Zinchenko",
            "Alexey Mikhaylov",
            "Lioubouv Bourmistrova",
            "Anastassiya Berezovskaya"
        ],
        "summary": "Any of views is formally defined within description logics that were established as a family of logics for modeling complex hereditary structures and have a suitable expressive power. This paper considers the Application Development Environment (ADE) over generalized variable concepts that are used to build database applications involving the supporting views. The front-end user interacts the database via separate ADE access mechanism intermediated by view support. The variety of applications may be generated that communicate with different and distinct desktop databases in a data warehouse. The advanced techniques allows to involve remote or stored procedures retrieval and call.",
        "published": "2001-06-12T12:23:20Z",
        "link": "http://arxiv.org/abs/cs/0106029v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.DS",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "The Risk Profile Problem for Stock Portfolio Optimization",
        "authors": [
            "Ming-Yang Kao",
            "Andreas Nolte",
            "Stephen R. Tate"
        ],
        "summary": "This work initiates research into the problem of determining an optimal investment strategy for investors with different attitudes towards the trade-offs of risk and profit. The probability distribution of the return values of the stocks that are considered by the investor are assumed to be known, while the joint distribution is unknown. The problem is to find the best investment strategy in order to minimize the probability of losing a certain percentage of the invested capital based on different attitudes of the investors towards future outcomes of the stock market.   For portfolios made up of two stocks, this work shows how to exactly and quickly solve the problem of finding an optimal portfolio for aggressive or risk-averse investors, using an algorithm based on a fast greedy solution to a maximum flow problem. However, an investor looking for an average-case guarantee (so is neither aggressive or risk-averse) must deal with a more difficult problem. In particular, it is #P-complete to compute the distribution function associated with the average-case bound. On the positive side, approximate answers can be computed by using random sampling techniques similar to those for high-dimensional volume estimation. When k>2 stocks are considered, it is proved that a simple solution based on the same flow concepts as the 2-stock algorithm would imply that P = NP, so is highly unlikely. This work gives approximation algorithms for this case as well as exact algorithms for some important special cases.",
        "published": "2001-07-03T15:18:48Z",
        "link": "http://arxiv.org/abs/cs/0107007v1",
        "categories": [
            "cs.CE",
            "cs.DM",
            "cs.DS",
            "E1; F2; G.1.6; G.1.10; G.2; G.3; J.4"
        ]
    },
    {
        "title": "Algorithms for Boolean Function Query Properties",
        "authors": [
            "Scott Aaronson"
        ],
        "summary": "We present new algorithms to compute fundamental properties of a Boolean function given in truth-table form. Specifically, we give an O(N^2.322 log N) algorithm for block sensitivity, an O(N^1.585 log N) algorithm for `tree decomposition,' and an O(N) algorithm for `quasisymmetry.' These algorithms are based on new insights into the structure of Boolean functions that may be of independent interest. We also give a subexponential-time algorithm for the space-bounded quantum query complexity of a Boolean function. To prove this algorithm correct, we develop a theory of limited-precision representation of unitary operators, building on work of Bernstein and Vazirani.",
        "published": "2001-07-05T08:00:57Z",
        "link": "http://arxiv.org/abs/cs/0107010v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "F.1.2; F.1.3; F.2.2"
        ]
    },
    {
        "title": "Distributed Broadcast in Wireless Networks with Unknown Topology",
        "authors": [
            "Andrea E. F. Clementi",
            "Angelo Monti",
            "Riccardo Silvestri"
        ],
        "summary": "A multi-hop synchronous wirelss network is said to be unknown if the nodes have no knowledge of the topology. A basic task in wireless network is that of broadcasting a message (created by a fixed source node) to all nodes of the network. The multi-broadcast that consists in performing a set of r independent broadcasts. In this paper, we study the completion and the termination time of distributed protocols for both the (single) broadcast and the multi-broadcast operations on unknown networks as functions of the number of nodes n, the maximum eccentricity D, the maximum in-degree Delta, and the congestion c of the networks. We establish new connections between these operations and some combinatorial concepts, such as selective families, strongly-selective families (also known as superimposed codes), and pairwise r-different families. Such connections, combined with a set of new lower and upper bounds on the size of the above families, allow us to derive new lower bounds and new distributed protocols for the broadcast and multi-broadcast operations. In particular, our upper bounds are almost tight and improve exponentially over the previous bounds when D and Delta are polylogarithmic in n. Network topologies having ``small'' eccentricity and ``small'' degree (such as bounded-degree expanders) are often used in practice to achieve efficient communication.",
        "published": "2001-07-06T16:18:37Z",
        "link": "http://arxiv.org/abs/cs/0107011v1",
        "categories": [
            "cs.DS",
            "cs.DM",
            "C.2.1; F.2.2"
        ]
    },
    {
        "title": "The Complexity of Clickomania",
        "authors": [
            "Therese C. Biedl",
            "Erik D. Demaine",
            "Martin L. Demaine",
            "Rudolf Fleischer",
            "Lars Jacobsen",
            "J. Ian Munro"
        ],
        "summary": "We study a popular puzzle game known variously as Clickomania and Same Game. Basically, a rectangular grid of blocks is initially colored with some number of colors, and the player repeatedly removes a chosen connected monochromatic group of at least two square blocks, and any blocks above it fall down. We show that one-column puzzles can be solved, i.e., the maximum possible number of blocks can be removed, in linear time for two colors, and in polynomial time for an arbitrary number of colors. On the other hand, deciding whether a puzzle is solvable (all blocks can be removed) is NP-complete for two columns and five colors, or five columns and three colors.",
        "published": "2001-07-21T14:55:20Z",
        "link": "http://arxiv.org/abs/cs/0107031v1",
        "categories": [
            "cs.CC",
            "cs.DM",
            "cs.DS",
            "F.2.2; F.1.3; F.1.1; G.2.1"
        ]
    },
    {
        "title": "Extreme Value Statistics and Traveling Fronts: An Application to   Computer Science",
        "authors": [
            "Satya N. Majumdar",
            "P. L. Krapivsky"
        ],
        "summary": "We study the statistics of height and balanced height in the binary search tree problem in computer science. The search tree problem is first mapped to a fragmentation problem which is then further mapped to a modified directed polymer problem on a Cayley tree. We employ the techniques of traveling fronts to solve the polymer problem and translate back to derive exact asymptotic properties in the original search tree problem. The second mapping allows us not only to re-derive the already known results for random binary trees but to obtain new exact results for search trees where the entries arrive according to an arbitrary distribution, not necessarily randomly. Besides it allows us to derive the asymptotic shape of the full probability distribution of height and not just its moments. Our results are then generalized to $m$-ary search trees with arbitrary distribution. An attempt has been made to make the article accessible to both physicists and computer scientists.",
        "published": "2001-09-18T08:30:19Z",
        "link": "http://arxiv.org/abs/cond-mat/0109313v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.dis-nn",
            "cs.DS"
        ]
    },
    {
        "title": "Routing Permutations in Partitioned Optical Passive Star Networks",
        "authors": [
            "Alessandro Mei",
            "Romeo Rizzi"
        ],
        "summary": "It is shown that a POPS network with g groups and d processors per group can efficiently route any permutation among the n=dg processors. The number of slots used is optimal in the worst case, and is at most the double of the optimum for all permutations p such that p(i)<>i for all i.",
        "published": "2001-09-18T10:05:26Z",
        "link": "http://arxiv.org/abs/cs/0109027v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "C.1.4"
        ]
    },
    {
        "title": "The Minimum Expectation Selection Problem",
        "authors": [
            "David Eppstein",
            "George Lueker"
        ],
        "summary": "We define the min-min expectation selection problem (resp. max-min expectation selection problem) to be that of selecting k out of n given discrete probability distributions, to minimize (resp. maximize) the expectation of the minimum value resulting when independent random variables are drawn from the selected distributions. We assume each distribution has finitely many atoms. Let d be the number of distinct values in the support of the distributions. We show that if d is a constant greater than 2, the min-min expectation problem is NP-complete but admits a fully polynomial time approximation scheme. For d an arbitrary integer, it is NP-hard to approximate the min-min expectation problem with any constant approximation factor. The max-min expectation problem is polynomially solvable for constant d; we leave open its complexity for variable d. We also show similar results for binary selection problems in which we must choose one distribution from each of n pairs of distributions.",
        "published": "2001-10-03T06:21:19Z",
        "link": "http://arxiv.org/abs/cs/0110011v1",
        "categories": [
            "cs.DS",
            "math.PR",
            "F.2.2; G.1.6"
        ]
    },
    {
        "title": "Counting is Easy",
        "authors": [
            "Joel Seiferas",
            "Paul Vitanyi"
        ],
        "summary": "For any fixed $k$, a remarkably simple single-tape Turing machine can simulate $k$ independent counters in real time. Informally, a counter is a storage unit that maintains a single integer (initially 0), incrementing it, decrementing it, or reporting its sign (positive, negative, or zero) on command. Any automaton that responds to each successive command as a counter would is said to simulate a counter. (Only for a sign inquiry is the response of interest, of course. And zeroness is the only real issue, since a simulator can readily use zero detection to keep track of positivity and negativity in finite-state control. In this paper we describe a remarkably simple real-time simulation, based on just five simple rewriting rules, of any fixed number $k$ of independent counters. On a Turing machine with a single, binary work tape, the simulation runs in real time, handling an arbitrary counter command at each step. The space used by the simulation can be held to $(k+\\epsilon) \\log_2 n$ bits for the first $n$ commands, for any specified $\\epsilon > 0$.",
        "published": "2001-10-18T13:21:01Z",
        "link": "http://arxiv.org/abs/cs/0110038v1",
        "categories": [
            "cs.CC",
            "cs.DS",
            "E.1, F.1.1., F.2.2, G.2.1, E.2, E.4"
        ]
    },
    {
        "title": "Distributed Computing for Localized and Multilayer Visualizations",
        "authors": [
            "Mark Burgin",
            "Walter Karplus",
            "Damon Liu"
        ],
        "summary": "The aim of this paper is to develop an approach to visualizations that benefits from distributed computing. Three schemes of process distribution are considered: parallel, pipeline, and expanding pipeline computations. Expanding pipeline structure synthesizes the advantages and traits of both parallel and pipeline computations. In expanding pipeline computing, a novel approach presented in this paper, a multiplicity of processes are concurrently developed in parallel and knotted processor pipelines. The theoretical foundations for expanding pipeline computing as a computational process are in the domains of alternating Turing machines, molecular computing, and E-machines. Expanding pipeline computing constitutes the development of the conventional pipeline architecture aimed at utilization of implicit parallel structures existing in algorithms. Such structures appear in various kinds of visualization. Image deriving and processing is a field that provides diverse opportunities for utilization of the advantages of distributed computing. The most relevant to the distributed architecture is stratified visualization with its two cases based on data localization and layer separation. Visualization is treated here as a special case of simulation. The conceptual approach to distributed computing developed in this paper have been applied to visualization in a computer support system, which is utilized in radiology and namely, for the noninvasive treatment of brain aneurysms.",
        "published": "2001-11-09T00:29:35Z",
        "link": "http://arxiv.org/abs/cs/0111022v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "F.1.2; I.1.2; H.3.4; I.4.10; J.3"
        ]
    },
    {
        "title": "Next Generation EPICS Interface to Abstract Data",
        "authors": [
            "J. Hill",
            "R. Lange"
        ],
        "summary": "The set of externally visible properties associated with process variables in the Experimental Physics and Industrial Control System (EPICS) is predefined in the EPICS base distribution and is therefore not extensible by plug-compatible applications. We believe that this approach, while practical for early versions of the system with a smaller user base, is now severely limiting expansion of the high-level application tool set for EPICS. To eliminate existing barriers, we propose a new C++ based interface to abstract containerized data. This paper describes the new interface, its application to message passing in distributed systems, its application to direct communication between tightly coupled programs co-resident in an address space, and its paramount position in an emerging role for EPICS - the integration of dissimilar systems.",
        "published": "2001-11-09T05:09:22Z",
        "link": "http://arxiv.org/abs/cs/0111026v1",
        "categories": [
            "cs.NI",
            "cs.DS",
            "c.2.4"
        ]
    },
    {
        "title": "Data Access - Experiences Implementing an Object Oriented Library on   Various Platforms",
        "authors": [
            "R. Lange",
            "J. Hill"
        ],
        "summary": "Data Access will be the next generation data abstraction layer for EPICS. Its implementation in C++ brought up a number of issues that are related to object oriented technology's impact on CPU and memory usage.   What is gained by the new abstract interface? What is the price that has to be paid for these gains? What compromises seem applicable and affordable?   This paper discusses tests that have been made about performance and memory usage as well as the different measures that have been taken to optimize the situation.",
        "published": "2001-11-12T16:11:39Z",
        "link": "http://arxiv.org/abs/cs/0111036v1",
        "categories": [
            "cs.SE",
            "cs.DS",
            "D.1.5; D.2.13"
        ]
    },
    {
        "title": "Arc consistency for soft constraints",
        "authors": [
            "Martin Cooper",
            "Thomas Schiex"
        ],
        "summary": "The notion of arc consistency plays a central role in constraint satisfaction. It is known that the notion of local consistency can be extended to constraint optimisation problems defined by soft constraint frameworks based on an idempotent cost combination operator. This excludes non idempotent operators such as + which define problems which are very important in practical applications such as Max-CSP, where the aim is to minimize the number of violated constraints. In this paper, we show that using a weak additional axiom satisfied by most existing soft constraints proposals, it is possible to define a notion of soft arc consistency that extends the classical notion of arc consistency and this even in the case of non idempotent cost combination operators. A polynomial time algorithm for enforcing this soft arc consistency exists and its space and time complexities are identical to that of enforcing arc consistency in CSPs when the cost combination operator is strictly monotonic (for example Max-CSP). A directional version of arc consistency is potentially even stronger than the non-directional version, since it allows non local propagation of penalties. We demonstrate the utility of directional arc consistency by showing that it not only solves soft constraint problems on trees, but that it also implies a form of local optimality, which we call arc irreducibility.",
        "published": "2001-11-14T12:06:42Z",
        "link": "http://arxiv.org/abs/cs/0111038v3",
        "categories": [
            "cs.AI",
            "cs.CC",
            "cs.DS",
            "I.2.8;F.4.1;D.3.3"
        ]
    },
    {
        "title": "Smoothed Analysis of Algorithms: Why the Simplex Algorithm Usually Takes   Polynomial Time",
        "authors": [
            "Daniel A. Spielman",
            "Shang-Hua Teng"
        ],
        "summary": "We introduce the smoothed analysis of algorithms, which is a hybrid of the worst-case and average-case analysis of algorithms. In smoothed analysis, we measure the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. We measure this performance in terms of both the input size and the magnitude of the perturbations. We show that the simplex algorithm has polynomial smoothed complexity.",
        "published": "2001-11-19T23:37:14Z",
        "link": "http://arxiv.org/abs/cs/0111050v7",
        "categories": [
            "cs.DS",
            "G.1.6"
        ]
    },
    {
        "title": "Predicting RNA Secondary Structures with Arbitrary Pseudoknots by   Maximizing the Number of Stacking Pairs",
        "authors": [
            "Samuel Ieong",
            "Ming-Yang Kao",
            "Tak-Wah Lam",
            "Wing-Kin Sung",
            "Siu-Ming Yiu"
        ],
        "summary": "The paper investigates the computational problem of predicting RNA secondary structures. The general belief is that allowing pseudoknots makes the problem hard. Existing polynomial-time algorithms are heuristic algorithms with no performance guarantee and can only handle limited types of pseudoknots. In this paper we initiate the study of predicting RNA secondary structures with a maximum number of stacking pairs while allowing arbitrary pseudoknots. We obtain two approximation algorithms with worst-case approximation ratios of 1/2 and 1/3 for planar and general secondary structures,respectively. For an RNA sequence of $n$ bases, the approximation algorithm for planar secondary structures runs in $O(n^3)$ time while that for the general case runs in linear time. Furthermore, we prove that allowing pseudoknots makes it NP-hard to maximize the number of stacking pairs in a planar secondary structure. This result is in contrast with the recent NP-hard results on psuedoknots which are based on optimizing some general and complicated energy functions.",
        "published": "2001-11-20T02:42:49Z",
        "link": "http://arxiv.org/abs/cs/0111051v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "q-bio",
            "F2.2; G2.3; J.3"
        ]
    },
    {
        "title": "The Floyd-Warshall Algorithm, the AP and the TSP",
        "authors": [
            "Howard Kleiman"
        ],
        "summary": "We use admissible permutations and a variant of the Floyd-Warshall algorithm to obtain an optimal solution to the Assignment Problem. Using another variant of the F-W algorithm, we obtain an approximate solution to the Traveling Salesman Problem. We also give a sufficient condition for the approximate solution to be an optimal solution.",
        "published": "2001-11-29T23:17:54Z",
        "link": "http://arxiv.org/abs/math/0111309v1",
        "categories": [
            "math.CO",
            "cs.DS"
        ]
    },
    {
        "title": "The Floyd-Warshall Algorithm, the AP and the TSP, Part II",
        "authors": [
            "Howard Kleiman"
        ],
        "summary": "In math.CO/0111309, we used admissible permutations and a variant of the Floyd-Warshall Algorithm to obtain an optimal solution to the Assignment Problem and an approximate solution to the Traveling Salesman Problem. Here we give a large, detailed illustration of how the algorithms are applied.",
        "published": "2001-12-06T06:52:33Z",
        "link": "http://arxiv.org/abs/math/0112052v1",
        "categories": [
            "math.CO",
            "cs.DS"
        ]
    },
    {
        "title": "A Straightforward Approach to Morphological Analysis and Synthesis",
        "authors": [
            "Kyriakos N. Sgarbas",
            "Nikos D. Fakotakis",
            "George K. Kokkinakis"
        ],
        "summary": "In this paper we present a lexicon-based approach to the problem of morphological processing. Full-form words, lemmas and grammatical tags are interconnected in a DAWG. Thus, the process of analysis/synthesis is reduced to a search in the graph, which is very fast and can be performed even if several pieces of information are missing from the input. The contents of the DAWG are updated using an on-line incremental process. The proposed approach is language independent and it does not utilize any morphophonetic rules or any other special linguistic information.",
        "published": "2001-12-10T13:01:21Z",
        "link": "http://arxiv.org/abs/cs/0112010v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "I.2.7; F.2.2; E.1"
        ]
    },
    {
        "title": "Fast Context-Free Grammar Parsing Requires Fast Boolean Matrix   Multiplication",
        "authors": [
            "Lillian Lee"
        ],
        "summary": "In 1975, Valiant showed that Boolean matrix multiplication can be used for parsing context-free grammars (CFGs), yielding the asympotically fastest (although not practical) CFG parsing algorithm known. We prove a dual result: any CFG parser with time complexity $O(g n^{3 - \\epsilson})$, where $g$ is the size of the grammar and $n$ is the length of the input string, can be efficiently converted into an algorithm to multiply $m \\times m$ Boolean matrices in time $O(m^{3 - \\epsilon/3})$.   Given that practical, substantially sub-cubic Boolean matrix multiplication algorithms have been quite difficult to find, we thus explain why there has been little progress in developing practical, substantially sub-cubic general CFG parsers. In proving this result, we also develop a formalization of the notion of parsing.",
        "published": "2001-12-15T19:43:17Z",
        "link": "http://arxiv.org/abs/cs/0112018v1",
        "categories": [
            "cs.CL",
            "cs.DS",
            "I.2.7; F.2.2"
        ]
    },
    {
        "title": "Faster Algorithm of String Comparison",
        "authors": [
            "Qi Xiao Yang",
            "Sung Sam Yuan",
            "Lu Chun",
            "Li Zhao",
            "Sun Peng"
        ],
        "summary": "In many applications, it is necessary to determine the string similarity. Edit distance[WF74] approach is a classic method to determine Field Similarity. A well known dynamic programming algorithm [GUS97] is used to calculate edit distance with the time complexity O(nm). (for worst case, average case and even best case) Instead of continuing with improving the edit distance approach, [LL+99] adopted a brand new approach-token-based approach. Its new concept of token-base-retain the original semantic information, good time complex-O(nm) (for worst, average and best case) and good experimental performance make it a milestone paper in this area. Further study indicates that there is still room for improvement of its Field Similarity algorithm. Our paper is to introduce a package of substring-based new algorithms to determine Field Similarity. Combined together, our new algorithms not only achieve higher accuracy but also gain the time complexity O(knm) (k<0.75) for worst case, O(*n) where <6 for average case and O(1) for best case. Throughout the paper, we use the approach of comparative examples to show higher accuracy of our algorithms compared to the one proposed in [LL+99]. Theoretical analysis, concrete examples and experimental result show that our algorithms can significantly improve the accuracy and time complexity of the calculation of Field Similarity. [US97] D. Guseld. Algorithms on Strings, Trees and Sequences, in Computer Science and Computational Biology. [LL+99] Mong Li Lee, Cleansing data for mining and warehousing, In Proceedings of the 10th International Conference on Database and Expert Systems Applications (DEXA99), pages 751-760,August 1999. [WF74] R. Wagner and M. Fisher, The String to String Correction Problem, JACM 21 pages 168-173, 1974.",
        "published": "2001-12-21T05:58:12Z",
        "link": "http://arxiv.org/abs/cs/0112022v2",
        "categories": [
            "cs.DS",
            "I.1.2"
        ]
    },
    {
        "title": "Open Archives Initiative protocol development and implementation at   arXiv",
        "authors": [
            "Simeon Warner"
        ],
        "summary": "I outline the involvement of the Los Alamos e-print archive (arXiv) within the Open Archives Initiative (OAI) and describe the implementation of the data provider side of the OAI protocol v1.0. I highlight the ways in which we map the existing structure of arXiv onto elements of the protocol.",
        "published": "2001-01-26T00:05:04Z",
        "link": "http://arxiv.org/abs/cs/0101027v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "The OLAC Metadata Set and Controlled Vocabularies",
        "authors": [
            "Steven Bird",
            "Gary Simons"
        ],
        "summary": "As language data and associated technologies proliferate and as the language resources community rapidly expands, it has become difficult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool can work with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to find language resources. This paper describes a new digital infrastructure for language resource discovery, based on the Open Archives Initiative, and called OLAC -- the Open Language Archives Community. The OLAC Metadata Set and the associated controlled vocabularies facilitate consistent description and focussed searching. We report progress on the metadata set and controlled vocabularies, describing current issues and soliciting input from the language resources community.",
        "published": "2001-05-21T16:53:55Z",
        "link": "http://arxiv.org/abs/cs/0105030v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Exposing and harvesting metadata using the OAI metadata harvesting   protocol: A tutorial",
        "authors": [
            "Simeon Warner"
        ],
        "summary": "In this article I outline the ideas behind the Open Archives Initiative metadata harvesting protocol (OAIMH), and attempt to clarify some common misconceptions. I then consider how the OAIMH protocol can be used to expose and harvest metadata. Perl code examples are given as practical illustration.",
        "published": "2001-06-28T23:08:20Z",
        "link": "http://arxiv.org/abs/cs/0106057v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Semantic Web Content Accessibility Guidelines for Current Research   Information Systems (CRIS)",
        "authors": [
            "A. Lopatenko"
        ],
        "summary": "The most exciting challenge for CRIS is to create a service for research information which should be wide-spread, distributed and actual like Google, but at the same time structured, trusted, with a complex search and navigation similar to today CRIS application. The core technology for such a \"new\" CRIS is the semantic web technology to integrate database contents with HTML and XML web pages for being provided to the research interested public. One (at the moment the best) possible way is to use RDF (Resource Description Framework) which is also recommended by the W3 consortium.",
        "published": "2001-07-29T18:50:17Z",
        "link": "http://arxiv.org/abs/cs/0107035v1",
        "categories": [
            "cs.NI",
            "cs.DL",
            "D.2.12;E.2;H.2.4"
        ]
    },
    {
        "title": "Links tell us about lexical and semantic Web content",
        "authors": [
            "Filippo Menczer"
        ],
        "summary": "The latest generation of Web search tools is beginning to exploit hypertext link information to improve ranking\\cite{Brin98,Kleinberg98} and crawling\\cite{Menczer00,Ben-Shaul99etal,Chakrabarti99} algorithms. The hidden assumption behind such approaches, a correlation between the graph structure of the Web and its content, has not been tested explicitly despite increasing research on Web topology\\cite{Lawrence98,Albert99,Adamic99,Butler00}. Here I formalize and quantitatively validate two conjectures drawing connections from link information to lexical and semantic Web content. The clink-content conjecture states that a page is similar to the pages that link to it, i.e., one can infer the lexical content of a page by looking at the pages that link to it. I also show that lexical inferences based on link cues are quite heterogeneous across Web communities. The link-cluster conjecture states that pages about the same topic are clustered together, i.e., one can infer the meaning of a page by looking at its neighbours. These results explain the success of the newest search technologies and open the way for more dynamic and scalable methods to locate information in a topic or user driven way.",
        "published": "2001-08-08T02:16:15Z",
        "link": "http://arxiv.org/abs/cs/0108004v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.1; H.3.3"
        ]
    },
    {
        "title": "The Open Language Archives Community and Asian Language Resources",
        "authors": [
            "Steven Bird",
            "Gary Simons",
            "Chu-Ren Huang"
        ],
        "summary": "The Open Language Archives Community (OLAC) is a new project to build a worldwide system of federated language archives based on the Open Archives Initiative and the Dublin Core Metadata Initiative. This paper aims to disseminate the OLAC vision to the language resources community in Asia, and to show language technologists and linguists how they can document their tools and data in such a way that others can easily discover them. We describe OLAC and the OLAC Metadata Set, then discuss two key issues in the Asian context: language classification and multilingual resource classification.",
        "published": "2001-10-03T12:45:41Z",
        "link": "http://arxiv.org/abs/cs/0110014v1",
        "categories": [
            "cs.CL",
            "cs.DL",
            "H.2.7; H.3.3; H.3.7; I.2.7; I.7.2; J.5"
        ]
    },
    {
        "title": "Information retrieval in Current Research Information Systems",
        "authors": [
            "Andrei Lopatenko"
        ],
        "summary": "In this paper we describe the requirements for research information systems and problems which arise in the development of such system. Here is shown which problems could be solved by using of knowledge markup technologies. Ontology for Research Information System offered. Architecture for collecting research data and providing access to it is described.",
        "published": "2001-10-10T15:28:00Z",
        "link": "http://arxiv.org/abs/cs/0110026v1",
        "categories": [
            "cs.IR",
            "cs.DL",
            "H.3.3; H.3.4; H.3.7"
        ]
    },
    {
        "title": "The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data",
        "authors": [
            "Alexander Szalay",
            "Jim Gray",
            "Ani Thakar",
            "Peter Z. Kunszt",
            "Tanu Malik",
            "Jordan Raddick",
            "Christopher Stoughton",
            "Jan vandenBerg"
        ],
        "summary": "The SkyServer provides Internet access to the public Sloan Digital Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and architecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.",
        "published": "2001-11-07T20:39:31Z",
        "link": "http://arxiv.org/abs/cs/0111015v1",
        "categories": [
            "cs.DL",
            "cs.DB",
            "H.3.5, H.4, J.2, H.2.8"
        ]
    },
    {
        "title": "Using Structural Metadata to Localize Experience of Digital Content",
        "authors": [
            "Naomi Dushay"
        ],
        "summary": "With the increasing technical sophistication of both information consumers and providers, there is increasing demand for more meaningful experiences of digital information. We present a framework that separates digital object experience, or rendering, from digital object storage and manipulation, so the rendering can be tailored to particular communities of users. Our framework also accommodates extensible digital object behaviors and interoperability. The two key components of our approach are 1) exposing structural metadata associated with digital objects -- metadata about the labeled access points within a digital object and 2) information intermediaries called context brokers that match structural characteristics of digital objects with mechanisms that produce behaviors. These context brokers allow for localized rendering of digital information stored externally.",
        "published": "2001-12-14T18:55:11Z",
        "link": "http://arxiv.org/abs/cs/0112017v1",
        "categories": [
            "cs.DL",
            "H.3.7"
        ]
    },
    {
        "title": "Automated Debugging In Java Using OCL And JDI",
        "authors": [
            "David J. Murray",
            "Dale E. Parson"
        ],
        "summary": "Correctness constraints provide a foundation for automated debugging within object-oriented systems. This paper discusses a new approach to incorporating correctness constraints into Java development environments. Our approach uses the Object Constraint Language (\"OCL\") as a specification language and the Java Debug Interface (\"JDI\") as a verification API. OCL provides a standard language for expressing object-oriented constraints that can integrate with Unified Modeling Language (\"UML\") software models. JDI provides a standard Java API capable of supporting type-safe and side effect free runtime constraint evaluation. The resulting correctness constraint mechanism: (1) entails no programming language modifications; (2) requires neither access nor changes to existing source code; and (3) works with standard off-the-shelf Java virtual machines (\"VMs\"). A prototype correctness constraint auditor is presented to demonstrate the utility of this mechanism for purposes of automated debugging.",
        "published": "2001-01-03T22:48:24Z",
        "link": "http://arxiv.org/abs/cs/0101002v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Slicing Event Traces of Large Software Systems",
        "authors": [
            "Raymond Smith",
            "Bogdan Korel"
        ],
        "summary": "Debugging of large software systems consisting of many processes accessing shared resources is a very difficult task. Many commercial systems record essential events during system execution for post-mortem analysis. However, the event traces of large and long-running systems can be quite voluminous. Analysis of such event traces to identify sources of incorrect behavior can be very tedious, error-prone, and inefficient. In this paper, we propose a novel technique of slicing event traces as a means of reducing the number of events for analysis. This technique identifies events that may have influenced observed incorrect system behavior. In order to recognize influencing events several types of dependencies between events are identified. These dependencies are determined automatically from an event trace. In order to improve the precision of slicing we propose to use additional dependencies, referred to as cause-effect dependencies, which can further reduce the size of sliced event traces. Our initial experience has shown that this slicing technique can significantly reduce the size of event traces for analysis.",
        "published": "2001-01-11T19:12:20Z",
        "link": "http://arxiv.org/abs/cs/0101005v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Assertion checker for the C programming language based on computations   over event traces",
        "authors": [
            "Mikhail Auguston"
        ],
        "summary": "This paper suggests an approach to the development of software testing and debugging automation tools based on precise program behavior models. The program behavior model is defined as a set of events (event trace) with two basic binary relations over events -- precedence and inclusion, and represents the temporal relationship between actions. A language for the computations over event traces is developed that provides a basis for assertion checking, debugging queries, execution profiles, and performance measurements. The approach is nondestructive, since assertion texts are separated from the target program source code and can be maintained independently. Assertions can capture the dynamic properties of a particular target program and can formalize the general knowledge of typical bugs and debugging strategies. An event grammar provides a sound basis for assertion language implementation via target program automatic instrumentation. An implementation architecture and preliminary experiments with a prototype assertion checker for the C programming language are discussed.",
        "published": "2001-01-12T01:13:27Z",
        "link": "http://arxiv.org/abs/cs/0101007v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "A Knowledge-based Automated Debugger in Learning System",
        "authors": [
            "Abdullah Mohd Zin",
            "Syed Ahmad Aljunid",
            "Zarina Shukur",
            "Mohd Jan Nordin"
        ],
        "summary": "Currently, programming instructors continually face the problem of helping to debug students' programs. Although there currently exist a number of debuggers and debugging tools in various platforms, most of these projects or products are crafted through the needs of software maintenance, and not through the perspective of teaching of programming. Moreover, most debuggers are too general, meant for experts as well as not user-friendly. We propose a new knowledge-based automated debugger to be used as a user-friendly tool by the students to self-debug their own programs. Stereotyped code (cliche) and bugs cliche will be stored as library of plans in the knowledge-base. Recognition of correct code or bugs is based on pattern matching and constraint satisfaction. Given a syntax error-free program and its specification, this debugger called Adil (Automated Debugger in Learning system) will be able locate, pinpoint and explain logical errors of programs. If there are no errors, it will be able to explain the meaning of the program. Adil is based on the design of the Conceiver, an automated program understanding system developed at Universiti Kebangsaan Malaysia.",
        "published": "2001-01-12T08:58:58Z",
        "link": "http://arxiv.org/abs/cs/0101008v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.5"
        ]
    },
    {
        "title": "Generation of and Debugging with Logical Pre and Postconditions",
        "authors": [
            "Angel Herrranz-Nieva Juan Jose Moreno Navarro"
        ],
        "summary": "This paper shows the debugging facilities provided by the SLAM system. The SLAM system includes i) a specification language that integrates algebraic specifications and model-based specifications using the object oriented model. Class operations are defined by using rules each of them with logical pre and postconditions but with a functional flavour. ii) A development environment that, among other features, is able to generate readable code in a high level object oriented language. iii) The generated code includes (part of) the pre and postconditions as assertions, that can be automatically checked in the debug mode execution of programs. We focus on this last aspect.   The SLAM language is expressive enough to describe many useful properties and these properties are translated into a Prolog program that is linked (via an adequate interface) with the user program. The debugging execution of the program interacts with the Prolog engine which is responsible for checking properties.",
        "published": "2001-01-12T15:32:42Z",
        "link": "http://arxiv.org/abs/cs/0101009v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.5, F.3.1"
        ]
    },
    {
        "title": "Reverse Engineering from Assembler to Formal Specifications via Program   Transformations",
        "authors": [
            "M. P. Ward"
        ],
        "summary": "The FermaT transformation system, based on research carried out over the last sixteen years at Durham University, De Montfort University and Software Migrations Ltd., is an industrial-strength formal transformation engine with many applications in program comprehension and language migration. This paper is a case study which uses automated plus manually-directed transformations and abstractions to convert an IBM 370 Assembler code program into a very high-level abstract specification.",
        "published": "2001-05-04T09:21:21Z",
        "link": "http://arxiv.org/abs/cs/0105006v1",
        "categories": [
            "cs.SE",
            "cs.PL",
            "D.2.7;D.3.2"
        ]
    },
    {
        "title": "Applying Slicing Technique to Software Architectures",
        "authors": [
            "Jianjun Zhao"
        ],
        "summary": "Software architecture is receiving increasingly attention as a critical design level for software systems. As software architecture design resources (in the form of architectural specifications) are going to be accumulated, the development of techniques and tools to support architectural understanding, testing, reengineering, maintenance, and reuse will become an important issue. This paper introduces a new form of slicing, named architectural slicing, to aid architectural understanding and reuse. In contrast to traditional slicing, architectural slicing is designed to operate on the architectural specification of a software system, rather than the source code of a program. Architectural slicing provides knowledge about the high-level structure of a software system, rather than the low-level implementation details of a program. In order to compute an architectural slice, we present the architecture information flow graph which can be used to represent information flows in a software architecture. Based on the graph, we give a two-phase algorithm to compute an architectural slice.",
        "published": "2001-05-05T08:09:08Z",
        "link": "http://arxiv.org/abs/cs/0105008v1",
        "categories": [
            "cs.SE",
            "D.2.4; D.2.5; D.2.7; D.2.11"
        ]
    },
    {
        "title": "Using Dependence Analysis to Support Software Architecture Understanding",
        "authors": [
            "Jianjun Zhao"
        ],
        "summary": "Software architecture is receiving increasingly attention as a critical design level for software systems. As software architecture design resources (in the form of architectural descriptions) are going to be accumulated, the development of techniques and tools to support architectural understanding, testing, reengineering, maintaining, and reusing will become an important issue. In this paper we introduce a new dependence analysis technique, named architectural dependence analysis to support software architecture development. In contrast to traditional dependence analysis, architectural dependence analysis is designed to operate on an architectural description of a software system, rather than the source code of a conventional program. Architectural dependence analysis provides knowledge of dependences for the high-level architecture of a software system, rather than the low-level implementation details of a conventional program.",
        "published": "2001-05-05T08:41:43Z",
        "link": "http://arxiv.org/abs/cs/0105009v1",
        "categories": [
            "cs.SE",
            "D.2.4; D.2.5; D.2.7; D.2.11"
        ]
    },
    {
        "title": "On Assessing the Complexity of Software Architectures",
        "authors": [
            "Jianjun Zhao"
        ],
        "summary": "This paper proposes some new architectural metrics which are appropriate for evaluating the architectural attributes of a software system. The main feature of our approach is to assess the complexity of a software architecture by analyzing various types of architectural dependences in the architecture.",
        "published": "2001-05-05T09:18:11Z",
        "link": "http://arxiv.org/abs/cs/0105010v1",
        "categories": [
            "cs.SE",
            "D.2.8; D.2.11"
        ]
    },
    {
        "title": "HTTP Cookies: Standards, Privacy, and Politics",
        "authors": [
            "David M. Kristol"
        ],
        "summary": "How did we get from a world where cookies were something you ate and where \"non-techies\" were unaware of \"Netscape cookies\" to a world where cookies are a hot-button privacy issue for many computer users? This paper will describe how HTTP \"cookies\" work, and how Netscape's original specification evolved into an IETF Proposed Standard. I will also offer a personal perspective on how what began as a straightforward technical specification turned into a political flashpoint when it tried to address non-technical issues such as privacy.",
        "published": "2001-05-09T16:12:44Z",
        "link": "http://arxiv.org/abs/cs/0105018v1",
        "categories": [
            "cs.SE",
            "cs.CY",
            "C.2.2; K.2"
        ]
    },
    {
        "title": "Model Checking Contractual Protocols",
        "authors": [
            "Aspassia Daskalopulu"
        ],
        "summary": "This paper discusses how model checking, a technique used for the verification of behavioural requirements of dynamic systems, can be usefully deployed for the verification of contracts. A process view of agreements between parties is taken, whereby a contract is modelled as it evolves over time in terms of actions or more generally events that effect changes in its state. Modelling is done with Petri Nets in the spirit of other research work on the representation of trade procedures. The paper illustrates all the phases of the verification technique through an example and argues that the approach is useful particularly in the context of pre-contractual negotiation and contract drafting. The work reported here is part of a broader project on the development of logic-based tools for the analysis and representation of legal contracts.",
        "published": "2001-06-07T14:53:00Z",
        "link": "http://arxiv.org/abs/cs/0106009v1",
        "categories": [
            "cs.SE",
            "cs.LO",
            "D.2.4;I.2.4"
        ]
    },
    {
        "title": "L.T.Kuzin: Research Program",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "Lev T. Kuzin (1928--1997) is one of the founders of modern cybernetics and information science in Russia. He was awarded and honored the USSR State Prize for inspiring vision into the future of technical cybernetics and his invention and innovation of key technologies.   The last years he interested in the computational models of geometrical and algebraic nature and their applications in various branches of computer science and information technologies. In the recent years the interest in computation models based on object notion has grown tremendously stimulating an interest to Kuzin's ideas. This year of 50th Anniversary of Cybernetics and on the occasion of his 70th birthday on September 12, 1998 seems especially appropriate for discussing Kuzin's Research Program.",
        "published": "2001-06-08T17:42:12Z",
        "link": "http://arxiv.org/abs/cs/0106014v1",
        "categories": [
            "cs.DM",
            "cs.AI",
            "cs.SE",
            "A.0; F.0; I.2"
        ]
    },
    {
        "title": "Event Driven Objects",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "A formal consideration in this paper is given for the essential notations to characterize the object that is distinguished in a problem domain. The distinct object is represented by another idealized object, which is a schematic element. When the existence of an element is significant, then a class of these partial elements is dropped down into actual, potential and virtual objects. The potential objects are gathered into the variable domains which are the extended ranges for unbound variables. The families of actual objects are shown to be parameterized with the types and events. The transitions between events are shown to be driven by the scripts. A computational framework arises which is described by the commutative diagrams.",
        "published": "2001-06-12T10:48:30Z",
        "link": "http://arxiv.org/abs/cs/0106027v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.SE",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Logic, Individuals and Concepts",
        "authors": [
            "Viacheslav Wolfengagen"
        ],
        "summary": "This extended abstract gives a brief outline of the connections between the descriptions and variable concepts. Thus, the notion of a concept is extended to include both the syntax and semantics features. The evaluation map in use is parameterized by a kind of computational environment, the index, giving rise to indexed concepts. The concepts are inhabited into language by the descriptions from the higher order logic. In general the idea of object-as-functor should assist the designer to outline a programming tool in conceptual shell style.",
        "published": "2001-06-12T12:58:27Z",
        "link": "http://arxiv.org/abs/cs/0106030v1",
        "categories": [
            "cs.LO",
            "cs.DB",
            "cs.DM",
            "cs.SE",
            "D.3.1; F.1; F.4.1; D.1.1; H.2.1"
        ]
    },
    {
        "title": "Tracing Execution of Software for Design Coverage",
        "authors": [
            "Raimondas Lencevicius",
            "Edu Metz",
            "Alexander Ran"
        ],
        "summary": "Test suites are designed to validate the operation of a system against requirements. One important aspect of a test suite design is to ensure that system operation logic is tested completely. A test suite should drive a system through all abstract states to exercise all possible cases of its operation. This is a difficult task. Code coverage tools support test suite designers by providing the information about which parts of source code are covered during system execution. Unfortunately, code coverage tools produce only source code coverage information. For a test engineer it is often hard to understand what the noncovered parts of the source code do and how they relate to requirements. We propose a generic approach that provides design coverage of the executed software simplifying the development of new test suites. We demonstrate our approach on common design abstractions such as statecharts, activity diagrams, message sequence charts and structure diagrams. We implement the design coverage using Third Eye tracing and trace analysis framework. Using design coverage, test suites could be created faster by focussing on untested design elements.",
        "published": "2001-09-14T15:12:51Z",
        "link": "http://arxiv.org/abs/cs/0109019v1",
        "categories": [
            "cs.SE",
            "D.2.5"
        ]
    },
    {
        "title": "Automated Real-Time Testing (ARTT) for Embedded Control Systems (ECS)",
        "authors": [
            "Jon Hawkins",
            "Haung V. Nguyen",
            "Reginald B. Howard"
        ],
        "summary": "Developing real-time automated test systems for embedded control systems has been a real problem. Some engineers and scientists have used customized software and hardware as a solution, which can be very expensive and time consuming to develop. We have discovered how to integrate a suite of commercially available off-the-shelf software tools and hardware to develop a scalable test platform that is capable of performing complete black-box testing for a dual-channel real-time Embedded-PLC-based control system (www.aps.anl.gov). We will discuss how the Vali/Test Pro testing methodology was implemented to structure testing for a personnel safety system with large quantities of requirements and test cases.   This work was supported by the U.S. Department of Energy, Basic Energy Sciences, under Contract No. W-31-109-Eng-38.",
        "published": "2001-11-02T15:44:06Z",
        "link": "http://arxiv.org/abs/cs/0111005v3",
        "categories": [
            "cs.OH",
            "cs.SE",
            "C.3"
        ]
    },
    {
        "title": "Quality Control, Testing and Deployment Results in NIF ICCS",
        "authors": [
            "John P. Woodruff",
            "Drew D. Casavant",
            "Barry D. Cline",
            "Michael R. Gorvad"
        ],
        "summary": "The strategy used to develop the NIF Integrated Computer Control System (ICCS) calls for incremental cycles of construction and formal test to deliver a total of 1 million lines of code. Each incremental release takes four to six months to implement specific functionality and culminates when offline tests conducted in the ICCS Integration and Test Facility verify functional, performance, and interface requirements. Tests are then repeated on line to confirm integrated operation in dedicated laser laboratories or ultimately in the NIF. Test incidents along with other change requests are recorded and tracked to closure by the software change control board (SCCB). Annual independent audits advise management on software process improvements. Extensive experience has been gained by integrating controls in the prototype laser preamplifier laboratory. The control system installed in the preamplifier lab contains five of the ten planned supervisory subsystems and seven of sixteen planned front-end processors (FEPs). Beam alignment, timing, diagnosis and laser pulse amplification up to 20 joules was tested through an automated series of shots. Other laboratories have provided integrated testing of six additional FEPs. Process measurements including earned-value, product size, and defect densities provide software project controls and generate confidence that the control system will be successfully deployed.",
        "published": "2001-11-06T22:12:14Z",
        "link": "http://arxiv.org/abs/cs/0111013v1",
        "categories": [
            "cs.SE",
            "D.2.9"
        ]
    },
    {
        "title": "Visual DCT - Visual EPICS Database Configuration Tool",
        "authors": [
            "M. Sekoranja",
            "S. Hunt",
            "A. Luedeke"
        ],
        "summary": "Visual DCT is an EPICS configuration tool completely written in Java and therefore supported in various systems. It was developed to provide features missing in existing configuration tools as Capfast and GDCT. Visually Visual DCT resembles GDCT - records can be created, moved and linked, fields and links can be easily modified. But Visual DCT offers more: using groups, records can be grouped together in a logical block, which allows a hierarchical design. Additionally indication of data flow direction using arrows makes the design easier to understand. Visual DCT has a powerful DB parser, which allows importing existing DB and DBD files. Output file is also DB file, all comments and record order is preserved and visual data saved as comment, which allows DBs to be edited in other tools or manually. Great effort has been taken and many tricks used to optimize the performance in order to compensate for the fact that Java is an interpreted language.",
        "published": "2001-11-07T10:01:57Z",
        "link": "http://arxiv.org/abs/cs/0111014v1",
        "categories": [
            "cs.SE",
            "K.8.1"
        ]
    },
    {
        "title": "Application Software Structure Enables Nif Operations Kirby W. Fong",
        "authors": [
            "Kirby W. Fong",
            "Christopher M. Estes",
            "John M. Fisher",
            "Randy T. Shelton"
        ],
        "summary": "The NIF Integrated Computer Control System (ICCS) application software uses a set of service frameworks that assures uniform behavior spanning the front-end processors (FEPs) and supervisor programs. This uniformity is visible both in the way each program employs shared services and in the flexibility it affords for attaching graphical user interfaces (GUIs). Uniformity of structure across applications is desired for the benefit of programmers who will be maintaining the many programs that constitute the ICCS. In this paper, the framework components that have the greatest impact on the application structure are discussed.",
        "published": "2001-11-07T21:50:43Z",
        "link": "http://arxiv.org/abs/cs/0111016v1",
        "categories": [
            "cs.SE",
            "D.2.11"
        ]
    },
    {
        "title": "Application of digital regulated Power Supplies for Magnet Control at   the Swiss Light Source",
        "authors": [
            "A. Luedeke"
        ],
        "summary": "The Swiss Light Source (SLS) has in the order of 500 magnet power supplies (PS) installed, ranging from from 3 A/20 V four-quadrant PS to a 950 A/1000 V two-quadrant 3 Hz PS. All magnet PS have a local digital controller for a digital regulation loop and a 5 MHz optical point-to-point link to the VME level. The PS controller is running a pulse width/pulse repetition regulation scheme, optional with multiple slave regulation loops. Many internal regulation parameters and controller diagnostics are readable by the control system. Industry Pack modules with standard VME carrier cards are used as VME hardware interface with the high control density of eight links per VME card. The low level EPICS interface is identical for all 500 magnet PS, including insertion devices. The digital PS have proven to be very stable and reliable during commissioning of the light source. All specifications were met for all PS. The advanced diagnostic for the magnet PS turned out to be very useful not only for the diagnostic of the PS but also to identify problems on the magnets.",
        "published": "2001-11-08T19:00:26Z",
        "link": "http://arxiv.org/abs/cs/0111019v2",
        "categories": [
            "cs.SE",
            "B.4.3;B.4.5;B.8;C.0;C.4"
        ]
    },
    {
        "title": "System Integration of High Level Applications during the Commissioning   of the Swiss Light Source",
        "authors": [
            "A. Luedeke"
        ],
        "summary": "The commissioning of the Swiss Light Source (SLS) started in Feb. 2000 with the Linac, continued in May 2000 with the booster synchrotron and by Dec. 2000 first light in the storage ring were produced. The first four beam lines had to be operational by August 2001. The thorough integration of all subsystems to the control system and a high level of automation was prerequisite to meet the tight time schedule. A careful balanced distribution of functionality into high level and low level applications allowed an optimization of short development cycles and high reliability of the applications. High level applications were implemented as CORBA based client/server applications (tcl/tk and Java based clients, C++ based servers), IDL applications using EZCA, medm/dm2k screens and tcl/tk applications using CDEV. Low level applications were mainly built as EPICS process databases, SNL state machines and customized drivers. Functionality of the high level application was encapsulated and pushed to lower levels whenever it has proven to be adequate. That enabled to reduce machine setups to a handful of physical parameters and allow the usage of standard EPICS tools for display, archiving and processing of complex physical values. High reliability and reproducibility were achieved with that approach.",
        "published": "2001-11-09T14:35:51Z",
        "link": "http://arxiv.org/abs/cs/0111021v3",
        "categories": [
            "cs.SE",
            "J.2"
        ]
    },
    {
        "title": "Data Access - Experiences Implementing an Object Oriented Library on   Various Platforms",
        "authors": [
            "R. Lange",
            "J. Hill"
        ],
        "summary": "Data Access will be the next generation data abstraction layer for EPICS. Its implementation in C++ brought up a number of issues that are related to object oriented technology's impact on CPU and memory usage.   What is gained by the new abstract interface? What is the price that has to be paid for these gains? What compromises seem applicable and affordable?   This paper discusses tests that have been made about performance and memory usage as well as the different measures that have been taken to optimize the situation.",
        "published": "2001-11-12T16:11:39Z",
        "link": "http://arxiv.org/abs/cs/0111036v1",
        "categories": [
            "cs.SE",
            "cs.DS",
            "D.1.5; D.2.13"
        ]
    },
    {
        "title": "User-friendly explanations for constraint programming",
        "authors": [
            "Narendra Jussien",
            "Samir Ouis"
        ],
        "summary": "In this paper, we introduce a set of tools for providing user-friendly explanations in an explanation-based constraint programming system. The idea is to represent the constraints of a problem as an hierarchy (a tree). Users are then represented as a set of understandable nodes in that tree (a cut). Classical explanations (sets of system constraints) just need to get projected on that representation in order to be understandable by any user. We present here the main interests of this idea.",
        "published": "2001-11-14T08:43:50Z",
        "link": "http://arxiv.org/abs/cs/0111037v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.2.6;D.3.3; F.4.1;D.2.5"
        ]
    },
    {
        "title": "An Integrated Development Environment for Declarative Multi-Paradigm   Programming",
        "authors": [
            "Michael Hanus",
            "Johannes Koj"
        ],
        "summary": "In this paper we present CIDER (Curry Integrated Development EnviRonment), an analysis and programming environment for the declarative multi-paradigm language Curry. CIDER is a graphical environment to support the development of Curry programs by providing integrated tools for the analysis and visualization of programs. CIDER is completely implemented in Curry using libraries for GUI programming (based on Tcl/Tk) and meta-programming. An important aspect of our environment is the possible adaptation of the development environment to other declarative source languages (e.g., Prolog or Haskell) and the extensibility w.r.t. new analysis methods. To support the latter feature, the lazy evaluation strategy of the underlying implementation language Curry becomes quite useful.",
        "published": "2001-11-14T12:50:17Z",
        "link": "http://arxiv.org/abs/cs/0111039v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.1; D.1.3; D.1.6; D.2.6; D.2.5; D.3.4"
        ]
    },
    {
        "title": "On the Design of a Tool for Supporting the Construction of Logic   Programs",
        "authors": [
            "Gustavo A. Ospina",
            "Baudouin Le Charlier"
        ],
        "summary": "Environments for systematic construction of logic programs are needed in the academy as well as in the industry. Such environments should support well defined construction methods and should be able to be extended and interact with other programming tools like debuggers and compilers. We present a variant of the Deville methodology for logic program development, and the design of a tool for supporting the methodology. Our aim is to facilitate the learning of logic programming and to set the basis of more sophisticated tools for program development.",
        "published": "2001-11-15T14:49:15Z",
        "link": "http://arxiv.org/abs/cs/0111041v3",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6;D.2.6"
        ]
    },
    {
        "title": "Combining Propagation Information and Search Tree Visualization using   ILOG OPL Studio",
        "authors": [
            "Christiane Bracchi",
            "Christophe Gefflot",
            "Frederic Paulin"
        ],
        "summary": "In this paper we give an overview of the current state of the graphical features provided by ILOG OPL Studio for debugging and performance tuning of OPL programs or external ILOG Solver based applications. This paper focuses on combining propagation and search information using the Search Tree view and the Propagation Spy. A new synthetic view is presented: the Christmas Tree, which combines the Search Tree view with statistics on the efficiency of the domain reduction and on the number of the propagation events triggered.",
        "published": "2001-11-15T15:03:23Z",
        "link": "http://arxiv.org/abs/cs/0111040v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.6; D.2.5; F.4.1"
        ]
    },
    {
        "title": "Proceedings of the Eleventh Workshop on Logic Programming Environments   (WLPE'01)",
        "authors": [
            "Anthony Kusalik"
        ],
        "summary": "The Eleventh Workshop on Logic Programming Environments (WLPE'01) was one in a series of international workshops in the topic area. It was held on December 1, 2001 in Paphos, Cyprus as a post-conference workshop at ICLP 2001. Eight refereed papers were presented at the conference. A majority of the papers involved, in some way, constraint logic programming and tools for software development. Other topics areas addressed include execution visualization, instructional aids (for learning users), software maintenance (including debugging), and provisions for new paradigms.",
        "published": "2001-11-16T14:40:57Z",
        "link": "http://arxiv.org/abs/cs/0111042v2",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.5; D.2.6; F.4.1; I.2.3"
        ]
    },
    {
        "title": "Prototyping CLP(FD) Tracers: a Trace Model and an Experimental   Validation Environment",
        "authors": [
            "Ludovic Langevine",
            "Pierre Deransart",
            "Mireille Ducasse",
            "Erwan Jahier"
        ],
        "summary": "Developing and maintaining CLP programs requires visualization and explanation tools. However, existing tools are built in an ad hoc way. Therefore porting tools from one platform to another is very difficult. We have shown in previous work that, from a fine-grained execution trace, a number of interesting views about logic program executions could be generated by trace analysis.   In this article, we propose a trace model for constraint solving by narrowing. This trace model is the first one proposed for CLP(FD) and does not pretend to be the ultimate one. We also propose an instrumented meta-interpreter in order to experiment with the model. Furthermore, we show that the proposed trace model contains the necessary information to build known and useful execution views. This work sets the basis for generic execution analysis of CLP(FD) programs.",
        "published": "2001-11-16T17:02:13Z",
        "link": "http://arxiv.org/abs/cs/0111043v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.6; D.2.5; F.4.1"
        ]
    },
    {
        "title": "The Overview of the National Ignition Facility Distributed Computer   Control System",
        "authors": [
            "L. J. Lagin",
            "R. C. Bettenhausen",
            "R. A. Carey",
            "C. M. Estes",
            "J. M. Fisher",
            "J. E. Krammen",
            "R. K. Reed",
            "P. J. VanArsdall",
            "J. P. Woodruff"
        ],
        "summary": "The Integrated Computer Control System (ICCS) for the National Ignition Facility (NIF) is a layered architecture of 300 front-end processors (FEP) coordinated by supervisor subsystems including automatic beam alignment and wavefront control, laser and target diagnostics, pulse power, and shot control timed to 30 ps. FEP computers incorporate either VxWorks on PowerPC or Solaris on UltraSPARC processors that interface to over 45,000 control points attached to VME-bus or PCI-bus crates respectively. Typical devices are stepping motors, transient digitizers, calorimeters, and photodiodes. The front-end layer is divided into another segment comprised of an additional 14,000 control points for industrial controls including vacuum, argon, synthetic air, and safety interlocks implemented with Allen-Bradley programmable logic controllers (PLCs). The computer network is augmented asynchronous transfer mode (ATM) that delivers video streams from 500 sensor cameras monitoring the 192 laser beams to operator workstations. Software is based on an object-oriented framework using CORBA distribution that incorporates services for archiving, machine configuration, graphical user interface, monitoring, event logging, scripting, alert management, and access control. Software coding using a mixed language environment of Ada95 and Java is one-third complete at over 300 thousand source lines. Control system installation is currently under way for the first 8 beams, with project completion scheduled for 2008.",
        "published": "2001-11-16T22:04:52Z",
        "link": "http://arxiv.org/abs/cs/0111045v1",
        "categories": [
            "cs.SE",
            "J.2"
        ]
    },
    {
        "title": "HyperPro An integrated documentation environment for CLP",
        "authors": [
            "AbdelAli Ed-Dbali",
            "Pierre Deransart",
            "Mariza A. S. Bigonha",
            "Jose de Siqueira",
            "Roberto da S. Bigonha"
        ],
        "summary": "The purpose of this paper is to present some functionalities of the HyperPro System. HyperPro is a hypertext tool which allows to develop Constraint Logic Programming (CLP) together with their documentation. The text editing part is not new and is based on the free software Thot. A HyperPro program is a Thot document written in a report style. The tool is designed for CLP but it can be adapted to other programming paradigms as well. Thot offers navigation and editing facilities and synchronized static document views. HyperPro has new functionalities such as document exportations, dynamic views (projections), indexes and version management. Projection is a mechanism for extracting and exporting relevant pieces of code program or of document according to specific criteria. Indexes are useful to find the references and occurrences of a relation in a document, i.e., where its predicate definition is found and where a relation is used in other programs or document versions and, to translate hyper-texts links into paper references. It still lack importation facilities.",
        "published": "2001-11-19T16:50:49Z",
        "link": "http://arxiv.org/abs/cs/0111046v1",
        "categories": [
            "cs.PL",
            "cs.SE",
            "D.1.6; D.2.6 (possibly also D.2.5; F.4.1; I.2.3)"
        ]
    },
    {
        "title": "Combinatorial Toolbox for Protein Sequence Design and Landscape Analysis   in the Grand Canonical Model",
        "authors": [
            "James Aspnes",
            "Julia Hartling",
            "Ming-Yang Kao",
            "Junhyong Kim",
            "Gauri Shah"
        ],
        "summary": "In modern biology, one of the most important research problems is to understand how protein sequences fold into their native 3D structures. To investigate this problem at a high level, one wishes to analyze the protein landscapes, i.e., the structures of the space of all protein sequences and their native 3D structures. Perhaps the most basic computational problem at this level is to take a target 3D structure as input and design a fittest protein sequence with respect to one or more fitness functions of the target 3D structure. We develop a toolbox of combinatorial techniques for protein landscape analysis in the Grand Canonical model of Sun, Brem, Chan, and Dill. The toolbox is based on linear programming, network flow, and a linear-size representation of all minimum cuts of a network. It not only substantially expands the network flow technique for protein sequence design in Kleinberg's seminal work but also is applicable to a considerably broader collection of computational problems than those considered by Kleinberg. We have used this toolbox to obtain a number of efficient algorithms and hardness results. We have further used the algorithms to analyze 3D structures drawn from the Protein Data Bank and have discovered some novel relationships between such native 3D structures and the Grand Canonical model.",
        "published": "2001-01-17T23:52:00Z",
        "link": "http://arxiv.org/abs/cs/0101015v1",
        "categories": [
            "cs.CE",
            "cs.CC",
            "q-bio.BM",
            "F.2; J.3"
        ]
    },
    {
        "title": "A Dynamic Programming Approach to De Novo Peptide Sequencing via Tandem   Mass Spectrometry",
        "authors": [
            "Ting Chen",
            "Ming-Yang Kao",
            "Matthew Tepel",
            "John Rush",
            "George M. Church"
        ],
        "summary": "The tandem mass spectrometry fragments a large number of molecules of the same peptide sequence into charged prefix and suffix subsequences, and then measures mass/charge ratios of these ions. The de novo peptide sequencing problem is to reconstruct the peptide sequence from a given tandem mass spectral data of k ions. By implicitly transforming the spectral data into an NC-spectrum graph G=(V,E) where |V|=2k+2, we can solve this problem in O(|V|+|E|) time and O(|V|) space using dynamic programming. Our approach can be further used to discover a modified amino acid in O(|V||E|) time and to analyze data with other types of noise in O(|V||E|) time. Our algorithms have been implemented and tested on actual experimental data.",
        "published": "2001-01-18T03:10:58Z",
        "link": "http://arxiv.org/abs/cs/0101016v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2; J.3"
        ]
    },
    {
        "title": "Tree Contractions and Evolutionary Trees",
        "authors": [
            "Ming-Yang Kao"
        ],
        "summary": "An evolutionary tree is a rooted tree where each internal vertex has at least two children and where the leaves are labeled with distinct symbols representing species. Evolutionary trees are useful for modeling the evolutionary history of species. An agreement subtree of two evolutionary trees is an evolutionary tree which is also a topological subtree of the two given trees. We give an algorithm to determine the largest possible number of leaves in any agreement subtree of two trees T_1 and T_2 with n leaves each. If the maximum degree d of these trees is bounded by a constant, the time complexity is O(n log^2(n)) and is within a log(n) factor of optimal. For general d, this algorithm runs in O(n d^2 log(d) log^2(n)) time or alternatively in O(n d sqrt(d) log^3(n)) time.",
        "published": "2001-01-26T21:36:30Z",
        "link": "http://arxiv.org/abs/cs/0101030v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; J.3"
        ]
    },
    {
        "title": "Cavity Matchings, Label Compressions, and Unrooted Evolutionary Trees",
        "authors": [
            "Ming-Yang Kao",
            "Tak-Wah Lam",
            "Wing-Kin Sung",
            "Hing-Fung Ting"
        ],
        "summary": "We present an algorithm for computing a maximum agreement subtree of two unrooted evolutionary trees. It takes O(n^{1.5} log n) time for trees with unbounded degrees, matching the best known time complexity for the rooted case. Our algorithm allows the input trees to be mixed trees, i.e., trees that may contain directed and undirected edges at the same time. Our algorithm adopts a recursive strategy exploiting a technique called label compression. The backbone of this technique is an algorithm that computes the maximum weight matchings over many subgraphs of a bipartite graph as fast as it takes to compute a single matching.",
        "published": "2001-01-26T23:59:55Z",
        "link": "http://arxiv.org/abs/cs/0101031v2",
        "categories": [
            "cs.CE",
            "cs.DS",
            "F.2.2; J.3"
        ]
    },
    {
        "title": "Fast Pricing of European Asian Options with Provable Accuracy:   Single-stock and Basket Options",
        "authors": [
            "Karhan Akcoglu",
            "Ming-Yang Kao",
            "Shuba Raghavan"
        ],
        "summary": "This paper develops three polynomial-time pricing techniques for European Asian options with provably small errors, where the stock prices follow binomial trees or trees of higher-degree. The first technique is the first known Monte Carlo algorithm with analytical error bounds suitable for pricing single-stock options with meaningful confidence and speed. The second technique is a general recursive bucketing-based scheme that can use the Aingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation and possibly others as the base-case subroutine. This scheme enables robust trade-offs between accuracy and time over subtrees of different sizes. For long-term options or high frequency price averaging, it can price single-stock options with smaller errors in less time than the base-case algorithms themselves. The third technique combines Fast Fourier Transform with bucketing-based schemes for pricing basket options. This technique takes polynomial time in the number of days and the number of stocks, and does not add any errors to those already incurred in the companion bucketing scheme. This technique assumes that the price of each underlying stock moves independently.",
        "published": "2001-02-02T20:52:36Z",
        "link": "http://arxiv.org/abs/cs/0102003v1",
        "categories": [
            "cs.CE",
            "G.2.2; G.2.3; G.3"
        ]
    },
    {
        "title": "Optimal Bid Sequences for Multiple-Object Auctions with Unequal Budgets",
        "authors": [
            "Yuyu Chen",
            "Ming-Yang Kao",
            "Hsueh-I Lu"
        ],
        "summary": "In a multiple-object auction, every bidder tries to win as many objects as possible with a bidding algorithm. This paper studies position-randomized auctions, which form a special class of multiple-object auctions where a bidding algorithm consists of an initial bid sequence and an algorithm for randomly permuting the sequence. We are especially concerned with situations where some bidders know the bidding algorithms of others. For the case of only two bidders, we give an optimal bidding algorithm for the disadvantaged bidder. Our result generalizes previous work by allowing the bidders to have unequal budgets. One might naturally anticipate that the optimal expected numbers of objects won by the bidders would be proportional to their budgets. Surprisingly, this is not true. Our new algorithm runs in optimal O(n) time in a straightforward manner. The case with more than two bidders is open.",
        "published": "2001-02-10T03:58:04Z",
        "link": "http://arxiv.org/abs/cs/0102008v1",
        "categories": [
            "cs.CE",
            "cs.DM",
            "cs.DS",
            "F.2.2; G.2.1; J.4"
        ]
    },
    {
        "title": "The Enhanced Double Digest Problem for DNA Physical Mapping",
        "authors": [
            "Ming-Yang Kao",
            "Jared Samet",
            "Wing-Kin Sung"
        ],
        "summary": "The double digest problem is a common NP-hard approach to constructing physical maps of DNA sequences. This paper presents a new approach called the enhanced double digest problem. Although this new problem is also NP-hard, it can be solved in linear time in certain theoretically interesting cases.",
        "published": "2001-02-10T14:13:07Z",
        "link": "http://arxiv.org/abs/cs/0102010v1",
        "categories": [
            "cs.CE",
            "cs.DM",
            "cs.DS",
            "F.2.2; G.2.3; J.3"
        ]
    },
    {
        "title": "Shooting Over or Under the Mark: Towards a Reliable and Flexible   Anticipation in the Economy",
        "authors": [
            "Koichiro Matsuno"
        ],
        "summary": "The real monetary economy is grounded upon monetary flow equilibration or the activity of actualizing monetary flow continuity at each economic agent except for the central bank. Every update of monetary flow continuity at each agent constantly causes monetary flow equilibration at the neighborhood agents. Every monetary flow equilibration as the activity of shooting the mark identified as monetary flow continuity turns out to be off the mark, and constantly generate the similar activities in sequence. Monetary flow equilibration ceaselessly reverberating in the economy performs two functions. One is to seek an organization on its own, and the other is to perturb the ongoing organization. Monetary flow equilibration as the agency of seeking and perturbing its organization also serves as a means of predicting its behavior. The likely organizational behavior could be the one that remains most robust against monetary flow equilibration as an agency of applying perturbations.",
        "published": "2001-04-09T06:43:12Z",
        "link": "http://arxiv.org/abs/cs/0104013v1",
        "categories": [
            "cs.CE",
            "J.1"
        ]
    },
    {
        "title": "Tracing a Faint Fingerprint of the Invisible Hand?",
        "authors": [
            "Koichiro Matsuno"
        ],
        "summary": "Any economic agent constituting the monetary economy maintains the activity of monetary flow equilibration for fulfilling the condition of monetary flow continuity in the record, except at the central bank. At the same time, monetary flow equilibration at one economic agent constantly induces at other agents in the economy further flow disequilibrium to be eliminated subsequently. We propose the rate of monetary flow disequilibration as a figure measuring the progressive movement of the economy. The rate of disequilibration was read out of both the Japanese and the United States monetary economy recorded over the last fifty years.",
        "published": "2001-04-09T08:49:57Z",
        "link": "http://arxiv.org/abs/cs/0104014v1",
        "categories": [
            "cs.CE",
            "J.1"
        ]
    },
    {
        "title": "Local Search Techniques for Constrained Portfolio Selection Problems",
        "authors": [
            "Andrea Schaerf"
        ],
        "summary": "We consider the problem of selecting a portfolio of assets that provides the investor a suitable balance of expected return and risk. With respect to the seminal mean-variance model of Markowitz, we consider additional constraints on the cardinality of the portfolio and on the quantity of individual shares. Such constraints better capture the real-world trading system, but make the problem more difficult to be solved with exact methods. We explore the use of local search techniques, mainly tabu search, for the portfolio selection problem. We compare and combine previous work on portfolio selection that makes use of the local search approach and we propose new algorithms that combine different neighborhood relations. In addition, we show how the use of randomization and of a simple form of adaptiveness simplifies the setting of a large number of critical parameters. Finally, we show how our techniques perform on public benchmarks.",
        "published": "2001-04-18T13:42:49Z",
        "link": "http://arxiv.org/abs/cs/0104017v1",
        "categories": [
            "cs.CE",
            "cs.AI",
            "J.1; I.2.8"
        ]
    },
    {
        "title": "Several new domain-type and boundary-type numerical discretization   schemes with radial basis function",
        "authors": [
            "W. Chen"
        ],
        "summary": "This paper is concerned with a few novel RBF-based numerical schemes discretizing partial differential equations. For boundary-type methods, we derive the indirect and direct symmetric boundary knot methods (BKM). The resulting interpolation matrix of both is always symmetric irrespective of boundary geometry and conditions. In particular, the direct BKM applies the practical physical variables rather than expansion coefficients and becomes very competitive to the boundary element method. On the other hand, based on the multiple reciprocity principle, we invent the RBF-based boundary particle method (BPM) for general inhomogeneous problems without a need using inner nodes. The direct and symmetric BPM schemes are also developed.   For domain-type RBF discretization schemes, by using the Green integral we develop a new Hermite RBF scheme called as the modified Kansa method (MKM), which differs from the symmetric Hermite RBF scheme in that the MKM discretizes both governing equation and boundary conditions on the same boundary nodes. The local spline version of the MKM is named as the finite knot method (FKM). Both MKM and FKM significantly reduce calculation errors at nodes adjacent to boundary. In addition, the nonsingular high-order fundamental or general solution is strongly recommended as the RBF in the domain-type methods and dual reciprocity method approximation of particular solution relating to the BKM.   It is stressed that all the above discretization methods of boundary-type and domain-type are symmetric, meshless, and integration-free. The spline-based schemes will produce desirable symmetric sparse banded interpolation matrix. In appendix, we present a Hermite scheme to eliminate edge effect on the RBF geometric modeling and imaging.",
        "published": "2001-04-23T18:50:45Z",
        "link": "http://arxiv.org/abs/cs/0104018v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "G.1.3; G.1.8"
        ]
    },
    {
        "title": "Parallel implementation of the TRANSIMS micro-simulation",
        "authors": [
            "Kai Nagel",
            "Marcus Rickert"
        ],
        "summary": "This paper describes the parallel implementation of the TRANSIMS traffic micro-simulation. The parallelization method is domain decomposition, which means that each CPU of the parallel computer is responsible for a different geographical area of the simulated region. We describe how information between domains is exchanged, and how the transportation network graph is partitioned. An adaptive scheme is used to optimize load balancing. We then demonstrate how computing speeds of our parallel micro-simulations can be systematically predicted once the scenario and the computer architecture are known. This makes it possible, for example, to decide if a certain study is feasible with a certain computing budget, and how to invest that budget. The main ingredients of the prediction are knowledge about the parallel implementation of the micro-simulation, knowledge about the characteristics of the partitioning of the transportation network graph, and knowledge about the interaction of these quantities with the computer system. In particular, we investigate the differences between switched and non-switched topologies, and the effects of 10 Mbit, 100 Mbit, and Gbit Ethernet. keywords: Traffic simulation, parallel computing, transportation planning, TRANSIMS",
        "published": "2001-05-02T12:43:39Z",
        "link": "http://arxiv.org/abs/cs/0105004v1",
        "categories": [
            "cs.CE",
            "I.6;J.2;J.4"
        ]
    },
    {
        "title": "Errata and supplements to: Orthonormal RBF Wavelet and Ridgelet-like   Series and Transforms for High-Dimensional Problems",
        "authors": [
            "W. Chen"
        ],
        "summary": "In recent years some attempts have been done to relate the RBF with wavelets in handling high dimensional multiscale problems. To the author's knowledge, however, the orthonormal and bi-orthogonal RBF wavelets are still missing in the literature. By using the nonsingular general solution and singular fundamental solution of differential operator, recently the present author, refer. 3, made some substantial headway to derive the orthonormal RBF wavelets series and transforms. The methodology can be generalized to create the RBF wavelets by means of the orthogonal convolution kernel function of various integral operators. In particular, it is stressed that the presented RBF wavelets does not apply the tensor product to handle multivariate problems at all.   This note is to correct some errata in reference 3 and also to supply a few latest advances in the study of orthornormal RBF wavelet transforms.",
        "published": "2001-05-07T16:53:05Z",
        "link": "http://arxiv.org/abs/cs/0105014v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "G1.3; G1.8"
        ]
    },
    {
        "title": "Solving Composed First-Order Constraints from Discrete-Time Robust   Control",
        "authors": [
            "Stefan Ratschan",
            "Luc Jaulin"
        ],
        "summary": "This paper deals with a problem from discrete-time robust control which requires the solution of constraints over the reals that contain both universal and existential quantifiers. For solving this problem we formulate it as a program in a (fictitious) constraint logic programming language with explicit quantifier notation. This allows us to clarify the special structure of the problem, and to extend an algorithm for computing approximate solution sets of first-order constraints over the reals to exploit this structure. As a result we can deal with inputs that are clearly out of reach for current symbolic solvers.",
        "published": "2001-05-11T08:28:33Z",
        "link": "http://arxiv.org/abs/cs/0105021v1",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.CE",
            "F.4.1;I.2.8"
        ]
    },
    {
        "title": "A note on radial basis function computing",
        "authors": [
            "W. Chen",
            "W. He"
        ],
        "summary": "This note carries three purposes involving our latest advances on the radial basis function (RBF) approach. First, we will introduce a new scheme employing the boundary knot method (BKM) to nonlinear convection-diffusion problem. It is stressed that the new scheme directly results in a linear BKM formulation of nonlinear problems by using response point-dependent RBFs, which can be solved by any linear solver. Then we only need to solve a single nonlinear algebraic equation for desirable unknown at some inner node of interest. The numerical results demonstrate high accuracy and efficiency of this nonlinear BKM strategy. Second, we extend the concepts of distance function, which include time-space and variable transformation distance functions. Finally, we demonstrate that if the nodes are symmetrically placed, the RBF coefficient matrices have either centrosymmetric or skew centrosymmetric structures. The factorization features of such matrices lead to a considerable reduction in the RBF computing effort. A simple approach is also presented to reduce the ill-conditioning of RBF interpolation matrices in general cases.",
        "published": "2001-06-03T11:41:56Z",
        "link": "http://arxiv.org/abs/cs/0106003v1",
        "categories": [
            "cs.CE",
            "cs.CG",
            "G.1.3; G.1.8"
        ]
    },
    {
        "title": "Pricing Virtual Paths with Quality-of-Service Guarantees as Bundle   Derivatives",
        "authors": [
            "Lars Rasmusson"
        ],
        "summary": "We describe a model of a communication network that allows us to price complex network services as financial derivative contracts based on the spot price of the capacity in individual routers. We prove a theorem of a Girsanov transform that is useful for pricing linear derivatives on underlying assets, which can be used to price many complex network services, and it is used to price an option that gives access to one of several virtual channels between two network nodes, during a specified future time interval. We give the continuous time hedging strategy, for which the option price is independent of the service providers attitude towards risk. The option price contains the density function of a sum of lognormal variables, which has to be evaluated numerically.",
        "published": "2001-06-12T12:06:47Z",
        "link": "http://arxiv.org/abs/cs/0106028v1",
        "categories": [
            "cs.NI",
            "cs.CE",
            "C.2.3; C.4"
        ]
    },
    {
        "title": "The Risk Profile Problem for Stock Portfolio Optimization",
        "authors": [
            "Ming-Yang Kao",
            "Andreas Nolte",
            "Stephen R. Tate"
        ],
        "summary": "This work initiates research into the problem of determining an optimal investment strategy for investors with different attitudes towards the trade-offs of risk and profit. The probability distribution of the return values of the stocks that are considered by the investor are assumed to be known, while the joint distribution is unknown. The problem is to find the best investment strategy in order to minimize the probability of losing a certain percentage of the invested capital based on different attitudes of the investors towards future outcomes of the stock market.   For portfolios made up of two stocks, this work shows how to exactly and quickly solve the problem of finding an optimal portfolio for aggressive or risk-averse investors, using an algorithm based on a fast greedy solution to a maximum flow problem. However, an investor looking for an average-case guarantee (so is neither aggressive or risk-averse) must deal with a more difficult problem. In particular, it is #P-complete to compute the distribution function associated with the average-case bound. On the positive side, approximate answers can be computed by using random sampling techniques similar to those for high-dimensional volume estimation. When k>2 stocks are considered, it is proved that a simple solution based on the same flow concepts as the 2-stock algorithm would imply that P = NP, so is highly unlikely. This work gives approximation algorithms for this case as well as exact algorithms for some important special cases.",
        "published": "2001-07-03T15:18:48Z",
        "link": "http://arxiv.org/abs/cs/0107007v1",
        "categories": [
            "cs.CE",
            "cs.DM",
            "cs.DS",
            "E1; F2; G.1.6; G.1.10; G.2; G.3; J.4"
        ]
    },
    {
        "title": "The Expresso Microarray Experiment Management System: The Functional   Genomics of Stress Responses in Loblolly Pine",
        "authors": [
            "Lenwood S. Heath",
            "Naren Ramakrishnan",
            "Ronald R. Sederoff",
            "Ross W. Whetten",
            "Boris I. Chevone",
            "Craig A. Struble",
            "Vincent Y. Jouenne",
            "Dawei Chen",
            "Leonel van Zyl",
            "Ruth G. Alscher"
        ],
        "summary": "Conception, design, and implementation of cDNA microarray experiments present a variety of bioinformatics challenges for biologists and computational scientists. The multiple stages of data acquisition and analysis have motivated the design of Expresso, a system for microarray experiment management. Salient aspects of Expresso include support for clone replication and randomized placement; automatic gridding, extraction of expression data from each spot, and quality monitoring; flexible methods of combining data from individual spots into information about clones and functional categories; and the use of inductive logic programming for higher-level data analysis and mining. The development of Expresso is occurring in parallel with several generations of microarray experiments aimed at elucidating genomic responses to drought stress in loblolly pine seedlings. The current experimental design incorporates 384 pine cDNAs replicated and randomly placed in two specific microarray layouts. We describe the design of Expresso as well as results of analysis with Expresso that suggest the importance of molecular chaperones and membrane transport proteins in mechanisms conferring successful adaptation to long-term drought stress.",
        "published": "2001-10-23T14:34:08Z",
        "link": "http://arxiv.org/abs/cs/0110047v1",
        "categories": [
            "cs.OH",
            "cs.CE",
            "q-bio.GN",
            "J.3"
        ]
    },
    {
        "title": "Multivariant Branching Prediction, Reflection, and Retrospection",
        "authors": [
            "Mark Burgin",
            "Walter Karplus",
            "Damon Liu"
        ],
        "summary": "In branching simulation, a novel approach to simulation presented in this paper, a multiplicity of plausible scenarios are concurrently developed and implemented. In conventional simulations of complex systems, there arise from time to time uncertainties as to which of two or more alternatives are more likely to be pursued by the system being simulated. Under these conditions the simulationist makes a judicious choice of one of these alternatives and embeds this choice in the simulation model. By contrast, in the branching approach, two or more of such alternatives (or branches) are included in the model and implemented for concurrent computer solution. The theoretical foundations for branching simulation as a computational process are in the domains of alternating Turing machines, molecular computing, and E-machines. Branching simulations constitute the development of diagrams of scenarios representing significant, alternative flows of events. Logical means for interpretation and investigation of the branching simulation and prediction are provided by the logical theories of possible worlds, which have been formalized by the construction of logical varieties. Under certain conditions, the branching approach can considerably enhance the efficiency of computer simulations and provide more complete insights into the interpretation of predictions based on simulations. As an example, the concepts developed in this paper have been applied to a simulation task that plays an important role in radiology - the noninvasive treatment of brain aneurysms.",
        "published": "2001-10-23T22:04:24Z",
        "link": "http://arxiv.org/abs/cs/0110048v1",
        "categories": [
            "cs.CE",
            "cs.DC",
            "F.1.2; F.1.1"
        ]
    },
    {
        "title": "Analytical solution of transient scalar wave and diffusion problems of   arbitrary dimensionality and geometry by RBF wavelet series",
        "authors": [
            "W. Chen"
        ],
        "summary": "This study applies the RBF wavelet series to the evaluation of analytical solutions of linear time-dependent wave and diffusion problems of any dimensionality and geometry. To the best of the author's knowledge, such analytical solutions have never been achieved before. The RBF wavelets can be understood an alternative for multidimensional problems to the standard Fourier series via fundamental and general solutions of partial differential equation. The present RBF wavelets are infinitely differential, compactly supported, orthogonal over different scales and very simple. The rigorous mathematical proof of completeness and convergence is still missing in this study. The present work may open a new window to numerical solution and theoretical analysis of many other high-dimensional time-dependent PDE problems under arbitrary geometry.",
        "published": "2001-10-28T10:28:55Z",
        "link": "http://arxiv.org/abs/cs/0110055v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "G1.3; G1.8"
        ]
    },
    {
        "title": "Analysis of Investment Policy in Belarus",
        "authors": [
            "Fedor S. Kilin"
        ],
        "summary": "The optimal planning trajectory is analyzed on the basis of the growth model with effectiveness. The saving per capital value has to be rather high initially with smooth decrement in the future years.",
        "published": "2001-10-31T20:18:35Z",
        "link": "http://arxiv.org/abs/cs/0110067v1",
        "categories": [
            "cs.CE",
            "J.1;J.4"
        ]
    },
    {
        "title": "Predicting RNA Secondary Structures with Arbitrary Pseudoknots by   Maximizing the Number of Stacking Pairs",
        "authors": [
            "Samuel Ieong",
            "Ming-Yang Kao",
            "Tak-Wah Lam",
            "Wing-Kin Sung",
            "Siu-Ming Yiu"
        ],
        "summary": "The paper investigates the computational problem of predicting RNA secondary structures. The general belief is that allowing pseudoknots makes the problem hard. Existing polynomial-time algorithms are heuristic algorithms with no performance guarantee and can only handle limited types of pseudoknots. In this paper we initiate the study of predicting RNA secondary structures with a maximum number of stacking pairs while allowing arbitrary pseudoknots. We obtain two approximation algorithms with worst-case approximation ratios of 1/2 and 1/3 for planar and general secondary structures,respectively. For an RNA sequence of $n$ bases, the approximation algorithm for planar secondary structures runs in $O(n^3)$ time while that for the general case runs in linear time. Furthermore, we prove that allowing pseudoknots makes it NP-hard to maximize the number of stacking pairs in a planar secondary structure. This result is in contrast with the recent NP-hard results on psuedoknots which are based on optimizing some general and complicated energy functions.",
        "published": "2001-11-20T02:42:49Z",
        "link": "http://arxiv.org/abs/cs/0111051v1",
        "categories": [
            "cs.CE",
            "cs.DS",
            "q-bio",
            "F2.2; G2.3; J.3"
        ]
    },
    {
        "title": "The similarity metric",
        "authors": [
            "Ming Li",
            "Xin Chen",
            "Xin Li",
            "Bin Ma",
            "Paul Vitanyi"
        ],
        "summary": "A new class of distances appropriate for measuring similarity relations between sequences, say one type of similarity per distance, is studied. We propose a new ``normalized information distance'', based on the noncomputable notion of Kolmogorov complexity, and show that it is in this class and it minorizes every computable distance in the class (that is, it is universal in that it discovers all computable similarities). We demonstrate that it is a metric and call it the {\\em similarity metric}. This theory forms the foundation for a new practical tool. To evidence generality and robustness we give two distinctive applications in widely divergent areas using standard compression programs like gzip and GenCompress. First, we compare whole mitochondrial genomes and infer their evolutionary history. This results in a first completely automatic computed whole mitochondrial phylogeny tree. Secondly, we fully automatically compute the language tree of 52 different languages.",
        "published": "2001-11-20T15:25:47Z",
        "link": "http://arxiv.org/abs/cs/0111054v3",
        "categories": [
            "cs.CC",
            "cond-mat.stat-mech",
            "cs.CE",
            "cs.CV",
            "math.CO",
            "math.MG",
            "math.ST",
            "physics.data-an",
            "q-bio.GN",
            "stat.TH",
            "J.3, E.4"
        ]
    },
    {
        "title": "New RBF collocation methods and kernel RBF with applications",
        "authors": [
            "W. Chen"
        ],
        "summary": "A few novel radial basis function (RBF) discretization schemes for partial differential equations are developed in this study. For boundary-type methods, we derive the indirect and direct symmetric boundary knot methods. Based on the multiple reciprocity principle, the boundary particle method is introduced for general inhomogeneous problems without using inner nodes. For domain-type schemes, by using the Green integral we develop a novel Hermite RBF scheme called the modified Kansa method, which significantly reduces calculation errors at close-to-boundary nodes. To avoid Gibbs phenomenon, we present the least square RBF collocation scheme. Finally, five types of the kernel RBF are also briefly presented.",
        "published": "2001-11-30T16:03:00Z",
        "link": "http://arxiv.org/abs/cs/0111063v1",
        "categories": [
            "cs.NA",
            "cs.CE",
            "G1.3, G1.8"
        ]
    },
    {
        "title": "DNA Self-Assembly For Constructing 3D Boxes",
        "authors": [
            "Ming-Yang Kao",
            "Vijay Ramachandran"
        ],
        "summary": "We propose a mathematical model of DNA self-assembly using 2D tiles to form 3D nanostructures. This is the first work to combine studies in self-assembly and nanotechnology in 3D, just as Rothemund and Winfree did in the 2D case. Our model is a more precise superset of their Tile Assembly Model that facilitates building scalable 3D molecules. Under our model, we present algorithms to build a hollow cube, which is intuitively one of the simplest 3D structures to construct. We also introduce five basic measures of complexity to analyze these algorithms. Our model and algorithmic techniques are applicable to more complex 2D and 3D nanostructures.",
        "published": "2001-12-08T21:36:28Z",
        "link": "http://arxiv.org/abs/cs/0112009v1",
        "categories": [
            "cs.CC",
            "cs.CE",
            "F.2.2; F.1.1; J.3"
        ]
    },
    {
        "title": "Automatic Differentiation Tools in Optimization Software",
        "authors": [
            "Jorge J. Moré"
        ],
        "summary": "We discuss the role of automatic differentiation tools in optimization software. We emphasize issues that are important to large-scale optimization and that have proved useful in the installation of nonlinear solvers in the NEOS Server. Our discussion centers on the computation of the gradient and Hessian matrix for partially separable functions and shows that the gradient and Hessian matrix can be computed with guaranteed bounds in time and memory requirements",
        "published": "2001-01-03T17:33:59Z",
        "link": "http://arxiv.org/abs/cs/0101001v1",
        "categories": [
            "cs.MS",
            "G.1.6"
        ]
    },
    {
        "title": "GPCG: A Case Study in the Performance and Scalability of Optimization   Algorithms",
        "authors": [
            "Steven J. Benson",
            "Lois Curfman McInnes",
            "Jorge J. Moré"
        ],
        "summary": "GPCG is an algorithm within the Toolkit for Advanced Optimization (TAO) for solving bound constrained, convex quadratic problems. Originally developed by More' and Toraldo, this algorithm was designed for large-scale problems but had been implemented only for a single processor. The TAO implementation is available for a wide range of high-performance architecture, and has been tested on up to 64 processors to solve problems with over 2.5 million variables.",
        "published": "2001-01-19T21:14:52Z",
        "link": "http://arxiv.org/abs/cs/0101018v1",
        "categories": [
            "cs.MS",
            "G.1.6"
        ]
    },
    {
        "title": "Benchmarking Optimization Software with Performance Profiles",
        "authors": [
            "Elizabeth D. Dolan",
            "Jorge J. Moré"
        ],
        "summary": "We propose performance profiles-distribution functions for a performance metric-as a tool for benchmarking and comparing optimization software. We show that performance profiles combine the best features of other tools for performance evaluation.",
        "published": "2001-02-01T20:37:46Z",
        "link": "http://arxiv.org/abs/cs/0102001v2",
        "categories": [
            "cs.MS",
            "G.4; G.1.6"
        ]
    },
    {
        "title": "Lectures on Reduce and Maple at UAM I - Mexico",
        "authors": [
            "Marc Toussaint"
        ],
        "summary": "These lectures give a brief introduction to the Computer Algebra systems Reduce and Maple. The aim is to provide a systematic survey of most important commands and concepts. In particular, this includes a discussion of simplification schemes and the handling of simplification and substitution rules (e.g., a Lie Algebra is implemented in Reduce by means of simplification rules).   Another emphasis is on the different implementations of tensor calculi and the exterior calculus by Reduce and Maple and their application in Gravitation theory and Differential Geometry.   I held the lectures at the Universidad Autonoma Metropolitana-Iztapalapa, Departamento de Fisica, Mexico, in November 1999.",
        "published": "2001-05-25T15:08:13Z",
        "link": "http://arxiv.org/abs/cs/0105033v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "gr-qc",
            "I.1.3"
        ]
    },
    {
        "title": "Users Guide for SnadiOpt: A Package Adding Automatic Differentiation to   Snopt",
        "authors": [
            "E. Michael Gertz",
            "Philip E. Gill",
            "Julia Muetherig"
        ],
        "summary": "SnadiOpt is a package that supports the use of the automatic differentiation package ADIFOR with the optimization package Snopt. Snopt is a general-purpose system for solving optimization problems with many variables and constraints. It minimizes a linear or nonlinear function subject to bounds on the variables and sparse linear or nonlinear constraints. It is suitable for large-scale linear and quadratic programming and for linearly constrained optimization, as well as for general nonlinear programs. The method used by Snopt requires the first derivatives of the objective and constraint functions to be available. The SnadiOpt package allows users to avoid the time-consuming and error-prone process of evaluating and coding these derivatives. Given Fortran code for evaluating only the values of the objective and constraints, SnadiOpt automatically generates the code for evaluating the derivatives and builds the relevant Snopt input files and sparse data structures.",
        "published": "2001-06-25T19:30:10Z",
        "link": "http://arxiv.org/abs/cs/0106051v1",
        "categories": [
            "cs.MS",
            "G.1.6; G.1.4"
        ]
    },
    {
        "title": "Computer validated proofs of a toolset for adaptable arithmetic",
        "authors": [
            "Sylvie Boldo",
            "Marc Daumas",
            "Claire Moreau-Finot",
            "Laurent Thery"
        ],
        "summary": "Most existing implementations of multiple precision arithmetic demand that the user sets the precision {\\em a priori}. Some libraries are said adaptable in the sense that they dynamically change the precision of each intermediate operation individually to deliver the target accuracy according to the actual inputs. We present in this text a new adaptable numeric core inspired both from floating point expansions and from on-line arithmetic.   The numeric core is cut down to four tools. The tool that contains arithmetic operations is proved to be correct. The proofs have been formally checked by the Coq assistant. Developing the proofs, we have formally proved many results published in the literature and we have extended a few of them. This work may let users (i) develop application specific adaptable libraries based on the toolset and / or (ii) write new formal proofs based on the set of validated facts.",
        "published": "2001-07-19T13:18:31Z",
        "link": "http://arxiv.org/abs/cs/0107025v2",
        "categories": [
            "cs.MS",
            "G.4"
        ]
    },
    {
        "title": "TeXmacs interfaces to Maxima, MuPAD and REDUCE",
        "authors": [
            "A. G. Grozin"
        ],
        "summary": "GNU TeXmacs is a free wysiwyg word processor providing an excellent typesetting quality of texts and formulae. It can also be used as an interface to Computer Algebra Systems (CASs). In the present work, interfaces to three general-purpose CASs have been implemented.",
        "published": "2001-07-29T11:00:59Z",
        "link": "http://arxiv.org/abs/cs/0107036v2",
        "categories": [
            "cs.SC",
            "cs.MS",
            "hep-ph",
            "I.1.3;G.4"
        ]
    },
    {
        "title": "Lectures on Reduce and Maple at UAM I - Mexico",
        "authors": [
            "Marc Toussaint"
        ],
        "summary": "These lectures give a brief introduction to the Computer Algebra systems Reduce and Maple. The aim is to provide a systematic survey of most important commands and concepts. In particular, this includes a discussion of simplification schemes and the handling of simplification and substitution rules (e.g., a Lie Algebra is implemented in Reduce by means of simplification rules).   Another emphasis is on the different implementations of tensor calculi and the exterior calculus by Reduce and Maple and their application in Gravitation theory and Differential Geometry.   I held the lectures at the Universidad Autonoma Metropolitana-Iztapalapa, Departamento de Fisica, Mexico, in November 1999.",
        "published": "2001-05-25T15:08:13Z",
        "link": "http://arxiv.org/abs/cs/0105033v1",
        "categories": [
            "cs.SC",
            "cs.MS",
            "gr-qc",
            "I.1.3"
        ]
    },
    {
        "title": "The Set of Equations to Evaluate Objects",
        "authors": [
            "Larissa Ismailova"
        ],
        "summary": "The notion of an equational shell is studied to involve the objects and their environment. Appropriate methods are studied as valid embeddings of refined objects. The refinement process determines the linkages between the variety of possible representations giving rise to variants of computations. The case study is equipped with the adjusted equational systems that validate the initial applicative framework.",
        "published": "2001-06-08T09:28:46Z",
        "link": "http://arxiv.org/abs/cs/0106013v1",
        "categories": [
            "cs.LO",
            "cs.PL",
            "cs.SC",
            "D.3.1; D.3.2"
        ]
    },
    {
        "title": "MACE 2.0 Reference Manual and Guide",
        "authors": [
            "William McCune"
        ],
        "summary": "MACE is a program that searches for finite models of first-order statements. The statement to be modeled is first translated to clauses, then to relational clauses; finally for the given domain size, the ground instances are constructed. A Davis-Putnam-Loveland-Logeman procedure decides the propositional problem, and any models found are translated to first-order models. MACE is a useful complement to the theorem prover Otter, with Otter searching for proofs and MACE looking for countermodels.",
        "published": "2001-06-19T16:11:19Z",
        "link": "http://arxiv.org/abs/cs/0106042v1",
        "categories": [
            "cs.LO",
            "cs.SC",
            "I.2.3; I.2.8"
        ]
    },
    {
        "title": "TeXmacs interfaces to Maxima, MuPAD and REDUCE",
        "authors": [
            "A. G. Grozin"
        ],
        "summary": "GNU TeXmacs is a free wysiwyg word processor providing an excellent typesetting quality of texts and formulae. It can also be used as an interface to Computer Algebra Systems (CASs). In the present work, interfaces to three general-purpose CASs have been implemented.",
        "published": "2001-07-29T11:00:59Z",
        "link": "http://arxiv.org/abs/cs/0107036v2",
        "categories": [
            "cs.SC",
            "cs.MS",
            "hep-ph",
            "I.1.3;G.4"
        ]
    },
    {
        "title": "Set Unification",
        "authors": [
            "Agostino Dovier",
            "Enrico Pontelli",
            "Gianfranco Rossi"
        ],
        "summary": "The unification problem in algebras capable of describing sets has been tackled, directly or indirectly, by many researchers and it finds important applications in various research areas--e.g., deductive databases, theorem proving, static analysis, rapid software prototyping. The various solutions proposed are spread across a large literature. In this paper we provide a uniform presentation of unification of sets, formalizing it at the level of set theory. We address the problem of deciding existence of solutions at an abstract level. This provides also the ability to classify different types of set unification problems. Unification algorithms are uniformly proposed to solve the unification problem in each of such classes.   The algorithms presented are partly drawn from the literature--and properly revisited and analyzed--and partly novel proposals. In particular, we present a new goal-driven algorithm for general ACI1 unification and a new simpler algorithm for general (Ab)(Cl) unification.",
        "published": "2001-10-09T10:57:56Z",
        "link": "http://arxiv.org/abs/cs/0110023v2",
        "categories": [
            "cs.LO",
            "cs.AI",
            "cs.SC",
            "F.2.2; F.4.1; I.1.2; I.2.3"
        ]
    },
    {
        "title": "Deterministic computations whose history is independent of the order of   asynchronous updating",
        "authors": [
            "Peter Gacs"
        ],
        "summary": "Consider a network of processors (sites) in which each site x has a finite set N(x) of neighbors. There is a transition function f that for each site x computes the next state \\xi(x) from the states in N(x). But these transitions (updates) are applied in arbitrary order, one or many at a time. If the state of site x at time t is \\eta(x,t) then let us define the sequence \\zeta(x,0), \\zeta(x,1), ... by taking the sequence \\eta(x,0), \\eta(x,1), ..., and deleting repetitions. The function f is said to have invariant histories if the sequence \\zeta(x,i), (while it lasts, in case it is finite) depends only on the initial configuration, not on the order of updates.   This paper shows that though the invariant history property is typically undecidable, there is a useful simple sufficient condition, called commutativity: For any configuration, for any pair x,y of neighbors, if the updating would change both \\xi(x) and \\xi(y) then the result of updating first x and then y is the same as the result of doing this in the reverse order.",
        "published": "2001-01-24T20:33:21Z",
        "link": "http://arxiv.org/abs/cs/0101026v1",
        "categories": [
            "cs.DC",
            "cs.CC",
            "F.1.2"
        ]
    },
    {
        "title": "A Scientific Data Management System for Irregular Applications",
        "authors": [
            "Jaechun No",
            "Rajeev Thakur",
            "Dinesh Kaushik",
            "Lori Freitag",
            "Alok Choudhary"
        ],
        "summary": "Many scientific applications are I/O intensive and generate or access large data sets, spanning hundreds or thousands of \"files.\" Management, storage, efficient access, and analysis of this data present an extremely challenging task. We have developed a software system, called Scientific Data Manager (SDM), that uses a combination of parallel file I/O and database support for high-performance scientific data management. SDM provides a high-level API to the user and internally, uses a parallel file system to store real data and a database to store application-related metadata. In this paper, we describe how we designed and implemented SDM to support irregular applications. SDM can efficiently handle the reading and writing of data in an irregular mesh as well as the distribution of index values. We describe the SDM user interface and how we implemented it to achieve high performance. SDM makes extensive use of MPI-IO's noncontiguous collective I/O functions. SDM also uses the concept of a history file to optimize the cost of the index distribution using the metadata stored in the database. We present performance results with two irregular applications, a CFD code called FUN3D and a Rayleigh-Taylor instability code, on the SGI Origin2000 at Argonne National Laboratory.",
        "published": "2001-02-20T20:17:22Z",
        "link": "http://arxiv.org/abs/cs/0102016v1",
        "categories": [
            "cs.DC",
            "B.4; B.4.3"
        ]
    },
    {
        "title": "Components and Interfaces of a Process Management System for Parallel   Programs",
        "authors": [
            "Ralph Butler",
            "William Gropp",
            "Ewing Lusk"
        ],
        "summary": "Parallel jobs are different from sequential jobs and require a different type of process management. We present here a process management system for parallel programs such as those written using MPI. A primary goal of the system, which we call MPD (for multipurpose daemon), is to be scalable. By this we mean that startup of interactive parallel jobs comprising thousands of processes is quick, that signals can be quickly delivered to processes, and that stdin, stdout, and stderr are managed intuitively. Our primary target is parallel machines made up of clusters of SMPs, but the system is also useful in more tightly integrated environments. We describe how MPD enables much faster startup and better runtime management of parallel jobs. We show how close control of stdio can support the easy implementation of a number of convenient system utilities, even a parallel debugger. We describe a simple but general interface that can be used to separate any process manager from a parallel library, which we use to keep MPD separate from MPICH.",
        "published": "2001-02-21T16:04:21Z",
        "link": "http://arxiv.org/abs/cs/0102017v1",
        "categories": [
            "cs.DC",
            "C.1.4"
        ]
    },
    {
        "title": "Construction of an algorithm in parallel for the Fast Fourier Transform",
        "authors": [
            "G. Mario A. Higuera",
            "Humberto Sarria",
            "Diana Fonseca",
            "John Idarraga"
        ],
        "summary": "It has been designed,built and executed a code for the Fast Fourier Transform (FFT),compiled and executed in a cluster of 2^n computers under the operating system MacOS and using the routines MacMPI. As practical application,the code has been used to obtain the transformed from an astronomic imagen,to execute a filter on its and with a transformed inverse to recover the image with the variates given by the filter.The computers arrangement are installed in the Observatorio Astronomico National in Colombia under the name OAN Cluster and in this has been executed several applications.",
        "published": "2001-03-01T20:40:06Z",
        "link": "http://arxiv.org/abs/cs/0103001v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "D.1.3"
        ]
    },
    {
        "title": "Fitness Uniform Selection to Preserve Genetic Diversity",
        "authors": [
            "Marcus Hutter"
        ],
        "summary": "In evolutionary algorithms, the fitness of a population increases with time by mutating and recombining individuals and by a biased selection of more fit individuals. The right selection pressure is critical in ensuring sufficient optimization progress on the one hand and in preserving genetic diversity to be able to escape from local optima on the other. We propose a new selection scheme, which is uniform in the fitness values. It generates selection pressure towards sparsely populated fitness regions, not necessarily towards higher fitness, as is the case for all other selection schemes. We show that the new selection scheme can be much more effective than standard selection schemes.",
        "published": "2001-03-14T18:40:32Z",
        "link": "http://arxiv.org/abs/cs/0103015v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "q-bio",
            "I.2; I.2.6; I.2.8; F.2"
        ]
    },
    {
        "title": "On the NP-completeness of Finding an Optimal Strategy in Games with   Common Payoffs",
        "authors": [
            "Francis Chu",
            "Joseph Y. Halpern"
        ],
        "summary": "Consider a very simple class of (finite) games: after an initial move by nature, each player makes one move. Moreover, the players have common interests: at each node, all the players get the same payoff. We show that the problem of determining whether there exists a joint strategy where each player has an expected payoff of at least r is NP-complete as a function of the number of nodes in the extensive-form representation of the game.",
        "published": "2001-03-27T19:51:09Z",
        "link": "http://arxiv.org/abs/cs/0103019v1",
        "categories": [
            "cs.GT",
            "cs.CC",
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "A Dualheap Selection Algorithm - A Call for Analysis",
        "authors": [
            "Greg Sepesi"
        ],
        "summary": "An algorithm is presented that efficiently solves the selection problem: finding the k-th smallest member of a set. Relevant to a divide-and-conquer strategy, the algorithm also partitions a set into small and large valued subsets. Applied recursively, this partitioning results in a sorted set. The algorithm's applicability is therefore much broader than just the selection problem. The presented algorithm is based upon R.W. Floyd's 1964 algorithm that constructs a heap from the bottom-up. Empirically, the presented algorithm's performance appears competitive with the popular quickselect algorithm, a variant of C.A.R. Hoare's 1962 quicksort algorithm. Furthermore, constructing a heap from the bottom-up is an inherently parallel process (processors can work independently and simultaneously on subheap construction), suggesting a performance advantage with parallel implementations. Given the presented algorithm's broad applicability, simplicity, serial performance, and parallel nature, further study is warranted. Specifically, worst-case analysis is an important but still unsolved problem.",
        "published": "2001-03-28T18:17:19Z",
        "link": "http://arxiv.org/abs/cs/0103023v1",
        "categories": [
            "cs.DS",
            "cs.DC",
            "F.2.2; E.1"
        ]
    },
    {
        "title": "Secure, Efficient Data Transport and Replica Management for   High-Performance Data-Intensive Computing",
        "authors": [
            "Bill Allcock",
            "Joe Bester",
            "John Bresnahan",
            "Ann L. Chervenak",
            "Ian Foster",
            "Carl Kesselman",
            "Sam Meder",
            "Veronika Nefedova",
            "Darcy Quesnel",
            "Steven Tuecke"
        ],
        "summary": "An emerging class of data-intensive applications involve the geographically dispersed extraction of complex scientific information from very large collections of measured or computed data. Such applications arise, for example, in experimental physics, where the data in question is generated by accelerators, and in simulation science, where the data is generated by supercomputers. So-called Data Grids provide essential infrastructure for such applications, much as the Internet provides essential services for applications such as e-mail and the Web. We describe here two services that we believe are fundamental to any Data Grid: reliable, high-speed transporet and replica management. Our high-speed transport service, GridFTP, extends the popular FTP protocol with new features required for Data Grid applciations, such as striping and partial file access. Our replica management service integrates a replica catalog with GridFTP transfers to provide for the creation, registration, location, and management of dataset replicas. We present the design of both services and also preliminary performance results. Our implementations exploit security and other services provided by the Globus Toolkit.",
        "published": "2001-03-28T20:42:34Z",
        "link": "http://arxiv.org/abs/cs/0103022v1",
        "categories": [
            "cs.DC",
            "cs.DB",
            "C.1.4; E.1"
        ]
    },
    {
        "title": "The Anatomy of the Grid - Enabling Scalable Virtual Organizations",
        "authors": [
            "Ian Foster",
            "Carl Kesselman",
            "Steven Tuecke"
        ],
        "summary": "\"Grid\" computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high-performance orientation. In this article, we define this new field. First, we review the \"Grid problem,\" which we define as flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources-what we refer to as virtual organizations. In such settings, we encounter unique authentication, authorization, resource access, resource discovery, and other challenges. It is this class of problem that is addressed by Grid technologies. Next, we present an extensible and open Grid architecture, in which protocols, services, application programming interfaces, and software development kits are categorized according to their roles in enabling resource sharing. We describe requirements that we believe any such mechanisms must satisfy, and we discuss the central role played by the intergrid protocols that enable interoperability among different Grid systems. Finally, we discuss how Grid technologies relate to other contemporary technologies, including enterprise integration, application service provider, storage service provider, and peer-to-peer computing. We maintain that Grid concepts and technologies complement and have much to contribute to these other approaches.",
        "published": "2001-03-29T14:30:18Z",
        "link": "http://arxiv.org/abs/cs/0103025v1",
        "categories": [
            "cs.AR",
            "cs.DC",
            "C.1.4; C.2.4"
        ]
    },
    {
        "title": "Replica Selection in the Globus Data Grid",
        "authors": [
            "Sudharshan Vazhkudai",
            "Steven Tuecke",
            "Ian Foster"
        ],
        "summary": "The Globus Data Grid architecture provides a scalable infrastructure for the management of storage resources and data that are distributed across Grid environments. These services are designed to support a variety of scientific applications, ranging from high-energy physics to computational genomics, that require access to large amounts of data (terabytes or even petabytes) with varied quality of service requirements. By layering on a set of core services, such as data transport, security, and replica cataloging, one can construct various higher-level services. In this paper, we discuss the design and implementation of a high-level replica selection service that uses information regarding replica location and user preferences to guide selection from among storage replica alternatives. We first present a basic replica selection service design, then show how dynamic information collected using Globus information service capabilities concerning storage system properties can help improve and optimize the selection process. We demonstrate the use of Condor's ClassAds resource description and matchmaking mechanism as an efficient tool for representing and matching storage resource capabilities and policies against application requirements.",
        "published": "2001-04-02T18:19:06Z",
        "link": "http://arxiv.org/abs/cs/0104002v1",
        "categories": [
            "cs.DC",
            "C.1.4"
        ]
    },
    {
        "title": "Dijkstra's Self-Stabilizing Algorithm in Unsupportive Environments",
        "authors": [
            "Shlomi Dolev",
            "Ted Herman"
        ],
        "summary": "The first self-stabilizing algorithm [Dij73] assumed the existence of a central daemon, that activates one processor at time to change state as a function of its own state and the state of a neighbor. Subsequent research has reconsidered this algorithm without the assumption of a central daemon, and under different forms of communication, such as the model of link registers. In all of these investigations, one common feature is the atomicity of communication, whether by shared variables or read/write registers. This paper weakens the atomicity assumptions for the communication model, proposing versions of [Dij73] that tolerate various weaker forms of atomicity. First, a solution for the case of regular registers is presented. Then the case of safe registers is considered, with both negative and positive results presented. The paper also presents an implementation of [Dij73] based on registers that have probabilistically correct behavior, which requires a notion of weak stabilization.",
        "published": "2001-05-07T14:29:59Z",
        "link": "http://arxiv.org/abs/cs/0105013v1",
        "categories": [
            "cs.DC",
            "D.1.3; B.1.3; B.8.1; D.4.5"
        ]
    },
    {
        "title": "On the Area of Hypercube Layouts",
        "authors": [
            "Ronald I. Greenberg",
            "Lee Guan"
        ],
        "summary": "This paper precisely analyzes the wire density and required area in standard layout styles for the hypercube. The most natural, regular layout of a hypercube of N^2 nodes in the plane, in a N x N grid arrangement, uses floor(2N/3)+1 horizontal wiring tracks for each row of nodes. (The number of tracks per row can be reduced by 1 with a less regular design.) This paper also gives a simple formula for the wire density at any cut position and a full characterization of all places where the wire density is maximized (which does not occur at the bisection).",
        "published": "2001-05-29T21:59:18Z",
        "link": "http://arxiv.org/abs/cs/0105034v1",
        "categories": [
            "cs.DC",
            "C.1.2"
        ]
    },
    {
        "title": "Economic Models for Management of Resources in Grid Computing",
        "authors": [
            "Rajkumar Buyya",
            "Heinz Stockinger",
            "Jonathan Giddy",
            "David Abrams"
        ],
        "summary": "The accelerated development in Grid and peer-to-peer computing has positioned them as promising next generation computing platforms. They enable the creation of Virtual Enterprises (VE) for sharing resources distributed across the world. However, resource management, application development and usage models in these environments is a complex undertaking. This is due to the geographic distribution of resources that are owned by different organizations. The resource owners of each of these resources have different usage or access policies and cost models, and varying loads and availability. In order to address complex resource management issues, we have proposed a computational economy framework for resource allocation and for regulating supply and demand in Grid computing environments. The framework provides mechanisms for optimizing resource provider and consumer objective functions through trading and brokering services. In a real world market, there exist various economic models for setting the price for goods based on supply-and-demand and their value to the user. They include commodity market, posted price, tenders and auctions. In this paper, we discuss the use of these models for interaction between Grid components in deciding resource value and the necessary infrastructure to realize them. In addition to normal services offered by Grid computing systems, we need an infrastructure to support interaction protocols, allocation mechanisms, currency, secure banking, and enforcement services. Furthermore, we demonstrate the usage of some of these economic models in resource brokering through Nimrod/G deadline and cost-based scheduling for two different optimization strategies on the World Wide Grid (WWG) testbed.",
        "published": "2001-06-11T07:46:56Z",
        "link": "http://arxiv.org/abs/cs/0106020v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Simple and Effective Distributed Computing with a Scheduling Service",
        "authors": [
            "David M. Mackie"
        ],
        "summary": "High-throughput computing projects require the solution of large numbers of problems. In many cases, these problems can be solved on desktop PCs, or can be broken down into independent \"PC-solvable\" sub-problems. In such cases, the projects are high-performance computing projects, but only because of the sheer number of the needed calculations. We briefly describe our efforts to increase the throughput of one such project. We then explain how to easily set up a distributed computing facility composed of standard networked PCs running Windows 95, 98, 2000, or NT. The facility requires no special software or hardware, involves little or no re-coding of application software, and operates almost invisibly to the owners of the PCs. Depending on the number and quality of PCs recruited, performance can rival that of supercomputers.",
        "published": "2001-06-15T17:09:49Z",
        "link": "http://arxiv.org/abs/cs/0106038v1",
        "categories": [
            "cs.DC",
            "C.m; D.m"
        ]
    },
    {
        "title": "Software Toolkit for Building Embedded and Distributed Knowledge-based   Systems",
        "authors": [
            "Dmitri Soshnikov"
        ],
        "summary": "The paper discusses the basic principles and the architecture of the software toolkit for constructing knowledge-based systems which can be used cooperatively over computer networks and also embedded into larger software systems in different ways. Presented architecture is based on frame knowledge representation and production rules, which also allows to interface high-level programming languages and relational databases by exposing corresponding classes or database tables as frames. Frames located on the remote computers can also be transparently accessed and used in inference, and the dynamic knowledge for specific frames can also be transferred over the network. The issues of implementation of such a system are addressed, which use Java programming language, CORBA and XML for external knowledge representation. Finally, some applications of the toolkit are considered, including e-business approach to knowledge sharing, intelligent web behaviours, etc.",
        "published": "2001-06-26T19:50:54Z",
        "link": "http://arxiv.org/abs/cs/0106054v1",
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "I.2; D.2.12"
        ]
    },
    {
        "title": "Randomized Two-Process Wait-Free Test-and-Set",
        "authors": [
            "John Tromp",
            "Paul Vitanyi"
        ],
        "summary": "We present the first explicit, and currently simplest, randomized algorithm for 2-process wait-free test-and-set. It is implemented with two 4-valued single writer single reader atomic variables. A test-and-set takes at most 11 expected elementary steps, while a reset takes exactly 1 elementary step. Based on a finite-state analysis, the proofs of correctness and expected length are compressed into one table.",
        "published": "2001-06-28T15:45:35Z",
        "link": "http://arxiv.org/abs/cs/0106056v2",
        "categories": [
            "cs.DC",
            "C.2; F.2.2"
        ]
    },
    {
        "title": "Enabling the Long-Term Archival of Signed Documents through Time   Stamping",
        "authors": [
            "Petros Maniatis",
            "T. J. Giuli",
            "Mary Baker"
        ],
        "summary": "In this paper we describe how to build a trusted reliable distributed service across administrative domains in a peer-to-peer network. The application we use to motivate our work is a public key time stamping service called Prokopius. The service provides a secure, verifiable but distributable stable archive that maintains time stamped snapshots of public keys over time. This in turn allows clients to verify time stamped documents or certificates that rely on formerly trusted public keys that are no longer in service or where the signer no longer exists. We find that such a service can time stamp the snapshots of public keys in a network of 148 nodes at the granularity of a couple of days, even in the worst case where an adversary causes the maximal amount of damage allowable within our fault model.",
        "published": "2001-06-28T23:08:48Z",
        "link": "http://arxiv.org/abs/cs/0106058v1",
        "categories": [
            "cs.DC",
            "cs.CR",
            "C.2.4; K.6.5; H.3.4"
        ]
    },
    {
        "title": "A Multi-Threaded Fast Convolver for Dynamically Parallel Image Filtering",
        "authors": [
            "Jeremy Kepner"
        ],
        "summary": "2D convolution is a staple of digital image processing. The advent of large format imagers makes it possible to literally ``pave'' with silicon the focal plane of an optical sensor, which results in very large images that can require a significant amount computation to process. Filtering of large images via 2D convolutions is often complicated by a variety of effects (e.g., non-uniformities found in wide field of view instruments). This paper describes a fast (FFT based) method for convolving images, which is also well suited to very large images. A parallel version of the method is implemented using a multi-threaded approach, which allows more efficient load balancing and a simpler software architecture. The method has been implemented within in a high level interpreted language (IDL), while also exploiting open standards vector libraries (VSIPL) and open standards parallel directives (OpenMP). The parallel approach and software architecture are generally applicable to a variety of algorithms and has the advantage of enabling users to obtain the convenience of an easy operating environment while also delivering high performance using a fully portable code.",
        "published": "2001-07-05T03:31:15Z",
        "link": "http://arxiv.org/abs/astro-ph/0107084v1",
        "categories": [
            "astro-ph",
            "cs.DC"
        ]
    },
    {
        "title": "A Blueprint for Building Serverless Applications on the Net",
        "authors": [
            "A. I. Khan",
            "R. Spindler"
        ],
        "summary": "A peer-to-peer application architecture is proposed that has the potential to eliminate the back-end servers for hosting services on the Internet. The proposed application architecture has been modeled as a distributed system for delivering an Internet service. The service thus created, though chaotic and fraught with uncertainties, would be highly scalable and capable of achieving unprecedented levels of robustness and viability with the increase in the number of the users. The core issues relating to the architecture, such as service discovery, distributed application architecture components, and inter-application communications, have been analysed. It is shown that the communications for the coordination of various functions, among the cooperating instances of the application, may be optimised using a divide-and-conquer strategy. Finally, the areas where future work needs to be directed have been identified.",
        "published": "2001-07-05T07:03:48Z",
        "link": "http://arxiv.org/abs/cs/0107009v1",
        "categories": [
            "cs.DC",
            "cs.NI",
            "C.2.4; C.1.4; D.2.11"
        ]
    },
    {
        "title": "NEOS Server 4.0 Administrative Guide",
        "authors": [
            "Elizabeth D. Dolan"
        ],
        "summary": "The NEOS Server 4.0 provides a general Internet-based client/server as a link between users and software applications. The administrative guide covers the fundamental principals behind the operation of the NEOS Server, installation and trouble-shooting of the Server software, and implementation details of potential interest to a NEOS Server administrator. The guide also discusses making new software applications available through the Server, including areas of concern to remote solver administrators such as maintaining security, providing usage instructions, and enforcing reasonable restrictions on jobs. The administrative guide is intended both as an introduction to the NEOS Server and as a reference for use when running the Server.",
        "published": "2001-07-26T15:19:00Z",
        "link": "http://arxiv.org/abs/cs/0107034v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "The Cactus Worm: Experiments with Dynamic Resource Discovery and   Allocation in a Grid Environment",
        "authors": [
            "Gabrielle Allen",
            "David Angulo",
            "Ian Foster",
            "Gerd Lanfermann",
            "Chuang Liu",
            "Thomas Radke",
            "Ed Seidel",
            "John Shalf"
        ],
        "summary": "The ability to harness heterogeneous, dynamically available \"Grid\" resources is attractive to typically resource-starved computational scientists and engineers, as in principle it can increase, by significant factors, the number of cycles that can be delivered to applications. However, new adaptive application structures and dynamic runtime system mechanisms are required if we are to operate effectively in Grid environments. In order to explore some of these issues in a practical setting, we are developing an experimental framework, called Cactus, that incorporates both adaptive application structures for dealing with changing resource characteristics and adaptive resource selection mechanisms that allow applications to change their resource allocations (e.g., via migration) when performance falls outside specified limits. We describe here the adaptive resource selection mechanisms and describe how they are used to achieve automatic application migration to \"better\" resources following performance degradation. Our results provide insights into the architectural structures required to support adaptive resource selection. In addition, we suggest that this \"Cactus Worm\" is an interesting challenge problem for Grid computing.",
        "published": "2001-08-01T23:14:34Z",
        "link": "http://arxiv.org/abs/cs/0108001v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "Bounded Concurrent Timestamp Systems Using Vector Clocks",
        "authors": [
            "Sibsankar Haldar",
            "Paul Vitanyi"
        ],
        "summary": "Shared registers are basic objects used as communication mediums in asynchronous concurrent computation. A concurrent timestamp system is a higher typed communication object, and has been shown to be a powerful tool to solve many concurrency control problems. It has turned out to be possible to construct such higher typed objects from primitive lower typed ones. The next step is to find efficient constructions. We propose a very efficient wait-free construction of bounded concurrent timestamp systems from 1-writer multireader registers. This finalizes, corrects, and extends, a preliminary bounded multiwriter construction proposed by the second author in 1986. That work partially initiated the current interest in wait-free concurrent objects, and introduced a notion of discrete vector clocks in distributed algorithms.",
        "published": "2001-08-02T17:13:48Z",
        "link": "http://arxiv.org/abs/cs/0108002v1",
        "categories": [
            "cs.DC",
            "F.1.2; C.2.4; B.3.2;B.4.3;D.1.3;D.4.1;D.4.4"
        ]
    },
    {
        "title": "Verifying Sequential Consistency on Shared-Memory Multiprocessors by   Model Checking",
        "authors": [
            "Shaz Qadeer"
        ],
        "summary": "The memory model of a shared-memory multiprocessor is a contract between the designer and programmer of the multiprocessor. The sequential consistency memory model specifies a total order among the memory (read and write) events performed at each processor. A trace of a memory system satisfies sequential consistency if there exists a total order of all memory events in the trace that is both consistent with the total order at each processor and has the property that every read event to a location returns the value of the last write to that location.   Descriptions of shared-memory systems are typically parameterized by the number of processors, the number of memory locations, and the number of data values. It has been shown that even for finite parameter values, verifying sequential consistency on general shared-memory systems is undecidable. We observe that, in practice, shared-memory systems satisfy the properties of causality and data independence. Causality is the property that values of read events flow from values of write events. Data independence is the property that all traces can be generated by renaming data values from traces where the written values are distinct from each other. If a causal and data independent system also has the property that the logical order of write events to each location is identical to their temporal order, then sequential consistency can be verified algorithmically. Specifically, we present a model checking algorithm to verify sequential consistency on such systems for a finite number of processors and memory locations and an arbitrary number of data values.",
        "published": "2001-08-25T00:58:27Z",
        "link": "http://arxiv.org/abs/cs/0108016v1",
        "categories": [
            "cs.DC",
            "cs.AR",
            "B.3.3; C.1.2"
        ]
    },
    {
        "title": "Scalable Unix Commands for Parallel Processors: A High-Performance   Implementation",
        "authors": [
            "E. Ong",
            "E. Lusk",
            "W. Gropp"
        ],
        "summary": "We describe a family of MPI applications we call the Parallel Unix Commands. These commands are natural parallel versions of common Unix user commands such as ls, ps, and find, together with a few similar commands particular to the parallel environment. We describe the design and implementation of these programs and present some performance results on a 256-node Linux cluster. The Parallel Unix Commands are open source and freely available.",
        "published": "2001-08-27T15:54:41Z",
        "link": "http://arxiv.org/abs/cs/0108019v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "Parallel Computing on a PC Cluster",
        "authors": [
            "X. Q. Luo",
            "E. B. Gregory",
            "J. C. Yang",
            "Y. L. Wang",
            "D. Chang",
            "Y. Lin"
        ],
        "summary": "The tremendous advance in computer technology in the past decade has made it possible to achieve the performance of a supercomputer on a very small budget. We have built a multi-CPU cluster of Pentium PC capable of parallel computations using the Message Passing Interface (MPI). We will discuss the configuration, performance, and application of the cluster to our work in physics.",
        "published": "2001-09-04T14:03:15Z",
        "link": "http://arxiv.org/abs/cs/0109004v1",
        "categories": [
            "cs.DC",
            "hep-ph",
            "D.1.3"
        ]
    },
    {
        "title": "Learning from the Success of MPI",
        "authors": [
            "William D. Gropp"
        ],
        "summary": "The Message Passing Interface (MPI) has been extremely successful as a portable way to program high-performance parallel computers. This success has occurred in spite of the view of many that message passing is difficult and that other approaches, including automatic parallelization and directive-based parallelism, are easier to use. This paper argues that MPI has succeeded because it addresses all of the important issues in providing a parallel programming model.",
        "published": "2001-09-13T21:14:21Z",
        "link": "http://arxiv.org/abs/cs/0109017v1",
        "categories": [
            "cs.DC",
            "D.1.3"
        ]
    },
    {
        "title": "Routing Permutations in Partitioned Optical Passive Star Networks",
        "authors": [
            "Alessandro Mei",
            "Romeo Rizzi"
        ],
        "summary": "It is shown that a POPS network with g groups and d processors per group can efficiently route any permutation among the n=dg processors. The number of slots used is optimal in the worst case, and is at most the double of the optimum for all permutations p such that p(i)<>i for all i.",
        "published": "2001-09-18T10:05:26Z",
        "link": "http://arxiv.org/abs/cs/0109027v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "C.1.4"
        ]
    },
    {
        "title": "Multivariant Branching Prediction, Reflection, and Retrospection",
        "authors": [
            "Mark Burgin",
            "Walter Karplus",
            "Damon Liu"
        ],
        "summary": "In branching simulation, a novel approach to simulation presented in this paper, a multiplicity of plausible scenarios are concurrently developed and implemented. In conventional simulations of complex systems, there arise from time to time uncertainties as to which of two or more alternatives are more likely to be pursued by the system being simulated. Under these conditions the simulationist makes a judicious choice of one of these alternatives and embeds this choice in the simulation model. By contrast, in the branching approach, two or more of such alternatives (or branches) are included in the model and implemented for concurrent computer solution. The theoretical foundations for branching simulation as a computational process are in the domains of alternating Turing machines, molecular computing, and E-machines. Branching simulations constitute the development of diagrams of scenarios representing significant, alternative flows of events. Logical means for interpretation and investigation of the branching simulation and prediction are provided by the logical theories of possible worlds, which have been formalized by the construction of logical varieties. Under certain conditions, the branching approach can considerably enhance the efficiency of computer simulations and provide more complete insights into the interpretation of predictions based on simulations. As an example, the concepts developed in this paper have been applied to a simulation task that plays an important role in radiology - the noninvasive treatment of brain aneurysms.",
        "published": "2001-10-23T22:04:24Z",
        "link": "http://arxiv.org/abs/cs/0110048v1",
        "categories": [
            "cs.CE",
            "cs.DC",
            "F.1.2; F.1.1"
        ]
    },
    {
        "title": "Teaching Parallel Programming Using Both High-Level and Low-Level   Languages",
        "authors": [
            "Yi Pan"
        ],
        "summary": "We discuss the use of both MPI and OpenMP in the teaching of senior undergraduate and junior graduate classes in parallel programming. We briefly introduce the OpenMP standard and discuss why we have chosen to use it in parallel programming classes. Advantages of using OpenMP over message passing methods are discussed. We also include a brief enumeration of some of the drawbacks of using OpenMP and how these drawbacks are being addressed by supplementing OpenMP with additional MPI codes and projects. Several projects given in my class are also described in this paper.",
        "published": "2001-10-29T19:00:15Z",
        "link": "http://arxiv.org/abs/cs/0110058v1",
        "categories": [
            "cs.DC",
            "D.1.3; D.3.2"
        ]
    },
    {
        "title": "Distributed Computing for Localized and Multilayer Visualizations",
        "authors": [
            "Mark Burgin",
            "Walter Karplus",
            "Damon Liu"
        ],
        "summary": "The aim of this paper is to develop an approach to visualizations that benefits from distributed computing. Three schemes of process distribution are considered: parallel, pipeline, and expanding pipeline computations. Expanding pipeline structure synthesizes the advantages and traits of both parallel and pipeline computations. In expanding pipeline computing, a novel approach presented in this paper, a multiplicity of processes are concurrently developed in parallel and knotted processor pipelines. The theoretical foundations for expanding pipeline computing as a computational process are in the domains of alternating Turing machines, molecular computing, and E-machines. Expanding pipeline computing constitutes the development of the conventional pipeline architecture aimed at utilization of implicit parallel structures existing in algorithms. Such structures appear in various kinds of visualization. Image deriving and processing is a field that provides diverse opportunities for utilization of the advantages of distributed computing. The most relevant to the distributed architecture is stratified visualization with its two cases based on data localization and layer separation. Visualization is treated here as a special case of simulation. The conceptual approach to distributed computing developed in this paper have been applied to visualization in a computer support system, which is utilized in radiology and namely, for the noninvasive treatment of brain aneurysms.",
        "published": "2001-11-09T00:29:35Z",
        "link": "http://arxiv.org/abs/cs/0111022v1",
        "categories": [
            "cs.DC",
            "cs.DS",
            "F.1.2; I.1.2; H.3.4; I.4.10; J.3"
        ]
    },
    {
        "title": "Distributed Control System for the Test Interferometer of the ALMA   Project",
        "authors": [
            "M. Pokorny",
            "M. Brooks",
            "B. Glendenning",
            "G. Harris",
            "R. Heald",
            "F. Stauffer",
            "J. Pisano"
        ],
        "summary": "The control system (TICS) for the test interferometer being built to support the development of the Atacama Large Millimeter Array (ALMA)will itself be a prototype for the final ALMA array, providing a test for the distributed control system under development. TICS will be based on the ALMA Common Software (ACS) (developed at the European Southern Observatory), which provides CORBA-based services and a device management framework for the control software.   Simple device controllers will run on single board computers, one of which (known as an LCU) is located at each antenna; whereas complex, compound device controllers may run on centrally located computers. In either circumstance, client programs may obtain direct CORBA references to the devices and their properties. Monitor and control requests are sent to devices or properties, which then process and forward the commands to the appropriate hardware devices as required. Timing requirements are met by tagging commands with (future) timestamps synchronized to a timing pulse, which is regulated by a central reference generator, and is distributed to all hardware devices in the array. Monitoring is provided through a publish/subscribe CORBA-based service.",
        "published": "2001-11-09T00:33:54Z",
        "link": "http://arxiv.org/abs/cs/0111023v2",
        "categories": [
            "cs.DC",
            "physics.ins-det",
            "J.2"
        ]
    },
    {
        "title": "The ESRF TANGO control system status",
        "authors": [
            "JM. Chaize",
            "A. Goetz",
            "WD. Klotz",
            "J. Meyer",
            "M. Perez",
            "E. Taurel",
            "P. Verdier"
        ],
        "summary": "TANGO is an object oriented control system toolkit based on CORBA presently under development at the ESRF. IN this paper, the TANGO philosophy is briefly presented. All the existing tools developed around TANGO will also be presented. This include a code genrator, a WEB interface to TANGO objects, an administration tool and an interface to LabView. Finally, an xample of a TANGO device server for OPC device is given.",
        "published": "2001-11-09T14:29:16Z",
        "link": "http://arxiv.org/abs/cs/0111028v1",
        "categories": [
            "cs.DC",
            "C.2.4;D.1.3"
        ]
    },
    {
        "title": "Large-Scale Corba-Distributed Software Framework for Nif Controls",
        "authors": [
            "Robert W. Carey",
            "Kirby W. Fong",
            "Randy J. Sanchez",
            "Joseph D. Tappero",
            "John P. Woodruff"
        ],
        "summary": "The Integrated Computer Control System (ICCS) is based on a scalable software framework that is distributed over some 325 computers throughout the NIF facility. The framework provides templates and services at multiple levels of abstraction for the construction of software applications that communicate via CORBA (Common Object Request Broker Architecture). Various forms of object-oriented software design patterns are implemented as templates to be extended by application software. Developers extend the framework base classes to model the numerous physical control points, thereby sharing the functionality defined by the base classes. About 56,000 software objects each individually addressed through CORBA are to be created in the complete ICCS. Most objects have a persistent state that is initialized at system start-up and stored in a database. Additional framework services are provided by centralized server programs that implement events, alerts, reservations, message logging, database/file persistence, name services, and process management. The ICCS software framework approach allows for efficient construction of a software system that supports a large number of distributed control points representing a complex control application.",
        "published": "2001-11-09T16:40:09Z",
        "link": "http://arxiv.org/abs/cs/0111031v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Modernising the ESRF control system with GNU/Linux",
        "authors": [
            "A. Gotz",
            "A. Homs",
            "B. Regad",
            "M. Perez",
            "P. Makijarvi",
            "W. D. Klotz"
        ],
        "summary": "he ESRF control system is in the process of being modernised. The present contrsystem is based on VME, 10 MHz Ethernet, OS9, Solaris, HP-UX, NFS/RPC, Motif and C. The new control system will be based on compact PCI, 100 MHz Ethernet, Linux, Windows, Solaris, CORBA/IIOP, C++, Java and Python. The main frontend operating system will be GNU/Linux running on Intel/x86 and Motorola/68k. Linux will also be used on handheld devices for mobile control. This poster describes how GNU/Linux is being used to modernise the control system and what problems have been encountered so far",
        "published": "2001-11-09T20:08:18Z",
        "link": "http://arxiv.org/abs/cs/0111033v1",
        "categories": [
            "cs.DC",
            "C.3"
        ]
    },
    {
        "title": "Virtual Laboratory: Enabling On-Demand Drug Design with the World Wide   Grid",
        "authors": [
            "Rajkumar Buyya",
            "Kim Branson",
            "Jon Giddy",
            "David Abramson"
        ],
        "summary": "Computational Grids are emerging as a popular paradigm for solving large-scale compute and data intensive problems in science, engineering, and commerce. However, application composition, resource management and scheduling in these environments is a complex undertaking. In this paper, we illustrate the creation of a virtual laboratory environment by leveraging existing Grid technologies to enable molecular modeling for drug design on distributed resources. It involves screening millions of molecules of chemical compounds against a protein target, chemical database (CDB) to identify those with potential use for drug design. We have grid-enabled the molecular docking process by composing it as a parameter sweep application using the Nimrod-G tools. We then developed new tools for remote access to molecules in CDB small molecule database. The Nimrod-G resource broker along with molecule CDB data broker is used for scheduling and on-demand processing of jobs on distributed grid resources. The results demonstrate the ease of use and suitability of the Nimrod-G and virtual laboratory tools.",
        "published": "2001-11-17T07:20:58Z",
        "link": "http://arxiv.org/abs/cs/0111047v1",
        "categories": [
            "cs.DC",
            "J.3"
        ]
    },
    {
        "title": "A Computational Economy for Grid Computing and its Implementation in the   Nimrod-G Resource Brok",
        "authors": [
            "David Abramson",
            "Rajkumar Buyya",
            "Jonathan Giddy"
        ],
        "summary": "Computational Grids, coupling geographically distributed resources such as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments continue to be a complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service. Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user defined quality of service requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength Grids. We discuss results of preliminary experiments on scheduling some parametric computations using the Nimrod-G resource broker on a world-wide grid testbed that spans five continents.",
        "published": "2001-11-17T07:32:44Z",
        "link": "http://arxiv.org/abs/cs/0111048v1",
        "categories": [
            "cs.DC",
            "C.2.4"
        ]
    },
    {
        "title": "Systolic and Hyper-Systolic Algorithms for the Gravitational N-Body   Problem, with an Application to Brownian Motion",
        "authors": [
            "E. N. Dorband",
            "Marc Hemsendorf",
            "David Merritt"
        ],
        "summary": "A systolic algorithm rhythmically computes and passes data through a network of processors. We investigate the performance of systolic algorithms for implementing the gravitational N-body problem on distributed-memory computers. Systolic algorithms minimize memory requirements by distributing the particles between processors. We show that the performance of systolic routines can be greatly enhanced by the use of non-blocking communication, which allows particle coordinates to be communicated at the same time that force calculations are being carried out. Hyper-systolic algorithms reduce the communication complexity at the expense of increased memory demands. As an example of an application requiring large N, we use the systolic algorithm to carry out direct-summation simulations using 10^6 particles of the Brownian motion of the supermassive black hole at the center of the Milky Way galaxy. We predict a 3D random velocity of 0.4 km/s for the black hole.",
        "published": "2001-12-04T21:00:18Z",
        "link": "http://arxiv.org/abs/astro-ph/0112092v1",
        "categories": [
            "astro-ph",
            "cs.DC",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Going through Rough Times: from Non-Equilibrium Surface Growth to   Algorithmic Scalability",
        "authors": [
            "G. Korniss",
            "M. A. Novotny",
            "P. A. Rikvold",
            "H. Guclu",
            "Z. Toroczkai"
        ],
        "summary": "Efficient and faithful parallel simulation of large asynchronous systems is a challenging computational problem. It requires using the concept of local simulated times and a synchronization scheme. We study the scalability of massively parallel algorithms for discrete-event simulations which employ conservative synchronization to enforce causality. We do this by looking at the simulated time horizon as a complex evolving system, and we identify its universal characteristics. We find that the time horizon for the conservative parallel discrete-event simulation scheme exhibits Kardar-Parisi-Zhang-like kinetic roughening. This implies that the algorithm is asymptotically scalable in the sense that the average progress rate of the simulation approaches a non-zero constant. It also implies, however, that there are diverging memory requirements associated with such schemes.",
        "published": "2001-12-06T20:10:18Z",
        "link": "http://arxiv.org/abs/cond-mat/0112103v1",
        "categories": [
            "cond-mat.stat-mech",
            "cond-mat.mtrl-sci",
            "cs.DC",
            "cs.PF",
            "physics.comp-ph"
        ]
    },
    {
        "title": "Computing the average parallelism in trace monoids",
        "authors": [
            "Daniel Krob",
            "Jean Mairesse",
            "Ioannis Michos"
        ],
        "summary": "The {\\em height} of a trace is the height of the corresponding heap of pieces in Viennot's representation, or equivalently the number of factors in its Cartier-Foata decomposition. Let $h(t)$ and $|t|$ stand respectively for the height and the length of a trace $t$. Roughly speaking, $|t|$ is the `sequential' execution time and $h(t)$ is the `parallel' execution time. We prove that the bivariate commutative series $\\sum_t x^{h(t)}y^{|t|}$ is rational, and we give a finite representation of it. We use the rationality to obtain precise information on the asymptotics of the number of traces of a given height or length. Then, we study the average height of a trace for various probability distributions on traces. For the uniform probability distribution on traces of the same length (resp. of the same height), the asymptotic average height (resp. length) exists and is an algebraic number. To illustrate our results and methods, we consider a couple of examples: the free commutative monoid and the trace monoid whose independence graph is the ladder graph.",
        "published": "2001-12-11T16:32:12Z",
        "link": "http://arxiv.org/abs/cs/0112012v2",
        "categories": [
            "cs.DM",
            "cs.DC",
            "G.2.1; G.2.2; F.4.3"
        ]
    },
    {
        "title": "Concurrent computing machines and physical space-time",
        "authors": [
            "Philippe Matherat",
            "Marc-Thierry Jaekel"
        ],
        "summary": "Concrete computing machines, either sequential or concurrent, rely on an intimate relation between computation and time. We recall the general characteristic properties of physical time and of present realizations of computing systems. We emphasize the role of computing interferences, i.e. the necessity to avoid them in order to give a causal implementation to logical operations. We compare synchronous and asynchronous systems, and make a brief survey of some methods used to deal with computing interferences. Using a graphic representation, we show that synchronous and asynchronous circuits reflect the same opposition as the Newtonian and relativistic causal structures for physical space-time.",
        "published": "2001-12-17T14:10:32Z",
        "link": "http://arxiv.org/abs/cs/0112020v1",
        "categories": [
            "cs.DC",
            "cs.LO",
            "gr-qc",
            "F.1.2;B.6.1;D.3.3"
        ]
    },
    {
        "title": "Communities of Practice in the Distributed International Environment",
        "authors": [
            "Paul Hildreth",
            "Chris Kimble",
            "Peter Wright"
        ],
        "summary": "Modern commercial organisations are facing pressures which have caused them to lose personnel. When they lose people, they also lose their knowledge. Organisations also have to cope with the internationalisation of business forcing collaboration and knowledge sharing across time and distance. Knowledge Management (KM) claims to tackle these issues. This paper looks at an area where KM does not offer sufficient support, that is, the sharing of knowledge that is not easy to articulate.   The focus in this paper is on Communities of Practice in commercial organisations. We do this by exploring knowledge sharing in Lave and Wenger's [1] theory of Communities of Practice and investigating how Communities of Practice may translate to a distributed international environment. The paper reports on two case studies that explore the functioning of Communities of Practice across international boundaries.",
        "published": "2001-01-16T12:18:12Z",
        "link": "http://arxiv.org/abs/cs/0101012v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.5.3"
        ]
    },
    {
        "title": "Tap Tips: Lightweight Discovery of Touchscreen Targets",
        "authors": [
            "Paul M. Aoki",
            "Amy Hurst",
            "Allison Woodruff"
        ],
        "summary": "We describe tap tips, a technique for providing touch-screen target location hints. Tap tips are lightweight in that they are non-modal, appear only when needed, require a minimal number of user gestures, and do not add to the standard touchscreen gesture vocabulary. We discuss our implementation of tap tips in an electronic guidebook system and some usability test results.",
        "published": "2001-01-26T07:27:02Z",
        "link": "http://arxiv.org/abs/cs/0101029v1",
        "categories": [
            "cs.HC",
            "H.5.2; H.5.4; I.3.6"
        ]
    },
    {
        "title": "The Guidebook, the Friend, and the Room: Visitor Experience in a   Historic House",
        "authors": [
            "Allison Woodruff",
            "Paul M. Aoki",
            "Amy Hurst",
            "Margaret H. Szymanski"
        ],
        "summary": "In this paper, we describe an electronic guidebook prototype and report on a study of its use in a historic house. Supported by mechanisms in the guidebook, visitors constructed experiences that had a high degree of interaction with three entities: the guidebook, their companions, and the house and its contents. For example, we found that most visitors played audio descriptions played through speakers (rather than using headphones or reading textual descriptions) to facilitate communication with their companions.",
        "published": "2001-01-28T07:14:46Z",
        "link": "http://arxiv.org/abs/cs/0101035v2",
        "categories": [
            "cs.HC",
            "H.5.1; H.5.2; J.5"
        ]
    },
    {
        "title": "Communities of Practice: Going Virtual",
        "authors": [
            "Chris Kimble",
            "Paul Hildreth",
            "Peter Wright"
        ],
        "summary": "With the current trends towards downsizing, outsourcing and globalisation, modern organisations are reducing the numbers of people they employ. In addition, organisations now have to cope with the increasing internationalisation of business forcing collaboration and knowledge sharing across time and distance simultaneously. There is a need for new ways of thinking about how knowledge is shared in distributed groups. In this paper we explore a relatively new approach to knowledge sharing using Lave and Wenger's (1991) theory of Communities of Practice (CoPs). We investigate whether CoPs might translate to a geographically distributed international environment through a case study that explores the functioning of a CoP across national boundaries.",
        "published": "2001-02-26T15:22:27Z",
        "link": "http://arxiv.org/abs/cs/0102028v1",
        "categories": [
            "cs.HC",
            "cs.CY",
            "H.5.3"
        ]
    },
    {
        "title": "Computer based Information Systems and Managers' Work",
        "authors": [
            "Chris Kimble",
            "Kevin McLoughlin"
        ],
        "summary": "This paper identifies three categories of model: the Technology Impact Model; the Social Impact Model and the Integrationist Model, which imply different views of the \"impact\" of Information Technology on work organisation. These models are used to structure data from case studies conducted by the authors to explore the implications of the use of computer-based information systems for managers' work. The paper argues that the \"impact\" of information systems is not a single stable and predictable outcome but a non-linear ongoing process that changes and evolves over time. It also argues that the actions of individuals and groups within an organisation are not wholly determined by outside forces: people can and do react to, and shape, systems in different ways. In this sense, the \"impact\" of computer-based information systems on managers' work reflects decisions made by managers themselves about how the technology is used.",
        "published": "2001-02-26T15:45:03Z",
        "link": "http://arxiv.org/abs/cs/0102029v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "K.4.3"
        ]
    },
    {
        "title": "Toward Natural Gesture/Speech Control of a Large Display",
        "authors": [
            "S. Kettebekov",
            "R. Sharma"
        ],
        "summary": "In recent years because of the advances in computer vision research, free hand gestures have been explored as means of human-computer interaction (HCI). Together with improved speech processing technology it is an important step toward natural multimodal HCI. However, inclusion of non-predefined continuous gestures into a multimodal framework is a challenging problem. In this paper, we propose a structured approach for studying patterns of multimodal language in the context of a 2D-display control. We consider systematic analysis of gestures from observable kinematical primitives to their semantics as pertinent to a linguistic structure. Proposed semantic classification of co-verbal gestures distinguishes six categories based on their spatio-temporal deixis. We discuss evolution of a computational framework for gesture and speech integration which was used to develop an interactive testbed (iMAP). The testbed enabled elicitation of adequate, non-sequential, multimodal patterns in a narrative mode of HCI. Conducted user studies illustrate significance of accounting for the temporal alignment of gesture and speech parts in semantic mapping. Furthermore, co-occurrence analysis of gesture/speech production suggests syntactic organization of gestures at the lexical level.",
        "published": "2001-05-17T15:41:29Z",
        "link": "http://arxiv.org/abs/cs/0105026v1",
        "categories": [
            "cs.CV",
            "cs.HC",
            "H.5.2; I.5.4; I.2.7"
        ]
    },
    {
        "title": "Computational properties of environment-based disambiguation",
        "authors": [
            "William Schuler"
        ],
        "summary": "The standard pipeline approach to semantic processing, in which sentences are morphologically and syntactically resolved to a single tree before they are interpreted, is a poor fit for applications such as natural language interfaces. This is because the environment information, in the form of the objects and events in the application's run-time environment, cannot be used to inform parsing decisions unless the input sentence is semantically analyzed, but this does not occur until after parsing in the single-tree semantic architecture. This paper describes the computational properties of an alternative architecture, in which semantic analysis is performed on all possible interpretations during parsing, in polynomial time.",
        "published": "2001-06-07T15:47:31Z",
        "link": "http://arxiv.org/abs/cs/0106011v1",
        "categories": [
            "cs.CL",
            "cs.HC",
            "I.2.7; H.5.2"
        ]
    },
    {
        "title": "location.location.location: Internet Addresses as Evolving Property",
        "authors": [
            "Kenton K. Yee"
        ],
        "summary": "I describe recent developments in the rules governing registration and ownership of Internet and World Wide Web addresses or \"domain names.\" I consider the idea that \"virtual\" properties like domain names are more similar to real estate than to trademarks. Therefore, it would be economically efficient to grant domain name owners stronger rights than those of trademarks and copyright holders.",
        "published": "2001-06-14T02:02:11Z",
        "link": "http://arxiv.org/abs/cs/0106033v2",
        "categories": [
            "cs.CY",
            "cs.HC",
            "cs.IR",
            "C.0; J.4; K.0; K.1; K.4; K.4.4; K.5; K.5.1; K.7.4"
        ]
    },
    {
        "title": "Electronic Commerce, Consumer Search and Retailing Cost Reduction",
        "authors": [
            "Pedro Pereira",
            "Cristina Mazón"
        ],
        "summary": "This paper explains four things in a unified way. First, how e-commerce can generate price equilibria where physical shops either compete with virtual shops for consumers with Internet access, or alternatively, sell only to consumers with no Internet access. Second, how these price equilibria might involve price dispersion on-line. Third, why prices may be higher on-line. Fourth, why established firms can, but need not, be more reluctant than newly created firm to adopt e-commerce. For this purpose we develop a model where e-commerce reduces consumers' search costs, involves trade-offs for consumers, and reduces retailing costs.",
        "published": "2001-10-02T12:08:41Z",
        "link": "http://arxiv.org/abs/cs/0110006v1",
        "categories": [
            "cs.HC",
            "K.4.m Miscellaneous"
        ]
    },
    {
        "title": "Limits To Certainty in QoS Pricing and Bandwidth",
        "authors": [
            "Carolyn Gideon",
            "L Jean Camp"
        ],
        "summary": "Advanced services require more reliable bandwidth than currently provided by the Internet Protocol, even with the reliability enhancements provided by TCP. More reliable bandwidth will be provided through QoS (quality of service), as currently discussed widely. Yet QoS has some implications beyond providing ubiquitous access to advance Internet service, which are of interest from a policy perspective. In particular, what are the implications for price of Internet services? Further, how will these changes impact demand and universal service for the Internet. This paper explores the relationship between certainty of bandwidth and certainty of price for Internet services over a statistically shared network and finds that these are mutually exclusive goals.",
        "published": "2001-10-03T19:32:08Z",
        "link": "http://arxiv.org/abs/cs/0110016v1",
        "categories": [
            "cs.CY",
            "cs.HC",
            "K.4.m Miscellaneous"
        ]
    },
    {
        "title": "Mixed-Initiative Interaction = Mixed Computation",
        "authors": [
            "Naren Ramakrishnan",
            "Robert Capra",
            "Manuel A. Perez-Quinones"
        ],
        "summary": "We show that partial evaluation can be usefully viewed as a programming model for realizing mixed-initiative functionality in interactive applications. Mixed-initiative interaction between two participants is one where the parties can take turns at any time to change and steer the flow of interaction. We concentrate on the facet of mixed-initiative referred to as `unsolicited reporting' and demonstrate how out-of-turn interactions by users can be modeled by `jumping ahead' to nested dialogs (via partial evaluation). Our approach permits the view of dialog management systems in terms of their native support for staging and simplifying interactions; we characterize three different voice-based interaction technologies using this viewpoint. In particular, we show that the built-in form interpretation algorithm (FIA) in the VoiceXML dialog management architecture is actually a (well disguised) combination of an interpreter and a partial evaluator.",
        "published": "2001-10-09T14:33:55Z",
        "link": "http://arxiv.org/abs/cs/0110022v1",
        "categories": [
            "cs.PL",
            "cs.HC",
            "F3.2; H.5.2"
        ]
    },
    {
        "title": "Explaining Scenarios for Information Personalization",
        "authors": [
            "Naren Ramakrishnan",
            "Mary Beth Rosson",
            "John M. Carroll"
        ],
        "summary": "Personalization customizes information access. The PIPE (\"Personalization is Partial Evaluation\") modeling methodology represents interaction with an information space as a program. The program is then specialized to a user's known interests or information seeking activity by the technique of partial evaluation. In this paper, we elaborate PIPE by considering requirements analysis in the personalization lifecycle. We investigate the use of scenarios as a means of identifying and analyzing personalization requirements. As our first result, we show how designing a PIPE representation can be cast as a search within a space of PIPE models, organized along a partial order. This allows us to view the design of a personalization system, itself, as specialized interpretation of an information space. We then exploit the underlying equivalence of explanation-based generalization (EBG) and partial evaluation to realize high-level goals and needs identified in scenarios; in particular, we specialize (personalize) an information space based on the explanation of a user scenario in that information space, just as EBG specializes a theory based on the explanation of an example in that theory. In this approach, personalization becomes the transformation of information spaces to support the explanation of usage scenarios. An example application is described.",
        "published": "2001-11-05T19:02:47Z",
        "link": "http://arxiv.org/abs/cs/0111007v1",
        "categories": [
            "cs.HC",
            "cs.IR",
            "H.3.5; H.4.2; H.5.4; I.2.6; K.8"
        ]
    },
    {
        "title": "Building Multi-Platform User Interfaces with UIML",
        "authors": [
            "Mir Farooq Ali",
            "Manuel A. Perez-Quinones",
            "Eric Shell",
            "Marc Abrams"
        ],
        "summary": "There has been a widespread emergence of computing devices in the past few years that go beyond the capabilities of traditional desktop computers. However, users want to use the same kinds of applications and access the same data and information on these appliances that they can access on their desktop computers. The user interfaces for these platforms go beyond the traditional interaction metaphors. It is a challenge to build User Interfaces (UIs) for these devices of differing capabilities that allow the end users to perform the same kinds of tasks. The User Interface Markup Language (UIML) is an XML-based language that allows the canonical description of UIs for different platforms. We describe the language features of UIML that facilitate the development of multi-platform UIs. We also describe the key aspects of our approach that makes UIML succeed where previous approaches failed, namely the division in the representation of a UI, the use of a generic vocabulary, and an integrated development environment specifically designed for transformation-based UI development. Finally we describe the initial details of a multi-step usability engineering process for building multi-platform UI using UIML.",
        "published": "2001-11-09T01:02:00Z",
        "link": "http://arxiv.org/abs/cs/0111024v1",
        "categories": [
            "cs.HC",
            "H.5.2;H.5.4;I.3.6;D.2.2"
        ]
    },
    {
        "title": "A Multi-Step Process for Generating Multi-Platform User Interfaces using   UIML",
        "authors": [
            "Mir Farooq Ali",
            "Manuel A. Perez-Quinones",
            "Marc Abrams"
        ],
        "summary": "There has been a widespread emergence of computing devices in the past few years that go beyond the capabilities of traditional desktop computers. These devices have varying input/output characteristics, modalities and interaction mechanisms. However, users want to use the same kinds of applications and access the same data and information on these appliances that they can access on their desktop computers. The user interfaces for these devices and platforms go beyond the traditional interaction metaphors. It is a challenge to build User Interfaces (UIs) for these devices of differing capabilities that allow the end users to perform the same kinds of tasks. The User Interface Markup Language (UIML) is an XML-based language that allows the canonical description of UIs for different platforms. We present a multi-step transformation-based framework for building Multi-Platform User Interfaces using UIML. We describe the language features of UIML that facilitate the development of multi-platform UIs, the multi-step process involved in our framework and the transformations needed to build the UIs.",
        "published": "2001-11-09T01:10:00Z",
        "link": "http://arxiv.org/abs/cs/0111025v1",
        "categories": [
            "cs.HC",
            "H.5.2;H.5.4;I.3.6;D.2.2"
        ]
    }
]